<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
  <front>
    <journal-meta>
      <journal-title-group>
        <journal-title>The Journal of Systems &amp; Software</journal-title>
      </journal-title-group>
    </journal-meta>
    <article-meta>
      <title-group>
        <article-title>Learning software configuration spaces: A systematic literature ✩ review</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <string-name>Juliana Alves Pereira</string-name>
          <email>jpereira@inf.puc-rio.br</email>
          <xref ref-type="aff" rid="aff0">0</xref>
          <xref ref-type="aff" rid="aff2">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Mathieu Acher</string-name>
          <xref ref-type="aff" rid="aff2">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Hugo Martin</string-name>
          <xref ref-type="aff" rid="aff2">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Jean-Marc Jézéquel</string-name>
          <xref ref-type="aff" rid="aff2">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Goetz Botterweck</string-name>
          <xref ref-type="aff" rid="aff1">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Anthony Ventresque</string-name>
          <xref ref-type="aff" rid="aff3">3</xref>
        </contrib>
        <aff id="aff0">
          <label>0</label>
          <institution>Pontifical Catholic University of Rio de Janeiro</institution>
          ,
          <country country="BR">Brazil</country>
        </aff>
        <aff id="aff1">
          <label>1</label>
          <institution>Trinity College Dublin, Lero-The Irish Software Research Centre</institution>
          ,
          <country country="IE">Ireland</country>
        </aff>
        <aff id="aff2">
          <label>2</label>
          <institution>Univ Rennes</institution>
          ,
          <addr-line>Inria, CNRS, IRISA</addr-line>
          ,
          <country country="FR">France</country>
        </aff>
        <aff id="aff3">
          <label>3</label>
          <institution>University College Dublin</institution>
          ,
          <country country="IE">Ireland</country>
        </aff>
      </contrib-group>
      <pub-date>
        <year>2021</year>
      </pub-date>
      <volume>182</volume>
      <issue>2021</issue>
      <history>
        <date date-type="accepted">
          <day>12</day>
          <month>7</month>
          <year>2021</year>
        </date>
        <date date-type="received">
          <day>7</day>
          <month>6</month>
          <year>2019</year>
        </date>
        <date date-type="revised">
          <day>15</day>
          <month>4</month>
          <year>2021</year>
        </date>
      </history>
      <abstract>
        <p>a b s t r a c t Most modern software systems (operating systems like Linux or Android, Web browsers like Firefox or Chrome, video encoders like ffmpeg, x264 or VLC, mobile and cloud applications, etc.) are highly configurable. Hundreds of configuration options, features, or plugins can be combined, each potentially with distinct functionality and effects on execution time, security, energy consumption, etc. Due to the combinatorial explosion and the cost of executing software, it is quickly impossible to exhaustively explore the whole configuration space. Hence, numerous works have investigated the idea of learning it from a small sample of configurations' measurements. The pattern ''sampling, measuring, learning" has emerged in the literature, with several practical interests for both software developers and end-users of configurable systems. In this systematic literature review, we report on the different application objectives (e.g., performance prediction, configuration optimization, constraint mining), use-cases, targeted software systems, and application domains. We review the various strategies employed to gather a representative and cost-effective sample. We describe automated software techniques used to measure functional and non-functional properties of configurations. We classify machine learning algorithms and how they relate to the pursued application. Finally, we also describe how researchers evaluate the quality of the learning process. The findings from this systematic review show that the potential application objective is important; there are a vast number of case studies reported in the literature related to particular domains or software systems. Yet, the huge variant space of configurable systems is still challenging and calls to further investigate the synergies between artificial intelligence and software engineering.</p>
      </abstract>
      <kwd-group>
        <kwd>Systematic literature review</kwd>
        <kwd>Software product lines</kwd>
        <kwd>Machine learning</kwd>
        <kwd>Configurable systems</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <fig>
      <graphic xlink:href="pdf_reader\article\Learning software configuration spaces - A systematic literature.images\img_1_1.png" />
    </fig>
    <fig>
      <graphic xlink:href="pdf_reader\article\Learning software configuration spaces - A systematic literature.images\img_1_2.png" />
    </fig>
    <fig>
      <graphic xlink:href="pdf_reader\article\Learning software configuration spaces - A systematic literature.images\img_4_1.png" />
    </fig>
    <sec id="sec-1">
      <title>-</title>
      <p>i n f o</p>
    </sec>
    <sec id="sec-2">
      <title>1. Introduction</title>
      <p>
        End-users, system administrators, software engineers, and
scientists have at their disposal thousands of options (a.k.a. features
or parameters) to configure various kinds of software systems in
order to fit their functional and non-functional needs (execution
time, output quality, security, energy consumption, etc.). It is
now ubiquitous that software comes in many variants and is
highly configurable through conditional compilations,
commandline options, runtime parameters, configuration files, or plugins.
Software product lines (SPLs), software generators, dynamic
system, self-adaptive systems, variability-intensive systems are well
studied in the literature and enter in this class of configurable
software systems
        <xref ref-type="bibr" rid="ref106 ref117 ref117 ref13 ref15 ref36 ref66 ref82 ref9 ref92">(Svahnberg et al., 2005; Pohl et al., 2005; Apel
et al., 2013; Sayagh et al., 2018; Benavides et al., 2010; Cashman
et al., 2018; Hallsteinsen et al., 2008; Morin et al., 2009)</xref>
        .
      </p>
      <p>From an abstract point of view, a software configuration is
simply a combination of options’ values. Though customization
is highly desirable, it introduces an enormous complexity due
to the combinatorial explosion of possible variants. For example,
the Linux kernel has 15,000+ options and most of them can
have 3 values: ‘‘yes", ‘‘no", or ‘‘module". Without considering the
presence of constraints to avoid some combinations of options,
there may be 315,000 possible variants of Linux — the estimated
number of atoms in the universe is 1080 and is already reached
with 300 Boolean options. Though Linux is an extreme case, many
software systems or projects exhibit a very large configuration
space; this might bring several challenges.</p>
      <p>
        On the one hand, developers struggle to maintain, understand,
and test configuration spaces since they can hardly analyze or
execute all possible variants. According to several studies
        <xref ref-type="bibr" rid="ref117 ref34 ref92">(Halin
et al., 2019; Sayagh et al., 2018)</xref>
        , the flexibility brought by
variability is expensive as configuration failures represent one of the
most common types of software failures. Configuration failures is
an ‘‘undesired effect observed in the system’s delivered service’’
that may occur due to a specific combination of options’ values
(configurations)
        <xref ref-type="bibr" rid="ref62">(Mathur, 2008)</xref>
        . On the other hand, end-users
fear software variability and stick to default configurations
        <xref ref-type="bibr" rid="ref126 ref132">(Xu
et al., 2015; Zheng et al., 2007)</xref>
        that may be sub-optimal (e.g., the
software system will run very slowly) or simply inadequate (e.g.,
the quality of the output will be unsuitable).
      </p>
      <p>Since it is hardly possible to fully explore all software
configurations, the use of machine learning techniques is a quite
natural and appealing approach. The basic idea is to learn out of
a sample of configurations’ observations and hopefully generalize
to the whole configuration space. There are several applications
ranging from performance prediction, configuration
optimization, software understanding to constraint mining (i.e., extraction
of variability rules) – we will give a more exhaustive list in
this literature review. For instance, end-users of x264 (a
configurable video encoder) can estimate in advance the execution time
of the command-line X264 -no_cabac -no_fast_pskip
rc_lookahead 60 -ref 5 -o vid.264 vid.y4m (see Fig. 1),
since a machine learning model has been crafted to predict the
performance of configurations. End-users may want to use the
fastest configuration or know all configurations that meet an
objective (e.g., encoding time should be less than 10 s). Developers
of x264 can be interested in understanding the effects of some
options and how options interact.</p>
      <p>
        For all these use-cases, a pattern has emerged in the scientific
literature: ‘‘sampling, measuring, learning". The basic principle is
that a procedure is able to learn out of a sample of configurations’
measurements (see Fig. 1). Specifically, many software
configuration problems can actually be framed as statistical machine
learning problems under the condition a sample of
configurations’ observations is available. For example, the prediction of the
performance of individual configurations can be formulated as a
regression problem; appropriate learning algorithms (e.g., CART)
can then be used to predict the performance of untested, new
configurations. In this respect, it is worth noticing the dual use
of the term feature in the software or machine learning fields:
features either refer to software features (a.k.a. configuration
options) or to variables a regressor aims to relate. A way to
reconcile and visualize both is to consider a configuration matrix
as depicted in Fig. 1. In a configuration matrix, each row describes
a configuration together with observations/values of each feature
and performance property. In the example of Fig. 1, the first
configuration has the feature no_cabac set to False value and the
feature ref set to 9 value while the encoding time performance
value is 3.1876 s. We can use a sample of configurations (i.e.,
a set of measured configurations) to train a machine learning
model (a regressor) with predictive variables being
commandline parameters of x264. Unmeasured configurations could then
be predicted. Even for large-scale systems like the Linux Kernel,
the same process of ‘‘sampling, measuring, learning" can be
followed (see, e.g.
        <xref ref-type="bibr" rid="ref2 ref3">Acher et al. (2019</xref>
        a,b)). Some additional steps are
worth exploring (like feature engineering prior to learning) while
techniques for sampling and learning should be adapted to scale
at its complexity, but the general process remains applicable.
      </p>
      <p>Learning software configuration spaces is, however, not a pure
machine learning problem and there are a number of specific
challenges to address at the intersection of software
engineering and artificial intelligence. For instance, the sampling phase
involves a number of difficult activities: (1) picking configurations
that are valid and conform to constraints among options – one
needs to resolve a satisfiability problem; (2) instrumenting the
executions and observations of software for a variety of
configurations – it can have an important computational cost and is hard
to engineer especially when measuring non-functional aspects of
software; (3) meanwhile, we expect that the sample is
representative to the whole population of valid configurations otherwise
the learning algorithm may hardly generalize to the whole
configuration space. The general problem is to find the right strategy
to decrease the cost of labeling software configurations while
minimizing prediction errors. From an empirical perspective, one
can also wonder to what extent learning approaches are effective
for real-world software systems present in numerous domains.</p>
      <p>
        While several studies have covered different aspects of
configurable systems over the last years, there has been no secondary
study (such as systematic literature reviews) that identifies and
catalogs individual contributions for machine learning
configuration spaces. Thus, there is no clear consensus on what techniques
are used to support the process, including which quantitative
and qualitative properties are considered and how they can be
measured and evaluated, as well as how to select a significant
sample of configurations and what is an ideal sample size. This
stresses the need for a secondary study to build knowledge from
combining findings from different approaches and present a
complete overview of the progress made in this field. To achieve this
aim, we conduct a Systematic Literature Review (SLR)
        <xref ref-type="bibr" rid="ref51">(Kitchenham
and Charters, 2007)</xref>
        to identify, analyze and interpret all available
important research in this domain. We systematically review
research papers in which the process of sampling, measuring, and
learning configuration spaces occurs — more details about our
research methodology are given in Section 2. Specifically, we aim
of synthesizing evidence to answer the following four research
questions:
• RQ1. What are the concrete applications of learning software
configuration spaces?
• RQ2. Which sampling methods and learning techniques are
adopted when learning software configuration spaces?
• RQ3. Which techniques are used to gather measurements of
functional and non-functional properties of configurations?
• RQ4. How are learning-based techniques validated?
      </p>
      <p>To address RQ1, we analyze the application objective of the
study (i.e., why they apply learning-based techniques). It would
allow us to assess whether the proposed approaches are
applicable. With respect to RQ2, we systematically investigate which
sampling methods and learning techniques are used in the
literature for exploring the SPL configuration space. With respect
to RQ3, we give an in-depth view of how each study measures
a sample of configurations. In addition, RQ4 follows
identifying which sampling design and evaluation metrics are used for
evaluation.</p>
      <p>
        By answering these questions, we make the following five
contributions:
1. We identified six main different application areas: pure
prediction, interpretability, optimization, dynamic configuration,
evolution, and mining constraints.
2. We provide a framework classification of four main stages
used for learning: Sampling, Measuring, Learning, Validation.
3. We describe 23 high-level sampling methods, 5
measurement strategies, 64 learning techniques, and 50 evaluation
metrics used in the literature. As case studies, we identify
95 configurable systems targeting several domains, and
functional and non-functional properties. We relate and
discuss the learning and validation techniques with regard
to their application objective.
4. We identify a set of open challenges faced by the current
approaches, in order to guide researchers and practitioners
to use and build appropriate solutions.
5. We build a Web repository
        <xref ref-type="bibr" rid="ref78">(Pereira et al., 2019)</xref>
        to make
our SLR results publicly available for the purpose of
reproducibility and extension.
      </p>
      <p>Overall, the findings of this SLR reveal that there is a
significant body of work specialized in learning software configurable
systems with an important application in terms of software
technologies, application domains, or goals. There is a wide variety
in the considered sampling or learning algorithms as well as
in the evaluation process, mainly due to the considered subject
systems and application objectives. Practitioners and researchers
can benefit from the findings reported in this SLR as a reference
when they select a learning technique for their own settings.
To this end, this review provides a classification and catalog of
specialized techniques in this field.</p>
      <p>The rest of the paper is structured as follows. In Section 2,
we describe the research protocol used to conduct the SLR. In
Section 3, we categorize a sequence of key learning stages used by
the ML state-of-the-art literature to explore highly configurable
systems. In Section 4, we discuss the research questions. In
Section 5, we discuss the current research themes in this field and
present the open challenges that need attention in the future. In
Section 6, we discuss the threats to the validity of our SLR. In
Section 7, we describe similar secondary studies and indicate how
our literature review differs from them. Finally, in Section 8, we
present the conclusions of our work.</p>
    </sec>
    <sec id="sec-3">
      <title>2. The review methodology</title>
      <p>
        We followed the SLR guidelines by
        <xref ref-type="bibr" rid="ref51">Kitchenham and Charters
(2007)</xref>
        to systematically investigate the use of learning
techniques for exploring the SPL configuration space. In this section,
we present the SLR methodology that covers two main phases:
planning the review and conducting the review. The paper selection
process is shown in Fig. 2. Next, we report the details about each
phase so that readers can assess their rigor and completeness, and
reproduce our findings.
      </p>
      <sec id="sec-3-1">
        <title>2.1. Planning the review</title>
        <p>For identifying the candidate primary studies of Fig. 2, we first
defined our SLR scope (i.e., identification of the need for a review
and specification of the research questions). Then, we developed
a review protocol.</p>
      </sec>
      <sec id="sec-3-2">
        <title>The need for a systematic review. The main goal of this SLR is</title>
        <p>
          to systematically investigate and summarize the state-of-the-art
of the research concerning learning techniques in the context of
software configurable systems. The purpose of conducting this
SLR has partially been addressed in the introduction and was
motivated by the lack of a systematic study carried on this topic.
According to
          <xref ref-type="bibr" rid="ref51">Kitchenham and Charters (2007)</xref>
          , an SLR should
provide a valuable overview of the status of the field to the
community through the summarization of existing empirical
evidence supported by current scientific studies. The outcomes of
such an overview can identify whether, or under what conditions,
the proposed learning approaches can support various use-cases
around configurable systems and be practically adopted (e.g., for
which context a specific learning technique is much suitable). By
mean of this outcome, we can detect the limitations in current
approaches to properly suggest areas for further investigation.
The research questions. The goal of this SLR is to answer the
following main research question: What studies have been reported
in the literature on learning software configuration spaces since the
introduction of Software Product Lines in the early 1990s
          <xref ref-type="bibr" rid="ref47">(Kang et al.,
1990)</xref>
          to date (2019)? However, this question is too broad, so we
derived the four sub-questions defined in Section 1. RQ1 classifies
the papers with regards to their application objective, i.e., for
which particular task the approach is suited and useful. We can
group studies into similar categories for comparison. It is also
of interest to identify the practical motivations behind learning
approaches. We verified whether the authors indicated a specific
application for their approach; otherwise, we classified the
approach as pure prediction. RQ2–RQ4 seek to understand key steps
of the learning process. RQ2 reviews the set of sampling
methods and learning-based techniques used in the literature. RQ3
describes which subject software systems, application domains,
and functional and non-functional properties of configurations
are measured and how the measurement process is conducted.
RQ4 aims to characterize the evaluation process used by the
researchers, including the sample design and supported
evaluation metric(s). Finally, addressing these questions will allow us
to identify trends and challenges in the current state-of-the-art
approaches, as well as analysis their maturity to summarize our
findings and propose future works.
        </p>
        <p>The review protocol. We searched for all relevant papers
published up to May 31st, 2019. The search process involved the
use of 5 scientific digital libraries1: IEEE Xplore Digital Library,2
1 We decided not to use Google Scholar due to search engine limitations,
such as the very strict size of the search string.</p>
        <p>
          2 http://ieee.org/ieeexplore.
ACM Digital Library,3 Science Direct,4 Springer-link,5 and Scopus6
(see Fig. 2). These search engines were selected because they
are known as the top five preferred on-line databases in the
software engineering literature
          <xref ref-type="bibr" rid="ref39 ref85">(Hoda et al., 2017)</xref>
          . We restricted
the search to publication titles and abstracts by applying the
selection criteria specified in Section 2.2. However, the library
Springer-link only enables a full-text search. Thus, we first used
the full-text option to generate an initial set of papers (the results
were stored in a .bib file). Then, we created a script to perform
an expert search in the title and abstract over these results.
        </p>
        <p>Each author of this paper had specific roles when performing
this SLR. The first author applied the search string to the
scientific databases and exported the results (i.e., detailed information
about the candidate papers) into a spreadsheet. After this stage,
papers were selected based on a careful reading of the titles
and abstracts (and if necessary checking the introduction and
conclusion). Each identified candidate paper in accordance with
the selection criteria defined in Section 2.2 were identified as
potentially relevant. When Pereira decided that a paper was not
relevant, she provided a short rationale why the paper should not
be included in the study. In addition, another researcher checked
each excluded paper at this stage. To minimize potential biases
introduced into the selection process, any disagreement between
researchers were put up for discussion between all authors until
a consensus agreement was obtained. This step was done in order
to check that all relevant papers were selected.</p>
        <p>
          The search in such databases is known to be challenging due
to different search limitations, e.g. different ways of constructing
the search string. Thus, apart from the automatic search, we also
consider the use of snowballing
          <xref ref-type="bibr" rid="ref122">(Wohlin, 2014)</xref>
          as a
complementary approach. Through snowballing, we searched for additional
relevant primary studies by following the references from all
preliminary selected studies (plus excluded secondary studies).
As we published some works related to the topic of this literature
review, we used our knowledge and expertise to complement the
pool of relevant papers.
        </p>
        <sec id="sec-3-2-1">
          <title>3 http://dl.acm.org.</title>
          <p>4 http://www.sciencedirect.com.
5 http://link.springer.com.
6 http://www.scopus.com.</p>
          <p>During the data extraction stage, each paper was assigned to
one researcher. Pereira coordinated the allocation of researchers
to tasks based on the availability of each researcher. The
researcher responsible for extracting the data of a specific selected
paper, applied the snowballing technique to the corresponding
paper. Pereira applied the snowballing technique for excluded
secondary studies. Each primary study was then assigned to
another researcher for review. Once all the primary studies were
reviewed, the extracted data was compared. Whenever there were
any discrepancies either about the data reported or about the list
of additional selected papers derived from the snowballing
process, we again resolved the problem through discussions among
all authors.</p>
        </sec>
      </sec>
      <sec id="sec-3-3">
        <title>2.2. Conducting the review</title>
        <p>The stages involved in conducting the review are: definition
of the search string, specification of the selection criteria, and
specification of the data extraction process.</p>
        <p>As a first step, we used our expertise in the field to obtain an
initial pool of relevant studies. Based on this effort, we defined
the search string.</p>
        <p>
          The search string. According to
          <xref ref-type="bibr" rid="ref51">Kitchenham and Charters (2007)</xref>
          there is no silver bullet for identifying good search strings since,
very often, the terminology used in the field is not standardized.
When using broad search terms, a large number of irrelevant
papers may be found in the search which makes the screening
process very challenging. The search string used in this review
was first initiated by selecting an initial list of relevant articles
based on our expertise in the field.
        </p>
        <p>We identified in the title and abstract the major terms to
be used for systematic searching the primary studies. Then, we
searched for synonyms related to each major term. Next, we
performed several test searches with alternative links between
keywords through the different digital libraries. The results from
the test searches were continuously discussed among the authors
to refine the search string until we were fully satisfied with the
capability of the string to detect as much of the initial set of
relevant publications as possible. Following this iterative strategy
and after a series of test executions and reviews, we obtained a
set of search terms and keywords (see Table 1).</p>
        <p>Specifically, Table 1 shows the term we are looking for and
related synonyms that we considered as keywords in our search.
Keywords associated with Product Line allow us to include
studies that focus on configurable systems. By combining
keywords associated to Learning and Performance Prediction,
we can find studies that focus on the use of learning-based
techniques for exploring the variability space. In addition,
keywords associated with Predict (most specific term) allow us to
focus on the application objective of such studies. We decided to
include the keywords associated with Predict so as to identify
the context of the study and have a more restricted number of
primary studies. Otherwise, the keywords (Product Line AND
(Learning OR Performance Prediction)) return a broad
number of studies, e.g. studies addressing the use of learning
techniques for product line testing or configuration guidance. In
addition, we used keywords from Medicine to exclude studies in
this field from our search. We decided to filter out these studies
once there are many papers in this field using also the term
configuration, e.g. for gene and patient characteristics. The final
result is the following search string:
(Product Line AND (Learning OR Performance Prediction)</p>
        <p>AND Predict) AND NOT Medicine</p>
        <p>
          The terms Product Line, Learning, Performance
Prediction, and Predict are represented as a disjunction of the
keywords in Table 1. The search string format we used was
slightly modified to meet the search engine requirements of the
different scientific databases. For example, the scientific library
Science Direct limits the size of the search string. Thus, when
searching in this library, we had to split the search string to
generate an initial set of papers. Then, we created a script to
perform an expert search over this initial set of papers and
filter just the relevant ones from these subsets, i.e. we made
every effort to ensure that the search strings used were logically
and semantically equivalent to the original string in Table 1.
The detailed search strings used in each digital search engine
and additional scripts are provided in the Web supplementary
material
          <xref ref-type="bibr" rid="ref78">(Pereira et al., 2019)</xref>
          .
        </p>
        <p>As a second step, we applied the search string to the scientific
digital libraries. Fig. 2 shows the number of papers obtained
from each library. At the end of step 2, the initial search from
all sources resulted in a total of 1,627 candidate papers, which
includes the 21 papers from our initial pool of relevant studies.
As a third step, we removed all duplicated papers (285) and we
carried out the selection process at the content level.
The selection criteria. We selected the studies using a set of
selection criteria for retrieving a relevant subset of publications.
First, we only selected papers published up to May 31st, 2019 that
satisfied all of the following three Inclusion Criteria (IC):
IC1 The paper is available online and in English;</p>
      </sec>
      <sec id="sec-3-4">
        <title>IC2 The paper should be about configurable software systems.</title>
        <p>IC3 The paper deals with techniques to statistically learn data
from a sample of configurations (see Section 4.1). When
different extensions of a paper were observed, e.g., an
algorithm is improved by parameter tuning, we intentionally
classified and evaluated them as separate primary studies
for a more rigorous analysis.</p>
        <p>Moreover, we excluded papers that satisfied at least one of the
following four Exclusion Criteria (EC):
EC1 Introductions to special issues, workshops, tutorials,
conferences, conference tracks, panels, poster sessions, as well as
editorials and books.</p>
        <p>EC2 Short papers (less than or equal to 4 pages) and
work-inprogress.</p>
        <p>EC3 Pure artificial intelligence papers.</p>
        <p>EC4 Secondary studies, such as literature reviews, articles
presenting lessons learned, position or philosophical papers,
with no technical contribution. However, the references of
these studies were read in order to identify other relevant
primary studies for inclusion through the snowballing
technique (see Section 2.1). Moreover, we consider secondary
studies in the related work section (see Section 7).</p>
        <p>
          We find it useful to give some examples of approaches that
were not included:
• the use of sampling techniques without learning (e.g., the
main application is testing or model-checking a software
product line). That is, the sample is not used to train a
machine learning model but rather for reducing the cost
of verifying a family of products. For a review on
sampling for product line testing, we refer to
          <xref ref-type="bibr" rid="ref63">Medeiros et al.
(2016)</xref>
          ,
          <xref ref-type="bibr" rid="ref59">Lopez-Herrejon et al. (2015)</xref>
          , do Carmo Machado
et al. (2014),
          <xref ref-type="bibr" rid="ref54">Lee et al. (2012)</xref>
          and
          <xref ref-type="bibr" rid="ref111">Thüm et al. (2014)</xref>
          . We
also discuss the complementary between the two lines of
work in Section 5.4;
• the use of state-of-the-art recommendations and
visualization techniques for configuration guidance (e.g.,
          <xref ref-type="bibr" rid="ref79 ref80">Pereira et al.
(2018)</xref>
          and
          <xref ref-type="bibr" rid="ref67">Murashkin et al. (2013)</xref>
          ) and optimization
methods based on evolutionary algorithms (e.g.,
          <xref ref-type="bibr" rid="ref31">Guo et al. (2011)</xref>
          and
          <xref ref-type="bibr" rid="ref93">Sayyad et al. (2013)</xref>
          ) since a sample of configurations’
measurements is not considered.
• the use of learning techniques to predict the existence of a
software defect or vulnerability based on source code
analysis (e.g., in
          <xref ref-type="bibr" rid="ref85">Putri et al. (2017)</xref>
          and
          <xref ref-type="bibr" rid="ref105">Stuckman et al. (2017)</xref>
          ,
where features do not refer to configurable systems, instead
features refer to properties or metrics of the source code).
        </p>
        <p>During this step, 1,301 papers were excluded, yielding a total
of 41 selected papers for inclusion in the review process. A fourth
step of the filtering was performed to select additional relevant
papers through the snowballing process. This step considered all
included papers, as well as removed secondary studies, which
resulted in the inclusion of 28 additional papers. This resulted in
the selection of 69 primary papers for data extraction.</p>
        <p>The data extraction. The data extraction process was conducted
using a structured extraction form in Google Sheets7 to
synthesize all data required for further analysis in such a way that the
research questions can be answered. In addition, Google Sheets
allow future contributions to be online updated by shareholders.
First, all candidate papers were analyzed regarding the selection
criteria. The following data were extracted from each retrieved
study:
• Date of search, scientific database, and search string.
• Database, authors, title, venue, publication type (i.e., journal,
conference, symposium, workshop, or report), publisher,
pages, and publication year.
• Inclusion criteria IC1, IC2, and IC3 (yes or no)?
• Exclusion criteria EC1, EC2, and EC3 (yes or no)?
• Selected (yes or no)? If not selected, justification regarding
exclusion.</p>
        <p>Once the list of primary studies was decided, each selected
publication was then read very carefully and the content data
for each selected paper was captured and extracted in a second
form. The data extraction aimed to summarize the data from
the selected primary studies for further analysis of the research
questions and for increasing confidence regarding their relevance.
All available documentation from studies served as data sources,
such as thesis, websites, tool support, as well as the
communication with authors (e.g., emails exchanged). The following data
were extracted from each selected paper:
• RQ1: Scope of the approach. We classified the approach
according to the following six categories: pure prediction,
interpretability of configurable systems, optimization, dynamic
configuration, mining constraints, and evolution.
• RQ2: Adopted sampling and learning technique(s).
• RQ3: Information about subject systems (i.e., reference,
name, domain, number of features, and valid configurations)
and the measurement procedure. We collected data about
the measured (non-)functional properties and the adopted
strategies of measurement.
• RQ4: Evaluation metrics and sample designs used by
approaches for the purpose of training and validating machine
learning models.</p>
        <p>
          The Web supplementary material
          <xref ref-type="bibr" rid="ref78">(Pereira et al., 2019)</xref>
          provides the results of the search procedure from each of these
steps.
        </p>
      </sec>
    </sec>
    <sec id="sec-4">
      <title>3. Literature review pattern: Sampling, measuring, learning</title>
      <p>
        Understanding how the system behavior varies across a large
number of variants of a configurable system is essential for
supporting end-users to choose a desirable product
        <xref ref-type="bibr" rid="ref9">(Apel et al.,
2013)</xref>
        . It is also useful for developers in charge of maintaining
such software systems. In this context, machine learning-based
techniques have been widely considered to predict
configurations’ behavior and assist stakeholders in making informed
decisions. Throughout our review effort, we have observed that such
approaches follow a 4-stage process: (1) sampling; (2) measuring;
(3) learning; and (4) validation. The aim of this section is to
introduce the background behind each of these stages in order
to answer the research questions in Section 4.
      </p>
      <p>The stages are sketched in Fig. 3. The dashed-line boxes
denote the inputs and outputs of each stage. The process starts by
building and measuring an initial sample of valid configurations.</p>
      <sec id="sec-4-1">
        <title>7 https://www.google.com/sheets/about/.</title>
        <p>
          The set of valid configurations in an SPL is predefined at design
time through variability models usually expressed as a feature
model
          <xref ref-type="bibr" rid="ref82">(Pohl et al., 2005)</xref>
          . Then, these measurements are used
to learn a prediction model. Prediction models help stakeholders
to better understand the characteristics of complex configurable
systems. They try to describe the behavior of all valid
configurations. Finally, the validation step computes the accuracy of the
prediction model. Some works use active learning
          <xref ref-type="bibr" rid="ref117 ref121 ref124 ref133 ref28 ref32 ref70 ref71 ref74 ref85 ref85 ref85">(Guo et al.,
2017; Oh et al., 2017; Nair et al., 2018; Westermann et al., 2012;
Zuluaga et al., 2016; Xi et al., 2012; Grebhahn et al., 2017)</xref>
          to
improve the sample in each interaction based on previous
accuracy results until it reaches a configuration that has a satisfactory
accuracy. Next, we describe in detail each stage.
        </p>
        <p>
          Sampling. Decision-makers may decide to select or deselect
features to customize a system. Each feature can have an effect on
a system’s non-functional properties (NFPs). The quantification of
the NFPs of each individual feature is not enough in most cases,
as unknown feature interactions among configuration options
may cause unpredictable measurements. Interactions occur when
combinations among features share a common component or
require additional component(s). Thus, understanding the
correlation between feature selections and system NFPs is important
for stakeholders to be able to find an appropriate system variant
that meets their requirements. In Fig. 3, let C = {C1, C2, . . . , Cn}
be the set of n valid configurations, and Ci = {f1, f2, . . . , fm} with
fj ∈ {0, 1} a combination of m selected (i.e., 1) and deselected
(i.e., 0) features. A straightforward way to determine whether a
specific variant meets the requirements is to measure its target
NFP P and repeat the process for all C variants of a system,
and then e.g. search for the cheapest configuration Ci with Ci ∈
C . However, it is usually unfeasible to benchmark all possible
variants, due to the exponentially growing configuration space.
ML techniques address this issue making use of a small measured
sample SC = {s1, . . . , sk} of configurations, where SC ⊆ C , and
the number of samples k and the prediction error ϵ are minimal.
With the promise to balance measurement effort and prediction
accuracy, several sample strategies have been reported in the
literature (see Table A.6). For example, Siegmund et al.
          <xref ref-type="bibr" rid="ref104 ref124 ref48 ref56 ref60 ref67 ref77 ref91 ref93 ref95 ref97">(2011,
2012a, 2013b, 2015)</xref>
          explore several ways of sampling
configurations, in order to find the most accurate prediction model.
Moreover, several authors
          <xref ref-type="bibr" rid="ref117 ref121 ref124 ref133 ref28 ref32 ref70 ref71 ref74 ref85 ref85 ref85">(Guo et al., 2017; Oh et al., 2017; Nair
et al., 2018; Westermann et al., 2012; Zuluaga et al., 2016; Xi
et al., 2012; Grebhahn et al., 2017)</xref>
          have tried to improve the
prediction power of the model by updating an initial sample
based on information gained from the previous set of samples
through active learning. The sample might be partitioned into
training, testing and validation sets which are used to train and
validate the prediction model (see Section 4.4).
        </p>
        <p>Measuring. This stage measures the set of NFPs {p1, . . . , pl} of a
configuration sample SC = {s1, . . . , sk}, where p1 = {p1(s1), . . . , p1
(sk)}. NFPs are measured either by execution, simulation, static
analysis, user feedback or synthetic measurements.</p>
        <p>
          Execution consists of executing the configuration samples and
monitoring the measurements of NFPs at runtime. Although
execution is much more precise, it may incur unacceptable
measurement costs since it is often not possible to create suddenly
potentially important scenarios in the real environment. To
overcome this issue, some approaches have adopted measurement
by simulation. Instead of measuring out of real executions of a
system which may result in high costs or risks of failure,
simulation learns the model using offline environmental conditions
that approximate the behavior of the real system faster and
cheaper. The use of simulators allows stakeholders to understand
the system behavior during early development stages and identify
alternative solutions in critical cases. Moreover, simulators can
be programmed offline which eliminates any downtime in online
environments. In addition to execution and simulation, static
analysis infers measurements only by examining the code, model,
or documentation. For example, the NFP cost can be measured
as the required effort to add a feature to a system under
construction by analyzing the system cycle evolution, such as the
number of lines of code, the development time, or other
functional size metrics. Although static analysis may not be always
accurate, it is much faster than collecting data dynamically by
execution and simulation. Moreover, partial configurations can
be also measured. Finally, instead of measuring configurations
statically or dynamically, some authors also make use of either
user feedback (UF) or synthetic measurements (SM). In contrast to
static and dynamic measurements, both approaches do not rely
on systems artifacts. User feedback relies only on domain expert
knowledge to label configurations (e.g., whether the configuration
is acceptable or not). Synthetic measurements are based on the use
of learning techniques to generate artificial (non-)functional
values to configurable systems
          <xref ref-type="bibr" rid="ref102 ref42 ref85">(Siegmund et al., 2017)</xref>
          . Researchers
can use the THOR generator
          <xref ref-type="bibr" rid="ref102 ref42 ref85">(Siegmund et al., 2017)</xref>
          to mimic and
experiment with properties of real-world configurable systems
(e.g., performance distributions, feature interactions).
Learning. The aim of this stage is to learn a prediction model
based on a given sample of measured configurations P(SC ) to
infer the behavior of non-measured configurations P(C − SC ). The
sampling set SC is divided into a training set ST and a validation
set SV , where SC = ST + SV . The training set is used as input to
learn a prediction model, i.e. describe how configuration options
and their interactions influence the behavior of a system. For
parameter tuning, interactive sampling, and active learning, the
training set is also partitioned into training and testing sets.
        </p>
        <p>
          Some authors (Van Aken et al. (2017a), Jamshidi et al.
          <xref ref-type="bibr" rid="ref115 ref116 ref127 ref15 ref25 ref27 ref28 ref6 ref79 ref80 ref85 ref88">(2017b,a,
2018)</xref>
          ,
          <xref ref-type="bibr" rid="ref114">Valov et al. (2017)</xref>
          ) applied transfer learning techniques to
accelerate the learning process. Instead of building the prediction
model from scratch, transfer learning reuses the knowledge
gathered from samples of other relevant related sources to a target
source. It uses a regression model that automatically captures
the correlation between target and related systems. Correlation
means the common knowledge that is shared implicitly between
the systems. This correlation is an indicator that there is a
potential to learn across the systems. If the correlation is strong,
the transfer learning method can lead to an accurate and reliable
prediction model more quickly by reusing measurements from
other sources.
        </p>
        <p>Validating. The validation stage quantitatively analysis the
quality of the sample ST for prediction using an evaluation metric on
the validation set SV . To be practically useful, an ideal sample
ST should result in a (i) low prediction error; (ii) small model
size; (iii) reasonable measurement effort. The aim is to find as
few samples as possible to yield an understandable and accurate
ML model in short computation time. In Section 4.4, we detail
the evaluation metrics used by the selected primary studies to
compute accuracy.</p>
        <p>Overall, exploring the configuration space based on a small
sample of configurations is a critical step since in practice, the
sample may not contain important feature interactions nor reflect
the system real behavior accurately. To overcome this issue,
numerous learning approaches have been proposed in the last years.
Next, we analyze the existing literature by investigating the more
fine-grained characteristics of these four learning stages.</p>
      </sec>
    </sec>
    <sec id="sec-5">
      <title>4. Results and discussion of the research questions</title>
      <p>In this section, we discuss the answers to our research
questions defined in Section 1. In Section 4.1, we identify the main
goal of the learning process. Next, in Sections 4.2–4.4 we analyze
in detail how each study address each learning stage defined in
Section 3 to accomplish the goals described in Section 4.1.</p>
      <sec id="sec-5-1">
        <title>4.1. RQ1: What are the concrete applications of learning software configuration spaces?</title>
        <p>In this section, we analyze the application objective of the
selected studies since learning may have different practical
interests and motivations. Learning techniques have been used in
the literature to target six different scenarios (see Fig. 4 and
Table 2). In Fig. 4, we represent configurations with small circles
and variables of a math formula (i.e. configuration options) with
geometric figures. Next, we describe each individual application
and give prominent examples of papers entering in this category.</p>
        <p>
          A1 Pure Prediction. The aim is to accurately predict labels (i.e.,
NFPs) p1 of unmeasured configurations {C1, C2, . . . , C6}.
Labels can be qualitative (e.g., whether the software
configuration has a defect) or quantitative (e.g., the execution
time in seconds of a software configuration). The outcome
is to associate through prediction some properties to all
configurations of the space (see Fig. 4(a)).
          <xref ref-type="bibr" rid="ref30">Guo et al. (2013)</xref>
          is
a seminal paper with respect to the use of statistical learning
for predicting performances. In this scenario, other factors
such as model interpretability and computation cost are less
important. In some engineering contexts, the sole prediction
of a property of a configuration has limited practical interest
per se and is sometimes used as a basis for targeting other
applications (e.g., configuration optimization).
        </p>
        <p>A1
A2
A3
A4
A5
A6</p>
        <p>
          A2 Interpretability of configurable systems. Understanding the
correlation between configuration options and system
quality is important for a wide variety of tasks, such as
optimization, program comprehension and debugging. To this end,
these studies aim at learning an accurate model that is fast
to compute and simple to interpret (see the math formula
in Fig. 4(b)). For example,
          <xref ref-type="bibr" rid="ref52">Kolesnikov et al. (2018)</xref>
          explore
how so-called performance-influence models quantify
options’ influences and can be used to explain the performance
behavior of a configurable system as a whole.
        </p>
        <p>
          A3 Optimization. Instead of labeling all configurations,
optimization approaches aim at finding a (near-)optimal valid
configuration to best satisfy requirements, e.g., ∑max(P1) (see
Fig. 4(c)). According to
          <xref ref-type="bibr" rid="ref72">Ochoa et al. (2018)</xref>
          , there are three
types of stakeholders’ requirements: resource constraints
(threshold such as response time &lt; 1 hour), stakeholders’
preferences (e.g., security is extremely more preferable
and relevant than response time) and optimization
objectives (e.g., minimization of response time). Although
the specification of requirements may reduce the
configuration space
          <xref ref-type="bibr" rid="ref1 ref10 ref24 ref73 ref87">(Acher et al., 2013; Bak et al., 2016; Ochoa
et al., 2015; Eichelberger et al., 2016; Roos-Frantz et al.,
2012)</xref>
          , searching for the most appropriate configuration is
still an overwhelming task due to the combinatorial
explosion. Learning approaches exposed in e.g.
          <xref ref-type="bibr" rid="ref74">Oh et al. (2017)</xref>
          and Nair et al. (2017, 2018) propose a recursive search
method to interactively add new samples to train the
prediction model until it reaches a (near-)optimal configuation
with an acceptable accuracy.
        </p>
        <p>A4 Dynamic Configuration. There are many dynamic systems
(e.g., robotic systems) that aim to manage run-time
adaptations of software to react to (uncertain) environmental
changes. Without self-adaptation, requirements would be</p>
        <p>
          et al., 2018). To overcome this issue, learning techniques
can be used to discover additional constraints that would
exclude unacceptable configurations. These studies seek an
accurate and complete set of constraints to restrict the space
of possible configurations (see the restricted configurations
into the small circle in Fig. 4(d)). Finally, it creates a new
variability model by adding the identified constraints to the
original model. Therefore, the aim is to accurately remove
invalid configurations that were never derived and tested
before. Mining constraints approaches work mainly with
qualitative properties, such as video quality
          <xref ref-type="bibr" rid="ref109 ref117 ref4">(Temple et al.,
2016, 2018)</xref>
          and defects
          <xref ref-type="bibr" rid="ref128 ref27 ref53 ref8 ref85 ref85">(Yilmaz et al., 2006; Krismayer et al.,
2017; Gargantini et al., 2017; Amand et al., 2019)</xref>
          .
        </p>
        <p>
          A6 Evolution. In the evolution scenario, a configurable system
will inevitably need to adapt to satisfy real-world changes in
external conditions, such as changes in requirements, design
and performance improvements, and changes in the source
code. Thus, new configuration options become available and
valid, while existing configurations may become obsolete
and invalid (see an example of obsolete configurations in
Fig. 4(e) represented by unfilled circles). Consequently, the
new variability model structure may influence certain NFPs.
In this scenario, it is important to make sure that the
learning stage is informed by evolution about changes and the set
of sample configurations is readjusted accordingly with the
new variability model by excluding invalid configurations
(C − C′) and considering the parts of the configuration space
not yet covered (C′ − C). In this context, there are a few
works (e.g., Jamshidi et al.
          <xref ref-type="bibr" rid="ref115 ref116 ref25 ref27 ref28 ref6 ref79 ref80 ref85 ref88">(2017a, 2018)</xref>
          ) that use transfer
learning techniques accross software versions.
        </p>
        <p>
          The use of machine learning techniques to learn software
configuration spaces has a wide application objective and
can be used for supporting developers or end-users in six
main different tasks: Pure Prediction, Interpretability of
Configurable Systems, Optimization, Dynamic Configuration,
Mining Constraints, and SPL Evolution. It is also possible to
combine different tasks (e.g., mining constraints for
supporting dynamic configuration
          <xref ref-type="bibr" rid="ref107 ref108 ref115 ref116 ref25 ref27 ref28 ref53 ref85 ref85 ref88">(Temple et al., 2017a; Krismayer
et al., 2017)</xref>
          ). There is still room to target other
applications (e.g., learning of multiple and composed configurable
systems).
        </p>
      </sec>
      <sec id="sec-5-2">
        <title>4.2. RQ2: Which sampling methods and learning techniques are adopted when learning software configuration spaces?</title>
        <p>
          In this section, we analyze the set of sampling methods used
by learning-based techniques in the literature. Also, we aim at
understanding which learning techniques were applied and in
which context. The complete list of sampling (resp. learning)
references can be found in Appendix A (resp. Appendix B). The
interested reader can also visualize the relationship between both
in our companion Website
          <xref ref-type="bibr" rid="ref78">(Pereira et al., 2019)</xref>
          .
        </p>
        <p>
          Random sampling. Several studies have used random
sampling
          <xref ref-type="bibr" rid="ref107 ref108 ref109 ref113 ref114 ref115 ref115 ref116 ref116 ref117 ref117 ref117 ref117 ref117 ref120 ref127 ref130 ref133 ref25 ref25 ref27 ref27 ref28 ref28 ref30 ref32 ref4 ref4 ref42 ref44 ref46 ref69 ref70 ref70 ref71 ref71 ref74 ref85 ref85 ref85 ref85 ref85 ref85 ref88 ref88 ref94 ref95">(Guo et al., 2013, 2017; Jamshidi et al., 2017b,a; Oh et al.,
2017; Nair et al., 2018; Temple et al., 2016, 2017a; Valov et al.,
2015; Weckesser et al., 2018; Zhang et al., 2015; Acher et al.,
2018; Jamshidi et al., 2019; Temple et al., 2018; Siegmund et al.,
2015; Valov et al., 2017; Nair et al., 2017, 2018; Kaltenecker et al.,
2019)</xref>
          with different notions of randomness.
        </p>
        <p>
          <xref ref-type="bibr" rid="ref30">Guo et al. (2013)</xref>
          consider four sizes of random samples for
training: N, 2N, 3N, and M, where N is the number of features of
a system, and M is the number of minimal valid configurations
covering each pair of features. They choose size N, 2N, and 3N,
because measuring a sample whose size is linear in the number
of features is likely feasible and reasonable in practice, given the
high cost of measurements by execution (see Section 4.3).
          <xref ref-type="bibr" rid="ref32">Guo
et al. (2017)</xref>
          and Valov et al. (2015, 2017) use a random sample
to train, but also to cross-validate their machine learning model.
Several works
          <xref ref-type="bibr" rid="ref117 ref130 ref69 ref70 ref71 ref74 ref85 ref85">(Nair et al., 2018, 2017; Oh et al., 2017; Zhang
et al., 2015)</xref>
          seek to determine the number of samples in an
adaptive, progressive sampling manner and a random strategy is
usually employed. Nair et al. (2018, 2017) and
          <xref ref-type="bibr" rid="ref74">Oh et al. (2017)</xref>
          aim
at optimizing a configuration. At each iteration, they randomly
add an arbitrary number of configurations to learn a prediction
model until they reach a model that exhibits a desired satisfactory
accuracy. They consider several sizes of samples from tens to
thousands of configurations. To focus on a reduced part of the
configuration space,
          <xref ref-type="bibr" rid="ref70 ref71">Nair et al. (2018)</xref>
          and
          <xref ref-type="bibr" rid="ref74">Oh et al. (2017)</xref>
          determine statistically significant parts of the configuration space that
contribute to good performance through active learning. In order
to have a more representative sample,
          <xref ref-type="bibr" rid="ref114">Valov et al. (2017)</xref>
          adopted
stratified random sampling. This sampling strategy exhaustively
divides a sampled population into mutually exclusive subsets of
observations before performing actual sampling.
        </p>
        <p>
          Prior works
          <xref ref-type="bibr" rid="ref114 ref30 ref32 ref85 ref85">(Guo et al., 2013, 2017; Valov et al., 2017)</xref>
          relied
on the random selection of features to create a configuration,
followed by a filter to eliminate invalid configurations (a.k.a,
pseudo-random sampling). Walker’s alias sampling
          <xref ref-type="bibr" rid="ref114 ref85">(Valov et al.,
2017)</xref>
          is an example of pseudo-random sampling. Quasi-random
sampling (e.g., sobol sampling) is similar to pseudo-random
sampling, however they are specifically designed to cover a
sampled population more uniformly
          <xref ref-type="bibr" rid="ref114 ref7 ref85 ref85">(Alipourfard et al., 2017; Valov
et al., 2017)</xref>
          . However, pseudo-random sampling may result in
too many invalid configurations, which makes this strategy
inefficient. To overcome this issue, several works
          <xref ref-type="bibr" rid="ref107 ref108 ref109 ref115 ref115 ref116 ref116 ref117 ref117 ref117 ref117 ref120 ref127 ref130 ref133 ref25 ref25 ref27 ref27 ref28 ref28 ref29 ref34 ref4 ref4 ref42 ref44 ref46 ref6 ref70 ref71 ref74 ref78 ref79 ref8 ref80 ref85 ref85 ref85 ref88 ref88 ref94">(Nair et al., 2018;
Oh et al., 2017; Acher et al., 2018; Zhang et al., 2015; Temple
et al., 2016, 2017a, 2018; Weckesser et al., 2018; Jamshidi et al.,
2017b,a, 2019)</xref>
          use solver-based sampling techniques (a.k.a., true
random sampling).
        </p>
        <p>Sampling and heuristics. Instead of randomly choosing
configurations as part of the sample, several heuristics have been
developed. The general motivation is to better cover features and
features’ interactions as part of the sample. The hope is to better
capture the essence of the configuration space with a lower sampling
size. We describe some heuristics hereafter.</p>
        <p>
          Knowledge-wise heuristic. This heuristic selects a sample of
configurations based on its influence on the target NFPs. The
sampling method described by
          <xref ref-type="bibr" rid="ref98">Siegmund et al. (2011</xref>
          , 2013b)
measures each feature in the feature model plus all known feature
interactions defined by a domain expert. Experts detect feature
interactions by analyzing the specification of features,
implementation assets, and source code, which require substantial domain
knowledge and exhaustive analysis. SPLCoqueror8 provides to
stakeholders an environment in which they can document and
incorporate known feature interactions. For each defined feature
interaction, a single configuration is added to the set of samples
for measurement. THOR
          <xref ref-type="bibr" rid="ref102 ref42 ref85">(Siegmund et al., 2017)</xref>
          is a generator
for synthesizing synthetic yet realistic variability models where
users (researchers) can specify the number of interactions and the
degree of interactions. Still, these works call to investigate further
questions, such as how to better synthesize knowledge actionable
by software developers.
        </p>
        <p>
          Feature-coverage heuristic. To automatically detect all first-order
feature interactions, Siegmund et al.
          <xref ref-type="bibr" rid="ref104 ref124 ref48 ref56 ref60 ref67 ref77 ref91 ref93 ref95 ref97">(2011, 2012a, 2013b, 2015)</xref>
          use a pair-wise measurement heuristic. This heuristic assumes
the existence of a feature interaction between each pair of
features in an SPL. It includes a minimal valid configuration for
each pair of features being selected. Pair-wise requires a number
of measurements that is quadratic in the number of optional
features. Some authors
          <xref ref-type="bibr" rid="ref46 ref57 ref91">(Sarkar et al., 2015; Kaltenecker et al.,
2019; Lillack et al., 2013)</xref>
          also use a 3-wise feature coverage
heuristic to discover interactions among 3 features. Siegmund
et al. (2012a) propose a 3rd-order coverage heuristic that
considers each minimal valid configuration where three features
interact pair-wise among them (adopted by
          <xref ref-type="bibr" rid="ref57">Lillack et al. (2013)</xref>
          ).
They also propose the idea that there are hot-spot features that
represent a performance-critical functionality within a system.
These hot-spot features are identified by counting the number of
interactions per features from the feature-coverage and
higherorder interaction heuristics.
          <xref ref-type="bibr" rid="ref129">Yilmaz et al. (2014)</xref>
          adopted even a
4-wise feature coverage heuristic, and
          <xref ref-type="bibr" rid="ref128">Yilmaz et al. (2006)</xref>
          a 5 and
6-wise heuristic. As there are nth order feature coverage
heuristics, the sample set might be likely unnecessarily large which
increases measurement effort substantially. However, not every
generated sample contains features that interact with each other.
Thus, the main problem of this strategy is that it requires prior
knowledge to select a proper coverage criterion. To overcome this
issue, state-of-the-art approaches might use the interaction-wise
heuristic to fix the size of the initial sample to the number of
features or potential feature interactions of a system
          <xref ref-type="bibr" rid="ref102 ref42 ref85">(Siegmund
et al., 2017)</xref>
          .
        </p>
        <p>
          Feature-frequency heuristic. The feature-frequency heuristic
considers a set of valid configurations in which each feature is
selected and deselected, at least, once.
          <xref ref-type="bibr" rid="ref91">Sarkar et al. (2015)</xref>
          propose
a heuristic that counts the number of times a feature has been
selected and deselected. Sampling stops when the count of features
that were selected or deselected reaches a predefined threshold.
          <xref ref-type="bibr" rid="ref69">Nair et al. (2017)</xref>
          analyze the number of samples required by
using the previous heuristic
          <xref ref-type="bibr" rid="ref91">(Sarkar et al., 2015)</xref>
          against a
rankbased random heuristic. Siegmund et al.
          <xref ref-type="bibr" rid="ref104 ref124 ref48 ref56 ref60 ref67 ref77 ref91 ref93 ref95 ref97">(2011, 2012a,b, 2013b,
2015)</xref>
          quantify the influence of an individual feature by
computing the delta of two minimal configurations with and without
the feature. They then relate to each feature a minimum valid
configuration that contains the current feature, which requires
the measurement of a configuration per feature. Hence, each
feature can exploit the previously defined configuration to compute
its delta over a performance value of interest. In addition, to
maximize the number of possible interactions,
          <xref ref-type="bibr" rid="ref95">Siegmund et al.
(2015)</xref>
          also relate to each feature a maximal valid configuration
that contains the current feature.
        </p>
        <p>
          There are several others sampling heuristics, such as Plackett–
Burman design
          <xref ref-type="bibr" rid="ref28 ref85 ref95">(Siegmund et al., 2015; Grebhahn et al., 2017)</xref>
          for reasoning with numerical options; Breakdown
          <xref ref-type="bibr" rid="ref121">(Westermann
et al., 2012)</xref>
          (random breakdown, adaptive random breakdown,
adaptive equidistant breakdown) for breaking down (in different
sectors) the parameter space; Constrained-driven sampling
          <xref ref-type="bibr" rid="ref27 ref85">(Gargantini et al., 2017)</xref>
          (constrained CIT, CIT of constraint validity,
constraints violating CIT, combinatorial union, unconstrained CIT)
to verify the validity of combinatorial interaction testing (CIT)
models; and many others (see Table A.6).
        </p>
        <p>
          Sampling and transfer learning. Jamshidi et al.
          <xref ref-type="bibr" rid="ref115 ref116 ref127 ref15 ref25 ref27 ref28 ref6 ref79 ref80 ref85 ref88">(2017b,a,
2018)</xref>
          aim at applying transfer learning techniques to learn a
prediction model. Jamshidi et al. (2017b) consider a combination
of random samples from target and source systems for training:
{0%, 10%, . . . , 100%} from the total number of valid
configurations of a source system, and {1%, 2%, . . . , 10%} from the total
number of valid configurations of a target system. In a similar
scenario, Jamshidi et al. (2017a) randomly select an arbitrary
number of valid configurations from a system before and after
environmental changes (e.g., using different hardware, different
workloads, and different versions of the system). In another
scenario,
          <xref ref-type="bibr" rid="ref43">Jamshidi et al. (2018)</xref>
          use transfer learning to sample.
Their sampling strategy, called L2S, exploits common similarities
between source and target systems. L2S progressively learns the
interesting regions of the target configuration space, based on
transferable knowledge from the source.
        </p>
      </sec>
    </sec>
    <sec id="sec-6">
      <title>Arbitrarily chosen sampling. Chen et al. (2005) and Murwan</title>
      <p>
        tara et al. (2014) have arbitrarily chosen a set of configurations
as their sample is based on their current available resources.
        <xref ref-type="bibr" rid="ref103">Sincero et al. (2010)</xref>
        use a subset of valid configurations from a
preliminary set of (de)selected features. This sample is arbitrarily
chosen by domain experts based on the use of features which will
probably have a high influence on the properties of interest. In
the context of investigating temporal variations,
        <xref ref-type="bibr" rid="ref90">Samreen et al.
(2016)</xref>
        consider on-demand instances at different times of the
day over a period of seven days with a delay of ten minutes
between each pair of runs. In a similar context,
        <xref ref-type="bibr" rid="ref23">Duarte et al.
(2018)</xref>
        and
        <xref ref-type="bibr" rid="ref16">Chen et al. (2009)</xref>
        also sample configurations under
different workloads (e.g., active servers and requests per second)
at different times of the day. An important insight is that there are
engineering contexts in which the sampling strategy is imposed
and can hardly be controlled.
      </p>
      <p>
        All configurations (no sampling). Sampling is not
applicable for four of the selected primary studies
        <xref ref-type="bibr" rid="ref117 ref132 ref19 ref52 ref85 ref85">(Kolesnikov et al.,
2018, 2017; Couto et al., 2017; Zheng et al., 2007)</xref>
        , mainly for
experimental reasons. For example, Kolesnikov et al. (2018, 2017)
consider all valid configurations in their experiments and use an
established learning technique to study and analyze the
tradeoffs among prediction error, model size, and computation time of
performance-prediction models. For the purpose of their study,
they were specifically interested to explore the evolution of the
model properties to see the maximum possible extent of the
corresponding trade-offs after each iteration of the learning
algorithm. So, they performed a whole-population exploration of
the largest possible learning set (i.e., all valid configurations). In
a similar scenario, Kolesnikov et al. (2017) explored the use of
control-flow feature interactions to identify potentially
interacting features based on detected interactions from performance
prediction techniques using performance measurements).
Therefore, they also performed a whole-population exploration of all
valid configurations.
      </p>
    </sec>
    <sec id="sec-7">
      <title>Reasoning about configuration validity. Sampling is realized</title>
      <p>either out of an enumerated set of configurations (e.g., the whole
ground truth) or a variability model (e.g., a feature model). The
former usually assumes that configurations of the set are logically
valid. The latter is more challenging, since picking a configuration
boils down to resolve a satisfiability or constraint problem.</p>
      <p>
        <xref ref-type="bibr" rid="ref4">Acher et al. (2018)</xref>
        and Temple et al.
        <xref ref-type="bibr" rid="ref115 ref116 ref133 ref25 ref27 ref28 ref6 ref79 ref80 ref85 ref88 ref94">(2016, 2017a, 2018)</xref>
        encoded variability models as Constraint Programming (CSP) by
using the Choco solver, while
        <xref ref-type="bibr" rid="ref120">Weckesser et al. (2018)</xref>
        and
        <xref ref-type="bibr" rid="ref102">Siegmund
et al. (2017)</xref>
        employed SAT solvers. Constraint solver may produce
clustered configurations with similar features due to the way
solvers enumerate solutions (i.e., often the sample set consists of
the closest k valid configurations). Therefore, these strategies do
not guarantee true randomness as in pseudo-random sampling.
Moreover, using CSP and SAT solvers to enumerate all valid
configurations are often impractical
        <xref ref-type="bibr" rid="ref64 ref83">(Mendonca et al., 2008; Pohl
et al., 2011)</xref>
        . Thus,
        <xref ref-type="bibr" rid="ref74">Oh et al. (2017)</xref>
        encoded variability models as
Binary Decision Diagrams (BDDs)
        <xref ref-type="bibr" rid="ref5">(Akers, 1978)</xref>
        , for which counting
the number of valid configurations is straightforward. Given the
number of valid configurations n, they randomly and uniformly
select the kth configuration, where k ∈ {1...n}, a.k.a. randomized
true-random sampling.
        <xref ref-type="bibr" rid="ref46">Kaltenecker et al. (2019)</xref>
        perform a
comparison among pseudo-random sampling, true-random sampling,
and randomized true-random sampling.
      </p>
      <sec id="sec-7-1">
        <title>Reasoning with numerical options. Ghamizi et al. (2019) trans</title>
        <p>
          form numeric and enumerated attributes into alternative Boolean
features to be handled as binary options. Temple et al.
          <xref ref-type="bibr" rid="ref115 ref116 ref133 ref25 ref27 ref28 ref6 ref79 ref80 ref85 ref88 ref94">(2016,
2017a, 2018)</xref>
          adopted random sampling of numeric options, i.e.
real and integer values. First, their approach randomly selects
a value for each feature within the boundaries of its domain.
Then, it propagates the values to other features with a solver
to avoid invalid configurations. In a similar scenario,
          <xref ref-type="bibr" rid="ref95">Siegmund
et al. (2015)</xref>
          and
          <xref ref-type="bibr" rid="ref28">Grebhahn et al. (2017)</xref>
          adopted pseudo-random
sampling of numeric options.
          <xref ref-type="bibr" rid="ref95">Siegmund et al. (2015)</xref>
          claim that
it is very unusual that numeric options have value ranges with
undefined or invalid holes and that constraints among numeric
options appear rarely in configurable systems.
          <xref ref-type="bibr" rid="ref28">Grebhahn et al.
(2017)</xref>
          adopted different reasoning techniques over binary and
numeric options, then they compute the Cartesian product of the
two sets to create single configurations used as input for learning.
In the scenario of reasoning with numerical options,
          <xref ref-type="bibr" rid="ref8">Amand et al.
(2019)</xref>
          arbitrarily select equidistant parameter values.
        </p>
        <p>Numeric sampling have substantially different value ranges in
comparison with binary sampling. The number of options’ values
to select can be huge while constraints should still be respected.
Sampling with numeric options is still an open issue — not a pure
SAT problem.</p>
        <p>Though random sampling is a widely used baseline,
numerous other sampling algorithms and heuristics have been
devised and described. There are a set of trade-offs to be
considered when sampling configurations: (1)
minimization of invalid configurations due to constraints’ violations
among options; (2) minimization of the cost (e.g., size) of
the sample; (3) generalization of the sample to the whole
configuration space. The question of a one-size-fits-all
sampling strategy remains open and several factors are to be
considered: targeted application, subject systems, functional
and NFPs, presence of domain knowledge, etc.</p>
        <p>Learning techniques. Supervised learning problems can be
grouped into regression and classification problems. In both cases,
the goal is to construct a machine learning model that can predict
the behavior of a configurable system. The difference between the
two problems is the fact that the value to predict is numerical
for regression and categorical for classification. In this literature
review, we found a similar dichotomy, depending on the targeted
use-case and the NFP of interest.</p>
        <p>A regression problem is when the output is a real or continuous
value, such as time or CPU power consumption. Most of the
learning techniques tackle a supervised regression problem.</p>
        <p>
          CART for regression. Several authors
          <xref ref-type="bibr" rid="ref107 ref108 ref109 ref115 ref115 ref116 ref116 ref117 ref133 ref25 ref25 ref27 ref27 ref28 ref28 ref30 ref32 ref42 ref44 ref68 ref69 ref70 ref71 ref85 ref85 ref85 ref85 ref88 ref88 ref91 ref94">(Guo et al., 2013, 2017;
Nair et al., 2017, 2018; Jamshidi et al., 2017a; Murwantara et al.,
2014; Sarkar et al., 2015; Temple et al., 2016, 2017a)</xref>
          use the
        </p>
      </sec>
      <sec id="sec-7-2">
        <title>Classification And Regression Trees (CART) technique, to model the</title>
        <p>correlation between feature selections and performance. CART
recursively partitions the sample into smaller clusters until the
performance of the configurations in the clusters is similar. These
recursive partitions are represented as a binary decision tree.
For each cluster, these approaches use the sample mean of the
performance measurements (or even the majority vote) as the
local prediction model of the cluster. So, when they need to
predict the performance of a new configuration not measured
so far, they use the decision tree to find the cluster which is
most similar to the new configuration. Each split of the set of
configurations is driven by the (de)selection of a feature that
would minimize a prediction error.</p>
        <p>
          CART uses two parameters to automatically control the
recursive partitioning process: minbucket and minsplit. Minbucket
is the minimum sample size for any leaf of the tree structure;
and minsplit is the minimum sample size of a cluster before it is
considered for partitioning.
          <xref ref-type="bibr" rid="ref30">Guo et al. (2013)</xref>
          compute minbucket
and minsplit based on the size of the input sample, i.e., if |SC | ≤
100, then minbucket = | |S1C0| + 21 | and minsplit = 2× minbucket; if
|SC | &gt; 100, then minsplit = | |S1C0| + 21 | and minbucket = | min2split |;
the minimum of minbucket is 2; and the minimum of minsplit is
4. It should be noted that CART can also be used for classification
problems (see hereafter).
        </p>
        <p>
          Instead of using a set of empirically-determined rules,
          <xref ref-type="bibr" rid="ref32">Guo
et al. (2017)</xref>
          combine the previous CART approach
          <xref ref-type="bibr" rid="ref30">(Guo et al.,
2013)</xref>
          with automated resampling and parameter tuning, which
they call a data-efficient learning approach (DECART). Using
resampling, DECART learns a prediction model by using different
sample designs (see Section 4.4). Using parameter tuning, DECART
ensures that the prediction model has been learned using optimal
parameter settings of CART based on the currently available
sample. They compare three parameter-tuning techniques: random
search, grid search, and Bayesian optimization.
          <xref ref-type="bibr" rid="ref121">Westermann et al.
(2012)</xref>
          also used grid search for tuning CART parameters.
        </p>
        <p>
          Approaches suggested by Nair et al. (2017, 2018) and
          <xref ref-type="bibr" rid="ref91">Sarkar
et al. (2015)</xref>
          build a prediction model in a progressive way by
using CART. They start with a small training sample and
subsequently add samples to improve performance predictions based
on the model accuracy (see Section 4.4). In each step, while
training the prediction model,
          <xref ref-type="bibr" rid="ref69">Nair et al. (2017)</xref>
          compare the
current accuracy of the model with the previous accuracy from
the prior iteration (before adding the new set of configurations to
the training set). If the current accuracy (with more data) does not
improve the previous accuracy (with less data), then the learning
reaches a termination criterion (i.e., adding more samples will not
result in significant accuracy improvements).
        </p>
        <p>
          Performance-influence models.
          <xref ref-type="bibr" rid="ref95">Siegmund et al. (2015)</xref>
          combine machine learning and sampling heuristics to build so-called
performance-influence models. A step-wise linear regression
algorithm is used to select relevant features as relevant terms of
a linear function and learn their coefficient to explain the
observations. In each iterative step, the algorithm selects the sample
configuration with the strongest influence regarding prediction
accuracy (i.e., yields the model’s lowest prediction error) until
improvements of model accuracy become marginal or a threshold
for expected accuracy is reached (below 19%). The algorithm
concludes with a backward learning step, in which every relevant
feature is tested for whether its removal would decrease model
accuracy. This can happen if initially a single feature is selected
because it better explains the measurements, but it becomes
obsolete by other features (e.g., because of feature interactions) later
in the learning process. Linear regression allows them to learn a
formula that can be understood by humans. It also makes it easy
to incorporate domain knowledge about an option’s influence on
the formula. However, the complete learning of a model using
this technique required from 1 to 5 h, depending on the size of the
learning set and the size of the models.
          <xref ref-type="bibr" rid="ref52">Kolesnikov et al. (2018)</xref>
          investigate how significant are the trade-offs among prediction
error, model size, and computation time.
        </p>
      </sec>
    </sec>
    <sec id="sec-8">
      <title>Other learning algorithms for regression. Westermann et al.</title>
      <p>
        (2012) used Multivariate Adaptive Regression Splines (MARS),
Genetic Programming (GP), and Kriging.
        <xref ref-type="bibr" rid="ref130">Zhang et al. (2015)</xref>
        used
Fourier learning algorithm. In all these works, for a set of samples,
it verifies if the resulting accuracy is acceptable for stakeholders
(e.g., prediction error rate below 10%). While the accuracy is
not satisfactory, the process continues by obtaining an additional
sample of measured configurations and iterates again to produce
an improved prediction model.
        <xref ref-type="bibr" rid="ref91">Sarkar et al. (2015)</xref>
        propose a
sampling cost metric as a stopping criterion, where the objective
is to ensure the most optimal trade-off between measurement
effort and prediction accuracy. Sampling stops when the count of
features selected and deselected passes a predefined threshold.
      </p>
      <p>
        <xref ref-type="bibr" rid="ref17">Chen et al. (2005)</xref>
        propose a linear regression approach to
describe the generic performance behavior of application server
components running on component-based middleware
technologies. The model focuses on two performance factors: workload
and degree of concurrency.
        <xref ref-type="bibr" rid="ref103">Sincero et al. (2010)</xref>
        employ analysis
of covariance for identifying factors with significant effects on the
response or interactions among features. They aim at proposing
a configuration process where the user is informed about the
impact of their feature selection on the NFPs of interest.
      </p>
      <p>
        <xref ref-type="bibr" rid="ref68">Murwantara et al. (2014)</xref>
        use a set of five ML techniques (i.e.,
Linear regression, CART, Multilayer Perceptrons (MLPs), Bagging
Ensembles of CART, and Bagging Ensembles of MLPs) to learn how
to predict the energy consumption of web service systems. They
use WEKA’s implementation of these techniques with its default
parameters
        <xref ref-type="bibr" rid="ref35">(Hall et al., 2009)</xref>
        .
      </p>
      <p>Learning to rank. Instead of predicting the raw performance
value, it can be of interest to predict the rank of a configuration
(typically to identify optimal configurations, see RQ1).</p>
      <p>
        <xref ref-type="bibr" rid="ref74">Oh et al. (2017)</xref>
        adopt statistical learning techniques to
progressively shrink a configuration space and search for
nearoptimal configurations. First, the approach samples and measures
a set of configurations. For each pair of sampled configurations,
this approach identifies features that are common (de)selected.
Then, it computes the performance influence of each common
decision to find the best regions for future sampling. The
performance influence measures the average performance over the
samples that have the feature selected against the samples that
have the feature deselected, i.e., the sample is partitioned by
whether a configuration includes a particular feature or not. In
addition, Welch’s t-test evaluates whether the performance mean
of one sample group is higher than the other group with 95%
confidence. The most influential decisions are added to the sample.
This process continues recursively until they identify all decisions
that are statistically certain to improve program performance.
      </p>
      <sec id="sec-8-1">
        <title>They call this a Statistical Recursive Searching technique.</title>
        <p>
          <xref ref-type="bibr" rid="ref69">Nair et al. (2017)</xref>
          compute accuracy by using the mean rank
difference measurement (the predicted rank order is compared
to the optimal rank order). They demonstrate that their
approach can find optimal configurations of a software system
using fewer measurements than the approach proposed by
          <xref ref-type="bibr" rid="ref91">Sarkar
et al. (2015)</xref>
          . One drawback of this approach is that it requires a
holdout set, against which the current model (built interactively)
is compared. To overcome this issue, instead of making
comparisons,
          <xref ref-type="bibr" rid="ref70 ref71">Nair et al. (2018)</xref>
          consider a predefined stopping criterion
(budget associated with the optimization process). While the
criterion is not met, the approach finds the configuration with
the best accuracy and add the configuration to the training set.
Consequently, in each step, the approach discards less satisfactory
configurations which have a high probability of being dominated.
This process terminates when the predefined stopping condition
is reached.
          <xref ref-type="bibr" rid="ref70 ref71">Nair et al. (2018)</xref>
          demonstrate that their approach is
much more effective for multi-objective configuration
optimization than state-of-the-art approaches
          <xref ref-type="bibr" rid="ref133 ref69 ref85 ref91">(Zuluaga et al., 2016; Sarkar
et al., 2015; Nair et al., 2017)</xref>
          .
        </p>
        <p>
          <xref ref-type="bibr" rid="ref61">Martinez et al. (2018)</xref>
          propose the use of data mining
interpolation techniques (i.e., similarity distance, similarity radius,
weighted mean) for ranking configurations through user
feedback on a configuration sample. They estimate the user
perceptions of each feature by computing the value of the chi-squared
statistic with respect to the correlation score given by users on
configurations.
        </p>
        <p>
          Transfer learning. The previous approaches assume a static
environment (e.g., hardware, workload) and NFP such that
learning has to be repeated once the environment and NFP changes. In
this scenario, transfer learning techniques are adopted to reuse
(already available) knowledge from other relevant sources to
learn a performance for a target system instead of relearning a
model from scratch
          <xref ref-type="bibr" rid="ref114 ref115 ref116 ref117 ref127 ref15 ref25 ref27 ref28 ref42 ref43 ref44 ref6 ref79 ref80 ref85 ref85 ref88">(Valov et al., 2017; Jamshidi et al., 2017b,a,
2018)</xref>
          .
        </p>
        <p>
          <xref ref-type="bibr" rid="ref114">Valov et al. (2017)</xref>
          investigate the use of transfer learning
across different hardware platforms. They used 25 different
hardware platforms to understand the similarity of performance
prediction. They created a prediction model using CART. With CART,
the resulting prediction models can be easily understood by end
users. To transfer knowledge from a related source to a target
source, they used a simple linear regression model.
        </p>
        <p>Jamshidi et al. (2017b) use Gaussian Process Models (GPM) to
model the correlation between source and target sources using
the measure of similarity. GPM offers a framework in which
predictions can be done using mean estimates with a confidence
interval for each estimation. Moreover, GPM computations are
based on linear algebra which is cheap to compute. This is
especially useful in the domain of dynamic SPL configuration where
learning a prediction model at runtime in a feedback loop under
time and resource constraints is typically time-constrained.</p>
        <p>Jamshidi et al. (2017a) combine many statistical and ML
techniques (i.e., Pearson linear correlation, Kullback–Leibler divergence,
Spearman correlation coefficient, paired t-test, CART, step-wise linear
regression, and multinomial logistic regression) to identify when
transfer learning can be applied. They use CART for estimating
the relative importance of configuration options by examining
how the prediction error will change for the trained trees on
the source and target. To investigate whether interactions across
environments will be preserved, they use step-wise linear
regression models (a.k.a., performance-influence models). This model
learns all pairwise interactions, then it compares the coefficients
of the pairwise interaction terms independently in the source
and target environments. To avoid exploration of invalid
configurations and reduce measurement effort, they use a multinomial
logistic regression model to predict the probability of a
configuration being invalid, then they compute the correlation between
the probabilities from both environments.</p>
        <p>
          <xref ref-type="bibr" rid="ref43">Jamshidi et al. (2018)</xref>
          propose a sampling strategy, called
L2S, that exploits common similarities across environments from
Jamshidi et al. (2017a). L2S extracts transferable knowledge from
the source to drive the selection of more informative samples in
the target environment. Based on identifying interesting regions
from the performance model of the source, it generates and
selects configurations in the target environment iteratively.
        </p>
        <p>
          Classification problem. Temple et al.
          <xref ref-type="bibr" rid="ref115 ref116 ref133 ref25 ref27 ref28 ref85 ref88 ref94">(2016, 2017a)</xref>
          and
          <xref ref-type="bibr" rid="ref4">Acher
et al. (2018)</xref>
          use CART to infer constraints to avoid the derivation
of invalid (non-acceptable) configurations. CART considers a path
in a tree as a set of decisions, where each decision corresponds
to the value of a single feature. The approach creates new
constraints in the variability model by building the negation of the
conjunction of a path to reach a faulty leaf. They learn constraints
among Boolean and numerical options. As an academic example,
          <xref ref-type="bibr" rid="ref4">Acher et al. (2018)</xref>
          introduce Vary LATEX to guide researchers to
meet paper constraints by using annotated LATEX sources. To
improve the learning process, Temple et al. (2018) specifically target
low confidence areas for sampling. The authors apply the idea
of using an adversarial learning technique, called evasion attack,
after a classifier is trained with a Support Vector Machine (SVM).
In addition, Temple et al. (2017a) support the specialization of
configurable systems for a deployment at runtime. In a similar
context,
          <xref ref-type="bibr" rid="ref88">Safdar et al. (2017)</xref>
          infer constraints over multi-SPLs (i.e.,
they take into account cross-SPLs rules).
        </p>
        <p>
          Input sensitivity. Several works consider the influence of
the input data on the resulting prediction model
          <xref ref-type="bibr" rid="ref110 ref125 ref21 ref25 ref28 ref40 ref57 ref85 ref89">(Lillack et al.,
2013; Ding et al., 2015; Thornton et al., 2013; Xu et al., 2008;
Hutter et al., 2011; Grebhahn et al., 2017; El Afia and Sarhani,
2017; Ghamizi et al., 2019)</xref>
          . Many authors
          <xref ref-type="bibr" rid="ref110 ref125 ref21 ref25 ref28 ref40 ref85 ref89">(Ding et al., 2015;
Thornton et al., 2013; Xu et al., 2008; Hutter et al., 2011;
Grebhahn et al., 2017; El Afia and Sarhani, 2017; Ghamizi et al.,
2019)</xref>
          address input sensitivity in algorithm auto-tuning. They
use learning techniques to search for the best algorithmic variants
and parameter settings to achieve optimal performance for a
given input instance. It is well known that the performance of
SAT solver and learning methods strongly depends on making the
right algorithmic and parameter choices, therefore SATzilla
          <xref ref-type="bibr" rid="ref125">(Xu
et al., 2008)</xref>
          and Auto-WEKA
          <xref ref-type="bibr" rid="ref110">(Thornton et al., 2013)</xref>
          search for the
best SAT solver and learning technique for a given input instance.
          <xref ref-type="bibr" rid="ref57">Lillack et al. (2013)</xref>
          treat the variability caused by the input data
as the variability of the SPL. As it is not feasible to model every
possible input data, they cluster the data based on its relevant
properties (e.g., 10 kB and 20 MB in an or-group for file inputs of
a compression library).
        </p>
        <p>
          Reinforcement learning.
          <xref ref-type="bibr" rid="ref94">Sharifloo et al. (2016)</xref>
          use a
reinforcement learning technique to automatically reconfigure
dynamic SPLs to deal with context changes. In their approach,
learning continuously observes measured configurations and
evaluates their ability to meet the contextual requirements. Unmet
requirements are addressed by learning new adaptation rules
dynamically, or by modifying and improving the set of existing
adaptation rules. This stage takes software evolution into account
to address new contexts faced at run-time.
        </p>
        <p>Numerous statistical learning algorithms are used in the
literature to learn software configuration spaces. The most
used are standard machine learning techniques, such as
polynomial linear regressions, decision trees, or Gaussian
process models. The targeted use-case and the engineering
context explain the diversity of solutions: either a
supervised classification or regression problem is tackled; the
requirements in terms of interpretability and accuracy may
differ; there is some innovation in the sampling phase to
progressively improve the learning. Still the use of others
(more powerful) ML techniques such as deep learning,
adversarial learning, and even the idea of learning different
models (e.g., one for each application objective) that could
co-evolve can be further explored in future works. Also,
unsupervised learning is another potential candidate to
support the exploration of configurable systems. For instance,
clustering techniques can be used to group together
configurations and reduce the costly measurements of each
individual configuration within a cluster. Still, analyzing its
feasibility remains unexplored.</p>
      </sec>
      <sec id="sec-8-2">
        <title>4.3. RQ3: Which techniques are used to gather measurements of functional and non-functional properties of configurations?</title>
        <p>The measuring step takes as input a sample of configurations;
and measures, for each configuration, their NFPs. In this section,
we investigate how the measurement procedures are technically
realized.</p>
        <p>
          Most proposals consider NFPs, such as elapsed time in seconds.
In essence, NFPs consist of a name, a domain, a value, and a
unit
          <xref ref-type="bibr" rid="ref12">(Benavides et al., 2005)</xref>
          . The domain type of an NFP can
be either quantitative (e.g., real and integer) or qualitative (e.g.,
string and Boolean). Quantitative (QT) properties are typically
represented as a numeric value, thus they can be measured on
a metric scale, e.g., the configuration is executed in 13.63 s.
Qualitative (QL) properties are represented using an ordinal scale,
such as low (− ) and high (+); e.g., the configuration produces a
high video quality.
        </p>
        <p>Measurements can be obtained through five different
strategies: execution (EX), static analysis (SA), simulation (SI), user
feedback (UF), and synthetic measurements (SM). Fig. 5 shows that
automated execution is by far the most used technique to measure
configuration properties.</p>
        <p>Fig. 5. Strategies applied to measure the sample of configurations.</p>
        <p>There are 95 SPLs documented in the literature (see Table C.8
in appendix). They are of different sizes, complexities,
implemented in different programming languages (C, C++, and Java),
varying implementation techniques (conditional compilation and
feature-oriented programming), and from several different
application domains (e.g., operating systems, video encoder, database
system) and developers (both academic and industrial).
Therefore, they cover a broad spectrum of scenarios. It is important
to mention that the same subject systems may differ in the
number of features, feature interactions, and in the number of
valid configurations — the experimental setup is simply different.</p>
        <p>Next, we detail the particularities of NFPs. Specifically, we
describe how the measurement is performed, what process and
strategies are adopted to avoid biases in the results, and also
discuss the cost of measuring.</p>
        <p>
          Time. The time spent by a software configuration to realize
a task is an important concern and has been intensively
considered under different flavors and terminologies (see Fig. 6).
          <xref ref-type="bibr" rid="ref95">Siegmund et al. (2015)</xref>
          invested more than two months (24/7)
for measuring the response time of all configurations of different
subject systems (Dune MGS, HIPAcc, HSMGP, JavaGC, SaC, x264).
For each system, they use a different configuration of hardware.
The configurations’ measurements are reused in many papers,
mainly for evaluating the proposed learning process
          <xref ref-type="bibr" rid="ref107 ref108 ref117 ref127 ref32 ref70 ref71 ref85 ref85">(Guo et al.,
2017; Nair et al., 2018; Temple et al., 2017b)</xref>
          .
          <xref ref-type="bibr" rid="ref17">Chen et al. (2005)</xref>
          used a simulator to measure the response time of a sequence
of 10 service requests by a client to the server. They use two
implementation technologies, CORBA and EJB.
        </p>
        <p>
          Time is a general notion and can be refined according to
the application domain. For instance,
          <xref ref-type="bibr" rid="ref52">Kolesnikov et al. (2018)</xref>
          considered compression time of a file on lrzip (Long Range ZIP).
The measurements were conducted on a dedicated server and
repeated multiple times to control measurements noise.
          <xref ref-type="bibr" rid="ref46">Kaltenecker et al. (2019)</xref>
          considered 7-ZIP, a file archiver written
in C++, and measured the compression time of the Canterbury
corpus.9
        </p>
        <p>
          In another engineering context, several works
          <xref ref-type="bibr" rid="ref113 ref117 ref117 ref130 ref30 ref46 ref52 ref69 ref70 ref71 ref85">(Guo et al.,
2013; Kolesnikov et al., 2018; Nair et al., 2017; Zhang et al.,
2015; Nair et al., 2018; Valov et al., 2015; Kaltenecker et al.,
2019)</xref>
          measured the encoding time of an input video over 1,152
valid configurations of x264. x264 is a configurable system for
encoding video streams into the H.264/MPEG-4 AVC format. As
benchmark, the Sintel trailer (735 MB) is used and an encoding
from AVI is considered.
          <xref ref-type="bibr" rid="ref46">Kaltenecker et al. (2019)</xref>
          measured the
encoding time of a short piece of the Big Buck Bunny trailer over
216,000 valid configurations of VPXENC (VP9), a video encoder
that uses the VP9 format.
        </p>
        <p>
          Latency has caught the attention of several researchers.
          <xref ref-type="bibr" rid="ref41">Jamshidi and Casale (2016)</xref>
          and Jamshidi et al. (2017b) measure
the average latency (i.e., how fast it can respond to a request)
of three stream processing applications on Apache Storm
(WordCount, RollingSort, SQL) over a window of 8 min and 2 h,
respectively. Jamshidi et al. (2017b) also measure the average latency of
the NoSQL database system on Apache Cassandra over a window
of 10 min.
          <xref ref-type="bibr" rid="ref41">Jamshidi and Casale (2016)</xref>
          performed the
measurements on a multi-node cluster on the EC2 cloud. Jamshidi et al.
(2017b) performed the same measurement procedure reported
for CPU usage. The measurements used by Nair et al. (2017,
2018) were derived from
          <xref ref-type="bibr" rid="ref41">Jamshidi and Casale (2016)</xref>
          .
          <xref ref-type="bibr" rid="ref120">Weckesser
et al. (2018)</xref>
          measure the latency of transferred messages over
an adaptive Wireless Sensor Networks (WSNs) via simulation.
100 fully-charged nodes were distributed randomly onto a square
region for each simulation run. Van Aken et al. (2017a) measure
the latency for two OLTP DBMSs (MySQL v5.6, Postgres v9.3) over
three workloads (The Yahoo Cloud Serving Benchmark (YCSB),
TPC-C, and Wikipedia) during five minutes observation periods.
The OLTP DBMS are deployed on m4.large instances with 4 vCPUs
and 16 GB RAM on Amazon EC2. They also consider the total
execution time of the OLAP DBMS (Actian Vector v4.2) over the
TPC-H workload on m3.xlarge instances with 4 vCPUs and 15 GB
RAM on Amazon EC2. All of the training data was collected using
the DBMSs’ default isolation level.
        </p>
        <p>
          <xref ref-type="bibr" rid="ref94">Sharifloo et al. (2016)</xref>
          measure time throughout the
evolution of a configurable system (an SPL). In an SPL evolution
scenario, the set of measured configurations may include a removed
feature or violate changed constraints. In this case,
configurations are removed from the sample. Moreover, a modified feature
implies to recompute the configuration measurements.
        </p>
      </sec>
    </sec>
    <sec id="sec-9">
      <title>Other NFPs (CPU power consumption, CPU usage, size, etc.)</title>
      <p>Beyond time, other quantitative properties have been considered.</p>
      <p>
        <xref ref-type="bibr" rid="ref69">Nair et al. (2017)</xref>
        measured the compressed size of a
specific input over 432 valid configurations of Lrzip, a compression
program optimized for large files.
        <xref ref-type="bibr" rid="ref98">Siegmund et al. (2011</xref>
        , 2012b,
2013b) measured the memory footprint of nine configurable
systems: LinkedList, Prevayler, ZipMe, PKJab, SensorNetwork, Violet,
Berkeley DB, SQLite, and Linux kernel.
        <xref ref-type="bibr" rid="ref70 ref71">Nair et al. (2018)</xref>
        and
      </p>
      <p>
        <xref ref-type="bibr" rid="ref133">Zuluaga et al. (2016)</xref>
        used LLVM, a configurable modular compiler
infrastructure. The footprint is measured as the binary size of a
compiled configuration.
      </p>
      <p>
        Several works
        <xref ref-type="bibr" rid="ref107 ref108 ref115 ref116 ref117 ref133 ref19 ref25 ref27 ref28 ref68 ref70 ref71 ref85 ref85 ref88">(Murwantara et al., 2014; Nair et al., 2018;
Temple et al., 2017a; Couto et al., 2017; Zuluaga et al., 2016; Jamshidi
et al., 2019)</xref>
        measured CPU power consumption.
        <xref ref-type="bibr" rid="ref68">Murwantara et al.
(2014)</xref>
        used a simulator to compute CPU power consumption over
a web service under different loads. They used a kernel-based
virtual machine to conduct the measurements based on several
combinations of HTTP servers and variant PHP technologies
connected to a MySQL database system. They divided the experiment
into blocks of 10 s for 100 increasing stages. In the first 10 s,
they produce loads of one user per second, and in the next 10 s,
they produce the load of two users per second and so on. This
results in 1–100 users per period of 10 s.
        <xref ref-type="bibr" rid="ref19">Couto et al. (2017)</xref>
        propose a static learning approach of CPU power consumption.
Their approach assures that the source code of every feature from
a configuration is analyzed only once. To prove its accuracy, they
measured at runtime the worst-case CPU power consumption
to execute a given instruction on 7 valid configurations of the
disparity SPL
        <xref ref-type="bibr" rid="ref118">(Venkata et al., 2009)</xref>
        . They repeated the
measurements 200 times for each configuration using the same input.
Since their goal was to determine the worst-case CPU power
consumption, they removed the outliers (5 highest and lowest
values) and retrieved the highest value from every configuration
for validation proposes.
      </p>
      <p>Jamshidi et al. (2017b) use simulation measurements. They
have repeatedly executed a specific robot mission to navigate
along a corridor offline in a simulator and measured performance
in terms of CPU usage on the CoBot system. To understand the
power of transfer learning techniques, they consider several
simple hardware changes (e.g., processor capacity) as well as severe
changes (e.g., local desktop computer to virtual machines in the
cloud).10 Each measurement took about 30 s. They measured each
configuration of each system and environment 3 times.</p>
      <p>
        To overcome the cost of measuring realistic NFPs,
        <xref ref-type="bibr" rid="ref102">Siegmund
et al. (2017)</xref>
        proposed a tool, called Thor, to generate artificial and
realistic synthetic measurements, based on different distribution
patterns of property values for features, interactions, and
configurations from a real-world system. Jamshidi et al. (2019) adopted
Thor to synthetically measure the property energy consumption
10 For a complete list of hardware/software variability, see the repository at
https://github.com/pooyanjamshidi/transferlearning.
for a robot to complete a mission consisting of randomly chosen
tasks within a map.
      </p>
      <p>
        Qualitative properties. Instead of measuring a numerical
value, several papers assign a qualitative value to configurations.
In
        <xref ref-type="bibr" rid="ref4">Acher et al. (2018)</xref>
        , a variant is considered acceptable or not
thanks to an automated procedure. In Temple et al. (2016, 2018),
a quantitative value is originally measured and then transformed
into a qualitative one through the definition of a threshold.
In
        <xref ref-type="bibr" rid="ref86">Queiroz et al. (2016)</xref>
        , a variant is considered as acceptable or
not based on the static evolution historical analysis of commit
messages (i.e., they identify a defect by searching for the following
keywords: bug, fix, error, and fail). The learning process then aims
to predict the class of the configuration — whether configurations
are acceptable or not, as defined by the threshold.
      </p>
      <p>
        Software defects are considered in
        <xref ref-type="bibr" rid="ref128">Yilmaz et al. (2006)</xref>
        ,
        <xref ref-type="bibr" rid="ref53">Krismayer et al. (2017)</xref>
        ,
        <xref ref-type="bibr" rid="ref27">Gargantini et al. (2017)</xref>
        ,
        <xref ref-type="bibr" rid="ref129">Yilmaz et al. (2014)</xref>
        ,
        <xref ref-type="bibr" rid="ref84">Porter et al. (2007)</xref>
        ,
        <xref ref-type="bibr" rid="ref8">Amand et al. (2019)</xref>
        .
        <xref ref-type="bibr" rid="ref128">Yilmaz et al. (2006)</xref>
        and
        <xref ref-type="bibr" rid="ref84">Porter et al. (2007)</xref>
        characterize defects of the ACE+TAO system.
        <xref ref-type="bibr" rid="ref128">Yilmaz et al. (2006)</xref>
        test each supposed valid configuration on
the Red Hat Linux 2.4.9-3 platform and on Windows XP
Professional using 96 developer-supplied regression tests. In
        <xref ref-type="bibr" rid="ref84">Porter
et al. (2007)</xref>
        , developers currently run the tests continuously on
more than 100 largely uncoordinated workstations and servers at
a dozen sites around the world. The platforms vary in versions
of UNIX, Windows, Mac OS, as well as to real-time operating
systems. To examine the impact of masking effects on Apache
v2.3.11-beta and MySQL v5.1,
        <xref ref-type="bibr" rid="ref129">Yilmaz et al. (2014)</xref>
        grouped the
test outcomes into three classes: passed, failed, and skipped. They
call this approach a ternary-class fault characterization.
        <xref ref-type="bibr" rid="ref27">Gargantini
et al. (2017)</xref>
        focus on comparing the defect detection capability on
different sample heuristics (see Section 4.2), while
        <xref ref-type="bibr" rid="ref8">Amand et al.
(2019)</xref>
        deal with comparing the accuracy of several learning
algorithms to predict whether a configuration will lead to a defect. In
the dynamic configuration scenario,
        <xref ref-type="bibr" rid="ref53">Krismayer et al. (2017)</xref>
        use
event logs (via simulation) from a real-world automation SoS to
mine different types of constraints according to real-time defects.
      </p>
      <p>
        Finally,
        <xref ref-type="bibr" rid="ref61">Martinez et al. (2018)</xref>
        consider a 5-point user likeability
scale with values ranging from 1 (strong dislike) to 5 (strong like).
In this work, humans have reviewed and labeled configurations.
      </p>
      <p>Accuracy of measurements. In general, measuring NFPs (e.g.,
time) is a difficult process since several confounding factors
should be controlled. The need to gather measures over numerous
configurations exacerbates the problem.</p>
      <p>
        <xref ref-type="bibr" rid="ref120">Weckesser et al. (2018)</xref>
        mitigated the construct threat of the
inherent randomness and repeated all runs five times with
different random seeds. Measurements started after a warm-up time
of 5 min.
        <xref ref-type="bibr" rid="ref46">Kaltenecker et al. (2019)</xref>
        measured each configuration
between 5 to 10 times until reaching a standard deviation of
less than 10%.
        <xref ref-type="bibr" rid="ref130">Zhang et al. (2015)</xref>
        repeated the measurements 10
times.
      </p>
      <p>
        To investigate the influence of measurement errors on the
resulting model,
        <xref ref-type="bibr" rid="ref52">Kolesnikov et al. (2018)</xref>
        and
        <xref ref-type="bibr" rid="ref23">Duarte et al. (2018)</xref>
        injected measurement errors to the original measurements and
repeated the learning process with polluted datasets. Then, they
compared the prediction error of the noisy models to the
prediction error of the original models to see the potential
influence of measurement errors. For each subject system,
        <xref ref-type="bibr" rid="ref52">Kolesnikov
et al. (2018)</xref>
        repeated the learning process five times for different
increasing measurement errors.
      </p>
      <p>
        Dynamic properties are susceptible to measurement errors
(due to non-controlled external influences) which may bias the
results of the measuring process. To account for measurement
noise and be subject to external influences, these properties need
to be measured multiple times on dedicated systems. Thus, the
total measurement cost to obtain the whole data used in the
experiments is overly expensive and time-consuming (e.g.,
        <xref ref-type="bibr" rid="ref46">Kaltenecker et al. (2019)</xref>
        spent multiple years of CPU time). According
to
        <xref ref-type="bibr" rid="ref57">Lillack et al. (2013)</xref>
        , there is a warm-up phase followed by
multiple times runs; and the memory must be set up large
enough to prevent disturbing effects from the Garbage
Collector, as well as all operations, must be executed in memory so
that disk or network I/O will also produce no disturbing effects.
Most of the works only consider the variability of the subject
system, while they use static inputs and hardware/software
environments. Therefore, the resulting model may not properly
characterize the performance of a different input or environment,
since most of the properties (e.g., CPU power consumption and
compression time) are dependent of the input task and the used
hardware/software. Consequently, hardware/software must also
be taken into account as dependent variables as considered by
Jamshidi et al. (2017b,a).
      </p>
      <p>There are some properties that are much accurate, because e.g.
they are not influenced by the used hardware, such as footprint.
Siegmund et al. (2013b) parallelized the measurements of the
footprint on three systems and used the same compiler.
Moreover, the footprint can be measured quickly only once, without
measurement bias.</p>
      <p>
        Cost of measurements. The cost of observing and measuring
software can be important, especially when multiple
configurations should be considered. The cost can be related to
computational resources needed (in time and space). It can also be
related to human resources involved in labeling some
configurations
        <xref ref-type="bibr" rid="ref117 ref61">(Martinez et al., 2018)</xref>
        .
      </p>
      <p>
        <xref ref-type="bibr" rid="ref133">Zuluaga et al. (2016)</xref>
        and
        <xref ref-type="bibr" rid="ref70 ref71">Nair et al. (2018)</xref>
        measure the
quantitative NFP area of a field-programmable gate array (FPGA)
platform consisting of 206 different hardware implementations of a
sorting network for 256 inputs. They report that the
measurement of each configuration is very costly and can take up to many
hours.
        <xref ref-type="bibr" rid="ref84">Porter et al. (2007)</xref>
        characterization of defects for each
configuration ranges from 3 h on quad-CPU machines to 12–18 h
on less powerful machines.
        <xref ref-type="bibr" rid="ref128">Yilmaz et al. (2006)</xref>
        took over two
machine years to run the total of 18,792 valid configurations.
In Siegmund et al. (2012b), a single measurement of memory
footprint took approximately 5 min.
        <xref ref-type="bibr" rid="ref109">Temple et al. (2016)</xref>
        report
that the execution of a configuration to measure video quality
took 30 min on average. They used grid computing to distribute
the computation and scale the measurement for handling 4,000+
configurations. The average time reported by
        <xref ref-type="bibr" rid="ref68">Murwantara et al.
(2014)</xref>
        to measure the CPU power consumption of all sampled
configurations was 1,000 s. The authors set up a predefined
threshold to speed up the process.
      </p>
      <p>
        There are thirteen main NFPs supported in the literature (see
Fig. 6). Some of them are less costly, such as code complexity,
which can be measured statically by analyzing the number of
code lines
        <xref ref-type="bibr" rid="ref100 ref124 ref96">(Siegmund et al., 2012b)</xref>
        . As a result, the measurements
can be parallelized and quickly done only once. Otherwise,
dynamic properties, such as CPU power consumption and response
time are directly related to hardware and external influences.
While CPU power consumption might be measured under different
loads over a predefined threshold time, the property response time
is much costly as a threshold cannot be defined and the user does
not know how long it will take. To support the labeling of
configurations, some works use synthetic measurements and statistical
analysis strategies. Practitioners need to find a sweet spot among
the available techniques to have accurate measurements with a
small sample.
      </p>
      <p>
        Depending on the subject system and application domain (see
the dataset of
        <xref ref-type="bibr" rid="ref95">Siegmund et al. (2015)</xref>
        ), there are more
favorable cases with only a few seconds per configuration.
However, even in this case, the overall cost can quickly become
prohibitive when the number of configurations to measure is too
high. In
        <xref ref-type="bibr" rid="ref120">Weckesser et al. (2018)</xref>
        , all training simulation runs took
approximately 412 h of CPU time.
      </p>
      <p>Numerous qualitative and quantitative properties of
configurations are measured mainly through the use of automated
software procedures. For a given subject system and its
application domain, there may be more than one measure
(e.g., CPU power consumption and video quality for x264).
Also, the same NFP may be measured in different ways (e.g.,
video quality can be measured either by user feedback or
execution of a program to automatically attribute labels to
features). Time is the most considered performance measure
and is obtained in the literature through either execution,
simulation, static analysis, or synthetic measurement. The
general problem is to find a good tradeoff between the cost
and the accuracy of measuring numerous configurations
(e.g., simulation can speed up the measurement process at
the price of approximating the real observations).</p>
      <sec id="sec-9-1">
        <title>4.4. RQ4: How are learning-based techniques validated?</title>
        <p>In this section, we aim at understanding how the validation
process is conducted in the literature.</p>
        <p>
          There are five design strategies documented in the literature to
explore the sample data for learning and validation (see Table 3).
Merge. Training, testing, and validation sets are merged.
          <xref ref-type="bibr" rid="ref52">Kolesnikov et al. (2018)</xref>
          studied and analyzed the trade-offs
among prediction error, model size, and computation time of
performance-prediction models. For the purpose of their study,
they were specifically interested to explore the evolution of
the model properties to see the maximum possible extent of
the corresponding trade-offs after each iteration of the learning
algorithm. So, they performed a whole-population exploration
of the largest possible learning set (i.e., all valid configurations).
Therefore, in their validation process, training, testing and
validation sets are merged. In a similar scenario, other authors
          <xref ref-type="bibr" rid="ref117 ref17 ref70 ref71 ref74 ref85">(Chen
et al., 2005; Oh et al., 2017; Nair et al., 2018)</xref>
          also used the whole
sample for both, learning and validation, even they considered
a small set of valid configurations as the sample. Some studies
justify the use of a merge pool with the need to compare different
sampling methods. However, training the algorithm on the
validation pool may introduce bias to the results of the accurateness
of the approach. To overcome this issue, studies have used other
well-established ML design strategies.
        </p>
        <p>
          In particular,
          <xref ref-type="bibr" rid="ref32">Guo et al. (2017)</xref>
          study the trade-offs among
hold-out, cross-validation, and bootstrapping. For most of the
case studies, 10-fold cross-validation outperformed hold-out and
bootstrapping. In terms of the running time, although these three
strategies usually take seconds to run,
          <xref ref-type="bibr" rid="ref32">Guo et al. (2017)</xref>
          show that
hold-out is the fastest one, and 10-fold cross-validation tends to
be faster than bootstrapping.
        </p>
        <p>
          Hold-out (HO). Most of the works
          <xref ref-type="bibr" rid="ref107 ref108 ref109 ref113 ref114 ref115 ref116 ref117 ref117 ref117 ref117 ref117 ref120 ref121 ref127 ref128 ref133 ref133 ref25 ref27 ref28 ref30 ref32 ref4 ref4 ref41 ref42 ref43 ref44 ref53 ref69 ref7 ref70 ref71 ref8 ref85 ref85 ref85 ref85 ref85 ref85 ref85 ref88 ref91 ref94">(Jamshidi and Casale, 2016;
Guo et al., 2013; Jamshidi et al., 2017b; Sarkar et al., 2015;
Temple et al., 2016, 2017a; Valov et al., 2015, 2017; Weckesser
et al., 2018; Acher et al., 2018; Temple et al., 2018; Nair et al.,
2018; Yilmaz et al., 2006; Krismayer et al., 2017; Jamshidi et al.,
2018; Zuluaga et al., 2016; Amand et al., 2019; Alipourfard et al.,
2017; Nair et al., 2017; Guo et al., 2017; Westermann et al.,
2012)</xref>
          used a hold-out design. Hold-out splits an input sample
SC into two disjoined sets St and Sv, one for training and the
other for validation, i.e. SC = St ∪ Sv and St ∩ Sv = ∅. To avoid
bias in the splitting procedure, some works repeated it multiple
times. For example, in
          <xref ref-type="bibr" rid="ref113">Valov et al. (2015)</xref>
          , the training set for
each sample size was selected randomly 10 times. For
transferlearning applications
          <xref ref-type="bibr" rid="ref114 ref117 ref127 ref42 ref43 ref44 ref85 ref85">(Jamshidi et al., 2017b; Valov et al., 2017;
Jamshidi et al., 2018)</xref>
          , the training set comes from samples of the
target and related sources, while the validation set comes from
samples only from the target source.
Cross-validation (CV). Cross-validation splits an input sample SC
into k disjoined subsets of the same size, i.e., S = S1 ∪ · · · ∪ Sk,
where Si ∩ Sj = ∅ (i ̸= j); each subset Si is selected as the
validation set Sv, and all the remaining k − 1 subsets are selected
as the training set St . As an example,
          <xref ref-type="bibr" rid="ref129">Yilmaz et al. (2014)</xref>
          used a
5-fold cross-validation to create multiple models from different
subsets of the input data. In a 5-fold cross-validation, the sample
S is partitioned into 5 subsets (i.e., S = S1 ∪ S2 ∪ S3 ∪ S4 ∪ S5)
of the same size. Each subset is selected as the validation set
and all of the remaining subsets form the training set. Most
authors
          <xref ref-type="bibr" rid="ref117 ref32 ref61 ref68 ref85 ref85 ref88 ref90">(Guo et al., 2017; Samreen et al., 2016; Martinez et al.,
2018; Murwantara et al., 2014; Safdar et al., 2017)</xref>
          relayed on
a 10-fold cross-validation. 10-fold cross-validation follows the
same idea of a 5-fold cross-validation, producing k = 10 groups
of training and validation sets.
          <xref ref-type="bibr" rid="ref32">Guo et al. (2017)</xref>
          show that a
10fold cross-validation design does not work well for very small
samples, e.g., it did not work for an Apache system.
        </p>
        <p>
          Bootstrapping (BT). Four studies
          <xref ref-type="bibr" rid="ref115 ref116 ref25 ref27 ref28 ref32 ref41 ref69 ref85 ref85 ref85 ref88">(Van Aken et al., 2017a; Jamshidi
and Casale, 2016; Nair et al., 2017; Guo et al., 2017)</xref>
          relayed on the
bootstrapping design. Bootstrapping relies on random sampling
with replacement. Given an input sample SC with k
configurations, bootstrapping randomly selects a configuration Cb, with
1 ≤ b ≤ k and copies it to the training set St . However, it keeps Cb
in SC for the next selection. This process is repeated k times. Given
the training sample, bootstrapping uses SC \St as the validation
set Sv.
          <xref ref-type="bibr" rid="ref69">Nair et al. (2017)</xref>
          used a non-parametric bootstrap test
with 95% confidence for evaluating the statistical significance of
their approach. In non-parametric bootstrap, the input sample SC
is drawn from a discrete set, i.e., 95% of the resulting sample St
should fall within the 95% confidence limits about SC .
Dynamic sector validation.
          <xref ref-type="bibr" rid="ref121">Westermann et al. (2012)</xref>
          relayed on
two types of Dynamic Sector Validation designs: with local
prediction error scope (DSL) and with global prediction error scope
(DSG). Dynamic Sector Validation decides if a sample
configuration Ci ∈ SC is part either of the training or testing set based on
the sector’s prediction error of the adopted learning techniques.
In DSL, all sectors have a prediction error that is less than the
predefined threshold, while in DSG the average prediction error
of all sectors is less than the predefined threshold.
        </p>
        <p>
          There are studies where a sample design is not applicable
          <xref ref-type="bibr" rid="ref101 ref102 ref115 ref116 ref124 ref132 ref19 ref25 ref26 ref27 ref27 ref28 ref42 ref42 ref44 ref84 ref85 ref85 ref85 ref85 ref85 ref88">(Jamshidi et al., 2017a; Gargantini et al., 2017; Kolesnikov et al.,
2017; Couto et al., 2017; Porter et al., 2007; Siegmund et al., 2017;
Xi et al., 2012; Zheng et al., 2007; Jamshidi et al., 2019; Siegmund
et al., 2008; Etxeberria et al., 2014)</xref>
          , as well as studies
          <xref ref-type="bibr" rid="ref104 ref130 ref40 ref48 ref67 ref75 ref93 ref94 ref97 ref99">(Sharifloo
et al., 2016; Zhang et al., 2015; Siegmund et al., 2013a; Hutter
et al., 2011; Osogami and Kato, 2007)</xref>
          without further details
about the sample design. Given the sampling design, evaluation
metrics are used to (learn and) validate the resulting prediction
model. Table 4 sketches which evaluation metrics are used in
the literature. The first column identifies the study reference(s).
The second and third columns identify the name of the
evaluation metric and its application objective, respectively. Similar
to Table B.7, here the application objective is related to the
scenarios in which each evaluation metric has been already used
in the SPL literature. Therefore, in future researches, some metrics
may be explored in other scenarios as well. Notice that some
studies
          <xref ref-type="bibr" rid="ref100 ref103 ref124 ref94 ref96">(Sharifloo et al., 2016; Siegmund et al., 2012b; Sincero
et al., 2010)</xref>
          did not report any detail about the validation process.
        </p>
        <p>
          Evaluation metrics. State-of-the-art techniques rely on 50
evaluation metrics from which it is possible to evaluate the
accuracy of the resulting models in different application scenarios.
There are metrics dedicated to supervised classification problems
(e.g., precision, recall, F-measure). In such settings, the goal is to
quantify the ratio of correct classifications to the total number of
input samples. For instance,
          <xref ref-type="bibr" rid="ref109">Temple et al. (2016)</xref>
          used precision
and recall to control whether acceptable and non-acceptable
configurations are correctly classified according to the ground truth.
Others
          <xref ref-type="bibr" rid="ref117 ref4 ref84">(Temple et al., 2018; Porter et al., 2007)</xref>
          use qualitative
analysis to identify features with significant effects on defects,
and understand feature interactions and decide whether further
investigation of features is justified.
        </p>
        <p>There are also well-known metrics for regression problems,
such as, Mean Relative Error (MRE - Eq. (1)) and Mean Absolute
Error (MAE - Eq. (2)). These metrics aim at estimating the accuracy
between the exact measurements and the predicted one.
(1)
(2)
MRE =
MAE =
1</p>
        <p>∑ |C(pi) − C(pi′)|
|Sv | C∈Sv
|C(pi′) −</p>
        <p>C(pi)</p>
        <p>C(pi)|</p>
        <p>C(pi)</p>
        <p>Where Sv is the validation set, and C(pi) and C(pi′) indicate the
exact and predicted value of pi for C ∈ Sv , respectively.</p>
        <p>
          Contributions addressing learning-to-rank problems develop
specific metrics capable of assessing the ability of a learning
method to correctly rank configurations. For example, Nair et al.
(2017, 2018) use the error difference between the ranks of the
predicted configurations and the true measured configurations.
To get insights about when stop sampling, they also discuss the
trade-off between the size of the training set and rank difference.
Interpretability metrics. Some metrics are also used to better
understand configuration spaces and their distributions, for
example, when to use a transfer learning technique. In this context,
Jamshidi et al. (2017a) use a set of similarity metrics (Kullback–
Leibler, Pearson Linear Correlation, Spearman Correlation
Coefficient, and Rank Correlation) to investigate the relatedness of
the source and target environments. These metrics compute the
correlation between predicted and exact values from source and
target systems. Moreover,
          <xref ref-type="bibr" rid="ref52">Kolesnikov et al. (2018)</xref>
          and
          <xref ref-type="bibr" rid="ref114">Valov et al.
(2017)</xref>
          use size metrics as insights of interpretability.
          <xref ref-type="bibr" rid="ref114">Valov et al.
(2017)</xref>
          compare the structure (size) of prediction models built for
the same configurable system and trained on different hardware
platforms. They measured the size of a performance model by
counting the features used in the nodes of a regression tree, while
          <xref ref-type="bibr" rid="ref52">Kolesnikov et al. (2018)</xref>
          define the model size metric as the
number of configuration options in every term of the linear regression
model.
          <xref ref-type="bibr" rid="ref109">Temple et al. (2016)</xref>
          reported that constraints extracted
from decision trees were consistent with the domain
knowledge of a video generator and could help developers preventing
non-acceptable configurations. In their context, interpretability is
important both for validating the insights of the learning and for
encoding the knowledge into a variability model.
        </p>
        <p>
          Sampling cost. Several works
          <xref ref-type="bibr" rid="ref117 ref117 ref120 ref7 ref70 ref71 ref85 ref91">(Alipourfard et al., 2017; Sarkar
et al., 2015; Weckesser et al., 2018; Nair et al., 2018,?)</xref>
          use
a sampling cost measurement to evaluate their prediction
approach. In order to reduce measurement effort, these approaches
aim at sampling configurations in a smart way by using as few
configurations as possible. In
          <xref ref-type="bibr" rid="ref7">Alipourfard et al. (2017)</xref>
          ,
          <xref ref-type="bibr" rid="ref120">Weckesser
et al. (2018)</xref>
          and Nair et al. (2018,?), sampling cost is
considered as the total number of measurements required to train the
model. These authors show the trade-off between the size of the
training set and the accuracy of the model. Nair et al. (2018,?)
compare different learning algorithms, along with different
sampling techniques to determine exactly those configurations for
measurement that reveal key performance characteristics and
consequently reach the minimum possible sampling cost.
          <xref ref-type="bibr" rid="ref91">Sarkar
et al. (2015)</xref>
          introduce a cost model, which they consider the
measurement cost involved in building the training and testing
sets, as well as the cost of building the prediction model and
computing the prediction error. Notice that sampling cost can
be seen as an additional objective of the problem of learning
configuration spaces, i.e. not only the usual learning accuracy
should be considered.
        </p>
        <p>There is a wide range of validation metrics reported in the
literature, and some are reused amongst papers (e.g., MRE).
However, several metrics can be used for the same task and there is
not necessarily a consensus. Most of the studies use a unique
metric for validation, which may provide incomplete quantification of
the accuracy.</p>
        <p>The evaluation of the learning process requires to
confront a trained model with new, unmeasured configurations.
Hold-out is the most considered strategy for splitting
configuration data into a training set and a testing set. Depending
on the targeted task (regression, ranking, classification,
understanding), several metrics have been reused or defined.
In addition to learning accuracy, sampling cost and
interpretability of the learning model can be considered as part
of the problem and thus assessed.</p>
      </sec>
    </sec>
    <sec id="sec-10">
      <title>5. Emerging research themes and open challenges</title>
      <p>In the previous sections, we have given an overview of the
state-of-the-art in sampling, measuring, and learning software
configuration spaces.11 Here, we will now discuss open
challenges and their implications for research and practice.</p>
      <p>
        There are a few reports of real-world adoption
        <xref ref-type="bibr" rid="ref109 ref117 ref117 ref120 ref52">(Kolesnikov
et al., 2018; Temple et al., 2016; Jamshidi et al., 2019; Weckesser
et al., 2018)</xref>
        , but overall it seems that despite wide applicability in
terms of application domains, subject systems, or measurement
11 We consolidate the results from all research questions through a set of
bubble charts in our supplementary material at
https://github.com/VaryVary/MLconfigurable-SLR.
Evaluation metric
Closeness Range, Winner
Probability
Coverage
Jaccard Similarity
Performance-Relevant Feature
Interactions Detection Accuracy
t-masked metric
True Positive (TP) Rate, False
Positive (FP) Rate, Receiver
Operating Characteristic (ROC)
Mean Relative Error (MRE)
Highest Error (HE)
Mean Absolute Error (MAE)
Mann–Whitney U-test
F-test
Precision, Recall
Precision, Recall, F-measure
GAP
Kullback–Leibler (KL), Pearson
Linear Correlation, Spearman
Correlation Coefficient
Structure of Prediction Models
Rank Correlation
Domain Experts Feedback
Error Probability
Global Confidence, Neighbors
Density Confidence, Neighbors
Similarity Confidence
LT15, LT30
Mean Rank Difference
Median Magnitude of the
Relative Error (MdMRE)
Pareto Prediction Error
Rank Accuracy (RA)
Tuning Time
Mean Square Error (MSE)
p-value, R2, Residual Standard
Error (RSE)
Reward
Statistical Significance
Qualitative Analysis
Ranking Constraints
Delaney’s Statistics
Distance Function,
Hyper-volume (HV)
Equivalence among
Combinatorial Models
Failure Index Delta (FID),
Totally Repaired Models (TRM)
A1, A5
A2
A2
A2
A2, A4
A3
A3
A3
A3
A3
A3
A3
A3
A3
A3, A4
A4
A4
A4
A4, A5
A4, A5
A5
A5
A5
A5
properties, there is little concrete evidence that the proposed
learning techniques are actually adopted in widespread everyday
practice. For instance, the x264 video encoder is widely
considered in the academic literature, but we are not aware of reports
that learning-based solutions, as discussed here, have an impact
on the way developers maintain it or on the way users configure
it.
      </p>
      <p>Hence, in this section, we discuss some scientific and technical
obstacles that contribute to this gap between what is available in
academic literature and what is actually adopted in practice. In
doing so, we aim to identify (1) points that should be considered
when choosing and applying particular techniques in practice and
(2) opportunities for further research. We summarize our findings
in Table 5.</p>
      <sec id="sec-10-1">
        <title>5.1. Generalization and transfer learning</title>
        <p>
          A first and important risk when adopting a learning-based
approach is that the model may not generalize to new, previously
unseen configuration data. Some works have already observed
Given a new ‘‘context" (e.g., hardware change), can a
learning model be reused?
Consider techniques with interpretable representation
(e.g., CART, step-wise linear regression); application
objectives might determine relative importance of
interpretability
Beyond raw ML performance (e.g., accuracy) consider
bigger picture (e.g., interpretability, integration in
surrounding software system, robustness against
changes)
No clear relation between application objective and
chosen sampling techniques; different strategies
perform best on different workloads and NFPs (see
e.g.,
          <xref ref-type="bibr" rid="ref76">Pereira et al. (2020)</xref>
          ,
          <xref ref-type="bibr" rid="ref29">Grebhahn et al. (2019)</xref>
          )
The cost for executing and measuring configurations can
be a barrier
Expect that techniques described in academic literature
do not ‘‘just work’’ out of the box
Transfer learning from a known configuration space to a
new configuration one; identification of factors that
influence performance distribution
Methods for assessing interpretability; techniques that
generate an explanation in hindsight
Lack of actionable frameworks to choose techniques
based on consideration of multiple goals and trade-offs
between them
More work needed on effectiveness and strengths of
sampling techniques in different contexts
Simulations; distributed infrastructures; handling of
uncertainty; transfer learning to reuse knowledge
Existing works often evaluated in controlled
environments, more work needed where techniques are
evaluated/applied in realistic settings
Generalization
and transfer
learning
(Section 5.1)
Interpretability
(Section 5.2)
Learning
trade-offs
(Section 5.3)
Sampling
(Section 5.4)
Costeffectiveness
(Section 5.5)
Evaluation/
application in
realistic
settings
(Section 5.6)
Integrated tools
(Section 5.7)
        </p>
        <p>
          Consider tools that are open for integration in
real-world projects
Lack of integrated tools for supporting developers and
users
that many factors can influence the performance distribution of
a configurable system: the version of the software, the hardware
on which the software is executed, the workload, etc. The
consequence is that, e.g., a prediction model of a configurable system
may reach high accuracy in a given situation (close to the one
used for training) and then the performance drops when some of
these factors change. The reuse of learning-based models is thus
an important concern; the worst-case scenario is to learn a new
model for every usage of a configurable system. We notice that
several recent works aim to transfer the learning-based
knowledge of a configuration space to another setting (e.g., a different
hardware setup)
          <xref ref-type="bibr" rid="ref114 ref115 ref116 ref117 ref127 ref15 ref25 ref27 ref28 ref42 ref43 ref44 ref6 ref79 ref80 ref85 ref85 ref88">(Valov et al., 2017; Jamshidi et al., 2017b,a,
2018)</xref>
          . Another research direction is to characterize which and
to what extent factors influence the performance distribution of
a configurable system. The hope is to propose learning-based
solutions that are effective independent of the context in which
the configurable software is deployed.
        </p>
      </sec>
      <sec id="sec-10-2">
        <title>5.2. Interpretability</title>
        <p>In many engineering scenarios, developers or users of
configurable systems want to understand learning-based models and
not treat them as black-boxes. This observation partly explains
why decision trees (CART) and step-wise linear regressions are
the most used learning algorithms (see Table B.7): the resulting
models provide interpretable information, through rules or
coefficients. The challenge is to find a good-enough compromise
between accuracy and interpretability. Interpretability can be
the major objective to fulfill (application A2) or an important
concern, e.g., mining constraints (application A5) requires the use
of rule-based learning. For other tasks such as optimization (A3),
accuracy seems more important and hence the choice of a
wellperforming learning algorithm might be more important than
interpretability.</p>
        <p>
          We notice that there are few studies reported in the literature
whose evaluation objective is the comprehension of configurable
systems, for instance, to understand which options are influential.
However, we are not aware of empirical user studies to validate
the learning approach with regards to the comprehension by
domain experts. The existing works typically rely on a model size
metric to report the level of comprehension
          <xref ref-type="bibr" rid="ref114 ref117 ref52 ref85">(Valov et al., 2017;
Kolesnikov et al., 2018)</xref>
          . How to evaluate interpretability is an
open issue in machine learning
          <xref ref-type="bibr" rid="ref65">(Molnar, 2019)</xref>
          that also applies
to the learning of software configuration space.
        </p>
        <p>
          Another concern to consider is the computation time needed
to apply a statistical learning method. Many factors should be
considered: the algorithm itself, the size of the training set, the
number of features, the number of trees (for random forest),
the hyperparameters, etc. Looking at experiments and subject
systems considered in the literature, the size of the training set
as well as the number of features remain affordable so far: only a
few seconds and minutes are needed to train. However, there are
some exceptions with studies involving large systems (e.g., Linux
see
          <xref ref-type="bibr" rid="ref2 ref3">Acher et al. (2019</xref>
          a,b)) or learning procedures that can take
hours.
        </p>
        <p>Overall, the choice of a learning algorithm is a tradeoff
between different concerns (interpretability, accuracy, training
time). Which one practitioners should use and under which
conditions? There is still a lack of actionable frameworks that
would automatically choose or recommend a suited learning
technique based on an engineering context (e.g., targeted NFPs,
need of interpretability). So far, tree-based methods constitute a
good compromise and can be applied for many applications. More
(empirical) research is needed to help practitioners systematically
choosing an adequate solution.</p>
      </sec>
      <sec id="sec-10-3">
        <title>5.4. Sampling: the quest for the right strategy</title>
        <p>Currently, there is no dominant sampling technique that can
be used without further consideration in any circumstance by
practitioners. We reported 23 high-level sampling techniques
used by state-of-the-art approaches. Data of our systematic
review shows that there is no clear relation between the choice of
a sampling strategy and a specific application objective (A1, . . . ,
A6). Hence, more research is needed to evaluate the effectiveness
of particular sampling techniques.</p>
        <p>
          A first specific challenge when sampling over a configurable
system is to generate configurations conforming to constraints.
This boils down to solving a satisfiability problem (SAT) or
constraint satisfaction problem (CSP) when numerical values should
be handled
          <xref ref-type="bibr" rid="ref107 ref108 ref109 ref115 ref116 ref117 ref133 ref25 ref27 ref28 ref4 ref6 ref79 ref8 ref80 ref85 ref88 ref94 ref95">(Temple et al., 2016, 2017a, 2018; Amand et al., 2019;
Siegmund et al., 2015)</xref>
          . Recent results show that uniform, random
sampling is still challenging for configurable systems
          <xref ref-type="bibr" rid="ref81">(Plazar et al.,
2019)</xref>
          . Hence, true random sampling is sometimes hard to achieve
and some approximate alternatives, such as distance-based
sampling, have been proposed
          <xref ref-type="bibr" rid="ref46">(Kaltenecker et al., 2019)</xref>
          .
        </p>
        <p>
          In the context of testing software product lines, there are
numerous available heuristics, algorithms, and empirical
studies
          <xref ref-type="bibr" rid="ref22 ref37 ref54 ref59 ref63 ref68">(Medeiros et al., 2016; Lopez-Herrejon et al., 2015; do Carmo
Machado et al., 2014; Lee et al., 2012; Thüm et al., 2014)</xref>
          . In this
case, sampling is used to efficiently test a subset of
configurations with the hope of finding bugs. Though the sample has not
the same purpose (efficient identification of bugs), we believe a
learning-based process could benefit from works done in the
software testing domain. An important question is whether heuristics
or sampling criterion devised originally for testing have the same
efficiency when applied for learning configuration spaces.
        </p>
        <p>
          A striking challenge is to investigate whether there exists a
one-size-fits-all strategy for efficiently sampling a configuration
space. The effectiveness of a sampling strategy may depend on
the choice of a learning algorithm
          <xref ref-type="bibr" rid="ref29 ref46">(Grebhahn et al., 2019)</xref>
          , which
is hard to anticipate. Besides, recent results suggest there is no
single dominant sampling even for the same configurable system.
Instead, different strategies perform best on different workloads
and NFPs
          <xref ref-type="bibr" rid="ref29 ref46 ref76">(Pereira et al., 2020; Grebhahn et al., 2019)</xref>
          . An open
issue is to automatically and a priori select the most effective
sampling in a given situation. Finally, we observe that most
of the reported techniques only use as input a model of the
configuration space (e.g., a feature model). The incorporation of
knowledge and other types of input data in the sampling phase
needs more investigation in future research (e.g., system
documentation, implementation artifacts, code smells, and system
evolution).
        </p>
      </sec>
      <sec id="sec-10-4">
        <title>5.5. Cost-effectiveness of learning-based solutions</title>
        <p>A threat to practical adoption is the cost of learning-based
approaches. In addition to the cost of training a machine
learning model (see the earlier discussion on trade-offs), the cost of
gathering data and measuring configurations can be significant
(several days). Most of the NFPs reported in the literature (see
Fig. 6 and Table C.8) are quantitative and related to performance.
An organization may not have the resources to instrument a large
and accurate campaign of performance measurements.</p>
        <p>
          A general problem is to reduce the cost of measuring many
configurations. For example, the use of a distributed
infrastructure (e.g., cloud) has the merit of offering numerous computing
resources at the expense of networking issues and
heterogeneity
          <xref ref-type="bibr" rid="ref11 ref117 ref55">(Leitner and Cito, 2016; Bao et al., 2018)</xref>
          . Though some
countermeasures can be considered, it remains an important practical
problem. Hence, there is a tension between the accuracy of
measurements and the need to scale up the process to thousands
of configurations. At the opposite end of the spectrum,
simulation and static analysis are less costly but may approximate
the real measures: few studies explore them which demand
further investigation. Another issue is how performance
measurements transfer to a computational environment (e.g., hardware).
The recent rise of transfer learning techniques is an interesting
direction, but much more research is needed.
        </p>
        <p>
          Besides, only a few works deal with uncertainty when learning
configuration spaces (e.g., Jamshidi et al. (2017b)). There are
questions we could ask, such as how many times do we need to
repeat measurements? To what degree can we trust the results?
The measurement of performance is subject to intensive research:
there can be bias, noise or uncertainty
          <xref ref-type="bibr" rid="ref112 ref117 ref117 ref18 ref6">(Aleti et al., 2018; Colmant
et al., 2018; Trubiani and Apel, 2019)</xref>
          that can have in turn an
impact on the effectiveness of the whole learning process.
        </p>
        <p>Ideally, the investments required to sample, measure, and
learn configuration spaces should be outweighed by the benefits
of effectively performing a task (e.g., finding an optimal
configuration). The reduction of the cost is a natural research direction.
Another is to find a reasonable tradeoff.</p>
      </sec>
      <sec id="sec-10-5">
        <title>5.6. Realistic evaluation</title>
        <p>
          When transferring techniques from early research into
practice, we have to be careful with respect to things that can go
wrong and effort that is required for the different activities,
e.g., data acquisition and preparation, feature engineering,
selecting/adapting algorithms and models, evaluation in a controlled
experimental setting, deployment for use in practice, and
evolution/maintenance. Looking at many papers in ML and related
fields, one might get the impression that they focus on activities
‘‘in the middle’’, e.g., the development and evaluation of
algorithms, since they can be performed in vitro (in a controlled lab
environment), whereas activities at the beginning or the end need
to be done in vivo (in ‘‘dirty’’ real-life). Also, see the discussion by
          <xref ref-type="bibr" rid="ref14">Bosch et al. (2020)</xref>
          .
        </p>
        <p>This preliminary hypothesis needs further investigation, but it
seems that research should move out of the comfort zone in the
lab and face the ‘‘dirty’’ reality. Some examples of works that go
into this direction are given in the following.</p>
        <p>
          A first step from theoretical research towards applicability is
to go beyond reporting bare evaluation metrics (e.g., accuracy)
and at least discuss the applicability of the presented approach.
As an example, consider
          <xref ref-type="bibr" rid="ref120">Weckesser et al. (2018)</xref>
          who discuss
the applicability and effectiveness of their approach for dynamic
reconfiguration with a real-world scenario.
        </p>
        <p>This can be taken further by performing the evaluation (or
parts of it) in on a real system. For instance, instead of evaluating
a control algorithm with theoretical considerations one can
deploy that algorithm onto real physical systems which then have
to face the challenges and imperfections of reality. As an example,
see Jamshidi et al. (2019) who deal with the self-adaptation
of autonomous robots. They first evaluate their approach in a
theoretical setting, which is validated on real systems (theoretical
evaluation grounded in practice), and second in a robotics scenario
deployed onto robots (evaluation in practice).</p>
        <p>
          Another example is work by
          <xref ref-type="bibr" rid="ref109">Temple et al. (2016)</xref>
          where the
use of machine learning was driven by an open practical problem
(how to specify constraints of a product line, how to explore
a large configuration space with automated support) and
obtained results (rules/constraints) where validated with
developers, i.e., through collaboration with domain experts.
        </p>
        <p>
          A related ingredient for applicable research is to identify and
describe problems with a practical perspective and with input from
domain experts, ideally with input from actual application
scenarios. As an example consider the explicit problem description
(of modeling performance as influenced by configuration options)
by
          <xref ref-type="bibr" rid="ref52">Kolesnikov et al. (2018)</xref>
          which is based on a discussion with
domain experts (for high-performance computing). The authors
complete the paper with an appendix where they discuss the
most influential configuration options and link that back to the
domain knowledge.
        </p>
        <p>As outlined in the beginning of this section, there is still a
gap between research and practice. Some papers in our literature
Table A.6
Sample methods reported in the literature.</p>
        <p>Sample method Reference
Random
review show it is possible to apply different research methods in
order to assess the actual potential of learning-based solutions for
engineering real-world systems.</p>
      </sec>
      <sec id="sec-10-6">
        <title>5.7. Integrated tools</title>
        <p>
          We observe that several studies provide either open source
or publicly available implementations, mainly on top of data
science libraries. Tools could assist end-users during, for instance,
the configuration process and the choice of options. Variability
management tools could be extended to encompass the learning
stages reported in this SLR. It could also benefit to software
developers in charge of maintaining configurable systems. However,
none of the reported studies provide tool support that is fully
integrated into existing SPL platforms or mainstream software
tools. An exception is SPLConqueror
          <xref ref-type="bibr" rid="ref100 ref124 ref96">(Siegmund et al., 2012b)</xref>
          that supports an interactive configuration process. We believe the
integration of tooling support is key for practical adoption and
the current lack may partly explain the gap between practice and
research.
        </p>
      </sec>
      <sec id="sec-10-7">
        <title>5.8. Final remarks and summary</title>
        <p>The problem of learning configuration spaces is at the
intersection of artificial intelligence (mainly statistical ML and constraint
reasoning) and software engineering. There are many existing
works that do not specifically address software systems and
are yet related. Hence, there is considerable potential to
‘‘connect the dots’’ and reuse state-of-the-art methods for learning
highly-dimensional spaces like software configuration spaces.</p>
        <p>For instance, an active research area that is related to choosing
the best algorithm is the automated algorithm selection
problem (Kerschke et al., 2019) where given a set of algorithms,
and a specific problem instance to be solved, the problem is to
determine which of the algorithms can be selected to perform
best on that instance. In our case, the set comprises all (valid)
configurations of a single, parameterized algorithm (whereas the
set of algorithms come from different software implementation
and systems for the problem of algorithm selection). We also
believe the line of research work presented in this literature review
can be related to specific problems like compiler autotuning (see
e.g., Ashouri et al. (2018)) or database management system tuning
(see e.g., Van Aken et al. (2017b)).</p>
        <p>We encourage researchers to explore possible synergies, e.g.,
how solutions and results in both areas can be reused or adapted.</p>
        <p>Finally, Table 5 summarizes the challenges discussed in this
section with (1) points to consider in practical applications and
(2) opportunities for further research.</p>
      </sec>
    </sec>
    <sec id="sec-11">
      <title>6. Threats to validity</title>
      <p>
        This section discusses potential threats to validity that might
have affected the results of the SLR. We faced similar threats
to validity as any other SLR. The findings of this SLR may have
been affected by bias in the selection of the primary studies,
inaccuracy in the data extraction and in the classification of
the primary studies, and incompleteness in defining the open
challenges. Next, we summarize the main threats to the validity
of our work and the strategies we have followed to minimize
their impact. We discussed the SLR validity with respect to the
two groups of common threats to validity: internal and external
validity
        <xref ref-type="bibr" rid="ref123">(Wohlin et al., 2000)</xref>
        .
      </p>
      <p>Internal validity. An internal validity threat concerns the
reliability of the selection and data extraction process. To further
Kernel Density Estimation and NSGA-II
C4.5 (J48)
Covariance Analysis
Multinomial Logistic Regression
K-Plane Algorithm
AdaRank
Bagging Ensembles of CART, Bagging Ensembles of MLPs
Data Mining Interpolation Technique
Factor Analysis, k-means, Ordinary Least Squares
Genetic Programming (GP)
Kriging
Max-Apriori Classifier, Exhaustive Feature Subsets
Classifiers, All Features Classifier, Incremental Feature
Examination classifier
Quick Optimization via Guessing
Random Online Adaptive Racing (ROAR), Sequential
Model-based Algorithm Configuration (SMAC)
Simulated Annealing
Smart Hill-Climbing
Statistical Recursive Searching
Tensorflow and Keras
Tree-structured Parzen Estimator (TPE)
Multivariate Adaptive Regression Splines (MARS),
Multivariate Polynomial Regression
Ridge Regression
Multilayer Perceptrons (MLPs)
Actor–Critic Learning
Lasso
Reinforcement Learning
CitLab Model
Evasion Attack
Hoeffding Tree, K*, kNN, Logistic Model Tree, Logistic
Regression, Naive Bayes, PART Decision List, Random
Committee, REP Tree, RIPPER
Pruning Rule-Based Classification (PART)
increase the internal validity of the review results, the search
for relevant studies was conducted in several relevant
scientific databases, and it was focused not only on journals but
also on conferences, symposiums, and workshops. Moreover, we
conducted the inclusion and exclusion processes in parallel by
involving three researchers and we cross-checked the outcome
after each phase. In the case of disagreements, we discussed
until a final decision was achieved. Furthermore, we documented
Learning technique
Adaptive ELAs, Multi-Class FDA-CIT, Static Error Locating
Arrays (ELAs), Ternary-Class FDA-CIT, Test Case-Aware CIT,
Traditional CIT
Bagging
Fourier Learning of Boolean Functions
Frequent Item Set Mining
Graph Family-Based Variant Simulator
Implicit Path Enumeration Technique (IPET)
Naive Bayes
Step-Wise Multiple Linear Regression
Classification and Regression Trees (CART)
A1, A2, A3, A4, A5
A1, A5
A2
A2
A2, A4
A3
A3
A3
A3
A3
A3
A3
A3
A3
A3
A3
A3
A3
A3
A3, A4
A3, A4
A3, A5
A4
A4
A4, A6
A5
A5
A5
A5
potentially relevant studies that were excluded. However, the
quality of the search engines could have influenced the
completeness of the identified primary studies (i.e., our search may have
missed those studies whose authors did not use the terms we
used in our search string to specify keywords).</p>
      <p>For the selected papers, a potential threat to validity is the
reliability and accuracy of the data extraction process, since not
all information was obvious to extract (e.g., many papers lacked
details about the measurement procedure and the validation
design of the reported study). Consequently, some data had to
be interpreted which involved subjective decisions by the
researchers. Therefore, to ensure the validity, multiple sources of
data were analyzed, i.e., papers, websites, technical reports,
manuals, and executable. Moreover, whenever there was a doubt
about some extracted data in a particular paper, we discussed
the reported data from different perspectives in order to resolve
all discrepancies. However, we are aware that the data extraction
process is a subjective activity and likely to yield different results
when executed by different researchers.</p>
      <p>External validity. A major threat to external validity is related
to the identification of primary studies. Key terms are directly
related to the scope of the paper and they can suffer a high
variation. We limited the search for studies mainly targeting
software systems (e.g., software product lines) and thus mainly
focus on software engineering conferences. This may affect the
completeness of our search results since we are aware of some
studies outside the software engineering community that also
address the learning of software configurable systems. To
minimize this limitation and avoid missing relevant papers, we also
analyzed the references of the primary studies to identify other
relevant studies. In addition, this SLR was based on a strict
protocol described in Section 2 which was discussed before the
start of the review to increase the reliability of the selection and
data extraction processes of the primary studies and allow other
researchers to replicate this review.</p>
      <p>Another external validity concerns the description of open
challenges. We are aware that the completeness of open
challenges is another limitation that should be considered while
interpreting the results of this review. It is also important to
explore general contributions from other fields outside the
software domain to fully gather the spread knowledge, which may
extend the list of findings in this field. Therefore, in future work,
the list of findings highlighted in Section 5 may be extended by
conducting an additional review, making use of other keywords
to be able to find additional relevant studies outside the software
community.</p>
    </sec>
    <sec id="sec-12">
      <title>7. Related work</title>
      <p>
        After the introduction of SLR in software engineering in 2004,
the number of published reviews in this field has grown
significantly
        <xref ref-type="bibr" rid="ref50">(Kitchenham et al., 2009)</xref>
        . A broad SLR has been conducted
by
        <xref ref-type="bibr" rid="ref38">Heradio et al. (2016)</xref>
        to identify the most influential researched
topics in SPL, and how the interest in those topics has evolved
over the years. Although these reviews are not directly related
to ours, the high level of detail of their research methodology
supported to structure and define our own methodology.
      </p>
      <p>
        <xref ref-type="bibr" rid="ref13">Benavides et al. (2010)</xref>
        presented the results of a literature
review to identify a set of operations and techniques that provide
support to the automatic analysis of variability models. In a
similar scenario,
        <xref ref-type="bibr" rid="ref58">Lisboa et al. (2010)</xref>
        and
        <xref ref-type="bibr" rid="ref77">Pereira et al. (2015)</xref>
        conducted an SLR on variability management of SPLs. They reported
several dozens of approaches and tools to support stakeholders in
an interactive and automatic configuration process. Strict to the
application engineering phase,
        <xref ref-type="bibr" rid="ref72">Ochoa et al. (2018)</xref>
        and
        <xref ref-type="bibr" rid="ref37">Harman
et al. (2014)</xref>
        conducted a literature review on the use of
searchbased software engineering techniques for optimization of SPL
configurations. In contrast to these previous reviews, our SLR
provides further details on the use of automatic learning techniques
to explore large configuration spaces. We contribute with a
catalogue of sampling approaches, measurement procedures, learning
techniques, and validation steps that serves as a summarization
of the results in this specific field.
      </p>
      <p>
        In
        <xref ref-type="bibr" rid="ref92">Sayagh et al. (2018)</xref>
        , 14 software engineering experts and
229 Java software engineers were interviewed to identify major
activities and challenges related to configuration engineering. In
complement, a SLR was conducted. In order to select papers,
authors focused on those in which the title and abstract
contain the keyword ‘‘config*’’, ‘‘misconfig*’’, or ‘‘mis-config*’’. On
the one hand, the scope is much broader: It spans all
engineering activities of configurable systems whereas our literature
review specifically targets learning-based approaches and
applications. On the other hand, we learn that configuration spaces
are studied in many different engineering contexts and we take
care of considering a broader terminology (see Table 1). Despite
different scope and reviewing methodology, the two surveys
are complementary. A research direction is to further explore
how learning-based approaches classified in this literature
review could support configuration engineering activities identified
in
        <xref ref-type="bibr" rid="ref92">Sayagh et al. (2018)</xref>
        .
      </p>
      <p>
        In the context of software fault prediction,
        <xref ref-type="bibr" rid="ref60">Malhotra (2015)</xref>
        conducts an SLR to summarize the learning techniques used in
this field and the performance accuracy of these techniques.
They classify machine learning techniques into seven categories:
Decision Tree, Bayesian Learning, Ensemble Learning, Rule Based
Learning, Evolutionary Algorithms, Neural Networks and
Miscellaneous. Although our SLR is much restricted by considering only
primary studies on the field of software product lines, we also
consider fault as a performance metric and thus we also report
on most of these techniques. Also, our results depict that the
evaluation metrics ‘‘precision, recall, and F-Measure" are the most
commonly used performance measures to compute the accuracy
of reported classification techniques. In addition, our SLR is much
broad once it also considers other properties besides fault,
therefore we report on both classification and regression techniques
and evaluation metrics.
      </p>
      <p>
        On the context of testing and verifying a software product
line, several works
        <xref ref-type="bibr" rid="ref117 ref22 ref37 ref54 ref59 ref63 ref68">(Varshosaz et al., 2018; Medeiros et al., 2016;
Lopez-Herrejon et al., 2015; do Carmo Machado et al., 2014;
Lee et al., 2012; Thüm et al., 2014)</xref>
        discussed product sampling
techniques. They classify the proposed techniques into different
categories and discuss the required input and criteria used to
evaluate these techniques, such as the sample size, the rate of
fault detection, and the tool support. Future work could benefit
from their results through the use of sampling techniques still
not explored by learning techniques (e.g. code and requirements
coverage), as well as the SPL testing community could benefit
from the sampling techniques reported in this paper still not
used for testing. In Khalil Abbasi et al. (2013), 111 real-world
Web configurators are analyzed but do not consider learning
mechanisms.
      </p>
      <p>
        None of surveys mentioned above directly address the use
of learning techniques to explore the behavior of large
configuration spaces through performance prediction. The main
topics of previous SLRs include variability model analysis,
variability management, configuration engineering, fault prediction
and SPL testing. There are several reviews investigating the use
of learning-based techniques in many other scenarios, such as
spam filtering
        <xref ref-type="bibr" rid="ref20 ref33">(Guzella and Caminhas, 2009; Crawford et al.,
2015)</xref>
        , text-documents classification
        <xref ref-type="bibr" rid="ref49">(Khan et al., 2010)</xref>
        ,
automated planning (
        <xref ref-type="bibr" rid="ref45">Jiménez et al., 2012</xref>
        ), genomic medicine (Leung
      </p>
      <p>
        Table C.8
Subject systems supported in the literature.
        <xref ref-type="bibr" rid="ref8">Amand et al. (2019)</xref>
        <xref ref-type="bibr" rid="ref124">Xi et al. (2012)</xref>
        <xref ref-type="bibr" rid="ref32">Guo et al. (2017)</xref>
        ,
        <xref ref-type="bibr" rid="ref52">Kolesnikov et al. (2018)</xref>
        ,
        <xref ref-type="bibr" rid="ref95">Siegmund et al. (2015)</xref>
        <xref ref-type="bibr" rid="ref133">Zuluaga et al. (2016)</xref>
        <xref ref-type="bibr" rid="ref21">Ding et al. (2015)</xref>
        <xref ref-type="bibr" rid="ref27">Gargantini et al. (2017)</xref>
        Siegmund et al. (2013b)
Siegmund et al. (2013b),
        <xref ref-type="bibr" rid="ref69">Nair et al. (2017)</xref>
        Van Aken et al. (2017a)
Jamshidi et al. (2017b)
Guo et al. (2013, 2017),
        <xref ref-type="bibr" rid="ref74">Oh et al. (2017)</xref>
        ,
        <xref ref-type="bibr" rid="ref52">Kolesnikov et al. (2018)</xref>
        ,
        <xref ref-type="bibr" rid="ref69">Nair et al.
(2017)</xref>
        ,
        <xref ref-type="bibr" rid="ref91">Sarkar et al. (2015)</xref>
        , Siegmund et al.
        <xref ref-type="bibr" rid="ref104 ref124 ref48 ref67 ref93 ref97">(2012a, 2013b)</xref>
        , Temple et al.
(2017a),
        <xref ref-type="bibr" rid="ref98">Siegmund et al. (2011</xref>
        , 2012b, 2015),
        <xref ref-type="bibr" rid="ref130">Zhang et al. (2015)</xref>
        ,
        <xref ref-type="bibr" rid="ref70 ref71">Nair et al.
(2018)</xref>
        ,
        <xref ref-type="bibr" rid="ref46">Kaltenecker et al. (2019)</xref>
        ,
        <xref ref-type="bibr" rid="ref101">Siegmund et al. (2008)</xref>
        ,
        <xref ref-type="bibr" rid="ref131">Zhang et al. (2016)</xref>
        <xref ref-type="bibr" rid="ref101">Siegmund et al. (2008)</xref>
        <xref ref-type="bibr" rid="ref129">Yilmaz et al. (2014)</xref>
        , Van Aken et al. (2017a),
        <xref ref-type="bibr" rid="ref132">Zheng et al. (2007)</xref>
        ,
        <xref ref-type="bibr" rid="ref104">Song et al.
(2013)</xref>
        Van Aken et al. (2017a)
        <xref ref-type="bibr" rid="ref98">Siegmund et al. (2011</xref>
        , 2012b, 2013b)
Guo et al. (2013, 2017),
        <xref ref-type="bibr" rid="ref69">Nair et al. (2017)</xref>
        ,
        <xref ref-type="bibr" rid="ref91">Sarkar et al. (2015)</xref>
        , Siegmund et al.
        <xref ref-type="bibr" rid="ref104 ref124 ref48 ref67 ref93 ref97">(2012a, 2013b)</xref>
        , Temple et al. (2017a),
        <xref ref-type="bibr" rid="ref98">Siegmund et al. (2011</xref>
        , 2012b, 2013b),
Jamshidi et al. (2017a),
        <xref ref-type="bibr" rid="ref114">Valov et al. (2017)</xref>
        ,
        <xref ref-type="bibr" rid="ref70 ref71">Nair et al. (2018)</xref>
        , Kolesnikov et al.
(2017)
        <xref ref-type="bibr" rid="ref17">Chen et al. (2005)</xref>
        <xref ref-type="bibr" rid="ref11">Bao et al. (2018)</xref>
        Ghamizi et al. (2019)
        <xref ref-type="bibr" rid="ref4">Acher et al. (2018)</xref>
        <xref ref-type="bibr" rid="ref4">Acher et al. (2018)</xref>
        <xref ref-type="bibr" rid="ref23">Duarte et al. (2018)</xref>
        Siegmund et al. (2013a)
      </p>
      <p>
        Name
Thingiverse’s
IBM WebSphere
Clasp
SNW
Binpacking
XGBoost
SaaS system
Clustering
Compressor SPL
AJStats
SaC
POLLY
Libssh
Telecom
LLVM
7Z
LRZIP
RAR
XZ
ZipMe
WordPress
LinkedList
MySQL
DNN
Curriculum vitae
Paper
RUBiS
EMAIL
Kolesnikov et al. (2017)
        <xref ref-type="bibr" rid="ref121">Westermann et al. (2012)</xref>
        <xref ref-type="bibr" rid="ref70 ref71">Nair et al. (2018)</xref>
        Name
MBED TLS
SAP ERP
noc-CM-log
sort-256
E-Health System
      </p>
      <sec id="sec-12-1">
        <title>HIPAcc</title>
        <p>
          Disparity SPL
PKJab
IBM ILOG
SPECjjbb2005
WEKA
SVD
Trimesh
MBENCH
ACE+TAO system
SensorNetwork
Helmholtz 3D
Poisson 2D
Linux kernel
DNN
Art system
Multigrid system
CoBot System
JavaGC
Robot
Hand
Indu
Rand
SAPS
SPEAR
QWS dataset
BusyBox
Plant automation
Sort
DUNE
HSMGP
Apache
Concurrency
VARD on EC2
Aircraft
ELEVATOR
WashingMachine
Violet
Encryption library
Enterprise
Application
FPGA
FPGA
Health
Image processing
Image processing
Instant messenger
Integer solver
Java Server
Learning algorithm
Linear algebra
Mesh solver
Micro benchmark
Middleware
software
Network simulator
Network simulator
Network-based
system
Numerical analysis
Numerical analysis
Operating system
Optimization
algorithm
Paint
Equations solving
Robotic system
Runtime
environment
Runtime
environment
SAT solver
SAT solver
SAT solver
SAT solver
SAT solver
Services
Software suite
Software-intensive
SoS
Sort algorithm
Stencil code
Stencil code
Stream processing
Testing problem
Text manager
Toy example
Toy example
Toy example
UML editor
(continued on next page)
et al., 2015), electricity load forecasting
          <xref ref-type="bibr" rid="ref127 ref85">(Yildiz et al., 2017)</xref>
          , and
        </p>
      </sec>
    </sec>
    <sec id="sec-13">
      <title>8. Conclusion</title>
      <p>others. Overall, these works provide us with a large dataset of
sampling, learning, and validation techniques that can be further
explored/adapted in the field of SPLs.</p>
      <p>We presented a systematic literature review related to the
use of learning techniques to analyze large configuration
software spaces. We analyzed the literature in terms of a four-stage</p>
      <p>Table C.8 (continued).</p>
      <p>
        Reference
process: "sampling, measuring, learning, and validation" (see
Section 3). Our contributions are fourfold. First, we identified the
application of each approach which can guide researchers and
industrial practitioners when searching for an appropriate
technique that fits their current needs. Second, we classified the
literature with respect to each learning stage. Mainly, we give an
in-depth view of (i) sampling techniques and employed design;
(ii) employed learning techniques; (iii) measurement properties
and effort for measurement; and (iv) how learning techniques are
empirically validated. Third, we identify the main shortcomings
of existing approaches and non-addressed research areas to be
explored by future work. Fourth, we provide a catalog of subject
systems used in the literature together with their application
domains and qualitative or quantitative properties of interest. We
welcome any contribution by the community: all the material can
be found in our supplementary Website
        <xref ref-type="bibr" rid="ref78">(Pereira et al., 2019)</xref>
        .
      </p>
      <p>Our results reveal that the research in this field is
applicationdependent: Though the overall scheme remains the same
(‘‘sampling, measuring, learning’’), the concrete choice of techniques
should trade various criterion like safety, cost, accuracy, and
interpretability. The proposed techniques have typically been
validated with respect to different metrics depending on their tasks
(e.g., performance prediction). Although the results are quite
accurate, there is still need to decrease learning errors or to
generalize predictions to multiple computing environments. Given the
increasing interest and importance of this field, there are many
exciting opportunities of research at the interplay of artificial
intelligence and software engineering.</p>
    </sec>
    <sec id="sec-14">
      <title>CRediT authorship contribution statement</title>
    </sec>
    <sec id="sec-15">
      <title>Declaration of competing interest</title>
      <p>The authors declare that they have no known competing
financial interests or personal relationships that could have appeared
to influence the work reported in this paper.</p>
      <p>Name
MOTIV
VP9
x264
OpenCV
C60 and MX300
Amazon EC2
Apache
vsftpd
ngIRCd
Video encoder
Video encoder
Video encoder
Video tracking
Virtual
environment
Web cloud service
Web server
Web system
FTP daemon
IRC daemon</p>
    </sec>
    <sec id="sec-16">
      <title>Acknowledgments</title>
      <p>This research was partially funded by the
ANR-17-CE25-001001 VaryVary project and by Science Foundation Ireland grant
13/RC/2094. We would like to thank Paul Temple for his early
comments on a draft of this article.</p>
    </sec>
    <sec id="sec-17">
      <title>Appendix A. Sampling methods</title>
      <p>Table A.6 shows the sample methods adopted by each study.
The first column is about the method and the second column
identifies the study reference(s). There are 23 high-level sample
methods documented in the literature.</p>
    </sec>
    <sec id="sec-18">
      <title>Appendix B. Learning techniques</title>
      <p>Table B.7 sketches which learning techniques are supported in
the literature and what application objective they address. The
first column identifies the study reference(s). The second and
third columns identify the name of the learning technique and
its application objective (see Section 4.1), respectively. (Notice
that the application objective is related to the scenarios in which
each learning technique has been already used in the literature; it
means some learning techniques could well be applied for other
scenarios in the future).</p>
    </sec>
    <sec id="sec-19">
      <title>Appendix C. Configurable systems</title>
      <p>Table C.8 presents all the subject systems used in the literature
together with NFPs. The first column identifies the reference(s).
The second and third columns describe the name and domain
of the system, respectively. The fourth column points out the
measured NFP(s).</p>
      <p>Juliana Alves Pereira is currently a Post-Doctoral researcher at PUC-Rio (Brazil).
She was a researcher at the University of Rennes I (Inria/Irisa, France). Juliana
received her Ph.D. degree with distinction in 2018 from the University of
Magdeburg, Germany. Her research thrives to automate software engineering
by combining methods from software analysis, machine learning, and
metaheuristic optimization. In recent years, she has published and revised research
papers in premier software engineering conferences, symposiums, and journals.
She is regularly presenting courses, tutorials, tools, and scientific results at
national and international venues.</p>
      <p>Mathieu Acher is Associate Professor at University of Rennes 1/Inria, France.
His research focuses on reverse engineering, modeling, and learning variability
of software intensive systems with different contributions published at ASE,
ESEC/FSE, SPLC, MODELS, IJCAI, or JSS, ESEM journal. He was PC co-chair of
SPLC 2017 and will be PC co-chair of VaMoS 2020. He is currently leading a
research project on machine learning and variability.12
Hugo Martin is a Ph.D. Student at the University of Rennes 1, France. His
research focuses on using interpretable machine learning to better understand
configurable systems.</p>
      <p>Dr. Jean-Marc Jzquel is a Professor at the University of Rennes and Director
of IRISA, one of the largest public research lab in Informatics in France. He
is also head of research of the French Cyber-defense Excellence Cluster and the
director of the Rennes Node of EIT Digital. In 2016, he received the Silver Medal
from CNRS. His interests include model driven software engineering for software
product lines, and specifically component based, dynamically adaptable systems
with quality of service constraints, including security, reliability, performance,
timeliness, etc. He is the author of 4 books and more than 250 publications
in international journals and conferences. He was a member of the steering
committees of the AOSD and MODELS conference series. He also served on the
editorial boards of IEEE Computer, IEEE Transactions on Software Engineering,
the Journal on Software and Systems, the Journal on Software and System
Modeling and the Journal of Object Technology. He received an engineering
degree from Telecom Bretagne in 1986, and a Ph.D. degree in Computer Science
from the University of Rennes, France, in 1989.</p>
      <p>Goetz Botterweck is a Assoc. Professor in Computer Science at Trinity College,
Dublin Ireland and with Lero – the Irish Software Research Centre. Previously,
he held positions at the University of Limerick, Ireland, as Lecturer in Computer
Science and Senior Research Fellow. His research interests are model-driven
software engineering, software evolution, and software product lines. Botterweck
received a Ph.D. in computer science from the University of Koblenz. He was PC
co-chair of SPLC 2015 and ICSR 2017.</p>
      <p>Anthony Ventresque received his Ph.D. degree in Computer Science from the
University of Nantes &amp; INRIA France in 2008. Dr Ventresque is currently an
Assistant Prof. in the School of Computer Science at University College Dublin, Ireland,
and a Funded Investigator with Lero, the SFI Irish Software Research Centre.
Previously, he held positions as Research Fellow at NTU, Singapore (20102011),
UCD, Ireland (20122014), and IBM Research Dublin, Ireland (20142015).</p>
    </sec>
  </body>
  <back>
    <ref-list>
      <ref id="ref1">
        <mixed-citation>
          <string-name>
            <surname>Acher</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Collet</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Lahire</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          , France, R.B.,
          <year>2013</year>
          .
          <article-title>Familiar: A domain-specific language for large scale management of feature models</article-title>
          .
          <source>Sci. Comput</source>
          . Program.
          <volume>78</volume>
          (
          <issue>6</issue>
          ),
          <fpage>657</fpage>
          -
          <lpage>681</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref2">
        <mixed-citation>
          <string-name>
            <surname>Acher</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Martin</surname>
            ,
            <given-names>H.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Pereira</surname>
            ,
            <given-names>J.A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Blouin</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Eddine Khelladi</surname>
            ,
            <given-names>D.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Jézéquel</surname>
            ,
            <given-names>J.- M.</given-names>
          </string-name>
          ,
          <year>2019a</year>
          .
          <article-title>Learning from Thousands of Build Failures of Linux Kernel Configurations</article-title>
          .
          <source>Technical report</source>
          , Inria; IRISA, URL https://hal.inria.fr/hal02147012.
        </mixed-citation>
      </ref>
      <ref id="ref3">
        <mixed-citation>
          <string-name>
            <surname>Acher</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Martin</surname>
            ,
            <given-names>H.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Pereira</surname>
            ,
            <given-names>J.A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Blouin</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Jézéquel</surname>
            ,
            <given-names>J.-M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Khelladi</surname>
            ,
            <given-names>D.E.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Lesoil</surname>
            ,
            <given-names>L.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Barais</surname>
            ,
            <given-names>O.</given-names>
          </string-name>
          ,
          <year>2019b</year>
          .
          <source>Learning Very Large Configuration Spaces: What Matters for Linux Kernel Sizes. Research report</source>
          , Inria Rennes - Bretagne
          <string-name>
            <surname>Atlantique</surname>
          </string-name>
          , URL https://hal.inria.fr/hal-02314830.
        </mixed-citation>
      </ref>
      <ref id="ref4">
        <mixed-citation>
          <string-name>
            <surname>Acher</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Temple</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Jezequel</surname>
            ,
            <given-names>J.-M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Galindo</surname>
            ,
            <given-names>J.A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Martinez</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Ziadi</surname>
            ,
            <given-names>T.</given-names>
          </string-name>
          ,
          <year>2018</year>
          . Varylatex:
          <article-title>Learning paper variants that meet constraints</article-title>
          .
          <source>In: Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems. ACM</source>
          , pp.
          <fpage>83</fpage>
          -
          <lpage>88</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref5">
        <mixed-citation>
          <string-name>
            <surname>Akers</surname>
            ,
            <given-names>S.B.</given-names>
          </string-name>
          ,
          <year>1978</year>
          .
          <article-title>Binary decision diagrams</article-title>
          .
          <source>IEEE Trans. Comput. (6)</source>
          ,
          <fpage>509</fpage>
          -
          <lpage>516</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref6">
        <mixed-citation>
          <string-name>
            <surname>Aleti</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Trubiani</surname>
          </string-name>
          , C.,
          <string-name>
            <surname>van Hoorn</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Jamshidi</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <year>2018</year>
          .
          <article-title>An efficient method for uncertainty propagation in robust software performance estimation</article-title>
          .
          <source>J. Syst. Softw</source>
          .
          <volume>138</volume>
          ,
          <fpage>222</fpage>
          -
          <lpage>235</lpage>
          . http://dx.doi.org/10.1016/j.jss.
          <year>2018</year>
          .
          <volume>01</volume>
          .010.
        </mixed-citation>
      </ref>
      <ref id="ref7">
        <mixed-citation>
          <string-name>
            <surname>Alipourfard</surname>
            ,
            <given-names>O.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Liu</surname>
            ,
            <given-names>H.H.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Chen</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Venkataraman</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Yu</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Zhang</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <year>2017</year>
          . Cherrypick:
          <article-title>Adaptively unearthing the best cloud configurations for big data analytics</article-title>
          .
          <source>In: 14th {USENIX} Symposium on Networked Systems Design and Implementation</source>
          ({
          <source>NSDI} 17)</source>
          , pp.
          <fpage>469</fpage>
          -
          <lpage>482</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref8">
        <mixed-citation>
          <string-name>
            <surname>Amand</surname>
            ,
            <given-names>B.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Cordy</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Heymans</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Acher</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Temple</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Jézéquel</surname>
            ,
            <given-names>J.-M.</given-names>
          </string-name>
          ,
          <year>2019</year>
          .
          <article-title>Towards learning-aided configuration in 3d printing: Feasibility study and application to defect prediction</article-title>
          .
          <source>In: Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems. ACM</source>
          , p.
          <fpage>7</fpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref9">
        <mixed-citation>
          <string-name>
            <surname>Apel</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Batory</surname>
            ,
            <given-names>D.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kästner</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Saake</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          ,
          <year>2013</year>
          . Feature-Oriented
          <source>Software Product Lines: Concepts and Implementation</source>
          . Springer-Verlag.
        </mixed-citation>
      </ref>
      <ref id="ref10">
        <mixed-citation>
          <string-name>
            <surname>Bak</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Diskin</surname>
            ,
            <given-names>Z.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Antkiewicz</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Czarnecki</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Wasowski</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <year>2016</year>
          .
          <article-title>Clafer: unifying class and feature modeling</article-title>
          .
          <source>Softw. Syst. Model</source>
          .
          <volume>15</volume>
          (
          <issue>3</issue>
          ),
          <fpage>811</fpage>
          -
          <lpage>845</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref11">
        <mixed-citation>
          <string-name>
            <surname>Bao</surname>
            ,
            <given-names>L.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Liu</surname>
            ,
            <given-names>X.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Xu</surname>
            ,
            <given-names>Z.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Fang</surname>
            ,
            <given-names>B.</given-names>
          </string-name>
          ,
          <year>2018</year>
          .
          <article-title>Autoconfig: automatic configuration tuning for distributed message systems</article-title>
          .
          <source>In: IEEE/ACM International Conference on Automated Software Engineering (ASE)</source>
          .
          <source>ACM</source>
          , pp.
          <fpage>29</fpage>
          -
          <lpage>40</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref12">
        <mixed-citation>
          <string-name>
            <surname>Benavides</surname>
            ,
            <given-names>D.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Martín-Arroyo</surname>
            ,
            <given-names>P.T.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Cortés</surname>
            ,
            <given-names>A.R.</given-names>
          </string-name>
          ,
          <source>2005. Automated reasoning on feature models. In: International Conference on Advanced Information Systems Engineering (CAiSE)</source>
          , Vol.
          <volume>5</volume>
          . Springer, pp.
          <fpage>491</fpage>
          -
          <lpage>503</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref13">
        <mixed-citation>
          <string-name>
            <surname>Benavides</surname>
            ,
            <given-names>D.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Segura</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Ruiz-Cortés</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <year>2010</year>
          .
          <article-title>Automated analysis of feature models 20 years later: a literature review</article-title>
          .
          <source>Inf. Syst</source>
          .
          <volume>35</volume>
          (
          <issue>6</issue>
          ),
          <fpage>615</fpage>
          -
          <lpage>708</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref14">
        <mixed-citation>
          <string-name>
            <surname>Bosch</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Crnkovic</surname>
            ,
            <given-names>I.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Olsson</surname>
            ,
            <given-names>H.H.</given-names>
          </string-name>
          ,
          <year>2020</year>
          .
          <article-title>Engineering ai systems: A research agenda</article-title>
          . arXiv:
          <year>2001</year>
          .07522.
        </mixed-citation>
      </ref>
      <ref id="ref15">
        <mixed-citation>
          <string-name>
            <surname>Cashman</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Cohen</surname>
            ,
            <given-names>M.B.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Ranjan</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Cottingham</surname>
            ,
            <given-names>R.W.</given-names>
          </string-name>
          ,
          <year>2018</year>
          .
          <article-title>Navigating the maze: the impact of configurability in bioinformatics software</article-title>
          .
          <source>In: IEEE/ACM International Conference on Automated Software Engineering (ASE)</source>
          . pp.
          <fpage>757</fpage>
          -
          <lpage>767</lpage>
          . http://dx.doi.org/10.1145/3238147.3240466, URL http://doi.acm.
          <source>org/10</source>
          . 1145/3238147.3240466.
        </mixed-citation>
      </ref>
      <ref id="ref16">
        <mixed-citation>
          <string-name>
            <surname>Chen</surname>
            ,
            <given-names>H.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Jiang</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          , Zhang, H.,
          <string-name>
            <surname>Yoshihira</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <year>2009</year>
          .
          <article-title>Boosting the performance of computing systems through adaptive configuration tuning</article-title>
          .
          <source>In: ACM Symposium on Applied Computing (SAC)</source>
          .
          <source>ACM</source>
          , pp.
          <fpage>1045</fpage>
          -
          <lpage>1049</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref17">
        <mixed-citation>
          <string-name>
            <surname>Chen</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          , Liu,
          <string-name>
            <given-names>Y.</given-names>
            ,
            <surname>Gorton</surname>
          </string-name>
          ,
          <string-name>
            <given-names>I.</given-names>
            ,
            <surname>Liu</surname>
          </string-name>
          ,
          <string-name>
            <surname>A.</surname>
          </string-name>
          ,
          <year>2005</year>
          .
          <article-title>Performance prediction of component-based applications</article-title>
          .
          <source>J. Syst. Softw</source>
          .
          <volume>74</volume>
          (
          <issue>1</issue>
          ),
          <fpage>35</fpage>
          -
          <lpage>43</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref18">
        <mixed-citation>
          <string-name>
            <surname>Colmant</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Rouvoy</surname>
            ,
            <given-names>R.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kurpicz</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Sobe</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Felber</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Seinturier</surname>
            ,
            <given-names>L.</given-names>
          </string-name>
          ,
          <year>2018</year>
          .
          <article-title>The next 700 CPU power models</article-title>
          .
          <source>J. Syst. Softw</source>
          .
          <volume>144</volume>
          ,
          <fpage>382</fpage>
          -
          <lpage>396</lpage>
          . http://dx.doi.org/ 10.1016/j.jss.
          <year>2018</year>
          .
          <volume>07</volume>
          .001.
        </mixed-citation>
      </ref>
      <ref id="ref19">
        <mixed-citation>
          <string-name>
            <surname>Couto</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Borba</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Cunha</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Fernandes</surname>
            ,
            <given-names>J.P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Pereira</surname>
            ,
            <given-names>R.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Saraiva</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <year>2017</year>
          .
          <article-title>Products go green: Worst-case energy consumption in software product lines</article-title>
          .
          <source>In: Proceedings of the 21st International Systems and Software Product Line Conference-Volume A. ACM</source>
          , pp.
          <fpage>84</fpage>
          -
          <lpage>93</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref20">
        <mixed-citation>
          <string-name>
            <surname>Crawford</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Khoshgoftaar</surname>
            ,
            <given-names>T.M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Prusa</surname>
            ,
            <given-names>J.D.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Richter</surname>
            ,
            <given-names>A.N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Al Najada</surname>
            ,
            <given-names>H.</given-names>
          </string-name>
          ,
          <year>2015</year>
          .
          <article-title>Survey of review spam detection using machine learning techniques</article-title>
          .
          <source>J. Big Data</source>
          <volume>2</volume>
          (
          <issue>1</issue>
          ),
          <fpage>23</fpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref21">
        <mixed-citation>
          <string-name>
            <surname>Ding</surname>
            ,
            <given-names>Y.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Ansel</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Veeramachaneni</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Shen</surname>
            ,
            <given-names>X.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>O'Reilly</surname>
          </string-name>
          , U.-M.,
          <string-name>
            <surname>Amarasinghe</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <year>2015</year>
          .
          <article-title>Autotuning algorithmic choice for input sensitivity</article-title>
          .
          <source>In: ACM SIGPLAN Notices</source>
          , Vol.
          <volume>50</volume>
          . ACM, pp.
          <fpage>379</fpage>
          -
          <lpage>390</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref22">
        <mixed-citation>
          <string-name>
            <surname>do Carmo Machado</surname>
            ,
            <given-names>I.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Mcgregor</surname>
            ,
            <given-names>J.D.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Cavalcanti</surname>
            ,
            <given-names>Y.C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>De Almeida</surname>
            ,
            <given-names>E.S.</given-names>
          </string-name>
          ,
          <year>2014</year>
          .
          <article-title>On strategies for testing software product lines: A systematic literature review</article-title>
          .
          <source>Inf. Softw. Technol</source>
          .
          <volume>56</volume>
          (
          <issue>10</issue>
          ),
          <fpage>1183</fpage>
          -
          <lpage>1199</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref23">
        <mixed-citation>
          <string-name>
            <surname>Duarte</surname>
            ,
            <given-names>F.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Gil</surname>
            ,
            <given-names>R.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Romano</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Lopes</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Rodrigues</surname>
            ,
            <given-names>L.</given-names>
          </string-name>
          ,
          <year>2018</year>
          .
          <article-title>Learning non-deterministic impact models for adaptation</article-title>
          .
          <source>In: Proceedings of the 13th International Conference on Software Engineering for Adaptive and Self-Managing Systems. ACM</source>
          , pp.
          <fpage>196</fpage>
          -
          <lpage>205</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref24">
        <mixed-citation>
          <string-name>
            <surname>Eichelberger</surname>
            ,
            <given-names>H.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Qin</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Sizonenko</surname>
            ,
            <given-names>R.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Schmid</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <year>2016</year>
          .
          <article-title>Using ivml to model the topology of big data processing pipelines</article-title>
          .
          <source>In: International Systems and Software Product Line Conference (SPLC)</source>
          .
          <source>ACM</source>
          , pp.
          <fpage>204</fpage>
          -
          <lpage>208</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref25">
        <mixed-citation>
          <string-name>
            <given-names>El</given-names>
            <surname>Afia</surname>
          </string-name>
          ,
          <string-name>
            <given-names>A.</given-names>
            ,
            <surname>Sarhani</surname>
          </string-name>
          ,
          <string-name>
            <surname>M.</surname>
          </string-name>
          ,
          <year>2017</year>
          .
          <article-title>Performance prediction using support vector machine for the configuration of optimization algorithms</article-title>
          .
          <source>In: 2017 3rd International Conference of Cloud Computing Technologies and Applications (CloudTech)</source>
          . IEEE, pp.
          <fpage>1</fpage>
          -
          <lpage>7</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref26">
        <mixed-citation>
          <string-name>
            <surname>Etxeberria</surname>
            ,
            <given-names>L.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Trubiani</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Cortellessa</surname>
            ,
            <given-names>V.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Sagardui</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          ,
          <year>2014</year>
          .
          <article-title>Performance-based selection of software and hardware features under parameter uncertainty</article-title>
          .
          <source>In: Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures. ACM</source>
          , pp.
          <fpage>23</fpage>
          -
          <lpage>32</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref27">
        <mixed-citation>
          <string-name>
            <surname>Gargantini</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Petke</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Radavelli</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <year>2017</year>
          .
          <article-title>Combinatorial interaction testing for automated constraint repair</article-title>
          .
          <source>In: 2017 IEEE International Conference on Software Testing</source>
          ,
          <article-title>Verification and Validation Workshops (ICSTW)</article-title>
          . IEEE, pp.
          <fpage>239</fpage>
          -
          <lpage>248</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref28">
        <mixed-citation>
          <string-name>
            <surname>Grebhahn</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Rodrigo</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Siegmund</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Gaspar</surname>
            ,
            <given-names>F.J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Apel</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <year>2017</year>
          .
          <article-title>Performanceinfluence models of multigrid methods: A case study on triangular grids</article-title>
          .
          <source>Concurr. Comput.: Pract. Exper</source>
          .
          <volume>29</volume>
          (
          <issue>17</issue>
          ),
          <year>e4057</year>
          .
        </mixed-citation>
      </ref>
      <ref id="ref29">
        <mixed-citation>
          <string-name>
            <surname>Grebhahn</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Siegmund</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Apel</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <year>2019</year>
          .
          <article-title>Predicting performance of software configurations: There is no silver bullet</article-title>
          . arXiv:
          <year>1911</year>
          .12643.
        </mixed-citation>
      </ref>
      <ref id="ref30">
        <mixed-citation>
          <string-name>
            <surname>Guo</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Czarnecki</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Apel</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Siegmund</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Wasowski</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <year>2013</year>
          .
          <article-title>Variabilityaware performance prediction: A statistical learning approach</article-title>
          . In: ASE.
        </mixed-citation>
      </ref>
      <ref id="ref31">
        <mixed-citation>
          <string-name>
            <surname>Guo</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>White</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Wang</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Li</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Wang</surname>
            ,
            <given-names>Y.</given-names>
          </string-name>
          ,
          <year>2011</year>
          .
          <article-title>A genetic algorithm for optimized feature selection with resource constraints in software product lines</article-title>
          .
          <source>J. Syst. Softw</source>
          .
          <volume>84</volume>
          (
          <issue>12</issue>
          ),
          <fpage>2208</fpage>
          -
          <lpage>2221</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref32">
        <mixed-citation>
          <string-name>
            <surname>Guo</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Yang</surname>
            ,
            <given-names>D.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Siegmund</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Apel</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Sarkar</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Valov</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Czarnecki</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Wasowski</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Yu</surname>
            ,
            <given-names>H.</given-names>
          </string-name>
          ,
          <year>2017</year>
          .
          <article-title>Data-efficient performance learning for configurable systems</article-title>
          .
          <source>Empir. Softw. Eng</source>
          .
          <volume>1</volume>
          -
          <fpage>42</fpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref33">
        <mixed-citation>
          <string-name>
            <surname>Guzella</surname>
            ,
            <given-names>T.S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Caminhas</surname>
            ,
            <given-names>W.M.</given-names>
          </string-name>
          ,
          <year>2009</year>
          .
          <article-title>A review of machine learning approaches to spam filtering</article-title>
          .
          <source>Expert Syst. Appl</source>
          .
          <volume>36</volume>
          (
          <issue>7</issue>
          ),
          <fpage>10206</fpage>
          -
          <lpage>10222</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref34">
        <mixed-citation>
          <string-name>
            <surname>Halin</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Nuttinck</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Acher</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Devroey</surname>
            ,
            <given-names>X.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Perrouin</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Baudry</surname>
            ,
            <given-names>B.</given-names>
          </string-name>
          ,
          <year>2019</year>
          .
          <article-title>Test them all, is it worth it? assessing configuration sampling on the jhipster web development stack</article-title>
          .
          <source>Empir</source>
          . Softw. Eng. http://dx.doi.org/10. 1007/s10664-018-9635-4.
        </mixed-citation>
      </ref>
      <ref id="ref35">
        <mixed-citation>
          <string-name>
            <surname>Hall</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Frank</surname>
            ,
            <given-names>E.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Holmes</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Pfahringer</surname>
            ,
            <given-names>B.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Reutemann</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Witten</surname>
            ,
            <given-names>I.H.</given-names>
          </string-name>
          ,
          <year>2009</year>
          .
          <article-title>The weka data mining software: an update</article-title>
          .
          <source>ACM SIGKDD Explor. Newsl</source>
          .
          <volume>11</volume>
          (
          <issue>1</issue>
          ),
          <fpage>10</fpage>
          -
          <lpage>18</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref36">
        <mixed-citation>
          <string-name>
            <surname>Hallsteinsen</surname>
            ,
            <given-names>S.O.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Hinchey</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Park</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Schmid</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <year>2008</year>
          .
          <article-title>Dynamic software product lines</article-title>
          .
          <source>IEEE Comput</source>
          .
          <volume>41</volume>
          (
          <issue>4</issue>
          ),
          <fpage>93</fpage>
          -
          <lpage>95</lpage>
          . http://dx.doi.org/10.1109/
          <string-name>
            <surname>MC</surname>
          </string-name>
          .
          <year>2008</year>
          .
          <volume>123</volume>
          .
        </mixed-citation>
      </ref>
      <ref id="ref37">
        <mixed-citation>
          <string-name>
            <surname>Harman</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Jia</surname>
            ,
            <given-names>Y.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Krinke</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Langdon</surname>
            ,
            <given-names>W.B.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Petke</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          , Zhang,
          <string-name>
            <surname>Y.</surname>
          </string-name>
          ,
          <year>2014</year>
          .
          <article-title>Search based software engineering for software product line engineering: a survey and directions for future work</article-title>
          .
          <source>In: Proceedings of the 18th International Software Product Line Conference-Volume 1. ACM</source>
          , pp.
          <fpage>5</fpage>
          -
          <lpage>18</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref38">
        <mixed-citation>
          <string-name>
            <surname>Heradio</surname>
            ,
            <given-names>R.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Perez-Morago</surname>
            ,
            <given-names>H.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Fernandez-Amoros</surname>
            ,
            <given-names>D.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Cabrerizo</surname>
            ,
            <given-names>F.J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>HerreraViedma</surname>
          </string-name>
          , E.,
          <year>2016</year>
          .
          <article-title>A bibliometric analysis of 20 years of research on software product lines</article-title>
          .
          <source>Inf. Softw. Technol</source>
          .
          <volume>72</volume>
          ,
          <fpage>1</fpage>
          -
          <lpage>15</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref39">
        <mixed-citation>
          <string-name>
            <surname>Hoda</surname>
            ,
            <given-names>R.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Salleh</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Grundy</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Tee</surname>
            ,
            <given-names>H.M.</given-names>
          </string-name>
          ,
          <year>2017</year>
          .
          <article-title>Systematic literature reviews in agile software development: A tertiary study</article-title>
          .
          <source>Inf. Softw. Technol</source>
          .
          <volume>85</volume>
          ,
          <fpage>60</fpage>
          -
          <lpage>70</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref40">
        <mixed-citation>
          <string-name>
            <surname>Hutter</surname>
            ,
            <given-names>F.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Hoos</surname>
            ,
            <given-names>H.H.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Leyton-Brown</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <year>2011</year>
          .
          <article-title>Sequential model-based optimization for general algorithm configuration</article-title>
          .
          <source>In: International Conference on Learning and Intelligent Optimization</source>
          . Springer, pp.
          <fpage>507</fpage>
          -
          <lpage>523</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref41">
        <mixed-citation>
          <string-name>
            <surname>Jamshidi</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Casale</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          ,
          <year>2016</year>
          .
          <article-title>An uncertainty-aware approach to optimal configuration of stream processing systems</article-title>
          .
          <source>In: 2016 IEEE 24th International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS)</source>
          . IEEE, pp.
          <fpage>39</fpage>
          -
          <lpage>48</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref42">
        <mixed-citation>
          <string-name>
            <surname>Jamshidi</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Siegmund</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Velez</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kästner</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Patel</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Agarwal</surname>
            ,
            <given-names>Y.</given-names>
          </string-name>
          ,
          <year>2017a</year>
          .
          <article-title>Transfer learning for performance modeling of configurable systems: an exploratory analysis</article-title>
          .
          <source>In: IEEE/ACM International Conference on Automated Software Engineering (ASE)</source>
          . IEEE Press, pp.
          <fpage>497</fpage>
          -
          <lpage>508</lpage>
          , URL http://dl.acm.org/ citation.cfm?id=
          <fpage>3155625</fpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref43">
        <mixed-citation>
          <string-name>
            <surname>Jamshidi</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Velez</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kästner</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Siegmund</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <year>2018</year>
          .
          <article-title>Learning to sample: exploiting similarities across environments to learn performance models for configurable systems</article-title>
          .
          <source>In: Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ACM</source>
          , pp.
          <fpage>71</fpage>
          -
          <lpage>82</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref44">
        <mixed-citation>
          <string-name>
            <surname>Jamshidi</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Velez</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kästner</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Siegmund</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kawthekar</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          , 2017b.
          <article-title>Transfer learning for improving model predictions in highly configurable software</article-title>
          .
          <source>In: 12th IEEE/ACM International Symposium on Software Engineering for Adaptive</source>
          and
          <string-name>
            <surname>Self-Managing</surname>
            <given-names>Systems</given-names>
          </string-name>
          ,
          <source>SEAMS@ICSE</source>
          <year>2017</year>
          ,
          <string-name>
            <given-names>Buenos</given-names>
            <surname>Aires</surname>
          </string-name>
          , Argentina, May
          <volume>22</volume>
          -23,
          <year>2017</year>
          . pp.
          <fpage>31</fpage>
          -
          <lpage>41</lpage>
          . http://dx.doi.org/10.1109/SEAMS.
          <year>2017</year>
          .
          <volume>11</volume>
          .
        </mixed-citation>
      </ref>
      <ref id="ref45">
        <mixed-citation>
          <string-name>
            <surname>Jiménez</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>De La Rosa</surname>
            ,
            <given-names>T.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Fernández</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Fernández</surname>
            ,
            <given-names>F.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Borrajo</surname>
            ,
            <given-names>D.</given-names>
          </string-name>
          ,
          <year>2012</year>
          .
          <article-title>A review of machine learning for automated planning</article-title>
          .
          <source>Knowl. Eng. Rev</source>
          .
          <volume>27</volume>
          (
          <issue>4</issue>
          ),
          <fpage>433</fpage>
          -
          <lpage>467</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref46">
        <mixed-citation>
          <string-name>
            <surname>Kaltenecker</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Grebhahn</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Siegmund</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Guo</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Apel</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <year>2019</year>
          .
          <article-title>Distance-based sampling of software configuration spaces</article-title>
          .
          <source>In: Proceedings of the IEEE/ACM International Conference on Software Engineering (ICSE)</source>
          .
          <source>ACM.</source>
        </mixed-citation>
      </ref>
      <ref id="ref47">
        <mixed-citation>
          <string-name>
            <surname>Kang</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Cohen</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Hess</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Novak</surname>
            ,
            <given-names>W.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Peterson</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <year>1990</year>
          .
          <article-title>Feature-Oriented Domain Analysis (FODA)</article-title>
          .
          <source>Tech. Rep</source>
          . CMU/SEI-90
          <string-name>
            <surname>-</surname>
          </string-name>
          TR-21, SEI.
        </mixed-citation>
      </ref>
      <ref id="ref48">
        <mixed-citation>
          <string-name>
            <given-names>Khalil</given-names>
            <surname>Abbasi</surname>
          </string-name>
          ,
          <string-name>
            <given-names>E.</given-names>
            ,
            <surname>Hubaux</surname>
          </string-name>
          ,
          <string-name>
            <given-names>A.</given-names>
            ,
            <surname>Acher</surname>
          </string-name>
          ,
          <string-name>
            <given-names>M.</given-names>
            ,
            <surname>Boucher</surname>
          </string-name>
          ,
          <string-name>
            <given-names>Q.</given-names>
            ,
            <surname>Heymans</surname>
          </string-name>
          ,
          <string-name>
            <surname>P.</surname>
          </string-name>
          ,
          <year>2013</year>
          .
          <article-title>The anatomy of a sales configurator: An empirical study of 111 cases</article-title>
          . In: CAiSE'
          <fpage>13</fpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref49">
        <mixed-citation>
          <string-name>
            <surname>Khan</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Baharudin</surname>
            ,
            <given-names>B.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Lee</surname>
            ,
            <given-names>L.H.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Khan</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <year>2010</year>
          .
          <article-title>A review of machine learning algorithms for text-documents classification</article-title>
          .
          <source>J. Adv. Inf. Technol</source>
          .
          <volume>1</volume>
          (
          <issue>1</issue>
          ),
          <fpage>4</fpage>
          -
          <lpage>20</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref50">
        <mixed-citation>
          <string-name>
            <surname>Kitchenham</surname>
            ,
            <given-names>B.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Brereton</surname>
            ,
            <given-names>O.P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Budgen</surname>
            ,
            <given-names>D.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Turner</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Bailey</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Linkman</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <year>2009</year>
          .
          <article-title>Systematic literature reviews in software engineering-a systematic literature review</article-title>
          .
          <source>Inf. Softw. Technol</source>
          .
          <volume>51</volume>
          (
          <issue>1</issue>
          ),
          <fpage>7</fpage>
          -
          <lpage>15</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref51">
        <mixed-citation>
          <string-name>
            <surname>Kitchenham</surname>
            ,
            <given-names>B.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Charters</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <year>2007</year>
          .
          <article-title>Guidelines for Performing Systematic Literature Reviews in Software Engineering</article-title>
          . Citeseer.
        </mixed-citation>
      </ref>
      <ref id="ref52">
        <mixed-citation>
          <string-name>
            <surname>Kolesnikov</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Siegmund</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kästner</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Grebhahn</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Apel</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <year>2018</year>
          .
          <article-title>Tradeoffs in modeling performance of highly configurable software systems</article-title>
          .
          <source>Softw. Syst. Model</source>
          .
          <volume>1</volume>
          -
          <fpage>19</fpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref53">
        <mixed-citation>
          <string-name>
            <surname>Krismayer</surname>
            ,
            <given-names>T.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Rabiser</surname>
            ,
            <given-names>R.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Grünbacher</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <year>2017</year>
          .
          <article-title>Mining constraints for eventbased monitoring in systems of systems</article-title>
          .
          <source>In: IEEE/ACM International Conference on Automated Software Engineering (ASE)</source>
          . IEEE Press, pp.
          <fpage>826</fpage>
          -
          <lpage>831</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref54">
        <mixed-citation>
          <string-name>
            <surname>Lee</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kang</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Lee</surname>
            ,
            <given-names>D.</given-names>
          </string-name>
          ,
          <year>2012</year>
          .
          <article-title>A survey on software product line testing</article-title>
          .
          <source>In: Proceedings of the 16th International Software Product Line Conference-Volume 1. ACM</source>
          , pp.
          <fpage>31</fpage>
          -
          <lpage>40</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref55">
        <mixed-citation>
          <string-name>
            <surname>Leitner</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Cito</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <year>2016</year>
          .
          <article-title>Patterns in the chaos - A study of performance variation and predictability in public iaas clouds</article-title>
          .
          <source>ACM Trans. Internet Tech</source>
          .
          <volume>16</volume>
          (
          <issue>3</issue>
          ),
          <volume>15</volume>
          :
          <fpage>1</fpage>
          -
          <lpage>15</lpage>
          :
          <fpage>23</fpage>
          . http://dx.doi.org/10.1145/2885497.
        </mixed-citation>
      </ref>
      <ref id="ref56">
        <mixed-citation>
          <string-name>
            <surname>Leung</surname>
            ,
            <given-names>M.K.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Delong</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Alipanahi</surname>
            ,
            <given-names>B.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Frey</surname>
            ,
            <given-names>B.J.</given-names>
          </string-name>
          ,
          <year>2015</year>
          .
          <article-title>Machine learning in genomic medicine: a review of computational problems and data sets</article-title>
          .
          <source>Proc. IEEE</source>
          <volume>104</volume>
          (
          <issue>1</issue>
          ),
          <fpage>176</fpage>
          -
          <lpage>197</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref57">
        <mixed-citation>
          <string-name>
            <surname>Lillack</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Müller</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Eisenecker</surname>
          </string-name>
          , U.W.,
          <year>2013</year>
          .
          <article-title>Improved prediction of nonfunctional properties in software product lines with domain context</article-title>
          .
          <source>Softw</source>
          . Eng..
        </mixed-citation>
      </ref>
      <ref id="ref58">
        <mixed-citation>
          <string-name>
            <surname>Lisboa</surname>
            ,
            <given-names>L.B.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Garcia</surname>
            ,
            <given-names>V.C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Lucrédio</surname>
          </string-name>
          , D.,
          <string-name>
            <surname>de Almeida</surname>
            , E.S., de Lemos Meira,
            <given-names>S.R.</given-names>
          </string-name>
          , de Mattos Fortes,
          <string-name>
            <surname>R.P.</surname>
          </string-name>
          ,
          <year>2010</year>
          .
          <article-title>A systematic review of domain analysis tools</article-title>
          .
          <source>Inf. Softw. Technol</source>
          .
          <volume>52</volume>
          (
          <issue>1</issue>
          ),
          <fpage>1</fpage>
          -
          <lpage>13</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref59">
        <mixed-citation>
          <string-name>
            <surname>Lopez-Herrejon</surname>
            ,
            <given-names>R.E.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Fischer</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Ramler</surname>
            ,
            <given-names>R.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Egyed</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <year>2015</year>
          .
          <article-title>A first systematic mapping study on combinatorial interaction testing for software product lines</article-title>
          . In: 2015 IEEE Eighth International Conference on Software Testing,
          <article-title>Verification and Validation Workshops (ICSTW)</article-title>
          . IEEE, pp.
          <fpage>1</fpage>
          -
          <lpage>10</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref60">
        <mixed-citation>
          <string-name>
            <surname>Malhotra</surname>
            ,
            <given-names>R.</given-names>
          </string-name>
          ,
          <year>2015</year>
          .
          <article-title>A systematic review of machine learning techniques for software fault prediction</article-title>
          .
          <source>Appl. Soft Comput</source>
          .
          <volume>27</volume>
          ,
          <fpage>504</fpage>
          -
          <lpage>518</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref61">
        <mixed-citation>
          <string-name>
            <surname>Martinez</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Sottet</surname>
            ,
            <given-names>J.-S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Frey</surname>
            ,
            <given-names>A.G.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Bissyandé</surname>
            ,
            <given-names>T.F.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Ziadi</surname>
            ,
            <given-names>T.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Klein</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Temple</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Acher</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <given-names>Le</given-names>
            <surname>Traon</surname>
          </string-name>
          ,
          <string-name>
            <surname>Y.</surname>
          </string-name>
          ,
          <year>2018</year>
          .
          <article-title>Towards estimating and predicting user perception on software product variants</article-title>
          .
          <source>In: International Conference on Software Reuse</source>
          . Springer, pp.
          <fpage>23</fpage>
          -
          <lpage>40</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref62">
        <mixed-citation>
          <string-name>
            <surname>Mathur</surname>
            ,
            <given-names>A.P.</given-names>
          </string-name>
          ,
          <year>2008</year>
          .
          <source>Foundations of Software Testing. Pearson Education</source>
          , India.
        </mixed-citation>
      </ref>
      <ref id="ref63">
        <mixed-citation>
          <string-name>
            <surname>Medeiros</surname>
            ,
            <given-names>F.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kästner</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Ribeiro</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Gheyi</surname>
            ,
            <given-names>R.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Apel</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <year>2016</year>
          .
          <article-title>A comparison of 10 sampling algorithms for configurable systems</article-title>
          .
          <source>In: Proceedings of the 38th International Conference on Software Engineering. ACM</source>
          , pp.
          <fpage>643</fpage>
          -
          <lpage>654</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref64">
        <mixed-citation>
          <string-name>
            <surname>Mendonca</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Wasowski</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Czarnecki</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Cowan</surname>
            ,
            <given-names>D.</given-names>
          </string-name>
          ,
          <year>2008</year>
          .
          <article-title>Efficient compilation techniques for large scale feature models</article-title>
          .
          <source>In: Proceedings of the 7th International Conference on Generative Programming and Component Engineering. ACM</source>
          , pp.
          <fpage>13</fpage>
          -
          <lpage>22</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref65">
        <mixed-citation>
          <string-name>
            <surname>Molnar</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <year>2019</year>
          .
          <article-title>Interpretable machine learning</article-title>
          . https://christophm.github.io/ interpretable-ml-book/.
        </mixed-citation>
      </ref>
      <ref id="ref66">
        <mixed-citation>
          <string-name>
            <surname>Morin</surname>
            ,
            <given-names>B.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Barais</surname>
            ,
            <given-names>O.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Jézéquel</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Fleurey</surname>
            ,
            <given-names>F.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Solberg</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <year>2009</year>
          .
          <article-title>Models@ run.time to support dynamic adaptation</article-title>
          .
          <source>IEEE Comput</source>
          .
          <volume>42</volume>
          (
          <issue>10</issue>
          ),
          <fpage>44</fpage>
          -
          <lpage>51</lpage>
          . http://dx.doi. org/10.1109/
          <string-name>
            <surname>MC</surname>
          </string-name>
          .
          <year>2009</year>
          .
          <volume>327</volume>
          .
        </mixed-citation>
      </ref>
      <ref id="ref67">
        <mixed-citation>
          <string-name>
            <surname>Murashkin</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Antkiewicz</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Rayside</surname>
            ,
            <given-names>D.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Czarnecki</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <year>2013</year>
          .
          <article-title>Visualization and exploration of optimal variants in product line engineering</article-title>
          .
          <source>In: Proceedings of the 17th International Software Product Line Conference. ACM</source>
          , pp.
          <fpage>111</fpage>
          -
          <lpage>115</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref68">
        <mixed-citation>
          <string-name>
            <surname>Murwantara</surname>
            ,
            <given-names>I.M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Bordbar</surname>
            ,
            <given-names>B.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Minku</surname>
            ,
            <given-names>L.L.</given-names>
          </string-name>
          ,
          <year>2014</year>
          .
          <article-title>Measuring energy consumption for web service product configuration</article-title>
          .
          <source>In: Proceedings of the 16th International Conference on Information Integration and Web-Based Applications &amp;#38; Services</source>
          , IiWAS '
          <fpage>14</fpage>
          .
          <string-name>
            <surname>ACM</surname>
          </string-name>
          , New York, NY, USA, pp.
          <fpage>224</fpage>
          -
          <lpage>228</lpage>
          . http://dx. doi.org/10.1145/2684200.2684314, URL http://doi.acm.
          <source>org/10</source>
          .1145/2684200. 2684314.
        </mixed-citation>
      </ref>
      <ref id="ref69">
        <mixed-citation>
          <string-name>
            <surname>Nair</surname>
            ,
            <given-names>V.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Menzies</surname>
            ,
            <given-names>T.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Siegmund</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Apel</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <year>2017</year>
          .
          <article-title>Using bad learners to find good configurations</article-title>
          .
          <source>In: Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering</source>
          , ESEC/FSE 2017, Paderborn, Germany, September 4-
          <issue>8</issue>
          ,
          <year>2017</year>
          . pp.
          <fpage>257</fpage>
          -
          <lpage>267</lpage>
          . http://dx.doi.org/10.1145/ 3106237.3106238, URL http://doi.acm.
          <source>org/10</source>
          .1145/3106237.3106238.
        </mixed-citation>
      </ref>
      <ref id="ref70">
        <mixed-citation>
          <string-name>
            <surname>Nair</surname>
            ,
            <given-names>V.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Menzies</surname>
            ,
            <given-names>T.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Siegmund</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Apel</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <year>2018</year>
          .
          <article-title>Faster discovery of faster system configurations with spectral learning</article-title>
          .
          <source>Autom. Softw. Eng</source>
          .
          <volume>1</volume>
          -
          <fpage>31</fpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref71">
        <mixed-citation>
          <string-name>
            <surname>Nair</surname>
            ,
            <given-names>V.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Yu</surname>
            ,
            <given-names>Z.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Menzies</surname>
            ,
            <given-names>T.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Siegmund</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Apel</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <year>2018</year>
          .
          <article-title>Finding faster configurations using flash</article-title>
          .
          <source>IEEE Trans. Softw</source>
          . Eng.
        </mixed-citation>
      </ref>
      <ref id="ref72">
        <mixed-citation>
          <string-name>
            <surname>Ochoa</surname>
            ,
            <given-names>L.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Gonzalez-Rojas</surname>
            ,
            <given-names>O.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Juliana</surname>
            ,
            <given-names>A.P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Castro</surname>
            ,
            <given-names>H.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Saake</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          ,
          <year>2018</year>
          .
          <article-title>A systematic literature review on the semi-automatic configuration of extended product lines</article-title>
          .
          <source>J. Syst. Softw</source>
          .
          <volume>144</volume>
          ,
          <fpage>511</fpage>
          -
          <lpage>532</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref73">
        <mixed-citation>
          <string-name>
            <surname>Ochoa</surname>
            ,
            <given-names>L.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>González-Rojas</surname>
            ,
            <given-names>O.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Thüm</surname>
            ,
            <given-names>T.</given-names>
          </string-name>
          ,
          <year>2015</year>
          .
          <article-title>Using decision rules for solving conflicts in extended feature models</article-title>
          .
          <source>In: International Conference on Software Language Engineering (SLE)</source>
          .
          <source>ACM</source>
          , pp.
          <fpage>149</fpage>
          -
          <lpage>160</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref74">
        <mixed-citation>
          <string-name>
            <surname>Oh</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Batory</surname>
            ,
            <given-names>D.S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Myers</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Siegmund</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <year>2017</year>
          .
          <article-title>Finding near-optimal configurations in product lines by random sampling</article-title>
          .
          <source>In: Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering</source>
          , ESEC/FSE 2017, Paderborn, Germany, September 4-
          <issue>8</issue>
          ,
          <year>2017</year>
          . pp.
          <fpage>61</fpage>
          -
          <lpage>71</lpage>
          . http://dx.doi.org/10. 1145/3106237.3106273, URL http://doi.acm.
          <source>org/10</source>
          .1145/3106237.3106273.
        </mixed-citation>
      </ref>
      <ref id="ref75">
        <mixed-citation>
          <string-name>
            <surname>Osogami</surname>
            ,
            <given-names>T.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kato</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <year>2007</year>
          .
          <article-title>Optimizing system configurations quickly by guessing at the performance</article-title>
          .
          <source>In: ACM SIGMETRICS Performance Evaluation Review</source>
          , Vol.
          <volume>35</volume>
          . ACM, pp.
          <fpage>145</fpage>
          -
          <lpage>156</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref76">
        <mixed-citation>
          <string-name>
            <surname>Pereira</surname>
            ,
            <given-names>J.A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Acher</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Martin</surname>
            ,
            <given-names>H.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Jézéquel</surname>
            ,
            <given-names>J.-M.</given-names>
          </string-name>
          ,
          <year>2020</year>
          .
          <article-title>Sampling Effect on Performance Prediction of Configurable Systems: A Case Study</article-title>
          . URL https: //hal.inria.fr/hal-02356290.
        </mixed-citation>
      </ref>
      <ref id="ref77">
        <mixed-citation>
          <string-name>
            <surname>Pereira</surname>
            ,
            <given-names>J.A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Constantino</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Figueiredo</surname>
            ,
            <given-names>E.</given-names>
          </string-name>
          ,
          <year>2015</year>
          .
          <article-title>A systematic literature review of software product line management tools</article-title>
          .
          <source>In: International Conference on Software Reuse (ICSR)</source>
          . Springer, pp.
          <fpage>73</fpage>
          -
          <lpage>89</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref78">
        <mixed-citation>
          <string-name>
            <surname>Pereira</surname>
            ,
            <given-names>J.A.</given-names>
          </string-name>
          ,
          <string-name>
            <given-names>M.</given-names>
            ,
            <surname>Acher</surname>
          </string-name>
          ,
          <string-name>
            <given-names>H.</given-names>
            ,
            <surname>Martin</surname>
          </string-name>
          ,
          <string-name>
            <given-names>J.</given-names>
            ,
            <surname>Jézéquel</surname>
          </string-name>
          ,
          <string-name>
            <given-names>G.</given-names>
            ,
            <surname>Botterweck</surname>
          </string-name>
          ,
          <string-name>
            <given-names>A.</given-names>
            ,
            <surname>Ventresque</surname>
          </string-name>
          ,
          <year>2019</year>
          .
          <article-title>Learning software configuration spaces: A systematic literature review</article-title>
          . URL https://github.com/VaryVary/ML-configurable-SLR.
          <source>(Accessed 04 June</source>
          <year>2019</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref79">
        <mixed-citation>
          <string-name>
            <surname>Pereira</surname>
            ,
            <given-names>J.A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Matuszyk</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Krieter</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Spiliopoulou</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Saake</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          ,
          <year>2018</year>
          .
          <article-title>Personalized recommender systems for product-line configuration processes</article-title>
          .
          <source>Comput. Lang. Syst. Struct..</source>
        </mixed-citation>
      </ref>
      <ref id="ref80">
        <mixed-citation>
          <string-name>
            <surname>Pereira</surname>
            ,
            <given-names>J.A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Schulze</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Figueiredo</surname>
            ,
            <given-names>E.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Saake</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          ,
          <year>2018</year>
          .
          <article-title>N-dimensional tensor factorization for self-configuration of software product lines at runtime</article-title>
          .
          <source>In: Proceeedings of the 22nd International Conference on Systems and Software Product Line-Volume 1. ACM</source>
          , pp.
          <fpage>87</fpage>
          -
          <lpage>97</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref81">
        <mixed-citation>
          <string-name>
            <surname>Plazar</surname>
            ,
            <given-names>Q.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Acher</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Perrouin</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Devroey</surname>
            ,
            <given-names>X.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Cordy</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <year>2019</year>
          .
          <article-title>Uniform sampling of sat solutions for configurable systems: Are we there yet?</article-title>
          <source>In: ICST 2019-12th International Conference on Software Testing</source>
          , Verification, and
          <string-name>
            <surname>Validation</surname>
          </string-name>
          , Xian, China. pp.
          <fpage>1</fpage>
          -
          <lpage>12</lpage>
          , URL https://hal.inria.fr/hal-01991857.
        </mixed-citation>
      </ref>
      <ref id="ref82">
        <mixed-citation>
          <string-name>
            <surname>Pohl</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Böckle</surname>
          </string-name>
          , G.,
          <string-name>
            <surname>van der Linden</surname>
            ,
            <given-names>F.J.</given-names>
          </string-name>
          ,
          <year>2005</year>
          . Software Product Line Engineering: Foundations, Principles and Techniques. Springer, Berlin Heidelberg.
        </mixed-citation>
      </ref>
      <ref id="ref83">
        <mixed-citation>
          <string-name>
            <surname>Pohl</surname>
            ,
            <given-names>R.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Lauenroth</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Pohl</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <year>2011</year>
          .
          <article-title>A performance comparison of contemporary algorithmic approaches for automated analysis operations on feature models</article-title>
          .
          <source>In: IEEE/ACM International Conference on Automated Software Engineering (ASE)</source>
          .
          <source>IEEE Computer Society</source>
          , pp.
          <fpage>313</fpage>
          -
          <lpage>322</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref84">
        <mixed-citation>
          <string-name>
            <surname>Porter</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Yilmaz</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Memon</surname>
            ,
            <given-names>A.M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Schmidt</surname>
            ,
            <given-names>D.C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Natarajan</surname>
            ,
            <given-names>B.</given-names>
          </string-name>
          ,
          <year>2007</year>
          .
          <article-title>Skoll: A process and infrastructure for distributed continuous quality assurance</article-title>
          .
          <source>IEEE Trans. Softw. Eng</source>
          .
          <volume>33</volume>
          (
          <issue>8</issue>
          ),
          <fpage>510</fpage>
          -
          <lpage>525</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref85">
        <mixed-citation>
          <string-name>
            <surname>Putri</surname>
            ,
            <given-names>S.A.</given-names>
          </string-name>
          , et al.,
          <year>2017</year>
          .
          <article-title>Combining integreted sampling technique with feature selection for software defect prediction</article-title>
          .
          <source>In: 2017 5th International Conference on Cyber and IT Service Management (CITSM)</source>
          . IEEE, pp.
          <fpage>1</fpage>
          -
          <lpage>6</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref86">
        <mixed-citation>
          <string-name>
            <surname>Queiroz</surname>
            ,
            <given-names>R.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Berger</surname>
            ,
            <given-names>T.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Czarnecki</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <year>2016</year>
          .
          <article-title>Towards predicting feature defects in software product lines</article-title>
          .
          <source>In: Proceedings of the 7th International Workshop on Feature-Oriented Software Development. ACM</source>
          , pp.
          <fpage>58</fpage>
          -
          <lpage>62</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref87">
        <mixed-citation>
          <string-name>
            <surname>Roos-Frantz</surname>
            ,
            <given-names>F.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Benavides</surname>
            ,
            <given-names>D.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Ruiz-Cortés</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Heuer</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Lauenroth</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <year>2012</year>
          .
          <article-title>Quality-aware analysis in product line engineering with the orthogonal variability model</article-title>
          .
          <source>Softw. Qual. J</source>
          .
          <volume>20</volume>
          (
          <issue>3-4</issue>
          ),
          <fpage>519</fpage>
          -
          <lpage>565</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref88">
        <mixed-citation>
          <string-name>
            <surname>Safdar</surname>
            ,
            <given-names>S.A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Lu</surname>
            ,
            <given-names>H.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Yue</surname>
            ,
            <given-names>T.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Ali</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <year>2017</year>
          .
          <article-title>Mining cross product line rules with multi-objective search and machine learning</article-title>
          .
          <source>In: Proceedings of the Genetic and Evolutionary Computation Conference. ACM</source>
          , pp.
          <fpage>1319</fpage>
          -
          <lpage>1326</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref89">
        <mixed-citation>
          <string-name>
            <surname>Saleem</surname>
            ,
            <given-names>M.S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Ding</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Liu</surname>
            ,
            <given-names>X.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Chi</surname>
          </string-name>
          , C.-H.,
          <year>2015</year>
          .
          <article-title>Personalized decision-strategy based web service selection using a learning-to-rank algorithm</article-title>
          .
          <source>IEEE Trans. Serv. Comput</source>
          .
          <volume>8</volume>
          (
          <issue>5</issue>
          ),
          <fpage>727</fpage>
          -
          <lpage>739</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref90">
        <mixed-citation>
          <string-name>
            <surname>Samreen</surname>
            ,
            <given-names>F.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Elkhatib</surname>
            ,
            <given-names>Y.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Rowe</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Blair</surname>
            ,
            <given-names>G.S.</given-names>
          </string-name>
          ,
          <year>2016</year>
          . Daleel:
          <article-title>Simplifying cloud instance selection using machine learning</article-title>
          .
          <source>In: NOMS</source>
          <year>2016</year>
          -
          <article-title>2016 IEEE/IFIP Network Operations and Management Symposium</article-title>
          . IEEE, pp.
          <fpage>557</fpage>
          -
          <lpage>563</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref91">
        <mixed-citation>
          <string-name>
            <surname>Sarkar</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Guo</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Siegmund</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Apel</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Czarnecki</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <year>2015</year>
          .
          <article-title>Cost-efficient sampling for performance prediction of configurable systems (t)</article-title>
          .
          <source>In: IEEE/ACM International Conference on Automated Software Engineering (ASE)</source>
          . IEEE, pp.
          <fpage>342</fpage>
          -
          <lpage>352</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref92">
        <mixed-citation>
          <string-name>
            <surname>Sayagh</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kerzazi</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Adams</surname>
            ,
            <given-names>B.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Petrillo</surname>
            ,
            <given-names>F.</given-names>
          </string-name>
          ,
          <year>2018</year>
          .
          <article-title>Software configuration engineering in practice: Interviews, survey, and systematic literature review</article-title>
          .
          <source>IEEE Trans. Softw</source>
          . Eng.
        </mixed-citation>
      </ref>
      <ref id="ref93">
        <mixed-citation>
          <string-name>
            <surname>Sayyad</surname>
            ,
            <given-names>A.S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Menzies</surname>
            ,
            <given-names>T.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Ammar</surname>
            ,
            <given-names>H.</given-names>
          </string-name>
          ,
          <year>2013</year>
          .
          <article-title>On the value of user preferences in search-based software engineering: a case study in software product lines</article-title>
          .
          <source>In: Proceedings of the 2013 International Conference on Software Engineering</source>
          . IEEE Press, pp.
          <fpage>492</fpage>
          -
          <lpage>501</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref94">
        <mixed-citation>
          <string-name>
            <surname>Sharifloo</surname>
            ,
            <given-names>A.M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Metzger</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Quinton</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Baresi</surname>
            ,
            <given-names>L.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Pohl</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <year>2016</year>
          .
          <article-title>Learning and evolution in dynamic software product lines</article-title>
          .
          <source>In: 2016 IEEE/ACM 11th International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS)</source>
          . IEEE, pp.
          <fpage>158</fpage>
          -
          <lpage>164</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref95">
        <mixed-citation>
          <string-name>
            <surname>Siegmund</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Grebhahn</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Apel</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kästner</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <year>2015</year>
          .
          <article-title>Performance-influence models for highly configurable systems</article-title>
          .
          <source>In: Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering</source>
          , ESEC/FSE 2015, pp.
          <fpage>284</fpage>
          -
          <lpage>294</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref96">
        <mixed-citation>
          <string-name>
            <surname>Siegmund</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kolesnikov</surname>
            ,
            <given-names>S.S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kästner</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Apel</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Batory</surname>
            ,
            <given-names>D.S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Rosenmüller</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Saake</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          ,
          <article-title>2012a. Predicting performance via automated feature-interaction detection</article-title>
          .
          <source>In: ICSE</source>
          , pp.
          <fpage>167</fpage>
          -
          <lpage>177</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref97">
        <mixed-citation>
          <string-name>
            <surname>Siegmund</surname>
          </string-name>
          , N.,
          <string-name>
            <surname>von Rhein</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Apel</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <year>2013a</year>
          .
          <article-title>Family-based performance measurement</article-title>
          .
          <source>In: ACM SIGPLAN Notices</source>
          , Vol.
          <volume>49</volume>
          . ACM, pp.
          <fpage>95</fpage>
          -
          <lpage>104</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref98">
        <mixed-citation>
          <string-name>
            <surname>Siegmund</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Rosenmüller</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kästner</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Giarrusso</surname>
            ,
            <given-names>P.G.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Apel</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kolesnikov</surname>
            ,
            <given-names>S.S.</given-names>
          </string-name>
          ,
          <year>2011</year>
          .
          <article-title>Scalable prediction of non-functional properties in software product lines</article-title>
          .
          <source>In: Software Product Line Conference (SPLC)</source>
          ,
          <year>2011</year>
          15th International, pp.
          <fpage>160</fpage>
          -
          <lpage>169</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref99">
        <mixed-citation>
          <string-name>
            <surname>Siegmund</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Rosenmüller</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kästner</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Giarrusso</surname>
            ,
            <given-names>P.G.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Apel</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kolesnikov</surname>
            ,
            <given-names>S.S.</given-names>
          </string-name>
          ,
          <year>2013b</year>
          .
          <article-title>Scalable prediction of non-functional properties in software product lines: Footprint and memory consumption</article-title>
          .
          <source>Inf. Softw. Technol</source>
          .
          <volume>55</volume>
          (
          <issue>3</issue>
          ),
          <fpage>491</fpage>
          -
          <lpage>507</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref100">
        <mixed-citation>
          <string-name>
            <surname>Siegmund</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Rosenmüller</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kuhlemann</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kästner</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Apel</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Saake</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          ,
          <year>2012b</year>
          .
          <article-title>Spl conqueror: Toward optimization of non-functional properties in software product lines</article-title>
          .
          <source>Softw. Qual. J</source>
          .
          <volume>20</volume>
          (
          <issue>3-4</issue>
          ),
          <fpage>487</fpage>
          -
          <lpage>517</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref101">
        <mixed-citation>
          <string-name>
            <surname>Siegmund</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Rosenmüller</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kuhlemann</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kästner</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Saake</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          ,
          <year>2008</year>
          .
          <article-title>Measuring non-functional properties in software product line for product derivation</article-title>
          .
          <source>In: 2008 15th Asia-Pacific Software Engineering Conference</source>
          . IEEE, pp.
          <fpage>187</fpage>
          -
          <lpage>194</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref102">
        <mixed-citation>
          <string-name>
            <surname>Siegmund</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Sobernig</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Apel</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <year>2017</year>
          .
          <article-title>Attributed variability models: outside the comfort zone</article-title>
          .
          <source>In: Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering. ACM</source>
          , pp.
          <fpage>268</fpage>
          -
          <lpage>278</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref103">
        <mixed-citation>
          <string-name>
            <surname>Sincero</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Schroder-Preikschat</surname>
            ,
            <given-names>W.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Spinczyk</surname>
            ,
            <given-names>O.</given-names>
          </string-name>
          ,
          <year>2010</year>
          .
          <article-title>Approaching nonfunctional properties of software product lines: Learning from products</article-title>
          .
          <source>In: Software Engineering Conference (APSEC)</source>
          ,
          <year>2010</year>
          17th
          <string-name>
            <given-names>Asia</given-names>
            <surname>Pacific</surname>
          </string-name>
          . pp.
          <fpage>147</fpage>
          -
          <lpage>155</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref104">
        <mixed-citation>
          <string-name>
            <surname>Song</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Porter</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Foster</surname>
            ,
            <given-names>J.S.</given-names>
          </string-name>
          ,
          <year>2013</year>
          .
          <article-title>itree: efficiently discovering high-coverage configurations using interaction trees</article-title>
          .
          <source>IEEE Trans. Softw. Eng</source>
          .
          <volume>40</volume>
          (
          <issue>3</issue>
          ),
          <fpage>251</fpage>
          -
          <lpage>265</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref105">
        <mixed-citation>
          <string-name>
            <surname>Stuckman</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Walden</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Scandariato</surname>
            ,
            <given-names>R.</given-names>
          </string-name>
          ,
          <year>2017</year>
          .
          <article-title>The effect of dimensionality reduction on software vulnerability prediction models</article-title>
          .
          <source>IEEE Trans. Reliab</source>
          .
          <volume>66</volume>
          (
          <issue>1</issue>
          ),
          <fpage>17</fpage>
          -
          <lpage>37</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref106">
        <mixed-citation>
          <string-name>
            <surname>Svahnberg</surname>
          </string-name>
          , M.,
          <string-name>
            <surname>van Gurp</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Bosch</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <year>2005</year>
          .
          <article-title>A taxonomy of variability realization techniques: Research articles</article-title>
          .
          <source>Softw. Pract. Exper</source>
          .
          <volume>35</volume>
          (
          <issue>8</issue>
          ),
          <fpage>705</fpage>
          -
          <lpage>754</lpage>
          . http: //dx.doi.org/10.1002/spe.v35:
          <fpage>8</fpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref107">
        <mixed-citation>
          <string-name>
            <surname>Temple</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Acher</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Jézéquel</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Barais</surname>
            ,
            <given-names>O.</given-names>
          </string-name>
          ,
          <year>2017a</year>
          .
          <article-title>Learning contextualvariability models</article-title>
          .
          <source>IEEE Softw</source>
          .
          <volume>34</volume>
          (
          <issue>6</issue>
          ),
          <fpage>64</fpage>
          -
          <lpage>70</lpage>
          . http://dx.doi.org/10.1109/MS.
          <year>2017</year>
          .4121211, URL https://doi.org/10.1109/MS.
          <year>2017</year>
          .
          <volume>4121211</volume>
          .
        </mixed-citation>
      </ref>
      <ref id="ref108">
        <mixed-citation>
          <string-name>
            <surname>Temple</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Acher</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Jézéquel</surname>
            ,
            <given-names>J.-M.A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Noel-Baron</surname>
            ,
            <given-names>L.A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Galindo</surname>
            ,
            <given-names>J.A.</given-names>
          </string-name>
          ,
          <year>2017b</year>
          .
          <source>Learning-Based Performance Specialization of Configurable Systems. Research report, IRISA</source>
          , Inria Rennes; University of Rennes 1, URL https://hal. archives-ouvertes.fr/hal-01467299.
        </mixed-citation>
      </ref>
      <ref id="ref109">
        <mixed-citation>
          <string-name>
            <surname>Temple</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Galindo</surname>
            <given-names>Duarte</given-names>
          </string-name>
          ,
          <string-name>
            <given-names>J.A.</given-names>
            ,
            <surname>Acher</surname>
          </string-name>
          ,
          <string-name>
            <given-names>M.</given-names>
            ,
            <surname>Jézéquel</surname>
          </string-name>
          ,
          <string-name>
            <surname>J.-M.</surname>
          </string-name>
          ,
          <year>2016</year>
          .
          <article-title>Using machine learning to infer constraints for product lines</article-title>
          .
          <source>In: Software Product Line Conference (SPLC)</source>
          , Beijing, China. http://dx.doi.org/10.1145/2934466.2934472, URL https://hal.inria.fr/hal-01323446.
        </mixed-citation>
      </ref>
      <ref id="ref110">
        <mixed-citation>
          <string-name>
            <surname>Thornton</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Hutter</surname>
            ,
            <given-names>F.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Hoos</surname>
            ,
            <given-names>H.H.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Leyton-Brown</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <year>2013</year>
          .
          <article-title>Auto-weka: Combined selection and hyperparameter optimization of classification algorithms</article-title>
          .
          <source>In: Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM</source>
          , pp.
          <fpage>847</fpage>
          -
          <lpage>855</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref111">
        <mixed-citation>
          <string-name>
            <surname>Thüm</surname>
            ,
            <given-names>T.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Apel</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kästner</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Schaefer</surname>
            ,
            <given-names>I.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Saake</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          ,
          <year>2014</year>
          .
          <article-title>A classification and survey of analysis strategies for software product lines</article-title>
          .
          <source>ACM Comput. Surv</source>
          .
          <volume>47</volume>
          (
          <issue>1</issue>
          ),
          <fpage>6</fpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref112">
        <mixed-citation>
          <string-name>
            <surname>Trubiani</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Apel</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <year>2019</year>
          . Plus:
          <article-title>Performance learning for uncertainty of software</article-title>
          .
          <source>In: International Conference on Software Engineering NIER</source>
          . ACM.
        </mixed-citation>
      </ref>
      <ref id="ref113">
        <mixed-citation>
          <string-name>
            <surname>Valov</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Guo</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Czarnecki</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <year>2015</year>
          .
          <article-title>Empirical comparison of regression methods for variability-aware performance prediction</article-title>
          .
          <source>In: SPLC'15.</source>
        </mixed-citation>
      </ref>
      <ref id="ref114">
        <mixed-citation>
          <string-name>
            <surname>Valov</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Petkovich</surname>
            ,
            <given-names>J.-C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Guo</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Fischmeister</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Czarnecki</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <year>2017</year>
          .
          <article-title>Transferring performance prediction models across different hardware platforms</article-title>
          .
          <source>In: Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering. ACM</source>
          , pp.
          <fpage>39</fpage>
          -
          <lpage>50</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref115">
        <mixed-citation>
          <string-name>
            <surname>Van Aken</surname>
            ,
            <given-names>D.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Pavlo</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Gordon</surname>
            ,
            <given-names>G.J.</given-names>
          </string-name>
          , Zhang,
          <string-name>
            <surname>B.</surname>
          </string-name>
          ,
          <year>2017a</year>
          .
          <article-title>Automatic database management system tuning through large-scale machine learning</article-title>
          .
          <source>In: Proceedings of the 2017 ACM International Conference on Management of Data. ACM</source>
          , pp.
          <fpage>1009</fpage>
          -
          <lpage>1024</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref116">
        <mixed-citation>
          <string-name>
            <surname>Van Aken</surname>
            ,
            <given-names>D.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Pavlo</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Gordon</surname>
            ,
            <given-names>G.J.</given-names>
          </string-name>
          , Zhang,
          <string-name>
            <surname>B.</surname>
          </string-name>
          ,
          <year>2017b</year>
          .
          <article-title>Automatic database management system tuning through large-scale machine learning</article-title>
          .
          <source>In: Proceedings of the 2017 ACM International Conference on Management of Data</source>
          , SIGMOD '
          <fpage>17</fpage>
          .
          <string-name>
            <surname>Association</surname>
          </string-name>
          for Computing Machinery, New York, NY, USA, pp.
          <fpage>1009</fpage>
          -
          <lpage>1024</lpage>
          . http://dx.doi.org/10.1145/3035918.3064029.
        </mixed-citation>
      </ref>
      <ref id="ref117">
        <mixed-citation>
          <string-name>
            <surname>Varshosaz</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Al-Hajjaji</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Thüm</surname>
            ,
            <given-names>T.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Runge</surname>
            ,
            <given-names>T.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Mousavi</surname>
            ,
            <given-names>M.R.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Schaefer</surname>
            ,
            <given-names>I.</given-names>
          </string-name>
          ,
          <year>2018</year>
          .
          <article-title>A classification of product sampling for software product lines</article-title>
          .
          <source>In: Proceeedings of the 22nd International Conference on Systems and Software Product Line-Volume 1. ACM</source>
          , pp.
          <fpage>1</fpage>
          -
          <lpage>13</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref118">
        <mixed-citation>
          <string-name>
            <surname>Venkata</surname>
            ,
            <given-names>S.K.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Ahn</surname>
            ,
            <given-names>I.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Jeon</surname>
            ,
            <given-names>D.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Gupta</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Louie</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Garcia</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Belongie</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Taylor</surname>
          </string-name>
          , M.B.,
          <year>2009</year>
          .
          <article-title>Sd-vbs: The san diego vision benchmark suite</article-title>
          .
          <source>In: 2009 IEEE International Symposium on Workload Characterization (IISWC)</source>
          . IEEE, pp.
          <fpage>55</fpage>
          -
          <lpage>64</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref119">
        <mixed-citation>
          <string-name>
            <surname>Švogor</surname>
            ,
            <given-names>I.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Crnković</surname>
            ,
            <given-names>I.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Vrček</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          ,
          <year>2019</year>
          .
          <article-title>An extensible framework for software configuration optimization on heterogeneous computing systems: Time and energy case study</article-title>
          .
          <source>Inf. Softw. Technol</source>
          .
          <volume>105</volume>
          ,
          <fpage>30</fpage>
          -
          <lpage>42</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref120">
        <mixed-citation>
          <string-name>
            <surname>Weckesser</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kluge</surname>
            ,
            <given-names>R.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Pfannemüller</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Matthé</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Schürr</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Becker</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <year>2018</year>
          .
          <article-title>Optimal reconfiguration of dynamic software product lines based on performance-influence models</article-title>
          .
          <source>In: Proceeedings of the 22nd International Conference on Systems and Software Product Line-Volume 1. ACM</source>
          , pp.
          <fpage>98</fpage>
          -
          <lpage>109</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref121">
        <mixed-citation>
          <string-name>
            <surname>Westermann</surname>
            ,
            <given-names>D.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Happe</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Krebs</surname>
            ,
            <given-names>R.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Farahbod</surname>
            ,
            <given-names>R.</given-names>
          </string-name>
          ,
          <year>2012</year>
          .
          <article-title>Automated inference of goal-oriented performance prediction functions</article-title>
          .
          <source>In: IEEE/ACM International Conference on Automated Software Engineering (ASE)</source>
          .
          <source>ACM</source>
          , pp.
          <fpage>190</fpage>
          -
          <lpage>199</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref122">
        <mixed-citation>
          <string-name>
            <surname>Wohlin</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <year>2014</year>
          .
          <article-title>Guidelines for snowballing in systematic literature studies and a replication in software engineering</article-title>
          .
          <source>In: Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering. ACM</source>
          , p.
          <fpage>38</fpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref123">
        <mixed-citation>
          <string-name>
            <surname>Wohlin</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Runeson</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Host</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Ohlsson</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Regnell</surname>
            ,
            <given-names>B.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Wesslen</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <year>2000</year>
          . Experimentation in Software Engineering: An Introduction.
        </mixed-citation>
      </ref>
      <ref id="ref124">
        <mixed-citation>
          <string-name>
            <surname>Xi</surname>
            ,
            <given-names>B.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Liu</surname>
            ,
            <given-names>Z.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Raghavachari</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Xia</surname>
            ,
            <given-names>C.H.</given-names>
          </string-name>
          , Zhang,
          <string-name>
            <surname>L.</surname>
          </string-name>
          ,
          <year>2012</year>
          .
          <article-title>Automated inference of goal-oriented performance prediction functions</article-title>
          .
          <source>In: IEEE/ACM International Conference on Automated Software Engineering (ASE)</source>
          .
          <source>ACM</source>
          , pp.
          <fpage>190</fpage>
          -
          <lpage>199</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref125">
        <mixed-citation>
          <string-name>
            <surname>Xu</surname>
            ,
            <given-names>L.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Hutter</surname>
            ,
            <given-names>F.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Hoos</surname>
            ,
            <given-names>H.H.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Leyton-Brown</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <year>2008</year>
          .
          <article-title>Satzilla: portfolio-based algorithm selection for sat</article-title>
          .
          <source>J. Artificial Intelligence Res</source>
          .
          <volume>32</volume>
          ,
          <fpage>565</fpage>
          -
          <lpage>606</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref126">
        <mixed-citation>
          <string-name>
            <surname>Xu</surname>
            ,
            <given-names>T.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Jin</surname>
            ,
            <given-names>L.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Fan</surname>
            ,
            <given-names>X.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Zhou</surname>
            ,
            <given-names>Y.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Pasupathy</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Talwadker</surname>
            ,
            <given-names>R.</given-names>
          </string-name>
          ,
          <year>2015</year>
          .
          <article-title>Hey, you have given me too many knobs!: understanding and dealing with over-designed configuration in system software</article-title>
          .
          <source>In: Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering</source>
          , ESEC/FSE 2015, Bergamo, Italy,
          <source>August 30 - September 4</source>
          ,
          <year>2015</year>
          , pp.
          <fpage>307</fpage>
          -
          <lpage>319</lpage>
          . http://dx.doi.org/10.1145/ 2786805.2786852. URL http://doi.acm.
          <source>org/10</source>
          .1145/2786805.2786852.
        </mixed-citation>
      </ref>
      <ref id="ref127">
        <mixed-citation>
          <string-name>
            <surname>Yildiz</surname>
            ,
            <given-names>B.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Bilbao</surname>
            ,
            <given-names>J.I.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Sproul</surname>
            ,
            <given-names>A.B.</given-names>
          </string-name>
          ,
          <year>2017</year>
          .
          <article-title>A review and analysis of regression and machine learning models on commercial building electricity load forecasting</article-title>
          .
          <source>Renew. Sustain. Energy Rev</source>
          .
          <volume>73</volume>
          ,
          <fpage>1104</fpage>
          -
          <lpage>1122</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref128">
        <mixed-citation>
          <string-name>
            <surname>Yilmaz</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Cohen</surname>
            ,
            <given-names>M.B.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Porter</surname>
            ,
            <given-names>A.A.</given-names>
          </string-name>
          ,
          <year>2006</year>
          .
          <article-title>Covering arrays for efficient fault characterization in complex configuration spaces</article-title>
          .
          <source>IEEE Trans. Softw. Eng</source>
          .
          <volume>32</volume>
          (
          <issue>1</issue>
          ),
          <fpage>20</fpage>
          -
          <lpage>34</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref129">
        <mixed-citation>
          <string-name>
            <surname>Yilmaz</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Dumlu</surname>
            ,
            <given-names>E.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Cohen</surname>
            ,
            <given-names>M.B.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Porter</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <year>2014</year>
          .
          <article-title>Reducing masking effects in combinatorialinteraction testing: A feedback drivenadaptive approach</article-title>
          .
          <source>IEEE Trans. Softw. Eng</source>
          .
          <volume>40</volume>
          (
          <issue>1</issue>
          ),
          <fpage>43</fpage>
          -
          <lpage>66</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref130">
        <mixed-citation>
          <string-name>
            <surname>Zhang</surname>
            ,
            <given-names>Y.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Guo</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Blais</surname>
            ,
            <given-names>E.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Czarnecki</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <year>2015</year>
          .
          <article-title>Performance prediction of configurable software systems by Fourier learning (t)</article-title>
          .
          <source>In: IEEE/ACM International Conference on Automated Software Engineering (ASE)</source>
          . IEEE, pp.
          <fpage>365</fpage>
          -
          <lpage>373</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref131">
        <mixed-citation>
          <string-name>
            <surname>Zhang</surname>
            ,
            <given-names>Y.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Guo</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Blais</surname>
            ,
            <given-names>E.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Czarnecki</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Yu</surname>
            ,
            <given-names>H.</given-names>
          </string-name>
          ,
          <year>2016</year>
          .
          <article-title>A mathematical model of performance-relevant feature interactions</article-title>
          .
          <source>In: Proceedings of the 20th International Systems and Software Product Line Conference. ACM</source>
          , pp.
          <fpage>25</fpage>
          -
          <lpage>34</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref132">
        <mixed-citation>
          <string-name>
            <surname>Zheng</surname>
            ,
            <given-names>W.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Bianchini</surname>
            ,
            <given-names>R.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Nguyen</surname>
          </string-name>
          , T.D.,
          <year>2007</year>
          .
          <article-title>Automatic configuration of internet services</article-title>
          .
          <source>Oper. Syst. Rev</source>
          .
          <volume>41</volume>
          (
          <issue>3</issue>
          ),
          <fpage>219</fpage>
          -
          <lpage>229</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref133">
        <mixed-citation>
          <string-name>
            <surname>Zuluaga</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Krause</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Püschel</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          ,
          <year>2016</year>
          .
          <article-title>ε-pal: an active learning approach to the multi-objective optimization problem</article-title>
          .
          <source>J. Mach. Learn. Res</source>
          .
          <volume>17</volume>
          (
          <issue>1</issue>
          ),
          <fpage>3619</fpage>
          -
          <lpage>3650</lpage>
          .
        </mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>