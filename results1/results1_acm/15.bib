@inproceedings{10.1145/3503229.3547055,
author = {Friesel, Birte and M\"{u}ller, Michael and Ferraz, Matheus and Spinczyk, Olaf},
title = {On the relation of variability modeling languages and non-functional properties},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547055},
doi = {10.1145/3503229.3547055},
abstract = {Non-functional properties (NFPs) such as code size (RAM, ROM), performance, and energy consumption are at least as important as functional properties in many software development domains. When configuring a software product line - especially in the area of resource-constrained embedded systems - developers must be aware of the NFPs of the configured product instance. Several NFP-aware variability modeling languages have been proposed to address this in the past. However, it is not clear whether a variability modeling language is the best place for handling NFP-related concerns, or whether separate NFP prediction models should be preferred. We shine light onto this question by discussing limitations of state-of-the-art NFP-aware variability modeling languages, and find that both in terms of the development process and model accuracy a separate NFP model is favorable. Our quantitative analysis is based on six different software product lines, including the widely used busybox multi-call binary and the x264 video encoder. We use classification and regression trees (CART) and our recently proposed Regression Model Trees [8] as separate NFP models. These tree-based models can cover the effects of arbitrary feature interactions and thus easily outperform variability models with static, feature-wise NFP annotations. For example, when estimating the throughput of an embedded AI product line, static annotations come with a mean generalization error of 114.5% while the error of CART is only 9.4 %.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {140–144},
numpages = {5},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3522664.3528602,
author = {Friesel, Birte and Spinczyk, Olaf},
title = {Black-box models for non-functional properties of AI software systems},
year = {2022},
isbn = {9781450392754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3522664.3528602},
doi = {10.1145/3522664.3528602},
abstract = {Non-functional properties (NFPs) such as latency, memory requirements, or hardware cost are an important characteristic of AI software systems, especially in the domain of resource-constrained embedded devices. Embedded AI products require sufficient resources for satisfactory latency and accuracy, but should also be cost-efficient and therefore not use more powerful hardware than strictly necessary. Traditionally, modeling and optimization efforts focus on the AI architecture, utilizing methods such as neural architecture search (NAS). However, before developers can start optimizing, they need to know which architectures are suitable candidates for their use case. To this end, architectures must be viewed in context: model post-processing (e.g. quantization), hardware platform, and run-time configuration such as batching all have significant effects on NFPs and therefore on AI architecture performance. Moreover, scalar parameters such as batch size cannot be benchmarked exhaustively. We argue that it is worthwhile to address this issue by means of black-box models before deciding on AI architectures for optimization and hardware/software platforms for inference. To support our claim, we present an AI product line with variable hardware and software components, perform benchmarks, and present notable results. Additionally, we evaluate both compactness and generalization capabilities of regression tree-based modeling approaches from the machine learning and product line engineering communities. We find that linear model trees perform best: they can capture NFPs of known AI configurations with a mean error of up to 13 %, and can predict unseen configurations with a mean error of 10 to 26 %. We find linear model trees to be more compact and interpretable than other tree-based approaches.},
booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
pages = {170–180},
numpages = {11},
keywords = {AI, performance prediction, product lines, regression trees},
location = {Pittsburgh, Pennsylvania},
series = {CAIN '22}
}

@article{10.1145/3442389,
author = {Castro, Thiago and Teixeira, Leopoldo and Alves, Vander and Apel, Sven and Cordy, Maxime and Gheyi, Rohit},
title = {A Formal Framework of Software Product Line Analyses},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3442389},
doi = {10.1145/3442389},
abstract = {A number of product-line analysis approaches lift analyses such as type checking, model checking, and theorem proving from the level of single programs to the level of product lines. These approaches share concepts and mechanisms that suggest an unexplored potential for reuse of key analysis steps and properties, implementation, and verification efforts. Despite the availability of taxonomies synthesizing such approaches, there still remains the underlying problem of not being able to describe product-line analyses and their properties precisely and uniformly. We propose a formal framework that models product-line analyses in a compositional manner, providing an overall understanding of the space of family-based, feature-based, and product-based analysis strategies. It defines precisely how the different types of product-line analyses compose and inter-relate. To ensure soundness, we formalize the framework, providing mechanized specification and proofs of key concepts and properties of the individual analyses. The formalization provides unambiguous definitions of domain terminology and assumptions as well as solid evidence of key properties based on rigorous formal proofs. To qualitatively assess the generality of the framework, we discuss to what extent it describes five representative product-line analyses targeting the following properties: safety, performance, dataflow facts, security, and functional program properties.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {34},
numpages = {37},
keywords = {product-line analysis, Software product lines}
}

@inproceedings{10.1145/2934466.2934481,
author = {Sion, Laurens and Van Landuyt, Dimitri and Joosen, Wouter and de Jong, Gjalt},
title = {Systematic quality trade-off support in the software product-line configuration process},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934481},
doi = {10.1145/2934466.2934481},
abstract = {Software product line engineering is a compelling methodology that accomplishes systematic reuse in families of systems by relying on two key principles: (i) the decomposition of complex systems into composable and reusable building blocks (often logical units called features), and (ii) on-demand construction of products and product variants by composing these building blocks.However, unless the stakeholder responsible for product configuration has detailed knowledge of the technical ins and outs of the software product line (e.g., the architectural impact of a specific feature, or potential feature interactions), he is in many cases flying in the dark. Although many initial approaches and techniques have been proposed that take into account quality considerations and involve trade-off decisions during product configuration, no systematic support exists.In this paper, we present a reference architecture for product configuration tooling, providing support for (i) up-front generation of variants, and (ii) quality analysis of these variants. This allows pro-actively assessing and predicting architectural quality properties for each product variant and in turn, product configuration tools can take into account architectural considerations. In addition, we provide an in-depth discussion of techniques and tactics for dealing with the problem of variant explosion, and as such to maintain practical feasibility of such approaches.We validated and implemented our reference architecture in the context of a real-world industrial application, a product-line for the firmware of an automotive sensor. Our prototype, based on FeatureIDE, is open for extension and readily available.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {164–173},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, M\'{a}rcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Software Product Lines, Recommender Systems, Non-Functional Properties, Feature Model, Configuration},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/2648511.2648513,
author = {Harman, M. and Jia, Y. and Krinke, J. and Langdon, W. B. and Petke, J. and Zhang, Y.},
title = {Search based software engineering for software product line engineering: a survey and directions for future work},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648513},
doi = {10.1145/2648511.2648513},
abstract = {This paper presents a survey of work on Search Based Software Engineering (SBSE) for Software Product Lines (SPLs). We have attempted to be comprehensive, in the sense that we have sought to include all papers that apply computational search techniques to problems in software product line engineering. Having surveyed the recent explosion in SBSE for SPL research activity, we highlight some directions for future work. We focus on suggestions for the development of recent advances in genetic improvement, showing how these might be exploited by SPL researchers and practitioners: Genetic improvement may grow new products with new functional and non-functional features and graft these into SPLs. It may also merge and parameterise multiple branches to cope with SPL branchmania.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {5–18},
numpages = {14},
keywords = {program synthesis, genetic programming, SPL, SBSE},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3579027.3608975,
author = {Burgue\~{n}o, Lola and Horcas, Jose-Miguel and Kienzle, J\"{o}rg},
title = {Development and Evolution of Software Product Lines Driven by Stakeholder Beliefs},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608975},
doi = {10.1145/3579027.3608975},
abstract = {The planning, realization, and release of a Software Product Line (SPL) are driven by features. Therefore, many high-level decisions about the evolution of an SPL are made at the feature level. However, a feature can involve many stakeholders with different expertise, and taking their opinions into account to make the right decisions is not trivial. In this paper, we propose using belief uncertainty in conjunction with feature models to assist in the evolution of SPLs by explicitly quantifying opinions. We outline three evolution scenarios in which subjective logic can be used to represent the opinions of stakeholders and explain in detail how to use subjective logic to make decisions in the context of the next release problem. We illustrate our ideas with a Smartwatch SPL. Finally, we discuss different ways of combining the opinions of stakeholders depending on the situation, the goals and the risks that can be assumed.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {34–40},
numpages = {7},
keywords = {uncertainty, subjective logic, software product line, feature model, Decision making support},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3646548.3676546,
author = {G\"{u}thing, Lukas and Pett, Tobias and Schaefer, Ina},
title = {Out-of-the-Box Prediction of Non-Functional Variant Properties Using Automated Machine Learning},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676546},
doi = {10.1145/3646548.3676546},
abstract = {A configurable system is characterized by the configuration options present or absent in its variants. Selecting and deselecting those configuration options directly influences the functional properties of the system. Apart from functional properties, there are system characteristics that influence the performance (e.g., power demand), safety (e.g., fault probabilities), and security (e.g., susceptibility to attacks) of the system, called Non-Functional Properties (NFPs). Knowledge of NFPs is crucial for evaluating a system’s feasibility, usability, and resource demands. Although variability influences these characteristics, NFPs do not compose linearly for every selected feature. Feature interactions can increase the overall NFP values through (potentially exponential) amplification or decrease them through mitigation effects. In this paper, we propose an automated machine learning (AutoML) approach to predict NFP values for new configurations based on previously measured configuration values. Using AutoML, we leverage the advantages of machine learning for predicting NFPs without having to parameterize and fine-tune machine learning models. This approach and the resulting pipeline aim to reduce the complexity of performance prediction for configurable systems. We test the feasibility of our pipeline in a first evaluation on 4 real-world subject systems and discuss cases where AutoML may improve the prediction of NFPs.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {82–87},
numpages = {6},
keywords = {AutoML, Cyber-physical systems, Machine learning, Software product lines},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3167132.3167353,
author = {Pereira, Juliana Alves and Martinez, Jabier and Gurudu, Hari Kumar and Krieter, Sebastian and Saake, Gunter},
title = {Visual guidance for product line configuration using recommendations and non-functional properties},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167353},
doi = {10.1145/3167132.3167353},
abstract = {Software Product Lines (SPLs) are a mature approach for the derivation of a family of products using systematic reuse. Different combinations of predefined features enable tailoring the product to fit the needs of each customer. These needs are related to functional properties of the system (optional features) as well as non-functional properties (e.g., performance or cost of the final product). In industrial scenarios, the configuration process of a final product is complex and the tool support is usually limited to check functional properties interdependencies. In addition, the importance of nonfunctional properties as relevant drivers during configuration has been overlooked. Thus, there is a lack of holistic paradigms integrating recommendation systems and visualizations that can help the decision makers. In this paper, we propose and evaluate an interrelated set of visualizations for the configuration process filling these gaps. We integrate them as part of the FeatureIDE tool and we evaluate its effectiveness, scalability, and performance.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {2058–2065},
numpages = {8},
keywords = {visualization, software product lines, recommendation systems, feature model, configuration},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3646548.3672586,
author = {Fernandez-Amoros, David and Heradio, Ruben and Horcas Aguilera, Jose Miguel and Galindo, Jos\'{e} A. and Benavides, David and Fuentes, Lidia},
title = {Pragmatic Random Sampling of the Linux Kernel: Enhancing the Randomness and Correctness of the conf Tool},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672586},
doi = {10.1145/3646548.3672586},
abstract = {The configuration space of some systems is so large that it cannot be computed. This is the case with the Linux Kernel, which provides almost 19,000 configurable options described across more than 1,600 files in the Kconfig language. As a result, many analyses of the Kernel rely on sampling its configuration space (e.g., debugging compilation errors, predicting configuration performance, finding the configuration that optimizes specific performance metrics, etc.). The Kernel can be sampled pragmatically, with its built-in tool conf, or idealistically, translating the Kconfig files into logic formulas. The pros of the idealistic approach are that it provides statistical guarantees for the sampled configurations, but the cons are that it sets out many challenging problems that have not been solved yet, such as scalability issues. This paper introduces a new version of conf called randconfig+, which incorporates a series of improvements that increase the randomness and correctness of pragmatic sampling and also help validate the Boolean translation required for the idealistic approach. randconfig+ has been tested on 20,000 configurations generated for 10 different Kernel versions from 2003 to the present day. The experimental results show that randconfig+ is compatible with all tested Kernel versions, guarantees the correctness of the generated configurations, and increases conf’s randomness for numeric and string options.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {24–35},
numpages = {12},
keywords = {Kconfig, SAT, configurable systems, randconfig, random sampling, software product lines, variability modeling},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3646548.3676539,
author = {M\"{u}ller, Robert and Wei\ss{}, Mathis and Lochau, Malte},
title = {Mapping Cardinality-based Feature Models to Weighted Automata over Featured Multiset Semirings},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676539},
doi = {10.1145/3646548.3676539},
abstract = {Cardinality-based feature models permit to select multiple copies of the same feature, thus generalizing the notion of product configurations from subsets of Boolean features to multisets of feature instances. This increased expressiveness shapes a-priori infinite and non-convex configuration spaces, which renders established solution-space mappings based on Boolean presence conditions insufficient for cardinality-based feature models. To address this issue, we propose weighted automata over featured multiset semirings as a novel behavioral variability modeling formalism for cardinality-based feature models. The formalism uses multisets over features as a predefined semantic domain for transition weights. It permits to use any algebraic structure forming a proper semiring on multisets to aggregate the weights traversed along paths to map accepted words to multiset configurations. In particular, tropical semirings constitute a promising sub-class with a reasonable trade-off between expressiveness and computational tractability of canonical analysis problems. The formalism is strictly more expressive than featured transition systems, as it enables upper-bound multiplicity constraints depending on the length of words. We provide a tool implementation of the behavioral variability model and present preliminary experimental results showing applicability and computational feasibility of the proposed approach.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {1–11},
numpages = {11},
keywords = {Behavioral Variability Modeling, Cardinality-Based Feature Models, Weighted Automata},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3546932.3546997,
author = {Acher, Mathieu and Martin, Hugo and Lesoil, Luc and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc and Khelladi, Djamel Eddine and Barais, Olivier and Pereira, Juliana Alves},
title = {Feature subset selection for learning huge configuration spaces: the case of linux kernel size},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3546997},
doi = {10.1145/3546932.3546997},
abstract = {Linux kernels are used in a wide variety of appliances, many of them having strong requirements on the kernel size due to constraints such as limited memory or instant boot. With more than nine thousands of configuration options to choose from, developers and users of Linux actually spend significant effort to document, understand, and eventually tune (combinations of) options for meeting a kernel size. In this paper, we describe a large-scale endeavour automating this task and predicting a given Linux kernel binary size out of unmeasured configurations. We first experiment that state-of-the-art solutions specifically made for configurable systems such as performance-influence models cannot cope with that number of options, suggesting that software product line techniques may need to be adapted to such huge configuration spaces. We then show that tree-based feature selection can learn a model achieving low prediction errors over a reduced set of options. The resulting model, trained on 95 854 kernel configurations, is fast to compute, simple to interpret and even outperforms the accuracy of learning without feature selection.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {85–96},
numpages = {12},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/2420942.2420948,
author = {Gonz\'{a}lez-Huerta, Javier and Insfran, Emilio and Abrah\~{a}o, Silvia and McGregor, John D.},
title = {Non-functional requirements in model-driven software product line engineering},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420948},
doi = {10.1145/2420942.2420948},
abstract = {Developing variant-rich software systems through the application of the software product line approach requires the management of a wide set of requirements. However, in most cases, the focus of those requirements is limited to the functional requirements. The non-functional requirements are often informally defined and their management does not provide traceability mechanisms for their validation. In this paper, we present a multimodel approach that allows the explicit representation of non-functional requirements for software product lines both at domain engineering, and application engineering levels. The multimodel allows the representation of different viewpoints of a software product line, including the non-functional requirements and the relationships that these non-functional requirements might have with features and functionalities. The feasibility of this approach is illustrated through a specific example from the automotive domain.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {6},
numpages = {6},
keywords = {software product lines, non-functional requirements, model driven engineering},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@inproceedings{10.5555/2337223.2337302,
author = {Cordy, Maxime and Classen, Andreas and Perrouin, Gilles and Schobbens, Pierre-Yves and Heymans, Patrick and Legay, Axel},
title = {Simulation-based abstractions for software product-line model checking},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Software Product Line (SPL) engineering is a software engineering paradigm that exploits the commonality between similar software products to reduce life cycle costs and time-to-market. Many SPLs are critical and would benefit from efficient verification through model checking. Model checking SPLs is more difficult than for single systems, since the number of different products is potentially huge. In previous work, we introduced Featured Transition Systems (FTS), a formal, compact representation of SPL behaviour, and provided efficient algorithms to verify FTS. Yet, we still face the state explosion problem, like any model checking-based verification. Model abstraction is the most relevant answer to state explosion. In this paper, we define a novel simulation relation for FTS and provide an algorithm to compute it. We extend well-known simulation preservation properties to FTS and thus lay the theoretical foundations for abstraction-based model checking of SPLs. We evaluate our approach by comparing the cost of FTS-based simulation and abstraction with respect to product-by-product methods. Our results show that FTS are a solid foundation for simulation-based model checking of SPL.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {672–682},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1145/3503229.3547026,
author = {Friesel, Birte and Elmenhorst, Kathrin and Kaiser, Lennart and M\"{u}ller, Michael and Spinczyk, Olaf},
title = {kconfig-webconf: retrofitting performance models onto kconfig-based software product lines},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547026},
doi = {10.1145/3503229.3547026},
abstract = {Despite decades of research and clear advantages, performance-aware configuration of real-world software product lines is still an exception rather than the norm. One reason for this may be tooling: configuration software with support for non-functional property models is generally not compatible with the configuration and build process of existing product lines. Specifically, the Kconfig language is popular in open source software projects, but neither language nor configuration frontends support performance models. To address this, we present kconfig-webconf: a performance-aware, Kconfig-compatible software product line configuration frontend. It is part of a toolchain that can automatically generate performance models with a minimal amount of changes to a software product line's build process. With such a performance model, kconfig-webconf can serve as a performance-aware drop-in replacement for existing Kconfig frontends. We evaluate its usage in five examples, including the busybox multi-call binary and the resKIL agricultural AI product line.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {58–61},
numpages = {4},
keywords = {kconfig, performance prediction, product lines, regression trees},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3382025.3414942,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Kr\"{u}ger, Jacob and Mendon\c{c}a, Willian D. F.},
title = {Variability management meets microservices: six challenges of re-engineering microservice-based webshops},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414942},
doi = {10.1145/3382025.3414942},
abstract = {A microservice implements a small unit of functionality that it provides through a network using lightweight protocols. So, microservices can be combined to fulfill tasks and implement features of a larger software system---resembling a variability mechanism in the context of a software product line (SPL). Microservices and SPLs have similar goals, namely facilitating reuse and customizing, but they are usually employed in different contexts. Any developer who has access to the network can provide a microservice for any task, while SPLs are usually intended to implement features of a specific domain. Due to their different concepts, using microservices to implement an SPL or adopting SPL practices (e.g., variability management) for microservices is a challenging cross-area research problem. However, both techniques can complement each other, and thus tackling this problem promises benefits for organizations that employ either technique. In this paper, we reason on the importance of advancing in this direction, and sketch six concrete challenges to initiate research, namely (1) feature identification, (2) variability modeling, (3) variable microservice architectures, (4) interchangeability, (5) deep customization, and (6) re-engineering an SPL. We intend these challenges to serve as a starting point for future research in this cross-area research direction---avoiding that the concepts of one area are reinvented in the other.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {22},
numpages = {6},
keywords = {variability management, software product line, re-engineering, microservices, cloud computing},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3307630.3342705,
author = {Krieter, Sebastian},
title = {Enabling Efficient Automated Configuration Generation and Management},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342705},
doi = {10.1145/3307630.3342705},
abstract = {Creating and managing valid configurations is one of the main tasks in software product line engineering. Due to the often complex constraints from a feature model, some kind of automated configuration generation is required to facilitate the configuration process for users and developers. For instance, decision propagation can be applied to support users in configuring a product from a software product line (SPL) with less manual effort and error potential, leading to a semi-automatic configuration process. Furthermore, fully-automatic configuration processes, such as random sampling or t-wise interaction sampling can be employed to test or to optimize an SPL. However, current techniques for automated configuration generation still do not scale well to SPLs with large and complex feature models. Within our thesis, we identify current challenges regarding the efficiency and effectiveness of the semi- and fully-automatic configuration process and aim to address these challenges by introducing novel techniques and improving current ones. Our preliminary results show already show promising progress for both, the semi- and fully-automatic configuration process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {215–221},
numpages = {7},
keywords = {uniform random sampling, t-wise sampling, software product lines, decision propagation, configurable system},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1145/3628158,
author = {Xiang, Yi and Huang, Han and Li, Sizhe and Li, Miqing and Luo, Chuan and Yang, Xiaowei},
title = {Automated Test Suite Generation for Software Product Lines Based on Quality-Diversity Optimization},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3628158},
doi = {10.1145/3628158},
abstract = {A Software Product Line (SPL) is a set of software products that are built from a variability model. Real-world SPLs typically involve a vast number of valid products, making it impossible to individually test each of them. This arises the need for automated test suite generation, which was previously modeled as either a single-objective or a multi-objective optimization problem considering only objective functions. This article provides a completely different mathematical model by exploiting the benefits of Quality-Diversity (QD) optimization that is composed of not only an objective function (e.g., t-wise coverage or test suite diversity) but also a user-defined behavior space (e.g., the space with test suite size as its dimension). We argue that the new model is more suitable and generic than the two alternatives because it provides at a time a large set of diverse (measured in the behavior space) and high-performing solutions that can ease the decision-making process. We apply MAP-Elites, one of the most popular QD algorithms, to solve the model. The results of the evaluation, on both realistic and artificial SPLs, are promising, with MAP-Elites significantly and substantially outperforming both single- and multi-objective approaches, and also several state-of-the-art SPL testing tools. In summary, this article provides a new and promising perspective on the test suite generation for SPLs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {46},
numpages = {52},
keywords = {Quality-Diversity (QD) optimization, automated test suite generation, Software Product Line}
}

@inproceedings{10.1145/2362536.2362548,
author = {Soltani, Samaneh and Asadi, Mohsen and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Bagheri, Ebrahim},
title = {Automated planning for feature model configuration based on functional and non-functional requirements},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362548},
doi = {10.1145/2362536.2362548},
abstract = {Feature modeling is one of the main techniques used in Software Product Line Engineering to manage the variability within the products of a family. Concrete products of the family can be generated through a configuration process. The configuration process selects and/or removes features from the feature model according to the stakeholders' requirements. Selecting the right set of features for one product from amongst all of the available features in the feature model is a complex task because: 1) the multiplicity of stakeholders' functional requirements; 2) the positive or negative impact of features on non-functional properties; and 3) the stakeholders' preferences w.r.t. the desirable non-functional properties of the final product. Many configurations techniques have already been proposed to facilitate automated product derivation. However, most of the current proposals are not designed to consider stakeholders' preferences and constraints especially with regard to non-functional properties. We address the software product line configuration problem and propose a framework, which employs an artificial intelligence planning technique to automatically select suitable features that satisfy both the stakeholders' functional and non-functional preferences and constraints. We also provide tooling support to facilitate the use of our framework. Our experiments show that despite the complexity involved with the simultaneous consideration of both functional and non-functional properties our configuration technique is scalable.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {56–65},
numpages = {10},
keywords = {software product line engineering, planning techniques, feature model, configuration, artificial intelligence},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2491627.2491631,
author = {Myll\"{a}rniemi, Varvana and Savolainen, Juha and M\"{a}nnist\"{o}, Tomi},
title = {Performance variability in software product lines: a case study in the telecommunication domain},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491631},
doi = {10.1145/2491627.2491631},
abstract = {In the research on software product lines, product variants typically differ by their functionality, and quality attributes are more or less similar across products. To accumulate empirical evidence, this paper presents a descriptive case study of performance variability in a software product line of mobile network base stations. The goal is to study the motivation to vary performance, and the strategy for realizing performance variability in the product line architecture. The results highlight that the evolution of customer needs motivates performance variability; performance variability can be realized either with software or hardware variability strategy, with the latter often being prevailing; and the software strategy can be kept focused by downgrading performance.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {32–41},
numpages = {10},
keywords = {variability, software product line, case study, architecture},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3382025.3414962,
author = {Chrszon, Philipp and Baier, Christel and Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha},
title = {From features to roles},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414962},
doi = {10.1145/3382025.3414962},
abstract = {The detection of interactions is a challenging task present in almost all stages of software development. In feature-oriented system design, this task is mainly investigated for interactions of features within a single system, detected by their emergent behaviors. We propose a formalism to describe interactions in hierarchies of feature-oriented systems (hierarchical interactions) and the actual situations where features interact (active interplays). Based on the observation that such interactions are also crucial in role-based systems, we introduce a compositional modeling framework based on concepts and notions of roles, comprising role-based automata (RBAs). To describe RBAs, we present a modeling language that is close to the input language of the probabilistic model checker Prism. To exemplify the use of RBAs, we implemented a tool that translates RBA models into Prism and thus enables the formal analysis of functional and non-functional properties including system dynamics, contextual changes, and interactions. We carry out two case studies as a proof of concept of such analyses: First, a peer-to-peer protocol case study illustrates how undesired hierarchical interactions can be discovered automatically. Second, a case study on a self-adaptive production cell demonstrates how undesired interactions influence quality-of-service measures such as reliability and throughput.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {19},
numpages = {11},
keywords = {verification, roles, formal methods, feature-oriented systems},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and J\'{e}z\'{e}quel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461001.3471149,
author = {Lesoil, Luc and Acher, Mathieu and T\'{e}rnava, Xhevahire and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {The interplay of compile-time and run-time options for performance prediction},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471149},
doi = {10.1145/3461001.3471149},
abstract = {Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {100–111},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3106195.3106205,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green Configurations of Functional Quality Attributes},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106205},
doi = {10.1145/3106195.3106205},
abstract = {Functional quality attributes (FQAs) are those quality attributes that, to be satisfied, require the incorporation of additional functionality into the application architecture. By adding an FQA (e.g., security) we can improve the quality of the final product, but there is also an increase in energy consumption. This paper proposes a solution to help the software architect to generate configurations of FQAs whilst keeping the energy consumed by the application as low as possible. For this, a usage model is defined for each FQA, taking into account the variables that affect the energy consumption, and that the values of these variables change according to the part of the application where the FQA is required. We extend a Software Product Line that models a family of FQAs to incorporate the variability of the usage model and the existing frameworks that implement FQAs. We generate the most eco-efficient configuration of FQAs by selecting the framework with the most suitable characteristics according to the requirements of the application.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {79–83},
numpages = {5},
keywords = {Variability, SPL, Quality Attributes, FQA, Energy Consumption},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3461001.3471146,
author = {Horcas, Jose-Miguel and Galindo, Jos\'{e} A. and Heradio, Ruben and Fernandez-Amoros, David and Benavides, David},
title = {Monte Carlo tree search for feature model analyses: a general framework for decision-making},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471146},
doi = {10.1145/3461001.3471146},
abstract = {The colossal solution spaces of most configurable systems make intractable their exhaustive exploration. Accordingly, relevant analyses remain open research problems. There exist analyses alternatives such as SAT solving or constraint programming. However, none of them have explored simulation-based methods. Monte Carlo-based decision making is a simulation-based method for dealing with colossal solution spaces using randomness. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo methods, which have proven to succeed in vast search spaces (e.g., game theory). Our general framework is described formally, and its flexibility to cope with a diversity of analysis problems is discussed (e.g., finding defective configurations, feature model reverse engineering or getting optimal performance configurations). Additionally, we present a Python implementation of the framework that shows the feasibility of our proposal. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state of the art a step forward.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {190–201},
numpages = {12},
keywords = {variability modeling, software product lines, monte carlo tree search, feature models, configurable systems},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3336294.3336302,
author = {Str\"{u}ber, Daniel and Mukelabai, Mukelabai and Kr\"{u}ger, Jacob and Fischer, Stefan and Linsbauer, Lukas and Martinez, Jabier and Berger, Thorsten},
title = {Facing the Truth: Benchmarking the Techniques for the Evolution of Variant-Rich Systems},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336302},
doi = {10.1145/3336294.3336302},
abstract = {The evolution of variant-rich systems is a challenging task. To support developers, the research community has proposed a range of different techniques over the last decades. However, many techniques have not been adopted in practice so far. To advance such techniques and to support their adoption, it is crucial to evaluate them against realistic baselines, ideally in the form of generally accessible benchmarks. To this end, we need to improve our empirical understanding of typical evolution scenarios for variant-rich systems and their relevance for benchmarking. In this paper, we establish eleven evolution scenarios in which benchmarks would be beneficial. Our scenarios cover typical lifecycles of variant-rich system, ranging from clone &amp; own to adopting and evolving a configurable product-line platform. For each scenario, we formulate benchmarking requirements and assess its clarity and relevance via a survey with experts in variant-rich systems and software evolution. We also surveyed the existing benchmarking landscape, identifying synergies and gaps. We observed that most scenarios, despite being perceived as important by experts, are only partially or not at all supported by existing benchmarks-a call to arms for building community benchmarks upon our requirements. We hope that our work raises awareness for benchmarking as a means to advance techniques for evolving variant-rich systems, and that it will lead to a benchmarking initiative in our community.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {177–188},
numpages = {12},
keywords = {software variability, software evolution, product lines, benchmark},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3382025.3414965,
author = {Young, Jeffrey M. and Walkingshaw, Eric and Th\"{u}m, Thomas},
title = {Variational satisfiability solving},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414965},
doi = {10.1145/3382025.3414965},
abstract = {Incremental satisfiability (SAT) solving is an extension of classic SAT solving that allows users to efficiently solve a set of related SAT problems by identifying and exploiting shared terms. However, using incremental solvers effectively is hard since performance is sensitive to a problem's structure and the order sub-terms are fed to the solver, and the burden to track results is placed on the end user. For analyses that generate sets of related SAT problems, such as those in software product lines, incremental SAT solvers are either not used at all, used but not explicitly stated so in the literature, or used but suffer from the aforementioned usability problems. This paper translates the ordering problem to an encoding problem and automates the use of incremental SAT solving. We introduce variational SAT solving, which differs from incremental SAT solving by accepting all related problems as a single variational input and returning all results as a single variational output. Our central idea is to make explicit the operations of incremental SAT solving, thereby encoding differences between related SAT problems as local points of variation. Our approach automates the interaction with the incremental solver and enables methods to automatically optimize sharing of the input. To evaluate our methods we construct a prototype variational SAT solver and perform an empirical analysis on two real-world datasets that applied incremental solvers to software evolution scenarios. We show, assuming a variational input, that the prototype solver scales better for these problems than naive incremental solving while also removing the need to track individual results.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {18},
numpages = {12},
keywords = {variation, software product lines, satisfiability solving, choice calculus},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3382025.3414945,
author = {G\"{o}ttmann, Hendrik and Luthmann, Lars and Lochau, Malte and Sch\"{u}rr, Andy},
title = {Real-time-aware reconfiguration decisions for dynamic software product lines},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414945},
doi = {10.1145/3382025.3414945},
abstract = {Dynamic Software Product Lines (DSPL) have recently shown promising potentials as integrated engineering methodology for (self-)adaptive software systems. Based on the software-configuration principles of software product lines, DSPL additionally foster reconfiguration capabilities to continuously adapt software products to ever-changing environmental contexts. However, in most recent works concerned with finding near-optimal reconfiguration decisions, real-time aspects of reconfiguration processes are usually out of scope. In this paper, we present a model-based methodology for specifying and automatically analyzing real-time constraints of reconfiguration decisions in a feature-oriented and compositional way. Those real-time aware DSPL specifications are internally translated into timed automata, a well-founded formalism for real-time behaviors. This representation allows for formally reasoning about consistency and worst-case/best-case execution-time behaviors of sequences of reconfiguration decisions. The technique is implemented in a prototype tool and experimentally evaluated with respect to a set of case studies1.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {13},
numpages = {11},
keywords = {timed automata, reconfiguration decisions, dynamic software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3106195.3106215,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Self-healing in Service Mashups Through Feature Adaptation},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106215},
doi = {10.1145/3106195.3106215},
abstract = {The composition of the functionality of multiple services into a single unique service mashup has received wide interest in the recent years. Given the distributed nature of these mashups where the constituent services can be located on different servers, it is possible that a change in the functionality or availability of a constituent service result in the failure of the service mashup. In this paper, we propose a novel method based on the Software Product Line Engineering (SPLE) paradigm which is able to find an alternate valid service mashup which has maximum possible number of original service mashup features in order to mitigate a service failure when complete recovery is not possible. This method also has an advantage that it can recover or mitigate the failure automatically without requiring the user to specify any adaptation rule or strategy. We show the practicality of our proposed approach through extensive experiments.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {94–103},
numpages = {10},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3233027.3233039,
author = {Pereira, Juliana Alves and Schulze, Sandro and Figueiredo, Eduardo and Saake, Gunter},
title = {N-dimensional tensor factorization for self-configuration of software product lines at runtime},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233039},
doi = {10.1145/3233027.3233039},
abstract = {Dynamic software product lines demand self-adaptation of their behavior to deal with runtime contextual changes in their environment and offer a personalized product to the user. However, taking user preferences and context into account impedes the manual configuration process, and thus, an efficient and automated procedure is required. To automate the configuration process, context-aware recommendation techniques have been acknowledged as an effective mean to provide suggestions to a user based on their recognized context. In this work, we propose a collaborative filtering method based on tensor factorization that allows an integration of contextual data by modeling an N-dimensional tensor User-Feature-Context instead of the traditional two-dimensional User-Feature matrix. In the proposed approach, different types of non-functional properties are considered as additional contextual dimensions. Moreover, we show how to self-configure software product lines by applying our N-dimensional tensor factorization recommendation approach. We evaluate our approach by means of an empirical study using two datasets of configurations derived for medium-sized product lines. Our results reveal significant improvements in the predictive accuracy of the configuration over a state-of-the-art non-contextual matrix factorization approach. Moreover, it can scale up to a 7-dimensional tensor containing hundred of configurations in a couple of milliseconds.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {87–97},
numpages = {11},
keywords = {software product lines, self-configuration, runtime decision-making, recommender systems},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3233027.3233035,
author = {Varshosaz, Mahsa and Al-Hajjaji, Mustafa and Th\"{u}m, Thomas and Runge, Tobias and Mousavi, Mohammad Reza and Schaefer, Ina},
title = {A classification of product sampling for software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233035},
doi = {10.1145/3233027.3233035},
abstract = {The analysis of software product lines is challenging due to the potentially large number of products, which grow exponentially in terms of the number of features. Product sampling is a technique used to avoid exhaustive testing, which is often infeasible. In this paper, we propose a classification for product sampling techniques and classify the existing literature accordingly. We distinguish the important characteristics of such approaches based on the information used for sampling, the kind of algorithm, and the achieved coverage criteria. Furthermore, we give an overview on existing tools and evaluations of product sampling techniques. We share our insights on the state-of-the-art of product sampling and discuss potential future work.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {1–13},
numpages = {13},
keywords = {testing, software product lines, sampling algorithms, feature interaction, domain models},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2934466.2946045,
author = {Noir, J\'{e}rome Le and Madel\'{e}nat, S\'{e}bastien and Gailliard, Gr\'{e}gory and Labreuche, Christophe and Acher, Mathieu and Barais, Olivier and Constant, Olivier},
title = {A decision-making process for exploring architectural variants in systems engineering},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946045},
doi = {10.1145/2934466.2946045},
abstract = {In systems engineering, practitioners shall explore numerous architectural alternatives until choosing the most adequate variant. The decision-making process is most of the time a manual, time-consuming, and error-prone activity. The exploration and justification of architectural solutions is ad-hoc and mainly consists in a series of tries and errors on the modeling assets. In this paper, we report on an industrial case study in which we apply variability modeling techniques to automate the assessment and comparison of several candidate architectures (variants). We first describe how we can use a model-based approach such as the Common Variability Language (CVL) to specify the architectural variability. We show that the selection of an architectural variant is a multi-criteria decision problem in which there are numerous interactions (veto, favor, complementary) between criteria.We present a tooled process for exploring architectural variants integrating both CVL and the MYRIAD method for assessing and comparing variants based on an explicit preference model coming from the elicitation of stakeholders' concerns. This solution allows understanding differences among variants and their satisfactions with respect to criteria. Beyond variant selection automation improvement, this experiment results highlight that the approach improves rationality in the assessment and provides decision arguments when selecting the preferred variants.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {277–286},
numpages = {10},
keywords = {systems engineering, multi-criteria decision analysis, model-driven engineering, design exploration, decision-making, architecture},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2647908.2655972,
author = {Meinicke, Jens and Th\"{u}m, Thomas and Schr\"{o}ter, Reimar and Benduhn, Fabian and Saake, Gunter},
title = {An overview on analysis tools for software product lines},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655972},
doi = {10.1145/2647908.2655972},
abstract = {A software product line is a set of different software products that share commonalities. For a selection of features, specialized products of one domain can be generated automatically from domain artifacts. However, analyses of software product lines need to handle a large number of products that can be exponential in the number of features. In the last decade, many approaches have been proposed to analyze software product lines efficiently. For some of these approaches tool support is available. Based on a recent survey on analysis for software product lines, we provide a first overview on such tools. While our discussion is limited to analysis tools, we provide an accompanying website covering further tools for product-line development. We compare tools according to their analysis and implementation strategy to identify underrepresented areas. In addition, we want to ease the reuse of existing tools for researchers and students, and to simplify research transfer to practice.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {94–101},
numpages = {8},
keywords = {type checking, tool support, theorem proving, testing, static analysis, software product lines, sampling, non-functional properties, model checking, code metrics},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2019136.2019158,
author = {Guana, Victor and Correal, Dario},
title = {Variability quality evaluation on component-based software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019158},
doi = {10.1145/2019136.2019158},
abstract = {Quality assurance and evaluation in Model Driven Software Product Lines (MD-SPLs) are pivotal points for the growing and solidification of the generative software factories. They are framed as one of the future fact methodologies for the construction of software systems. Although several approximations address the problem of generative environments, software product line scope expression, and core asset definition, not many of them try to solve, as a fundamental step, the automation of the quality attribute evaluation in the MD-SPL development cycle. This paper presents a model-driven engineering method and a tool for the quality evaluation of product line configurations through a cross architectural view analysis.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {19},
numpages = {8},
keywords = {sensitivity point, quality attribute, model-driven software product line, model composition, domain specific modeling},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2791060.2791099,
author = {Filho, Jo\~{a}o Bosco Ferreira and Allier, Simon and Barais, Olivier and Acher, Mathieu and Baudry, Benoit},
title = {Assessing product line derivation operators applied to Java source code: an empirical study},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791099},
doi = {10.1145/2791060.2791099},
abstract = {Product Derivation is a key activity in Software Product Line Engineering. During this process, derivation operators modify or create core assets (e.g., model elements, source code instructions, components) by adding, removing or substituting them according to a given configuration. The result is a derived product that generally needs to conform to a programming or modeling language. Some operators lead to invalid products when applied to certain assets, some others do not; knowing this in advance can help to better use them, however this is challenging, specially if we consider assets expressed in extensive and complex languages such as Java. In this paper, we empirically answer the following question: which product line operators, applied to which program elements, can synthesize variants of programs that are incorrect, correct or perhaps even conforming to test suites? We implement source code transformations, based on the derivation operators of the Common Variability Language. We automatically synthesize more than 370,000 program variants from a set of 8 real large Java projects (up to 85,000 lines of code), obtaining an extensive panorama of the sanity of the operations.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {36–45},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2934466.2946046,
author = {Arrieta, Aitor and Wang, Shuai and Sagardui, Goiuria and Etxeberria, Leire},
title = {Search-based test case selection of cyber-physical system product lines for simulation-based validation},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946046},
doi = {10.1145/2934466.2946046},
abstract = {Cyber-Physical Systems (CPSs) are often tested at different test levels following "X-in-the-Loop" configurations: Model-, Software- and Hardware-in-the-loop (MiL, SiL and HiL). While MiL and SiL test levels aim at testing functional requirements at the system level, the HiL test level tests functional as well as non-functional requirements by performing a real-time simulation. As testing CPS product line configurations is costly due to the fact that there are many variants to test, test cases are long, the physical layer has to be simulated and co-simulation is often necessary. It is therefore extremely important to select the appropriate test cases that cover the objectives of each level in an allowable amount of time. We propose an efficient test case selection approach adapted to the "X-in-the-Loop" test levels. Search algorithms are employed to reduce the amount of time required to test configurations of CPS product lines while achieving the test objectives of each level. We empirically evaluate three commonly-used search algorithms, i.e., Genetic Algorithm (GA), Alternating Variable Method (AVM) and Greedy (Random Search (RS) is used as a baseline) by employing two case studies with the aim of integrating the best algorithm into our approach. Results suggest that as compared with RS, our approach can reduce the costs of testing CPS product line configurations by approximately 80% while improving the overall test quality.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {297–306},
numpages = {10},
keywords = {test case selection, search-based software engineering, cyber-physical system product lines},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2491627.2491630,
author = {Linsbauer, Lukas and Lopez-Herrejon, E. Roberto and Egyed, Alexander},
title = {Recovering traceability between features and code in product variants},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491630},
doi = {10.1145/2491627.2491630},
abstract = {Many companies offer a palette of similar software products though they do not necessarily have a Software Product Line (SPL). Rather, they start building and selling individual products which they then adapt, customize and extend for different customers. As the number of product variants increases, these companies then face the severe problem of having to maintain them all. Software Product Lines can be helpful here - not so much as a platform for creating new products but as a means of maintaining the existing ones with their shared features. Here, an important first step is to determine where features are implemented in the source code and in what product variants. To this end, this paper presents a novel technique for deriving the traceability between features and code in product variants by matching code overlaps and feature overlaps. This is a difficult problem because a feature's implementation not only covers its basic functionality (which does not change across product variants) but may include code that deals with feature interaction issues and thus changes depending on the combination of features present in a product variant. We empirically evaluated the approach on three non-trivial case studies of different sizes and domains and found that our approach correctly identifies feature to code traces except for code that traces to multiple disjunctive features, a rare case involving less than 1% of the code.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {131–140},
numpages = {10},
keywords = {traceability, product variants, features},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2362536.2362567,
author = {Savolainen, Juha and Mannion, Mike and Kuusela, Juha},
title = {Developing platforms for multiple software product lines},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362567},
doi = {10.1145/2362536.2362567},
abstract = {Many approaches to software product line engineering have been founded on the development of a single product line platform. However as customer requirements change and new products are added to the product line, software producers recognize that the platform cannot be "stretched" indefinitely and a significant problem is striking a balance between development efficiency by increasing platform commonality and customer dissatisfaction from products with additional undesirable features and properties.One alternative is to develop multiple product lines (MPLs). However the challenge remains about what to include in a multiple product line platform. Drawing upon industrial experience of working with 4 companies, this paper explores the characteristics of the contexts in which MPLs are a viable alternative development strategy and then proposes a framework of approaches to platform development.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {220–228},
numpages = {9},
keywords = {software reuse, multiple product lines, industrial experience},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1145/3361146,
author = {Hierons, Robert M. and Li, Miqing and Liu, Xiaohui and Parejo, Jose Antonio and Segura, Sergio and Yao, Xin},
title = {Many-Objective Test Suite Generation for Software Product Lines},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3361146},
doi = {10.1145/3361146},
abstract = {A Software Product Line (SPL) is a set of products built from a number of features, the set of valid products being defined by a feature model. Typically, it does not make sense to test all products defined by an SPL and one instead chooses a set of products to test (test selection) and, ideally, derives a good order in which to test them (test prioritisation). Since one cannot know in advance which products will reveal faults, test selection and prioritisation are normally based on objective functions that are known to relate to likely effectiveness or cost. This article introduces a new technique, the grid-based evolution strategy (GrES), which considers several objective functions that assess a selection or prioritisation and aims to optimise on all of these. The problem is thus a many-objective optimisation problem. We use a new approach, in which all of the objective functions are considered but one (pairwise coverage) is seen as the most important. We also derive a novel evolution strategy based on domain knowledge. The results of the evaluation, on randomly generated and realistic feature models, were promising, with GrES outperforming previously proposed techniques and a range of many-objective optimisation algorithms.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {2},
numpages = {46},
keywords = {test selection, test prioritisation, multi-objective optimisation, Software product line}
}

@inproceedings{10.1145/2362536.2362560,
author = {Lettner, Daniela and Vierhauser, Michael and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {Supporting end users with business calculations in product configuration},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362560},
doi = {10.1145/2362536.2362560},
abstract = {Business calculations like break-even, return on investment, or cost are essential in many domains to support decision making while configuring products. For instance, customers and sales people need to estimate and compare the business value of different product variants. Some product line approaches provide initial support, e.g., by defining quality attributes in relation to features. However, an approach that allows domain engineers to easily define business calculations together with variability models is still lacking. In product configuration, calculation results need to be instantly presented to end users after making configuration choices. Further, due to the often high number of calculations, the presentation of calculation results to end users can be challenging. These challenges cannot be addressed by integrating off-the-shelf applications performing the calculations with product line tools. We thus present an approach based on dedicated calculation models that are related to variability models. Our approach seamlessly integrates business calculations with product configuration and provides support for formatting calculations and calculation results. We use the DOPLER tool suite to deploy calculations together with variability models to end users in product configuration. We evaluate the expressiveness and practical relevance of the approach by investigating the development of business calculations for 15 product lines from the domain of industrial automation.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {171–180},
numpages = {10},
keywords = {variability models, product configuration, business calculations},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1145/2853073.2853082,
author = {Soujanya, K. L.S. and AnandaRao, A.},
title = {A Generic Framework for Configuration Management of SPL and Controlling Evolution of Complex Software Products},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853082},
doi = {10.1145/2853073.2853082},
abstract = {Efficient configuration management system is crucial for the success of any software product line (SPL). Due to ever changing needs of customers, SPL undergoes constant changes that are to be tracked in real time. In the context of customer-driven development, anticipation and change management are to be given paramount importance. It demands implementation of software variability that drives home changed, extended and customized configurations besides economy at scale. Moreover, the emergence of distributed technologies, the unprecedented growth of component based, serviceoriented systems throw ever increasing challenges to software product line configuration management. Derivation of a new product is a dynamic process in software product line that should consider functionality and quality attributes. Very few approaches are found on configuration management (CM) of SPL though CM is enough matured for traditional products. They are tailor made and inadequate to provide a general solution. Stated differently, a comprehensive approach for SPL configuration management and product derivation is still to be desired. In this paper, we proposed a framework that guides in doing so besides helping in SPL definitions in generic way. Our framework facilitates SPL configuration management and product derivation based on critical path analysis, weight computation and feedback. We proposed two algorithms namely Quality Driven Product Derivation (QDPD) and Composition Analysis algorithm for generating satisfied compositions and to find best possible composition respectively. The usage of weights and critical path analysis improves quality of product derivation. The framework is extensible and flexible thus it can be leveraged with variability-aware design patterns and ontology. We built a prototype that demonstrates the proof of concept. We tested our approach with Dr. School product line. The results reveal that the framework supports configuration management of SPL and derivation of high quality product in the product line. We evaluated results with ground truth to establish significance of our implementation},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–10},
numpages = {10},
keywords = {weighted approach, product derivation, critical path analysis, configuration management, Software product line}
}

@inproceedings{10.1145/2491627.2491647,
author = {Murashkin, Alexandr and Antkiewicz, Micha\l{} and Rayside, Derek and Czarnecki, Krzysztof},
title = {Visualization and exploration of optimal variants in product line engineering},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491647},
doi = {10.1145/2491627.2491647},
abstract = {The decision-making process in Product Line Engineering (PLE) is often concerned with variant qualities such as cost, battery life, or security. Pareto-optimal variants, with respect to a set of objectives such as minimizing a variant's cost while maximizing battery life and security, are variants in which no single quality can be improved without sacrificing other qualities. We propose a novel method and a tool for visualization and exploration of a multi-dimensional space of optimal variants (i.e., a Pareto front). The visualization method is an integrated, interactive, and synchronized set of complementary views onto a Pareto front specifically designed to support PLE scenarios, including: understanding differences among variants and their positioning with respect to quality dimensions; solving trade-offs; selecting the most desirable variants; and understanding the impact of changes during product line evolution on a variant's qualities. We present an initial experimental evaluation showing that the visualization method is a good basis for supporting these PLE scenarios.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {111–115},
numpages = {5},
keywords = {visualization, product line engineering, pareto front, optimal variant, feature modeling, exploration, clafer, ClaferMoo visualizer, ClaferMoo},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3180155.3180159,
author = {Krieter, Sebastian and Th\"{u}m, Thomas and Schulze, Sandro and Schr\"{o}ter, Reimar and Saake, Gunter},
title = {Propagating configuration decisions with modal implication graphs},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180159},
doi = {10.1145/3180155.3180159},
abstract = {Highly-configurable systems encompass thousands of interdependent configuration options, which require a non-trivial configuration process. Decision propagation enables a backtracking-free configuration process by computing values implied by user decisions. However, employing decision propagation for large-scale systems is a time-consuming task and, thus, can be a bottleneck in interactive configuration processes and analyses alike. We propose modal implication graphs to improve the performance of decision propagation by precomputing intermediate values used in the process. Our evaluation results show a significant improvement over state-of-the-art algorithms for 120 real-world systems.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {898–909},
numpages = {12},
keywords = {configuration, decision propagation, software product line},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3194078.3194082,
author = {Pukhkaiev, Dmytro and G\"{o}tz, Sebastian},
title = {BRISE: energy-efficient benchmark reduction},
year = {2018},
isbn = {9781450357326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194078.3194082},
doi = {10.1145/3194078.3194082},
abstract = {A considerable portion of research activities in computer science heavily relies on the process of benchmarking, e.g., to evaluate a hypothesis in an empirical study. The goal is to reveal how a set of independent variables (factors) influences one or more dependent variables. With a vast number of factors or a high amount of factors' values (levels), this process becomes time- and energy-consuming. Current approaches to lower the benchmarking effort suffer from two deficiencies: (1) they focus on reducing the number of factors and, hence, are inapplicable to experiments with only two factors, but a vast number of levels and (2) being adopted from, e.g., combinatorial optimization they are designed for a different search space structure and, thus, can be very wasteful. This paper provides an approach for benchmark reduction, based on adaptive instance selection and multiple linear regression. We evaluate our approach using four empirical studies, which investigate the effect made by dynamic voltage and frequency scaling in combination with dynamic concurrency throttling on the energy consumption of a computing system (parallel compression, sorting, and encryption algorithms as well as database query processing). Our findings show the effectiveness of the approach. We can save 78% of benchmarking effort, while the result's quality decreases only by 3 pp, due to using only a near-optimal configuration.},
booktitle = {Proceedings of the 6th International Workshop on Green and Sustainable Software},
pages = {23–30},
numpages = {8},
keywords = {non-functional properties, fractional factorial design, benchmarking, adaptive instance selection, active learning},
location = {Gothenburg, Sweden},
series = {GREENS '18}
}

@inproceedings{10.1145/3555776.3578613,
author = {T\"{e}rnava, Xhevahire and Acher, Mathieu and Combemale, Benoit},
title = {Specialization of Run-time Configuration Space at Compile-time: An Exploratory Study},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3578613},
doi = {10.1145/3555776.3578613},
abstract = {Numerous software systems are highly configurable through runtime options (e.g., command-line parameters). Users can tune some of the options to meet various functional and non-functional requirements such as footprint, security, or execution time. However, some options are never set for a given system instance, and their values remain the same whatever the use cases of the system. Herein, we design a controlled experiment in which the system's run-time configuration space can be specialized at compile-time and combinations of options can be removed on demand. We perform an in-depth study of the well-known x264 video encoder and quantify the effects of its specialization to its non-functional properties, namely on binary size, attack surface, and performance while ensuring its validity. Our exploratory study suggests that the configurable specialization of a system has statistically significant benefits on most of its analysed non-functional properties, which benefits depend on the number of the debloated options. While our empirical results and insights show the importance of removing code related to unused run-time options to improve software systems, an open challenge is to further automate the specialization process.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1459–1468},
numpages = {10},
keywords = {unused variability, debloating, performance, program specialization},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/2814251.2814263,
author = {Ochoa, Lina and Gonz\'{a}lez-Rojas, Oscar and Th\"{u}m, Thomas},
title = {Using decision rules for solving conflicts in extended feature models},
year = {2015},
isbn = {9781450336864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814251.2814263},
doi = {10.1145/2814251.2814263},
abstract = {Software Product Line Engineering has introduced feature modeling as a domain analysis technique used to represent the variability of software products and decision-making scenarios. We present a model-based transformation approach to solve conflicts among configurations performed by different stakeholders on feature models. We propose the usage of a domain-specific language named CoCo to specify attributes as non-functional properties of features, and to describe business-related decision rules in terms of costs, time, and human resources. These specifications along with the stakeholders' configurations and the feature model are transformed into a constraint programming problem, on which decision rules are executed to find a non-conflicting set of solution configurations that are aligned to business objectives. We evaluate CoCo's compositionality and model complexity simplification while using a set of motivating decision scenarios.},
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Software Language Engineering},
pages = {149–160},
numpages = {12},
keywords = {model transformation chain, extended feature model, domain-specific language, constraint satisfaction problem, conflicting configurations, Domain engineering},
location = {Pittsburgh, PA, USA},
series = {SLE 2015}
}

@inproceedings{10.1145/3023956.3023968,
author = {Mjeda, Anila and Wasala, Asanka and Botterweck, Goetz},
title = {Decision spaces in product lines, decision analysis, and design exploration: an interdisciplinary exploratory study},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023968},
doi = {10.1145/3023956.3023968},
abstract = {Context. From recent works on product properties resulting from configurations and the optimisation of these properties, one comes quickly to more complex challenges such as multi-objective optimisation, conflicting objectives, multiple stakeholders, and conflict resolution. The intuition is that Software Product Line Engineering (SPLE) can draw from other disciplines that deal with decision spaces and complex decision scenarios.Objectives. We aim to (1) explore links to such disciplines, (2) systematise and compare concepts, and (3) identify opportunities, where SPLE approaches can be enriched.Method. We undertake an exploratory study: Starting from common SPLE activities and artefacts, we identify aspects where we expect to find corresponding counterparts in other disciplines. We focus on Multiple Criteria Decision Analysis (MCDA), Multi-Objective Optimisation (MOO), and Design Space Exploration (DSE), and perform a comparison of the key concepts.Results. The resulting comparison relates SPLE activities and artefacts to concepts from MCDA, MOO, and DSE and identifies areas where SPLE approaches can be enriched. We also provide examples of existing work at the intersections of SPLE with the other fields. These findings are aimed to foster the conversation on research opportunities where SPLE can draw techniques from other disciplines dealing with complex decision scenarios.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {68–75},
numpages = {8},
keywords = {multi-objective optimisation, multi-criteria decision analysis, design-space exploration, decision modelling},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/1982185.1982522,
author = {Mohabbati, Bardia and Hatala, Marek and Ga\v{s}evi\'{c}, Dragan and Asadi, Mohsen and Bo\v{s}kovi\'{c}, Marko},
title = {Development and configuration of service-oriented systems families},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982522},
doi = {10.1145/1982185.1982522},
abstract = {Software Product Lines (SPLs) are families of software systems which share a common sets of feature and are developed through common set of core assets in order to promotes software reusability, mass customization, reducing cost, time-to-market and improving the quality of the product. SPLs are sets (i.e., families) of software applications developed as a whole for a specific business domain. Particular applications are derived from software families by selecting the desired features through configuration process. Traditionally, SPLs are implemented with systematically developed components, shared by members of the SPLs and reused every time a new application is derived. In this paper, we propose an approach to the development and configuration of Service-Oriented SPLs in which services are used as reusable assets and building blocks of implementation. Our proposed approach also suggests prioritization of family features according to stakeholder's non-functional requirements (NFRs) and preferences. Priorities of NFRs are used to filter the most important features of the family, which is performed by Stratified Analytic Hierarchical Process (S-AHP). The priorities also are used further for the selection of appropriate services implementation for business processes realizing features. We apply Mixed Integer Linear Programming to find the optimal service selection within the constraints boundaries specified by stakeholders.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {1606–1613},
numpages = {8},
keywords = {software product line, service-oriented architecture, service selection, optimization, feature-oriented development},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@inproceedings{10.1145/2556624.2556628,
author = {Lengauer, Philipp and Bitto, Verena and Angerer, Florian and Gr\"{u}nbacher, Paul and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {Where has all my memory gone? determining memory characteristics of product variants using virtual-machine-level monitoring},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556628},
doi = {10.1145/2556624.2556628},
abstract = {Non-functional properties such as memory footprint have recently gained importance in software product line research. However, determining the memory characteristics of individual features and product variants is extremely challenging. We present an approach that supports the monitoring of memory characteristics of individual features at the level of Java virtual machines. Our approach provides extensions to Java virtual machines to track memory allocations and deal-locations of individual features based on a feature-to-code mapping. The approach enables continuous monitoring at the level of features to detect anomalies such as memory leaks, excessive memory consumption, or abnormal garbage collection times in product variants. We provide an evaluation of our approach based on different product variants of the DesktopSearcher product line. Our experiment with different program inputs demonstrates the feasibility of our technique.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {13},
numpages = {8},
keywords = {monitoring, memory footprint, feature-oriented software development, Java},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@inproceedings{10.1145/3510003.3510200,
author = {Dubslaff, Clemens and Weis, Kallistos and Baier, Christel and Apel, Sven},
title = {Causality in configurable software systems},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510200},
doi = {10.1145/3510003.3510200},
abstract = {Detecting and understanding reasons for defects and inadvertent behavior in software is challenging due to their increasing complexity. In configurable software systems, the combinatorics that arises from the multitude of features a user might select from adds a further layer of complexity. We introduce the notion of feature causality, which is based on counterfactual reasoning and inspired by the seminal definition of actual causality by Halpern and Pearl. Feature causality operates at the level of system configurations and is capable of identifying features and their interactions that are the reason for emerging functional and non-functional properties. We present various methods to explicate these reasons, in particular well-established notions of responsibility and blame that we extend to the feature-oriented setting. Establishing a close connection of feature causality to prime implicants, we provide algorithms to effectively compute feature causes and causal explications. By means of an evaluation on a wide range of configurable software systems, including community benchmarks and real-world systems, we demonstrate the feasibility of our approach: We illustrate how our notion of causality facilitates to identify root causes, estimate the effects of features, and detect feature interactions.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {325–337},
numpages = {13},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/1385486.1385488,
author = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Schirmeier, Horst and Sincero, Julio and Apel, Sven and Leich, Thomas and Spinczyk, Olaf and Saake, Gunter},
title = {FAME-DBMS: tailor-made data management solutions for embedded systems},
year = {2008},
isbn = {9781595939647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1385486.1385488},
doi = {10.1145/1385486.1385488},
abstract = {Data management functionality is not only needed in large-scale server systems, but also in embedded systems. Resource restrictions and heterogeneity of hardware, however, complicate the development of data management solutions for those systems. In current practice, this typically leads to the redevelopment of data management because existing solutions cannot be reused and adapted appropriately. In this paper, we present our ongoing work on FAME-DBMS, a research project that explores techniques to implement highly customizable data management solutions, and illustrate how such systems can be created with a software product line approach. With this approach a concrete instance of a DBMS is derived by composing features of the DBMS product line that are needed for a specific application scenario. This product derivation process is getting complex if a large number of features is available. Furthermore, in embedded systems also non-functional properties, e.g., memory consumption, have to be considered when creating a DBMS instance. To simplify the derivation process we present approaches for its automation.},
booktitle = {Proceedings of the 2008 EDBT Workshop on Software Engineering for Tailor-Made Data Management},
pages = {1–6},
numpages = {6},
location = {Nantes, France},
series = {SETMDM '08}
}

@inproceedings{10.1145/2884781.2884821,
author = {Devroey, Xavier and Perrouin, Gilles and Papadakis, Mike and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Featured model-based mutation analysis},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884821},
doi = {10.1145/2884781.2884821},
abstract = {Model-based mutation analysis is a powerful but expensive testing technique. We tackle its high computation cost by proposing an optimization technique that drastically speeds up the mutant execution process. Central to this approach is the Featured Mutant Model, a modelling framework for mutation analysis inspired by the software product line paradigm. It uses behavioural variability models, viz., Featured Transition Systems, which enable the optimized generation, configuration and execution of mutants. We provide results, based on models with thousands of transitions, suggesting that our technique is fast and scalable. We found that it outperforms previous approaches by several orders of magnitude and that it makes higher-order mutation practically applicable.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {655–666},
numpages = {12},
keywords = {variability, mutation analysis, featured transition systems},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/2420942.2420944,
author = {Olaechea, Rafael and Stewart, Steven and Czarnecki, Krzysztof and Rayside, Derek},
title = {Modelling and multi-objective optimization of quality attributes in variability-rich software},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420944},
doi = {10.1145/2420942.2420944},
abstract = {Variability-rich software, such as software product lines, offers optional and alternative features to accommodate varying needs of users. Designers of variability-rich software face the challenge of reasoning about the impact of selecting such features on the quality attributes of the resulting software variant. Attributed feature models have been proposed to model such features and their impact on quality attributes, but existing variability modelling languages and tools have limited or no support for such models and the complex multi-objective optimization problem that arises. This paper presents ClaferMoo, a language and tool that addresses these shortcomings. ClaferMoo uses type inheritance to modularize the attribution of features in feature models and allows specifying multiple optimization goals. We evaluate an implementation of the language on a set of attributed feature models from the literature, showing that the optimization infrastructure can handle small-scale feature models with about a dozen features within seconds.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {2},
numpages = {6},
keywords = {software product lines, multi-objective optimization},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@inproceedings{10.1109/ICSE.2019.00112,
author = {Kaltenecker, Christian and Grebhahn, Alexander and Siegmund, Norbert and Guo, Jianmei and Apel, Sven},
title = {Distance-based sampling of software configuration spaces},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00112},
doi = {10.1109/ICSE.2019.00112},
abstract = {Configurable software systems provide a multitude of configuration options to adjust and optimize their functional and non-functional properties. For instance, to find the fastest configuration for a given setting, a brute-force strategy measures the performance of all configurations, which is typically intractable. Addressing this challenge, state-of-the-art strategies rely on machine learning, analyzing only a few configurations (i.e., a sample set) to predict the performance of other configurations. However, to obtain accurate performance predictions, a representative sample set of configurations is required. Addressing this task, different sampling strategies have been proposed, which come with different advantages (e.g., covering the configuration space systematically) and disadvantages (e.g., the need to enumerate all configurations). In our experiments, we found that most sampling strategies do not achieve a good coverage of the configuration space with respect to covering relevant performance values. That is, they miss important configurations with distinct performance behavior. Based on this observation, we devise a new sampling strategy, called distance-based sampling, that is based on a distance metric and a probability distribution to spread the configurations of the sample set according to a given probability distribution across the configuration space. This way, we cover different kinds of interactions among configuration options in the sample set. To demonstrate the merits of distance-based sampling, we compare it to state-of-the-art sampling strategies, such as t-wise sampling, on 10 real-world configurable software systems. Our results show that distance-based sampling leads to more accurate performance models for medium to large sample sets.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1084–1094},
numpages = {11},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.5555/2667025.2667027,
author = {Siegmund, Norbert and Mory, Maik and Feigenspan, Janet and Saake, Gunter and Nykolaychuk, Mykhaylo and Schumann, Marco},
title = {Interoperability of non-functional requirements in complex systems},
year = {2012},
isbn = {9781467318532},
publisher = {IEEE Press},
abstract = {Heterogeneity of embedded systems leads to the development of variable software, such as software product lines. From such a family of programs, stakeholders select the specific variant that satisfies their functional requirements. However, different functionality exposes different non-functional properties of these variants. Especially in the embedded-system domain, non-functional requirements are vital, because resources are scarce. Hence, when selecting an appropriate variant, we have to fulfill also non-functional requirements. Since more systems are interconnected, the challenge is to find a variant that additionally satisfies global nonfunctional (or quality) requirements. In this paper, we advert the problem of achieving interoperability of non-functional requirements among multiple interacting systems using a real-world scenario. Furthermore, we show an approach to find optimal variants for multiple systems that reduces computation effort by means of a stepwise configuration process.},
booktitle = {Proceedings of the Second International Workshop on Software Engineering for Embedded Systems},
pages = {2–8},
numpages = {7},
location = {Zurich, Switzerland},
series = {SEES '12}
}

@inproceedings{10.1145/2110147.2110160,
author = {Schroeter, Julia and Cech, Sebastian and G\"{o}tz, Sebastian and Wilke, Claas and A\ss{}mann, Uwe},
title = {Towards modeling a variable architecture for multi-tenant SaaS-applications},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110160},
doi = {10.1145/2110147.2110160},
abstract = {A widespread business model in cloud computing is to offer software as a service (SaaS) over the Internet. Such applications are often multi-tenant aware, which means that multiple tenants share hardware and software resources of the same application instance. However, SaaS stakeholders have different or even contradictious requirements and interests: For a user, the application's quality and non-functional properties have to be maximized (e.g., choosing the fastest available algorithm for a computation at runtime). In contrast, a resource or application provider is interested in minimizing the operating costs while maximizing his profit. Finally, tenants are interested in offering a customized functionality to their users. To identify an optimal compromise for all these objectives, multiple levels of variability have to be supported by reference architectures for multi-tenant SaaS applications. In this paper, we identify requirements for such a runtime architecture addressing the individual interests of all involved stakeholders. Furthermore, we show how our existing architecture for dynamically adaptive applications can be extended for the development and operation of multi-tenant applications.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {111–120},
numpages = {10},
keywords = {variability modeling, software-as-a-service, self-optimization, multi-tenancy, auto-tuning},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@inproceedings{10.1145/1837154.1837157,
author = {Siegmund, Norbert and Feigenspan, Janet and Soffner, Michael and Fruth, Jana and K\"{o}ppen, Veit},
title = {Challenges of secure and reliable data management in heterogeneous environments},
year = {2010},
isbn = {9781605589923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1837154.1837157},
doi = {10.1145/1837154.1837157},
abstract = {Ubiquitous computing is getting more important since requirements for complex systems grow fast. In these systems, embedded devices have to fulfill different tasks. They have to monitor the environment, store data, communicate with other devices, and react to user input. In addition to this complexity, quality issues such as security and reliability have to be considered, as well, due to their increasing use in life critical application scenarios. Finally, different devices with different application goals are used, which results in interoperability problems. In this paper, we highlight challenges for interoperability, data management, and security, which arise with complex systems. Furthermore, we present approaches to overcome different problems and how an integrated solution can be realized using software product line techniques.},
booktitle = {Proceedings of the First International Workshop on Digital Engineering},
pages = {17–24},
numpages = {8},
keywords = {software product lines, security, digital engineering, data management},
location = {Magdeburg, Germany},
series = {IWDE '10}
}

@inproceedings{10.1145/2892664.2892686,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Towards the dynamic reconfiguration of quality attributes},
year = {2016},
isbn = {9781450340335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2892664.2892686},
doi = {10.1145/2892664.2892686},
abstract = {There are some Quality Attributes (QAs) whose variability is addressed through functional variability in the software architecture. Separately modelling the variability of these QAs from the variability of the base functionality of the application has many advantages (e.g., a better reusability), and facilitates the reconfiguration of the QA variants at runtime. Many factors may vary the QA functionality: variations in the user preferences and usage needs; variations in the non-functional QAs; variations in resources, hardware, or even in the functionality of the base application, that directly affect the product's QAs. In this paper, we aim to elicit the relationships and dependencies between the functionalities required to satisfy the QAs and all those factors that can provoke a reconfiguration of the software architecture at runtime. We follow an approach in which the variability of the QAs is modelled separately from the base application functionality, and propose a dynamic approach to reconfigure the software architecture based on those reconfiguration criteria.},
booktitle = {Companion Proceedings of the 15th International Conference on Modularity},
pages = {131–136},
numpages = {6},
keywords = {variability, software architecture, reconfiguration, SPL, Quality attributes},
location = {M\'{a}laga, Spain},
series = {MODULARITY Companion 2016}
}

@inproceedings{10.1145/2577080.2577095,
author = {Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha and Baier, Christel},
title = {Probabilistic model checking for energy analysis in software product lines},
year = {2014},
isbn = {9781450327725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2577080.2577095},
doi = {10.1145/2577080.2577095},
abstract = {In a software product line (SPL), a collection of software products is defined by their commonalities in terms of features rather than explicitly specifying all products one-by-one. Several verification techniques were adapted to establish temporal properties of SPLs. Symbolic and family-based model checking have been proven to be successful for tackling the combinatorial blow-up arising when reasoning about several feature combinations. However, most formal verification approaches for SPLs presented in the literature focus on the static SPLs, where the features of a product are fixed and cannot be changed during runtime. This is in contrast to dynamic SPLs, allowing to adapt feature combinations of a product dynamically after deployment.The main contribution of the paper is a compositional modeling framework for dynamic SPLs, which supports probabilistic and nondeterministic choices and allows for quantitative analysis. We specify the feature changes during runtime within an automata-based coordination component, enabling to reason over strategies how to trigger dynamic feature changes for optimizing various quantitative objectives, e.g., energy or monetary costs and reliability. For our framework there is a natural and conceptually simple translation into the input language of the prominent probabilistic model checker PRISM. This facilitates the application of PRISM's powerful symbolic engine to the operational behavior of dynamic SPLs and their family-based analysis against various quantitative queries. We demonstrate feasibility of our approach by a case study issuing an energy-aware bonding network device.},
booktitle = {Proceedings of the 13th International Conference on Modularity},
pages = {169–180},
numpages = {12},
keywords = {software product lines, probabilistic model checking, energy analysis, dynamic features},
location = {Lugano, Switzerland},
series = {MODULARITY '14}
}

@article{10.1145/3528100,
author = {Cheng, Jiezhu and Gao, Cuiyun and Zheng, Zibin},
title = {HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3528100},
doi = {10.1145/3528100},
abstract = {Modern software systems are usually highly configurable, providing users with customized functionality through various configuration options. Understanding how system performance varies with different option combinations is important to determine optimal configurations that meet specific requirements. Due to the complex interactions among multiple options and the high cost of performance measurement under a huge configuration space, it is challenging to study how different configurations influence the system performance. To address these challenges, we propose HINNPerf, a novel hierarchical interaction neural network for performance prediction of configurable systems. HINNPerf employs the embedding method and hierarchic network blocks to model the complicated interplay between configuration options, which improves the prediction accuracy of the method. In addition, we devise a hierarchical regularization strategy to enhance the model robustness. Empirical results on 10 real-world configurable systems show that our method statistically significantly outperforms state-of-the-art approaches by achieving average 22.67% improvement in prediction accuracy. In addition, combined with the Integrated Gradients method, the designed hierarchical architecture provides some insights about the interaction complexity and the significance of configuration options, which might help users and developers better understand how the configurable system works and efficiently identify significant options affecting the performance.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {46},
numpages = {30},
keywords = {machine learning, deep neural network, highly configurable systems, Software performance prediction}
}

@inproceedings{10.1145/3510455.3512792,
author = {Randrianaina, Georges Aaron and Khelladi, Djamel Eddine and Zendra, Olivier and Acher, Mathieu},
title = {Towards incremental build of software configurations},
year = {2022},
isbn = {9781450392242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510455.3512792},
doi = {10.1145/3510455.3512792},
abstract = {Building software is a crucial task to compile, test, and deploy software systems while continuously ensuring quality. As software is more and more configurable, building multiple configurations is a pressing need, yet, costly and challenging to instrument. The common practice is to independently build (a.k.a., clean build) a software for a subset of configurations. While incremental build has been considered for software evolution and relatively small modifications of the source code, it has surprisingly not been considered for software configurations. In this vision paper, we formulate the hypothesis that incremental build can reduce the cost of exploring the configuration space of software systems. We detail how we apply incremental build for two real-world application scenarios and conduct a preliminary evaluation on two case studies, namely x264 and Linux Kernel. For x264, we found that one can incrementally build configurations in an order such that overall build time is reduced. Nevertheless, we could not find any optimal order with the Linux Kernel, due to a high distance between random configurations. Therefore, we show it is possible to control the process of generating configurations: we could reuse commonality and gain up to 66% of build time compared to only clean builds.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {101–105},
numpages = {5},
keywords = {incremental build, highly configurable system, build system},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-NIER '22}
}

@proceedings{10.1145/3571788,
title = {VaMoS '23: Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Odense, Denmark}
}

@inproceedings{10.1145/2884781.2884861,
author = {Tan, Tian Huat and Chen, Manman and Sun, Jun and Liu, Yang and Andr\'{e}, \'{E}tienne and Xue, Yinxing and Dong, Jin Song},
title = {Optimizing selection of competing services with probabilistic hierarchical refinement},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884861},
doi = {10.1145/2884781.2884861},
abstract = {Recently, many large enterprises (e.g., Netflix, Amazon) have decomposed their monolithic application into services, and composed them to fulfill their business functionalities. Many hosting services on the cloud, with different Quality of Service (QoS) (e.g., availability, cost), can be used to host the services. This is an example of competing services. QoS is crucial for the satisfaction of users. It is important to choose a set of services that maximize the overall QoS, and satisfy all QoS requirements for the service composition. This problem, known as optimal service selection, is NP-hard. Therefore, an effective method for reducing the search space and guiding the search process is highly desirable. To this end, we introduce a novel technique, called Probabilistic Hierarchical Refinement (ProHR). ProHR effectively reduces the search space by removing competing services that cannot be part of the selection. ProHR provides two methods, probabilistic ranking and hierarchical refinement, that enable smart exploration of the reduced search space. Unlike existing approaches that perform poorly when QoS requirements become stricter, ProHR maintains high performance and accuracy, independent of the strictness of the QoS requirements. ProHR has been evaluated on a publicly available dataset, and has shown significant improvement over existing approaches.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {85–95},
numpages = {11},
location = {Austin, Texas},
series = {ICSE '16}
}

@proceedings{10.1145/3634713,
title = {VaMoS '24: Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bern, Switzerland}
}

@inproceedings{10.1145/3510003.3510190,
author = {Randrianaina, Georges Aaron and T\"{e}rnava, Xhevahire and Khelladi, Djamel Eddine and Acher, Mathieu},
title = {On the benefits and limits of incremental build of software configurations: an exploratory study},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510190},
doi = {10.1145/3510003.3510190},
abstract = {Software projects use build systems to automate the compilation, testing, and continuous deployment of their software products. As software becomes increasingly configurable, the build of multiple configurations is a pressing need, but expensive and challenging to implement. The current state of practice is to build independently (a.k.a., clean build) a software for a subset of configurations. While incremental build has been studied for software evolution and relatively small changes of the source code, it has surprisingly not been considered for software configurations. In this exploratory study, we examine the benefits and limits of building software configurations incrementally, rather than always building them cleanly. By using five real-life configurable systems as subjects, we explore whether incremental build works, outperforms a sequence of clean builds, is correct w.r.t. clean build, and can be used to find an optimal ordering for building configurations. Our results show that incremental build is feasible in 100% of the times in four subjects and in 78% of the times in one subject. In average, 88.5% of the configurations could be built faster with incremental build while also finding several alternatives faster incremental builds. However, only 60% of faster incremental builds are correct. Still, when considering those correct incremental builds with clean builds, we could always find an optimal order that is faster than just a collection of clean builds with a gain up to 11.76%.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1584–1596},
numpages = {13},
keywords = {build systems, configurable software systems, configuration build},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3023956.3023959,
author = {Ochoa, Lina and Pereira, Juliana Alves and Gonz\'{a}lez-Rojas, Oscar and Castro, Harold and Saake, Gunter},
title = {A survey on scalability and performance concerns in extended product lines configuration},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023959},
doi = {10.1145/3023956.3023959},
abstract = {Product lines have been employed as a mass customisation method that reduces production costs and time-to-market. Multiple product variants are represented in a product line, however the selection of a particular configuration depends on stakeholders' functional and non-functional requirements. Methods like constraint programming and evolutionary algorithms have been used to support the configuration process. They consider a set of product requirements like resource constraints, stakeholders' preferences, and optimization objectives. Nevertheless, scalability and performance concerns start to be an issue when facing large-scale product lines and runtime environments. Thus, this paper presents a survey that analyses strengths and drawbacks of 21 approaches that support product line configuration. This survey aims to: i) evidence which product requirements are currently supported by studied methods; ii) how scalability and performance is considered in existing approaches; and iii) point out some challenges to be addressed in future research.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {5–12},
numpages = {8},
keywords = {survey, scalability, product requirements, product line, performance, literature review, configuration},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/3442391.3442409,
author = {G\"{o}ttmann, Hendrik and Bacher, Isabelle and Gottwald, Nicolas and Lochau, Malte},
title = {Static Analysis Techniques for Efficient Consistency Checking of Real-Time-Aware DSPL Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442409},
doi = {10.1145/3442391.3442409},
abstract = {Dynamic Software Product Lines (DSPL) have recently gained momentum as integrated engineering methodology for (self-)adaptive software. DSPL enhance statically configurable software by enabling run-time reconfiguration to facilitate continuous adaptations to changing environmental contexts. In a previous work, we presented a model-based methodology for specifying and automatically analyzing real-time constraints of reconfiguration decisions in a feature-oriented and compositional way. Internally, we translate real-time-aware DSPL specifications into timed automata serving as input for off-the-shelf model checkers like Uppaal for automatically checking semantic consistency properties. However, due to the very high computational complexity of model checking timed automata, those consistency checks suffer from scalability problems thus obstructing practical applications of the proposed approach. In this paper, we tackle this issue by investigating various kinds of static-analysis techniques that (1) aim to avoid expensive model checker calls by statically detecting certain classes of inconsistencies beforehand and otherwise (2) perform model reduction by detecting and merging equivalence states prior to model checker calls. The results of our experimental evaluation show very promising performance improvements achievable by those techniques, especially by the model-reduction approach.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {17},
numpages = {9},
keywords = {Timed Automata, Reconfiguration Decisions, Dynamic Software Product Lines},
location = {Krems, Austria},
series = {VaMoS '21}
}

@article{10.1145/3487921,
author = {Hezavehi, Sara M. and Weyns, Danny and Avgeriou, Paris and Calinescu, Radu and Mirandola, Raffaela and Perez-Palacin, Diego},
title = {Uncertainty in Self-adaptive Systems: A Research Community Perspective},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/3487921},
doi = {10.1145/3487921},
abstract = {One of the primary drivers for self-adaptation is ensuring that systems achieve their goals regardless of the uncertainties they face during operation. Nevertheless, the concept of uncertainty in self-adaptive systems is still insufficiently understood. Several taxonomies of uncertainty have been proposed, and a substantial body of work exists on methods to tame uncertainty. Yet, these taxonomies and methods do not fully convey the research community’s perception on what constitutes uncertainty in self-adaptive systems and on the key characteristics of the approaches needed to tackle uncertainty. To understand this perception and learn from it, we conducted a survey comprising two complementary stages in which we collected the views of 54 and 51 participants, respectively. In the first stage, we focused on current research and development, exploring how the concept of uncertainty is understood in the community and how uncertainty is currently handled in the engineering of self-adaptive systems. In the second stage, we focused on directions for future research to identify potential approaches to dealing with unanticipated changes and other open challenges in handling uncertainty in self-adaptive systems. The key findings of the first stage are: (a) an overview of uncertainty sources considered in self-adaptive systems, (b) an overview of existing methods used to tackle uncertainty in concrete applications, (c) insights into the impact of uncertainty on non-functional requirements, (d) insights into different opinions in the perception of uncertainty within the community and the need for standardised uncertainty-handling processes to facilitate uncertainty management in self-adaptive systems. The key findings of the second stage are: (a) the insight that over 70% of the participants believe that self-adaptive systems can be engineered to cope with unanticipated change, (b) a set of potential approaches for dealing with unanticipated change, (c) a set of open challenges in mitigating uncertainty in self-adaptive systems, in particular in those with safety-critical requirements. From these findings, we outline an initial reference process to manage uncertainty in self-adaptive systems. We anticipate that the insights on uncertainty obtained from the community and our proposed reference process will inspire valuable future research on self-adaptive systems.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = dec,
articleno = {10},
numpages = {36},
keywords = {survey, uncertainty challenges, unanticipated change, uncertainty methods, uncertainty models, uncertainty, Self-adaptation}
}

@article{10.1145/3034827,
author = {Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz},
title = {CASE Tool Support for Variability Management in Software Product Lines},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3034827},
doi = {10.1145/3034827},
abstract = {Software product lines (SPL) aim at reducing time-to-market and increasing software quality through extensive, planned reuse of artifacts. An essential activity in SPL is variability management, i.e., defining and managing commonality and variability among member products. Due to the large scale and complexity of today's software-intensive systems, variability management has become increasingly complex to conduct. Accordingly, tool support for variability management has been gathering increasing momentum over the last few years and can be considered a key success factor for developing and maintaining SPLs. While several studies have already been conducted on variability management, none of these analyzed the available tool support in detail. In this work, we report on a survey in which we analyzed 37 existing variability management tools identified using a systematic literature review to understand the tools’ characteristics, maturity, and the challenges in the field. We conclude that while most studies on variability management tools provide a good motivation and description of the research context and challenges, they often lack empirical data to support their claims and findings. It was also found that quality attributes important for the practical use of tools such as usability, integration, scalability, and performance were out of scope for most studies.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {14},
numpages = {45},
keywords = {software variability, computer-aided software engineering, Software engineering}
}

@inproceedings{10.1145/3324884.3416620,
author = {Dorn, Johannes and Apel, Sven and Siegmund, Norbert},
title = {Mastering uncertainty in performance estimations of configurable software systems},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416620},
doi = {10.1145/3324884.3416620},
abstract = {Understanding the influence of configuration options on performance is key for finding optimal system configurations, system understanding, and performance debugging. In prior research, a number of performance-influence modeling approaches have been proposed, which model a configuration option's influence and a configuration's performance as a scalar value. However, these point estimates falsely imply a certainty regarding an option's influence that neglects several sources of uncertainty within the assessment process, such as (1) measurement bias, (2) model representation and learning process, and (3) incomplete data. This leads to the situation that different approaches and even different learning runs assign different scalar performance values to options and interactions among them. The true influence is uncertain, though. There is no way to quantify this uncertainty with state-of-the-art performance modeling approaches. We propose a novel approach, P4, based on probabilistic programming that explicitly models uncertainty for option influences and consequently provides a confidence interval for each prediction of a configuration's performance alongside a scalar. This way, we can explain, for the first time, why predictions may cause errors and which option's influences may be unreliable. An evaluation on 12 real-world subject systems shows that P4's accuracy is in line with the state of the art while providing reliable confidence intervals, in addition to scalar predictions.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {684–696},
numpages = {13},
keywords = {P4, configurable software systems, performance-influence modeling, probabilistic programming},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3634713.3634715,
author = {B\"{o}hm, Sabrina and Krieter, Sebastian and He\ss{}, Tobias and Th\"{u}m, Thomas and Lochau, Malte},
title = {Incremental Identification of T-Wise Feature Interactions},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634715},
doi = {10.1145/3634713.3634715},
abstract = {Developers of configurable software use the concept of selecting and deselecting features to create different variants of a software product. In this context, one of the most challenging aspects is to identify unwanted interactions between those features. Due to the combinatorial explosion of the number of potentially interacting features, it is currently an open question how to systematically identify a particular feature interaction that causes a specific fault in a set of software products. In this paper, we propose an incremental approach to identify such t-wise feature interactions based on testing additional configurations in a black-box setting. We present the algorithm Inciident, which generates and selects new configurations based on a divide-and-conquer strategy to efficiently identify the feature interaction with a preferably minimal number of configurations. We evaluate our approach by considering simulated and real interactions of different sizes for 48 real-world feature models. Our results show that on average, Inciident requires 80&nbsp;% less configurations to identify an interaction than using randomly selected configurations.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {27–36},
numpages = {10},
keywords = {Configurable Systems, Feature Interaction, Feature-Model Analysis, Software Product Lines},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.1145/3377024.3377026,
author = {Kenner, Andy and Dassow, Stephan and Lausberger, Christian and Kr\"{u}ger, Jacob and Leich, Thomas},
title = {Using variability modeling to support security evaluations: virtualizing the right attack scenarios},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377026},
doi = {10.1145/3377024.3377026},
abstract = {A software system's security is constantly threatened by vulnerabilities that result from faults in the system's design (e.g., unintended feature interactions) and which can be exploited with attacks. While various databases summarize information on vulnerabilities and other security issues for many software systems, these databases face severe limitations. For example, the information's quality is unclear, often only semi-structured, and barely connected to other information. Consequently, it can be challenging for any security-related stakeholder to extract and understand what information is relevant, considering that most systems exist in different variants and versions. To tackle this problem, we propose to design vulnerability feature models that represent the vulnerabilities of a system and enable developers to virtualize corresponding attack scenarios. In this paper, we report a first case study on Mozilla Firefox for which we extracted vulnerabilities and used them to virtualize vulnerable instances in Docker. To this end, we focused on extracting information from available databases and on evaluating the usability of the results. Our findings indicate several problems with the extraction that complicate modeling, understanding, and testing of vulnerabilities. Nonetheless, the databases provide a valuable foundation for our technique, which we aim to extend with automatic synthesis and analyses of feature models, as well as virtualization for attack scenarios in future work.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {10},
numpages = {9},
keywords = {vulnerability, variability model, software architecture, feature model, exploit, docker-container, attack scenarios},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1145/3368089.3409675,
author = {Siegmund, Norbert and Ruckel, Nicolai and Siegmund, Janet},
title = {Dimensions of software configuration: on the configuration context in modern software development},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409675},
doi = {10.1145/3368089.3409675},
abstract = {With the rise of containerization, cloud development, and continuous integration and delivery, configuration has become an essential aspect not only to tailor software to user requirements, but also to configure a software system’s environment and infrastructure. This heterogeneity of activities, domains, and processes blurs the term configuration, as it is not clear anymore what tasks, artifacts, or stakeholders are involved and intertwined. However, each re- search study and each paper involving configuration places their contributions and findings in a certain context without making the context explicit. This makes it difficult to compare findings, translate them to practice, and to generalize the results. Thus, we set out to evaluate whether these different views on configuration are really distinct or can be summarized under a common umbrella. By interviewing practitioners from different domains and in different roles about the aspects of configuration and by analyzing two qualitative studies in similar areas, we derive a model of configuration that provides terminology and context for research studies, identifies new research opportunities, and allows practitioners to spot possible challenges in their current tasks. Although our interviewees have a clear view about configuration, it substantially differs due to their personal experience and role. This indicates that the term configuration might be overloaded. However, when taking a closer look, we see the interconnections and dependencies among all views, arriving at the conclusion that we need to start considering the entire spectrum of dimensions of configuration.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {338–349},
numpages = {12},
keywords = {variability, developer study, configuration management and life cycle, Dimensions of software configuration},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/2420942.2420943,
author = {Bo\v{s}kovi\'{c}, Marko and Mussbacher, Gunter and Ga\v{s}evi\'{c}, Dragan and Bagheri, Ebrahim},
title = {The Fourth International Workshop on Non-functional System Properties in Domain Specific Modeling Languages (NFPinDSML2012)},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420943},
doi = {10.1145/2420942.2420943},
abstract = {The International Workshop on Non-functional System Properties in Domain Specific Modeling Languages (NFPinDSML) series traditionally takes place as part of the Satellite Events of the ACM/IEEE International Conference on Model Driven Engineering Languages and Systems (MODELS). Traditionally, NFPinDSML gathers researchers and practitioners interested in the estimation and evaluation of system quality and their integration in Domain Specific Modeling Languages and Model Driven Engineering in general. This paper is the summary of the fourth NFPinDSML workshop which was affiliated with MODELS 2012.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {1},
numpages = {2},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@inproceedings{10.1145/2576768.2598305,
author = {Lopez-Herrejon, Roberto Erick and Javier Ferrer, Javier and Chicano, Francisco and Haslinger, Evelyn Nicole and Egyed, Alexander and Alba, Enrique},
title = {A parallel evolutionary algorithm for prioritized pairwise testing of software product lines},
year = {2014},
isbn = {9781450326629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2576768.2598305},
doi = {10.1145/2576768.2598305},
abstract = {Software Product Lines (SPLs) are families of related software systems, which provide different feature combinations. Different SPL testing approaches have been proposed. However, despite the extensive and successful use of evolutionary computation techniques for software testing, their application to SPL testing remains largely unexplored. In this paper we present the Parallel Prioritized product line Genetic Solver (PPGS), a parallel genetic algorithm for the generation of prioritized pairwise testing suites for SPLs. We perform an extensive and comprehensive analysis of PPGS with 235 feature models from a wide range of number of features and products, using 3 different priority assignment schemes and 5 product prioritization selection strategies. We also compare PPGS with the greedy algorithm prioritized-ICPL. Our study reveals that overall PPGS obtains smaller covering arrays with an acceptable performance difference with prioritized-ICPL.},
booktitle = {Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {1255–1262},
numpages = {8},
keywords = {software product lines, pairwise testing, feature models, combinatorial interaction testing},
location = {Vancouver, BC, Canada},
series = {GECCO '14}
}

@inproceedings{10.1145/2602576.2602585,
author = {Etxeberria, Leire and Trubiani, Catia and Cortellessa, Vittorio and Sagardui, Goiuria},
title = {Performance-based selection of software and hardware features under parameter uncertainty},
year = {2014},
isbn = {9781450325769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602576.2602585},
doi = {10.1145/2602576.2602585},
abstract = {Configurable software systems allow stakeholders to derive variants by selecting software and/or hardware features. Performance analysis of feature-based systems has been of large interest in the last few years, however a major research challenge is still to conduct such analysis before achieving full knowledge of the system, namely under a certain degree of uncertainty. In this paper we present an approach to analyze the correlation between selection of features embedding uncertain parameters and system performance. In particular, we provide best and worst case performance bounds on the basis of selected features and, in cases of wide gaps among these bounds, we carry on a sensitivity analysis process aimed at taming the uncertainty of parameters. The application of our approach to a case study in the e-health domain demonstrates how to support stakeholders in the identification of system variants that meet performance requirements.},
booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {23–32},
numpages = {10},
keywords = {uncertainty, software architectures, performance analysis, feature selection},
location = {Marcq-en-Bareul, France},
series = {QoSA '14}
}

@article{10.1145/3582571,
author = {Xu, Qinghua and Ali, Shaukat and Yue, Tao},
title = {Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3582571},
doi = {10.1145/3582571},
abstract = {Anomaly detection is critical to ensure the security of cyber-physical systems (CPS). However, due to the increasing complexity of attacks and CPS themselves, anomaly detection in CPS is becoming more and more challenging. In our previous work, we proposed a digital twin-based anomaly detection method, called ATTAIN, which takes advantage of both historical and real-time data of CPS. However, such data vary significantly in terms of difficulty. Therefore, similar to human learning processes, deep learning models (e.g., ATTAIN) can benefit from an easy-to-difficult curriculum. To this end, in this paper, we present a novel approach, named digitaL twin-based Anomaly deTecTion wIth Curriculum lEarning (LATTICE), which extends ATTAIN by introducing curriculum learning to optimize its learning paradigm. LATTICE attributes each sample with a difficulty score, before being fed into a training scheduler. The training scheduler samples batches of training data based on these difficulty scores such that learning from easy to difficult data can be performed. To evaluate LATTICE, we use five publicly available datasets collected from five real-world CPS testbeds. We compare LATTICE with ATTAIN and two other state-of-the-art anomaly detectors. Evaluation results show that LATTICE outperforms the three baselines and ATTAIN by 0.906%-2.367% in terms of the F1 score. LATTICE also, on average, reduces the training time of ATTAIN by 4.2% on the five datasets and is on par with the baselines in terms of detection delay time.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {113},
numpages = {32},
keywords = {anomaly detection, deep learning, curriculum learning, digital twin, Cyber-physical system}
}

@inproceedings{10.1145/3194760.3194761,
author = {Kessel, Marcus and Atkinson, Colin},
title = {Integrating reuse into the rapid, continuous software engineering cycle through test-driven search},
year = {2018},
isbn = {9781450357456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194760.3194761},
doi = {10.1145/3194760.3194761},
abstract = {Today's advanced agile practices such as Continuous Integration and Test-Driven Development support a wide range of software development activities to facilitate the rapid delivery of high-quality software. However, the reuse of pre-existing, third-party software components is not one of them. Software reuse is still primarily perceived as a time-consuming, unsystematic and ultimately, "discontinuous" activity even though it aims to deliver the same basic benefits as continuous software engineering - namely, a reduction in the time and effort taken to deliver quality software. However, the increasingly central role of testing in continuous software engineering offers a way of addressing this problem by exploiting the new generation of test-driven search engines that can harvest components based on tests. This search technology not only exploits artifacts that have already been created as part of the continuous testing process to harvest components, it returns results that have a high likelihood of being fit for purpose and thus of being worth reusing. In this paper, we propose to augment continuous software engineering with the rapid, continuous reuse of software code units by integrating the test-driven mining of software artifact repositories into the continuous integration process. More specifically, we propose to use tests written as part of the Test-First Development approach to perform test-driven searches for matching functionality while developers are working on their normal development activities. We discuss the idea of rapid, continuous code reuse based on recent advances in our test-driven search platform and elaborate on scenarios for its application in the future.},
booktitle = {Proceedings of the 4th International Workshop on Rapid Continuous Software Engineering},
pages = {8–11},
numpages = {4},
keywords = {test-driven search, test-driven reuse, test-driven development, rapid continuous integration, rapid continuous code reuse},
location = {Gothenburg, Sweden},
series = {RCoSE '18}
}

@inproceedings{10.1145/2668930.2688051,
author = {Hork\'{y}, Vojt\v{e}ch and Libi\v{c}, Peter and Marek, Luk\'{a}\v{s} and Steinhauser, Antonin and T\r{u}ma, Petr},
title = {Utilizing Performance Unit Tests To Increase Performance Awareness},
year = {2015},
isbn = {9781450332484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668930.2688051},
doi = {10.1145/2668930.2688051},
abstract = {Many decisions taken during software development impact the resulting application performance. The key decisions whose potential impact is large are usually carefully weighed. In contrast, the same care is not used for many decisions whose individual impact is likely to be small -- simply because the costs would outweigh the benefits. Developer opinion is the common deciding factor for these cases, and our goal is to provide the developer with information that would help form such opinion, thus preventing performance loss due to the accumulated effect of many poor decisions.Our method turns performance unit tests into recipes for generating performance documentation. When the developer selects an interface and workload of interest, relevant performance documentation is generated interactively. This increases performance awareness -- with performance information available alongside standard interface documentation, developers should find it easier to take informed decisions even in situations where expensive performance evaluation is not practical. We demonstrate the method on multiple examples, which show how equipping code with performance unit tests works.},
booktitle = {Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
pages = {289–300},
numpages = {12},
keywords = {performance testing, performance documentation, performance awareness, javadoc, java},
location = {Austin, Texas, USA},
series = {ICPE '15}
}

@inproceedings{10.1145/2601248.2601257,
author = {H\"{a}ser, Florian and Felderer, Michael and Breu, Ruth},
title = {Software paradigms, assessment types and non-functional requirements in model-based integration testing: a systematic literature review},
year = {2014},
isbn = {9781450324762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2601248.2601257},
doi = {10.1145/2601248.2601257},
abstract = {Context: In modern systems, like cyber-physical systems, where software and physical services are interacting, safety, security or performance play an important role. In order to guarantee the correct interoperability of such systems, with respect to functional and non-functional requirements, integration testing is an effective measure to achieve this. Model-based testing moreover not only enables early definition and validation, but also test automation. This makes it a good choice to overcome urgent challenges of integration testing. Objective: Many publications on model-based integration testing (MBIT) approaches can be found. Nevertheless, a study giving a systematic overview on the underlying software paradigms, measures for guiding the integration testing process as well as non-functional requirements they are suitable for, is missing. The aim of this paper is to find and synthesize the relevant primary studies to gain a comprehensive understanding of the current state of model-based integration testing. Method: For synthesizing the relevant studies, we conducted a systematic literature review (SLR) according to the guidelines of Kitchenham. Results: The systematic search and selection retrieved 83 relevant studies from which data has been extracted. Our review identified three assessment criteria for guiding the testing process, namely static metrics, dynamic metrics and stochastic &amp;random. In addition it shows that just a small fraction considers non-functional requirements. Most approaches are for component-oriented systems. Conclusion: Results from the SLR show that there are two major research gaps. First, there is an accumulated need for approaches in the MBIT field that support non-functional requirements, as they are gaining importance. Second, means for steering the integration testing process, especially together with automation, need to evolve.},
booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {29},
numpages = {10},
keywords = {systematic literature review, non-functional requirements, model-based integration testing, assessment types},
location = {London, England, United Kingdom},
series = {EASE '14}
}

@proceedings{10.1145/3540250,
title = {ESEC/FSE 2022: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of all members of the organizing committee, we are delighted to welcome everyone to the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2022. The event continues the long, distinguished ESEC/FSE tradition of presenting the most innovative research, and facilitating interactions between scientists and engineers who are passionate about advancing the theory and practice of software engineering.},
location = {Singapore, Singapore}
}

@proceedings{10.1145/3611643,
title = {ESEC/FSE 2023: Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to ESEC/FSE 2023, the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ESEC/FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. ESEC/FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {San Francisco, CA, USA}
}

@inproceedings{10.1109/ICSE.2019.00113,
author = {Ha, Huong and Zhang, Hongyu},
title = {DeepPerf: performance prediction for configurable software with deep sparse neural network},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00113},
doi = {10.1109/ICSE.2019.00113},
abstract = {Many software systems provide users with a set of configuration options and different configurations may lead to different runtime performance of the system. As the combination of configurations could be exponential, it is difficult to exhaustively deploy and measure system performance under all possible configurations. Recently, several learning methods have been proposed to build a performance prediction model based on performance data collected from a small sample of configurations, and then use the model to predict system performance under a new configuration. In this paper, we propose a novel approach to model highly configurable software system using a deep feedforward neural network (FNN) combined with a sparsity regularization technique, e.g. the L1 regularization. Besides, we also design a practical search strategy for automatically tuning the network hyperparameters efficiently. Our method, called DeepPerf, can predict performance values of highly configurable software systems with binary and/or numeric configuration options at much higher prediction accuracy with less training data than the state-of-the art approaches. Experimental results on eleven public real-world datasets confirm the effectiveness of our approach.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1095–1106},
numpages = {12},
keywords = {sparsity regularization, software performance prediction, highly configurable systems, deep sparse feedforward neural network},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/2739482.2768422,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Assun\c{c}\~{a}o, Wesley K.G. and Fischer, Stefan and Vergilio, Silvia R. and Egyed, Alexander},
title = {Genetic Improvement for Software Product Lines: An Overview and a Roadmap},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2768422},
doi = {10.1145/2739482.2768422},
abstract = {Software Product Lines (SPLs) are families of related software systems that provide different combinations of features. Extensive research and application attest to the significant economical and technological benefits of employing SPL practices. However, there are still several challenges that remain open. Salient among them is reverse engineering SPLs from existing variants of software systems and their subsequent evolution. In this paper, we aim at sketching connections between research on these open SPL challenges and ongoing work on Genetic Improvement. Our hope is that by drawing such connections we can spark the interest of both research communities on the exciting synergies at the intersection of these subject areas.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {823–830},
numpages = {8},
keywords = {variability, software product lines, genetic programming, genetic improvement, evolutionary algorithms},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@inproceedings{10.1145/2993236.2993246,
author = {Kienzle, J\"{o}rg and Mussbacher, Gunter and Collet, Philippe and Alam, Omar},
title = {Delaying decisions in variable concern hierarchies},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993246},
doi = {10.1145/2993236.2993246},
abstract = {Concern-Oriented Reuse (CORE) proposes a new way of structuring model-driven software development, where models of the system are modularized by domains of abstraction within units of reuse called concerns. Within a CORE concern, models are further decomposed and modularized by features. This paper extends CORE with a technique that enables developers of high-level concerns to reuse lower-level concerns without unnecessarily committing to a specific feature selection. The developer can select the functionality that is minimally needed to continue development, and reexpose relevant alternative lower-level features of the reused concern in the reusing concern's interface. This effectively delays decision making about alternative functionality until the higher-level reuse context, where more detailed requirements are known and further decisions can be made. The paper describes the algorithms for composing the variation (i.e., feature and impact models), customization, and usage interfaces of a concern, as well as the concern's realization models and finally an entire concern hierarchy, as is necessary to support delayed decision making in CORE.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {93–103},
numpages = {11},
keywords = {Reuse Hierarchies, Model-Driven Engineering, Model Reuse, Model Interfaces, Delaying of Decisions},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@inproceedings{10.1145/2866614.2866628,
author = {Th\"{u}m, Thomas and Winkelmann, Tim and Schr\"{o}ter, Reimar and Hentschel, Martin and Kr\"{u}ger, Stefan},
title = {Variability Hiding in Contracts for Dependent Software Product Lines},
year = {2016},
isbn = {9781450340199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2866614.2866628},
doi = {10.1145/2866614.2866628},
abstract = {Software product lines are used to efficiently develop and verify similar software products. While they focus on reuse of artifacts between products, a product line may also be reused itself in other product lines. A challenge with such dependent product lines is evolution; every change in a product line may influence all dependent product lines. With variability hiding, we aim to hide certain features and their artifacts in dependent product lines. In prior work, we focused on feature models and implementation artifacts. We build on this by discussing how variability hiding can be extended to specifications in terms of method contracts. We illustrate variability hiding in contracts by means of a running example and share our insights with preliminary experiments on the benefits for formal verification. In particular, we find that not every change in a certain product line requires a re-verification of other dependent product lines.},
booktitle = {Proceedings of the 10th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {method contracts, deductive verification, Multi product line},
location = {Salvador, Brazil},
series = {VaMoS '16}
}

@proceedings{10.1145/2993236,
title = {GPCE 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.5555/2337223.2337243,
author = {Siegmund, Norbert and Kolesnikov, Sergiy S. and K\"{a}stner, Christian and Apel, Sven and Batory, Don and Rosenm\"{u}ller, Marko and Saake, Gunter},
title = {Predicting performance via automated feature-interaction detection},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Customizable programs and program families provide user-selectable features to allow users to tailor a program to an application scenario. Knowing in advance which feature selection yields the best performance is difficult because a direct measurement of all possible feature combinations is infeasible. Our work aims at predicting program performance based on selected features. However, when features interact, accurate predictions are challenging. An interaction occurs when a particular feature combination has an unexpected influence on performance. We present a method that automatically detects performance-relevant feature interactions to improve prediction accuracy. To this end, we propose three heuristics to reduce the number of measurements required to detect interactions. Our evaluation consists of six real-world case studies from varying domains (e.g., databases, encoding libraries, and web servers) using different configuration techniques (e.g., configuration files and preprocessor flags). Results show an average prediction accuracy of 95%.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {167–177},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@proceedings{10.1145/3564719,
title = {GPCE 2022: Proceedings of the 21st ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2022},
isbn = {9781450399203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 21st ACM SIGPLAN International Conference on Generative Programming: Concept &amp; Experiences (GPCE 2022) held on December 6th and 7th, 2022 in Auckland, New Zealand. GPCE is the premiere venue for researchers and practitioners interested in techniques that use program generation, domain-specific languages, and component deployment to increase programmer productivity, improve software quality, and shorten the time-to-market of software products. In addition to exploring cutting-edge techniques of generative software, GPCE seeks to foster cross-fertilization between software engineering and programming language.},
location = {Auckland, New Zealand}
}

@inproceedings{10.1145/2658761.2658767,
author = {Ruprecht, Andreas and Heinloth, Bernhard and Lohmann, Daniel},
title = {Automatic feature selection in large-scale system-software product lines},
year = {2014},
isbn = {9781450331616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658761.2658767},
doi = {10.1145/2658761.2658767},
abstract = {System software can typically be configured at compile time via a comfortable feature-based interface to tailor its functionality towards a specific use case. However, with the growing number of features, this tailoring process becomes increasingly difficult: As a prominent example, the Linux kernel in v3.14 provides nearly 14 000 configuration options to choose from. Even developers of embedded systems refrain from trying to build a minimized distinctive kernel configuration for their device – and thereby waste memory and money for unneeded functionality. In this paper, we present an approach for the automatic use-case specific tailoring of system software for special-purpose embedded systems. We evaluate the effectiveness of our approach on the example of Linux by generating tailored kernels for well-known applications of the Rasperry Pi and a Google Nexus 4 smartphone. Compared to the original configurations, our approach leads to memory savings of 15–70 percent and requires only very little manual intervention.},
booktitle = {Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences},
pages = {39–48},
numpages = {10},
keywords = {Software Tailoring, Software Product Lines, Linux, Feature Selection},
location = {V\"{a}ster\r{a}s, Sweden},
series = {GPCE 2014}
}

@inproceedings{10.1145/3510003.3510094,
author = {He, Haochen and Jia, Zhouyang and Li, Shanshan and Yu, Yue and Zhou, Chenglong and Liao, Qing and Wang, Ji and Liao, Xiangke},
title = {Multi-intention-aware configuration selection for performance tuning},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510094},
doi = {10.1145/3510003.3510094},
abstract = {Automatic configuration tuning helps users who intend to improve software performance. However, the auto-tuners are limited by the huge configuration search space. More importantly, they focus only on performance improvement while being unaware of other important user intentions (e.g., reliability, security). To reduce the search space, researchers mainly focus on pre-selecting performance-related parameters which requires a heavy stage of dynamically running under different configurations to build performance models. Given that other important user intentions are not paid attention to, we focus on guiding users in pre-selecting performance-related parameters in general while warning about side-effects on non-performance intentions. We find that the configuration document often, if it does not always, contains rich information about the parameters' relationship with diverse user intentions, but documents might also be long and domain-specific.In this paper, we first conduct a comprehensive study on 13 representative software containing 7,349 configuration parameters, and derive six types of ways in which configuration parameters may affect non-performance intentions. Guided by this study, we design SafeTune, a multi-intention-aware method that preselects important performance-related parameters and warns about their side-effects on non-performance intentions. Evaluation on target software shows that SafeTune correctly identifies 22--26 performance-related parameters that are missed by state-of-the-art tools but have significant performance impact (up to 14.7x). Furthermore, we illustrate eight representative cases to show that SafeTune can effectively prevent real-world and critical side-effects on other user intentions.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1431–1442},
numpages = {12},
keywords = {non-performance property, performance tuning, user intention},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.5555/2663546.2663573,
author = {Fredericks, Erik M. and Ramirez, Andres J. and Cheng, Betty H. C.},
title = {Towards run-time testing of dynamic adaptive systems},
year = {2013},
isbn = {9781467344012},
publisher = {IEEE Press},
abstract = {It is challenging to design, develop, and validate a dynamically adaptive system (DAS) that satisfies requirements, particularly when requirements can change at run time. Testing at design time can help verify and validate that a DAS satisfies its specified requirements and constraints. While offline tests may demonstrate that a DAS is capable of satisfying its requirements before deployment, a DAS may encounter unanticipated system and environmental conditions that can prevent it from achieving its objectives. In working towards a requirements-aware DAS, this paper proposes run-time monitoring and adaptation of tests as another technique for evaluating whether a DAS satisfies, or is even capable of satisfying, its requirements given its current execution context. To this end, this paper motivates the need and identifies challenges for adaptively testing a DAS at run time, as well as suggests possible methods for leveraging offline testing techniques for verifying run-time behavior.},
booktitle = {Proceedings of the 8th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {169–174},
numpages = {6},
location = {San Francisco, California},
series = {SEAMS '13}
}

@inproceedings{10.1145/2695664.2695875,
author = {Almeida, Andr\'{e} and Bencomo, Nelly and Batista, Thais and Cavalcante, Everton and Dantas, Francisco},
title = {Dynamic decision-making based on NFR for managing software variability and configuration selection},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695875},
doi = {10.1145/2695664.2695875},
abstract = {Due to dynamic variability, identifying the specific conditions under which non-functional requirements (NFRs) are satisfied may be only possible at runtime. Therefore, it is necessary to consider the dynamic treatment of relevant information during the requirements specifications. The associated data can be gathered by monitoring the execution of the application and its underlying environment to support reasoning about how the current application configuration is fulfilling the established requirements. This paper presents a dynamic decision-making infrastructure to support both NFRs representation and monitoring, and to reason about the degree of satisfaction of NFRs during runtime. The infrastructure is composed of: (i) an extended feature model aligned with a domain-specific language for representing NFRs to be monitored at runtime; (ii) a monitoring infrastructure to continuously assess NFRs at runtime; and (iii) a flexible decision-making process to select the best available configuration based on the satisfaction degree of the NRFs. The evaluation of the approach has shown that it is able to choose application configurations that well fit user NFRs based on runtime information. The evaluation also revealed that the proposed infrastructure provided consistent indicators regarding the best application configurations that fit user NFRs. Finally, a benefit of our approach is that it allows us to quantify the level of satisfaction with respect to NFRs specification.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1376–1382},
numpages = {7},
keywords = {variability, non-functional requirements, monitoring, SPLs},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/2897010.2897011,
author = {Fischer, Stefan and Lopez-Herrejon, Roberto E. and Ramler, Rudolf and Egyed, Alexander},
title = {A preliminary empirical assessment of similarity for combinatorial interaction testing of software product lines},
year = {2016},
isbn = {9781450341660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897010.2897011},
doi = {10.1145/2897010.2897011},
abstract = {Extensive work on Search-Based Software Testing for Software Product Lines has been published in the last few years. Salient among them is the use of similarity as a surrogate metric for t-wise coverage whenever higher strengths are needed or whenever the size of the test suites is infeasible because of technological or budget limitations. Though promising, this metric has not been assessed with real fault data. In this paper, we address this limitation by using Drupal, a widely used open source web content management system, as an industry-strength case study for which both variability information and fault data have been recently made available. Our preliminary assessment corroborates some of the previous findings but also raises issues on some assumptions and claims made. We hope our work encourages further empirical evaluations of Combinatorial Interaction Testing approaches for Software Product Lines.},
booktitle = {Proceedings of the 9th International Workshop on Search-Based Software Testing},
pages = {15–18},
numpages = {4},
location = {Austin, Texas},
series = {SBST '16}
}

@inproceedings{10.1145/2000259.2000263,
author = {Koziolek, Heiko},
title = {Sustainability evaluation of software architectures: a systematic review},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000263},
doi = {10.1145/2000259.2000263},
abstract = {Long-living software systems are sustainable if they can be cost-efficiently maintained and evolved over their entire life-cycle. The quality of software architectures determines sustainability to a large extent. Scenario-based software architecture evaluation methods can support sustainability analysis, but they are still reluctantly used in practice. They are also not integrated with architecture-level metrics when evaluating implemented systems, which limits their capabilities. Existing literature reviews for architecture evaluation focus on scenario-based methods, but do not provide a critical reflection of the applicability of such methods for sustainability evaluation. Our goal is to measure the sustainability of a software architecture both during early design using scenarios and during evolution using scenarios and metrics, which is highly relevant in practice. We thus provide a systematic literature review assessing scenario-based methods for sustainability support and categorize more than 40 architecture-level metrics according to several design principles. Our review identifies a need for further empirical research, for the integration of existing methods, and for the more efficient use of formal architectural models.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {3–12},
numpages = {10},
keywords = {sustainability, survey, software architecture, evolution scenario, architectural metric},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@proceedings{10.1145/3567512,
title = {SLE 2022: Proceedings of the 15th ACM SIGPLAN International Conference on Software Language Engineering},
year = {2022},
isbn = {9781450399197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 15th ACM SIGPLAN International Conference on Software Language Engineering (SLE), co-located with the ACM SIGPLAN conference on Systems, Programming, Languages, and Applications (SPLASH) in Auckland, a vibrant port city in northern New Zealand, from December 5th to December 10th 2022. Like its predecessors, the this edition of the SLE conference, SLE 2022, is devoted to the principles of software languages: their design, their implementation, and their evolution. As such, SLE brings together researchers united by their common interest in the creation, capture, and tooling of software languages.},
location = {Auckland, New Zealand}
}

@proceedings{10.1145/3555776,
title = {SAC '23: Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tallinn, Estonia}
}

@inproceedings{10.1145/1596495.1596500,
author = {Peper, Christian and Schneider, Daniel},
title = {On runtime service quality models in adaptive ad-hoc systems},
year = {2009},
isbn = {9781605586816},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1596495.1596500},
doi = {10.1145/1596495.1596500},
abstract = {Ad-hoc computer systems can automatically realize higher services when at least two distributed and communicating (embedded) devices come together. For this purpose, they must able to manage appearance and loss of devices and resources, and they have to adapt to changes in requirements and environment. Based on a component-oriented approach for adaptive ad-hoc systems, this paper suggests a high-level service quality reference model to advocate further research on the quality matching problem between service provider and client components.},
booktitle = {Proceedings of the 2009 ESEC/FSE Workshop on Software Integration and Evolution @ Runtime},
pages = {11–18},
numpages = {8},
keywords = {ubiquitous computing, quality-of-service, distributed systems, component-orientation, ambient intelligence, adaptivity, ad-hoc systems},
location = {Amsterdam, The Netherlands},
series = {SINTER '09}
}

@article{10.1145/3571854,
author = {Zampetti, Fiorella and Tamburri, Damian and Panichella, Sebastiano and Panichella, Annibale and Canfora, Gerardo and Di Penta, Massimiliano},
title = {Continuous Integration and Delivery Practices for Cyber-Physical Systems: An Interview-Based Study},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3571854},
doi = {10.1145/3571854},
abstract = {Continuous Integration and Delivery (CI/CD) practices have shown several benefits for software development and operations, such as faster release cycles and early discovery of defects. For Cyber-Physical System (CPS) development, CI/CD can help achieving required goals, such as high dependability, yet it may be challenging to apply. This article empirically investigates challenges, barriers, and their mitigation occurring when applying CI/CD practices to develop CPSs in 10 organizations working in eight different domains. The study has been conducted through semi-structured interviews, by applying an open card sorting procedure together with a member-checking survey within the same organizations, and by validating the results through a further survey involving 55 professional developers. The study reveals several peculiarities in the application of CI/CD to CPSs. These include the need for (i) combining continuous and periodic builds while balancing the use of Hardware-in-the-Loop and simulators, (ii) coping with difficulties in software deployment (iii) accounting for simulators and Hardware-in-the-Loop differing in their behavior, and (vi) combining hardware/software expertise in the development team. Our findings open the road toward recommenders aimed at supporting the setting and evolution of CI/CD pipelines, as well as university curricula requiring interdisciplinarity, such as knowledge about hardware, software, and their interplay.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {73},
numpages = {44},
keywords = {empirical software engineering, Cyber-Physical Systems, Continuous Integration and Delivery}
}

@proceedings{10.1145/3603166,
title = {UCC '23: Proceedings of the IEEE/ACM 16th International Conference on Utility and Cloud Computing},
year = {2023},
isbn = {9798400702341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The IEEE/ACM International Conference on Utility and Cloud Computing (UCC) is a premier annual conference series aiming to provide a platform for researchers from both academia and industry to present new discoveries in the broad area of Cloud and Edge utility computing and applications.},
location = {Taormina (Messina), Italy}
}

@inproceedings{10.1145/2610384.2610411,
author = {Galindo, Jos\'{e} A. and Alf\'{e}rez, Mauricio and Acher, Mathieu and Baudry, Benoit and Benavides, David},
title = {A variability-based testing approach for synthesizing video sequences},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610411},
doi = {10.1145/2610384.2610411},
abstract = {A key problem when developing video processing software is the difficulty to test different input combinations. In this paper, we present VANE, a variability-based testing approach to derive video sequence variants. The ideas of VANE are i) to encode in a variability model what can vary within a video sequence; ii) to exploit the variability model to generate testable configurations; iii) to synthesize variants of video sequences corresponding to configurations. VANE computes T-wise covering sets while optimizing a function over attributes. Also, we present a preliminary validation of the scalability and practicality of VANE in the context of an industrial project involving the test of video processing algorithms.},
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {293–303},
numpages = {11},
keywords = {Video analysis, Variability, Combinatorial testing},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@inproceedings{10.1145/2786805.2786845,
author = {Siegmund, Norbert and Grebhahn, Alexander and Apel, Sven and K\"{a}stner, Christian},
title = {Performance-influence models for highly configurable systems},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786845},
doi = {10.1145/2786805.2786845},
abstract = {Almost every complex software system today is configurable. While configurability has many benefits, it challenges performance prediction, optimization, and debugging. Often, the influences of individual configuration options on performance are unknown. Worse, configuration options may interact, giving rise to a configuration space of possibly exponential size. Addressing this challenge, we propose an approach that derives a performance-influence model for a given configurable system, describing all relevant influences of configuration options and their interactions. Our approach combines machine-learning and sampling heuristics in a novel way. It improves over standard techniques in that it (1) represents influences of options and their interactions explicitly (which eases debugging), (2) smoothly integrates binary and numeric configuration options for the first time, (3) incorporates domain knowledge, if available (which eases learning and increases accuracy), (4) considers complex constraints among options, and (5) systematically reduces the solution space to a tractable size. A series of experiments demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {284–294},
numpages = {11},
keywords = {sampling, machine learning, Performance-influence models},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@proceedings{10.5555/3643142,
title = {WSC '23: Proceedings of the Winter Simulation Conference},
year = {2023},
isbn = {9798350369663},
publisher = {IEEE Press},
location = {San Antonio, Texas, USA}
}

@proceedings{10.1145/3597503,
title = {ICSE '24: Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3643991,
title = {MSR '24: Proceedings of the 21st International Conference on Mining Software Repositories},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {MSR is a thriving research community that organizes a yearly conference with a solid reputation amongst software engineering researchers.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/2642937.2642990,
author = {Abal, Iago and Brabrand, Claus and Wasowski, Andrzej},
title = {42 variability bugs in the linux kernel: a qualitative analysis},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2642990},
doi = {10.1145/2642937.2642990},
abstract = {Feature-sensitive verification pursues effective analysis of the exponentially many variants of a program family. However, researchers lack examples of concrete bugs induced by variability, occurring in real large-scale systems. Such a collection of bugs is a requirement for goal-oriented research, serving to evaluate tool implementations of feature-sensitive analyses by testing them on real bugs. We present a qualitative study of 42 variability bugs collected from bug-fixing commits to the Linux kernel repository. We analyze each of the bugs, and record the results in a database. In addition, we provide self-contained simplified C99 versions of the bugs, facilitating understanding and tool evaluation. Our study provides insights into the nature and occurrence of variability bugs in a large C software system, and shows in what ways variability affects and increases the complexity of software bugs.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {421–432},
numpages = {12},
keywords = {software variability, linux, feature interactions, bugs},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@proceedings{10.1145/3650212,
title = {ISSTA 2024: Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 33rd edition of the International Symposium on Software Testing and Analysis, ISSTA 2024, held on September 16--20, 2024 in Vienna, Austria. ISSTA 2024 is co-located with ECOOP and MPLR 2024. ISSTA brings together academics, industrial researchers, and practitioners from all over the world working on testing and analyzing software systems.},
location = {Vienna, Austria}
}

@proceedings{10.1145/3551349,
title = {ASE '22: Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
year = {2022},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rochester, MI, USA}
}

@inproceedings{10.1145/2509136.2509522,
author = {Bhattacharya, Suparna and Gopinath, Kanchi and Nanda, Mangala Gowri},
title = {Combining concern input with program analysis for bloat detection},
year = {2013},
isbn = {9781450323741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509136.2509522},
doi = {10.1145/2509136.2509522},
abstract = {Framework based software tends to get bloated by accumulating optional features (or concerns) just-in-case they are needed. The good news is that such feature bloat need not always cause runtime execution bloat. The bad news is that often enough, only a few statements from an optional concern may cause execution bloat that may result in as much as 50% runtime overhead.We present a novel technique to analyze the connection between optional concerns and the potential sources of execution bloat induced by them. Our analysis automatically answers questions such as (1) whether a given set of optional concerns could lead to execution bloat and (2) which particular statements are the likely sources of bloat when those concerns are not required. The technique combines coarse grain concern input from an external source with a fine-grained static analysis. Our experimental evaluation highlights the effectiveness of such concern augmented program analysis in execution bloat assessment of ten programs.},
booktitle = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages &amp; Applications},
pages = {745–764},
numpages = {20},
keywords = {software bloat, program concerns, feature oriented programming},
location = {Indianapolis, Indiana, USA},
series = {OOPSLA '13}
}

@proceedings{10.5555/3606010,
title = {ICSE '23: Proceedings of the 45th International Conference on Software Engineering},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Victoria, Australia}
}

@proceedings{10.1145/3544549,
title = {CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hamburg, Germany}
}

@proceedings{10.1145/3411764,
title = {CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@article{10.1145/979743.979745,
author = {ACM SIGSOFT Software Engineering Notes staff},
title = {Back matter (abstracts and calendar)},
year = {2004},
issue_date = {March 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/979743.979745},
doi = {10.1145/979743.979745},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {27–62},
numpages = {36}
}

@proceedings{10.1145/3570361,
title = {ACM MobiCom '23: Proceedings of the 29th Annual International Conference on Mobile Computing and Networking},
year = {2023},
isbn = {9781450399906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Madrid, Spain}
}

@article{10.1145/270849.270854,
author = {Edwards, Stephen H. and Weide, Bruce W.},
title = {WISR8: 8th annual workshop on software reuse: summary and working group reports},
year = {1997},
issue_date = {Sept. 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/270849.270854},
doi = {10.1145/270849.270854},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {17–32},
numpages = {16}
}

@article{10.1145/279437.279460,
author = {Thompson, Craig},
title = {Workshop on compositional software architectures: workshop report},
year = {1998},
issue_date = {May 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/279437.279460},
doi = {10.1145/279437.279460},
journal = {SIGSOFT Softw. Eng. Notes},
month = may,
pages = {44–63},
numpages = {20}
}

@proceedings{10.1145/2951913,
title = {ICFP 2016: Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming},
year = {2016},
isbn = {9781450342193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nara, Japan}
}

@proceedings{10.1145/2950290,
title = {FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

