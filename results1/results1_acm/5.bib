@inproceedings{10.1145/3646548.3672597,
author = {Purandare, Salil and Cohen, Myra B.},
title = {Exploration of Failures in an sUAS Controller Software Product Line},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672597},
doi = {10.1145/3646548.3672597},
abstract = {Small uncrewed aerial systems (sUAS) are growing in their use for commercial, scientific, recreational, and emergency management purposes. A critical part of a successful flight is a correctly tuned controller which manages the physics of the vehicle. If improperly configured, it can lead to flight instability, deviation, or crashes. These types of misconfigurations are often within the valid ranges specified in the documentation; hence, they are hard to identify. Recent research has used fuzzing or explored only a small part of the parameter space, providing little understanding of the configuration landscape itself. In this work we leverage software product line engineering to model a subset of the parameter space of a widely used flight control software, using it to guide a systematic exploration of the controller space. Via simulation, we test over 20,000 configurations from a feature model with 50 features and 8.88 \texttimes{} 1034 products, covering all single parameter value changes and all pairs of changes from their default values. Our results show that only a small number of single configuration changes fail (15%), however almost 40% fail when we evaluate changes to two-parameters at a time. We explore the interactions between parameters in more detail, finding what appear to be many dependencies and interactions between parameters which are not well documented. We then explore a smaller, exhaustive product line model, with eight of the most important features (and 6,561 configurations) and uncover a complex set of interactions; over 48% of all configurations fail.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {125–135},
numpages = {11},
keywords = {Configurability, Software Product Lines, sUAS},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3646548.3672587,
author = {Becker, Martin and Rabiser, Rick and Botterweck, Goetz},
title = {Not Quite There Yet: Remaining Challenges in Systems and Software Product Line Engineering as Perceived by Industry Practitioners},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672587},
doi = {10.1145/3646548.3672587},
abstract = {Research on system and software product line engineering (SPLE) and the community around it have been inspired by industrial applications. However, despite decades of research, industry is still struggling with adopting product line approaches and more generally with managing system variability. We argue that it is essential to better understand why this is the case. Particularly, we need to understand the current challenges industry is facing wrt. adopting SPLE practices, how far existing research helps industry practitioners to cope with their challenges, and where additional research would be required. We conducted a hybrid workshop at the 2023 Systems and Software Product Line Conference (SPLC) with over 30 participants from industry and academia. 9 companies from diverse domains and in different phases of SPLE adoption presented their context and perceived challenges. We grouped, discussed, and rated the relevance of the articulated challenges. We then formed clusters of relevant research topics to discuss existing literature as well as research opportunities. In this paper, we report the industry cases, the identified challenges and clusters of research topics, provide pointers to existing work, and discuss research opportunities. With this, we want to enable industry practitioners to become aware of typical challenges and find their way into the existing body of knowledge and to relevant fields of research.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {179–190},
numpages = {12},
keywords = {Software product line engineering, industry challenges},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@article{10.1145/3695987,
author = {Oliveria de Souza, Leandro and Santana de Almeida, Eduardo and Silveira Neto, Paulo Anselmo da Mota and Barr, Earl T. and Petke, Justyna},
title = {Software Product Line Engineering via Software Transplantation},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3695987},
doi = {10.1145/3695987},
abstract = {Software Product Lines (SPLs) improve time-to-market, enhance software quality, and reduce maintenance costs. Current SPL reengineering practices are largely manual and require domain knowledge. Thus, adopting and, to a lesser extent, maintaining SPLs are expensive tasks, preventing many companies from enjoying their benefits. To address these challenges, we introduce Foundry, an approach utilising software transplantation to reduce the manual effort of SPL adoption and maintenance. Foundry enables integrating features across different codebases, even codebases that are unaware that they are contributing features to a software product line. Each product produced by Foundry is pure code, without variability annotation, unlike feature flags, which eases variability management and reduces code bloat.We realise Foundry in prodScalpel, a tool that transplants multiple organs (i.e., a set of interesting features) from donor systems into an emergent product line for codebases written in C. Given tests and lightweight annotations identifying features and implantation points, prodScalpel automates feature extraction and integration. To evaluate its effectiveness, our evaluation compares feature transplantation using prodScalpel to the current state of practice: on our dataset, prodScalpel’s use speeds up feature migration by an average of 4.8 times when compared to current practice.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {31},
numpages = {27},
keywords = {Software Product Lines, Software Transplantation, Genetic Improvement}
}

@inproceedings{10.1145/3646548.3676537,
author = {Marinho, Euler and Ferreira, Fischer and Fernandes, Eduardo and Diniz, Jo\~{a}o Paulo and Figueiredo, Eduardo},
title = {Resource Interaction Failures in Mobile Applications: A Challenge for the Software Product Line Testing Community},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676537},
doi = {10.1145/3646548.3676537},
abstract = {Context: Many mobile applications run on multiple platforms with specific available resources. These resources are associated with communication capabilities, sensors, and user customization. Certain resource combinations imply interactions between resources that are likely to produce failures in mobile applications, thereby harming the user experience. Challenge: There may be a large number of resource combinations for a single mobile application. Consequently, exhaustively testing resource interactions to spot failures can be very challenging. However, in order to address this challenge, having robust, well-documented, and publicly available datasets for mobile application testing is necessary. Proposal: This paper proposes the Resource Interaction Challenge targeting mobile applications. We introduce a curated dataset of 20 mobile applications with varying sizes (up to 350K lines of code) and required resources (Bluetooth, Wi-Fi, etc.). Due to the shortage of sampling strategies for testing resource interactions in mobile applications, we opted for strategies commonly used for configurable systems in general. Our dataset includes failures detected and source code metrics computed for each mobile application. Conclusion: We expect to engage both researchers and practitioners in reusing our dataset, especially to propose and evaluate novel testing strategies.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {203–208},
numpages = {6},
keywords = {Mobile Application Testing, Resource Interaction Failures;},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3503229.3547055,
author = {Friesel, Birte and M\"{u}ller, Michael and Ferraz, Matheus and Spinczyk, Olaf},
title = {On the relation of variability modeling languages and non-functional properties},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547055},
doi = {10.1145/3503229.3547055},
abstract = {Non-functional properties (NFPs) such as code size (RAM, ROM), performance, and energy consumption are at least as important as functional properties in many software development domains. When configuring a software product line - especially in the area of resource-constrained embedded systems - developers must be aware of the NFPs of the configured product instance. Several NFP-aware variability modeling languages have been proposed to address this in the past. However, it is not clear whether a variability modeling language is the best place for handling NFP-related concerns, or whether separate NFP prediction models should be preferred. We shine light onto this question by discussing limitations of state-of-the-art NFP-aware variability modeling languages, and find that both in terms of the development process and model accuracy a separate NFP model is favorable. Our quantitative analysis is based on six different software product lines, including the widely used busybox multi-call binary and the x264 video encoder. We use classification and regression trees (CART) and our recently proposed Regression Model Trees [8] as separate NFP models. These tree-based models can cover the effects of arbitrary feature interactions and thus easily outperform variability models with static, feature-wise NFP annotations. For example, when estimating the throughput of an embedded AI product line, static annotations come with a mean generalization error of 114.5% while the error of CART is only 9.4 %.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {140–144},
numpages = {5},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3643667.3648224,
author = {Sep\'{u}lveda, Samuel and Piattini, Mario and P\'{e}rez Del Castillo, Ricardo},
title = {Developing hybrid quantum-classical software: a software product line approach},
year = {2024},
isbn = {9798400705700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643667.3648224},
doi = {10.1145/3643667.3648224},
abstract = {Quantum computing is rapidly emerging as a transformative force in technology. In the near future we will increasingly encounter hybrid systems that combine quantum technology with classical software. Software engineering techniques will be needed to manage the complexity of designing such systems and their reuse. This paper introduces preliminary ideas for developing quantum-classical software using a Software Product Line approach in line with the Model-Driven Engineering principles. This approach addresses the mentioned challenges and drafts a framework for developing hybrid quantum-classical software. The preliminary insights show the feasibility and suitability of applying the proposed approach for developing complex quantum-classical software systems with high levels of variability.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Quantum Software Engineering},
pages = {37–40},
numpages = {4},
keywords = {quantum computing, software product lines, variability, feature modeling},
location = {Lisbon, Portugal},
series = {Q-SE 2024}
}

@inproceedings{10.1145/3652620.3687798,
author = {Sousa, Tiago and Ries, Beno\^{\i}t and Guelfi, Nicolas},
title = {Model-Driven Software Product Line Engineering of AI-Based Applications for Achieving Sustainable Development Goals: Vision Paper},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687798},
doi = {10.1145/3652620.3687798},
abstract = {Achieving the Sustainable Development Goals (SDGs) set by the United Nations requires innovative solutions to address the related complex and interconnected challenges. The application of AI has demonstrated the potential to significantly contribute to these efforts by providing advanced analytics and decision-making capabilities. However, integrating AI into sustainability initiatives faces several challenges, including the need for flexible and reusable solutions that can be adapted to diverse and evolving SDG contexts, as well as the challenge of making these technologies accessible to nonexpert stakeholders. This paper proposes an integrated approach that combines Model-Driven Engineering (MDE) with Software Product Line Engineering (SPLE) to address these challenges. The proposed process includes key activities such as domain analysis, metamodel-driven requirements specification, product derivation, and AI model training. This approach aims to automate the derivation of flexible and reusable AI architectures tailored to specific SDG contexts, thus reducing the development time of AI-based software solutions for sustainability efforts.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {523–527},
numpages = {5},
keywords = {model-driven engineering, software product line, artificial intelligence, sustainable development goals, vision paper},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3336294.3336304,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Software Product Line Engineering: A Practical Experience},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336304},
doi = {10.1145/3336294.3336304},
abstract = {The lack of mature tool support is one of the main reasons that make the industry to be reluctant to adopt Software Product Line (SPL) approaches. A number of systematic literature reviews exist that identify the main characteristics offered by existing tools and the SPL phases in which they can be applied. However, these reviews do not really help to understand if those tools are offering what is really needed to apply SPLs to complex projects. These studies are mainly based on information extracted from the tool documentation or published papers. In this paper, we follow a different approach, in which we firstly identify those characteristics that are currently essential for the development of an SPL, and secondly analyze whether the tools provide or not support for those characteristics. We focus on those tools that satisfy certain selection criteria (e.g., they can be downloaded and are ready to be used). The paper presents a state of practice with the availability and usability of the existing tools for SPL, and defines different roadmaps that allow carrying out a complete SPL process with the existing tool support.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {164–176},
numpages = {13},
keywords = {spl in practice, state of practice, tool support, tooling roadmap},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3461001.3473060,
author = {Sch\"{a}fer, Andreas and Becker, Martin and Andres, Markus and Kistenfeger, Tim and Rohlf, Florian},
title = {Variability realization in model-based system engineering using software product line techniques: an industrial perspective},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3473060},
doi = {10.1145/3461001.3473060},
abstract = {Efficiently handling system variants is rising of importance in industry and challenges the application of model-based systems engineering.This paper reveals the increasing industrial demand of guidance and decision support on how to handle variants and variability within SysML and UML models. While a substantial amount of variability realization approaches has already been published on source code level, there is little guidance for practitioners on system model level. Hence, there is major uncertainty in dealing with system changes or concurrent system modeling of related system. Due to a poor modularization and variability realization these model variants are ending up in interwoven and complex system models.In this paper, we aim to raise awareness of the need for appropriate guidance and decision support, identify important contextual factors of MBSE that influence variability realization, and derive well known variability mechanisms used in software coding for their applicability in system modeling.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {25–34},
numpages = {10},
keywords = {SysML, UML, decision support, model-based systems engineering, system and software product line engineering, variability mechanism, variability realization, variant management},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3109729.3109744,
author = {Munoz, Daniel-Jesus},
title = {Achieving energy efficiency using a Software Product Line Approach},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109744},
doi = {10.1145/3109729.3109744},
abstract = {Green computing and energy-aware software engineering are trend approaches that try to address the development of applications respectful with the environment. To reduce the energy consumption of an application the developer needs: (i) to identify what are the concerns that will impact more in the energy consumption; (ii) to model the variability of alternative designs and implementations of each concern; (iii) to store and compare the experimentation results related with the energy and time consumption of concerns; (iv) to find out what is the most eco-efficient solution for each concern. HADAS addresses these issues by modelling the variability of energy consuming concerns for different energy contexts. It connects the variability model with a repository that stores energy measurements, providing a Software Product Line (SPL) service, helping developers to reason and find out what are the most eco-friendly configurations. We have an initial implementation of the HADAS toolkit using Clafer. We have tested our implementation with several case studies.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {131–138},
numpages = {8},
keywords = {Clafer, Energy Efficiency, Metrics, Optimisation, Repository, Software Product Line, Variability},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3233027.3233038,
author = {Martinez, Jabier and T\"{e}rnava, Xhevahire and Ziadi, Tewfik},
title = {Software product line extraction from variability-rich systems: the robocode case study},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233038},
doi = {10.1145/3233027.3233038},
abstract = {The engineering of a Software Product Line (SPL), either by creating it from scratch or through the re-engineering of existing variants, it uses to be a project that spans several years with a high investment. It is often hard to analyse and quantify this investment, especially in the context of extractive SPL adoption when the related software variants are independently created by different developers following different system architectures and implementation conventions. This paper reports an experience on the creation of an SPL by re-engineering system variants implemented around an educational game called Robocode. The objective of this game is to program a bot (a battle tank) that battles against the bots of other developers. The world-wide Robocode community creates and maintains a large base of knowledge and implementations that are mainly organized in terms of features, although not presented as an SPL. Therefore, a group of master students analysed this variability-rich domain and extracted a Robocode SPL. We present the results of such extraction augmented with an analysis and a quantification regarding the spent time and effort. We believe that the results and the a-posteriori analysis can provide insights on global challenges on SPL adoption. We also provide all the elements to SPL educators to reproduce the teaching activity, and we make available this SPL to be used for any research purpose.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {132–142},
numpages = {11},
keywords = {education, extractive software product line adoption, reverse-engineering, robocode, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3233027.3233032,
author = {Kr\"{o}her, Christian and Gerling, Lea and Schmid, Klaus},
title = {Identifying the intensity of variability changes in software product line evolution},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233032},
doi = {10.1145/3233027.3233032},
abstract = {The evolution of a Software Product Line (SPL) typically affects a variety of artifact types. The intensity (the frequency and the amount) in which developers change variability information in these different types of artifacts is currently unknown. In this paper, we present a fine-grained approach for the variability-centric extraction and analysis of changes to code, build, and variability model artifacts introduced by commits. This approach complements existing work that is typically based on a feature-perspective and, thus, abstracts from this level of detail. Further, it provides a detailed understanding of the intensity of changes affecting variability information in these types of artifacts. We apply our approach to the Linux kernel revealing that changes to variability information occur infrequently and only affect small parts of the analyzed artifacts. Further, we outline how these results may improve certain analysis and verification tasks during SPL evolution.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {54–64},
numpages = {11},
keywords = {evolution analysis, intensity, software product line evolution, variability changes},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3522664.3528602,
author = {Friesel, Birte and Spinczyk, Olaf},
title = {Black-box models for non-functional properties of AI software systems},
year = {2022},
isbn = {9781450392754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3522664.3528602},
doi = {10.1145/3522664.3528602},
abstract = {Non-functional properties (NFPs) such as latency, memory requirements, or hardware cost are an important characteristic of AI software systems, especially in the domain of resource-constrained embedded devices. Embedded AI products require sufficient resources for satisfactory latency and accuracy, but should also be cost-efficient and therefore not use more powerful hardware than strictly necessary. Traditionally, modeling and optimization efforts focus on the AI architecture, utilizing methods such as neural architecture search (NAS). However, before developers can start optimizing, they need to know which architectures are suitable candidates for their use case. To this end, architectures must be viewed in context: model post-processing (e.g. quantization), hardware platform, and run-time configuration such as batching all have significant effects on NFPs and therefore on AI architecture performance. Moreover, scalar parameters such as batch size cannot be benchmarked exhaustively. We argue that it is worthwhile to address this issue by means of black-box models before deciding on AI architectures for optimization and hardware/software platforms for inference. To support our claim, we present an AI product line with variable hardware and software components, perform benchmarks, and present notable results. Additionally, we evaluate both compactness and generalization capabilities of regression tree-based modeling approaches from the machine learning and product line engineering communities. We find that linear model trees perform best: they can capture NFPs of known AI configurations with a mean error of up to 13 %, and can predict unseen configurations with a mean error of 10 to 26 %. We find linear model trees to be more compact and interpretable than other tree-based approaches.},
booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
pages = {170–180},
numpages = {11},
keywords = {AI, performance prediction, product lines, regression trees},
location = {Pittsburgh, Pennsylvania},
series = {CAIN '22}
}

@inproceedings{10.1145/3483899.3483909,
author = {Furtado, Viviane and OliveiraJr, Edson and Kalinowski, Marcos},
title = {Guidelines for Promoting Software Product Line Experiments},
year = {2021},
isbn = {9781450384193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483899.3483909},
doi = {10.1145/3483899.3483909},
abstract = {The importance of experimentation for Software Engineering research has been notably established in the last years. The software engineering community has discussed how to proper report and evaluate experiments using different approaches, such as quality criteria, scales, and checklists. Nevertheless, there are no guidelines to support researchers and practitioners active in straightforward software engineering research areas, as in Software Product Lines (SPL), at conducting experiments. We hypothesize that experimentation guidelines may aid such a specific area by providing advice and actual excerpts reflecting good practices of SPL experimentation, thus experimentally evolving this area. Therefore, the goal of this paper is to provide guidelines for properly reporting and promoting SPL experiments. We defined such guidelines based on well-known software engineering experiment reports, quality evaluation checklists, and data extracted from 211 SPL experiments identified in a systematic mapping study. We evaluated the guidelines with a qualitative study with SPL and experimentation experts applying open and axial coding procedures. The evaluation enabled us to improve the guidelines. The resulting guidelines contain specific advice to researchers active in SPL and provide examples taken from published SPL experiments. The experts’ positive points indicate that the proposed guidelines can aid SPL researchers and practitioners. Sharing the resulting guidelines could support conducting SPL experiments and allow further area evolution based on prospective experiment replications and reproductions from well-designed and reported experiments.},
booktitle = {Proceedings of the 15th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {31–40},
numpages = {10},
keywords = {Experiment Reporting and Sharing, Guidelines, Qualitative Study, SPL Experiments},
location = {Joinville, Brazil},
series = {SBCARS '21}
}

@inproceedings{10.1145/3382026.3431248,
author = {Ferreira, Thiago Nascimento and Vergilio, Silvia Regina and Kessentini, Mauroane},
title = {Many-objective Search-based Selection of Software Product Line Test Products with Nautilus},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431248},
doi = {10.1145/3382026.3431248},
abstract = {The Variability Testing of Software Product Lines (VTSPL) concerns the selection of the most representative products to be tested according to specific goals. Works in the literature use a great variety of objectives and distinct algorithms. However, they neither address all the objectives at the same time nor offer an automatic tool to support this task. To this end, this work introduces Nautilus/VTSPL, a tool to address the VTSPL problem, created by instantiating Nautilus Framework. Nautilus/VTSPL allows the tester to experiment and configure different objectives and categories of many-objective algorithms. The tool also offers support to visualization of the generated solutions, easing the decision-making process.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {1–4},
numpages = {4},
keywords = {many-objective algorithms, product line testing, sbse},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3307630.3342421,
author = {Chac\'{o}n-Luna, Ana E. and Ruiz, Elvira G. and Galindo, Jos\'{e} A. and Benavides, David},
title = {Variability Management in a Software Product Line Unaware Company: Towards a Real Evaluation},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342421},
doi = {10.1145/3307630.3342421},
abstract = {Software Product Lines (SPL) enable systematic reuse within an organization thus, enabling the reduction of costs, efforts, development time and the average number of defects per product. However, there is little empirical evidence of SPL adoption in the literature, which makes it difficult to strengthen or elaborate adjustments or improvements to SPL frameworks. In this article, we present the first steps towards an empirical evaluation by showing how companies that do not know about of SPL manage variability in their products, pointing out the strengths and weaknesses of their approaches. To this end, we present the design of a case study that we plan to carry out in the future in two companies to evaluate how companies perform variability management when they are not aware of software product lines. Our assumption is that most of the companies manage variability but no many of them are aware of software product lines. In addition, the first preliminary results of the case study applied in a company are presented.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {82–89},
numpages = {8},
keywords = {a case study, software product lines, variability management},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1145/3442389,
author = {Castro, Thiago and Teixeira, Leopoldo and Alves, Vander and Apel, Sven and Cordy, Maxime and Gheyi, Rohit},
title = {A Formal Framework of Software Product Line Analyses},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3442389},
doi = {10.1145/3442389},
abstract = {A number of product-line analysis approaches lift analyses such as type checking, model checking, and theorem proving from the level of single programs to the level of product lines. These approaches share concepts and mechanisms that suggest an unexplored potential for reuse of key analysis steps and properties, implementation, and verification efforts. Despite the availability of taxonomies synthesizing such approaches, there still remains the underlying problem of not being able to describe product-line analyses and their properties precisely and uniformly. We propose a formal framework that models product-line analyses in a compositional manner, providing an overall understanding of the space of family-based, feature-based, and product-based analysis strategies. It defines precisely how the different types of product-line analyses compose and inter-relate. To ensure soundness, we formalize the framework, providing mechanized specification and proofs of key concepts and properties of the individual analyses. The formalization provides unambiguous definitions of domain terminology and assumptions as well as solid evidence of key properties based on rigorous formal proofs. To qualitatively assess the generality of the framework, we discuss to what extent it describes five representative product-line analyses targeting the following properties: safety, performance, dataflow facts, security, and functional program properties.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {34},
numpages = {37},
keywords = {Software product lines, product-line analysis}
}

@inproceedings{10.1145/3425269.3425271,
author = {Nicolodi, Luciane Baldo and Colanzi, Thelma Elita and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Architectural Feature Re-Modularization for Software Product Line Evolution},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425271},
doi = {10.1145/3425269.3425271},
abstract = {Extensive maintenance leads to the Software Product Line Architecture (PLA) degradation over time. When there is the need of evolving the Software Product Line (SPL) to include new features, or move to a new platform, a degraded PLA requires considerable effort to understand and modify, demanding expensive refactoring activity. In the state of the art, search-based algorithms are used to improve PLA at package level. However, recent studies have shown that the most variability and implementation details of an SPL are described in the level of classes. There is a gap between existing approaches and existing practical needs. In this work, we extend the current state of the art to deal with feature modularization in the level of classes by introducing a new search operator and a set of objective functions to deal with feature modularization in a finer granularity of the architectural elements, namely at class level. We evaluated the proposal in an exploratory study with a PLA widely investigated and a real-world PLA. The results of quantitative and qualitative analysis point out that our proposal provides solutions to properly re-modularize features in a PLA, being preferred by practitioners, in order to support the evolution of SPLs.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {31–40},
numpages = {10},
keywords = {Architectural Degradation, Feature Modularization, Search-based Software Engineering, Software Evolution},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1145/3546932.3547008,
author = {Amraoui, Yassine El and Blay-Fornarino, Mireille and Collet, Philippe and Precioso, Fr\'{e}d\'{e}ric and Muller, Julien},
title = {Evolvable SPL management with partial knowledge: an application to anomaly detection in time series},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547008},
doi = {10.1145/3546932.3547008},
abstract = {In Machine Learning (ML), the resolution of anomaly detection problems in time series presents a great diversity of practices as it can correspond to many different contexts. These practices cover both grasping the business problem and designing the solution itself. By practice, we designate explicit and implicit steps toward resolving a problem, while a solution corresponds to a combination of algorithms selected for their performance on a given problem. Two related issues arise. The first one is that the practices are individual and not explicitly mutualized. The second one is that choosing one solution over another is all the more difficult to justify because the space of solutions and the evaluation criteria are vast and evolve rapidly with the advances in ML. To solve these issues and tame the evolving diversity in ML, a Software Product Line (SPL) approach can be envisaged to represent the variable set of solutions. However, this requires characterizing an ML business problem through an explicit set of criteria and justifying one ML solution over all others. The resolution of anomaly detection problems is thus different from finding the best configuration workflow from past configurations but lies more in guiding the configuration towards a solution that may never have been studied before. This paper proposes an SPL approach that capitalizes on past practices by exploiting a variability-aware representation to detect new criteria and constraints when practices adopt different solutions to seemingly similar problems. We report on the evaluation of our approach using a set of applications from the literature and an ML software company. We show how the analysis of practices makes it possible to consolidate the knowledge contained in the SPL.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {222–233},
numpages = {12},
keywords = {evolution, machine learning, metrics, software product line},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3646548.3676541,
author = {Jadoon, Gullelala},
title = {Preserving Non-Functional Requirements in Goal Models Using Meta-models of the Software Product Lines},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676541},
doi = {10.1145/3646548.3676541},
abstract = {Non-functional requirements (NFRs) play a critical role in software product line (SPL) engineering, ensuring products meet essential criteria beyond mere functionality. However, preserving NFRs across product variants induces considerable challenges, particularly in goal-oriented SPLE where goals guide product derivation. This research proposes a novel framework to preserve NFRs in goal models using meta-models of SPLs and manage inconsistent NFRs. The framework utilizes product and domain meta-models to accurately capture and represent NFRs, addressing construct validity concerns. This research aims to enhance the credibility and generalizability of findings in SPL engineering, contributing to the advancement of goal-oriented modeling and NFR preservation practices.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {1–5},
numpages = {5},
keywords = {Goal models, Meta-models, Non-functional requirements, Property preservation, Software Product Line Engineering},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3646548.3672582,
author = {Corti\~{n}as, Alejandro and Lamas, Victor and R. Luaces, Miguel},
title = {SensorPublisher: Applying Software Product Lines to the development of IoT dashboards},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672582},
doi = {10.1145/3646548.3672582},
abstract = {Geosciences have witnessed a revolution in data collection thanks to the Internet of Things (IoT), which has made it possible to monitor complex phenomena using sensor networks. However, developing web-centric, sensor-based, data warehousing information systems presents challenges because of their complexity and cost. This paper presents an intuitive low-code development system (called SensorPublisher), based on a software product line (SPL) and a domain-specific language (DSL), that speeds up the creation of data warehousing applications for geographic sensor data. SensorPublisher allows the geoscientist to define the sensor network, to generate a software product, and to deploy the product to a local or a remote server. Our tool seeks to encourage scientists to share the outcomes of their sensor data analysis projects with their communities by means of a simple, user-friendly and cost-effective approach. We showcase the system in different geoscientific domains, such as meteorological monitoring services, traffic data and air quality monitoring in urban areas, and marine area monitoring systems.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {153–163},
numpages = {11},
keywords = {Domain Specific Language (DSL), Internet of Things (IoT), Software Product Line (SPL)},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/2362536.2362545,
author = {Lee, Jihyun and Kang, Sungwon and Lee, Danhyung},
title = {A survey on software product line testing},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362545},
doi = {10.1145/2362536.2362545},
abstract = {Software product line (SPL) testing consists of two separate but closely related test engineering activities: domain testing and application testing. Various software product line testing approaches have been developed over the last decade, and surveys have been conducted on them. However, thus far none of them deeply addressed the questions of what researches have been conducted in order to overcome the challenges posed by the two separate testing activities and their relationships. Thus, this paper surveys the current software product line testing approaches by defining a reference SPL testing processes and identifying, based on them, key research perspectives that are important in SPL testing. Through this survey, we identify the researches that addressed the challenges and also derive open research opportunities from each perspective.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {31–40},
numpages = {10},
keywords = {software testing, software product line testing, software product line engineering},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2647908.2655967,
author = {Assun\c{c}\~{a}o, Wesley Klewerton Guez and Vergilio, Silvia Regina},
title = {Feature location for software product line migration: a mapping study},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655967},
doi = {10.1145/2647908.2655967},
abstract = {Developing software from scratch is a high cost and error-prone activity. A possible solution to reduce time-to-market and produce high quality software is the reuse of existing software. But when the number of features in the system grows, the maintenance becomes more complex. In such cases, to adopt a systematic approach, such as Software Product Line Engineering, is necessary. Existing systems are generally migrated to a product line, allowing systematic reuse of artefacts and easing maintenance. To this end, some approaches have been proposed in the literature in the last years. A mapping of works on this subject and the identification of some research gaps can lead to an improvement of such approaches. This paper describes the main outcomes of a systematic mapping study on the evolution and migration of systems to SPL. The main works found are presented and classified according to adopted strategy, artefacts used, and evaluation conducted. Analysis of the evolution along the past years are also presented. At the end, we summarize some trends and open issues to serve as reference to new researches.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {52–59},
numpages = {8},
keywords = {software product line, reuse, reengineering, evolution},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2934466.2934481,
author = {Sion, Laurens and Van Landuyt, Dimitri and Joosen, Wouter and de Jong, Gjalt},
title = {Systematic quality trade-off support in the software product-line configuration process},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934481},
doi = {10.1145/2934466.2934481},
abstract = {Software product line engineering is a compelling methodology that accomplishes systematic reuse in families of systems by relying on two key principles: (i) the decomposition of complex systems into composable and reusable building blocks (often logical units called features), and (ii) on-demand construction of products and product variants by composing these building blocks.However, unless the stakeholder responsible for product configuration has detailed knowledge of the technical ins and outs of the software product line (e.g., the architectural impact of a specific feature, or potential feature interactions), he is in many cases flying in the dark. Although many initial approaches and techniques have been proposed that take into account quality considerations and involve trade-off decisions during product configuration, no systematic support exists.In this paper, we present a reference architecture for product configuration tooling, providing support for (i) up-front generation of variants, and (ii) quality analysis of these variants. This allows pro-actively assessing and predicting architectural quality properties for each product variant and in turn, product configuration tools can take into account architectural considerations. In addition, we provide an in-depth discussion of techniques and tactics for dealing with the problem of variant explosion, and as such to maintain practical feasibility of such approaches.We validated and implemented our reference architecture in the context of a real-world industrial application, a product-line for the firmware of an automotive sensor. Our prototype, based on FeatureIDE, is open for extension and readily available.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {164–173},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3106195.3106224,
author = {Tizzei, Leonardo P. and Nery, Marcelo and Segura, Vin\'{\i}cius C. V. B. and Cerqueira, Renato F. G.},
title = {Using Microservices and Software Product Line Engineering to Support Reuse of Evolving Multi-tenant SaaS},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106224},
doi = {10.1145/3106195.3106224},
abstract = {In order to achieve economies of scale, a Software as a Service (SaaS) should be configurable, multi-tenant efficient, and scalable. But building SaaS with these characteristics comes at a price of having more complex services. Some works in the literature integrate software product line engineering and service-oriented architecture to tackle the complexity of building multi-tenant SaaS. Most of these works focused on centralized approaches that rely on middleware or platforms, but they do not investigate the use of decentralized architectural style. Microservices architecture is an architectural style that relies on small, decentralized, and autonomous services that work together. Thus, this paper investigates the integrated use of microservices architecture and software produt line techniques to develop multi-tenant SaaS. We conducted an empirical study that analyzes the behavior of software reuse during the evolution of a multi-tenant SaaS. This empirical study showed an average software reuse of 62% of lines of code among tenants. We also provide lessons we learned during the the re-engineering and maintenance of such multi-tenant SaaS.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {205–214},
numpages = {10},
keywords = {Software Reuse, Software Evolution, Service-oriented Architectures, Multi-tenancy, Microservices},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3289402.3289504,
author = {Sebbaq, Hanane and Retbi, Asmaa and Idrissi, Mohammed Khalidi and Bennani, Samir},
title = {Software Product Line to overcome the variability issue in E-Learning: Systematic literature review},
year = {2018},
isbn = {9781450364621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289402.3289504},
doi = {10.1145/3289402.3289504},
abstract = {The disparity of educational technologies, pedagogies and learning styles implies a problem of variability when modeling E-learning systems. Furthermore, the current learning context, which has become very open and heterogeneous, raises the problem of automating the modeling, development and maintenance of personalized E-learning systems based on various pedagogies. For its part, the "Software Product Line" is a paradigm that aims to produce product families based on the principles of reuse, configuration and derivation. The main purpose of this literature review is to explore the different potential applications of "SPL" in the E-learning domain to figure out the problem of variability. We will adopt a protocol for a systematic review of literature, after which we will draw up an analysis report.},
booktitle = {Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications},
articleno = {4},
numpages = {8},
keywords = {variety, systematic literature review, scale, heterogeneity, Variability, Software Product line, E-learning},
location = {Rabat, Morocco},
series = {SITA'18}
}

@inproceedings{10.1145/2648511.2648513,
author = {Harman, M. and Jia, Y. and Krinke, J. and Langdon, W. B. and Petke, J. and Zhang, Y.},
title = {Search based software engineering for software product line engineering: a survey and directions for future work},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648513},
doi = {10.1145/2648511.2648513},
abstract = {This paper presents a survey of work on Search Based Software Engineering (SBSE) for Software Product Lines (SPLs). We have attempted to be comprehensive, in the sense that we have sought to include all papers that apply computational search techniques to problems in software product line engineering. Having surveyed the recent explosion in SBSE for SPL research activity, we highlight some directions for future work. We focus on suggestions for the development of recent advances in genetic improvement, showing how these might be exploited by SPL researchers and practitioners: Genetic improvement may grow new products with new functional and non-functional features and graft these into SPLs. It may also merge and parameterise multiple branches to cope with SPL branchmania.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {5–18},
numpages = {14},
keywords = {program synthesis, genetic programming, SPL, SBSE},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, M\'{a}rcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Software Product Lines, Recommender Systems, Non-Functional Properties, Feature Model, Configuration},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/2647908.2655970,
author = {ter Beek, Maurice H. and de Vink, Erik P.},
title = {Software product line analysis with mCRL2},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655970},
doi = {10.1145/2647908.2655970},
abstract = {The mCRL2 language and supporting software provide a state-of-the-art tool suite for the verification of distributed systems. In this paper, we present the general principles, extrapolated from [7,8], which make us believe that mCRL2 can also be used for behavioral variability analysis of product families. The mCRL2 data language allows to smoothly deal with feature sets and attributes, its process language is sufficiently rich to model feature selection, as well as product behavior based on an FTS-like semantics. Because of the feature-orientation, our modeling strategy allows a natural refactoring of the semantic model of a product family into a parallel composition of components that reflects coherent sets of features. This opens the way for dedicated abstraction and reduction techniques that strengthen the prospect of a scalable verification approach to software product lines. In this paper, we sketch how to model product families in mCRL2 and how to apply a modular verification method, preparing the ground to further assess the scalability of our approach, in particular regarding model checking.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {78–85},
numpages = {8},
keywords = {variability, product families, modular verification, model checking, behavioral analysis},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2648511.2648537,
author = {Colanzi, Thelma Elita and Vergilio, Silvia Regina and Gimenes, Itana M. S. and Oizumi, Willian Nalepa},
title = {A search-based approach for software product line design},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648537},
doi = {10.1145/2648511.2648537},
abstract = {The Product Line Architecture (PLA) can be improved by taking into account key factors such as feature modularization, and by continuously evaluating its design according to metrics. Search-Based Software Engineering (SBSE) principles can be used to support an informed-design of PLAs. However, existing search-based design works address only traditional software design not considering intrinsic Software Product Line aspects. This paper presents MOA4PLA, a search-based approach to support the PLA design. It gives a multi-objective treatment to the design problem based on specific PLA metrics. A metamodel to represent the PLA and a novel search operator to improve feature modularization are proposed. Results point out that the application of MOA4PLA leads to PLA designs with well modularized features, contributing to improve features reusability and extensibility. It raises a set of solutions with different design trade-offs that can be used to improve the PLA design.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {237–241},
numpages = {5},
keywords = {software product lines, searchbased PLA design, multi-objective algorithms},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2666581.2666589,
author = {Shi, Yufeng and Wei, Ou and Zhou, Yu},
title = {Model checking partial software product line designs},
year = {2014},
isbn = {9781450332262},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666581.2666589},
doi = {10.1145/2666581.2666589},
abstract = {Software product line (SPL) maximizes commonality between software products to reduce cost and improve productivity, where each product is represented by a selection of features that corresponds to particular customer requirements. SPL has been widely applied in critical systems such as communications, automobile, and aerospace, and ensuring correctness of the system is thus of great importance. In this paper, we consider model checking partial software product line designs, i.e., the incomplete designs in the early stage of software development, where the design decisions for a feature may be unknown. This enables detecting design errors earlier, reducing the cost of later development of final products. To this end, we first propose bilattice-based feature transitions systems (BFTSs) for modeling partial software product line designs, which support description of uncertainty and preserve features as a first class notion. We then express system behavioral properties using ACTL formulas and define its semantics over BFTSs. Finally, to leverage the power of existing model checking engine for verification, we provide the procedures that translate BFTSs and ACTL formulas to the inputs of the symbolic model checker $Chi$Chek. We implement our approach and illustrate its effectiveness on a benchmark from literature.},
booktitle = {Proceedings of the International Workshop on Innovative Software Development Methodologies and Practices},
pages = {21–29},
numpages = {9},
keywords = {Software Product Line, Partial Model, Model Checking},
location = {Hong Kong, China},
series = {InnoSWDev 2014}
}

@inproceedings{10.1145/2993236.2993251,
author = {Steindorfer, Michael J. and Vinju, Jurgen J.},
title = {Towards a software product line of trie-based collections},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993251},
doi = {10.1145/2993236.2993251},
abstract = {Collection data structures in standard libraries of programming languages are designed to excel for the average case by carefully balancing memory footprint and runtime performance. These implicit design decisions and hard-coded trade-offs do constrain users from using an optimal variant for a given problem. Although a wide range of specialized collections is available for the Java Virtual Machine (JVM), they introduce yet another dependency and complicate user adoption by requiring specific Application Program Interfaces (APIs) incompatible with the standard library.  A product line for collection data structures would relieve library designers from optimizing for the general case. Furthermore, a product line allows evolving the potentially large code base of a collection family efficiently. The challenge is to find a small core framework for collection data structures which covers all variations without exhaustively listing them, while supporting good performance at the same time.  We claim that the concept of Array Mapped Tries (AMTs) embodies a high degree of commonality in the sub-domain of immutable collection data structures. AMTs are flexible enough to cover most of the variability, while minimizing code bloat in the generator and the generated code. We implemented a Data Structure Code Generator (DSCG) that emits immutable collections based on an AMT skeleton foundation. The generated data structures outperform competitive hand-optimized implementations, and the generator still allows for customization towards specific workloads.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {168–172},
numpages = {5},
keywords = {Software product line, Persistent data structure, Performance, Immutability, Hash trie, Code generation},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@inproceedings{10.1145/2648511.2648515,
author = {Wang, Shuai and Buchmann, David and Ali, Shaukat and Gotlieb, Arnaud and Pradhan, Dipesh and Liaaen, Marius},
title = {Multi-objective test prioritization in software product line testing: an industrial case study},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648515},
doi = {10.1145/2648511.2648515},
abstract = {Test prioritization is crucial for testing products in a product line considering limited budget in terms of available time and resources. In general, it is not practically feasible to execute all the possible test cases and so, ordering test case execution permits test engineers to discover faults earlier in the testing process. An efficient prioritization of test cases for one or more products requires a clear consideration of the tradeoff among various costs (e.g., time, required resources) and effectiveness (e.g., feature coverage) objectives. As an integral part of the future Cisco's test scheduling system for validating video conferencing products, we introduce a search-based multi-objective test prioritization technique, considering multiple cost and effectiveness measures. In particular, our multi-objective optimization setup includes the minimization of execution cost (e.g., time), and the maximization of number of prioritized test cases, feature pairwise coverage and fault detection capability. Based on cost-effectiveness measures, a novel fitness function is defined for such test prioritization problem. The fitness function is empirically evaluated together with three commonly used search algorithms (e.g., (1+1) Evolutionary algorithm (EA)) and Random Search as a comparison baseline based on the Cisco's industrial case study and 500 artificial designed problems. The results show that (1+1) EA achieves the best performance for solving the test prioritization problem and it scales up to solve the problems of varying complexity.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {32–41},
numpages = {10},
keywords = {test prioritization, software product lines, search algorithms, multi-objective optimization},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3266237.3266275,
author = {Filho, Helson Luiz Jakubovski and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Multiple objective test set selection for software product line testing: evaluating different preference-based algorithms},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266275},
doi = {10.1145/3266237.3266275},
abstract = {The selection of optimal test sets for Software Product Lines (SPLs) is a complex task impacted by many factors and that needs to consider the tester's preferences. To help in this task, Preference-based Evolutionary Multi-objective Algorithms (PEMOAs) have been explored. They use a Reference Point (RP), which represents the user preference and guides the search, resulting in a greater number of solutions in the ROI (Region of Interest). This region contains solutions that are more interesting from the tester's point of view. However, the explored PEMOAs have not been compared yet and the results reported in the literature do not consider many-objective formulations. Such an evaluation is important because in the presence of more than three objectives the performance of the algorithms may change and the number of solutions increases. Considering this fact, this work presents evaluation results of four PEMOAs for selection of products in the SPL testing considering cost, testing criteria coverage, products similarity, and the number of revealed faults, given by the mutation score. The PEMOAs present better performance than traditional algorithms, avoiding uninteresting solutions. We introduce a hyper-heuristic version of the PEMOA R-NSGA-II that presents the best results in a general case.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {162–171},
numpages = {10},
keywords = {software product line testing, search-based software engineering, preference-based multi-objective algorithms},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@inproceedings{10.1145/3579027.3608975,
author = {Burgue\~{n}o, Lola and Horcas, Jose-Miguel and Kienzle, J\"{o}rg},
title = {Development and Evolution of Software Product Lines Driven by Stakeholder Beliefs},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608975},
doi = {10.1145/3579027.3608975},
abstract = {The planning, realization, and release of a Software Product Line (SPL) are driven by features. Therefore, many high-level decisions about the evolution of an SPL are made at the feature level. However, a feature can involve many stakeholders with different expertise, and taking their opinions into account to make the right decisions is not trivial. In this paper, we propose using belief uncertainty in conjunction with feature models to assist in the evolution of SPLs by explicitly quantifying opinions. We outline three evolution scenarios in which subjective logic can be used to represent the opinions of stakeholders and explain in detail how to use subjective logic to make decisions in the context of the next release problem. We illustrate our ideas with a Smartwatch SPL. Finally, we discuss different ways of combining the opinions of stakeholders depending on the situation, the goals and the risks that can be assumed.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {34–40},
numpages = {7},
keywords = {uncertainty, subjective logic, software product line, feature model, Decision making support},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3056662.3056663,
author = {Kang, Sungwon and Kim, Jungmin and Baek, Haeun and Ahn, Hwi and Jung, Pilsu and Lee, Jihyun},
title = {Comparison of software product line test derivation methods from the reuse viewpoint},
year = {2017},
isbn = {9781450348577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3056662.3056663},
doi = {10.1145/3056662.3056663},
abstract = {Product line test development is more complicated than test development for a single application, as the former has to deal with variability among assets (artifacts) and is carried out in two separate but related development phases, i.e. domain engineering and application engineering. Different software product test development methods provide different opportunities for reuse depending on how variability is represented in the domain test artifacts, when binding is formed and applied and also when test data are determined. This paper compares and analyzes the six major methods for the software product line test development in the literature. Through the comparison, we find out that existing software product line testing methods did not fully consider the aspects of software product line that are essential for reuse in software product line development such as variability representation, binding formation and application time and test data determination time. As the conclusion of this literature review, this paper suggests future research opportunities for software product line testing to explore.},
booktitle = {Proceedings of the 6th International Conference on Software and Computer Applications},
pages = {1–8},
numpages = {8},
keywords = {systematic product line testing, software testing, software product line development},
location = {Bangkok, Thailand},
series = {ICSCA '17}
}

@inproceedings{10.1145/3167132.3167353,
author = {Pereira, Juliana Alves and Martinez, Jabier and Gurudu, Hari Kumar and Krieter, Sebastian and Saake, Gunter},
title = {Visual guidance for product line configuration using recommendations and non-functional properties},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167353},
doi = {10.1145/3167132.3167353},
abstract = {Software Product Lines (SPLs) are a mature approach for the derivation of a family of products using systematic reuse. Different combinations of predefined features enable tailoring the product to fit the needs of each customer. These needs are related to functional properties of the system (optional features) as well as non-functional properties (e.g., performance or cost of the final product). In industrial scenarios, the configuration process of a final product is complex and the tool support is usually limited to check functional properties interdependencies. In addition, the importance of nonfunctional properties as relevant drivers during configuration has been overlooked. Thus, there is a lack of holistic paradigms integrating recommendation systems and visualizations that can help the decision makers. In this paper, we propose and evaluate an interrelated set of visualizations for the configuration process filling these gaps. We integrate them as part of the FeatureIDE tool and we evaluate its effectiveness, scalability, and performance.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {2058–2065},
numpages = {8},
keywords = {visualization, software product lines, recommendation systems, feature model, configuration},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/2739482.2764650,
author = {Karimpour, Reza and Ruhe, Guenther},
title = {A Search Based Approach Towards Robust Optimization in Software Product Line Scoping},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2764650},
doi = {10.1145/2739482.2764650},
abstract = {Software product line (SPL) scoping is important for planning upfront investment. One challenge with scoping comes from inaccuracies in estimated parameters and uncertainty in environment. In this paper, a method to incorporate uncertainty in SPL scoping optimization and its application to generate robust solutions is proposed. We model scoping optimization as a multi-objective problem with profit and stability as heuristics. To evaluate our proposal, a number of experiments are conducted. Analysis of results show that both performance stability and feasibility stability were improved providing the product line manager enhanced decision-making support.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1415–1416},
numpages = {2},
keywords = {uncertainty, software product line portfolio scoping, robust optimization, multi-objective},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@inproceedings{10.1145/3646548.3676546,
author = {G\"{u}thing, Lukas and Pett, Tobias and Schaefer, Ina},
title = {Out-of-the-Box Prediction of Non-Functional Variant Properties Using Automated Machine Learning},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676546},
doi = {10.1145/3646548.3676546},
abstract = {A configurable system is characterized by the configuration options present or absent in its variants. Selecting and deselecting those configuration options directly influences the functional properties of the system. Apart from functional properties, there are system characteristics that influence the performance (e.g., power demand), safety (e.g., fault probabilities), and security (e.g., susceptibility to attacks) of the system, called Non-Functional Properties (NFPs). Knowledge of NFPs is crucial for evaluating a system’s feasibility, usability, and resource demands. Although variability influences these characteristics, NFPs do not compose linearly for every selected feature. Feature interactions can increase the overall NFP values through (potentially exponential) amplification or decrease them through mitigation effects. In this paper, we propose an automated machine learning (AutoML) approach to predict NFP values for new configurations based on previously measured configuration values. Using AutoML, we leverage the advantages of machine learning for predicting NFPs without having to parameterize and fine-tune machine learning models. This approach and the resulting pipeline aim to reduce the complexity of performance prediction for configurable systems. We test the feasibility of our pipeline in a first evaluation on 4 real-world subject systems and discuss cases where AutoML may improve the prediction of NFPs.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {82–87},
numpages = {6},
keywords = {AutoML, Cyber-physical systems, Machine learning, Software product lines},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3579027.3608992,
author = {Knop, Udo and Hofman, Peter and Mihatsch, Michael and Siegmund, Martin},
title = {Balancing Variability and Costs in Software Product Lines: An Experience Report in Safety-Critical Systems},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608992},
doi = {10.1145/3579027.3608992},
abstract = {This paper provides a detailed testing strategy for Software Product Lines (SPLs), aiming to balance the variability offered with its associated cost. Utilizing feature modeling, combinatorial testing, and deployment-based testing, the strategy addresses the unique challenges of testing SPLs, including the large number of possible feature interactions.The paper includes an experience report in the Syngo SPL by Siemens Healthineers that had approximately 900 optional features and was deployed at about 35.000 end-customer installations. Testing this variability using conventional approaches was an immense challenge due to the vast number of required test cases and test configurations. As a result, the actual variability offered to customers was restricted to four configurations, and the time-to-market of new features was limited to four releases per year.The project's goal was to devise a testing strategy to deliver a greater variety of configurations in a shorter time without substantially increasing testing effort or compromising quality. By restricting feature interaction testing to dependent features and those included in customer offerings, the strategy demonstrated the feasibility of maintaining high-quality deliverable configurations without significant increase in testing effort. Additionally, it allowed the accurate identification of necessary test case adjustments or developments when introducing new functionality, enabling significant reuse of existing test cases, and facilitating a transition to monthly release cycles.Overall, this paper provides valuable insights for practitioners and researchers working with SPLs and facing the challenges of testing them. The presented strategy offers a promising approach to reducing the number of cross-module test cases required in software platform development, demonstrating its feasibility and potential benefits.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {213–222},
numpages = {10},
keywords = {test case construction, feature model, deployment-based testing, Software product Line},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/2814204.2814212,
author = {Seidl, Christoph and Schuster, Sven and Schaefer, Ina},
title = {Generative software product line development using variability-aware design patterns},
year = {2015},
isbn = {9781450336871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814204.2814212},
doi = {10.1145/2814204.2814212},
abstract = {Software Product Lines (SPLs) are an approach to reuse in-the-large that models a set of closely related software systems in terms of commonalities and variabilities. Design patterns are best practices for addressing recurring design problems in object-oriented source code. In the practice of implementing an SPL, instances of certain design patterns are employed to handle variability, which makes these "variability-aware design patterns" a best practice for SPL design. However, there currently is no dedicated method for proactively developing SPL using design patterns suitable for realizing variable functionality. In this paper, we present a method to perform generative SPL development with design patterns. We use role models to capture design patterns and their relation to a variability model. We further allow mapping of individual design pattern roles to elements of realization artifacts to be generated (e.g., classes, methods) and check the conformance of the realization with the specification of the pattern. With this method, we support proactive development of SPL using design patterns to apply best practices for the realization of variability. We present an implementation of our approach within the Eclipse IDE and demonstrate it within a case study.},
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {151–160},
numpages = {10},
keywords = {Software Product Line (SPL), Role Modeling, Generative Development, Design Pattern},
location = {Pittsburgh, PA, USA},
series = {GPCE 2015}
}

@inproceedings{10.1145/3646548.3676548,
author = {Romero-Organvidez, David and Neira, Pablo and Galindo, Jos\'{e} A. and Benavides, David},
title = {Kconfig metamodel: a first approach},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676548},
doi = {10.1145/3646548.3676548},
abstract = {Kconfig is the de facto configuration language for describing and configuring the variability of the Linux kernel. Nonetheless, it has been used since the early stages of kernel development. Moreover, Kconfig is also used as a niche configuration languages, such as microkernel compilation for air navigation systems, proprietary routers or embedded systems. In the last decade, the software product line (SPL) community worked intensively on observing Linux Kernel and Kconfig. However, the official documentation is difficult to understand, and the examples are long and challenging to synthesize for non-Kconfig experts, such as SPL engineers, and researchers. In this paper, we propose a Kconfig metamodel based on the documentation and the feedback of a kernel developer expert. Thanks to this metamodel, the design of transformations from Kconfig to other variability models such as UVL (Universal Variability Language) can be facilitated. To our knowledge, this is the first proposal for a metamodel of the Kconfig language. This opens the door to further research, such as Kconfig analysis and transformations, and leverage interoperability among the Kconfig toolchain and SPL tools.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {55–60},
numpages = {6},
keywords = {Kconfig, linux kernel, metamodel, variability},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/2364412.2364425,
author = {Cordy, Maxime and Schobbens, Pierre-Yves and Heymans, Patrick and Legay, Axel},
title = {Towards an incremental automata-based approach for software product-line model checking},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364425},
doi = {10.1145/2364412.2364425},
abstract = {Most model-checking algorithms are based on automata theory. For instance, determining whether or not a transition system satisfies a Linear Temporal Logic (LTL) formula requires computing strongly connected component of its transition graph. In Software Product-Line (SPL) engineering, the model checking problem is more complex due to the huge amount of software products that may compose the line. Indeed, one has to determine the exact subset of those products that do not satisfy an intended property. Efficient dedicated verification methods have been recently developed to answer this problem. However, most of them does not allow incremental verification. In this paper, we introduce an automata-based incremental approach for SPL model checking. Our method makes use of previous results to determine whether or not the addition of conservative features (i.e., features that do not remove behaviour from the system) preserves the satisfaction of properties expressed in LTL. We provide a detailed description of the approach and propose algorithms that implement it. We discuss how our method can be combined with SPL dedicated verification methods, viz. Featured Transition Systems.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {74–81},
numpages = {8},
keywords = {software product lines, modularity, model checking},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3579027.3608994,
author = {Fadhlillah, Hafiyyan Sayyid and Fern\'{a}ndez, Antonio M. Guti\'{e}rrez and Rabiser, Rick and Zoitl, Alois},
title = {Managing Cyber-Physical Production Systems Variability using V4rdiac: Industrial Experiences},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608994},
doi = {10.1145/3579027.3608994},
abstract = {Cyber-Physical Production Systems (CPPSs) are highly robust and versatile production systems that utilize diverse hardware components through control software. Employing a systematic variability management approach for developing variants of control software can reduce cost and time-to-market to build such complex systems. However, employing this approach in the CPPS domain is challenging. Engineering CPPSs require multidisciplinary engineering knowledge (e.g., process, signal, mechanical). Knowledge about CPPS variability is thus typically scattered across diverse engineering artifacts. Also, variability knowledge is usually not documented explicitly but rather tacit knowledge of mostly senior engineers. Furthermore, control software is commonly implemented using a graphical Domain-Specific Modeling Language (DSML) which only provides minimal support to express variability. This paper describes our experiences dealing with these challenges in an industrial context using a multidisciplinary variability management approach called Variability for 4diac (V4rdiac). V4rdiac is an integrated approach that allows CPPS engineers to conduct stepwise product configuration based on heterogeneous variability models from multiple engineering disciplines. V4rdiac also provides a mechanism to automatically generate control software based on a set of selected configuration options. We evaluate how V4rdiac implements and manages CPPS control software variants in the metallurgical production plant domain. We describe the benefits and lessons learned from using V4rdiac in this domain based on feedback from industrial practitioners.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {223–233},
numpages = {11},
keywords = {Variability Modeling, Software Product Line, Software Configuration, Cyber-Physical Production System},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3646548.3672584,
author = {Kogler, Philipp and Chen, Wei and Falkner, Andreas and Haselb\"{o}ck, Alois and Wallner, Stefan},
title = {Modelling Engineering Processes in Natural Language: A Case Study},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672584},
doi = {10.1145/3646548.3672584},
abstract = {Engineering process management aims to formally specify processes which are executable, measurable, and controllable. Common representations include text-based domain-specific languages (DSLs) or graphical notations such as the Business Process Modelling Notation (BPMN). The specification itself can be seen as a Software Product Line (SPL), building upon concepts such as tasks, UI forms, fields and actions. Domain experts provide requirements for processes but often lack the technical programming skills to formalize them in a process specification language. We present an interactive SPL application prototype that allows domain experts to model simple processes in natural language. Our framework for the reliable generation of formal specifications with Large Language Models (LLMs) supports the machine-translation from natural language to a JSON-based process DSL. In this case study, five domain experts were asked to model any process of their choice through natural-language interactions. As a result, the user interface corresponding to the process DSL was shown as immediate feedback. We documented their perceived translation quality and interviewed them on their impressions of this methodology. An average user-assessed performance rating of 68% was achieved. Even though the modelling strategies differed greatly between individuals, the tool was able to adequately capture the majority of instructions, leaving an overall positive impression on the participants. More context awareness and additional conventional interaction elements were the main aspects found to be improved for a productive implementation.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {170–178},
numpages = {9},
keywords = {Domain-specific Languages, Generative Artificial Intelligence, Large Language Models, Process Management, Process Modelling, Reliable Code Generation},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3646548.3672588,
author = {Burgstaller, Tamim and Garber, Damian and Le, Viet-Man and Felfernig, Alexander},
title = {Optimization Space Learning: A Lightweight, Noniterative Technique for Compiler Autotuning},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672588},
doi = {10.1145/3646548.3672588},
abstract = {Compilers are highly configurable systems. One can influence the performance of a compiled program by activating and deactivating selected compiler optimizations. However, automatically finding well-performing configurations is a challenging task. We consider expensive iteration, paired with recompilation of the program to optimize, as one of the main shortcomings of state-of-the-art approaches. Therefore, we propose Optimization Space Learning, a lightweight and noniterative technique. It exploits concepts known from configuration space learning and recommender systems to discover well-performing compiler configurations. This reduces the overhead induced by the approach significantly, compared to existing approaches. The process of finding a well-performing configuration is 800k times faster than with the state-of-the-art techniques.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {36–46},
numpages = {11},
keywords = {Collaborative Filtering, Compiler, Compiler Autotuning, Configuration, Configuration Space Learning, Performance Optimization},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.5555/2814058.2814112,
author = {Lobato, Luanna Lopes and Bittar, Thiago Jabur},
title = {A Risk Management Approach for Software Product Line Engineering},
year = {2015},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {TSoftware Product Line (SPL) Engineering is a software development paradigm that fosters systematic reuse. It is focused on improving software practices, leading companies to experience benefits, such as reduced time-to-market and effort, and higher quality for the products delivered to customers. However, establishing a SPL is neither a simple nor a cheap task, and may affect several aspects of a software company. Besides, it involves a range of risks that may hinder project success. These have to be managed accordingly, so as to minimize the likelihood of project failure. Despite the importance of Risk Management (RM) for SPL Engineering, little has been published in terms of suitable and structured practices to cope with that. This present paper reports an approach for RM in SPL Engineering, named RiPLERM (Rise Product Line Engineering and Risk Management). The approach presents activities to structure RM in SPL projects, The design of the RiPLE-RM approach elaborated on results from empirical investigations, and was proposed to facilitate the management and provide significant insights that can be used to avoid and solve risks.},
booktitle = {Proceedings of the Annual Conference on Brazilian Symposium on Information Systems: Information Systems: A Computer Socio-Technical Perspective - Volume 1},
pages = {331–338},
numpages = {8},
keywords = {Software Product Line Engineering, Software Process, Risk Management, Project management},
location = {Goiania, Goias, Brazil},
series = {SBSI '15}
}

@inproceedings{10.1145/3546932.3547007,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Quality-aware analysis and optimisation of virtual network functions},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547007},
doi = {10.1145/3546932.3547007},
abstract = {The softwarisation and virtualisation of network functionality is the last milestone in the networking industry. Software-Defined Networks (SDN) and Network Function Virtualization (NFV) offer the possibility of using software to manage computer and mobile networks and build novel Virtual Network Functions (VNFs) deployed in heterogeneous devices. To reason about the variability of network functions and especially about the quality of a software product defined as a set of VNFs instantiated as part of a service (i.e., Service Function Chaining), a variability model along with a quality model is required.However, this domain imposes certain challenges to quality-aware reasoning of service function chains, such as numerical features or configuration-level Quality Attributes (QAs) (e.g., energy consumption). Incorporating numerical reasoning with quality data into SPL analyses is challenging and tool support is rare. In this work, we present 3 groups of operations: model report, aggregate functions to dynamically convert QAs at the feature-level into the configuration-level, and quality-aware optimisation. Our objective is to test the most complete reasoning tools to exploit the extended variability with quality attributes needed for VNFs.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {210–221},
numpages = {12},
keywords = {virtual network function, variability, reasoning, quality attribute, optimization, numerical feature},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/2816839.2816850,
author = {Lahiani, Nesrine and Bennouar, Djamal},
title = {A Model Driven Approach to Derive e-Learning Applications in Software Product Line},
year = {2015},
isbn = {9781450334587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2816839.2816850},
doi = {10.1145/2816839.2816850},
abstract = {Platforms such as Moodle aims to ease and improve the teaching-learning process by means of taking advantage of internet technologies. All existing e-learning platforms are pretty similar the concepts of activity, assignment, deliverable or grade. But also a wide range of differences among them exists. Software Product Line (SPL) has as goal the effective production of similar software systems.. Product derivation represents a fundamental aspect in SPL. It is also the main challenge that SPL faces. Despite its importance, there is only a little research on product derivation compared to the large work on developing product lines. In addition, the few available research reports guidance about how to derive a product from a product line. In this paper we describe a combination of SPL and MDA which both fit perfectly together in order to build applications in cost effective way. We proposed an approach for product derivation that adopts MDA with its organized layers of models to achieve SPL goals.},
booktitle = {Proceedings of the International Conference on Intelligent Information Processing, Security and Advanced Communication},
articleno = {78},
numpages = {6},
keywords = {e-learning, Software Product Line, Product Derivation, Model Driven Architecture},
location = {Batna, Algeria},
series = {IPAC '15}
}

@inproceedings{10.5555/2525401.2525415,
author = {Tan, Lei and Lin, Yuqing and Ye, Huilin and Zhang, Guoheng},
title = {Improving product configuration in software product line engineering},
year = {2013},
isbn = {9781921770203},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {Software Product Line Engineering (SPLE) is a emerging software reuse paradigm. SPLE focuses on systematic software reuse from requirement engineering to product derivation throughout the software development life-cycle. Feature model is one of the most important reusable assets which represents all design considerations of a software product line. Feature model will be used in the product configuration process to produce a software. The product configuration is a decision-making process, where all kinds of relationships among configurable features will be considered to select the desired features for the product. To improve the efficiency and quality of product configuration, we are proposing a new approach which aims at identifying a small set of key features. The product configuration should always start from this set of features since, based on the feature dependencies, the decisions made on these features will imply decisions on the rest of the features of the product line, thus reduce the features visited in the configuration process. We have also conducted some experiments to demonstrate how the proposed approach works and evaluate the efficiency of the approach.},
booktitle = {Proceedings of the Thirty-Sixth Australasian Computer Science Conference - Volume 135},
pages = {125–133},
numpages = {9},
keywords = {software product line, product configuration, minimum vertex cover, feature model},
location = {Adelaide, Australia},
series = {ACSC '13}
}

@inproceedings{10.1145/3001867.3001872,
author = {Lity, Sascha and Kowal, Matthias and Schaefer, Ina},
title = {Higher-order delta modeling for software product line evolution},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001872},
doi = {10.1145/3001867.3001872},
abstract = {In software product lines (SPL), i.e., a family of similar software systems sharing common and variable artifacts, modeling evolution and reasoning about it is challenging, as not only a single system, but rather a set of system variants as well as their interdependencies change. An integrated modeling formalism for variability and evolution is required to allow the capturing of evolution operations that are applied to SPL artifacts, and to facilitate the impact analysis of evolution on the artifact level. Delta modeling is a flexible transformational variability modeling approach, where the variability and commonality between variants are explicitly documented and analyzable by means of transformations modeled as deltas. In this paper, we lift the notion of delta modeling to capture both, variability and evolution, by deltas. We evolve a delta model specifying a set of variants by applying higher-order deltas. A higher-order delta encapsulates evolution operations, i.e., additions, removals, or modifications of deltas, and transforms a delta model in its new version. In this way, we capture the complete evolution history of delta-oriented SPLs by higher-order delta models. By analyzing each higher-order delta application, we are further able to reason about the impact and, thus, the changes to the specified set of variants. We prototypically implement our formalism and show its applicability using a system from the automation engineering domain.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {39–48},
numpages = {10},
keywords = {Software Product Lines, Software Evolution, Delta Modeling},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@inproceedings{10.1145/3646548.3676552,
author = {Malenfant, Jacques and Ziadi, Tewfik},
title = {Can Conditional Preferences and *CP-net Concepts Enhance Feature Models?},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676552},
doi = {10.1145/3646548.3676552},
abstract = {Variability in software systems is a key concept in mastering complexity. Most software systems exhibit functionalities that can either be implemented in several different ways or may be options that can be chosen late in their design, depending on the deployment conditions. Expressing these forms of variability attracted a lot of attention since several years, leading to a predominant expression media: Feature models. Feature models can express the mandatory/optional status of a feature, as well as alternative realizations of a feature, exclusive of each others or not. Though extensions of feature models have been proposed to express more properties, they are still limited in their ability to capture complex configuration problems, especially those involving extra functional properties. Lines of research have proposed extended feature models with non-functional attributes and constraint-solving approach to reason about them. Yet, these approaches appear too restrictive and inflexible to cater for really complex SPL. In this paper, we rather propose to extend feature models with conditional preferences concepts from the multi-criteria decision-making field, and more specifically on relationships introduced in the graphical modeling approach of the * CP-net family of models. We show how these new relationships enable the expression of complex configuration constraints, while ensuring that the feature models remain intuitive and user-friendly across various feature model analysis-based activities.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {66–74},
numpages = {9},
keywords = {conditional preferences, feature models, variability management},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3546932.3547001,
author = {Six, Nicolas and Herbaut, Nicolas and Lopez-Herrejon, Roberto Erick and Salinesi, Camille},
title = {Using software product lines to create blockchain products: application to supply chain traceability},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547001},
doi = {10.1145/3546932.3547001},
abstract = {In recent years, blockchain has been growing rapidly from a niche technology to a promising solution for many sectors, due to its unique properties that empower the design of innovative applications. Nevertheless, the development of blockchain applications is still a challenge. Due to the technological novelty, only a few developers are familiar with blockchain technologies and smart contracts. Others might face a steep learning curve or difficulties to reuse existing code to build blockchain applications. This study proposes a novel approach to tackle these issues, through software product line engineering. To support the approach, a web platform to configure and generate a blockchain application for on-chain traceability is introduced. First, a feature model has been designed to model core features of the chosen domain, based on the existing literature. Then, a configurator has been implemented to support the feature selection phase. Finally, a generator is able to ingest such configurations to generate on-the-shelf blockchain products. The generalizability of the contribution is validated by reproducing on-chain traceability applications proposed in the literature by using the platform. This work provides the first evidence that the implementation of blockchain applications using software product lines enhances the quality of produced applications and reduces the time to market.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {97–107},
numpages = {11},
keywords = {software product line, code generation, blockchain},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/2695664.2695797,
author = {Tizzei, Leonardo P. and Azevedo, Leonardo G. and de Bayser, Maximilien and Cerqueira, Renato F. G.},
title = {Architecting cloud tools using software product line techniques: an exploratory study},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695797},
doi = {10.1145/2695664.2695797},
abstract = {Multitenant cloud computing tools are usually complex and have to manage variabilities to support customization. Software Product Line (SPL) techniques have been successfully applied in the industry to manage variability in complex systems. However, few works in the literature discuss the application of SPL techniques to architect industry cloud computing tools, resulting in a lack of support to cloud architects on how to apply such techniques. This work presents how software product line techniques can be applied for architecting cloud tools, and discusses the benefits, drawbacks, and some challenges of applying such techniques to develop a real industry cloud tool, named as Installation Service.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1441–1448},
numpages = {8},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/3167132.3167350,
author = {Fischer, Stefan and Lopez-Herrejon, Roberto Erick and Egyed, Alexander},
title = {Towards a fault-detection benchmark for evaluating software product line testing approaches},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167350},
doi = {10.1145/3167132.3167350},
abstract = {Software Product Lines (SPLs) are families of related software systems distinguished by the set of features each one provides. The commonly large number of variants that can be derived from an SPL poses a unique set of challenges, because it is not feasible to test all the individual variants. Over the last few years many approaches for SPL testing have been devised. They usually select a set of variants to test based on some covering criterion. A problem when evaluating these testing approaches is properly comparing them to one another. Even though some benchmarks have been proposed, they focus on covering criteria and do not consider fault data in their analysis. Considering the dire lack of publicly available fault data, in this paper we present the first results of our ongoing project to introduce simulated faults into SPLs along with using evolutionary techniques for synthesizing unit test cases for SPL examples.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {2034–2041},
numpages = {8},
keywords = {software product lines, mutation testing},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3503229.3547026,
author = {Friesel, Birte and Elmenhorst, Kathrin and Kaiser, Lennart and M\"{u}ller, Michael and Spinczyk, Olaf},
title = {kconfig-webconf: retrofitting performance models onto kconfig-based software product lines},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547026},
doi = {10.1145/3503229.3547026},
abstract = {Despite decades of research and clear advantages, performance-aware configuration of real-world software product lines is still an exception rather than the norm. One reason for this may be tooling: configuration software with support for non-functional property models is generally not compatible with the configuration and build process of existing product lines. Specifically, the Kconfig language is popular in open source software projects, but neither language nor configuration frontends support performance models. To address this, we present kconfig-webconf: a performance-aware, Kconfig-compatible software product line configuration frontend. It is part of a toolchain that can automatically generate performance models with a minimal amount of changes to a software product line's build process. With such a performance model, kconfig-webconf can serve as a performance-aware drop-in replacement for existing Kconfig frontends. We evaluate its usage in five examples, including the busybox multi-call binary and the resKIL agricultural AI product line.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {58–61},
numpages = {4},
keywords = {kconfig, performance prediction, product lines, regression trees},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.5555/2819009.2819208,
author = {Klewerton, Wesley and Assun\c{c}\~{a}o, Guez},
title = {Search-based migration of model variants to software product line architectures},
year = {2015},
publisher = {IEEE Press},
abstract = {Software Product Lines (SPLs) are families of related software systems developed for specific market segments or domains. Commonly, SPLs emerge from sets of existing variants when their individual maintenance becomes infeasible. However, current approaches for SPL migration do not support design models, are partially automated, or do not reflect constraints from SPL domains. To tackle these limitations, the goal of this doctoral research plan is to propose an automated approach to the SPL migration process at the design level. This approach consists of three phases: detection, analysis and transformation. It uses as input the class diagrams and lists of features for each system variant, and relies on search-based algorithms to create a product line architecture that best captures the variability present in the variants. Our expected contribution is to support the adoption of SPL practices in companies that face the scenario of migrating variants to SPLs.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {895–898},
numpages = {4},
keywords = {software product line, search-based software engineering, reuse, re-engineering, migration},
location = {Florence, Italy},
series = {ICSE '15}
}

@article{10.1145/2382756.2382778,
author = {Tekinerdogan, Bedir},
title = {First turkish software product line engineering workshop summary},
year = {2012},
issue_date = {November 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/2382756.2382778},
doi = {10.1145/2382756.2382778},
abstract = {Software reuse has been a goal of the software community since the early days of software engineering. In this context software product line engineering (SPLE) has gained a broad interest in both academic institutions and industry. This trend can also be observed in Turkey. In the recent years an increasing number of software companies in Turkey have adopted a SPLE approach while others are planning to make the transition. This paper summarizes the results of the First Turkish Software Product Line Engineering Workshop that has been organized in Ankara in June 2012. The primary goal of the workshop was to reflect on the state of practice in SPLE in Turkey. For this five leading SPLE companies in Turkey have shared their experiences in adopting SPLE, and using interactive discussions a research agenda for SPLE in Turkey has been defined. We report both on the experiences from the workshop and the resulting research topics.},
journal = {SIGSOFT Softw. Eng. Notes},
month = nov,
pages = {30–34},
numpages = {5},
keywords = {workshop organization, technology transfer, software reuse, software product line engineering}
}

@inproceedings{10.1145/1985484.1985489,
author = {Michalik, Bartosz and Weyns, Danny and Van Betsbrugge, Wim},
title = {On the problems with evolving Egemin's software product line},
year = {2011},
isbn = {9781450305846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985484.1985489},
doi = {10.1145/1985484.1985489},
abstract = {Egemin, an industrial manufacturer of logistic systems is adopting a Software Product Line (SPL) approach to manage the development of their product portfolio. However, due to the intrinsic complexity of the logistic systems and lack of explicitly documented architectural knowledge evolution of the products is error-prone. Faulty updates increase maintenance costs and harm the company's reputation. Therefore, Egemin searches for a systematic solution that can improve their SPL evolution strategy.},
booktitle = {Proceedings of the 2nd International Workshop on Product Line Approaches in Software Engineering},
pages = {15–19},
numpages = {5},
keywords = {spl, software product line, evolution},
location = {Waikiki, Honolulu, HI, USA},
series = {PLEASE '11}
}

@inproceedings{10.1145/3132498.3133835,
author = {Cardoso, Mateus Passos Soares and Lima, Crescencio and de Almeida, Eduardo Santana and do Carmo Machado, Ivan and von Flach G. Chavez, Christina},
title = {Investigating the variability impact on the recovery of software product line architectures: an exploratory study},
year = {2017},
isbn = {9781450353250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132498.3133835},
doi = {10.1145/3132498.3133835},
abstract = {The Product Line Architecture (PLA) of a Software Product Line (SPL) is the core architecture that represents a high-level design for all the products of an SPL, including variation points and variants. If PLA documentation is missing, it can be recovered by reverse engineering the products. The recovered PLA is a relevant asset for developers and architects, that can be used to drive specific activities of SPL development and evolution, such as, understanding its structure and its variation points, and assessing reuse. This paper presents an exploratory study that investigated the effectiveness of recovered PLAs to address variability identification and support reuse assessment. We recovered the PLA of 15 open source SPL projects using the PLAR, a tool that supports PLA recovery and assessment based on information extracted from SPL products' source code. For each project, reuse assessment was supported by existing reuse metrics. The yielded results revealed that the number of products used in PLA recovery affected the variability identification, and the number of optional features affected the components reuse rate. These findings suggest that a minimum set of representative products should be identified and selected for PLA recovery, and the component reuse rate is a candidate metric for SPL reuse assessment.},
booktitle = {Proceedings of the 11th Brazilian Symposium on Software Components, Architectures, and Reuse},
articleno = {12},
numpages = {10},
keywords = {variability, software product lines, product line architecture recovery, product line architecture},
location = {Fortaleza, Cear\'{a}, Brazil},
series = {SBCARS '17}
}

@inproceedings{10.5555/2662572.2662582,
author = {Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {Representation of software product line architectures for search-based design},
year = {2013},
isbn = {9781467362849},
publisher = {IEEE Press},
abstract = {The Product-Line Architecture (PLA) is the main artifact of a Software Product Line (SPL). Search-based approaches can provide automated discovery of near-optimal PLAs and make its design less dependent on human architects. To do this, it is necessary to adopt a suitable PLA representation to apply the search operators. In this sense, we review existing architecture representations proposed by related work, but all of them need to be extended to encompass specific characteristics of SPL. Then, the use of such representations for PLA is discussed and, based on the performed analysis, we introduce a novel direct PLA representation for search-based optimization. Some implementation aspects are discussed involving implementation details about the proposed PLA representation, constraints and impact on specific search operators. Ongoing work addresses the application of specific search operators for the proposed representation and the definition of a fitness function to be applied in a multi-objective search-based approach for the PLA design.},
booktitle = {Proceedings of the 1st International Workshop on Combining Modelling and Search-Based Software Engineering},
pages = {28–33},
numpages = {6},
keywords = {software product line, multi-objective search-based approach, architecture modelling},
location = {San Francisco, California},
series = {CMSBSE '13}
}

@inproceedings{10.1145/3646548.3672595,
author = {Bounouas, Nassim and Blay-Fornarino, Mireille and Collet, Philippe},
title = {Tracing and Fixing Inconsistencies in Clone-and-Own Tabular Data Models},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672595},
doi = {10.1145/3646548.3672595},
abstract = {Many data-intensive applications handle tabular data with more advanced structuring and processes than spreadsheets, enabling end-users to copy and adapt tabular data and processes to create new templates or datasets anytime. Recent research advances demonstrated that, in such clone-and-own scenarios, actions performed on the data structure, together with cloning and adaptation actions, can be captured within an operation-based model to prevent the drift of the internal tabular data model. However, this approach is limited by the assumption that each operation must maintain consistency regarding dependencies generated by the domain-specific languages that connect the observed and computed data. To address this challenge, this paper first introduces an evolved operation-based model that is designed to capture inconsistent tabular data while keeping a fine-grained trace of what part of the model is inconsistent. We then define specific trace operations to either fix a dependency in a model or remove one if its creating process is no longer relevant to the user. These operations support high-level editing scenarios on the tabular data, which enables easily fixing the equivalent of a spreadsheet formula or a process statement, or making the user aware that some part of the model is inconsistent while it is cloned. Additionally, we report on a positive scalability experiment on the tracing of large tabular data models with inconsistencies.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {191–202},
numpages = {12},
keywords = {Tabular data, agronomy, clone-and-own, model-driven engineering, operation-based modeling, variability management},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3233027.3233045,
author = {Becker, Martin and Zhang, Bo},
title = {How do our neighbours do product line engineering? a comparison of hardware and software product line engineering approaches from an industrial perspective},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233045},
doi = {10.1145/3233027.3233045},
abstract = {Product line engineering (PLE) approaches have been followed in industry for hardware and software solutions for more than three decades now. However, the different engineering disciplines (e.g. mechanics, electrics, software) have developed and evolved their approaches within their own realms, which is fine as long as there is no need for integrated approaches. Driven by the increasing complexity of systems, there is a rising need for interdisciplinary systems engineering these days. Companies engineering cyber-physical systems and their components have to integrate product line engineering approaches across the involved engineering disciplines to enable a global optimization of portfolio, solution structures, and assets along their lifecycle. From a bird's-eye view, there is noticeable commonality but also variety in the approaches followed for PLE in the different engineering disciplines, which renders the integration of approaches a non-trivial endeavour. In order to foster the development of integrated PLE approaches, this paper explores, maps, and compares PLE approaches in the field of hardware and software engineering. Furthermore, the paper identifies integration opportunities and challenges. As the paper targets industrial practitioners, it mainly provides references to respective industrial events and material and does not fully cover related work in the respective research communities.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {190–195},
numpages = {6},
keywords = {software product lines, industry, academia, SPLC},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3646548.3672586,
author = {Fernandez-Amoros, David and Heradio, Ruben and Horcas Aguilera, Jose Miguel and Galindo, Jos\'{e} A. and Benavides, David and Fuentes, Lidia},
title = {Pragmatic Random Sampling of the Linux Kernel: Enhancing the Randomness and Correctness of the conf Tool},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672586},
doi = {10.1145/3646548.3672586},
abstract = {The configuration space of some systems is so large that it cannot be computed. This is the case with the Linux Kernel, which provides almost 19,000 configurable options described across more than 1,600 files in the Kconfig language. As a result, many analyses of the Kernel rely on sampling its configuration space (e.g., debugging compilation errors, predicting configuration performance, finding the configuration that optimizes specific performance metrics, etc.). The Kernel can be sampled pragmatically, with its built-in tool conf, or idealistically, translating the Kconfig files into logic formulas. The pros of the idealistic approach are that it provides statistical guarantees for the sampled configurations, but the cons are that it sets out many challenging problems that have not been solved yet, such as scalability issues. This paper introduces a new version of conf called randconfig+, which incorporates a series of improvements that increase the randomness and correctness of pragmatic sampling and also help validate the Boolean translation required for the idealistic approach. randconfig+ has been tested on 20,000 configurations generated for 10 different Kernel versions from 2003 to the present day. The experimental results show that randconfig+ is compatible with all tested Kernel versions, guarantees the correctness of the generated configurations, and increases conf’s randomness for numeric and string options.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {24–35},
numpages = {12},
keywords = {Kconfig, SAT, configurable systems, randconfig, random sampling, software product lines, variability modeling},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3646548.3676602,
author = {Sundermann, Chico and He\ss{}, Tobias and Sundermann, Rahel and Kuiter, Elias and Krieter, Sebastian and Th\"{u}m, Thomas},
title = {Generating Feature Models with UVL's Full Expressiveness},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676602},
doi = {10.1145/3646548.3676602},
abstract = {The Universal Variability Language (UVL) is a textual format for specifying feature models. UVL has optional language levels (i.e., extensions) that add more expressive functionality to the base language, such as numerical constraints over attributes. However, those levels have been a rather recent addition. Also, most other established formats only support a subset of UVL’s expressiveness. Consequently, there are currently only very few feature models available that use the more sophisticated language levels of UVL. The lack of such feature models hinders research and tool development that targets more expressive feature models. In particular, this makes it difficult to develop efficient reasoning engines or conversions between the different levels. In this work, we present UVLGenerator, a first prototype for generating UVL models that make use of all available language levels. The generator supports configuring various structural properties that, inter alia, control which language levels are used.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {61–65},
numpages = {5},
keywords = {UVL, benchmark, feature modeling, feature-model generator},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3579027.3608981,
author = {Horcas, Jose-Miguel and Ballesteros, Joaquin and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Elimination of constraints for parallel analysis of feature models},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608981},
doi = {10.1145/3579027.3608981},
abstract = {Cross-tree constraints give feature models maximal expressive power since any interdependency between features can be captured through arbitrary propositional logic formulas. However, the existence of these constraints increases the complexity of reasoning about feature models, both for using SAT solvers or compiling the model to a binary decision diagram for efficient analyses. Although some works have tried to refactor constraints to eliminate them, they deal only with simple constraints (i.e., requires and excludes) or require the introduction of an additional set of features, increasing the complexity of the resulting feature model. This paper presents an approach that eliminates all the cross-tree constraints present in regular boolean feature models, including arbitrary constraints, in propositional logic formulas. Our approach for removing constraints consists of splitting the semantics of feature models into orthogonal disjoint feature subtrees, which are then analyzed in parallel to alleviate the exponential blow-up in memory of the resulting feature tree.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {99–110},
numpages = {12},
keywords = {software product line, parallelization, feature tree, feature model, constraint, Automated analysis},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3646548.3676550,
author = {He\ss{}, Tobias and Karrer, Simon and Ostheimer, Lukas},
title = {Multi-Version Decision Propagation for Configuring Feature Models in Space and Time},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676550},
doi = {10.1145/3646548.3676550},
abstract = {Real-world feature models are typically too large and complex to be configured manually. In practice, configuration tasks are, therefore, accomplished by employing interactive configurators. After each explicit feature selection or deselection by the user, these configurators use decision propagation to detect features that are implied by the current partial configuration and, consequently, select or deselect them accordingly. This way, the configuration remains valid throughout the configuration process. However, valid configurations may become invalid when then underlying model changes due to model evolution. As one is potentially interested in retaining a configuration for multiple versions, for instance, in testing or certification applications, this prompts the question on how to configure for multiple versions at once. In this work, we introduce multi-version decision propagation which allows to interactively configure on multiple model versions at once. Our prototype adapts the set of possible versions to the current configuration but also allows users to configure for a fixed set of versions.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {88–92},
numpages = {5},
keywords = {Configuration, Decision Propagation, Feature Models, Feature-Model Evolution},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/2593882.2593888,
author = {Metzger, Andreas and Pohl, Klaus},
title = {Software product line engineering and variability management: achievements and challenges},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593888},
doi = {10.1145/2593882.2593888},
abstract = {Software product line engineering has proven to empower organizations to develop a diversity of similar software-intensive systems (applications) at lower cost, in shorter time, and with higher quality when compared with the development of single systems. Over the last decade the software product line engineering research community has grown significantly. It has produced impressive research results both in terms of quality as well as quantity. We identified over 600 relevant research and experience papers published within the last seven years in established conferences and journals. We briefly summarize the major research achievements of these past seven years. We structure this research summary along a standardized software product line framework. Further, we outline current and future research challenges anticipated from major trends in software engineering and technology.},
booktitle = {Future of Software Engineering Proceedings},
pages = {70–84},
numpages = {15},
keywords = {variability modeling, variability management, requirements engineering, quality assurance, design, Software product lines},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@inproceedings{10.1145/3503229.3547041,
author = {Kucher, Maximilian and Balyo, Tom\'{a}\v{s} and Christensen, Noemi},
title = {Black-box optimization in a configuration system},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547041},
doi = {10.1145/3503229.3547041},
abstract = {The product configurator Merlin is a CPQ solution (Configure, Price, Quote) that enables fast, error-free configuration and quotation generation for products with many variants. In the context of this paper an optimization module was developed and integrated into Merlin. Previously, Merlin could only minimize the number of changes that must be made when a user changes a configuration. With this work, the optimization capability in Merlin was extended in a way, that a user can define a custom target function. Specific features and variables then can be selected for optimization. The optimization module can optimize the values of these attributes and variables with respect to the defined target function. The optimization process has no limited runtime and does not stop automatically when reaching certain predefined values, since in the field of optimization often no promises can be made on finding global extrema. Instead, the optimization process is monitored live by the user and can be terminated at any time as soon as the user is satisfied with the current solution. In addition to the adaptation of the Merlin frontend, two black-box and derivative-free optimization algorithms are implemented and tested for performance to solve the optimization problem.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {229–236},
numpages = {8},
keywords = {black-box, configuration, optimization},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3646548.3676601,
author = {Marinho, Euler},
title = {Characterizing Resource Interaction Failures in Mobile Applications},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676601},
doi = {10.1145/3646548.3676601},
abstract = {Resource interaction failures can compromise the mobile application quality and harm the user experience. In this thesis proposal, we first investigated resource interaction failures using exhaustive testing. After increasing the number of resources, we used sampling strategies for the investigation of these failures. In addition, we examined the feasibility of Spectrum-based Fault Localization for locating faults in Android applications and the sensitivity to resource interaction failures. We plan to extend the previous study by considering manually injected faults based on bug fix patterns. Moreover, we plan to identify guidelines for developers, testers, and researchers to handle resource interaction failures and faults based on the lessons learned in this thesis proposal.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {11–16},
numpages = {6},
keywords = {Resource Interaction Failures, Software Quality, Software Testing},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/2996890.3007893,
author = {Ruiz, Carlos and Duran-Limon, Hector A. and Parlavantzas, Nikos},
title = {Towards a software product line-based approach to adapt IaaS cloud configurations},
year = {2016},
isbn = {9781450346160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2996890.3007893},
doi = {10.1145/2996890.3007893},
abstract = {Cloud computing is nowadays one of the most promising IT technologies, since it provides seemingly unlimited resources on demand at low costs. Hence, different types of applications have been migrated to IaaS environments, e.g. multi-tier (distributed) applications. However, in order to benefit from such characteristics, cloud configurations (i.e. virtual resource configurations) should be designed accordingly to the necessities of the applications. Furthermore, such configurations have to provide the required resources not only at the application deployment-time, but also during the whole application execution time. Hence, adaptive paradigms are required when designing solutions to cloud applications with dynamic resource requirements. Software Product Lines (SPLs) provide great flexibility and a high level of abstraction to describe complete system configurations. Even though SPLs are not commonly used to describe changes after an initial product (configuration) has been created, their inherent characteristics can enable producing the required virtual resource configuration to adapt applications after their initial deployment, i.e., at runtime. In this paper, we present an approach to create and adapt cloud configurations at the IaaS level by using SPLs. We focus on the architectural design of our solution as well as on the possible implementation challenges we could face.},
booktitle = {Proceedings of the 9th International Conference on Utility and Cloud Computing},
pages = {398–403},
numpages = {6},
keywords = {software product lines, self-adaptation, cloud computing},
location = {Shanghai, China},
series = {UCC '16}
}

@inproceedings{10.1145/3461001.3471148,
author = {Krieter, Sebastian and Arens, Rahel and Nieke, Michael and Sundermann, Chico and He\ss{}, Tobias and Th\"{u}m, Thomas and Seidl, Christoph},
title = {Incremental construction of modal implication graphs for evolving feature models},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471148},
doi = {10.1145/3461001.3471148},
abstract = {A feature model represents a set of variants as configurable features and dependencies between them. During variant configuration, (de)selection of a feature may entail that other features must or cannot be selected. A Modal Implication Graph (MIG) enables efficient decision propagation to perform automatic (de)selection of subsequent features. In addition, it facilitates other configuration-related activities such as t-wise sampling. Evolution of a feature model may change its configuration logic, thereby invalidating an existing MIG and forcing a full recomputation. However, repeated recomputation of a MIG is expensive, and thus hampers the overall usefulness of MIGs for frequently evolving feature models. In this paper, we devise a method to incrementally compute updated MIGs after feature model evolution. We identify expensive steps in the MIG construction algorithm, enable them for incremental computation, and measure performance compared to a full rebuild of a complete MIG within the evolution histories of four real-world feature models. Results show that our incremental method can increase the speed of MIG construction by orders of magnitude, depending on the given scenario and extent of evolutionary changes.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {64–74},
numpages = {11},
keywords = {software product line, evolution, configurable system},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3646548.3672594,
author = {Bombarda, Andrea and Gargantini, Angelo},
title = {On the Use of Multi-valued Decision Diagrams to Count Valid Configurations of Feature Models},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672594},
doi = {10.1145/3646548.3672594},
abstract = {This paper addresses the challenge of efficiently counting valid configurations in Software Product Lines (SPLs). We propose a novel approach leveraging Multi-Valued Decision Diagrams (MDDs) for building the set of products. Building upon the MDD structure, we introduce several algorithmic optimizations to achieve a more compact and efficient representation of the product set compared to existing methods based on Binary Decision Diagrams. The effectiveness of our approach is evaluated through experimentation on two datasets: a set of synthetic benchmarks and large-scale industrial feature models. The results demonstrate significant improvements in scalability for models of medium complexity, particularly those rich in alternative groups. However, challenges remain for other model types, highlighting areas for future research.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {96–106},
numpages = {11},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3646548.3672590,
author = {Sundermann, Chico and Brancaccio, Vincenzo Francesco and Kuiter, Elias and Krieter, Sebastian and He\ss{}, Tobias and Th\"{u}m, Thomas},
title = {Collecting Feature Models from the Literature: A Comprehensive Dataset for Benchmarking},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672590},
doi = {10.1145/3646548.3672590},
abstract = {Feature models are widely used for specifying the valid configurations of product lines. Many automated analyses on feature models have been considered, but they often depend on computationally complex algorithms (e.g., solving satisfiability problems). To identify and develop efficient reasoning engines, it is necessary to compare their performance on practically relevant feature models. However, empirical evaluations on feature-model analysis often suffer from the limitations of available feature-model datasets in terms of transferability. A major problem is the accessibility of relevant feature models as they are scattered over numerous publications. In this work, we perform a literature survey on empirical evaluations that target the performance of feature-model analyses to examine common evaluation practices and collect feature models for future evaluations. Furthermore, we examine the suitability of the derived collection for benchmarking performance. To improve accessibility, we provide a repository including all 2,518 identified feature models from 13 application domains, such as system software.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {54–65},
numpages = {12},
keywords = {benchmark, evaluation, feature model, product line, survey},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.5555/1753235.1753277,
author = {Botterweck, Goetz and Groher, Iris and Polzer, Andreas and Schwanninger, Christa and Thiel, Steffen and V\"{o}lter, Markus},
title = {1st International Workshop on Model-driven Approaches in Software Product Line Engineering: (MAPLE 2009)},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {The MAPLE workshop focuses on the combination of Model-driven Software Engineering and Software Product Lines (SPL). It explores how model-driven approaches can help to achieve the goals of product lines in terms of reducing cost and time to market and increasing quality and productivity. In particular the workshop revolves around three themes: Efficient product derivation, the link between SPL research and industry practice, and SPL models with a meaning.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {297–298},
numpages = {2},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/3646548.3676539,
author = {M\"{u}ller, Robert and Wei\ss{}, Mathis and Lochau, Malte},
title = {Mapping Cardinality-based Feature Models to Weighted Automata over Featured Multiset Semirings},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676539},
doi = {10.1145/3646548.3676539},
abstract = {Cardinality-based feature models permit to select multiple copies of the same feature, thus generalizing the notion of product configurations from subsets of Boolean features to multisets of feature instances. This increased expressiveness shapes a-priori infinite and non-convex configuration spaces, which renders established solution-space mappings based on Boolean presence conditions insufficient for cardinality-based feature models. To address this issue, we propose weighted automata over featured multiset semirings as a novel behavioral variability modeling formalism for cardinality-based feature models. The formalism uses multisets over features as a predefined semantic domain for transition weights. It permits to use any algebraic structure forming a proper semiring on multisets to aggregate the weights traversed along paths to map accepted words to multiset configurations. In particular, tropical semirings constitute a promising sub-class with a reasonable trade-off between expressiveness and computational tractability of canonical analysis problems. The formalism is strictly more expressive than featured transition systems, as it enables upper-bound multiplicity constraints depending on the length of words. We provide a tool implementation of the behavioral variability model and present preliminary experimental results showing applicability and computational feasibility of the proposed approach.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {1–11},
numpages = {11},
keywords = {Behavioral Variability Modeling, Cardinality-Based Feature Models, Weighted Automata},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3461001.3471152,
author = {Silva, Publio and Bezerra, Carla I. M. and Machado, Ivan},
title = {A machine learning model to classify the feature model maintainability},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471152},
doi = {10.1145/3461001.3471152},
abstract = {Software Product Lines (SPL) are generally specified using a Feature Model (FM), an artifact designed in the early stages of the SPL development life cycle. This artifact can quickly become too complex, which makes it challenging to maintain an SPL. Therefore, it is essential to evaluate the artifact's maintainability continuously. The literature brings some approaches that evaluate FM maintainability through the aggregation of maintainability measures. Machine Learning (ML) models can be used to create these approaches. They can aggregate the values of independent variables into a single target data, also called a dependent variable. Besides, when using white-box ML models, it is possible to interpret and explain the ML model results. This work proposes white-box ML models intending to classify the FM maintainability based on 15 measures. To build the models, we performed the following steps: (i) we compared two approaches to evaluate the FM maintainability through a human-based oracle of FM maintainability classifications; (ii) we used the best approach to pre-classify the ML training dataset; (iii) we generated three ML models and compared them against classification accuracy, precision, recall, F1 and AUC-ROC; and, (iv) we used the best model to create a mechanism capable of providing improvement indicators to domain engineers. The best model used the decision tree algorithm that obtained accuracy, precision, and recall of 0.81, F1-Score of 0.79, and AUC-ROC of 0.91. Using this model, we could reduce the number of measures needed to evaluate the FM maintainability from 15 to 9 measures.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {35–45},
numpages = {11},
keywords = {software product line, quality evaluation, machine learning, feature model},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3646548.3672598,
author = {He\ss{}, Tobias and Semmler, Sean Niklas and Sundermann, Chico and Tor\'{a}n, Jacobo and Th\"{u}m, Thomas},
title = {Towards Deterministic Compilation of Binary Decision Diagrams From Feature Models},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672598},
doi = {10.1145/3646548.3672598},
abstract = {Compiling binary decision diagrams (BDD) from feature models is a challenging task whose outcome and performance depends on many interconnected factors. As almost all of these factors trace back to the BDD’s variable order, finding suitable orders is paramount for compilation success. In addition to cross-tree constraints, Alternative groups in feature models pose a challenge for variable ordering as separating group variables in the order can lead to duplication of large parts of the BDD. Previous approaches only scale to under-constrained models and require knowledge of the feature hierarchy. We address both shortcomings with a novel deterministic variable-ordering heuristic that detects Alternative groups in Boolean formulas and exploits them for variable ordering using the well-known FORCE heuristic in a divide-and-conquer approach. Our evaluation shows that this heuristic, together with our compilation strategy, scales to many models for which BDDs could not be compiled previously. Thereby, this work resolves SPLC’s knowledge compilation challenge for an important subset of real-world models.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {136–147},
numpages = {12},
keywords = {Binary Decision Diagrams, Feature-Model Analysis, Variable Ordering},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3461002.3473948,
author = {Xu, Hao and Baarir, Souheib and Ziadi, Tewfik and Hillah, Lom Messan and Essodaigui, Siham and Bossu, Yves},
title = {Optimisation for the product configuration system of Renault: towards an integration of symmetries},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473948},
doi = {10.1145/3461002.3473948},
abstract = {The problem of configuring model variability is widespread in many different domains. Renault, a leading french automobile manufacturer, has developed its technology internally to model vehicle diversity. This technology relies on the approach known as knowledge compilation. Since its inception, continuous progress has been made in the tool while monitoring the latest developments from the software field and academia. However, the growing number of vehicle models brings potential risks and higher requirements for the tool. This paper presents a short reminder of Renault's technology principles and the improvements we intend to achieve by analyzing and leveraging notable data features of Renault problem instances. In particular, the aim is to exploit symmetry properties.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {86–90},
numpages = {5},
keywords = {symmetries, product line, knowledge compilation, SAT},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/1964138.1964139,
author = {Silva, Alan Pedro da and Costa, Evandro and Bittencourt, Ig Ibert and Brito, Patrick H. S. and Holanda, Olavo and Melo, Jean},
title = {Ontology-based software product line for building semantic web applications},
year = {2010},
isbn = {9781450305426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1964138.1964139},
doi = {10.1145/1964138.1964139},
abstract = {The Software Product Lines (SPL) has proved very effective in building large-scale software. However, few works seek to adjust the approach of software product line to applications in the context of semantic web. This is because applications in this context assume the use of semantic services and intelligent agents. As a result, it is necessary that there are assets that provide adequate interoperability both semantic services and intelligent agents. In this sense, it is proposed in this paper the use of ontologies for the specification of entire a project of a SPL. With this, it can be a sufficiently formal specification that can be interpreted by both software engineers and computational algorithms.},
booktitle = {Proceedings of the 2010 Workshop on Knowledge-Oriented Product Line Engineering},
articleno = {1},
numpages = {6},
keywords = {software product line, semantic web, ontology},
location = {Reno, Nevada},
series = {KOPLE '10}
}

@inproceedings{10.1145/3646548.3672585,
author = {Landsberg, Tobias and Dietrich, Christian and Lohmann, Daniel},
title = {Should I Bother? Fast Patch Filtering for Statically-Configured Software Variants},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672585},
doi = {10.1145/3646548.3672585},
abstract = {In the face of critical security vulnerabilities, patch and update management are a crucial and challenging part of the software life cycle. In software product families, patching becomes even more challenging as we have to support different variants, which are not equally affected by critical patches. While the naive “better-patched-than-sorry” approach will apply all necessary updates, it provokes avoidable costs for developers and customers. In this paper we introduce SiB (Should I Bother?), a heuristic patch-filtering method for statically-configurable software that efficiently identifies irrelevant patches for specific variants. To solve the variability-aware patch-filtering problem, SiB compares modified line ranges from patches with those source-code ranges included in variants currently deployed. We apply our prototype for CPP-managed variability to four open-source projects (Linux, OpenSSL, SQLite, Bochs), demonstrating that SiB is both effective and efficient in reducing the number of to-be-considered patches for unaffected software variants. It correctly classifies up to 68 percent of variants as unaffected, with a recall of 100 percent, thus reducing deployments significantly, without missing any relevant patches.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {12–23},
numpages = {12},
keywords = {Patch Filtering, Software Evolution, Software Product Lines},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3382025.3414953,
author = {Abbas, Muhammad and Jongeling, Robbert and Lindskog, Claes and Enoiu, Eduard Paul and Saadatmand, Mehrdad and Sundmark, Daniel},
title = {Product line adoption in industry: an experience report from the railway domain},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414953},
doi = {10.1145/3382025.3414953},
abstract = {The software system controlling a train is typically deployed on various hardware architectures and must process various signals across those deployments. The increase of such customization scenarios and the needed adherence of the software to various safety standards in different application domains has led to the adoption of product line engineering within the railway domain. This paper explores the current state-of-practice of software product line development within a team developing industrial embedded software for a train propulsion control system. Evidence is collected using a focus group session with several engineers and through inspection of archival data. We report several benefits and challenges experienced during product line adoption and deployment. Furthermore, we identify and discuss improvement opportunities, focusing mainly on product line evolution and test automation.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {3},
numpages = {11},
keywords = {software product-line engineering, overloaded assets, challenges and opportunities},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3579027.3608988,
author = {Zellmer, Philipp and Holsten, Lennart and Leich, Thomas and Kr\"{u}ger, Jacob},
title = {Product-Structuring Concepts for Automotive Platforms: A Systematic Mapping Study},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608988},
doi = {10.1145/3579027.3608988},
abstract = {The products of the automotive industry are facing one of the biggest changes: becoming digital smart devices on wheels. Driven by the rising amount of vehicle functions, electronic control units, and software, today's vehicles are becoming cyber-physical systems that are increasingly complex and hard to manage over their life cycle. To handle these challenges, the automotive industry is adopting and integrating methods like software product-line engineering, electrics/electronics platforms, and product generation. While these concepts are widely recognized in their respective research areas and various domains, there is limited research regarding the practical effectiveness of implementing these concepts in a software-driven automotive context. In this paper, we investigate existing product-structuring concepts and methods that consider both hardware and software artifacts, and their applicability to the automotive as well as other cyber-physical industries. For this purpose, we conducted a systematic mapping study to capture a comprehensive overview of existing product-structuring concepts and methods, based on which we discuss how the state-of-the-art can or cannot help solve the challenges of the automotive industry. Specifically, we analyze the practical applicability of the existing solutions to help practitioners apply them and to guide future research.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {170–181},
numpages = {12},
keywords = {product-structuring concept, product line, life-cycle management, electrics/electronics, cyber-physical system, automotive},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.5555/1753235.1753272,
author = {Dordowsky, Frank and Hipp, Walter},
title = {Adopting software product line principles to manage software variants in a complex avionics system},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Eurocopter is the majority partner in NH Industries, the international consortium that develops and produces the medium weight multi-role helicopter NH90. AgustaWestland and Stork Fokker are additional partners. The NH90 has been successfully sold to 14 nations and their armed forces. The software division at Eurocopter Germany develops the on-board software for three computers of the NH90 avionics CORE and MISSION Systems. The growing number of customers and their specific application domains for the NH90 has led to an increasing number of functionally different helicopter variants. Moreover, during the long development time that is typical for complex military avionics projects, the computing technology has changed considerably over time so that the current operational software has to fit to several processor architectures. In order to cope with the high number of software variants and technology variations, the NH90 software team developed concepts and strategies for SW architecture and tool modifications based on Software Product Line (SPL) principles.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {265–274},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/3461002.3473941,
author = {Fadhlillah, Hafiyyan Sayyid and Feichtinger, Kevin and Sonnleithner, Lisa and Rabiser, Rick and Zoitl, Alois},
title = {Towards heterogeneous multi-dimensional variability modeling in cyber-physical production systems},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473941},
doi = {10.1145/3461002.3473941},
abstract = {Cyber-Physical Production Systems (CPPSs) are complex systems interacting with their environment by sensors and actuators. Such systems typically have a long lifespan, over which a plethora of variants are developed and maintained. The heterogeneity of hardware and software components used in CPPSs and the multiple disciplines (mechanical, electrical, software engineering) involved in the development and maintenance of CPPSs, however, make it difficult to manage their variability. Specifically, variability needs to be expressed in and across multiple disciplines, which use heterogeneous methods and tools. This also affects configuration as well as co-evolution of models and artifacts. In this short paper, we discuss our first ideas towards a Heterogeneous Multi-Dimensional Variability Modeling approach for CPPSs. Our approach builds on and extends existing work to address the challenges of modeling the variability of CPPSs and supporting their configuration and evolution. We showcase our idea using a case study system and outline a research agenda.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {123–129},
numpages = {7},
keywords = {variability modeling, software product line, software configuration, cyber-physical production system},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3307630.3342704,
author = {Ca\~{n}ete, Angel},
title = {Energy Efficient Assignment and Deployment of Tasks in Structurally Variable Infrastructures},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342704},
doi = {10.1145/3307630.3342704},
abstract = {The importance of cyber-physical systems is growing very fast, being part of the Internet of Things vision. These devices generate data that could collapse the network and can not be assumed by the cloud. New technologies like Mobile Cloud Computing and Mobile Edge Computing are taking importance as solution for this issue. The idea is offloading some tasks to devices situated closer to the user device, reducing network congestion and improving applications performance (e.g., in terms of latency and energy). However, the variability of the target devices' features and processing tasks' requirements is very diverse, being difficult to decide which device is more adequate to deploy and run such processing tasks. Once decided, task offloading used to be done manually. Then, it is necessary a method to automatize the task assignation and deployment process. In this thesis we propose to model the structural variability of the deployment infrastructure and applications using feature models, on the basis of a SPL engineering process. Combining SPL methodology with Edge Computing, the deployment of applications is addressed as the derivation of a product. The data of the valid configurations is used by a task assignment framework, which determines the optimal tasks offloading solution in different network devices, and the resources of them that should be assigned to each task/user. Our solution provides the most energy and latency efficient deployment solution, accomplishing the QoS requirements of the application in the process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {222–229},
numpages = {8},
keywords = {software product line, optimisation, mobile edge computing, mobile cloud computing, latency, energy efficiency},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3579027.3608972,
author = {Acher, Mathieu and Duarte, Jos\'{e} Galindo and J\'{e}z\'{e}quel, Jean-Marc},
title = {On Programming Variability with Large Language Model-based Assistant},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608972},
doi = {10.1145/3579027.3608972},
abstract = {Programming variability is central to the design and implementation of software systems that can adapt to a variety of contexts and requirements, providing increased flexibility and customization. Managing the complexity that arises from having multiple features, variations, and possible configurations is known to be highly challenging for software developers. In this paper, we explore how large language model (LLM)-based assistants can support the programming of variability.We report on new approaches made possible with LLM-based assistants, like: features and variations can be implemented as prompts; augmentation of variability out of LLM-based domain knowledge; seamless implementation of variability in different kinds of artefacts, programming languages, and frameworks, at different binding times (compile-time or run-time). We are sharing our data (prompts, sessions, generated code, etc.) to support the assessment of the effectiveness and robustness of LLMs for variability-related tasks.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {8–14},
numpages = {7},
keywords = {variability, software product lines, programming, large language model, generative AI},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/2304676.2304679,
author = {Klatt, Benjamin and K\"{u}ster, Martin},
title = {Respecting component architecture to migrate product copies to a software product line},
year = {2012},
isbn = {9781450313483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2304676.2304679},
doi = {10.1145/2304676.2304679},
abstract = {Software product lines (SPL) are a well-known concept to efficiently develop product variants. However, migrating existing, customised product copies to a product line is still an open issue due to the required comprehension of differences among products and SPL design decisions. Most existing SPL approaches are focused on forward engineering. Only few aim to handle SPL evolution, but even those lack support of variability reverse engineering, which is necessary for migrating product copies to a product line. In this paper, we present how component architecture information can be used to enhance a variabilty reverse engineering process to target this challenge and show the relevance of component architecture in the individual requirements on the resulting SPL. We further provide an illustrating example to show how the concept is applied.},
booktitle = {Proceedings of the 17th International Doctoral Symposium on Components and Architecture},
pages = {7–12},
numpages = {6},
keywords = {software product line, reverse engineering, component architecture},
location = {Bertinoro, Italy},
series = {WCOP '12}
}

@inproceedings{10.1145/3546932.3546997,
author = {Acher, Mathieu and Martin, Hugo and Lesoil, Luc and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc and Khelladi, Djamel Eddine and Barais, Olivier and Pereira, Juliana Alves},
title = {Feature subset selection for learning huge configuration spaces: the case of linux kernel size},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3546997},
doi = {10.1145/3546932.3546997},
abstract = {Linux kernels are used in a wide variety of appliances, many of them having strong requirements on the kernel size due to constraints such as limited memory or instant boot. With more than nine thousands of configuration options to choose from, developers and users of Linux actually spend significant effort to document, understand, and eventually tune (combinations of) options for meeting a kernel size. In this paper, we describe a large-scale endeavour automating this task and predicting a given Linux kernel binary size out of unmeasured configurations. We first experiment that state-of-the-art solutions specifically made for configurable systems such as performance-influence models cannot cope with that number of options, suggesting that software product line techniques may need to be adapted to such huge configuration spaces. We then show that tree-based feature selection can learn a model achieving low prediction errors over a reduced set of options. The resulting model, trained on 95 854 kernel configurations, is fast to compute, simple to interpret and even outperforms the accuracy of learning without feature selection.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {85–96},
numpages = {12},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.5555/2337223.2337302,
author = {Cordy, Maxime and Classen, Andreas and Perrouin, Gilles and Schobbens, Pierre-Yves and Heymans, Patrick and Legay, Axel},
title = {Simulation-based abstractions for software product-line model checking},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Software Product Line (SPL) engineering is a software engineering paradigm that exploits the commonality between similar software products to reduce life cycle costs and time-to-market. Many SPLs are critical and would benefit from efficient verification through model checking. Model checking SPLs is more difficult than for single systems, since the number of different products is potentially huge. In previous work, we introduced Featured Transition Systems (FTS), a formal, compact representation of SPL behaviour, and provided efficient algorithms to verify FTS. Yet, we still face the state explosion problem, like any model checking-based verification. Model abstraction is the most relevant answer to state explosion. In this paper, we define a novel simulation relation for FTS and provide an algorithm to compute it. We extend well-known simulation preservation properties to FTS and thus lay the theoretical foundations for abstraction-based model checking of SPLs. We evaluate our approach by comparing the cost of FTS-based simulation and abstraction with respect to product-by-product methods. Our results show that FTS are a solid foundation for simulation-based model checking of SPL.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {672–682},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1145/3461001.3471150,
author = {Pietsch, Christopher and Kelter, Udo and Kehrer, Timo},
title = {From pairwise to family-based generic analysis of delta-oriented model-based SPLs},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471150},
doi = {10.1145/3461001.3471150},
abstract = {One way to implement model-based software product lines (MBSPLs) is to use a transformational approach known as Delta Modeling (DM). Here, an MBSPL is implemented by one core model and a set of delta modules. Delta modules define model transformations using edit operations which add, remove or modify model elements. Editings of different delta modules can be in conflict or depend on each other, leading to conflict and dependency relations between delta modules. Conflicts and unfulfilled dependencies can cause the generation of a product to fail or to lead to invalid models. In order to spot such defects, one needs analysis tools for each modeling (sub-)language used. Existing generic approaches to statically detect such defects in a language-agnostic manner analyze pairs of delta modules. However, the pairwise approach can lead to false positives, i.e., conflicts and unfulfilled dependencies are reported although product generation does not fail. Following the idea of family-based analysis, this paper presents a new approach to detect pseudo defects resolved by "healing effects" implied by the network of dependencies. These effects typically occur when a delta module (partially) reverts the effect of a preceding delta module. We have implemented our approach within the SiPL framework and evaluated our family-based analysis using a realistic MBSPL known as Body Comfort System (BCS).},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {13–24},
numpages = {12},
keywords = {model-based software product line engineering, graph transformation, family-based analysis, delta modeling, conflicts and dependencies},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3579027.3608978,
author = {Wang, Alan and Feng, Nick and Chechik, Marsha},
title = {Code-Level Functional Equivalence Checking of Annotative Software Product Lines},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608978},
doi = {10.1145/3579027.3608978},
abstract = {Software functional equivalence checking is a technique for analyzing the impact of change of a portion of code on the rest of the system. The existing functional equivalence checking approaches are applicable only at the individual software product level. In this paper, we propose a lifted functional equivalence checking approach, CLEVER-V, that can efficiently handle annotative software product lines. Instead of checking functional equivalence of every product separately, CLEVER-V analyzes all products together to iteratively identify groups of non-equivalent products with common causes. We report on the implementation of the lifted functional equivalence checking approach and demonstrate its effectiveness and scalability on a suite of 288 realistic software updates from BusyBox.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {64–75},
numpages = {12},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3236405.3236426,
author = {Belarbi, Maouaheb},
title = {A methodological framework to enable the generation of code from DSML in SPL},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236426},
doi = {10.1145/3236405.3236426},
abstract = {Software Product Line has acquired a significant momentum at the end of the 1990ies since it allows the production of variable software systems corresponding to the same domain portfolio. The effectiveness of the derivation process depends on how well variability is defined and implemented which is a crucial topic area that was addressed among two essential trends: On the one hand, starting from Domain Specific Modelling Language to express domain requirements and automate the code generation with Model-Driven Engineering techniques and on the second hand, exploiting the soar of variability mechanisms.In this context, the current research presents a method that unifies the two aforementioned approaches to cover the overall strategies by defining a framework that allows a better code generation in terms of documentation, maintainability, rapidity,etc. The starting point is the usage of the Domain Specific Modelling Language to represent the stakeholders requirements. Then, the resulting meta-model will be converted into one our several Feature Diagrams on which variability mechanisms can be applied to generate all the family products.A preliminary experiment has been undertaken to design the methodology of the proposed software factory in a meta-model. The validation task was evaluated with an academic use case called HandiWeb developed to facilitate handicap persons access to the internet. The first results allow us to put the hand on the key challenges that must be resolved by the proposed methodology.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {64–71},
numpages = {8},
keywords = {variability, software factory, methodology, SPL, DSML},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/1982185.1982336,
author = {Asadi, Mohsen and Bagheri, Ebrahim and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Mohabbati, Bardia},
title = {Goal-driven software product line engineering},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982336},
doi = {10.1145/1982185.1982336},
abstract = {Feature Models encapsulate functionalities and quality properties of a product family. The employment of feature models for managing variability and commonality of large-scale product families raises an important question: on what basis should the features of a product family be selected for a target software application, which is going to be derived from the product family. Thus, the selection of the most suitable features for a specific application requires the understanding of its stakeholders' intentions and also the relationship between their intentions and the available software features. To address this important issue, we adopt a standard goal-oriented requirements engineering framework, i.e., the i* framework, for identifying stakeholders' intentions and propose an approach for explicitly mapping and bridging between the features of a product family and the goals and objectives of the stakeholders. We propose a novel approach to automatically preconfigure a given feature model based on the objectives of the target product stakeholders. Also, our approach is able to elucidate the rationale behind the selection of the most important features of a family for a target application.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {691–698},
numpages = {8},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@inproceedings{10.1145/2420942.2420948,
author = {Gonz\'{a}lez-Huerta, Javier and Insfran, Emilio and Abrah\~{a}o, Silvia and McGregor, John D.},
title = {Non-functional requirements in model-driven software product line engineering},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420948},
doi = {10.1145/2420942.2420948},
abstract = {Developing variant-rich software systems through the application of the software product line approach requires the management of a wide set of requirements. However, in most cases, the focus of those requirements is limited to the functional requirements. The non-functional requirements are often informally defined and their management does not provide traceability mechanisms for their validation. In this paper, we present a multimodel approach that allows the explicit representation of non-functional requirements for software product lines both at domain engineering, and application engineering levels. The multimodel allows the representation of different viewpoints of a software product line, including the non-functional requirements and the relationships that these non-functional requirements might have with features and functionalities. The feasibility of this approach is illustrated through a specific example from the automotive domain.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {6},
numpages = {6},
keywords = {software product lines, non-functional requirements, model driven engineering},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@inproceedings{10.1145/3579028.3609018,
author = {Nienaber, S\"{o}ren and Soorati, Mohammad D. and Ghasemzadeh, Arash and Ghofrani, Javad},
title = {Software Product Lines for Development of Evolutionary Robots},
year = {2023},
isbn = {9798400700927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579028.3609018},
doi = {10.1145/3579028.3609018},
abstract = {Evolutionary Robotics utilizes evolutionary algorithms for training robot controllers (e.g., neural networks) and adapting robot morphologies for different environments in design and runtime. One of the main challenges in robotics is the lack of reusability as AI-based robot controllers have to be trained from scratch for any change in the environment or a new task specification that a robot should adapt to. Training Artificial Neural Networks can be computationally heavy, time-consuming, and hard to reuse due to their monolithic black-box nature. The building blocks of emerging behaviors from Artificial Neural Networks cannot be fully separated or reused. We address the issue of reusability and propose an incremental approach for applying the reusability of behaviors. We implemented an Evolutionary Robotics framework to form a product family of robots. This product family is used to show the feasibility of our method for handling variability in a domain. Our results can be used to demonstrate a sample binding between the software product lines and machine learning domains.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
pages = {77–84},
numpages = {8},
keywords = {Software Product Lines, Primitive Behaviors, Mobile Robots, Evolutionary Robotics, Configuration},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3461001.3461660,
author = {Michelon, Gabriela Karoline and Obermann, David and Assun\c{c}\~{a}o, Wesley K. G. and Linsbauer, Lukas and Gr\"{u}nbacher, Paul and Egyed, Alexander},
title = {Managing systems evolving in space and time: four challenges for maintenance, evolution and composition of variants},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3461660},
doi = {10.1145/3461001.3461660},
abstract = {Software companies need to provide a large set of features satisfying functional and non-functional requirements of diverse customers, thereby leading to variability in space. Feature location techniques have been proposed to support software maintenance and evolution in space. However, so far only one feature location technique also analyses the evolution in time of system variants, which is required for feature enhancements and bug fixing. Specifically, existing tools for managing a set of systems over time do not offer proper support for keeping track of feature revisions, updating existing variants, and creating new product configurations based on feature revisions. This paper presents four challenges concerning such capabilities for feature (revision) location and composition of new product configurations based on feature/s (revisions). We also provide a benchmark containing a ground truth and support for computing metrics. We hope that this will motivate researchers to provide and evaluate tool-supported approaches aiming at managing systems evolving in space and time. Further, we do not limit the evaluation of techniques to only this benchmark: we introduce and provide instructions on how to use a benchmark extractor for generating ground truth data for other systems. We expect that the feature (revision) location techniques maximize information retrieval in terms of precision, recall, and F-score, while keeping execution time and memory consumption low.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {75–80},
numpages = {6},
keywords = {software product line, repository mining, feature revision, feature location, benchmark extractor},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3579028.3609010,
author = {Barbie, Philippe and Tenev, Vasil and Becker, Martin},
title = {InTra: Automatic Reduction of Model Complexity and Generation of System Variants - A Tool Demonstration},
year = {2023},
isbn = {9798400700927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579028.3609010},
doi = {10.1145/3579028.3609010},
abstract = {Efficient construction and management of variability is becoming increasingly crucial and poses a growing obstacle for model-based system engineering (MBSE). In this paper, we propose a transformative method that addresses these challenges by automating the creation of system model variants using the model transformation approach InTra (Interaction-based Transformation). The main advantage of this approach is the reduction of the complexity of the system model by using rule-based variants. Even models with a limited number of elements can quickly become confusing and difficult to read due to the high density of relationships. Our proposed approach, offers a way to significantly reduce system model complexity by minimizing the number of connectors through the application of interaction rules. By implementing this approach, we were able to generate an abstracted variant of the original system model with a substantially reduced number of connectors, thereby resulting in an overall decrease in model complexity. Thus, InTra not only improves productivity, but also ensures consistency of the model, leading to an overall higher quality of results and simplification of the model for future maintenance. An additional application of the approach is to generate variants of a system model by selectively activating or deactivating individual rules of a predefined rule catalogue, thus enabling easy variant management.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
pages = {25–29},
numpages = {5},
keywords = {variant management, tool demonstration, rule-based modeling, model-based systems engineering, model transformation, complexity, add-in, PLE, MBSE, InTra, Enterprise Architect},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3579027.3608976,
author = {Dimovski, Aleksandar S. and Lazreg, Sami and Cordy, Maxime and Legay, Axel},
title = {Family-based model checking of fMultiLTL properties},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608976},
doi = {10.1145/3579027.3608976},
abstract = {We introduce a new logic for expressing multi-properties of system families (Software Product Lines - SPLs). While the standard LTL logic refers only to a single trace at a time, fMultiLTL logic proposed here refers to multiple traces originating from different sets of variants of the SPL. This is achieved by allowing so-called featured quantification over traces, ∀ψ and ∃ψ, where the feature expression ψ describes a set of variants (sub-family) the quantified trace comes from. A specialized family-based model checking algorithm for verifying some fragments of fMultiLTL is given. A prototype family-based model checker, called D\k{a}dalux, has been implemented. We illustrate the practicality of this approach on several interesting SPL models.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {41–51},
numpages = {11},
keywords = {Temporal Multi-Properties, Software Product Lines, Model Checking, LTL},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3461001.3471142,
author = {Gu\'{e}gain, \'{E}douard and Quinton, Cl\'{e}ment and Rouvoy, Romain},
title = {On reducing the energy consumption of software product lines},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471142},
doi = {10.1145/3461001.3471142},
abstract = {Along the last decade, several studies considered green software design as a key development concern to improve the energy efficiency of software. Yet, few techniques address this concern for Software Product Lines (SPL). In this paper, we therefore introduce two approaches to measure and reduce the energy consumption of a SPL by analyzing a limited set of products sampled from this SPL. While the first approach relies on the analysis of individual feature consumptions, the second one takes feature interactions into account to better mitigate energy consumption of resulting products.Our experimental results on a real-world SPL indicate that both approaches succeed to produce significant energy improvements on a large number of products, while consumption data was modeled from a small set of sampled products. Furthermore, we show that taking feature interactions into account leads to more products improved with higher energy savings per product.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {89–99},
numpages = {11},
keywords = {software product lines, mitigation, measurement, energy, consumption},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3646548.3676538,
author = {Dubslaff, Clemens and Husung, Nils and K\"{a}fer, Nikolai},
title = {Configuring BDD Compilation Techniques for Feature Models},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676538},
doi = {10.1145/3646548.3676538},
abstract = {The compilation of feature models into binary decision diagrams (BDDs) is a major challenge in the area of configurable systems analysis. Many large-scale feature models have been reported to exceed state-of-the-art compilation capabilities, e.g., for variants of the Linux kernel product line. However, experiments have been mainly conducted on standard settings of the BDD compilers themselves, not taking advanced configurations into account. In this paper, we investigate the impact of various BDD compilation techniques for compiling feature models in conjunctive normal form. Specifically, we evaluate preprocessing techniques from satisfiability (SAT) solving, variable and clause ordering heuristics, non-incremental construction schemes, as well as parallelization of BDD construction. Our experiments on current feature models show that BDD compilation of feature models greatly benefits from these techniques, enabling to construct many previously not constructible large-scale feature models within seconds.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {209–216},
numpages = {8},
keywords = {Binary Decision Diagrams, Configurable Systems, Feature Models, Knowledge Compilation},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/2480362.2480694,
author = {Diwan, Piyush and Carey, Patricia and Franz, Eric and Li, Yixue and Bitterman, Thomas and Hudak, David E. and Ramnath, Rajiv},
title = {Applying software product line engineering in building web portals for supercomputing services},
year = {2013},
isbn = {9781450316569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2480362.2480694},
doi = {10.1145/2480362.2480694},
abstract = {Supercomputing centers, typically non-profit, government or university-based organizations with scarce resources, are increasingly being requested to provide customized web portals for user-centered access to their services in order to support a demanding customer base. These portals often have very similar architectures and meet similar requirements, with the variations primarily being in the specialized analysis applications, and in the input and output of these applications. Given these characteristics, Software Production Line Engineering (SPLE) approaches will be valuable in enabling development teams to cost-effectively meet demands. In this paper, we demonstrate a suite of web portals developed at The Ohio Supercomputer Center (OSC) by applying SPLE methodologies. We show how we applied feature modeling on these applications to identify commonalities in their application level features despite differences in their problem domains. We describe a common framework (we term it Per User DrupaL, or PUDL), which serves as the common foundation for these portals. We demonstrate the effectiveness of SPLE in terms of reduced development time and effort, and discuss the technical challenges faced in this process. Finally we propose, as an extension to our work, an automation framework for portal generation, which users could build their own customized portals.},
booktitle = {Proceedings of the 28th Annual ACM Symposium on Applied Computing},
pages = {1765–1771},
numpages = {7},
keywords = {supercomputing, software-as-a-service, software product line engineering, portals, high performance computing, feature modeling, end-user computing, drupal},
location = {Coimbra, Portugal},
series = {SAC '13}
}

@inproceedings{10.1145/3579027.3608980,
author = {Pett, Tobias and He\ss{}, Tobias and Krieter, Sebastian and Th\"{u}m, Thomas and Schaefer, Ina},
title = {Continuous T-Wise Coverage},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608980},
doi = {10.1145/3579027.3608980},
abstract = {Quality assurance for highly configurable systems uses t-wise feature interaction coverage as a metric to measure the quality of selected samples for testing. Achieving t-wise feature interaction coverage requires testing many configurations, often exceeding the available testing time for frequently evolving systems. As testing time is a limiting factor, current testing procedures face the challenge of finding a reasonable trade-off between achieving t-wise feature interaction coverage and reducing the time required for testing. To address this challenge, we can consider t-wise feature interactions covered in previous test executions when calculating the achieved t-wise feature interaction coverage. However, the current definition of t-wise feature interaction coverage does not consider previously tested configurations. Therefore, we propose continuous t-wise coverage as a new customizable metric for tracking the ratio of achieved t-wise feature interaction coverage over time. Our metric allows customizing the tradeoff between test effort per system version and the time to achieve t-wise coverage. We evaluate various parameterizations for our metric on four real-world evolution histories and investigate how they impact the calculated t-wise feature interaction coverage. Our results show that a high t-wise feature interaction coverage can be achieved by testing significant (up to 50%) smaller samples per commit, when the evolution of the system is considered.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {87–98},
numpages = {12},
keywords = {t-wise coverage, spl testing, spl evolution, software-product lines, sampling},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3233027.3236397,
author = {Mendon\c{c}a, Willian D. F. and Assun\c{c}\~{a}o, Wesley K. G. and Linsbauer, Lukas},
title = {Multi-objective optimization for reverse engineering of apo-games feature models},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236397},
doi = {10.1145/3233027.3236397},
abstract = {Software Product Lines Engineering (SPLE) is a software development approach intended for the development and maintenance of variable systems, i.e. systems that exist in many different variants. In the long run SPLE has many advantages. However, it requires a large upfront investment of time and money, which is why in practice Software Product Lines (SPLs) are rarely developed from scratch. Instead, they are often built using an extractive approach by which a set of existing system variants is consolidated (i.e. reverse engineered) into an SPL. A crucial part of this process is the construction of a variability model like a Feature Model (FM) that describes the common and variable parts of the system variants. In this paper we apply an approach for reverse engineering feature models based on a multi-objective optimization algorithm to the given challenge of constructing a feature model for a set of game variants and we present the results.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {279–283},
numpages = {5},
keywords = {software product line, reverse engineering, feature model},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3382025.3414942,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Kr\"{u}ger, Jacob and Mendon\c{c}a, Willian D. F.},
title = {Variability management meets microservices: six challenges of re-engineering microservice-based webshops},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414942},
doi = {10.1145/3382025.3414942},
abstract = {A microservice implements a small unit of functionality that it provides through a network using lightweight protocols. So, microservices can be combined to fulfill tasks and implement features of a larger software system---resembling a variability mechanism in the context of a software product line (SPL). Microservices and SPLs have similar goals, namely facilitating reuse and customizing, but they are usually employed in different contexts. Any developer who has access to the network can provide a microservice for any task, while SPLs are usually intended to implement features of a specific domain. Due to their different concepts, using microservices to implement an SPL or adopting SPL practices (e.g., variability management) for microservices is a challenging cross-area research problem. However, both techniques can complement each other, and thus tackling this problem promises benefits for organizations that employ either technique. In this paper, we reason on the importance of advancing in this direction, and sketch six concrete challenges to initiate research, namely (1) feature identification, (2) variability modeling, (3) variable microservice architectures, (4) interchangeability, (5) deep customization, and (6) re-engineering an SPL. We intend these challenges to serve as a starting point for future research in this cross-area research direction---avoiding that the concepts of one area are reinvented in the other.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {22},
numpages = {6},
keywords = {variability management, software product line, re-engineering, microservices, cloud computing},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3106195.3106223,
author = {Iglesias, Aitziber and Lu, Hong and Arellano, Crist\'{o}bal and Yue, Tao and Ali, Shaukat and Sagardui, Goiuria},
title = {Product Line Engineering of Monitoring Functionality in Industrial Cyber-Physical Systems: A Domain Analysis},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106223},
doi = {10.1145/3106195.3106223},
abstract = {In recent years, manufacturing technology is evolving and progressively becoming more dynamic and complex. This means that manufacturing technology (e.g., based on Industry 4.0) should be able to control the production process at runtime by monitoring physical elements and adapting itself. Such functionality is aimed at increasing production effectiveness and reducing the production cost. We argue that monitoring process can be viewed as a software product line having commonalities and variability. To support our argument, we analyzed and conducted domain analysis of two monitoring systems of Industrial Cyber-Physical Systems (ICPSs) from two industrial domains including automated warehouses and press machines. Based on the domain analysis, we present a common solution for monitoring including a software product line. With such product line, a user can configure, monitor, and visualize data of an ICPS at runtime. However, such solution could not handle the dynamic functionality related to monitoring of ICPS. Thus, we propose the use of dynamic product line and present a set of research questions that must be addressed for such solution.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {195–204},
numpages = {10},
keywords = {Software Product Line, Industrial domains, Dynamic Software Product Line, Cyber Physical System},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3646548.3672593,
author = {Greiner, Sandra and Schulthei\ss{}, Alexander and Bittner, Paul Maximilian and Th\"{u}m, Thomas and Kehrer, Timo},
title = {Give an Inch and Take a Mile? Effects of Adding Reliable Knowledge to Heuristic Feature Tracing},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672593},
doi = {10.1145/3646548.3672593},
abstract = {Tracing features to software artifacts is a crucial yet challenging activity for developers of variability-intensive software projects. Developers can provide feature traces either proactively in a manual and rarely semi-automated way or recover them retroactively where automated approaches mainly rely on heuristics. While proactive tracing promises high reliability as developers know which features they realize when working on them, the task is cumbersome and without immediate benefit. Conversely, automated retroactive tracing offers high automation by employing heuristics but remains unreliable and dependent on the quality of the heuristic. To exploit the benefits of proactive and retroactive tracing while mitigating their drawbacks, this paper examines how providing a minimal seed of accurate feature traces proactively (give an inch) can boost the accuracy of automated, heuristic-based retroactive tracing (take&nbsp;a&nbsp;mile). We examine how comparison-based feature location, as one representative of retroactive feature tracing, can benefit from increasing amounts of proactively provided feature mappings. For retroactive comparison-based feature tracing, we find not only that increasing amounts of proactive information can boost the overall accuracy of the tracing but also that the number of variants available for comparison affects the effectiveness of the combined tracing. As a result, our work lays the foundations to optimize the accuracy of retroactive feature tracing techniques with pinpointed proactive knowledge exploitation.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {84–95},
numpages = {12},
keywords = {software evolution, software product lines, software variability},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3579028.3609007,
author = {Fortz, Sophie},
title = {Variability-aware Behavioural Learning},
year = {2023},
isbn = {9798400700927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579028.3609007},
doi = {10.1145/3579028.3609007},
abstract = {Addressing variability proactively during software engineering activities means shifting from reasoning on individual systems to reasoning on families of systems. Adopting appropriate variability management techniques can yield important economies of scale and quality improvements. Conversely, variability can also be a curse, especially for Quality Assurance (QA), i.e., verification and testing of such systems, due to the combinatorial explosion of the number of software variants. Featured Transition Systems (FTSs) were introduced as a way to represent and reason about the behaviour of Variaility-intensive Systems (VISs). By labelling a transition system with feature expressions, FTSs capture multiple variants of a system in a single model, enabling reasoning at the family level. They have shown significant improvements in automated QA activities such as model-checking and model-based testing, as well as guiding design exploration activities. Yet, as most model-based approaches, FTS modelling requires both strong human expertise and significant effort that would be unaffordable in many cases, in particular for large legacy systems with outdated specifications and/or systems that evolve continuously.Therefore, this PhD project aims to automatically learn FTSs from existing artefacts, to ease the burden of modelling FTS and support continuous QA activities. To answer this research challenge, we propose a two-phase approach. First, we rely on deep learning techniques to locate variability from execution traces. For this purpose, we implemented a tool called VaryMinions. Then, we use these annotated traces to learn an FTS. In this second part, we adapt the seminal L algorithm to learn behavioural variability. Both frameworks are open-source and we evaluated them separately on several datasets of different sizes and origins (e.g., software product lines and configurable business processes).},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
pages = {11–15},
numpages = {5},
keywords = {Variability Mining, Software Product Lines, Reverse Engineering, Featured Transition Systems, Active Automata Learning},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3579028.3609017,
author = {Bombarda, Andrea and Bonfanti, Silvia and Gargantini, Angelo},
title = {On the Reuse of Existing Configurations for Testing Evolving Feature Models},
year = {2023},
isbn = {9798400700927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579028.3609017},
doi = {10.1145/3579028.3609017},
abstract = {Software Product Lines (SPLs) are used for representing a variety of highly configurable systems or families of systems. They are commonly represented by feature models (FMs). Starting from FMs, configurations, used as test cases, can be generated to identify the products of interest for further activities. As the other types of software, SPLs and their FMs may evolve due to changing requirements or bug-fixing. However, no guidance is usually given on what to do with derived configurations when an FM evolves. The common approach is based on generating all configurations from scratch, which is not optimal since a greater effort is required for concretizing the new tests, and some of the old ones may be still applicable.In this paper, we present the use of a technique for generating combinatorial tests for evolving feature models: this technique incrementally builds the new combinatorial configuration set starting from the one generated from the previous model. Furthermore, we present a novel definition of dissimilarity among configuration sets that can be used to evaluate how much an evolved test suite differs from the previous one and thus allows evaluating the effort required for adapting old test cases to the new ones.Our experiments confirm that using the proposed technique, in general, leads to lower dissimilarity and test suite size w.r.t. the generation of tests from scratch.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
pages = {67–76},
numpages = {10},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/2245276.2231956,
author = {Horikoshi, Hisayuki and Nakagawa, Hiroyuki and Tahara, Yasuyuki and Ohsuga, Akihiko},
title = {Dynamic reconfiguration in self-adaptive systems considering non-functional properties},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2231956},
doi = {10.1145/2245276.2231956},
abstract = {Self-adaptive systems have recently been receiving much attention because of their ability to cope with the changes of environment, failures, and unanticipated events. These systems need an adaptation mechanism, which automatically computes the possible configurations, and decides the most appropriate configuration to fit the environment. In particular, the satisfaction of non-functional requirements must be considered when selecting the best reconfiguration. However, there are trade-off problems among non-functional requirements. Moreover, the adaptation mechanisms are typically developed separately from the components to be implemented, and it complicates the construction of such systems. We propose (1) a feature-oriented analysis technique, which can identify adaptation points, and calculate the contribution to non-functional goals of the configuration; (2) a component specification model, which extends an architectural description language for self-adaptation; (3) a reconfiguration framework aimed to reduce the complexity of the reconfiguration and generate the best configuration at run-time. We evaluate the feasibility of our framework by four different scenarios, and show that our framework reduces the complexity of the reconfiguration, and solves the trade-off problem among non-functional requirements.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1144–1150},
numpages = {7},
keywords = {software architecture, self-adaptive systems, feature-oriented analysis, dynamic reconfiguration, architecture description language},
location = {Trento, Italy},
series = {SAC '12}
}

@inproceedings{10.1145/3382026.3425775,
author = {Nair, Suparna S. and Becker, Martin and Tenev, Vasil},
title = {A Comparative Study on Variability Code Analysis Technology},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3425775},
doi = {10.1145/3382026.3425775},
abstract = {Product line engineering is often conducted in an incremental way, in which the variability artifacts evolve in the space, the time, as well as the asset dimension. In order to cope with the evolution of the variability, the VITAL approach and tool have been developed and used in different industrial settings to analyze variability realizations relying on the C preprocessor. Over the last decade, further promising analysis approaches and tools have been developed. To understand, if and how they could enhance the VITAL approach, we have conducted an analysis of promising technologies.In this paper, we share some of our findings along our comparative study on variability code analysis technologies. As we have conducted the study in the light of the intended VITAL enhancement, the study does not claim completeness. Nevertheless, we believe that the findings can help researchers and industrial practitioners to gain an overview and find entry points for their own investigations.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {37–43},
numpages = {7},
keywords = {software product line, reverse engineering, configuration knowledge, Variability realization},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3579027.3608983,
author = {Murphy, Logan and Di Sandro, Alessio and Shahin, Ramy and Chechik, Marsha},
title = {Reusing Your Favourite Analysis Framework to Handle Workflows of Product Line Models},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608983},
doi = {10.1145/3579027.3608983},
abstract = {Model management frameworks support a wide array of analyses, transformations, and workflows, but lack native support for handling product lines of models. Yet the ubiquity of domains that heavily use model-driven techniques and are built using product lines, such as automotive, require adaptation, or lifting, of model management frameworks to be variability-aware. Lifting might introduce new implementation and validation costs, especially in safety-critical contexts. To facilitate the implementation and validation of variability-aware model management workflows, this paper provides a novel taxonomy of lifting methods. We compare the lifting methods in their capacity to reuse existing components and validation results. We then define a general framework for lifting and validating model management workflows, and report on an experience of lifting and validating modeling tasks and workflows in an existing Eclipse-based model management framework.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {117–128},
numpages = {12},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3461002.3473947,
author = {Pinnecke, Marcus},
title = {Product-lining the elinvar wealthtech microservice platform},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473947},
doi = {10.1145/3461002.3473947},
abstract = {Software product lining is the act of providing different but related software products under the same brand, known as a software product line (SPL). As engineering, management and validation of SPLs is far from trivial, special solutions for software product line engineering (SPLE) have a continuous momentum in both academic and industry. In general, it is hard to judge when to reasonably favor SPLE over alternative solutions that are more common in the industry. In this paper, we illustrate how we as Elinvar manage variability within our WealthTech Platform as a Service (PaaS) at different granularity levels, and discuss methods for SPLE in this context. More in detail, we share our techniques and concepts to address configuration management, and show how we manage a single microservice SPL including inter-service communication. Finally, we provide insights into platform solutions by means of packages for our clients. We end with a discussion on SPLE techniques in context of service SPLs and our packaging strategy. We conclude that while we are good to go with industry-standard approaches for microservice SPLs, the variability modeling and analysis advantages within SPLE is promising for our packaging strategy.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {60–68},
numpages = {9},
keywords = {variability management, technologies and concepts, product families, microservice platforms, configuration management},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461001.3471149,
author = {Lesoil, Luc and Acher, Mathieu and T\'{e}rnava, Xhevahire and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {The interplay of compile-time and run-time options for performance prediction},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471149},
doi = {10.1145/3461001.3471149},
abstract = {Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {100–111},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3503229.3547057,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Gurov, Dilian and Fuentes, Lidia},
title = {Defining categorical reasoning of numerical feature models with feature-wise and variant-wise quality attributes},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547057},
doi = {10.1145/3503229.3547057},
abstract = {Automatic analysis of variability is an important stage of Software Product Line (SPL) engineering. Incorporating quality information into this stage poses a significant challenge. However, quality-aware automated analysis tools are rare, mainly because in existing solutions variability and quality information are not unified under the same model.In this paper, we make use of the Quality Variability Model (QVM), based on Category Theory (CT), to redefine reasoning operations. We start defining and composing the six most common operations in SPL, but now as quality-based queries, which tend to be unavailable in other approaches. Consequently, QVM supports interactions between variant-wise and feature-wise quality attributes. As a proof of concept, we present, implement and execute the operations as lambda reasoning for CQL IDE - the state-of-the-art CT tool.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {132–139},
numpages = {8},
keywords = {automated reasoning, category theory, extended feature model, numerical features, quality attribute},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3461002.3473944,
author = {Ballesteros, Joaqu\'{\i}n and Fuentes, Lidia},
title = {Transfer learning for multiobjective optimization algorithms supporting dynamic software product lines},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473944},
doi = {10.1145/3461002.3473944},
abstract = {Dynamic Software Product Lines (DSPLs) are a well-accepted approach for self-adapting Cyber-Physical Systems (CPSs) at run-time. The DSPL approaches make decisions supported by performance models, which capture system features' contribution to one or more optimization goals. Combining performance models with Multi-Objectives Evolutionary Algorithms (MOEAs) as decision-making mechanisms is common in DSPLs. However, MOEAs algorithms start solving the optimization problem from a randomly selected population, not finding good configurations fast enough after a context change, requiring too many resources so scarce in CPSs. Also, the DSPL engineer must deal with the hardware and software particularities of the target platform in each CPS deployment. And although each system instantiation has to solve a similar optimization problem of the DSPL, it does not take advantage of experiences gained in similar CPS. Transfer learning aims at improving the efficiency of systems by sharing the previously acquired knowledge and applying it to similar systems. In this work, we analyze the benefits of transfer learning in the context of DSPL and MOEAs testing on 8 feature models with synthetic performance models. Results are good enough, showing that transfer learning solutions dominate up to 71% of the non-transfer learning ones for similar DSPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {51–59},
numpages = {9},
keywords = {transfer learning, self-adaptation, multiobjective optimization algorithms, dynamic software product lines, cyber-physical systems},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461001.3471141,
author = {Casquina, Junior Cupe and Montecchi, Leonardo},
title = {A proposal for organizing source code variability in the git version control system},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471141},
doi = {10.1145/3461001.3471141},
abstract = {Often, either to expand the target market or to satisfy specific new requirements, software systems inside a company are cloned, refactored, and customized, generating new derived software systems. Although this is a practical solution, it is not effective in the long-term because of the high maintenance costs when maintaining each of these derived software systems. Software product lines (SPLs) were proposed to reduce these costs; however, the lack of integration between variability realization mechanisms and version control systems reduces its attractiveness in the software development industry, especially in small and medium software companies. In this paper we propose an approach to integrate the conditional compilation mechanism used to implement the SPL variabilities and the Git version control system used to manage software versions in order to increase the attractiveness of the SPLs in the industry. The proposed solution also could be seen as a method to manage software system families' evolution in space and time.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {82–88},
numpages = {7},
keywords = {software product lines, conditional compilation, VarCS, SPL, Git},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and J\'{e}z\'{e}quel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3579027.3608971,
author = {Eichhorn, Domenik and Pett, Tobias and Osborne, Tobias and Schaefer, Ina},
title = {Quantum Computing for Feature Model Analysis: Potentials and Challenges},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608971},
doi = {10.1145/3579027.3608971},
abstract = {Feature modeling is a technique to model the variability of configurable systems. When working with feature models, it is possible to analyze them, for instance, by counting the number of valid configurations, searching feature model anomalies, or creating samples of configurations for testing. Classical feature model analysis techniques are based on solving algorithmic problems such as boolean satisfiability, satisfiability modulo theories, or integer linear programming. Existing analysis approaches provide satisfactory solutions for small and medium-sized problem instances, but scaling issues are observed for large-sized feature models. Quantum computers provide up to superpolynomial speedups for specific algorithmic problems and have the potential to solve those scaling issues. This paper analyzes the algorithmic techniques used in classical product line analysis and identifies potentials and challenges for quantum speedups. Our findings show that quantum algorithms like QAOA and Grover have the potential to speed up SAT and ILP-based feature model analysis techniques, but only after additional improvements in quantum hardware have been made.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {1–7},
numpages = {7},
keywords = {quantum computing, quantum algorithms, feature model analysis},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3546932.3546989,
author = {Bertolotti, Francesco and Cazzola, Walter and Favalli, Luca},
title = {Features, believe it or not! a design pattern for first-class citizen features on stock JVM},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3546989},
doi = {10.1145/3546932.3546989},
abstract = {Modern software systems must fulfill the needs of an ever-growing customer base. Due to the innate diversity of human needs, software should be highly customizable and reconfigurable. Researchers and practitioners gained interest in software product lines (SPL), mimicking aspects of product lines in industrial production for the engineering of highly-variable systems. There are two main approaches towards the engineering of SPLs. The first uses macros---such as the #ifdef macro in C. The second---called feature-oriented programming (FOP)---uses variability-aware preprocessors called composers to generate a program variant from a set of features and a configuration. Both approaches have disadvantages. Most notably, these approaches are usually not supported by the base language; for instance Java is one of the most commonly used FOP languages among researchers, but it does not support macros rather it relies on the C preprocessor or a custom one to translate macros into actual Java code. As a result, developers must struggle to keep up with the evolution of the base language, hindering the general applicability of SPL engineering. Moreover, to effectively evolve a software configuration and its features, their location must be known. The problem of recording and maintaining traceability information is considered expensive and error-prone and it is once again handled externally through dedicated modeling languages and tools. Instead, to properly convey the FOP paradigm, software features should be treated as first-class citizens using concepts that are proper to the host language, so that the variability can be expressed and analyzed with the same tools used to develop any other software in the same language. In this paper, we present a simple and flexible design pattern for JVM-based languages---dubbed devise pattern---that can be used to express feature dependencies and behaviors with a light-weight syntax both at domain analysis and at domain implementation level. To showcase the qualities and feasibility of our approach, we present several variability-aware implementations of a MNIST-encoder---including one using the devise pattern---and compare strengths and weaknesses of each approach.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {32–42},
numpages = {11},
keywords = {variability modeling, software product lines, design patterns},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/1842752.1842809,
author = {Helleboogh, Alexander and Avgeriou, Paris and Bouck\'{e}, Nelis and Heymans, Patrick},
title = {Workshop on Variability in Software Product Line Architectures (VARI-ARCH 2010)},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842809},
doi = {10.1145/1842752.1842809},
abstract = {The objective of this workshop is to bring together researchers from the software product line community and software architecture community to identify critical challenges and progress the state-of-the-art on variability in software product line architectures.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {309–311},
numpages = {3},
keywords = {viewpoint, view, variability, software architecture, product lines, product line architecture, model, concern, assets},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@inproceedings{10.1145/3461001.3471147,
author = {Kenner, Andy and May, Richard and Kr\"{u}ger, Jacob and Saake, Gunter and Leich, Thomas},
title = {Safety, security, and configurable software systems: a systematic mapping study},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471147},
doi = {10.1145/3461001.3471147},
abstract = {Safety and security are important properties of any software system, particularly in safety-critical domains, such as embedded, automotive, or cyber-physical systems. Moreover, particularly those domains also employ highly-configurable systems to customize variants, for example, to different customer requirements or regulations. Unfortunately, we are missing an overview understanding of what research has been conducted on the intersection of safety and security with configurable systems. To address this gap, we conducted a systematic mapping study based on an automated search, covering ten years (2011--2020) and 65 relevant (out of 367) publications. We classified each publication based on established security and safety concerns (e.g., CIA triad) as well as the connection to configurable systems (e.g., ensuring security of such a system). In the end, we found that considerably more research has been conducted on safety concerns, but both properties seem under-explored in the context of configurable systems. Moreover, existing research focuses on two directions: Ensuring safety and security properties in product-line engineering; and applying product-line techniques to ensure safety and security properties. Our mapping study provides an overview of the current state-of-the-art as well as open issues, helping practitioners identify existing solutions and researchers define directions for future research.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {148–159},
numpages = {12},
keywords = {software product line engineering, security, safety, mapping study, configurable systems},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461002.3473942,
author = {Kahraman, G\"{o}khan and Cleophas, Loek},
title = {Automated derivation of variants in manufacturing systems design},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473942},
doi = {10.1145/3461002.3473942},
abstract = {The Logistics Specification and Analysis Tool (LSAT) is a modelbased engineering tool used for design-space exploration of flexible manufacturing systems. LSAT provides domain specific languages to model a manufacturing system and means to analyze the productivity characteristics of such a system. In LSAT, developers can specify a system and model its deterministic operations as a set of activities. Given a set of activities, it is possible to construct an individual activity sequence that represents one valid system execution, and with minor variations in the specification individual systems can be obtained. To avoid modeling each variant separately, which means cloning and maintaining the common parts, new functionality is needed to deal with the variability of system specifications. In this study, we aim to establish integration between LSAT and product line engineering techniques. Specifically, we provide a realization of a toolchain including variability representation of LSAT realization artifacts and automated variant derivation for the LSAT model variants. Delta modeling, a transformational variability realization mechanism, is employed to model the variability within LSAT realization artifacts. Using the toolchain, we develop an industry-related case for a product line, the so called Extended Twilight System, a Cyber Physical System (CPS) inspired by the CPSs of our industrial partner.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {45–50},
numpages = {6},
keywords = {variability modeling, product lines, model-based engineering, manufacturing systems, delta modeling},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3579027.3608985,
author = {Bittner, Paul Maximilian and Schulthei\ss{}, Alexander and Greiner, Sandra and Moosherr, Benjamin and Krieter, Sebastian and Tinnes, Christof and Kehrer, Timo and Th\"{u}m, Thomas},
title = {Views on Edits to Variational Software},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608985},
doi = {10.1145/3579027.3608985},
abstract = {Software systems are subject to frequent changes, for example to fix bugs or meet new customer requirements. In variational software systems, developers are confronted with the complexity of evolution and configurability on a daily basis; essentially handling changes to many distinct software variants simultaneously. To reduce the complexity of configurability for developers, filtered or projectional editing was introduced: By providing a partial or complete configuration, developers can interact with a simpler view of the variational system that shows only artifacts belonging to that configuration. Yet, such views are available for individual revisions only but not for edits performed across revisions. To reduce the complexity of evolution in variational software for developers, we extend the concept of views to edits. We formulate a correctness criterion for views on edits and introduce two correct operators for view generation, one operator suitable for formal reasoning, and a runtime optimized operator. In an empirical study, we demonstrate the feasibility of our operators by applying them to the change histories of 44 open-source software systems.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {141–152},
numpages = {12},
keywords = {variation control, software variability, software product lines, software evolution, projectional editing},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3546932.3547009,
author = {J\'{e}z\'{e}quel, Jean-Marc and Kienzle, J\"{o}rg and Acher, Mathieu},
title = {From feature models to feature toggles in practice},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547009},
doi = {10.1145/3546932.3547009},
abstract = {Feature Toggles (often also referred to as Feature Flags) are a powerful technique, providing an alternative to maintaining multiple feature branches in source code. A condition within the code enables or disables a feature at runtime, hence providing a kind of runtime variability resolution. Several works have already identified the proximity of this concept with the notion of Feature found in Software Product Lines. In this paper, we propose to go one step further in unifying these concepts to provide a seamless transition between design time and runtime variability resolutions. We propose to model all the variability using a feature model. Then this feature model can be partially resolved at design time (yielding an incomplete product derivation), the unresolved variability being used to generate feature toggles that can be enabled/disabled at runtime. We first demonstrate these ideas on the toy example of the Expression Product Line, and then show how it can scale to build a configurable authentication system, where a partially resolved feature model can interface with popular feature toggle frameworks such as Togglz.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {234–244},
numpages = {11},
keywords = {variability, feature toggles and flags, configuration, binding times},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3503229.3547069,
author = {Comploi-Taupe, Richard and Francescutto, Giulia and Schenner, Gottfried},
title = {Applying incremental answer set solving to product configuration},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547069},
doi = {10.1145/3503229.3547069},
abstract = {In this paper, we apply incremental answer set solving to product configuration. Incremental answer set solving is a step-wise incremental approach to Answer Set Programming (ASP). We demonstrate how to use this technique to solve product configurations problems incrementally. Every step of the incremental solving process corresponds to a predefined configuration action. Using complex domain-specific configuration actions makes it possible to tightly control the level of non-determinism and performance of the solving process. We show applications of this technique for reasoning about product configuration, like simulating the behavior of a deterministic configuration algorithm and describing user actions.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {150–155},
numpages = {6},
keywords = {answer set programming, incremental solving, product configuration},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3579027.3608973,
author = {Galindo, Jos\'{e} A. and Dominguez, Antonio J. and White, Jules and Benavides, David},
title = {Large Language Models to generate meaningful feature model instances},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608973},
doi = {10.1145/3579027.3608973},
abstract = {Feature models are the "de facto" standard for representing variability in software-intensive systems. Automated analysis of feature models is the computer-aided extraction of information of feature models and is used in testing, maintenance, configuration, and derivation, among other tasks. Testing the analyses of feature models often requires relying on a large number of models that are as realistic as possible. There exist different proposals to generate synthetic feature models using random techniques or metamorphic relations; however, the existing methods do not take into account the semantics of the concepts of the domain that are being represented and the interrelations between them, leading to less realistic feature models. In this paper, we propose a novel approach that uses Large Language Models (LLMs), such as Codex or GPT-3, to generate realistic feature models that preserve semantic coherence while maintaining syntactic validity. The approach automatically generates instances of feature models from a given domain. Concretely, two language models were used, first OpenAI's Codex to generate new instances of feature models using the Universal Variability Language (UVL) syntax and then Cohere's semantic analysis to verify if the newly introduced concepts are from the same domain. This approach enabled the generation of 90% of valid instances according to the UVL syntax. In addition, the valid models score well on model complexity metrics, and the generated features mirror the domain of the original UVL instance used as prompts. With this work, we envision a new thread of research where variability is generated and analyzed using LLMs. This opens the door for a new generation of techniques and tools for variability management.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {15–26},
numpages = {12},
keywords = {universal variability language, synthetic models, large language models, deep learning},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/1244002.1244266,
author = {Inoki, Mari and Fukazawa, Yoshiaki},
title = {Software product line evolution method based on kaizen approach},
year = {2007},
isbn = {1595934804},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1244002.1244266},
doi = {10.1145/1244002.1244266},
abstract = {Continuing optimal product line development needs to evolve core assets in response to market, technology or organization changes. In this paper, we propose a product line evolution method based on the kaizen approach. Kaizen is a continuous improvement method that is adopted in Japanese industry. The important points of the kaizen are to prepare a work standard and continue to improve processes by correcting the differences between the standard and actual results. Our core asset kaizen method provides a standard that includes core asset types based on simple metrics, kaizen patterns representing expertise, and kaizen processes for continuous improvement.},
booktitle = {Proceedings of the 2007 ACM Symposium on Applied Computing},
pages = {1207–1214},
numpages = {8},
keywords = {software product line, pattern, evolution, core asset, kaizen},
location = {Seoul, Korea},
series = {SAC '07}
}

@inproceedings{10.1145/2815782.2815799,
author = {Schaefer, Ina and Seidl, Christoph and Cleophas, Loek and Watson, Bruce W.},
title = {SPLicing TABASCO: Custom-Tailored Software Product Line Variants from Taxonomy-Based Toolkits},
year = {2015},
isbn = {9781450336833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815782.2815799},
doi = {10.1145/2815782.2815799},
abstract = {Taxonomy-Based Software Construction (TABASCO) applies extensive domain analyses to create conceptual hierarchies of algorithmic domains. Those are used as basis for the implementation of software toolkits. The monolithic structure of TABASCO-based toolkits restricts their adoption on resource-constrained or special-purpose devices. In this paper, we address this problem by applying Software Product Line (SPL) techniques to TABASCO-based toolkits: We use software taxonomies as input to creating a conceptual representation of variability as feature models of an SPL. We apply the variability realization mechanism delta modeling to transform realization artifacts, such as source code, to only contain elements for a particular selection of features. Our method is suitable for proactive, reactive and extractive SPL development so that it supports a seamless adoption and evolution of an SPL approach for TABASCO-based toolkits. We demonstrate the feasibility of the method with three case studies by proactively, reactively and extractively transforming TABASCO-based toolkits to SPLs, which allow derivation of variants with custom-tailored functionality.},
booktitle = {Proceedings of the 2015 Annual Research Conference on South African Institute of Computer Scientists and Information Technologists},
articleno = {34},
numpages = {10},
keywords = {Taxonomy-Based Software Construction (TABASCO) toolkit, Software Product Line (SPL) adoption},
location = {Stellenbosch, South Africa},
series = {SAICSIT '15}
}

@inproceedings{10.1145/3503229.3547049,
author = {Sandrin, Enrico and Forza, Cipriano and Leitner, Gerhard and Trentin, Alessio},
title = {Configuration manager: describing an emerging professional figure},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547049},
doi = {10.1145/3503229.3547049},
abstract = {The implementation and management of product configurators in enterprises are accompanied by several challenges and it is widely recognized that the organizational ones are among the biggest. In order to overcome such challenges, companies need people with adequate competencies. What are the required individual competencies to successfully implement and use product configurators (both sales and technical ones)? Which are the characteristics of an ideal professional figure that can have all these competencies? How could the needed competencies be developed through training? This paper brings these questions to the scientific discussion on product and sales configurators. However, these questions have a wider scope since they also relate to the enquiry on mass customization and on product variety management: they deepen the perspective on organizational design related to these issues.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {193–200},
numpages = {8},
keywords = {configuration, configuration manager, individual competencies, mass customization, organization design, professional profile},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3579028.3609005,
author = {Nguyen, Guillaume},
title = {A configurable approach to cyber-physical systems fuzzing},
year = {2023},
isbn = {9798400700927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579028.3609005},
doi = {10.1145/3579028.3609005},
abstract = {Operational Technology has gotten a growing place in our daily lives. With the increasing number of devices (connected or not), the need for a clean environment that allows effective and efficient testing is also increasing. Furthermore, some devices are connected to the physical world with the ability to affect it. Assembling those specific devices with at least a sensor, an actuator, and a (micro)processor creates Cyber-Physical Systems (CPSs). With such power in the hands of machines, it is imperative that they behave as expected and that they resist disruptive environments (whether from cyber attacks, unwanted noise, or environmental disturbance). Indeed, the impacts of an unexpected behavior could lead to significant damage (disruption of the production line, overheating of a nuclear reactor, false fire alarm, etc.). That is why the safety and the security of those systems should also be at the center of concerns. As the definition of those systems is quite simple, one can assemble various components to create a unique CPS. One could also modify an existing CPS to satisfy a specific need (e.g., a fire alarm system modified to detect carbon monoxide in the air, changing communication protocols or programming languages used for the sake of maintainability). To test such highly-configurable systems, there are multiple techniques. Fuzzing works particularly well with any system by sending pseudo-random inputs. To adapt to specific systems and test requirements (coverage, resources, etc.), fuzzing is itself highly-configurable (Grammar-based, symbolic, probabilistic, etc.). This is why it could perform particularly well with CPSs, which all might require a different and specific testing approach depending on their interfaces, components, etc. Currently, no frameworks allow for the classification of CPSs to enable the automatization of the generation of tests following their requirements. That is why this thesis will take a configurable approach to find and recommend the most suitable classification of CPS for testing and comparing the various fuzzing techniques to find the most effective ones based on relevant features and requirements of CPSs.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
pages = {1–5},
numpages = {5},
keywords = {testing and quality assurance, fuzzing, embedded systems, IoT, CPS},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@article{10.1145/3628158,
author = {Xiang, Yi and Huang, Han and Li, Sizhe and Li, Miqing and Luo, Chuan and Yang, Xiaowei},
title = {Automated Test Suite Generation for Software Product Lines Based on Quality-Diversity Optimization},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3628158},
doi = {10.1145/3628158},
abstract = {A Software Product Line (SPL) is a set of software products that are built from a variability model. Real-world SPLs typically involve a vast number of valid products, making it impossible to individually test each of them. This arises the need for automated test suite generation, which was previously modeled as either a single-objective or a multi-objective optimization problem considering only objective functions. This article provides a completely different mathematical model by exploiting the benefits of Quality-Diversity (QD) optimization that is composed of not only an objective function (e.g., t-wise coverage or test suite diversity) but also a user-defined behavior space (e.g., the space with test suite size as its dimension). We argue that the new model is more suitable and generic than the two alternatives because it provides at a time a large set of diverse (measured in the behavior space) and high-performing solutions that can ease the decision-making process. We apply MAP-Elites, one of the most popular QD algorithms, to solve the model. The results of the evaluation, on both realistic and artificial SPLs, are promising, with MAP-Elites significantly and substantially outperforming both single- and multi-objective approaches, and also several state-of-the-art SPL testing tools. In summary, this article provides a new and promising perspective on the test suite generation for SPLs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {46},
numpages = {52},
keywords = {Quality-Diversity (QD) optimization, automated test suite generation, Software Product Line}
}

@inproceedings{10.1145/3233027.3236395,
author = {Pereira, Juliana Alves and Maciel, Lucas and Noronha, Thiago F. and Figueiredo, Eduardo},
title = {Heuristic and exact algorithms for product configuration in software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236395},
doi = {10.1145/3233027.3236395},
abstract = {The Software Product Line (SPL) configuration field is an active area of research and has attracted both practitioners and researchers attention in the last years. A key part of an SPL configuration is a feature model that represents features and their dependencies (i.e., SPL configuration rules). This model can be extended by adding Non-Functional Properties (NFPs) as feature attributes resulting in Extended Feature Models (EFMs). Configuring products from an EFM requires considering the configuration rules of the model and satisfying the product functional and non-functional requirements. Although the configuration of a product arising from EFMs may reduce the space of valid configurations, selecting the most appropriate set of features is still an overwhelming task due to many factors including technical limitations and diversity of contexts. Consequently, configuring large and complex SPLs by using configurators is often beyond the users' capabilities of identifying valid combinations of features that match their (non-functional) requirements. To overcome this limitation, several approaches have modeled the product configuration task as a combinatorial optimization problem and proposed constraint programming algorithms to automatically derive a configuration. Although these approaches do not require any user intervention to guarantee the optimality of the generated configuration, due to the NP-hard computational complexity of finding an optimal variant, exact approaches have inefficient exponential time. Thus, to improve scalability and performance issues, we introduced the adoption of a greedy heuristic algorithm and a biased random-key genetic algorithm (BRKGA). Our experiment results show that our proposed heuristics found optimal solutions for all instances where those are known. For the instances where optimal solutions are not known, the greedy heuristic outperformed the best solution obtained by a one-hour run of the exact algorithm by up to 67.89%. Although the BRKGA heuristic slightly outperformed the greedy heuristic, it has shown larger running times (especially on the largest instances). Therefore, to ensure a good user experience and enable a very fast configuration task, we extended a state-of-the-art configurator with the proposed greedy heuristic approach.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {247},
numpages = {1},
keywords = {software product lines, software product line configuration, search-based software engineering, configuration optimization},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3382026.3431247,
author = {Meixner, Kristof},
title = {Integrating Variability Modeling of Products, Processes, and Resources in Cyber-Physical Production Systems Engineering},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431247},
doi = {10.1145/3382026.3431247},
abstract = {The Industry 4.0 initiative envisions the flexible and optimized production of customized products on Cyber-Physical Production Systems (CPPSs) that consist of subsystems coordinated to conduct complex production processes. Hence, accurate CPPS modeling requires integrating the modeling of variability for Product-Process-Resource (PPR) aspects. Yet, current variability modeling approaches treat structural and behavioral variability separately, leading to inaccurate CPPS production models that impede CPPS engineering and optimization. This paper proposes a PhD project for integrated variability modeling of PPR aspects to improve the accuracy of production models with variability for CPPS engineers and production optimizers. The research project follows the Design Science approach aiming for the iterative design and evaluation of (a) a framework to categorize currently incomplete and scattered models and methods for PPR variability modeling as a foundation for an integrated model; and (b) a modeling approach for more accurate integrated PPR variability modeling. The planned research will provide the Software Product Line (SPL) and CPPS engineering research communities with (a) novel models, methods, and insights on integrated PPR variability modeling, (b) open data from CPPS engineering use cases for common modeling, and (c) empirical data from field studies for shared analysis and evaluation.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {96–103},
numpages = {8},
keywords = {Variability Modelling, Product-Process-Resource, Cyber-Physical Production System},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3461001.3473065,
author = {Michelon, Gabriela K. and Sotto-Mayor, Bruno and Martinez, Jabier and Arrieta, Aitor and Abreu, Rui and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Spectrum-based feature localization: a case study using ArgoUML},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3473065},
doi = {10.1145/3461001.3473065},
abstract = {Feature localization (FL) is a basic activity in re-engineering legacy systems into software product lines. In this work, we explore the use of the Spectrum-based localization technique for this task. This technique is traditionally used for fault localization but with practical applications in other tasks like the dynamic FL approach that we propose. The ArgoUML SPL benchmark is used as a case study and we compare it with a previous hybrid (static and dynamic) approach from which we reuse the manual and testing execution traces of the features. We conclude that it is feasible and sound to use the Spectrum-based approach providing promising results in the benchmark metrics.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {126–130},
numpages = {5},
keywords = {spectrum-based localization, dynamic feature localization, ArgoUML SPL benchmark},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3546932.3546996,
author = {Uta, Mathias and Felfernig, Alexander and Helic, Denis and Le, Viet-Man},
title = {Accuracy- and consistency-aware recommendation of configurations},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3546996},
doi = {10.1145/3546932.3546996},
abstract = {Constraint-based configurators support users in deciding which components and features should be included in a configuration. Due to the increasing size and complexity of configurable products and services, recommender systems are used to personalize the interaction with configurators. Since basic recommendation approaches such as collaborative filtering do not take into account constraints between variable values, recommendations can induce inconsistencies between user requirements and the underlying configuration knowledge base. In this paper, we introduce a constraint-based configuration approach that integrates the results of model-based collaborative filtering (e.g., implemented as feed forward neural network) into constraint solving in such a way that the solver (configurator) is able to determine consistency-preserving and user-relevant configurations.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {79–84},
numpages = {6},
keywords = {neural networks, feature models, constraint solving, configuration, collaborative filtering},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/2491627.2491631,
author = {Myll\"{a}rniemi, Varvana and Savolainen, Juha and M\"{a}nnist\"{o}, Tomi},
title = {Performance variability in software product lines: a case study in the telecommunication domain},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491631},
doi = {10.1145/2491627.2491631},
abstract = {In the research on software product lines, product variants typically differ by their functionality, and quality attributes are more or less similar across products. To accumulate empirical evidence, this paper presents a descriptive case study of performance variability in a software product line of mobile network base stations. The goal is to study the motivation to vary performance, and the strategy for realizing performance variability in the product line architecture. The results highlight that the evolution of customer needs motivates performance variability; performance variability can be realized either with software or hardware variability strategy, with the latter often being prevailing; and the software strategy can be kept focused by downgrading performance.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {32–41},
numpages = {10},
keywords = {variability, software product line, case study, architecture},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3579027.3608984,
author = {Krieter, Sebastian and Kr\"{u}ger, Jacob and Leich, Thomas and Saake, Gunter},
title = {VariantInc: Automatically Pruning and Integrating Versioned Software Variants},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608984},
doi = {10.1145/3579027.3608984},
abstract = {Developers use version-control systems and software-hosting platforms to manage their software systems. They rely on the provided branching and forking mechanisms to implement new features, fix bugs, and develop customized system variants. A particular problem arises when forked variants are not re-integrated (i.e., merged), but kept and co-evolved as individual systems. This can cause maintenance overheads, due to change propagation and limitations in simultaneously managing variations in space (variants) and time (revisions). Thus, most organizations decide to integrate their set of variants into a single platform at some point, and several techniques have been proposed to semi-automate such an integration. However, existing techniques usually consider only a single revision of each variant and do not merge the revision histories, disregarding that not only variants (i.e., configuring the features of the system) but also revisions (i.e., checking out specific versions of the features) are important. We propose an automated technique, VariantInc, for analyzing, pruning, and integrating variants of a system that also merges the revision history of each variant into the resulting platform (i.e., using presence conditions). To validate VariantInc, we employed it on 160 open-source C systems of various sizes (i.e., number of forks, revisions, source code). The results show that VariantInc works as intended, and allows developers or researchers to automatically integrate variants into a platform as well as to perform software analyses.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {129–140},
numpages = {12},
keywords = {Version control, Variant-rich systems, Variant integration, Forks},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3579027.3608996,
author = {Fischer, Stefan and Ramler, Rudolf and Assun\c{c}\~{a}o, Wesley K. G. and Egyed, Alexander and Gradl, Christian and Auberger, Sebastian},
title = {Model-based Testing for a Family of Mobile Applications: Industrial Experiences},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608996},
doi = {10.1145/3579027.3608996},
abstract = {Testing is a fundamental verification activity to produce high-quality software. However, testing is a costly and complex activity. The success of software testing depends on the quality of test cases but finding a good set of test cases is laborious. To make matters worse, when dealing with a family of systems (e.g., variants of a mobile applications), test cases must assure that a diversity of configurations in potentially many variants work as expected. This is the case of hello again GmbH, a company that develops mobile applications for customer loyalty (e.g., discounts, free products, rewards, or insider perks). The company targets several business domains, and currently supports about 700 application variants. Testing such applications including all their variability is a cumbersome task. Even simple test cases designed for one variant most likely cannot be reused for other variants. To support developers at hello again GmbH, we present a solution to employ a model-based testing approach to their family of mobile apps. Model-based testing focuses on automatizing the design and generation of test cases. We present results of applying model-based testing on 27 applications from hello again GmbH and report the challenges and lessons learned for designing a variable test model. Our expected contribution is to support companies and practitioners looking for solutions to test families of software products.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {242–253},
numpages = {12},
keywords = {Variability Testing, Software Product Lines, Mobile Testing},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3503229.3547046,
author = {Le, Viet-Man and Felfernig, Alexander and Tran, Thi Ngoc Trang},
title = {Test case aggregation for efficient feature model testing},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547046},
doi = {10.1145/3503229.3547046},
abstract = {The increasing size and complexity of feature models (FM) requires the provision of efficient testing and debugging techniques. Feature models can be tested, for example, with regard to their conformance with a pre-defined set of analysis operations. In this paper, we show how the number of consistency checks for FM testing can be reduced on the basis of test case aggregation. Using a divide-and-conquer based approach, we show how to transform a feature model test suite into a corresponding aggregated representation where individual test cases can be combined if specific consistency criteria are fulfilled. Performance improvements are also analyzed on the basis of a best- and worst-case runtime analysis.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {174–177},
numpages = {4},
keywords = {testing and debugging, variability modeling},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3307630.3342705,
author = {Krieter, Sebastian},
title = {Enabling Efficient Automated Configuration Generation and Management},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342705},
doi = {10.1145/3307630.3342705},
abstract = {Creating and managing valid configurations is one of the main tasks in software product line engineering. Due to the often complex constraints from a feature model, some kind of automated configuration generation is required to facilitate the configuration process for users and developers. For instance, decision propagation can be applied to support users in configuring a product from a software product line (SPL) with less manual effort and error potential, leading to a semi-automatic configuration process. Furthermore, fully-automatic configuration processes, such as random sampling or t-wise interaction sampling can be employed to test or to optimize an SPL. However, current techniques for automated configuration generation still do not scale well to SPLs with large and complex feature models. Within our thesis, we identify current challenges regarding the efficiency and effectiveness of the semi- and fully-automatic configuration process and aim to address these challenges by introducing novel techniques and improving current ones. Our preliminary results show already show promising progress for both, the semi- and fully-automatic configuration process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {215–221},
numpages = {7},
keywords = {uniform random sampling, t-wise sampling, software product lines, decision propagation, configurable system},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3503229.3547047,
author = {Felfernig, Alexander and Ortner, Bettina and Le, Viet-Man},
title = {Table-based knowledge representations for industrial feature models},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547047},
doi = {10.1145/3503229.3547047},
abstract = {Configuration knowledge engineering in industrial settings often has to face the challenge that product domain experts do not have background knowledge in formal configuration knowledge representations. In this context, graphical configuration models such as feature models provide a basis for the communication between domain experts and knowledge engineers. However, in many cases domain experts are used to define configuration knowledge in table-based representations. In this paper, we discuss ways of how to represent basic configuration constraint types in the form of a table-based representation thus allowing an alternative definition and exchange of product configuration knowledge.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {245–248},
numpages = {4},
keywords = {feature models, knowledge-based configuration, table-based knowledge representations},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3503229.3547058,
author = {M\'{a}rquez, Germ\'{a}n and Galindo, Jos\'{e} A. and Varela-Vaca, \'{A}ngel Jes\'{u}s and L\'{o}pez, Mar\'{\i}a Teresa G\'{o}mez and Benavides, David},
title = {Advisory: vulnerability analysis in software development project dependencies},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547058},
doi = {10.1145/3503229.3547058},
abstract = {Security has become a crucial factor in the development of software systems. The number of dependencies in software systems is becoming a source of countless bugs and vulnerabilities. In the past, the product line community has proposed several techniques and mechanisms to cope with the problems that arise when dealing with variability and dependency management in such systems. In this paper, we present Advisory, a solution that allows automated dependency analysis for vulnerabilities within software projects based on techniques from the product line community. Advisory first inspects software dependencies, then generates a dependency graph, to which security information about vulnerabilities is attributed and translated into a formal model, in this case, based on SMT. Finally, Advisory provides a set of analysis and reasoning operations on these models that allow extracting helpful information about the location of vulnerabilities of the project configuration space, as well as details for advising on the security risk of these projects and their possible configurations.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {99–102},
numpages = {4},
keywords = {CVE, dependency, impact, library, risk, security, software project, verification, vulnerability},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3503229.3547038,
author = {Workalemahu, Robel Negussie and Forza, Cipriano and Suzic, Nikola},
title = {Product configurators for additively manufactured products: exploring their peculiar characteristics},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547038},
doi = {10.1145/3503229.3547038},
abstract = {The capability of realizing individually customized products with complex geometries makes additive manufacturing (AM) ever more considered by companies engaged in mass customized manufacturing. In order to be exploited in the market, the AM allowed geometry freedom has to be transferred to the customers for the customer-specific customization. Notably, this is a new request posed to product configurators (PC). So, in this research we ask: How is this request being answered by pioneers who engage in this challenge? Are there other new requests that AM poses to configurators? The present paper aims at answering these exploratory questions by looking at how these issues have been considered in existing literature and by providing some examples. We hope that considerations derived from this investigation will open a discussion on this topic in the product configuration research community with the goal to identify peculiar PC capabilities needed to customize additively manufactured products using PCs.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {201–208},
numpages = {8},
keywords = {additive manufacturing, mass customization, personalization, product configuration},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3503229.3547056,
author = {Romano, Dario and Feichtinger, Kevin and Beuche, Danilo and Ryssel, Uwe and Rabiser, Rick},
title = {Bridging the gap between academia and industry: transforming the universal variability language to pure::variants and back},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547056},
doi = {10.1145/3503229.3547056},
abstract = {In the last 30 years, many variability modeling approaches have been developed and new ones are still developed regularly. Most of them are only described in academic papers, only few come with tool support. The sheer plethora of approaches, all differing in terms of scope and expressiveness, makes it difficult to assess their properties, experiment with them and find the right approach for a specific use case. Implementing transformations between variability modeling approaches or importers/exporters for tools can help, but are hard to realize without information loss. In this paper, we describe how we derived and implemented transformations between the academically developed Universal Variability Language and the commercially developed pure::variants tool, with as little information loss as possible. Our approach can also be used to optimize constraints, e.g., reduce their number without an effect on the configuration space, using particular capabilities pure::variants provides. Also, via an existing variability model transformation approach, which uses UVL as a pivot language, we enable the transformation of FeatureIDE feature models, DOPLER decision models, and Orthogonal Variability Models into/from pure::variants and back. With our approach, we work towards bridging the gap between academic and industrial variability modeling tools and enable experiments with the different capabilities these tools provide.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {123–131},
numpages = {9},
keywords = {software product lines, variability model transformations, variability modeling, variability modeling tools},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3382025.3414943,
author = {Th\"{u}m, Thomas},
title = {A BDD for Linux? the knowledge compilation challenge for variability},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414943},
doi = {10.1145/3382025.3414943},
abstract = {What is the number of valid configurations for Linux? How to generate uniform random samples for Linux? Can we create a binary decision diagram for Linux? It seems that the product-line community tries hard to answer such questions for Linux and other configurable systems. However, attempts are often not published due to the publication bias (i.e., unsuccessful attempts are not published). As a consequence, researchers keep trying by potentially spending redundant effort. The goal of this challenge is to guide research on these computationally complex problems and to foster the exchange between researchers and practitioners.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {16},
numpages = {6},
keywords = {software product line, software configuration, satisfiability solving, product configuration, knownledge compilation, feature models, decision models, configurable system, binary decision diagrams, artificial intelligence},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3503229.3547025,
author = {Horcas, Jose M. and Galindo, Jose A. and Pinto, M\'{o}nica and Fuentes, Lidia and Benavides, David},
title = {FM fact label: a configurable and interactive visualization of feature model characterizations},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547025},
doi = {10.1145/3503229.3547025},
abstract = {Recognizing specific characteristics of feature models (FM) can be challenging due to the different nature and domains of the models. There are several metrics to characterize FMs. However, there is no standard way to visualize and identify the properties that make an FM unique and distinguishable. We propose FM Fact Label as a tool to visualize an FM characterization based on its metadata, structural measures, and analytical metrics. Although existing tools can provide a visualization of the FM and report some metrics, the feature diagram of large-scale FMs becomes ineffective to take an overall shape of the FM properties. Moreover, the reported metrics are often embedded in the tool user interface, preventing further analysis. FM Fact Label is a standalone web-based tool that provides a configurable and interactive visualization of FM characterizations that can be exported to several formats. Our contribution becomes important because the Universal Variability Language (UVL) is starting to gain attraction in the software product line community as a unified textual language to specify FMs and share knowledge. With this contribution, we help to advance the UVL ecosystem one step forward while providing a common representation for the results of existing analysis tools.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {42–45},
numpages = {4},
keywords = {characterization, feature model, metrics, variability, visualization},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/1147249.1147257,
author = {Cohen, Myra B. and Dwyer, Matthew B. and Shi, Jiangfan},
title = {Coverage and adequacy in software product line testing},
year = {2006},
isbn = {1595934596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1147249.1147257},
doi = {10.1145/1147249.1147257},
abstract = {Software product line modeling has received a great deal of attention for its potential in fostering reuse of software artifacts across development phases. Research on the testing phase, has focused on identifying the potential for reuse of test cases across product line instances. While this offers potential reductions in test development effort for a given product line instance, it does not focus on and leverage the fundamental abstraction that is inherent in software product lines - variability.In this paper, we illustrate how rich software product line modeling notations can be mapped onto an underlying relational model that captures variability in the feasible product line instances. This relational model serves as the semantic basis for defining a family of coverage criteria for testing of a product line. These criteria make it possible to accumulate test coverage information for the product line itself over the course of multiple product line instance development efforts. Cumulative coverage, in turn, enables targeted testing efforts for new product line instances. We describe how combinatorial interaction testing methods can be applied to define test configurations that achieve a desired level of coverage and identify challenges to scaling such methods to large, complex software product lines.},
booktitle = {Proceedings of the ISSTA 2006 Workshop on Role of Software Architecture for Testing and Analysis},
pages = {53–63},
numpages = {11},
location = {Portland, Maine},
series = {ROSATEA '06}
}

@inproceedings{10.1145/3546932.3546992,
author = {Le, Viet-Man and Felfernig, Alexander and Uta, Mathias and Tran, Thi Ngoc Trang and Silva, Cristian Vidal},
title = {WipeOutR: automated redundancy detection for feature models},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3546992},
doi = {10.1145/3546932.3546992},
abstract = {Feature models are used to specify variability and commonality properties of software artifacts. In order to assure high-quality models, different feature model analysis and testing operations can be applied. In this paper, we present two new algorithms that help to make feature model configuration as well as different kinds of analysis operations more efficient. Specifically, we focus on the automated identification of redundancies in feature models and cor-responding test suites. Redundant constraints in feature models can lead to low-performing configuration (solution) search and also to additional efforts in feature model debugging. Redundant feature model test cases can trigger inefficiencies in testing operations. In this paper, we introduce WipeOutR which is an algorithmic approach to support the automated identification of redundancies. This approach has the potential to significantly improve the quality of feature model development and configuration.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {164–169},
numpages = {6},
keywords = {variability modeling, testing and debugging, redundancy detection, quality assurance, feature models},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3503229.3547059,
author = {Wittler, Jan Willem and K\"{u}hn, Thomas and Reussner, Ralf},
title = {Towards an integrated approach for managing the variability and evolution of both software and hardware components},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547059},
doi = {10.1145/3503229.3547059},
abstract = {Although the development of mass-customized products has been successfully applied to both hardware and software, companies struggle managing the variability and evolution of software-intensive products within a coherent product engineering approach. While the variability and evolution of software alone is manageable, managing both software and hardware within one product line is a complex task and requires an integrated approach. Moreover, as the release cycle for hardware is longer than for software, a product line of hardware and software is usually developed in generations. While one generation is validated and produced, the next generation is already planned and designed, reusing both software and hardware of previous generations. Thus, the different generations and artifacts shared between them must be managed together. Finally, when approaches directly assign software to hardware, managing their evolution becomes increasingly complex. Evolved resource demands may be missed, exhausting the resources provided by the hardware, possibly leading to degraded or faulty functionality. To remedy this, we refine the Unified Conceptual Model to our Variability Model for both Software and Hardware capturing the notion of product line generations, versions and variants of both software and hardware components, as well as resource demands of software on hardware. This is the first step towards the development of an integrated product engineering approach for managing the variability and evolution of software-intensive products.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {94–98},
numpages = {5},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3546932.3547074,
author = {Dhungana, Deepak and Haselb\"{o}ck, Alois and Ruiz-Torrubiano, Rub\'{e}n and Wallner, Stefan},
title = {Variability of safety risks in production environments},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547074},
doi = {10.1145/3546932.3547074},
abstract = {One of the major steps between (re-)configuration of a factory and the start of production is the commissioning phase, where certification of safety requirements and assessment of potential hazards is a key activity. Typically, assessment of safety risks is a manual process that incorporates the experience and knowledge of the involved stakeholders. The flexibility and the speed gained by automated (re-)configuration of production environments is decelerated by the manual safety certification process, before the factory can start production. This paper is an attempt to eliminate this bottleneck by proposing a model-driven approach to safety risk assessment. An approach based on several models of safety risks enables potential safety risks to be "instantiated" for any given factory at hand. This shortens the recurring process of identifying the risks. A model-driven approach was chosen to capture and utilize the tacit knowledge of the involved stakeholders.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {178–187},
numpages = {10},
keywords = {safety certification, model-driven commissioning, hazards in production environments, automated identification of risks},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3461002.3473951,
author = {Morais Ferreira, David and Tenev, Vasil L. and Becker, Martin},
title = {Product-line analysis cookbook: a classification system for complex analysis toolchains},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473951},
doi = {10.1145/3461002.3473951},
abstract = {Adopting Product Line Engineering (PLE) approaches in the context of software-intensive systems reduces overall development and maintenance costs, reduces time to market and leads to an overall improvement in product quality. The Software and System Product Line (SPL) community has provided a large number of different analysis approaches and tools, which were developed in different contexts, answer different questions, and can contribute to the fulfillment of different analysis goals. Typically, these analysis tools are initially developed as part of a research study, where they serve a specific purpose, e. g. for investigating the use of a new technology, or to demonstrate the transfer of methods from other fields. Generally, such purpose is aligned with a specific, but not explicitly stated, high-level goal. The pursuit of these goals requires holistic approaches, i. e. integrated toolchains and classification of analyses, which are documented as a centralized collection of wisdom. Therefore, we propose a classification system which describes existing analyses and reveals possible combinations, i. e. integrated toolchains, and provide first examples. This method supports the search for toolchains which address complex industrial needs. With the support of the SPL community, we hope to collaboratively document existing analyses and corresponding goals on an open platform.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {99–104},
numpages = {6},
keywords = {reverse engineering, product-line aware analyses, product line engineering, holistic toolchain},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3503229.3547040,
author = {Allibe, Mathis and Sylla, Abdourahim and Alpan, G\"{u}lg\"{u}n},
title = {A generic knowledge model for resource reconfiguration in the context of reconfigurable manufacturing systems},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547040},
doi = {10.1145/3503229.3547040},
abstract = {In this article, in order to help manufacturers to better manage manufacturing resource reconfiguration in the context of reconfigurable manufacturing systems, we propose a generic knowledge-based model that can support resource reconfiguration decision-making while considering various manufacturing requirements and constraints. The model is based on Constraint Satisfaction Problem (CSP) framework. The two presented scenarios demonstrate that the application of a Knowledge-Based System (KBS) is a great opportunity to improve manufacturing systems' responsiveness.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {217–223},
numpages = {7},
keywords = {constraint satisfaction problem (CSP), knowledge modeling, knowledge-based system (KBS), reconfigurable manufacturing system (RMS), resource reconfiguration},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3461002.3473070,
author = {Acher, Mathieu and Perrouin, Gilles and Cordy, Maxime},
title = {BURST: a benchmarking platform for uniform random sampling techniques},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473070},
doi = {10.1145/3461002.3473070},
abstract = {We present BURST, a benchmarking platform for uniform random sampling techniques. With BURST, researchers have a flexible, controlled environment in which they can evaluate the scalability and uniformity of their sampling. BURST comes with an extensive --- and extensible --- benchmark dataset comprising 128 feature models, including challenging, real-world models of the Linux kernel. BURST takes as inputs a sampling tool, a set of feature models and a sampling budget. It automatically translates any feature model of the set in DIMACS and invokes the sampling tool to generate the budgeted number of samples. To evaluate the scalability of the sampling tool, BURST measures the time the tool needs to produce the requested sample. To evaluate the uniformity of the produced sample, BURST integrates the state-of-the-art and proven statistical test Barbarik. We envision BURST to become the starting point of a standardisation initiative of sampling tool evaluation. Given the huge interest of research for sampling algorithms and tools, this initiative would have the potential to reach and crosscut multiple research communities including AI, ML, SAT and SPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {36–40},
numpages = {5},
keywords = {variability model, software product lines, sampling, configurable systems, benchmark, SAT},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3546932.3547002,
author = {Schulze, Sandro and Kr\"{u}ger, Jacob and W\"{u}nsche, Johannes},
title = {Towards developer support for merging forked test cases},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547002},
doi = {10.1145/3546932.3547002},
abstract = {Developers rely on branching and forking mechanisms of modern versioning systems to evolve and maintain their software systems. As a result, systems often exist in the form of various short-living or even long-living (i.e., clone &amp; own development) variants. Such variants may have to be merged with the main system or other variants, for instance, to propagate features or bug fixes. Within such merging processes, test cases are highly interesting, since they allow to improve the test coverage and hopefully the reliability of the system (e.g., by merging missing tests and bug fixes in test code). However, as all source code, test cases may evolve independently between two or more variants, which makes it non-trivial to decide what changes of the test cases are relevant for the merging. For instance, some test cases in one variant may be irrelevant in another variant (e.g., because the feature shall not be propagated) or may subsume existing test cases. In this paper, we propose a technique that allows for a fine-grained comparison of test cases to support developers in deciding whether and how to merge these. Precisely, inspired by code-clone detection, we use abstract syntax trees to decide on the relations between test cases of different variants. We evaluate the applicability of our technique qualitatively on five open-source systems written in Java (e.g., JUnit 5, Guava). Our insights into the merge potential of 50 pull requests with test cases from these systems indicate that our technique can support the comprehension of differences in variants' test cases, and also highlight future research opportunities.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {131–141},
numpages = {11},
keywords = {variant-rich systems, test cases, merging, feature forks},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3336294.3342358,
author = {M\"{u}ller, Richard and Eisenecker, Ulrich},
title = {A Graph-Based Feature Location Approach Using Set Theory},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342358},
doi = {10.1145/3336294.3342358},
abstract = {The ArgoUML SPL benchmark addresses feature location in Software Product Lines (SPLs), where single features as well as feature combinations and feature negations have to be identified. We present a solution for this challenge using a graph-based approach and set theory. The results are promising. Set theory allows to exactly define which parts of feature locations can be computed and which precision and which recall can be achieved. This has to be complemented by a reliable identification of feature-dependent class and method traces as well as refinements. The application of our solution to one scenario of the benchmark supports this claim.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {88–92},
numpages = {5},
keywords = {static analysis, software product lines, set theory, reverse engineering, jQAssistant, graph database, feature location, extractive software product line adoption, cypher, benchmark, Neo4j, ArgoUML},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3109729.3109745,
author = {Markiegi, Urtzi},
title = {Test optimisation for Highly-Configurable Cyber-Physical Systems},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109745},
doi = {10.1145/3109729.3109745},
abstract = {Cyber-Physical Systems (CPS) have become one of the core-enabling technologies for multiple domains, such as manufacturing, healthcare, energy and transportation. Furthermore, these domains are demanding CPS to be highly-configurable in order to respond to multiple and changing market requirements. Testing these Highly-Configurable Cyber-Physical Systems (HCCPS) is challenging. First, when working with CPSs, considerable time is required in order to tackle physical processes during testing. And secondly, in highly-configurable systems, a large number of system variants need to be tested. Consequently, reducing HCCPS testing time is essential.In this context, a research work is presented to reduce the overall testing time of HCCPS, focusing on a merged strategy of product and test cases optimisation. In particular, two approaches are proposed in order to achieve the testing time reduction. The first approach aims to reduce the HCCPS testing time by an iterative allocation of products and test cases. The second approach aims to reduce the HCCPS testing time by a feedback driven dynamic and iterative allocation of products and test cases.A preliminary experiment has been undertaken to test the iterative allocation approach. In this experiment, products to be tested are selected and prioritised. Next, multiple testing iterations are perform until the time-budget is consumed. In each iteration a small number of test cases are allocated for each of the products to be tested. The experiment was evaluated with an academic HCCPS and preliminary results suggest that the proposed approach reduces the fault detection time when compared with traditional approaches.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {139–144},
numpages = {6},
keywords = {Software Engineering, Search-Based Software Engineering, Product Line Testing, Highly-Configurable Systems, Fault Detection, Cyber-Physical Systems},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2362536.2362559,
author = {Gillain, Joseph and Faulkner, Stephane and Heymans, Patrick and Jureta, Ivan and Snoeck, Monique},
title = {Product portfolio scope optimization based on features and goals},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362559},
doi = {10.1145/2362536.2362559},
abstract = {In this paper we propose a mathematical program able to optimize the product portfolio scope of a software product line and sketch both a development and a release planning. Our model is based on the description of customer needs in terms of goals. We show that this model can be instantiated in several contexts such as a market customization strategy or a mass-customization strategy. It can deal with Software Product Line development from scratch as well as starting from a legacy software base. We demonstrate its applicability with an example based on a case study.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {161–170},
numpages = {10},
keywords = {software product line, scoping optimization, release planning, product portfolio},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3233027.3236399,
author = {Kuiter, Elias and Krieter, Sebastian and Kr\"{u}ger, Jacob and Ludwig, Kai and Leich, Thomas and Saake, Gunter},
title = {PClocator: a tool suite to automatically identify configurations for code locations},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236399},
doi = {10.1145/3233027.3236399},
abstract = {The source code of highly-configurable software is challenging to comprehend, analyze, and test. In particular, it is hard to identify all configurations that comprise a certain code location. We contribute PCLocator, a tool suite that solves this problem by utilizing static analysis tools for compile-time variability. Using BusyBox and the Variability Bugs Database (VBDb), we evaluate the correctness and performance of PCLocator. The results show that we are able to analyze files in a matter of seconds and derive correct configurations in 95% of all cases.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {284–288},
numpages = {5},
keywords = {static source code analysis, software product line, preprocessor, configuration, build system},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3461001.3473061,
author = {Tomashchuk, Oleksandr and Van Landuyt, Dimitri and Joosen, Wouter},
title = {The architectural divergence problem in security and privacy of eHealth IoT product lines},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3473061},
doi = {10.1145/3461001.3473061},
abstract = {The Internet of Things (IoT) seamlessly becomes integrated into many aspects of daily life, and in the case of healthcare, it arises in the shape of eHealth IoT systems. Evidently, the design of such systems must apply best practices when it comes to security and privacy, in addition to ensuring compliance with various national and international regulations. When it comes to the required functionality, commonalities and variations can effectively be managed in a product line approach that involves deriving specific application architecture variants from a common reference architecture.This paper illustrates and discusses a specific problem encountered in the establishment of a software product-line in this specific context: the adoption of systematic security and privacy threat modeling and risk assessment approaches introduces a variation space that is very difficult to capture in a proactive product-line approach. One of the main causes for this is that threat assessment itself suffers from the problem of threat explosion, i.e. combinatorial explosions of threats that have to be investigated and systematically mitigated. The highlighted divergence of the security and privacy threats across architectural variants is illustrated in the specific case of an industry IoT-based e-health software product line.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {114–119},
numpages = {6},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3307630.3342413,
author = {Arcaini, Paolo and Gargantini, Angelo and Radavelli, Marco},
title = {A Process for Fault-Driven Repair of Constraints Among Features},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342413},
doi = {10.1145/3307630.3342413},
abstract = {The variability of a Software Product Line is usually both described in the problem space (by using a variability model) and in the solution space (i.e., the system implementation). If the two spaces are not aligned, wrong decisions can be done regarding the system configuration. In this work, we consider the case in which the variability model is not aligned with the solution space, and we propose an approach to automatically repair (possibly) faulty constraints in variability models. The approach takes as input a variability model and a set of combinations of features that trigger conformance faults between the model and the real system, and produces the repaired set of constraints as output. The approach consists of three major phases. First, it generates a test suite and identifies the condition triggering the faults. Then, it modifies the constraints of the variability model according to the type of faults. Lastly, it uses a logic minimization method to simplify the modified constraints. We evaluate the process on variability models of 7 applications of various sizes. An empirical analysis on these models shows that our approach can effectively repair constraints among features in an automated way.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {73–81},
numpages = {9},
keywords = {variability model, system evolution, fault, automatic repair},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3503229.3547039,
author = {Vandevelde, Simon and Callewaert, Benjamin and Vennekens, Joost},
title = {Interactive feature modeling with background knowledge for validation and configuration},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547039},
doi = {10.1145/3503229.3547039},
abstract = {Feature modeling enables a straightforward representation of a product's features, components, and the relations between them. In this way, feature models serve as an excellent approach to diagrammatically model a product design for manufacturing purposes. However, the actual usage of such a feature model to generate suitable designs in the context of real-life industry applications is often limited, as crucial background knowledge cannot be expressed. Moreover, even though expert validation of a feature model is an important aspect of its creation, current tooling often falls short on this aspect. Indeed, although state-of-the-art tools are capable of generating possible configurations, this is not sufficient to completely validate complex applications: instead, we should enable the expert to interactively explore the problem domain. In this paper, we present our feature modeling tool, called FM-IDP, which aims to overcome both of these shortcomings. In FM-IDP, background knowledge can be expressed in FO(·), a rich extension of classical first-order logic. Using an off-the-shelf logical reasoning engine and an integrated interactive configuration interface, modelers can interact with the feature model and its background knowledge to explore the problem space on-the-fly. We motivate our approach using an industrial use case focused on real-life component design.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {209–216},
numpages = {8},
keywords = {IDP, background knowledge, feature modeling, interactive configuration},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3382025.3414961,
author = {Favalli, Luca and K\"{u}hn, Thomas and Cazzola, Walter},
title = {Neverlang and FeatureIDE just married: integrated language product line development environment},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414961},
doi = {10.1145/3382025.3414961},
abstract = {Language development is inherently complex. With the support of a suitable language development environment most computer scientists could develop their own domain-specific language (DSL) with relative ease. Yet, when the DSL is the result of a configuration over a language product line (LPL)---a special software product line (SPL) of compilers/interpreters and corresponding IDE services---they fail to provide adequate support. An environment for LPL engineering should facilitate the underlying process involving three distinct roles: a language engineer developing the LPL, a language deployer configuring a language product, and a language user using the language product. Neither IDEs nor SPLE environments can cater all three roles and fully support the LPL engineering process with distributed, incremental development, configuration, and deployment of language variants. In this paper, we present an LPL engineering process for the distributed, incremental development of LPLs and an integrated language product line development environment supporting this process, catering the three roles, and ensuring the consistency among all artifacts of the LPL: language components implementing a language feature, the feature model, language configurations and the resulting language products. To create such an environment, we married the Neverlang language workbench and AiDE its LPL engineering environment with the FeatureIDE SPL engineering environment. While Neverlang supports the development of LPLs and deployment of language products, AiDE generates the feature model for the LPL under development, whereas FeatureIDE handles the feature configuration. We illustrate the applicability of the LPL engineering process and the suitability of our development environment for the three roles by showcasing its application for teaching programming with a growable language. In there, an LPL for Javascript was developed/refactored, 15 increasingly complex language products were configured/updated and finally deployed.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {33},
numpages = {11},
keywords = {neverlang, language product lines, domain specific languages},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/1297846.1297914,
author = {Krueger, Charles W.},
title = {Leveraging integrated model-driven development and software product line development technologies},
year = {2007},
isbn = {9781595938657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1297846.1297914},
doi = {10.1145/1297846.1297914},
abstract = {As emerging Software Product Line (SPL) technologies have evolved, Model-Driven Development (MDD) has remained an under-served part of the SPL portfolio development lifecycle, making it difficult to simultaneously leverage the benefits of both practices. The Telelogic Rhapsody®/BigLever Gears™ Bridge is the industry's first solution to provide fully integrated MDD and SPL technologies. With the Bridge's innovative capabilities, you can achieve new levels of efficiency by utilizing: (1) Rhapsody MDD models, rather than working with conventional source code, and (2) Gears' SPL consolidation, first-class model variation points, and automated production capabilities -- rather than creating "clone-and-own" copies of MDD models for each product or building "one-size-fits-all" models for all products. This increased efficiency enables you to deliver more new products and features faster, while reducing the development effort and optimizing product quality.},
booktitle = {Companion to the 22nd ACM SIGPLAN Conference on Object-Oriented Programming Systems and Applications Companion},
pages = {836–837},
numpages = {2},
keywords = {software product lines, model-driven development},
location = {Montreal, Quebec, Canada},
series = {OOPSLA '07}
}

@inproceedings{10.1145/3109729.3109758,
author = {Ben Snaiba, Ziad and de Vink, Erik P. and Willemse, Tim A.C.},
title = {Family-Based Model Checking of SPL based on mCRL2},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109758},
doi = {10.1145/3109729.3109758},
abstract = {We discuss how the general-purpose model checker mCRL2 can be used for family-based verification of behavioral properties of software product lines. This is achieved by exploiting a feature-oriented extension of the modal μ-calculus for the specification of SPL properties, and for its model checking by encoding it back into the logic of mCRL2. Using the example of the well-known minepump SPL an illustration of the possibilities of the approach is given.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {13–16},
numpages = {4},
keywords = {mCRL2, Software Product Lines, Family-based model checking},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/1176887.1176897,
author = {Yoshimura, Kentaro and Ganesan, Dharmalingam and Muthig, Dirk},
title = {Defining a strategy to introduce a software product line using existing embedded systems},
year = {2006},
isbn = {1595935428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1176887.1176897},
doi = {10.1145/1176887.1176897},
abstract = {Engine Control Systems (ECS) for automobiles have numerous variants for many manufactures and different markets. To improve development efficiency, exploiting ECS commonalities and predicting their variability are mandatory. The concept of software product line engineering meets the business background of ECS. However, we should carefully investigate the expected technical, economical, and organizational effects of introducing this strategy into existing products.This paper explains an approach for assessing the potential of merging existing embedded software into a product line approach. The definition of an economically useful product line approach requires two things: analyzing return on investment (ROI) expectations of a product line and understanding the effort required for building reusable assets. We did a clone analysis to provide the basis for effort estimation for merge potential assessment of existing variants. We also report on a case study with ECS. We package the lessons learned and open issues that arose during the case study.},
booktitle = {Proceedings of the 6th ACM &amp; IEEE International Conference on Embedded Software},
pages = {63–72},
numpages = {10},
keywords = {software product line, software, reverse rngineering, engine control systems, economics, clone detection and classification},
location = {Seoul, Korea},
series = {EMSOFT '06}
}

@inproceedings{10.1145/3382026.3425772,
author = {Ca\~{n}ete, Angel and Amor, Mercedes and Fuentes, Lidia},
title = {Supporting the evolution of applications deployed on edge-based infrastructures using multi-layer feature models},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3425772},
doi = {10.1145/3382026.3425772},
abstract = {The proliferation of cyber-physical systems has encouraged the emergence of new technologies and paradigms to improve the performance of IoT-based applications. Edge Computing proposes using the nearby devices in the frontier/Edge of the access network for deploying application tasks. However, the functionality of cyberphysical systems, which is usually distributed in several devices and computers, imposes specific requirements on the infrastructure to run properly. The evolution of an application to meet new user requirements and the high diversity of hardware and software technologies in the edge can complicate the deployment of evolved applications.The aim of our approach is to apply Multi Layer Feature Models, which capture the variability of applications and the infrastructure, to support the deployment in edge-based environments of cyber-physical systems applications. This separation can support the evolution of application and infrastructure. Considering that IoT/Edge/Cloud infrastructures are usually shared by many applications, the SPL deployment process has to assure that there will be enough resources for all of them, informing developers about the alternatives of deployment. Prior to its deployment and leaning on the infrastructure feature models, the developer can calculate what is the configuration of minimal set of devices supporting application requirements of the evolved application. In addition, the developer can find which is the application configuration that can be hosted in the current evolved infrastructure.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {79–87},
numpages = {9},
keywords = {Software Product Line, Software Evolution, Multi Layer Feature Models, Internet of Things, Edge Computing},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3106195.3106205,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green Configurations of Functional Quality Attributes},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106205},
doi = {10.1145/3106195.3106205},
abstract = {Functional quality attributes (FQAs) are those quality attributes that, to be satisfied, require the incorporation of additional functionality into the application architecture. By adding an FQA (e.g., security) we can improve the quality of the final product, but there is also an increase in energy consumption. This paper proposes a solution to help the software architect to generate configurations of FQAs whilst keeping the energy consumed by the application as low as possible. For this, a usage model is defined for each FQA, taking into account the variables that affect the energy consumption, and that the values of these variables change according to the part of the application where the FQA is required. We extend a Software Product Line that models a family of FQAs to incorporate the variability of the usage model and the existing frameworks that implement FQAs. We generate the most eco-efficient configuration of FQAs by selecting the framework with the most suitable characteristics according to the requirements of the application.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {79–83},
numpages = {5},
keywords = {Variability, SPL, Quality Attributes, FQA, Energy Consumption},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3503229.3547068,
author = {Plappert, Stefan and Teves, Simon and \"{O}zt\"{u}rk, Mevali and Gembarski, Paul Christoph},
title = {Constraint solver for a fixture design: results of a student case study},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547068},
doi = {10.1145/3503229.3547068},
abstract = {For teaching students the skills of programming and usage of knowledge-based engineering systems, we conduct student projects in a lecture in which they independently represent a configuration solution space and resolve it using a constraint solver. For this purpose, the lecture is conducted in a flipped classroom concept to not only teach the students the theoretical basics but to enable them to independently formulate and integrate design problems, which can be abstracted as configuration problems, so that they develop a sustainable competence through learning-by-doing. The configuration problem of the student case study represented here is the positioning of a cast part for manufacturing, where the positioning is done via three subassemblies consisting of parts from a fixture toolbox. For this purpose, a development environment written in the Python programming language was set up, which uses an external Excel database as a knowledge base to provide the sizes of the fixture elements. Through a graphical user interface, the designer can specify how the fixture should be used so that the constraint solver can find a solution. If there are several possible solutions, an optimization loop is executed so that the designer can be given a clear recommendation. An interface to the CAD program Autodesk Inventor offers the possibility to build the fixture assembly of the selected solution from parameterized CAD models of the components by linking their custom coordinate systems. To reduce computing time, a case base is also provided for configurations that have already been created, so that existing subassemblies can be used if the same or similar configuration problem arises.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {237–244},
numpages = {8},
keywords = {case study, constraint solver, fixture design, knowledge-based engineering system, product configuration},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3336294.3336317,
author = {Duszynski, Slawomir and Dhar, Saura Jyoti and Beichter, Tobias},
title = {Using Relation Graphs for Improved Understanding of Feature Models in Software Product Lines},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336317},
doi = {10.1145/3336294.3336317},
abstract = {Feature models are widely used for describing the variability of a software product line. A feature model contains a tree of features and a set of constraints over these features, which define valid feature combinations. In the industrial practice, large feature models containing hundreds of features and constraints are common. Furthermore, in a hierarchical product line a feature model can be related to other feature models through inter-model constraints. Due to the model size and complexity, understanding industrial feature models is a challenging task.In this paper, we describe the feature model understanding challenges reported by feature model developers at Robert Bosch GmbH. To support the developers in model understanding, we extend the idea of a feature implication graph to feature relation graph by abstracting groups of implications to feature relations. A transitively closed relation graph shows all modeled and implicit feature relations and spans all related feature models. The graph is also used to identify modeling problems, such as false optional or dead features, and to show the derivation of any implicit relation or problem from the modeled constraints. In a case study at Bosch, we evaluate the use of feature relation graph for model understanding. We propose further use cases of the graph, supporting model maintenance, evolution and configuration.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {309–319},
numpages = {11},
keywords = {model understanding, implication graph, feature model},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3461002.3473946,
author = {Meixner, Kristof and Feichtinger, Kevin and Rabiser, Rick and Biffl, Stefan},
title = {A reusable set of real-world product line case studies for comparing variability models in research and practice},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473946},
doi = {10.1145/3461002.3473946},
abstract = {Real-world cases describing (product) variability in production systems are rare and often not accessible. Thus, researchers often use toy examples or develop fictitious case studies. These are designed to demonstrate their approach but rarely to compare multiple approaches. In this paper, we aim at making variability modeling evaluations comparable. We present and provide a reusable set of four real-world case studies that are easy to access, with artifacts represented in a universal, variability-model-agnostic way, the industrial Product-Process-Resource Domain-Specific Language (PPR DSL). We report how researchers can use the case studies, automatically transforming the Domain-Specific Language (DSL) artifacts to well-known variability models, e.g., product feature models, using the Variability Evolution Roundtrip Transformation (VERT) process. We compare the expressiveness and complexity of the transformed feature models. We argue that the case studies with the DSL and the flexible transformation capabilities build a valuable contribution to making future research results more comparable and facilitating evaluations with real-world product lines.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {105–112},
numpages = {8},
keywords = {variability modeling, feature extraction, cyber-physical production system, case studies},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3336294.3336303,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Galindo, Jos\'{e} A. and Ramos-Guti\'{e}rrez, Bel\'{e}n and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Benavides, David},
title = {Process Mining to Unleash Variability Management: Discovering Configuration Workflows Using Logs},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336303},
doi = {10.1145/3336294.3336303},
abstract = {Variability models are used to build configurators. Configurators are programs that guide users through the configuration process to reach a desired configuration that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the elements that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suites stakeholders according to previous configurations. For example, when configuring a Linux distribution, the configuration process start by choosing the network or the graphic card, and then other packages with respect to a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), an automated technique that given a set of logs of previous configurations and a variability model can automatically assist to determine the configuration workflow that better fits the configuration logs generated by user activities. The technique is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Our proposal is validated using existing data from an ERP configuration environment showing its feasibility. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {265–276},
numpages = {12},
keywords = {variability, process mining, process discovery, configuration workflow, clustering},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3382025.3414955,
author = {Ananieva, Sofia and Greiner, Sandra and K\"{u}hn, Thomas and Kr\"{u}ger, Jacob and Linsbauer, Lukas and Gr\"{u}ner, Sten and Kehrer, Timo and Klare, Heiko and Koziolek, Anne and L\"{o}nn, Henrik and Krieter, Sebastian and Seidl, Christoph and Ramesh, S. and Reussner, Ralf and Westfechtel, Bernhard},
title = {A conceptual model for unifying variability in space and time},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414955},
doi = {10.1145/3382025.3414955},
abstract = {Software engineering faces the challenge of developing and maintaining systems that are highly variable in space (concurrent variations of the system at a single point in time) and time (sequential variations of the system due to its evolution). Recent research aims to address this need by managing variability in space and time simultaneously. However, such research often relies on nonuniform terminologies and a varying understanding of concepts, as it originates from different communities: software product-line engineering and software configuration management. These issues complicate the communication and comprehension of the concepts involved, impeding the development of techniques to unify variability in space and time. To tackle this problem, we performed an iterative, expert-driven analysis of existing tools to derive the first conceptual model that integrates and unifies terminologies and concepts of both dimensions of variability. In this paper, we present the unification process of concepts for variability in space and time, and the resulting conceptual model itself. We show that the conceptual model achieves high coverage and that its concepts are of appropriate granularity with respect to the tools for managing variability in space, time, or both that we considered. The conceptual model provides a well-defined, uniform terminology that empowers researchers and developers to compare their work, clarifies communication, and prevents redundant developments.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {15},
numpages = {12},
keywords = {version control, variability, revision management, product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3336294.3342376,
author = {Lanna, Andre and Castro, Thiago and Alves, Vander and Rodrigues, Genaina and Schobbens, Pierre-Yves and Apel, Sven},
title = {Feature-Family-Based Reliability Analysis of Software Product Lines},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342376},
doi = {10.1145/3336294.3342376},
abstract = {Context: Verification techniques such as model checking are being applied to ensure that software systems achieve desired quality levels and fulfill their functional and non-functional specification. However, applying these techniques to software product lines is a twofold challenge, given the exponential blowup of the number of products and the state-explosion problem inherent to model checking. Current product-line verification techniques leverage symbolic model checking and variability information to optimize the analysis but still face limitations that make them costly or infeasible. In particular, state-of-the-art verification techniques for product-line reliability analysis are enumerative which hinders their applicability, given the latent blowup of the configuration space.Objective: Our objectives are the following: (a) we present a method to efficiently compute the reliability of all configurations of a compositional or annotation-based software product line from its UML behavioral models, (b) we provide a tool that implements the proposed method, and (c) we report on an empirical study comparing the performance of different reliability analysis strategies for software product lines.Method: We present a novel feature-family-based analysis strategy to compute the reliability of all products of a (compositional or annotation-based) software product line. The strategy employs a divide-and-conquer approach over UML behavioral models endowed with probabilistic and variability information. The feature-based step of our strategy divides the behavioral models into smaller feature-dependent fragments that can be analyzed more efficiently. Such analysis consists of creating a probabilistic model for each behavioral fragment and analyzing such model using a parametric model checker that returns an expression denoting its reliability. Parameters in such expression represent the reliabilities of fragments on which it depends at runtime. The family-based step performs the reliability computation for all configurations at once (conquer) by evaluating reliability expressions in terms of a suitable variational data structure. This step solves the expression computed for each behavioral fragment taking into account (a) the fragment's variability information and (b) the reliability values already computed for the fragments on which it depends. The result is an Algebraic Decision Diagram (ADD) whose terminals different than zero represent the reliability value of valid (partial) configurations for the fragment. Therefore, the ADD computed for the last evaluated fragment contains the reliability values for all valid configurations of the software product line.Results: We performed an experiment to compare our feature-family-based and other four state-of-the-art evaluation strategies (product-based, family-based, feature-product-based and family-product-based). The subjects were variations of six publicly available product lines, whose configuration spaces were progressively increased. The empirical results show that our feature-family-based strategy outperforms, in terms of time and space, the other four state-of-the-art strategies. In addition, it is the only one that could be scaled to a 220-fold increase in the size of the configuration space.Conclusion: Our feature-family-based strategy leverages both feature-based and family-based strategies by taming the size of the models to be analyzed (due to the decomposition of behavioral models into fragments) and by avoiding the products enumeration inherent to some state-of-the-art analysis methods by using ADDs to represent both variability and reliability values.Journal paper: This paper was published at the Information and Software Technology Journal. It is available at https://doi.org/10.1016/j.infsof.2017.10.001.Supplementary material: Additional material to the IST submission is available at https://splmc.github.io/scalabilityAnalysis/. This material comprises experiments data, the tool implementing the feature-family-based reliability analysis strategy and the environment for experiment replication.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {64},
numpages = {1},
keywords = {software reliability analysis, software product lines, parametric verification},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3382025.3414970,
author = {Kr\"{u}ger, Jacob and Mahmood, Wardah and Berger, Thorsten},
title = {Promote-pl: a round-trip engineering process model for adopting and evolving product lines},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414970},
doi = {10.1145/3382025.3414970},
abstract = {Process models for software product-line engineering focus on proactive adoption scenarios---that is, building product-line platforms from scratch. They comprise the two phases domain engineering (building a product-line platform) and application engineering (building individual variants), each of which defines various development activities. Established more than two decades ago, these process models are still the de-facto standard for steering the engineering of platforms and variants. However, observations from industrial and open-source practice indicate that the separation between domain and application engineering, with their respective activities, does not fully reflect reality. For instance, organizations rarely build platforms from scratch, but start with developing individual variants that are re-engineered into a platform when the need arises. Organizations also appear to evolve platforms by evolving individual variants, and they use contemporary development activities aligned with technical advances. Recognizing this discrepancy, we present an updated process model for engineering software product lines. We employ a method for constructing process theories, building on recent literature as well as our experiences with industrial partners to identify development activities and the orders in which these are performed. Based on these activities, we synthesize and discuss the new process model, called promote-pl. Also, we explain its relation to modern software-engineering practices, such as continuous integration, model-driven engineering, or simulation testing. We hope that our work offers contemporary guidance for product-line engineers developing and evolving platforms, and inspires researchers to build novel methods and tools aligned with current practice.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {2},
numpages = {12},
keywords = {software reuse, round-trip engineering, process model},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2362536.2362548,
author = {Soltani, Samaneh and Asadi, Mohsen and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Bagheri, Ebrahim},
title = {Automated planning for feature model configuration based on functional and non-functional requirements},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362548},
doi = {10.1145/2362536.2362548},
abstract = {Feature modeling is one of the main techniques used in Software Product Line Engineering to manage the variability within the products of a family. Concrete products of the family can be generated through a configuration process. The configuration process selects and/or removes features from the feature model according to the stakeholders' requirements. Selecting the right set of features for one product from amongst all of the available features in the feature model is a complex task because: 1) the multiplicity of stakeholders' functional requirements; 2) the positive or negative impact of features on non-functional properties; and 3) the stakeholders' preferences w.r.t. the desirable non-functional properties of the final product. Many configurations techniques have already been proposed to facilitate automated product derivation. However, most of the current proposals are not designed to consider stakeholders' preferences and constraints especially with regard to non-functional properties. We address the software product line configuration problem and propose a framework, which employs an artificial intelligence planning technique to automatically select suitable features that satisfy both the stakeholders' functional and non-functional preferences and constraints. We also provide tooling support to facilitate the use of our framework. Our experiments show that despite the complexity involved with the simultaneous consideration of both functional and non-functional properties our configuration technique is scalable.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {56–65},
numpages = {10},
keywords = {software product line engineering, planning techniques, feature model, configuration, artificial intelligence},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2362536.2362562,
author = {Mohalik, Swarup and Ramesh, S. and Millo, Jean-Vivien and Krishna, Shankara Narayanan and Narwane, Ganesh Khandu},
title = {Tracing SPLs precisely and efficiently},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362562},
doi = {10.1145/2362536.2362562},
abstract = {In a Software Product Line (SPL), the central notion of implementability provides the requisite connection between specifications (feature sets) and their implementations (component sets), leading to the definition of products. While it appears to be a simple extension (to sets) of the trace-ability relation between components and features, it actually involves several subtle issues which are overlooked in the definitions in existing literature. In this paper, we give a precise and formal definition of implementability over a fairly expressive traceability relation to solve these issues. The consequent definition of products in the given SPL naturally entails a set of useful analysis problems that are either refinements of known problems, or are completely novel. We also propose a new approach to solve these analysis problems by encoding them as Quantified Boolean Formula(QBF) and solving them through Quantified Satisfiability (QSAT) solvers. The methodology scales much better than the SAT-based solutions hinted in the literature and is demonstrated through a prototype tool called SPLANE (SPL Analysis Engine), on a couple of fairly large case studies.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {186–195},
numpages = {10},
keywords = {software product line, formal methods, feature model, QSAT},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3461002.3473073,
author = {Pett, Tobias and Krieter, Sebastian and Th\"{u}m, Thomas and Lochau, Malte and Schaefer, Ina},
title = {AutoSMP: an evaluation platform for sampling algorithms},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473073},
doi = {10.1145/3461002.3473073},
abstract = {Testing configurable systems is a challenging task due to the combinatorial explosion problem. Sampling is a promising approach to reduce the testing effort for product-based systems by finding a small but still representative subset (i.e., a sample) of all configurations for testing. The quality of a generated sample wrt. evaluation criteria such as run time of sample generation, feature coverage, sample size, and sampling stability depends on the subject systems and the sampling algorithm. Choosing the right sampling algorithm for practical applications is challenging because each sampling algorithm fulfills the evaluation criteria to a different degree. Researchers keep developing new sampling algorithms with improved performance or unique properties to satisfy application-specific requirements. Comparing sampling algorithms is therefore a necessary task for researchers. However, this task needs a lot of effort because of missing accessibility of existing algorithm implementations and benchmarks. Our platform AutoSMP eases practitioners and researchers lifes by automatically executing sampling algorithms on predefined benchmarks and evaluating the sampling results wrt. specific user requirements. In this paper, we introduce the open-source application of AutoSMP and a set of predefined benchmarks as well as a set of T-wise sampling algorithms as examples.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {41–44},
numpages = {4},
keywords = {sampling evalutaion, sampling, product lines},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3382025.3414962,
author = {Chrszon, Philipp and Baier, Christel and Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha},
title = {From features to roles},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414962},
doi = {10.1145/3382025.3414962},
abstract = {The detection of interactions is a challenging task present in almost all stages of software development. In feature-oriented system design, this task is mainly investigated for interactions of features within a single system, detected by their emergent behaviors. We propose a formalism to describe interactions in hierarchies of feature-oriented systems (hierarchical interactions) and the actual situations where features interact (active interplays). Based on the observation that such interactions are also crucial in role-based systems, we introduce a compositional modeling framework based on concepts and notions of roles, comprising role-based automata (RBAs). To describe RBAs, we present a modeling language that is close to the input language of the probabilistic model checker Prism. To exemplify the use of RBAs, we implemented a tool that translates RBA models into Prism and thus enables the formal analysis of functional and non-functional properties including system dynamics, contextual changes, and interactions. We carry out two case studies as a proof of concept of such analyses: First, a peer-to-peer protocol case study illustrates how undesired hierarchical interactions can be discovered automatically. Second, a case study on a self-adaptive production cell demonstrates how undesired interactions influence quality-of-service measures such as reliability and throughput.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {19},
numpages = {11},
keywords = {verification, roles, formal methods, feature-oriented systems},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3503229.3547067,
author = {Martinez, Jabier and Str\"{u}ber, Daniel and Horcas, Jose Miguel and Burdusel, Alexandru and Zschaler, Steffen},
title = {Acapulco: an extensible tool for identifying optimal and consistent feature model configurations},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547067},
doi = {10.1145/3503229.3547067},
abstract = {Configuring feature-oriented variability-rich systems is complex because of the large number of features and, potentially, the lack of visibility of the implications on quality attributes when selecting certain features. We present Acapulco as an alternative to the existing tools for automating the configuration process with a focus on mono- and multi-criteria optimization. The soundness of the tool has been proven in a previous publication comparing it to SATIBEA and MODAGAME. The main advantage was obtained through consistency-preserving configuration operators (CPCOs) that guarantee the validity of the configurations during the IBEA genetic algorithm evolution process. We present a new version of Acapulco built on top of FeatureIDE, extensible through the easy integration of objective functions, providing pre-defined reusable objectives, and being able to handle complex feature model constraints.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {50–53},
numpages = {4},
keywords = {genetic algorithms, software product lines, variability management},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/1094855.1094946,
author = {Liu, Shih-Hsi},
title = {A software product line architecture for distributed real-time and embedded systems: a separation of concerns approach},
year = {2005},
isbn = {1595931937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1094855.1094946},
doi = {10.1145/1094855.1094946},
abstract = {This paper presents a separation of concerns approach to solve the tangling problem of functional and Quality of Service (QoS) concerns in traditional Component-based Software Engineering (CBSE) and Software Product Line (SPL) technologies applied to Distributed Real-time and Embedded (DRE) systems. This problem originates from the interchangeability for fulfilling functional and QoS concerns during composition. The approach utilizes the perspective of QoS to design and analyze a set of software systems represented by a collection of QoS systemic paths, which determine how well functional tasks perform in terms of flows of application-specific and functionality-determined information between components. Our approach not only reserves the virtues of reusability, changeability, productivity and expeditiousness that traditional CBSE and SPL technologies possess, but also dedicates the contributions in terms of separation of concerns, design space exploration, fine-grained commonality and reusability evaluation and less subjective feasibility analyses for a component-based SPL.},
booktitle = {Companion to the 20th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {224–225},
numpages = {2},
keywords = {two-level grammar++, software product line architecture, real-time, quality of service, UniFrame},
location = {San Diego, CA, USA},
series = {OOPSLA '05}
}

@inproceedings{10.1145/3233027.3233030,
author = {Weckesser, Markus and Kluge, Roland and Pfannem\"{u}ller, Martin and Matth\'{e}, Michael and Sch\"{u}rr, Andy and Becker, Christian},
title = {Optimal reconfiguration of dynamic software product lines based on performance-influence models},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233030},
doi = {10.1145/3233027.3233030},
abstract = {Today's adaptive software systems (i) are often highly configurable product lines, exhibiting hundreds of potentially conflicting configuration options; (ii) are context dependent, forcing the system to reconfigure to ever-changing contextual situations at runtime; (iii) need to fulfill context-dependent performance goals by optimizing measurable nonfunctional properties. Usually, a large number of consistent configurations exists for a given context, and each consistent configuration may perform differently with regard to the current context and performance goal(s). Therefore, it is crucial to consider nonfunctional properties for identifying an appropriate configuration. Existing black-box approaches for estimating the performance of configurations provide no means for determining context-sensitive reconfiguration decisions at runtime that are both consistent and optimal, and hardly allow for combining multiple context-dependent quality goals. In this paper, we propose a comprehensive approach based on Dynamic Software Product Lines (DSPL) for obtaining consistent and optimal reconfiguration decisions. We use training data obtained from simulations to learn performance-influence models. A novel integrated runtime representation captures both consistency properties and the learned performance-influence models. Our solution provides the flexibility to define multiple context-dependent performance goals. We have implemented our approach as a standalone component. Based on an Internet-of-Things case study using adaptive wireless sensor networks, we evaluate our approach with regard to effectiveness, efficiency, and applicability.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {98–109},
numpages = {12},
keywords = {performance-influence models, machine learning, dynamic software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3382026.3425770,
author = {Feichtinger, Kevin and Meixner, Kristof and Rabiser, Rick and Biffl, Stefan},
title = {Variability Transformation from Industrial Engineering Artifacts: An Example in the Cyber-Physical Production Systems Domain},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3425770},
doi = {10.1145/3382026.3425770},
abstract = {Many variability modeling approaches have been proposed to explicitly represent the commonalities and variability in (software) product lines. Unfortunately, practitioners in industry still develop custom solutions to manage variability of various artifacts, like requirements documents or design spreadsheets. These custom-developed variability representations often miss important variability information, e.g., information required to assemble production goods. In this paper, we introduce the Variability Evolution Roundtrip Transformation (VERT) process. The process enables practitioners from the Cyber-Physical Production Systems domain to transform custom-developed engineering variability artifacts to a feature model, evolve and optimize the model, and transform it back to the original engineering artifacts. We build on an existing transformation approach for variability models and show the feasibility of the process using a real-world use case from an industry partner. We report on an initial feasibility study conducted with our industry partners' domain experts and on lessons learned regarding variability transformation of engineering variability artifacts.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {65–73},
numpages = {9},
keywords = {Variability Modeling, Variability Evolution, Feature Extraction, Cyber-Physical Production System Engineering, CPPS},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3461001.3471146,
author = {Horcas, Jose-Miguel and Galindo, Jos\'{e} A. and Heradio, Ruben and Fernandez-Amoros, David and Benavides, David},
title = {Monte Carlo tree search for feature model analyses: a general framework for decision-making},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471146},
doi = {10.1145/3461001.3471146},
abstract = {The colossal solution spaces of most configurable systems make intractable their exhaustive exploration. Accordingly, relevant analyses remain open research problems. There exist analyses alternatives such as SAT solving or constraint programming. However, none of them have explored simulation-based methods. Monte Carlo-based decision making is a simulation-based method for dealing with colossal solution spaces using randomness. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo methods, which have proven to succeed in vast search spaces (e.g., game theory). Our general framework is described formally, and its flexibility to cope with a diversity of analysis problems is discussed (e.g., finding defective configurations, feature model reverse engineering or getting optimal performance configurations). Additionally, we present a Python implementation of the framework that shows the feasibility of our proposal. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state of the art a step forward.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {190–201},
numpages = {12},
keywords = {variability modeling, software product lines, monte carlo tree search, feature models, configurable systems},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461002.3473066,
author = {Fortz, Sophie},
title = {LIFTS: learning featured transition systems},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473066},
doi = {10.1145/3461002.3473066},
abstract = {This PhD project aims to automatically learn transition systems capturing the behaviour of a whole family of software-based systems. Reasoning at the family level yields important economies of scale and quality improvements for a broad range of systems such as software product lines, adaptive and configurable systems. Yet, to fully benefit from the above advantages, a model of the system family's behaviour is necessary. Such a model is often prohibitively expensive to create manually due to the number of variants. For large long-lived systems with outdated specifications or for systems that continuously adapt, the modelling cost is even higher. Therefore, this PhD proposes to automate the learning of such models from existing artefacts. To advance research at a fundamental level, our learning target are Featured Transition Systems (FTS), an abstract formalism that can be used to provide a pivot semantics to a range of variability-aware state-based modelling languages. The main research questions addressed by this PhD project are: (1) Can we learn variability-aware models efficiently? (2) Can we learn FTS in a black-box fashion? (i.e., with access to execution logs but not to source code); (3) Can we learn FTS in a white/grey-box testing fashion? (i.e., with access to source code); and (4) How do the proposed techniques scale in practice?},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {1–6},
numpages = {6},
keywords = {variability mining, software product lines, model learning, featured transition systems, active automata learning},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1109/ASE.2015.106,
author = {Pietsch, Christopher and Kehrer, Timo and Kelter, Udo and Reuling, Dennis and Ohrndorf, Manuel},
title = {SiPL: a delta-based modeling framework for software product line engineering},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.106},
doi = {10.1109/ASE.2015.106},
abstract = {Model-based development has become a widely-used approach to implement software, e.g. for embedded systems. Models replace source code as primary executable artifacts in these cases. Software product line technologies for these domains must be able to generate models as instances of an SPL. This need is addressed among others by an implementation technology for SPLs known as delta modeling. Current approaches to delta modeling require deltas to be written manually using delta languages, and they offer only very limited support for creating and testing a network of deltas. This paper presents a new approach to delta modeling and a supporting tool suite: the abstract notion of a delta is refined to be a consistency-preserving edit script which is generated by comparing two models. The rich structure of edit scripts allows us to detect conflicts and further relations between deltas statically and to implement restructurings in delta sets such as the merging of two deltas. We illustrate the tooling using a case study.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {852–857},
numpages = {6},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1145/3307630.3342411,
author = {Meixner, Kristof and Rabiser, Rick and Biffl, Stefan},
title = {Towards Modeling Variability of Products, Processes and Resources in Cyber-Physical Production Systems Engineering},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342411},
doi = {10.1145/3307630.3342411},
abstract = {Planning and developing Cyber-Physical Production Systems (CPPS) are multi-disciplinary engineering activities that rely on effective and efficient knowledge exchange for better collaboration between engineers of different disciplines. The Product-Process-Resource (PPR) approach allows modeling products produced by industrial processes using specific production resources. In practice, a CPPS manufactures a portfolio of product type variants, i.e., a product line. Therefore, engineers need to create and maintain several PPR models to cover PPR variants and their evolving versions. In this paper, we detail a representative use case, identify challenges for using Variability Modeling (VM) methods to describe and manage PPR variants, and present a first solution approach based on cooperation with domain experts at an industry partner, a system integrator of automation for high-performance CPPS. We conclude that integrating basic variability concepts into PPR models is a promising first step and describe our further research plans to support PPR VM in CPPS.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {49–56},
numpages = {8},
keywords = {variability modelling, product-process-resource, cyber-physical production system},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3503229.3547030,
author = {Baranov, Eduard and Legay, Axel},
title = {Baital: an adaptive weighted sampling platform for configurable systems},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547030},
doi = {10.1145/3503229.3547030},
abstract = {The diversity of software application scenarios has led the evolution towards highly configurable systems. Testing of such systems is challenging due to an immense number of configurations and is usually performed on a small sample set. Sampling is a promising approach for the sample set generation. t-wise coverage is often used to measure the quality of sample sets. Uniform sampling being most known method can fail to achieve high coverage in presence of complex constraints on configurations. Another challenge is a scalability hurdle for the t-wise coverage computation leaving sampling for higher values of t unexplored.In this work, we present Baital, a platform that combines two novel techniques for sampling of configurable systems. It is based on the adaptive weighted sampling approach to generate sample sets with high t-wise coverage. The approximation techniques for the t-wise coverage computation allow the consideration of higher values of t; they improve scalability for both t-wise coverage computation and sampling process.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {46–49},
numpages = {4},
keywords = {adaptive weighted sampling, configurable systems, t-wise coverage},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/2934466.2934473,
author = {Olaechea, Rafael and Fahrenberg, Uli and Atlee, Joanne M. and Legay, Axel},
title = {Long-term average cost in featured transition systems},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934473},
doi = {10.1145/2934466.2934473},
abstract = {A software product line is a family of software products that share a common set of mandatory features and whose individual products are differentiated by their variable (optional or alternative) features. Family-based analysis of software product lines takes as input a single model of a complete product line and analyzes all its products at the same time. As the number of products in a software product line may be large, this is generally preferable to analyzing each product on its own. Family-based analysis, however, requires that standard algorithms be adapted to accomodate variability.In this paper we adapt the standard algorithm for computing limit average cost of a weighted transition system to software product lines. Limit average is a useful and popular measure for the long-term average behavior of a quality attribute such as performance or energy consumption, but has hitherto not been available for family-based analysis of software product lines. Our algorithm operates on weighted featured transition systems, at a symbolic level, and computes limit average cost for all products in a software product line at the same time. We have implemented the algorithm and evaluated it on several examples.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {109–118},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2491627.2491637,
author = {Kanda, Tetsuya and Ishio, Takashi and Inoue, Katsuro},
title = {Extraction of product evolution tree from source code of product variants},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491637},
doi = {10.1145/2491627.2491637},
abstract = {A large number of software products may be derived from an original single product. Although software product line engineering is advocated as an effective approach to maintaining such a family of products, re-engineering existing products requires developers to understand the evolution history of the products. This can be challenging because developers typically only have access to product source code. In this research, we propose to extract a Product Evolution Tree that approximates the evolution history from source code of products. Our key idea is that two successive products are the most similar to one another in the evolution history. We construct a Product Evolution Tree as a minimum spanning tree whose cost function is defined by the number of similar files between products. As an experiment, we extracted Product Evolution Trees from 6 datasets of open-source projects. The result showed that 53% to 92% of edges in the extracted trees were consistent with the actual evolution history of the projects.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {141–150},
numpages = {10},
keywords = {visualization, software product line, software evolution},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3336294.3336302,
author = {Str\"{u}ber, Daniel and Mukelabai, Mukelabai and Kr\"{u}ger, Jacob and Fischer, Stefan and Linsbauer, Lukas and Martinez, Jabier and Berger, Thorsten},
title = {Facing the Truth: Benchmarking the Techniques for the Evolution of Variant-Rich Systems},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336302},
doi = {10.1145/3336294.3336302},
abstract = {The evolution of variant-rich systems is a challenging task. To support developers, the research community has proposed a range of different techniques over the last decades. However, many techniques have not been adopted in practice so far. To advance such techniques and to support their adoption, it is crucial to evaluate them against realistic baselines, ideally in the form of generally accessible benchmarks. To this end, we need to improve our empirical understanding of typical evolution scenarios for variant-rich systems and their relevance for benchmarking. In this paper, we establish eleven evolution scenarios in which benchmarks would be beneficial. Our scenarios cover typical lifecycles of variant-rich system, ranging from clone &amp; own to adopting and evolving a configurable product-line platform. For each scenario, we formulate benchmarking requirements and assess its clarity and relevance via a survey with experts in variant-rich systems and software evolution. We also surveyed the existing benchmarking landscape, identifying synergies and gaps. We observed that most scenarios, despite being perceived as important by experts, are only partially or not at all supported by existing benchmarks-a call to arms for building community benchmarks upon our requirements. We hope that our work raises awareness for benchmarking as a means to advance techniques for evolving variant-rich systems, and that it will lead to a benchmarking initiative in our community.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {177–188},
numpages = {12},
keywords = {software variability, software evolution, product lines, benchmark},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/1217935.1217955,
author = {Krishna, Arvind S. and Gokhale, Aniruddha S. and Schmidt, Douglas C.},
title = {Context-specific middleware specialization techniques for optimizing software product-line architectures},
year = {2006},
isbn = {1595933220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1217935.1217955},
doi = {10.1145/1217935.1217955},
abstract = {Product-line architectures (PLAs) are an emerging paradigm for developing software families for distributed real-time and embedded (DRE) systems by customizing reusable artifacts, rather than hand-crafting software from scratch. To reduce the effort of developing software PLAs and product variants for DRE systems, developers are applying general-purpose -- ideally standard -- middleware platforms whose reusable services and mechanisms support a range of application quality of service (QoS) requirements, such as low latency and jitter. The generality and flexibility of standard middleware, however, often results in excessive time/space overhead for DRE systems, due to lack of optimizations tailored to meet the specific QoS requirements of different product variants in a PLA.This paper provides the following contributions to the study of middleware specialization techniques for PLA-based DRE systems. First, we identify key dimensions of generality in standard middleware stemming from framework implementations, deployment platforms, and middleware standards. Second, we illustrate how context-specific specialization techniques can be automated and used to tailor standard middleware to better meet the QoS needs of different PLA product variants. Third, we quantify the benefits of applying automated tools to specialize a standard Realtime CORBA middleware implementation. When applied together, these middleware specializations improved our application product variant throughput by ~65%, average- and worst-case end-to-end latency measures by ~43% and ~45%, respectively, and predictability by a factor of two over an already optimized middleware implementation, with little or no effect on portability, standard middleware APIs, or application software implementations, and interoperability.},
booktitle = {Proceedings of the 1st ACM SIGOPS/EuroSys European Conference on Computer Systems 2006},
pages = {205–218},
numpages = {14},
keywords = {specializations, product lines, middleware},
location = {Leuven, Belgium},
series = {EuroSys '06}
}

@inproceedings{10.1145/3382026.3431249,
author = {Lewellen, Stephanie},
title = {Identifying Key Stakeholders as Part of Requirements Elicitation in Software Ecosystems},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431249},
doi = {10.1145/3382026.3431249},
abstract = {Software ecosystems (SECOs) involve a number of actors that work together for a shared market. The software products within the software ecosystem typically have a common technological platform, and consist of a keystone player at the center of the ecosystem with niche players addressing market segments the keystone player would otherwise not have access to.Stakeholder identification is critical to the financial and functional success of software development projects, however the task of identifying all stakeholders in a SECO is often not possible due to the high volume of stakeholders and open interfaces. The identification of key stakeholders should ensure that the most relevant requirements are elicited during a software planning cycle.The objective of this research is to examine how key stakeholders can be identified in complex SECOs. This research takes a design science approach and the main component of the current research is the design of an artifact in the form of a reference process model, that is applied in a real-world environment to develop a business process model. Consequently, this research aims to facilitate academia and industry in SECO optimization, especially from a requirements management perspective.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {88–95},
numpages = {8},
keywords = {stakeholder identification, software product lines, software ecosystems, requirements elicitation},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3382026.3431246,
author = {Kenner, Andy},
title = {Model-Based Evaluation of Vulnerabilities in Software Systems},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431246},
doi = {10.1145/3382026.3431246},
abstract = {Vulnerabilities in software systems result from faults, which occur at different stages in a software's life cycle, for example, in the design (i.e., undesired feature-interactions), the development (i.e., buffer overflows), or the operation (i.e., configuration errors). Various databases provide detailed information about vulnerabilities in software systems or the way to exploit it, but face severe limitations. The information is scattered across these databases, fluctuates in quality and granularity, and provides only an insight into a single vulnerability per entry. Even for a single software system it is challenging for any security-related stakeholder to determine the threat level, which consists of all vulnerabilities of the software system and its environment (i.e., operating system). Manual vulnerability management is feasible only to a limited extend if we want to identify all configurations that are affected by vulnerabilities, or determine a system's threat level and the resulting risk we have to deal with. For variant-rich systems, we also have to deal with variability, allowing different stakeholders to understand the threats to their particular setup. To deal with this variability, we propose vulnerability feature models, which offer a homogeneous view on all vulnerabilities of a software system. These models and the resulting analyses offer advantages in many disciplines of the vulnerability management process. In this paper, we report the research plan for our project, in which we focus on the model-based evaluation of vulnerabilities. This includes research objectives that take into account the design of vulnerability feature models, their application in the process of vulnerability management, and the impact of evolution, discovery, and verification of vulnerabilities.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {112–119},
numpages = {8},
keywords = {Vulnerability Analysis and Management, Vulnerability, Variability Model, Feature Model, Exploit},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2499777.2500723,
author = {Alsawalqah, Hamad and Kang, Sungwon and Lee, Danhyung},
title = {A method for software product platform design based on features},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500723},
doi = {10.1145/2499777.2500723},
abstract = {Due to the increased competition and the advent of mass customization, software firms are applying the Software Product Line Engineering (SPLE) approach to provide product variety in a cost-effective manner. Although the key to designing a successful software product family is the product platform, yet there is lack of measures and methods that are useful to optimize the product platform design. This paper proposes a method to provide decision support to determine the optimized product platform design. The method targets at identifying the optimized product platform design in order to maximize the cost savings and the amount of commonality while meeting the goals and needs of the envisioned customers' segments. It generates, validates, and evaluates alternative product platform designs while considering market concerns (e.g., customer preferences) and technical product platform concerns (e.g., decisions regarding shared features, economic benefit). We demonstrate its applicability with an example of platform design problem in smart phones domain.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {18–25},
numpages = {8},
keywords = {software product line, product platform design, commonality index, Kano scheme},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3579027.3608998,
author = {Luo, Chuan and Song, Jianping and Zhao, Qiyuan and Li, Yibei and Cai, Shaowei and Hu, Chunming},
title = {Generating Pairwise Covering Arrays for Highly Configurable Software Systems},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608998},
doi = {10.1145/3579027.3608998},
abstract = {Highly configurable software systems play crucial roles in real-world applications, which urgently calls for useful testing methods. Combinatorial interaction testing (CIT) is an effective methodology for detecting those faults that are triggered by the interaction of any t options, where t is the testing strength. Pairwise testing, i.e., CIT with t = 2, is known to be the most practical and popular CIT technique, and the pairwise covering array generation (PCAG) problem is the most critical problem in pairwise testing. Due to the practical importance of PCAG, many PCAG algorithms have been proposed. Unfortunately, existing PCAG algorithms suffer from the severe scalability problem. To this end, the SPLC Scalability Challenge (i.e., Product Sampling for Product Lines: The Scalability Challenge) has been proposed since 2019, in order to motivate researchers to develop practical PCAG algorithms for overcoming this scalability problem. In this work, we present a practical PCAG algorithm dubbed SamplingCA-ASF. To the best of our knowledge, our experiments show that SamplingCA-ASF is the first algorithm that can generate PCAs for Automotive02 and Linux, the two hardest and largest-scale instances in the SPLC Scalability Challenge, within reasonable time. Our experimental results indicate that SamplingCA-ASF can effectively alleviate the scalability problem in pairwise testing.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {261–267},
numpages = {7},
keywords = {scalability problem, pairwise testing, covering array generation},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3382025.3414965,
author = {Young, Jeffrey M. and Walkingshaw, Eric and Th\"{u}m, Thomas},
title = {Variational satisfiability solving},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414965},
doi = {10.1145/3382025.3414965},
abstract = {Incremental satisfiability (SAT) solving is an extension of classic SAT solving that allows users to efficiently solve a set of related SAT problems by identifying and exploiting shared terms. However, using incremental solvers effectively is hard since performance is sensitive to a problem's structure and the order sub-terms are fed to the solver, and the burden to track results is placed on the end user. For analyses that generate sets of related SAT problems, such as those in software product lines, incremental SAT solvers are either not used at all, used but not explicitly stated so in the literature, or used but suffer from the aforementioned usability problems. This paper translates the ordering problem to an encoding problem and automates the use of incremental SAT solving. We introduce variational SAT solving, which differs from incremental SAT solving by accepting all related problems as a single variational input and returning all results as a single variational output. Our central idea is to make explicit the operations of incremental SAT solving, thereby encoding differences between related SAT problems as local points of variation. Our approach automates the interaction with the incremental solver and enables methods to automatically optimize sharing of the input. To evaluate our methods we construct a prototype variational SAT solver and perform an empirical analysis on two real-world datasets that applied incremental solvers to software evolution scenarios. We show, assuming a variational input, that the prototype solver scales better for these problems than naive incremental solving while also removing the need to track individual results.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {18},
numpages = {12},
keywords = {variation, software product lines, satisfiability solving, choice calculus},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3382025.3414963,
author = {Creff, Stephen and Noir, J\'{e}r\^{o}me Le and Lenormand, Eric and Madel\'{e}nat, S\'{e}bastien},
title = {Towards facilities for modeling and synthesis of architectures for resource allocation problem in systems engineering},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414963},
doi = {10.1145/3382025.3414963},
abstract = {Exploring architectural design space is often beyond human capacity and makes architectural design a difficult task. Model-based systems engineering must include assistance to the system designer in identifying candidate architectures to subsequently analyze tradeoffs. Unfortunately, existing languages and approaches do not incorporate this concern, generally favoring solution analysis over exploring a set of candidate architectures.In this paper, we explore the advantages of designing and configuring the variability problem to solve one of the problems of exploring (synthesizing) candidate architectures in systems engineering: the resource allocation problem. More specifically, this work reports on the use of the Clafer modeling language and its gateway to the CSP Choco Solver, on an industrial case study of heterogeneous hardware resource allocation (GPP-GPGPU-FPGA).Based on experiments on the modeling in Clafer, and the impact of its translation into the constraint programming paradigm (performance studies), discussions highlight some issues concerning facilities for modeling and synthesis of architectures and recommendations are proposed towards the use of this variability approach.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {32},
numpages = {11},
keywords = {variability modeling, empirical study, constraint solving, architecture synthesis, allocation problem},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3336294.3342383,
author = {Martin, Hugo and Pereira, Juliana Alves and Acher, Mathieu and Temple, Paul},
title = {Machine Learning and Configurable Systems: A Gentle Introduction},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342383},
doi = {10.1145/3336294.3342383},
abstract = {The goal of this tutorial is to give an introduction to how machine learning can be used to support activities related to the engineering of configurable systems and software product lines. To the best of our knowledge, this is the first practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance prediction) on real-world systems (VaryLaTeX, x264).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {325–326},
numpages = {2},
keywords = {software product lines, machine learning, configurable systems},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3336294.3336300,
author = {Drave, Imke and Kautz, Oliver and Michael, Judith and Rumpe, Bernhard},
title = {Semantic Evolution Analysis of Feature Models},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336300},
doi = {10.1145/3336294.3336300},
abstract = {During the development process, feature models change continuously. Analyzing the semantic differences between consecutive feature model versions is important throughout the entire development process to detect unintended changes of the modeled product line. Previous work introduced a semantic differencing technique for feature models based on a closed-world assumption, which reveals the differences between two feature models when allowing products to only contain features used in the models. However, this does not reflect the stepwise refinement of feature models in early development stages. Therefore, we introduce an open-world semantics, an automatic method for semantic differencing of feature models with respect to the novel semantics, and formally relate the open- and closed-world semantics. We formally prove our results, including the relation between the different semantics as well as the soundness and completeness of the semantic differencing procedure. In conjunction with previous work, the results enable effective semantic feature model evolution analyses throughout the entire development process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {245–255},
numpages = {11},
keywords = {semantic differences, open-world semantics, model evolution analysis, feature modeling},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2934466.2934478,
author = {Galindo, Jos\'{e} A. and Acher, Mathieu and Tirado, Juan Manuel and Vidal, Cristian and Baudry, Benoit and Benavides, David},
title = {Exploiting the enumeration of all feature model configurations: a new perspective with distributed computing},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934478},
doi = {10.1145/2934466.2934478},
abstract = {Feature models are widely used to encode the configurations of a software product line in terms of mandatory, optional and exclusive features as well as propositional constraints over the features. Numerous computationally expensive procedures have been developed to model check, test, configure, debug, or compute relevant information of feature models. In this paper we explore the possible improvement of relying on the enumeration of all configurations when performing automated analysis operations. We tackle the challenge of how to scale the existing enumeration techniques by relying on distributed computing. We show that the use of distributed computing techniques might offer practical solutions to previously unsolvable problems and opens new perspectives for the automated analysis of software product lines.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {74–78},
numpages = {5},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1145/1279711.1279715,
author = {Krishna, Arvind S. and Gokhale, Aniruddha and Schmidt, Douglas C. and Ranganath, Venkatesh Prasad and Hatcliff, John},
title = {Towards highly optimized real-time middleware for software product-line architectures},
year = {2006},
issue_date = {January 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/1279711.1279715},
doi = {10.1145/1279711.1279715},
abstract = {This paper provides the following contributions to the study of middleware optimization techniques for product line architectures in real-time systems. First, we identify different dimensions of generality in standards based middleware implementations. Second, we describe how specialization approaches used in other domains including OS, compiler and programming languages can be applied to address middleware generality challenges. Third, we present preliminary results from the application of our specialization techniques. Our results illustrate that specialization techniques represent a promising approach for minimizing time/space overheads in middleware.},
journal = {SIGBED Rev.},
month = jan,
pages = {13–16},
numpages = {4}
}

@inproceedings{10.1145/3307630.3342391,
author = {Beuche, Danilo},
title = {Industrial Variant Management with pure::variants},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342391},
doi = {10.1145/3307630.3342391},
abstract = {The paper describes a demonstration of pure::variants, a commercial tool for variant and variability management for product lines. The demonstration shows how flexible product line (PL) architectures can be built, tested and maintained by using the modeling and integration capabilities provided by pure::variants. With pure::variants being available for a long time, the demonstration (and the paper) combines both basics of pure::variants, known to parts of the audience, and new capabilities, introduced within the last year.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {37–39},
numpages = {3},
keywords = {tools, software product lines, feature modelling},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3307630.3342400,
author = {Kamali, Seiede Reyhane and Kasaei, Shirin and Lopez-Herrejon, Roberto E.},
title = {Answering the Call of the Wild? Thoughts on the Elusive Quest for Ecological Validity in Variability Modeling},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342400},
doi = {10.1145/3307630.3342400},
abstract = {Ecological validity is a term commonly used in several disciplines to refer to the fact that in a research study, the methods, the materials, and the settings must approximate the real world, i.e. what happens in everyday life. Variability modeling is no exception, it has striven for this form of validity by looking at two main sources, industrial projects and open source projects. Despite their unquestionable value, industrial projects inherently pose limitations; for instance, in terms of open access or results replication, which are two important tenets for any scientific endeavor. In this paper, we present our first findings on the use of open source projects in variability modeling research, and identify trends and avenues for further research.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {143–150},
numpages = {8},
keywords = {variability models, open source projects, feature models},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2934466.2934474,
author = {Myll\"{a}rniemi, Varvana and Raatikainen, Mikko and Savolainen, Juha and M\"{a}nnist\"{o}, Tomi},
title = {Purposeful performance variability in software product lines: a comparison of two case studies},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934474},
doi = {10.1145/2934466.2934474},
abstract = {Within software product lines, customers may have different quality needs. To produce products with purposefully different quality attributes, several challenges must be addressed. First, one must be able to distinguish product quality attributes to the customers in a meaningful way. Second, one must create the desired quality attribute differences during product-line architecture design and derivation. To study how performance is varied purposefully in software product lines, we conducted a comparison and re-analysis of two industrial case studies in the telecommunication and mobile game domains. The results show that performance variants must be communicated to the customer in a way that links to customer value and her role. When performance or its adaptation are crucial for the customer, performance differences must be explicitly "designed in" with software or hardware means. Due to the emergent nature of performance, it is important to test performance and manage how other variability affects performance.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {144–153},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3233027.3233043,
author = {Masri, Samer AL and Nadi, Sarah and Gaudet, Matthew and Liang, Xiaoli and Young, Robert W.},
title = {Using static analysis to support variability implementation decisions in C++},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233043},
doi = {10.1145/3233027.3233043},
abstract = {Eclipse OMR is an open-source C++ framework for building robust language runtimes. The OMR toolkit includes a dynamic Just-In-Time (JIT) compiler, a garbage collector, a platform abstraction library, and a set of developer tooling capabilities. To support the diverse languages and architectures targeted by the framework, OMR's variability implementation uses a combination of build-system variability and static polymorphism. That is, all implementation classes that depend on the selected language and architecture are decided at compile time. However, OMR developers now realize that the current variability design decision, specifically the static polymorphism implementation, has its drawbacks. They are considering using dynamic polymorphism instead of static polymorphism. Before making such a fundamental design change, however, it is crucial to collect function information and overload/override statistics about the current variability in the code base.In this paper, we present OMRStatistics, a static analysis tool that we built for OMR developers to help them collect this information. Specifically, OMRStatistics (1) visualizes the class hierarchy from OMR's current static polymorphic implementation, (2) visualizes the function overloads and overrides with their respective locations in the source code, (3) collects important information about the classes and functions, and (4) stores all the collected information in a database for further analysis. Our tool OMRStatistics allows OMR developers to make better design decisions on which variability extension points should be switched from static polymorphism to dynamic polymorphism.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {236–245},
numpages = {10},
keywords = {static polymorphism, static analysis, software variability analysis, dynamic polymorphism, clang plugin, build path variability, C++},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2648511.2648546,
author = {Van Landuyt, Dimitri and Op de beeck, Steven and Hovsepyan, Aram and Michiels, Sam and Joosen, Wouter and Meynckens, Sven and de Jong, Gjalt and Barais, Olivier and Acher, Mathieu},
title = {Towards managing variability in the safety design of an automotive hall effect sensor},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648546},
doi = {10.1145/2648511.2648546},
abstract = {This paper discusses the merits and challenges of adopting software product line engineering (SPLE) as the main development process for an automotive Hall Effect sensor. This versatile component is integrated into a number of automotive applications with varying safety requirements (e.g., windshield wipers and brake pedals).This paper provides a detailed explanation as to why the process of safety assessment and verification of the Hall Effect sensor is currently cumbersome and repetitive: it must be repeated entirely for every automotive application in which the sensor is to be used. In addition, no support is given to the engineer to select and configure the appropriate safety solutions and to explain the safety implications of his decisions.To address these problems, we present a tailored SPLE-based approach that combines model-driven development with advanced model composition techniques for applying and reasoning about specific safety solutions. In addition, we provide insights about how this approach can reduce the overall complexity, improve reusability, and facilitate safety assessment of the Hall Effect sensor.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {304–309},
numpages = {6},
keywords = {software product line engineering, safety patterns, hardware/software co-design, automotive, ASIL validation},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3307630.3342404,
author = {Th\"{u}m, Thomas and Seidl, Christoph and Schaefer, Ina},
title = {On Language Levels for Feature Modeling Notations},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342404},
doi = {10.1145/3307630.3342404},
abstract = {Configuration is a key enabling technology for the engineering of systems and software as wells as physical goods. A selection of configuration options (aka. features) is often enough to automatically generate a product tailored to the needs of a customer. It is common that not all combinations of features are possible in a given domain. Feature modeling is the de-facto standard for specifying features and their valid combinations. However, a pivotal hurdle for practitioners, researchers, and teachers in applying feature modeling is that there are hundreds of tools and languages available. While there have been first attempts to define a standard feature modeling language, they still struggle with finding an appropriate level of expressiveness. If the expressiveness is too high, the language will not be adopted, as it is too much effort to support all language constructs. If the expressiveness is too low, the language will not be adopted, as many interesting domains cannot be modeled in such a language. Towards a standard feature modeling notation, we propose the use of language levels with different expressiveness each and discuss criteria to be used to define such language levels. We aim to raise the awareness on the expressiveness and eventually contribute to a standard feature modeling notation.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {158–161},
numpages = {4},
keywords = {variability modeling, product lines, language design, feature model, expressiveness, automated analysis},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3336294.3336314,
author = {Asano, Masaki and Nishiura, Yoichi and Nakanishi, Tsuneo and Fujiwara, Keiichi},
title = {Feature Oriented Refinement from Requirements to System Decomposition: Quantitative and Accountable Approach},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336314},
doi = {10.1145/3336294.3336314},
abstract = {This paper presents the revised domain engineering process to develop product lines of automotive body parts in Aisin Seiki Co., Ltd. In the process, feature analysis is conducted by a limited number of engineers with talent of abstraction and separation and other work including specifications and architecture design is conducted by average engineers who know the products. Feature analysis defines a hierarchy of abstraction, achieves separation of concerns, and disciplines other artifacts to follow the structure of abstraction and separation. Requirements and specifications are refined by the use case, use case scenario, and hierarchical tabular description (USDM) in a step-wise manner. The specification in USDM is refined to a system decomposition in a quantitative and accountable manner using the robustness diagram and design structure matrix. The revised domain engineering process reduced the issues pointed out in software reviews concerning errors on specifications and architecture design. Moreover, it reduced lead time for architecture design and produced the architecture tolerant to changes.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {195–205},
numpages = {11},
keywords = {use case approach, software product lines, robustness analysis, feature analysis, design structure matrix, automotive body parts},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3106195.3106219,
author = {Gregg, Susan P. and Albert, Denise M. and Clements, Paul},
title = {Product Line Engineering on the Right Side of the "V"},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106219},
doi = {10.1145/3106195.3106219},
abstract = {Product line engineering (PLE) is well-known for the savings it brings to organizations. This paper shows how a very large, in-service systems and software product line is achieving PLE-based savings in their verification and validation phase of development. The paper addresses how to achieve the sharing across product variants while the products being tested are evolving over time. Additionally, we will give a pragmatic set of decision criteria to help answer the longstanding issue in PLE-based testing of whether to test on the domain side or the application (product) side of the product derivation process.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {165–174},
numpages = {10},
keywords = {variation points, software product lines, second generation product line engineering, product portfolio, product configurator, feature profiles, feature modeling, bill-of-features, Product line engineering, PLE factory, AEGIS Combat System},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2791060.2791106,
author = {Smiley, Karen and Schmidt, Werner and Dagnino, Aldo},
title = {Evolving an industrial analytics product line architecture},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791106},
doi = {10.1145/2791060.2791106},
abstract = {This paper focuses on an industrial experience with software product lines of analytics-enabled solutions, specifically the evolution of the software product line architecture for a Subject Matter Expert Workbench toolset which supports analytic plugins for multiple software product lines. As context, the toolset product line was intended for integration of expert knowledge into a family of industrial asset health applications at runtime. The toolset architecture is now being evolved to build and manage plugins for multiple Industrial Analytics solutions (software systems and services) beyond asset health. This evolution is driving changes in the desired architecture qualities of the toolset; widening the stakeholder pool and influencing priorities; affecting the architecture tradeoffs and decisions; and triggering updates to the product line architecture, the guidance for applying it, and the current prototype of the toolset. We describe our experiences in handling this evolution, assess lessons learned, and discuss potential relevance to other product line scenarios.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {263–272},
numpages = {10},
keywords = {software product line, reusability, performance, knowledge, interoperability, industrial analytics, extensibility, asset health},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3336294.3336318,
author = {Ebert, Rolf and Jolianis, Jahir and Kriebel, Stefan and Markthaler, Matthias and Pruenster, Benjamin and Rumpe, Bernhard and Salman, Karin Samira},
title = {Applying Product Line Testing for the Electric Drive System},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336318},
doi = {10.1145/3336294.3336318},
abstract = {The growth in electrification and digitalization of vehicles leads to increasing variability and complexity of automotive systems. This poses new challenges for verification and validation, identified in a Product Line Engineering case study for the electric drive system. To overcome those challenges we developed a Product Line Testing methodology called TIGRE. In this paper, we present the TIGRE methodology. TIGRE comprises the identification and documentation of relevant data for efficient product line testing and the application of this data in the test management of an agile project environment. Furthermore, we present our experiences from the introduction into a large-scale industrial context. Based on our results from the introduction, we conclude that the TIGRE approach reduces the testing effort for automotive product lines significantly and, furthermore, allows us to transfer the results to untested products.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {14–24},
numpages = {11},
keywords = {software product lines, product line testing, product line engineering, automotive industry},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2499777.2499783,
author = {Kalender, Mert Emin and T\"{u}z\"{u}n, Eray and Tekinerdogan, Bedir},
title = {Decision support for adopting SPLE with Transit-PL},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499783},
doi = {10.1145/2499777.2499783},
abstract = {It is generally acknowledged that the decision to adopt a software product line engineering (SPLE) approach needs to be performed carefully due to the different risks involved in taking such an important decision. To mitigate the potential risks of the transition to SPLE, several studies have been proposed that include many different rules for analyzing the feasibility of the SPLE adoption and the selection of transition process. However, it is not easy to apply these manually and likewise provide a proper decision with the corresponding justification. In this paper, we propose the tool Transit-PL, a web based decision support system for analyzing the feasibility of SPLE for an organization and selecting the appropriate transition strategy. Transit-PL provides a framework to build particular decision support system for selected strategies using different types of questions and corresponding rules and set of answers.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {150–153},
numpages = {4},
keywords = {software product line engineering, decision support systems},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3336294.3336321,
author = {Ghofrani, Javad and Kozegar, Ehsan and Fehlhaber, Anna Lena and Soorati, Mohammad Divband},
title = {Applying Product Line Engineering Concepts to Deep Neural Networks},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336321},
doi = {10.1145/3336294.3336321},
abstract = {Deep Neural Networks (DNNs) are increasingly being used as a machine learning solution thanks to the complexity of their architecture and hyperparameters-weights. A drawback is the excessive demand for massive computational power during the training process. Not only as a whole but parts of neural networks can also be in charge of certain functionalities. We present a novel challenge in an intersection between machine learning and variability management communities to reuse modules of DNNs without further training. Let us assume that we are given a DNN for image processing that recognizes cats and dogs. By extracting a part of the network, without additional training a new DNN should be divisible with the functionality of recognizing only cats. Existing research in variability management can offer a foundation for a product line of DNNs composing the reusable functionalities. An ideal solution can be evaluated based on its speed, granularity of determined functionalities, and the support for adding variability to the network. The challenge is decomposed in three subchallenges: feature extraction, feature abstraction, and the implementation of a product line of DNNs.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {72–77},
numpages = {6},
keywords = {variability, transfer learning, software product lines, machine learning, deep neural networks},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3307630.3342398,
author = {Beek, Maurice H. ter and Schmid, Klaus and Eichelberger, Holger},
title = {Textual Variability Modeling Languages: An Overview and Considerations},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342398},
doi = {10.1145/3307630.3342398},
abstract = {During the three decades since the invention of the first variability modeling approach [28], there have been multiple attempts to introduce advanced variability modeling capabilities. More recently, we have seen increased attention on textual variability modeling languages. In this paper, we summarize the main capabilities of state of the art textual variability modeling languages, based on [23], including updates regarding more recent work. Based on this integrated characterization, we provide a discussion of additional concerns, opportunities and challenges that are relevant for designing future (textual) variability modeling languages. The paper also summarizes relevant contributions by the authors as input to further discussions on future (textual) variability modeling languages.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {151–157},
numpages = {7},
keywords = {variability modeling, textual specification languages, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3336294.3336306,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {Automated Search for Configurations of Convolutional Neural Network Architectures},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336306},
doi = {10.1145/3336294.3336306},
abstract = {Convolutional Neural Networks (CNNs) are intensively used to solve a wide variety of complex problems. Although powerful, such systems require manual configuration and tuning. To this end, we view CNNs as configurable systems and propose an end-to-end framework that allows the configuration, evaluation and automated search for CNN architectures. Therefore, our contribution is threefold. First, we model the variability of CNN architectures with a Feature Model (FM) that generalizes over existing architectures. Each valid configuration of the FM corresponds to a valid CNN model that can be built and trained. Second, we implement, on top of Tensorflow, an automated procedure to deploy, train and evaluate the performance of a configured model. Third, we propose a method to search for configurations and demonstrate that it leads to good CNN models. We evaluate our method by applying it on image classification tasks (MNIST, CIFAR-10) and show that, with limited amount of computation and training, our method can identify high-performing architectures (with high accuracy). We also demonstrate that we outperform existing state-of-the-art architectures handcrafted by ML researchers. Our FM and framework have been released to support replication and future research.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {119–130},
numpages = {12},
keywords = {neural architecture search, feature model, configuration search, NAS, AutoML},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3382025.3414951,
author = {Heradio, Ruben and Fernandez-Amoros, David and Galindo, Jos\'{e} A. and Benavides, David},
title = {Uniform and scalable SAT-sampling for configurable systems},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414951},
doi = {10.1145/3382025.3414951},
abstract = {Several relevant analyses on configurable software systems remain intractable because they require examining vast and highly-constrained configuration spaces. Those analyses could be addressed through statistical inference, i.e., working with a much more tractable sample that later supports generalizing the results obtained to the entire configuration space. To make this possible, the laws of statistical inference impose an indispensable requirement: each member of the population must be equally likely to be included in the sample, i.e., the sampling process needs to be "uniform". Various SAT-samplers have been developed for generating uniform random samples at a reasonable computational cost. Unfortunately, there is a lack of experimental validation over large configuration models to show whether the samplers indeed produce genuine uniform samples or not. This paper (i) presents a new statistical test to verify to what extent samplers accomplish uniformity and (ii) reports the evaluation of four state-of-the-art samplers: Spur, QuickSampler, Unigen2, and Smarch. According to our experimental results, only Spur satisfies both scalability and uniformity.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {17},
numpages = {11},
keywords = {variability modeling, uniform sampling, software product lines, configurable systems, SAT},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3382025.3414945,
author = {G\"{o}ttmann, Hendrik and Luthmann, Lars and Lochau, Malte and Sch\"{u}rr, Andy},
title = {Real-time-aware reconfiguration decisions for dynamic software product lines},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414945},
doi = {10.1145/3382025.3414945},
abstract = {Dynamic Software Product Lines (DSPL) have recently shown promising potentials as integrated engineering methodology for (self-)adaptive software systems. Based on the software-configuration principles of software product lines, DSPL additionally foster reconfiguration capabilities to continuously adapt software products to ever-changing environmental contexts. However, in most recent works concerned with finding near-optimal reconfiguration decisions, real-time aspects of reconfiguration processes are usually out of scope. In this paper, we present a model-based methodology for specifying and automatically analyzing real-time constraints of reconfiguration decisions in a feature-oriented and compositional way. Those real-time aware DSPL specifications are internally translated into timed automata, a well-founded formalism for real-time behaviors. This representation allows for formally reasoning about consistency and worst-case/best-case execution-time behaviors of sequences of reconfiguration decisions. The technique is implemented in a prototype tool and experimentally evaluated with respect to a set of case studies1.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {13},
numpages = {11},
keywords = {timed automata, reconfiguration decisions, dynamic software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3336294.3336295,
author = {Beek, Maurice H. ter and Damiani, Ferruccio and Lienhardt, Michael and Mazzanti, Franco and Paolini, Luca},
title = {Static Analysis of Featured Transition Systems},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336295},
doi = {10.1145/3336294.3336295},
abstract = {A Featured Transition System (FTS) is a formal behavioural model for software product lines, which represents the behaviour of all the products of an SPL in a single compact structure by associating transitions with features that condition their existence in products. In general, an FTS may contain featured transitions that are unreachable in any product (so called dead transitions) or, on the contrary, mandatorily present in all products for which their source state is reachable (so called false optional transitions), as well as states from which only for certain products progress is possible (so called hidden deadlocks). In this paper, we provide algorithms to analyse an FTS for such ambiguities and to transform an ambiguous FTS into an unambiguous FTS. The scope of our approach is twofold. First and foremost, an ambiguous model is typically undesired as it gives an unclear idea of the SPL. Second, an unambiguous FTS paves the way for efficient family-based model checking. We apply our approach to illustrative examples from the literature.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {39–51},
numpages = {13},
keywords = {static analysis, software product lines, formal specification, featured transition systems, behavioural model},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2362536.2362572,
author = {Leitner, Andrea and Zehetner, Josef and Toeglhofer, Philipp and Watzenig, Daniel},
title = {Requirement identification for variability management in a co-simulation environment},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362572},
doi = {10.1145/2362536.2362572},
abstract = {Co-simulation is a powerful approach to verify a system design and to support concept decisions early in the automotive development process. Due to the heterogeneous nature of the co-simulation framework there is a lot of potential for variability requiring the systematic handling of it.We identified two main scenarios for variability management techniques in a co-simulation environment. Variability management capabilities can be included in the co-simulation tool itself or provide variability mechanisms to configure the co-simulation externally from a software product line. Depending on the context, one or even both scenarios can be applied.This work addresses different types of variability in an independent co-simulation framework (ICOS) and defines requirements for a realization concept.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {269–274},
numpages = {6},
keywords = {variability management, software product line engineering, co-simulation},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3461001.3473064,
author = {Ayala, Inmaculada and Papadopoulos, Alessandro V. and Amor, Mercedes and Fuentes, Lidia},
title = {ProDSPL: proactive self-adaptation based on dynamic software product lines},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3473064},
doi = {10.1145/3461001.3473064},
abstract = {This is an extended abstract of the article: Inmaculada Ayala, Alessandro V. Papadopoulos, Mercedes Amor, Lidia Fuentes, ProDSPL: Proactive self-adaptation based on Dynamic Software Product Lines, Journal of Systems and Software, Volume 175, 2021, 110909, ISSN 0164-1212, https://doi.org/10.1016/j.jss.2021.110909.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {81},
numpages = {1},
keywords = {self-adaptation, proactive control, optimization, linear constraint, dynamic software product lines},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3106195.3106215,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Self-healing in Service Mashups Through Feature Adaptation},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106215},
doi = {10.1145/3106195.3106215},
abstract = {The composition of the functionality of multiple services into a single unique service mashup has received wide interest in the recent years. Given the distributed nature of these mashups where the constituent services can be located on different servers, it is possible that a change in the functionality or availability of a constituent service result in the failure of the service mashup. In this paper, we propose a novel method based on the Software Product Line Engineering (SPLE) paradigm which is able to find an alternate valid service mashup which has maximum possible number of original service mashup features in order to mitigate a service failure when complete recovery is not possible. This method also has an advantage that it can recover or mitigate the failure automatically without requiring the user to specify any adaptation rule or strategy. We show the practicality of our proposed approach through extensive experiments.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {94–103},
numpages = {10},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3106195.3106214,
author = {Couto, Marco and Borba, Paulo and Cunha, J\'{a}come and Fernandes, Jo\~{a}o Paulo and Pereira, Rui and Saraiva, Jo\~{a}o},
title = {Products go Green: Worst-Case Energy Consumption in Software Product Lines},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106214},
doi = {10.1145/3106195.3106214},
abstract = {The optimization of software to be (more) energy efficient is becoming a major concern for the software industry. Although several techniques have been presented to measure energy consumption for software, none has addressed software product lines (SPLs). Thus, to measure energy consumption of a SPL, the products must be generated and measured individually, which is too costly.In this paper, we present a technique and a prototype tool to statically estimate the worst case energy consumption for SPL. The goal is to provide developers with techniques and tools to reason about the energy consumption of all products in a SPL, without having to produce, run and measure the energy in all of them.Our technique combines static program analysis techniques and worst case execution time prediction with energy consumption analysis. This technique analyzes all products in a feature-sensitive manner, that is, a feature used in several products is analyzed only once, while the energy consumption is estimated once per product.We implemented our technique in a tool called Serapis. We did a preliminary evaluation using a product line for image processing implemented in C. Our experiments considered 7 products from such line and our initial results show that the tool was able to estimate the worst-case energy consumption with a mean error percentage of 9.4% and standard deviation of 6.2% when compared with the energy measured when running the products.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {84–93},
numpages = {10},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2019136.2019157,
author = {Elsner, Christoph and Lohmann, Daniel and Schr\"{o}der-Preikschat, Wolfgang},
title = {An infrastructure for composing build systems of software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019157},
doi = {10.1145/2019136.2019157},
abstract = {Deriving a product from a software product line may require various build tasks, such as model transformations, source code generation, preprocessing, compiling, as well as linking and packaging the compiled sources. Usually implemented using simple scripting languages, such as Apache ant or GNU make, build systems tend to become monolithic entities, which are intricate to adapt and maintain.This makes developing the build system for a multi-- product-line, which is composed of several sub--product-lines and maybe other configurable components, particularly challenging. Several, previously independent build systems--- possibly implemented using different build tools (ant, make, etc.)---need to be integrated. In this paper, we approach this by using models to describe the involved build tasks (including their input and output parameters) as well as their composition. An interpreter evaluates the models and executes the tasks in the composed order with the configured parameters to produce the final product.Our approach enables the interaction of build systems implemented with different tools with only little development effort, whereas the build order and parameter flow is made explicit in the models. We have started to apply our tooling to model the build system of two multi--product-lines, where it reveals sufficient expressiveness and clarifies the build system interaction.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {18},
numpages = {8},
keywords = {software product line, build system integration},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3307630.3342410,
author = {Rosiak, Kamil and Urbaniak, Oliver and Schlie, Alexander and Seidl, Christoph and Schaefer, Ina},
title = {Analyzing Variability in 25 Years of Industrial Legacy Software: An Experience Report},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342410},
doi = {10.1145/3307630.3342410},
abstract = {In certain domains, safety-critical software systems may remain operational for decades. To comply with changing requirements, new system variants are commonly created by copying and modifying existing ones. Typically denoted clone-and-own, software quality and overall maintainability are adversely affected in the long-run. With safety being pivotal, a fault in one variant may require the entire portfolio to be assessed. Thus, engineers need to maintain legacy systems dating back decades, implemented in programming languages such as Pascal. Software product lines (SPLs) can be a remedy but migrating legacy systems requires their prior analysis and comparison. For industrial software systems, this remains a challenge.In this paper, we introduce a comparison procedure and customizable metrics to allow for a fine-grained comparison of Pascal modules to the level of individual expressions. By that, we identify common parts of while also capturing different parts between modules as a basis for a transition towards anSPLs practice. Moreover, we demonstrate the feasibility of our approach using a case study with seven Pascal modules totaling 13,271 lines of code with an evolution-history of 25 years and show our procedure to be fast and precise. Furthermore, we elaborate on the case study and detail peculiarities of the Pascal modules, which are characteristic for an evolution-history of a quarter century.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {65–72},
numpages = {8},
keywords = {variability, software prodct line, legacy software, clone-and-own},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3233027.3233039,
author = {Pereira, Juliana Alves and Schulze, Sandro and Figueiredo, Eduardo and Saake, Gunter},
title = {N-dimensional tensor factorization for self-configuration of software product lines at runtime},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233039},
doi = {10.1145/3233027.3233039},
abstract = {Dynamic software product lines demand self-adaptation of their behavior to deal with runtime contextual changes in their environment and offer a personalized product to the user. However, taking user preferences and context into account impedes the manual configuration process, and thus, an efficient and automated procedure is required. To automate the configuration process, context-aware recommendation techniques have been acknowledged as an effective mean to provide suggestions to a user based on their recognized context. In this work, we propose a collaborative filtering method based on tensor factorization that allows an integration of contextual data by modeling an N-dimensional tensor User-Feature-Context instead of the traditional two-dimensional User-Feature matrix. In the proposed approach, different types of non-functional properties are considered as additional contextual dimensions. Moreover, we show how to self-configure software product lines by applying our N-dimensional tensor factorization recommendation approach. We evaluate our approach by means of an empirical study using two datasets of configurations derived for medium-sized product lines. Our results reveal significant improvements in the predictive accuracy of the configuration over a state-of-the-art non-contextual matrix factorization approach. Moreover, it can scale up to a 7-dimensional tensor containing hundred of configurations in a couple of milliseconds.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {87–97},
numpages = {11},
keywords = {software product lines, self-configuration, runtime decision-making, recommender systems},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3307630.3342417,
author = {Achtaich, Asmaa and Roudies, Ounsa and Souissi, Nissrine and Salinesi, Camille and Mazo, Ra\'{u}l},
title = {Evaluation of the State-Constraint Transition Modelling Language: A Goal Question Metric Approach},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342417},
doi = {10.1145/3307630.3342417},
abstract = {Self-adaptive systems (SAS) are exceptional systems, on account of their versatile composition, dynamic behavior and evolutive nature. Existing formal languages for the specification of SAS focus on adapting system elements to achieve a target goal, following specific rules, without much attention on the adaptation of requirements themselves. The State-Constraint Transition (SCT) modeling language enables the specification of dynamic requirements, both at the domain and application level, as a result of space or time variability. This language, evaluated in this paper, enables the specification of a variety of requirement types, for SASs from different domains, while generating a configuration, all configurations, and number of possible configurations, in milliseconds. This paper presents these results, namely; expressiveness, domain independence and scalability, from the viewpoint of designers and domain engineers, following a goal-question-metric approach. However, being primarily based on constraint programming (CP), the language suffers from drawbacks inherited from this paradigm, specifically time related requirements, like (e.g. order, frequency and staged requirements).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {106–113},
numpages = {8},
keywords = {state machine, modeling language, dynamic software product lines, constraint programming, IoT},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3233027.3233035,
author = {Varshosaz, Mahsa and Al-Hajjaji, Mustafa and Th\"{u}m, Thomas and Runge, Tobias and Mousavi, Mohammad Reza and Schaefer, Ina},
title = {A classification of product sampling for software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233035},
doi = {10.1145/3233027.3233035},
abstract = {The analysis of software product lines is challenging due to the potentially large number of products, which grow exponentially in terms of the number of features. Product sampling is a technique used to avoid exhaustive testing, which is often infeasible. In this paper, we propose a classification for product sampling techniques and classify the existing literature accordingly. We distinguish the important characteristics of such approaches based on the information used for sampling, the kind of algorithm, and the achieved coverage criteria. Furthermore, we give an overview on existing tools and evaluations of product sampling techniques. We share our insights on the state-of-the-art of product sampling and discuss potential future work.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {1–13},
numpages = {13},
keywords = {testing, software product lines, sampling algorithms, feature interaction, domain models},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3233027.3233034,
author = {K\"{u}hn, Thomas and Kassin, Kevin Ivo and Cazzola, Walter and A\ss{}mann, Uwe},
title = {Modular feature-oriented graphical editor product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233034},
doi = {10.1145/3233027.3233034},
abstract = {Software Product Lines (SPLs) have a long tradition and aim at reducing development costs by increasing reuse. They have been successfully applied to develop families of languages, ultimately establishing the field of Language Product Lines (LPLs). Currently, LPLs facilitate a family of textual languages by defining an SPL of compilers/interpreters. In contrast, this work aims at supporting families of graphical languages by defining an SPL of graphical editors, whereas each language variant is supported by a corresponding product of a Graphical Editor Product Line (GEPL). Thus far, there exists no modular approach for the development of GEPLs for families of visual languages. To remedy this, this paper introduces a feature-oriented development approach for GEPLs that ensures modularity, maintainability, and extensibility of the resulting product line. To showcase the suitability and applicability of our approach, we developed a modular GEPL for the family of role-based modeling languages, a feature rich family of conceptual modeling languages. Finally, we illustrate its extensibility by adding a complex language feature.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {76–86},
numpages = {11},
keywords = {software product lines, modeling languages, language product lines, graphical editors product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3336294.3336316,
author = {Tolvanen, Juha-Pekka and Kelly, Steven},
title = {How Domain-Specific Modeling Languages Address Variability in Product Line Development: Investigation of 23 Cases},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336316},
doi = {10.1145/3336294.3336316},
abstract = {Domain-Specific Modeling raises the level of abstraction beyond programming by specifying the solution directly with domain concepts. Within product lines domain-specific approaches are applied to specify variability and then generate final products together with commonality. Such automated product derivation is possible because both the modeling language and generator are made for a particular product line --- often inside a single company. In this paper we examine which kinds of reuse and product line approaches are applied in industry with domain-specific modeling. Our work is based on empirical analysis of 23 cases and the languages and models created there. The analysis reveals a wide variety and some commonalities in the size of languages and in the ways they apply reuse and product line approaches.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {155–163},
numpages = {9},
keywords = {product line variability, product derivation, domain-specific modeling, domain-specific language, code generation},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3233027.3233031,
author = {Kaindl, Hermann and Kramer, Stefan and Hoch, Ralph},
title = {An inductive learning perspective on automated generation of feature models from given product specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233031},
doi = {10.1145/3233027.3233031},
abstract = {For explicit representation of commonality and variability of a product line, a feature model is mostly used. An open question is how a feature model can be inductively learned in an automated way from a limited number of given product specifications in terms of features.We propose to address this problem through machine learning, more precisely inductive generalization from examples. However, no counter-examples are assumed to exist. Basically, a feature model needs to be complete with respect to all the given example specifications. First results indicate the feasibility of this approach, even for generating hierarchies, but many open challenges remain.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {25–30},
numpages = {6},
keywords = {machine learning, inductive generalization from examples, generating feature models},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3382025.3414954,
author = {Michelon, Gabriela Karoline and Obermann, David and Linsbauer, Lukas and Assun\c{c}\~{a}o, Wesley Klewerton G. and Gr\"{u}nbacher, Paul and Egyed, Alexander},
title = {Locating feature revisions in software systems evolving in space and time},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414954},
doi = {10.1145/3382025.3414954},
abstract = {Software companies encounter variability in space as variants of software systems need to be produced for different customers. At the same time, companies need to handle evolution in time because the customized variants need to be revised and kept up-to-date. This leads to a predicament in practice with many system variants significantly diverging from each other. Maintaining these variants consistently is difficult, as they diverge across space, i.e., different feature combinations, and over time, i.e., revisions of features. This work presents an automated feature revision location technique that traces feature revisions to their implementation. To assess the correctness of our technique, we used variants and revisions from three open source highly configurable software systems. In particular, we compared the original artifacts of the variants with the composed artifacts that were located by our technique. The results show that our technique can properly trace feature revisions to their implementation, reaching traces with 100% precision and 98% recall on average for the three analyzed subject systems, taking on average around 50 seconds for locating feature revisions per variant used as input.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {14},
numpages = {11},
keywords = {variants, repository mining, feature revisions, feature location},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3307630.3342408,
author = {Schlie, Alexander and Rosiak, Kamil and Urbaniak, Oliver and Schaefer, Ina and Vogel-Heuser, Birgit},
title = {Analyzing Variability in Automation Software with the Variability Analysis Toolkit},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342408},
doi = {10.1145/3307630.3342408},
abstract = {Control software for automated production systems (aPs) becomes increasingly complex as it evolves due to changing requirements. To address varying customer demands or altered regulatory guidelines, it is common practice to create a new system variant by copying and subsequently modifying existing control software. Referred to as clone-and-own, proper documentation is typically not cherished, thereby entailing severe maintenance issues in the long-run. To mitigate such problems and to reinstate sustainable development, respective software systems need to be compared and their variability information needs to be reverse-engineered. However, recent work identified variability management in the domain of aPs to remain a challenging endevour and appropriate tool support to be missing.We bridge this gap and introduce the Variability Analysis Toolkit (VAT), an extensible platform that allows for the customizable definition of metrics to compare IEC61131-3 control software variants as well as providing means to visualize results. The VAT facilitates a working environment that allows for the exchange of produced results between users. By that, we aim to support engineers in re-engineering control software systems by providing them with means to define metrics based on their individual demands. We demonstrate the feasibility of the VAT using 24 software system variants implemented in accordance to the IEC61131-3 standard.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {191–198},
numpages = {8},
keywords = {variability, tooling, software product lines, legacy systems, automation software},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2648511.2648538,
author = {Shaker, Pourya and Atlee, Joanne M.},
title = {Behaviour interactions among product-line features},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648538},
doi = {10.1145/2648511.2648538},
abstract = {A software product line (SPL) is often constructed as a set of features, such that individual products can be assembled from a set of common features and a selection of optional features. Although features are conceptualized, developed, and evolved as separate concerns, it is often the case that, in practice, they interfere with each other -- called a feature interaction. In this paper, we precisely define what it means for one feature to have a behaviour interaction with another feature, where the behaviour of one feature is affected by the presence of another feature. Specifically, we use a form of bisimilarity to define when the behaviour of a feature in isolation differs from its behaviour in the presence of an interacting feature. We also consider the case where features are modelled in a language that allows the specification of intended interactions, and we adapt our use of bisimilarity to provide a formal definition for unintended behaviour interactions.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {242–246},
numpages = {5},
keywords = {product lines, feature interactions, bisimulation},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3236405.3236407,
author = {Ghofrani, Javad and Fehlhaber, Anna Lena},
title = {ProductlinRE: online management tool for requirements engineering of software product lines},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236407},
doi = {10.1145/3236405.3236407},
abstract = {The lack of online tools for managing various artifacts of software product lines is problematic, and stands in contradiction to findings about the need to support collaboration. In this paper, we present ProductLinRE, a web application allowing product line engineers to work cooperatively on artifacts of requirements engineering for software product lines. Our proposed online tool allows distributed teamwork, using a tracking mechanism for projects, artifacts and features while tailoring the requirements artifacts according to the selected features.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {17–22},
numpages = {6},
keywords = {software product lines, requirements engineering, online tools},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3106195.3106202,
author = {Wille, David and Wehling, Kenny and Seidl, Christoph and Pluchator, Martin and Schaefer, Ina},
title = {Variability Mining of Technical Architectures},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106202},
doi = {10.1145/3106195.3106202},
abstract = {Technical architectures (TAs) represent the computing infrastructure of a company with all its hardware and software components. Over the course of time, the number of TAs grows with the companies' requirements and usually a large variety of TAs has to be maintained. Core challenge is the missing information on relations between the existing variants of TAs, which complicates reuse of solutions across systems. However, identifying these relations is an expensive task as architects have to manually analyze each TA individually. Restructuring the existing TAs poses severe risks as often sufficient information is not available (e.g., due to time constraints). To avoid failures in productive systems and resulting loss of profit, companies continue to create new solutions without restructuring existing ones. This increased variability in TAs represents technical debt. In this paper, we adapt the idea of variability mining from the software product line domain and present an efficient and automatic mining algorithm to identify the common and varying parts of TAs by analyzing a potentially arbitrary number of TAs in parallel. Using the identified variability information, architects are capable of analyzing the relations of TAs, identifying reuse potential, and making well-founded maintenance decisions. We show the feasibility and scalability of our approach by applying it to a real-world industrial case study with large sets of TAs.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {39–48},
numpages = {10},
keywords = {variability mining, technical architecture, enterprise architecture},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2648511.2648521,
author = {Olaechea, Rafael and Rayside, Derek and Guo, Jianmei and Czarnecki, Krzysztof},
title = {Comparison of exact and approximate multi-objective optimization for software product lines},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648521},
doi = {10.1145/2648511.2648521},
abstract = {Software product lines (SPLs) allow stakeholders to manage product variants in a systematical way and derive variants by selecting features. Finding a desirable variant is often difficult, due to the huge configuration space and usually conflicting objectives (e.g., lower cost and higher performance). This scenario can be characterized as a multi-objective optimization problem applied to SPLs. We address the problem using an exact and an approximate algorithm and compare their accuracy, time consumption, scalability, parameter setting requirements on five case studies with increasing complexity. Our empirical results show that (1) it is feasible to use exact techniques for small SPL multi-objective optimization problems, and (2) approximate methods can be used for large problems but require substantial effort to find the best parameter setting for acceptable approximation which can be ameliorated with known good parameter ranges. Finally, we discuss the tradeoff between accuracy and time consumption when using exact and approximate techniques for SPL multi-objective optimization and guide stakeholders to choose one or the other in practice.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {92–101},
numpages = {10},
keywords = {software product lines, multi-objective optimization},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3106195.3106204,
author = {Luthmann, Lars and Stephan, Andreas and B\"{u}rdek, Johannes and Lochau, Malte},
title = {Modeling and Testing Product Lines with Unbounded Parametric Real-Time Constraints},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106204},
doi = {10.1145/3106195.3106204},
abstract = {Real-time requirements are crucial for embedded software in many modern application domains of software product lines. Hence, techniques for modeling and analyzing time-critical software have to be lifted to software product line engineering, too. Existing approaches extend timed automata (TA) by feature constraints to so-called featured timed automata (FTA) facilitating efficient verification of real-time properties for entire product lines in a single run. In this paper, we propose a novel modeling formalism, called configurable parametric timed automata (CoPTA), extending expressiveness of FTA by supporting freely configurable and therefore a-priori unbounded timing intervals for real-time constraints, which are defined as feature attributes in extended feature models with potentially infinite configuration spaces. We further describe an efficient test-suite generation methodology for CoPTA models, achieving location coverage on every possible model configuration. Finally, we present evaluation results gained from applying our tool implementation to a collection of case studies, demonstrating efficiency improvements compared to a variant-by-variant analysis.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {104–113},
numpages = {10},
keywords = {Timed Automata, Software Product Lines, Real-Time Systems, Model-based Testing},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3233027.3233037,
author = {Butting, Arvid and Eikermann, Robert and Kautz, Oliver and Rumpe, Bernhard and Wortmann, Andreas},
title = {Modeling language variability with reusable language components},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233037},
doi = {10.1145/3233027.3233037},
abstract = {Proliferation of modeling languages has produced a great variety of similar languages whose individual maintenance is challenging and costly. Reusing the syntax and semantics of modeling languages and their heterogeneous constituents, however, is rarely systematic. Current research on modeling language reuse focuses on reusing abstract syntax in form of metamodel parts. Systematic reuse of static and dynamic semantics is yet to be achieved. We present an approach to compose syntax and semantics of independently developed modeling languages through language product lines and derive new stand-alone language products. Using the MontiCore language workbench, we implemented a mechanism to compose language syntaxes and the realization of their semantics in form of template-based code generators according to language product line configurations. Leveraging variability of product lines greatly facilitates reusing modeling language and alleviates their proliferation.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {65–75},
numpages = {11},
keywords = {software language engineering, language variability, language product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2934466.2946045,
author = {Noir, J\'{e}rome Le and Madel\'{e}nat, S\'{e}bastien and Gailliard, Gr\'{e}gory and Labreuche, Christophe and Acher, Mathieu and Barais, Olivier and Constant, Olivier},
title = {A decision-making process for exploring architectural variants in systems engineering},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946045},
doi = {10.1145/2934466.2946045},
abstract = {In systems engineering, practitioners shall explore numerous architectural alternatives until choosing the most adequate variant. The decision-making process is most of the time a manual, time-consuming, and error-prone activity. The exploration and justification of architectural solutions is ad-hoc and mainly consists in a series of tries and errors on the modeling assets. In this paper, we report on an industrial case study in which we apply variability modeling techniques to automate the assessment and comparison of several candidate architectures (variants). We first describe how we can use a model-based approach such as the Common Variability Language (CVL) to specify the architectural variability. We show that the selection of an architectural variant is a multi-criteria decision problem in which there are numerous interactions (veto, favor, complementary) between criteria.We present a tooled process for exploring architectural variants integrating both CVL and the MYRIAD method for assessing and comparing variants based on an explicit preference model coming from the elicitation of stakeholders' concerns. This solution allows understanding differences among variants and their satisfactions with respect to criteria. Beyond variant selection automation improvement, this experiment results highlight that the approach improves rationality in the assessment and provides decision arguments when selecting the preferred variants.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {277–286},
numpages = {10},
keywords = {systems engineering, multi-criteria decision analysis, model-driven engineering, design exploration, decision-making, architecture},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3233027.3233044,
author = {Al-Hajjaji, Mustafa and Schulze, Michael and Ryssel, Uwe},
title = {Similarity analysis of product-line variants},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233044},
doi = {10.1145/3233027.3233044},
abstract = {Many existing approaches have exploited the similarity notion to analyze software systems. In product-line engineering, similarity notion has been considered to fulfill analysis objectives, such as improving the testing effectiveness and reducing the testing efforts. However, most of the existing approaches consider in the similarity measurement only information of high level of abstraction, such as the feature selections of variants. In this paper, we present the notion of similarity in product-line engineering using different types of problem-space as well as solution-space information. In particular, we discuss different scenarios of measuring the similarity between variants and the possibility of combining different types of information to output the similarity between the compared variants. Moreover, we realized these scenarios in the industrial variant management tool pure::variants to fulfill analysis functionalities.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {226–235},
numpages = {10},
keywords = {variants analysis, software product lines, similarity, highly configurable systems},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2791060.2791118,
author = {ter Beek, Maurice H. and Fantechi, Alessandro and Gnesi, Stefania and Mazzanti, Franco},
title = {Using FMC for family-based analysis of software product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791118},
doi = {10.1145/2791060.2791118},
abstract = {We show how the FMC model checker can successfully be used to model and analyze behavioural variability in Software Product Lines. FMC accepts parameterized specifications in a process-algebraic input language and allows the verification of properties of such models by means of efficient on-the-fly model checking. The properties can be expressed in a logic that allows to correlate the parameters of different actions within the same formula. We show how this feature can be used to tailor formulas to the verification of only a specific subset of products of a Software Product Line, thus allowing for scalable family-based analyses with FMC. We present a proof-of-concept that shows the application of FMC to an illustrative Featured Transition System from the literature.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {432–439},
numpages = {8},
keywords = {variability, process algebra, model transformation, features, featured transition systems},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1145/1183236.1183261,
author = {Clements, Paul C. and Jones, Lawrence G. and McGregor, John D. and Northrop, Linda M.},
title = {Getting there from here: a roadmap for software product line adoption},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183261},
doi = {10.1145/1183236.1183261},
abstract = {Mapping the technical and business activities and steps required for successful organizational adoption.},
journal = {Commun. ACM},
month = dec,
pages = {33–36},
numpages = {4}
}

@inproceedings{10.1145/3336294.3336297,
author = {Munoz, Daniel-Jesus and Oh, Jeho and Pinto, M\'{o}nica and Fuentes, Lidia and Batory, Don},
title = {Uniform Random Sampling Product Configurations of Feature Models That Have Numerical Features},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336297},
doi = {10.1145/3336294.3336297},
abstract = {Analyses of Software Product Lines (SPLs) rely on automated solvers to navigate complex dependencies among features and find legal configurations. Often these analyses do not support numerical features with constraints because propositional formulas use only Boolean variables. Some automated solvers can represent numerical features natively, but are limited in their ability to count and Uniform Random Sample (URS) configurations, which are key operations to derive unbiased statistics on configuration spaces.Bit-blasting is a technique to encode numerical constraints as propositional formulas. We use bit-blasting to encode Boolean and numerical constraints so that we can exploit existing #SAT solvers to count and URS configurations. Compared to state-of-art Satisfiability Modulo Theory and Constraint Programming solvers, our approach has two advantages: 1) faster and more scalable configuration counting and 2) reliable URS of SPL configurations. We also show that our work can be used to extend prior SAT-based SPL analyses to support numerical features and constraints.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {289–301},
numpages = {13},
keywords = {software product lines, propositional formula, numerical features, model counting, feature model, bit-blasting},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2934466.2962729,
author = {Beuche, Danilo},
title = {Using pure: variants across the product line lifecycle},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2962729},
doi = {10.1145/2934466.2962729},
abstract = {The paper describes a demonstration of pure::variants, a commercial tool for variant and variability management for product lines. The demonstration shows how flexible product line (PL) architectures can be built, tested and maintained by using the modeling and integration capabilities provided by pure::variants. With pure::variants being available for a long time, the demonstration (and the paper) combines both basics of pure::variants, known to parts of the audience, and new capabilities, introduced within the last year.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {333–336},
numpages = {4},
keywords = {tools, software product lines, feature modelling},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2647908.2655972,
author = {Meinicke, Jens and Th\"{u}m, Thomas and Schr\"{o}ter, Reimar and Benduhn, Fabian and Saake, Gunter},
title = {An overview on analysis tools for software product lines},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655972},
doi = {10.1145/2647908.2655972},
abstract = {A software product line is a set of different software products that share commonalities. For a selection of features, specialized products of one domain can be generated automatically from domain artifacts. However, analyses of software product lines need to handle a large number of products that can be exponential in the number of features. In the last decade, many approaches have been proposed to analyze software product lines efficiently. For some of these approaches tool support is available. Based on a recent survey on analysis for software product lines, we provide a first overview on such tools. While our discussion is limited to analysis tools, we provide an accompanying website covering further tools for product-line development. We compare tools according to their analysis and implementation strategy to identify underrepresented areas. In addition, we want to ease the reuse of existing tools for researchers and students, and to simplify research transfer to practice.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {94–101},
numpages = {8},
keywords = {type checking, tool support, theorem proving, testing, static analysis, software product lines, sampling, non-functional properties, model checking, code metrics},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2019136.2019158,
author = {Guana, Victor and Correal, Dario},
title = {Variability quality evaluation on component-based software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019158},
doi = {10.1145/2019136.2019158},
abstract = {Quality assurance and evaluation in Model Driven Software Product Lines (MD-SPLs) are pivotal points for the growing and solidification of the generative software factories. They are framed as one of the future fact methodologies for the construction of software systems. Although several approximations address the problem of generative environments, software product line scope expression, and core asset definition, not many of them try to solve, as a fundamental step, the automation of the quality attribute evaluation in the MD-SPL development cycle. This paper presents a model-driven engineering method and a tool for the quality evaluation of product line configurations through a cross architectural view analysis.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {19},
numpages = {8},
keywords = {sensitivity point, quality attribute, model-driven software product line, model composition, domain specific modeling},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2791060.2791108,
author = {Berger, Thorsten and Lettner, Daniela and Rubin, Julia and Gr\"{u}nbacher, Paul and Silva, Adeline and Becker, Martin and Chechik, Marsha and Czarnecki, Krzysztof},
title = {What is a feature? a qualitative study of features in industrial software product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791108},
doi = {10.1145/2791060.2791108},
abstract = {The notion of features is commonly used to describe the functional and non-functional characteristics of a system. In software product line engineering, features often become the prime entities of software reuse and are used to distinguish the individual products of a product line. Properly decomposing a product line into features, and correctly using features in all engineering phases, is core to the immediate and long-term success of such a system. Yet, although more than ten different definitions of the term feature exist, it is still a very abstract concept. Definitions lack concrete guidelines on how to use the notion of features in practice.To address this gap, we present a qualitative empirical study on actual feature usage in industry. Our study covers three large companies and an in-depth, contextualized analysis of 23 features, perceived by the interviewees as typical, atypical (outlier), good, or bad representatives of features. Using structured interviews, we investigate the rationales that lead to a feature's perception, and identify and analyze core characteristics (facets) of these features. Among others, we find that good features precisely describe customer-relevant functionality, while bad features primarily arise from rashly executed processes. Outlier features, serving unusual purposes, are necessary, but do not require the full engineering process of typical features.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {16–25},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3622748.3622754,
author = {Lima, Rafael and Bezerra, Carla and Machado, Ivan},
title = {A Self-Adaptation Mechanism for Variability Management in Dynamic Software Product Lines},
year = {2023},
isbn = {9798400709524},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622748.3622754},
doi = {10.1145/3622748.3622754},
abstract = {Variability management is crucial for companies that need to offer multiple product variants quickly. However, the increasing complexity of software-intensive systems has made variability management increasingly challenging. This challenge is compounded by the need for such systems to run continuously and adapt to changes in the environment and user needs at runtime. To address this challenge, Dynamic Software Product Line (DSPL) Engineering has emerged as a strategy for managing variability in complex and dynamic environments. The key challenge in DSPL engineering is to manage product configurations at runtime by detecting changes in the context and adapting accordingly. In this paper, we propose an adaptation mechanism for DSPL feature models that supports dynamic variability and is based on the MAPE-K model. The mechanism transforms feature model constraints into rules that enable the activation of each feature and annotates contexts in the corresponding features to be activated when changes occur. We have implemented the mechanism in the DyMMer 2.0 modeling tool and evaluated its performance using various DSPL feature models. Additionally, we performed a preliminary evaluation with a proof-of-concept study with an expert to assess its practical usage. Our results demonstrate the effectiveness and practicality of the proposed mechanism in managing variability in complex and dynamic environments.},
booktitle = {Proceedings of the 17th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {51–60},
numpages = {10},
keywords = {variability management, feature model, dynamic software product line},
location = {Campo Grande, Brazil},
series = {SBCARS '23}
}

@inproceedings{10.1145/2934466.2934472,
author = {Temple, Paul and Galindo, Jos\'{e} A. and Acher, Mathieu and J\'{e}z\'{e}quel, Jean-Marc},
title = {Using machine learning to infer constraints for product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934472},
doi = {10.1145/2934466.2934472},
abstract = {Variability intensive systems may include several thousand features allowing for an enormous number of possible configurations, including wrong ones (e.g. the derived product does not compile). For years, engineers have been using constraints to a priori restrict the space of possible configurations, i.e. to exclude configurations that would violate these constraints. The challenge is to find the set of constraints that would be both precise (allow all correct configurations) and complete (never allow a wrong configuration with respect to some oracle). In this paper, we propose the use of a machine learning approach to infer such product-line constraints from an oracle that is able to assess whether a given product is correct. We propose to randomly generate products from the product line, keeping for each of them its resolution model. Then we classify these products according to the oracle, and use their resolution models to infer cross-tree constraints over the product-line. We validate our approach on a product-line video generator, using a simple computer vision algorithm as an oracle. We show that an interesting set of cross-tree constraint can be generated, with reasonable precision and recall.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {209–218},
numpages = {10},
keywords = {variability modeling, software testing, software product lines, machine learning, constraints and variability mining},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2934466.2934491,
author = {Fogdal, Thomas and Scherrebeck, Helene and Kuusela, Juha and Becker, Martin and Zhang, Bo},
title = {Ten years of product line engineering at Danfoss: lessons learned and way ahead},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934491},
doi = {10.1145/2934466.2934491},
abstract = {Software and systems product line engineering (PLE) has been an established approach for reducing time to market as well as cost and increasing quality in a set of related products for two decades now. Although there is a huge body of knowledge on PLE, adopting a concrete PLE approach is still not a trivial endeavor for interested companies. With the increasing importance of development speed, the advent of agile engineering approaches, and decreasing management interest in improvements that require large organizational transformations and only show benefits after several years, companies are facing challenges in successfully adopting this approach. They often hesitate as there is no clear adoption path, nor any certainty, that the intended improvement steps will also provide added value in the short- and mid-term perspective. In consequence, a considerable amount of PLE potential still remains unexploited.To help such companies with the adoption of PLE, the goal of this paper is to provide inspiration and evidence that PLE is a sound approach and its successful introduction is possible even in settings that differ substantially from those of pioneer product lines.To this end, this paper presents the following main contributions with the PLE adoption case at Danfoss Drives: an overview of the key change drivers and the motivation for adopting a PLE approach, a discussion of incremental PLE introduction in an agile engineering context, a presentation of the current PLE setting with a focus on key concepts, and finally a presentation of motivators and directions for future improvements.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {252–261},
numpages = {10},
keywords = {product line evaluation, product line adoption, industrial experiences},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2791060.2791088,
author = {Domis, Dominik and Adler, Rasmus and Becker, Martin},
title = {Integrating variability and safety analysis models using commercial UML-based tools},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791088},
doi = {10.1145/2791060.2791088},
abstract = {Software and System Product Lines (SSPL) are the state-of-the-art for systematically reusing a common set of core assets in the development of similar products in a product family. A large number of SSPL success stories have been published in the last decade and commercial tool support is also available. SSPLs promise to reduce cost, to shorten time-to-market for new features, and to increase product quality by systematically reusing core assets in the development of three or more systems. However, an open challenge is SSPL engineering for safety-relevant systems such as automotive, avionic, or industrial automation systems. Safety-relevant systems have to be developed, analyzed, and certified according to safety standards such as IEC 61508. These standards demand the application of safety analyses such as Fault Tree Analysis and Failure Mode and Effect Analysis. Starting the safety analysis of each product variant of a SSPL from scratch is complex and very time-consuming. However, there are only few convincing cases, where SSPL approaches have been followed in safety engineering. To pave the way for a broader adoption of SSPL approaches, this paper reports practical experiences with industrial-strength methods and tools along an adaptive cruise control SSPL. The paper demonstrates how commercial tools can be used (i) to analyze safety-related aspects already in the architectural design, (ii) to model the results as component integrated component fault trees (C2FT), and (iii) to systematically reuse C2FT in the safety analysis of a concrete product. The results of the case study show that C2FT (i) can be easily integrated into a feature-oriented development process of SSPL, (ii) facilitate early consideration of safety in domain engineering, and (iii) reduce effort and complexity of safety analyses in application engineering.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {225–234},
numpages = {10},
keywords = {variant management, variability, tool support, software product line engineering, safety engineering, safety analysis, functional-safety, feature model, fault tree analysis},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3106195.3106207,
author = {Li, Yang and Schulze, Sandro and Saake, Gunter},
title = {Reverse Engineering Variability from Natural Language Documents: A Systematic Literature Review},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106207},
doi = {10.1145/3106195.3106207},
abstract = {Identifying features and their relations (i.e., variation points) is crucial in the process of migrating single software systems to software product lines (SPL). Various approaches have been proposed to perform feature extraction automatically from different artifacts, for instance, feature location in legacy code. Usually such approaches a) omit variability information and b) rely on artifacts that reside in advanced phases of the development process, thus, being only of limited usefulness in the context of SPLs. In contrast, feature and variability extraction from natural language (NL) documents is more favorable, because a mapping to several other artifacts is usually established from the very beginning. In this paper, we provide a multi-dimensional overview of approaches for feature and variability extraction from NL documents by means of a systematic literature review (SLR). We selected 25 primary studies and carefully evaluated them regarding different aspects such as techniques used, tool support, or accuracy of the results. In a nutshell, our key insights are that i) standard NLP techniques are commonly used, ii) post-processing often includes clustering &amp; machine learning algorithms, iii) only in rare cases, the approaches support variability extraction, iv) tool support, apart from text pre-processing is often not available, and v) many approaches lack a comprehensive evaluation. Based on these observations, we derive future challenges, arguing that more effort need to be invested for making such approaches applicable in practice.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {133–142},
numpages = {10},
keywords = {Variability Extraction, Systematic Literature Review, Software Product Lines, Reverse Engineering, Natural Language Documents, Feature Identification},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2491627.2491646,
author = {Marijan, Dusica and Gotlieb, Arnaud and Sen, Sagar and Hervieu, Aymeric},
title = {Practical pairwise testing for software product lines},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491646},
doi = {10.1145/2491627.2491646},
abstract = {One key challenge for software product lines is efficiently managing variability throughout their lifecycle. In this paper, we address the problem of variability in software product lines testing. We (1) identify a set of issues that must be addressed to make software product line testing work in practice and (2) provide a framework that combines a set of techniques to solve these issues. The framework integrates feature modelling, combinatorial interaction testing and constraint programming techniques. First, we extract variability in a software product line as a feature model with specified feature interdependencies. We then employ an algorithm that generates a minimal set of valid test cases covering all 2-way feature interactions for a given time interval. Furthermore, we evaluate the framework on an industrial SPL and show that using the framework saves time and provides better test coverage. In particular, our experiments show that the framework improves industrial testing practice in terms of (i) 17% smaller set of test cases that are (a) valid and (b) guarantee all 2-way feature coverage (as opposite to 19.2% 2-way feature coverage in the hand made test set), and (ii) full flexibility and adjustment of test generation to available testing time.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {227–235},
numpages = {9},
keywords = {variability management, software product lines, feature modelling},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3106195.3106213,
author = {Fu\ss{}berger, Nicolas and Zhang, Bo and Becker, Martin},
title = {A Deep Dive into Android's Variability Realizations},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106213},
doi = {10.1145/3106195.3106213},
abstract = {The open source Android operation system is widely used in both mobile consumer electronics as well as other industrial devices. It has actually become a variability-intensive system that can be highly customized to support different customers' requirements and hardware environments, which is a good inspiration for both practitioners and researchers. However, it is still unclear where and how variability is realized in its source code repository. In this paper, we conduct a systematic analysis on the variability realization of the Android operation system. The analysis focuses on the usage of different variability realization mechanisms (e.g., Conditional Compilation) in the Android source code and build environment. Finally, the study provides qualitative and quantitative results that help to understand i) what variability-specific artefacts exist in the Android source repository using which variability mechanisms and techniques; ii) how these artefacts express and instantiate variability along the layered Android realization architecture.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {69–78},
numpages = {10},
keywords = {Variability Realization, Variability Mechanisms, Android},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2491627.2491644,
author = {Rubin, Julia and Czarnecki, Krzysztof and Chechik, Marsha},
title = {Managing cloned variants: a framework and experience},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491644},
doi = {10.1145/2491627.2491644},
abstract = {In our earlier work, we have proposed a generic framework for managing collections of related products realized via cloning -- both in the case when such products are refactored into a single-copy software product line representation and the case when they are maintained as distinct clones. In this paper, we ground the framework in empirical evidence and exemplify its usefulness. In particular, we systematically analyze three industrial case studies of organizations with cloned product lines and derive the set of basic operators comprising the framework. We discuss options for implementing the operators and benefits of the operator-based view.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {101–110},
numpages = {10},
keywords = {legacy software product lines, industrial case studies, cloned product variants},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2791060.2791093,
author = {Souto, Sabrina and Gopinath, Divya and d'Amorim, Marcelo and Marinov, Darko and Khurshid, Sarfraz and Batory, Don},
title = {Faster bug detection for software product lines with incomplete feature models},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791093},
doi = {10.1145/2791060.2791093},
abstract = {A software product line (SPL) is a family of programs that are differentiated by features --- increments in functionality. Systematically testing an SPL is challenging because it requires running each test of a test suite against a combinatorial number of programs. Feature models capture dependencies among features and can (1) reduce the space of programs to test and (2) enable accurate categorization of failing tests as failures of programs or the tests themselves, not as failures due to illegal combinations of features. In practice, sadly, feature models are not always available.We introduce SPLif, the first approach for testing SPLs that does not require the a priori availability of feature models. Our insight is to use a profile of passing and failing test runs to quickly identify failures that are indicative of real problems in test or code rather than specious failures due to illegal feature combinations.Experimental results on five SPLs and one large configurable system (GCC) demonstrate the effectiveness of our approach. SPLif enabled the discovery of five news bugs in GCC, three of which have already been fixed.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {151–160},
numpages = {10},
keywords = {software testing, feature models, GCC},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2934466.2934492,
author = {Groher, Iris and Weinreich, Rainer and Buchgeher, Georg and Schossleitner, Robert},
title = {Reusable architecture variants for customer-specific automation solutions},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934492},
doi = {10.1145/2934466.2934492},
abstract = {Manufacturing execution systems (MES) are key elements of industrial automation systems. MES can be deployed at different levels of scale from a single site or plant to a company with globally distributed production sites all over the world. Establishing or extending an MES is a complex process, which requires taking the already existing software and system architecture into account in addition to the desired MES features. We developed an approach and an associated tool to support the process of creating offers for customer-specific MES solutions based on a vendor-specific automation platform. We define architecture variants for selecting a specific MES feature set and for supporting different MES expansion stages. Additionally, we provide an architecture modeling approach to explore the integration with existing software and system infrastructures. The approach has been applied at the STIWA Group, a vendor of MES for industrial production lines.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {242–251},
numpages = {10},
keywords = {manufacturing execution system (MES), feature set, customer-specific offer, automation platform, architecture variants},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2934466.2934485,
author = {Lape\~{n}a, Ra\'{u}l and Ballarin, Manuel and Cetina, Carlos},
title = {Towards clone-and-own support: locating relevant methods in legacy products},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934485},
doi = {10.1145/2934466.2934485},
abstract = {Clone-and-Own (CAO) is a common practice in families of software products consisting of reusing code from methods in legacy products in new developments. In industrial scenarios, CAO consumes high amounts of time and effort without guaranteeing good results. We propose a novel approach, Computer Assisted CAO (CACAO), that given the natural language requirements of a new product, and the legacy products from that family, ranks the legacy methods in the family for each of the new product requirements according to their relevancy to the new development. We evaluated our approach in the industrial domain of train control software. Without CACAO, software engineers tasked with the development of a new product had to manually review a total of 2200 methods in the family. Results show that CACAO can reduce the number of methods to be reviewed, and guide software engineers towards the identification of relevant legacy methods to be reused in the new product.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {194–203},
numpages = {10},
keywords = {software reuse, families of software products, clone and own},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2791060.2791111,
author = {Cordy, Maxime and Davril, Jean-Marc and Greenyer, Joel and Gressi, Erika and Heymans, Patrick},
title = {All-at-once-synthesis of controllers from scenario-based product line specifications},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791111},
doi = {10.1145/2791060.2791111},
abstract = {Software-intensive systems often consist of multiple components that interact to realize complex requirements. An additional dimension of complexity arises when one designs many variants of a system at once, that is, a software product line (SPL). We propose a scenario-based approach to design SPLs, based on a combination of Modal Sequence Diagrams (MSDs) and a feature model. It consists in associating every MSD to the set of variants that have to satisfy its specification. Variability constitutes a new source of complexity, which can lead to inconsistencies in the specification of one or multiple variants. It is therefore crucial to detect these inconsistencies, and to produce a controller for each variant that makes it behave so that it satisfies its specification. We present a new controller synthesis technique that checks the absence of inconsistencies in all variants at once, thereby more radically exploiting the similarities between them. Our method first translates the MSD specification into a variability-aware B\"{u}chi game, and then solves this game for all variants in a single execution. We implemented the approach in ScenarioTools, a software tool which we use to evaluate our algorithms against competing methods.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {26–35},
numpages = {10},
keywords = {message sequence diagrams, features, controller synthesis},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1145/2853073.2853082,
author = {Soujanya, K. L.S. and AnandaRao, A.},
title = {A Generic Framework for Configuration Management of SPL and Controlling Evolution of Complex Software Products},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853082},
doi = {10.1145/2853073.2853082},
abstract = {Efficient configuration management system is crucial for the success of any software product line (SPL). Due to ever changing needs of customers, SPL undergoes constant changes that are to be tracked in real time. In the context of customer-driven development, anticipation and change management are to be given paramount importance. It demands implementation of software variability that drives home changed, extended and customized configurations besides economy at scale. Moreover, the emergence of distributed technologies, the unprecedented growth of component based, serviceoriented systems throw ever increasing challenges to software product line configuration management. Derivation of a new product is a dynamic process in software product line that should consider functionality and quality attributes. Very few approaches are found on configuration management (CM) of SPL though CM is enough matured for traditional products. They are tailor made and inadequate to provide a general solution. Stated differently, a comprehensive approach for SPL configuration management and product derivation is still to be desired. In this paper, we proposed a framework that guides in doing so besides helping in SPL definitions in generic way. Our framework facilitates SPL configuration management and product derivation based on critical path analysis, weight computation and feedback. We proposed two algorithms namely Quality Driven Product Derivation (QDPD) and Composition Analysis algorithm for generating satisfied compositions and to find best possible composition respectively. The usage of weights and critical path analysis improves quality of product derivation. The framework is extensible and flexible thus it can be leveraged with variability-aware design patterns and ontology. We built a prototype that demonstrates the proof of concept. We tested our approach with Dr. School product line. The results reveal that the framework supports configuration management of SPL and derivation of high quality product in the product line. We evaluated results with ground truth to establish significance of our implementation},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–10},
numpages = {10},
keywords = {weighted approach, product derivation, critical path analysis, configuration management, Software product line}
}

@inproceedings{10.1145/2934466.2934475,
author = {Sousa, Gustavo and Rudametkin, Walter and Duchien, Laurence},
title = {Extending feature models with relative cardinalities},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934475},
doi = {10.1145/2934466.2934475},
abstract = {Feature modeling is widely used to capture and manage commonalities and variabilities in software product lines. Cardinality-based feature models are used when variability applies not only to the selection or exclusion of features but also to the number of times a feature can be included in a product. Feature cardinalities are usually considered to apply in either a local or global scope. However, we have identified that these interpretations are insufficient to capture the variability of cloud environments. In this paper, we redefine cardinality-based feature models to allow multiple relative cardinalities between features and we discuss the effects of relative cardinalities on feature modeling semantics, consistency and cross-tree constraints. To evaluate our approach we conducted an analysis of relative cardinalities in four cloud computing providers. In addition, we developed tools for reasoning on feature models with relative cardinalities and performed experiments to verify the performance and scalability of the approach. The results from our study indicate that extending feature models with relative cardinalities is feasible and improves variability modeling, particularly in the case of cloud environments.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {79–88},
numpages = {10},
keywords = {feature model, constraints, cardinality},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3382025.3414969,
author = {Bilic, Damir and Carlson, Jan and Sundmark, Daniel and Afzal, Wasif and Wallin, Peter},
title = {Detecting inconsistencies in annotated product line models},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414969},
doi = {10.1145/3382025.3414969},
abstract = {Model-based product line engineering applies the reuse practices from product line engineering with graphical modeling for the specification of software intensive systems. Variability is usually described in separate variability models, while the implementation of the variable systems is specified in system models that use modeling languages such as SysML. Most of the SysML modeling tools with variability support, implement the annotation-based modeling approach. Annotated product line models tend to be error-prone since the modeler implicitly describes every possible variant in a single system model. To identifying variability-related inconsistencies, in this paper, we firstly define restrictions on the use of SysML for annotative modeling in order to avoid situations where resulting instances of the annotated model may contain ambiguous model constructs. Secondly, inter-feature constraints are extracted from the annotated model, based on relations between elements that are annotated with features. By analyzing the constraints, we can identify if the combined variability- and system model can result in incorrect or ambiguous instances. The evaluation of our prototype implementation shows the potential of our approach by identifying inconsistencies in the product line model of our industrial partner which went undetected through several iterations of the model.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {20},
numpages = {11},
keywords = {variability modeling, product line engineering, model-based systems engineering, consistency checking, SysML},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2934466.2946046,
author = {Arrieta, Aitor and Wang, Shuai and Sagardui, Goiuria and Etxeberria, Leire},
title = {Search-based test case selection of cyber-physical system product lines for simulation-based validation},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946046},
doi = {10.1145/2934466.2946046},
abstract = {Cyber-Physical Systems (CPSs) are often tested at different test levels following "X-in-the-Loop" configurations: Model-, Software- and Hardware-in-the-loop (MiL, SiL and HiL). While MiL and SiL test levels aim at testing functional requirements at the system level, the HiL test level tests functional as well as non-functional requirements by performing a real-time simulation. As testing CPS product line configurations is costly due to the fact that there are many variants to test, test cases are long, the physical layer has to be simulated and co-simulation is often necessary. It is therefore extremely important to select the appropriate test cases that cover the objectives of each level in an allowable amount of time. We propose an efficient test case selection approach adapted to the "X-in-the-Loop" test levels. Search algorithms are employed to reduce the amount of time required to test configurations of CPS product lines while achieving the test objectives of each level. We empirically evaluate three commonly-used search algorithms, i.e., Genetic Algorithm (GA), Alternating Variable Method (AVM) and Greedy (Random Search (RS) is used as a baseline) by employing two case studies with the aim of integrating the best algorithm into our approach. Results suggest that as compared with RS, our approach can reduce the costs of testing CPS product line configurations by approximately 80% while improving the overall test quality.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {297–306},
numpages = {10},
keywords = {test case selection, search-based software engineering, cyber-physical system product lines},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2364412.2364417,
author = {Ripon, Shamim and Azad, Keya and Hossain, Sk Jahir and Hassan, Mehidee},
title = {Modeling and analysis of product-line variants},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364417},
doi = {10.1145/2364412.2364417},
abstract = {Formal verification of variant requirements has gained much interest in the software product line (SPL) community. Feature diagrams are widely used to model product line variants. However, there is a lack of precisely defined formal notation for representing and verifying such models. This paper presents an approach to modeling and analyzing SPL variant feature diagrams using first-order logic. It provides a precise and rigorous formal interpretation of the feature diagrams. Logical expressions can be built by modeling variants and their dependencies by using propositional connectives. These expressions can then be validated by any suitable verification tool such as Alloy. A case study of a Computer Aided Dispatch (CAD) system variant feature model is presented to illustrate the analysis and verification process.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {26–31},
numpages = {6},
keywords = {variants, product line, first-order logic, feature model, alloy},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791099,
author = {Filho, Jo\~{a}o Bosco Ferreira and Allier, Simon and Barais, Olivier and Acher, Mathieu and Baudry, Benoit},
title = {Assessing product line derivation operators applied to Java source code: an empirical study},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791099},
doi = {10.1145/2791060.2791099},
abstract = {Product Derivation is a key activity in Software Product Line Engineering. During this process, derivation operators modify or create core assets (e.g., model elements, source code instructions, components) by adding, removing or substituting them according to a given configuration. The result is a derived product that generally needs to conform to a programming or modeling language. Some operators lead to invalid products when applied to certain assets, some others do not; knowing this in advance can help to better use them, however this is challenging, specially if we consider assets expressed in extensive and complex languages such as Java. In this paper, we empirically answer the following question: which product line operators, applied to which program elements, can synthesize variants of programs that are incorrect, correct or perhaps even conforming to test suites? We implement source code transformations, based on the derivation operators of the Common Variability Language. We automatically synthesize more than 370,000 program variants from a set of 8 real large Java projects (up to 85,000 lines of code), obtaining an extensive panorama of the sanity of the operations.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {36–45},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3106195.3106210,
author = {Markiegi, Urtzi and Arrieta, Aitor and Sagardui, Goiuria and Etxeberria, Leire},
title = {Search-based product line fault detection allocating test cases iteratively},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106210},
doi = {10.1145/3106195.3106210},
abstract = {The large number of possible configurations makes it unfeasible to test every single system variant in a product line. Consequently, a small subset of the product line products must be selected, typically following combinatorial interaction testing approaches. Recently, many product line engineering approaches have considered the selection and prioritization of relevant products within the product line. In a further step, these products are thoroughly tested individually. However, the test cases that must be executed in each of the products are not always insignificant, and in systems such as Cyber-Physical System Product Lines (CPSPLs), their test execution time can vary from tens to thousands of seconds. This issue leads to spending a lot of time testing each individual product. To solve this problem we propose a search-based approach to perform the testing of product lines by allocating small number of test cases in each of the products. This approach increases the probability of detecting faults faster. Specifically, our search-based approach obtains a set of products, which are derived from using any state-of-the-art approach as inputs, and a set of attributed test cases. As an output a list of allocated test cases for each product is obtained. We also define a novel fitness function to guide the search and we propose corresponding crossover and mutation operators. The search and test process is iteratively repeated until the time budget is consumed. We performed an evaluation with a CPSPL as a case study. Results suggest that our approach can reduce the fault detection time by 61% and 65% on average when compared with the traditional test process and the Random Search algorithm respectively.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {123–132},
numpages = {10},
keywords = {Search-based Software Engineering, Product Line Testing, Fault Detection},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2866614.2866627,
author = {Devroey, Xavier and Perrouin, Gilles and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Search-based Similarity-driven Behavioural SPL Testing},
year = {2016},
isbn = {9781450340199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2866614.2866627},
doi = {10.1145/2866614.2866627},
abstract = {Dissimilar test cases have been proven to be effective to reveal faults in software systems. In the Software Product Line (SPL) context, this criterion has been applied successfully to mimic combinatorial interaction testing in an efficient and scalable manner by selecting and prioritising most dissimilar configurations of feature models using evolutionary algorithms. In this paper, we extend dissimilarity to behavioural SPL models (FTS) in a search-based approach, and evaluate its effectiveness in terms of product and fault coverage. We investigate different distances as well as as single-objective algorithms, (dissimilarity on actions, random, all-actions). Our results on four case studies show the relevance of dissimilarity-based test generation for behavioural SPL models, especially on the largest case-study where no other approach can match it.},
booktitle = {Proceedings of the 10th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {89–96},
numpages = {8},
keywords = {Software Product Line Testing, Featured Transition System, Dissimilarity Testing},
location = {Salvador, Brazil},
series = {VaMoS '16}
}

@inproceedings{10.1145/2491627.2491630,
author = {Linsbauer, Lukas and Lopez-Herrejon, E. Roberto and Egyed, Alexander},
title = {Recovering traceability between features and code in product variants},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491630},
doi = {10.1145/2491627.2491630},
abstract = {Many companies offer a palette of similar software products though they do not necessarily have a Software Product Line (SPL). Rather, they start building and selling individual products which they then adapt, customize and extend for different customers. As the number of product variants increases, these companies then face the severe problem of having to maintain them all. Software Product Lines can be helpful here - not so much as a platform for creating new products but as a means of maintaining the existing ones with their shared features. Here, an important first step is to determine where features are implemented in the source code and in what product variants. To this end, this paper presents a novel technique for deriving the traceability between features and code in product variants by matching code overlaps and feature overlaps. This is a difficult problem because a feature's implementation not only covers its basic functionality (which does not change across product variants) but may include code that deals with feature interaction issues and thus changes depending on the combination of features present in a product variant. We empirically evaluated the approach on three non-trivial case studies of different sizes and domains and found that our approach correctly identifies feature to code traces except for code that traces to multiple disjunctive features, a rare case involving less than 1% of the code.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {131–140},
numpages = {10},
keywords = {traceability, product variants, features},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3307630.3342414,
author = {Th\"{u}m, Thomas and Teixeira, Leopoldo and Schmid, Klaus and Walkingshaw, Eric and Mukelabai, Mukelabai and Varshosaz, Mahsa and Botterweck, Goetz and Schaefer, Ina and Kehrer, Timo},
title = {Towards Efficient Analysis of Variation in Time and Space},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342414},
doi = {10.1145/3307630.3342414},
abstract = {Variation is central to today's software development. There are two fundamental dimensions to variation: Variation in time refers to the fact that software exists in numerous revisions that typically replace each other (i.e., a newer version supersedes an older one). Variation in space refers to differences among variants that are designed to coexist in parallel. There are numerous analyses to cope with variation in space (i.e., product-line analyses) and others that cope with variation in time (i.e., regression analyses). The goal of this work is to discuss to which extent product-line analyses can be applied to revisions and, conversely, where regression analyses can be applied to variants. In addition, we discuss challenges related to the combination of product-line and regression analyses. The overall goal is to increase the efficiency of analyses by exploiting the inherent commonality between variants and revisions.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {57–64},
numpages = {8},
keywords = {variability-aware analysis, variability management, software variation, software product lines, software evolution, software configuration management, regression analysis, product-line analysis},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2362536.2362549,
author = {Cordy, Maxime and Schobbens, Pierre-Yves and Heymans, Patrick and Legay, Axel},
title = {Behavioural modelling and verification of real-time software product lines},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362549},
doi = {10.1145/2362536.2362549},
abstract = {In Software Product Line (SPL) engineering, software products are build in families rather than individually. Many critical software are nowadays build as SPLs and most of them obey hard real-time requirements. Formal methods for verifying SPLs are thus crucial and actively studied. The verification problem for SPL is, however, more complicated than for individual systems; the large number of different software products multiplies the complexity of SPL model-checking. Recently, promising model-checking approaches have been developed specifically for SPLs. They leverage the commonality between the products to reduce the verification effort. However, none of them considers real time.In this paper, we combine existing SPL verification methods with established model-checking procedures for real-time systems. We introduce Featured Timed Automata (FTA), a formalism that extends the classical Timed Automata with constructs for modelling variability. We show that FTA model-checking can be achieved through a smart combination of real-time and SPL model checking.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {66–75},
numpages = {10},
keywords = {software product lines, real-time, model checking, features},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791087,
author = {ter Beek, M. H. and Legay, A. and Lafuente, A. Lluch and Vandin, A.},
title = {Statistical analysis of probabilistic models of software product lines with quantitative constraints},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791087},
doi = {10.1145/2791060.2791087},
abstract = {We investigate the suitability of statistical model checking for the analysis of probabilistic models of software product lines with complex quantitative constraints and advanced feature installation options. Such models are specified in the feature-oriented language QFLan, a rich process algebra whose operational behaviour interacts with a store of constraints, neatly separating product configuration from product behaviour. The resulting probabilistic configurations and behaviour converge seamlessly in a semantics based on DTMCs, thus enabling quantitative analyses ranging from the likelihood of certain behaviour to the expected average cost of products. This is supported by a Maude implementation of QFLan, integrated with the SMT solver Z3 and the distributed statistical model checker MultiVeStA. Our approach is illustrated with a bikes product line case study.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {11–15},
numpages = {5},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3106195.3106216,
author = {Li, Yi and Zhu, Chenguang and Rubin, Julia and Chechik, Marsha},
title = {FHistorian: Locating Features in Version Histories},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106216},
doi = {10.1145/3106195.3106216},
abstract = {Feature location techniques aim to locate software artifacts that implement a specific program functionality, a.k.a. a feature. In this paper, we build upon the previous work of semantic history slicing to locate features in software version histories. We leverage the information embedded in version histories for identifying changes implementing features and discovering relationships between features.The identified feature changes are fully functional and guaranteed to preserve the desired behaviors. The resulting feature relationship graph is precise and can be used to assist in understanding of the underlying connections between the features.We evaluate the technique on a number of real-world case studies and compare our results with developer-specified feature annotations.We conclude that, when available, historical information of software changes can lead to precise identification of features in existing software artifacts.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {49–58},
numpages = {10},
keywords = {version history, feature relationship, Feature location},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2019136.2019187,
author = {Abbas, Nadeem},
title = {Towards autonomic software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019187},
doi = {10.1145/2019136.2019187},
abstract = {We envision an Autonomic Software Product Line (ASPL). The ASPL is a dynamic software product line that supports self adaptable products. We plan to use reflective architecture to model and develop ASPL. To evaluate the approach, we have implemented three autonomic product lines which show promising results. The ASPL approach is at initial stages, and require additional work. We plan to exploit online learning to realize more dynamic software product lines to cope with the problem of product line evolution. We propose on-line knowledge sharing among products in a product line to achieve continuous improvement of quality in product line products.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {44},
numpages = {8},
keywords = {self-adaptation, on-line learning, knowledge},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2791060.2791100,
author = {ter Beek, Maurice H. and Fantechi, Alessandro and Gnesi, Stefania},
title = {Applying the product lines paradigm to the quantitative analysis of collective adaptive systems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791100},
doi = {10.1145/2791060.2791100},
abstract = {Engineering a Collective Adaptive System (CAS) requires the support of a framework for quantitative modeling and analysis of the system. In order to jointly address variability and quantitative analysis, we apply the Product Lines paradigm, considered at the level of system engineering, to a case study of the European project QUANTICOL, by first defining a reference feature model and then adding feature attributes and global quantitative constraints, in the form of a Clafer attributed feature model. ClaferMOOVisualizer is subsequently used for quantitative analyses and multi-objective optimization of the resulting attributed feature model.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {321–326},
numpages = {6},
keywords = {variability analysis, quantitative modeling, quantitative analysis, multi-objective optimization, collective adaptive systems, ClaferMOO},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2362536.2362553,
author = {Andersen, Nele and Czarnecki, Krzysztof and She, Steven and W\k{a}sowski, Andrzej},
title = {Efficient synthesis of feature models},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362553},
doi = {10.1145/2362536.2362553},
abstract = {Variability modeling, and in particular feature modeling, is a central element of model-driven software product line architectures. Such architectures often emerge from legacy code, but, unfortunately creating feature models from large, legacy systems is a long and arduous task.We address the problem of automatic synthesis of feature models from propositional constraints. We show that this problem is NP-hard. We design efficient techniques for synthesis of models from respectively CNF and DNF formulas, showing a 10- to 1000-fold performance improvement over known techniques for realistic benchmarks.Our algorithms are the first known techniques that are efficient enough to be applied to dependencies extracted from real systems, opening new possibilities of creating reverse engineering and model management tools for variability models. We discuss several such scenarios in the paper.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {106–115},
numpages = {10},
keywords = {variability models, software product lines, feature models},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3442391.3442397,
author = {Silva, Leandro Flores da and OliveiraJr, Edson},
title = {SMartyModeling: an Environment for Engineering UML-based Software Product Lines},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442397},
doi = {10.1145/3442391.3442397},
abstract = {Software Product Line (SPL) has been successfully consolidated as an approach for systematic reuse. The adoption of the SPL approach aims at increasing the reuse of requirements and artifacts, thus reusing documents, source code and artifacts and ensuring better quality control to software production in a large-scale. One of the essential activities for SPL management is the modeling of variability. Variability modeling in UML-based SPL has been carried out mostly using the UML Profiling mechanism, in which new stereotypes and tagged values are created for such purpose. The available option in general-purpose UML tools for exporting UML models is through XMI files, standardized by OMG. This option is important to process XMI files in particular environments or tools, for example, managing variabilities, generating product configurations from an SPL, and even collecting metrics, and estimating SPLs. However, different versions, tool restrictions, and different file standards compromise operations involving XMI files. In this scenario, the industry has increasingly required the supporting tools for the SPL approach. However, the current support tools are mainly restricted to the problem space based on feature modeling and present problems with data integration with other tools. Therefore, we developed SMartyModeling, an environment for engineering UML-based SPLs in which variabilities are modeled as stereotypes using any UML compliant profile. This paper presents an overview of SMartyModeling, describing its motivation, main components, and available features.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {5},
numpages = {5},
keywords = {variability modeling, software product line, UML, SMarty},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.1145/2491627.2491636,
author = {Kato, Tadahisa and Kawakami, Masumi and Myojin, Tomoyuki and Ogawa, Hideto and Hirono, Koji and Hasegawa, Takashi},
title = {Case study of applying SPLE to development of network switch products},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491636},
doi = {10.1145/2491627.2491636},
abstract = {Software product line engineering has spread as a technique for promoting the efficient development of embedded products with many product line-ups. During the development of network switch products at Hitachi Metals, Ltd., the number of development man-months increased as the number of product line-ups increased. Therefore, we shifted our development paradigm to product line development for efficient product development. We classified software assets as implementation assets, test assets, and design assets, and from these three assets, we extracted common objects and integrated them as reusable elements. By doing so, we promoted the efficient development of software assets and reduced the contradictions between the contents of the software assets. As a result, we reduced the amount of the source code by 53.1%. In this paper, we discuss the details of our technique and the effect of applying it. In addition, we discuss how you can apply our technique in the development of other products.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {198–207},
numpages = {10},
keywords = {test automation, software reuse, software maintenance, software integration, document integration},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2362536.2362567,
author = {Savolainen, Juha and Mannion, Mike and Kuusela, Juha},
title = {Developing platforms for multiple software product lines},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362567},
doi = {10.1145/2362536.2362567},
abstract = {Many approaches to software product line engineering have been founded on the development of a single product line platform. However as customer requirements change and new products are added to the product line, software producers recognize that the platform cannot be "stretched" indefinitely and a significant problem is striking a balance between development efficiency by increasing platform commonality and customer dissatisfaction from products with additional undesirable features and properties.One alternative is to develop multiple product lines (MPLs). However the challenge remains about what to include in a multiple product line platform. Drawing upon industrial experience of working with 4 companies, this paper explores the characteristics of the contexts in which MPLs are a viable alternative development strategy and then proposes a framework of approaches to platform development.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {220–228},
numpages = {9},
keywords = {software reuse, multiple product lines, industrial experience},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1145/3361146,
author = {Hierons, Robert M. and Li, Miqing and Liu, Xiaohui and Parejo, Jose Antonio and Segura, Sergio and Yao, Xin},
title = {Many-Objective Test Suite Generation for Software Product Lines},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3361146},
doi = {10.1145/3361146},
abstract = {A Software Product Line (SPL) is a set of products built from a number of features, the set of valid products being defined by a feature model. Typically, it does not make sense to test all products defined by an SPL and one instead chooses a set of products to test (test selection) and, ideally, derives a good order in which to test them (test prioritisation). Since one cannot know in advance which products will reveal faults, test selection and prioritisation are normally based on objective functions that are known to relate to likely effectiveness or cost. This article introduces a new technique, the grid-based evolution strategy (GrES), which considers several objective functions that assess a selection or prioritisation and aims to optimise on all of these. The problem is thus a many-objective optimisation problem. We use a new approach, in which all of the objective functions are considered but one (pairwise coverage) is seen as the most important. We also derive a novel evolution strategy based on domain knowledge. The results of the evaluation, on randomly generated and realistic feature models, were promising, with GrES outperforming previously proposed techniques and a range of many-objective optimisation algorithms.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {2},
numpages = {46},
keywords = {test selection, test prioritisation, multi-objective optimisation, Software product line}
}

@inproceedings{10.1145/2934466.2934477,
author = {Krieter, Sebastian and Schr\"{o}ter, Reimar and Th\"{u}m, Thomas and Fenske, Wolfram and Saake, Gunter},
title = {Comparing algorithms for efficient feature-model slicing},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934477},
doi = {10.1145/2934466.2934477},
abstract = {Feature models are a well-known concept to represent variability in software product lines by defining features and their dependencies. During feature-model evolution, for information hiding, and for feature-model analyses, it is often necessary to remove certain features from a model. As the crude deletion of features can have undesirable effects on their dependencies, dependency-preserving algorithms, known as feature-model slicing, have been proposed. However, current algorithms do not perform well when removing a high number of features from large feature models. Therefore, we propose an efficient algorithm for feature-model slicing based on logical resolution and the minimization of logical formulas. We empirically evaluate the scalability of our algorithm on a number of feature models and find that our algorithm generally outperforms existing algorithms.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {60–64},
numpages = {5},
keywords = {software product lines, feature-model evolution, feature-model analyses},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3321408.3326676,
author = {Yan, Liu and Hu, Wenxin and Han, Longzhe},
title = {Optimize SPL test cases with adaptive simulated annealing genetic algorithm},
year = {2019},
isbn = {9781450371582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321408.3326676},
doi = {10.1145/3321408.3326676},
abstract = {In Software Product Line (SPL) testing, reduced test suite with high coverage is useful for early features interaction detection. sGA (simplified genetic algorithm) and SAGA(simulated annealing genetic algorithm) can generate high coverage test suite. However, small probability mutations in updating test suite may reduce search efficiency and thus miss better solutions. An improved test cases generation method based on ASAGA (Adaptive simulated annealing genetic algorithm) is proposed. Experiments on SPLOT (Software Product Lines Online Tools) feature models show that the proposed hybrid ASAGA method can ensure local optimization accuracy and achieve smaller-size test suite with higher coverage.},
booktitle = {Proceedings of the ACM Turing Celebration Conference - China},
articleno = {148},
numpages = {7},
keywords = {test case, software test, similarity measurement, feature model, ASAGA},
location = {Chengdu, China},
series = {ACM TURC '19}
}

@inproceedings{10.1145/3233027.3233046,
author = {Beek, Maurice H. ter and Fantechi, Alessandro and Gnesi, Stefania},
title = {Product line models of large cyber-physical systems: the case of ERTMS/ETCS},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233046},
doi = {10.1145/3233027.3233046},
abstract = {A product line perspective may help to understand the possible variants in interactions between the subsystems of a large, cyber-physical system. This observation is exemplified in this paper by proposing a feature model of the family of ERTMS/ETCS train control systems and their foreseen extensions. This model not only shows the different components that have to be installed when deploying the system at the different levels established by the ERTMS/ETCS standards, but it also helps to identify and discuss specific issues, such as the borders between onboard and wayside equipment, different manufacturers of the subsystems, interoperability among systems developed at different levels, backward compatibility of trains equipped with higher level equipment running on lines equipped with lower level equipment, and evolution towards future trends of railway signalling. The feature model forms the basis for formal modelling of the behaviour of the critical components of the system and for evaluating the overall cost, effectiveness and sustainability, for example by adding cost and performance attributes to the feature model.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {208–214},
numpages = {7},
keywords = {variability, product lines, feature models, cyber-physical systems, ERTMS/ETCS train control systems},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2791060.2791077,
author = {Rumpe, Bernhard and Schulze, Christoph and von Wenckstern, Michael and Ringert, Jan Oliver and Manhart, Peter},
title = {Behavioral compatibility of simulink models for product line maintenance and evolution},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791077},
doi = {10.1145/2791060.2791077},
abstract = {Embedded software systems, e.g. automotive, robotic or automation systems are highly configurable and consist of many software components being available in different variants and versions. To identify the degree of reusability between these different occurrences of a component, it is necessary to determine the functional backward and forward compatibility between them. Based on this information it is possible to identify in which system context a component can be replaced safely by another version, e.g. exchanging an older component, or variant, e.g. introducing new features, to achieve the same functionality.This paper presents a model checking approach to determine behavioral compatibility of Simulink models, obtained from different component variants or during evolution. A prototype for automated compatibility checking demonstrates its feasibility. In addition implemented optimizations make the analysis more efficient, when the compared variants or versions are structurally similar.A case study on a driver assistance system provided by Daimler AG shows the effectiveness of the approach to automatically compare Simulink components.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {141–150},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2791060.2791080,
author = {Van Landuyt, Dimitri and Walraven, Stefan and Joosen, Wouter},
title = {Variability middleware for multi-tenant SaaS applications: a research roadmap for service lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791080},
doi = {10.1145/2791060.2791080},
abstract = {Software product line engineering (SPLE) and variability enforcement techniques have been applied to run-time adaptive systems for quite some years, also in the context of multi-tenant Software-as-a-Service (SaaS) applications. The focus has been mainly on (1) the pre-deployment phases of the development life cycle and (2) fine-grained (tenant-level), run-time activation of specific variants. However, with upcoming trends such as DevOps and continuous delivery and deployment, operational aspects become increasingly important.In this paper, we present our integrated vision on the positive interplay between SPLE and adaptive middleware for multi-tenant SaaS applications, focusing on the operational aspects of running and maintaining a successful SaaS offering. This vision, called Service Lines, is based on and motivated by our experience and frequent interactions with a number of Belgian SaaS providers.We concretely highlight and motivate a number of operational use cases that require advanced variability support in middleware and have promising added value for the economic feasibility of SaaS offerings. In addition, we provide a gap analysis of what is currently lacking from the perspectives of variability modeling and management techniques and middleware support, and as such sketch a concrete roadmap for continued research in this area.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {211–215},
numpages = {5},
keywords = {variability middleware, service lines, run-time variability, operational support, multi-tenant SaaS, models at run time},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2362536.2362547,
author = {Johansen, Martin Fagereng and Haugen, \O{}ystein and Fleurey, Franck},
title = {An algorithm for generating t-wise covering arrays from large feature models},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362547},
doi = {10.1145/2362536.2362547},
abstract = {A scalable approach for software product line testing is required due to the size and complexity of industrial product lines. In this paper, we present a specialized algorithm (called ICPL) for generating covering arrays from feature models. ICPL makes it possible to apply combinatorial interaction testing to software product lines of the size and complexity found in industry. For example, ICPL allows pair-wise testing to be readily applied to projects of about 7,000 features and 200,000 constraints, the Linux Kernel, one of the largest product lines where the feature model is available. ICPL is compared to three of the leading algorithms for t-wise covering array generation. Based on a corpus of 19 feature models, data was collected for each algorithm and feature model when the algorithm could finish 100 runs within three days. These data are used for comparing the four algorithms. In addition to supporting large feature models, ICPL is quick, produces small covering arrays and, even though it is non-deterministic, produces a covering array of a similar size within approximately the same time each time it is run with the same feature model.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {46–55},
numpages = {10},
keywords = {testing, product lines, feature models, combinatorial interaction testing},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791074,
author = {Reuling, Dennis and B\"{u}rdek, Johannes and Rot\"{a}rmel, Serge and Lochau, Malte and Kelter, Udo},
title = {Fault-based product-line testing: effective sample generation based on feature-diagram mutation},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791074},
doi = {10.1145/2791060.2791074},
abstract = {Testing every member of a product line individually is often impracticable due to large number of possible product configurations. Thus, feature models are frequently used to generate samples, i.e., subsets of product configurations under test. Besides the extensively studied combinatorial interaction testing (CIT) approach for coverage-driven sample generation, only few approaches exist so far adopting mutation testing to emulate faults in feature models to be detected by a sample. In this paper, we present a mutation-based sampling framework for fault-based product-line testing. We define a comprehensive catalog of atomic mutation operators on the graphical representation of feature models. This way, we are able (1) to also define complex mutation operators emulating more subtle faults, and (2) to classify operators semantically, e.g., to avoid redundant and equivalent mutants. We further introduce similarity-based mutant selection and higher order mutation strategies to reduce testing efforts. Our implementation is based on the graph transformation engine Henshin and is evaluated concerning effectiveness/efficiency trade-offs.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {131–140},
numpages = {10},
keywords = {mutation testing, combinatorial interaction testing},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2791060.2791102,
author = {Mu\~{n}oz-Fern\'{a}ndez, Juan C. and Tamura, Gabriel and Raicu, Irina and Mazo, Ra\'{u}l and Salinesi, Camille},
title = {REFAS: a PLE approach for simulation of self-adaptive systems requirements},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791102},
doi = {10.1145/2791060.2791102},
abstract = {Model simulation has demonstrated its usefulness in evaluation and decision-making for improving preliminary versions of artefacts before production. Particularly, one of the main goals of simulation is to verify model properties based on data collected from its execution. In this paper, we present the simulation capabilities of our REFAS framework for specifying requirements models for dynamic software products lines and self-adaptive systems. The simulation is controlled by a feedback loop and a reasoning engine that operates on the functional and non-functional requirements. The paper contribution is threefold. First, REFAS allows developers to evaluate and improve requirements models through their simulation capabilities. Second, REFAS provides rich feedback in its interactive simulations for the human modeller to make informed decisions to improve her model. Third, REFAS automates the generation of simulation scenarios required to verify the model adequacy and correctness. We evaluate our contribution by comparing the application of REFAS to a case study used in other approaches.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {121–125},
numpages = {5},
keywords = {simulation, requirements engineering, dynamic software product lines, dynamic adaptation, MAPE-K loops},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3483899.3483905,
author = {Freire, Willian and Tonh\~{a}o, Simone and Bonetti, Tiago and Shigenaga, Marcelo and Cadette, William and Felizardo, Fernando and Amaral, Aline and OliveiraJr, Edson and Colanzi, Thelma},
title = {On the configuration of multi-objective evolutionary algorithms for PLA design optimization},
year = {2021},
isbn = {9781450384193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483899.3483905},
doi = {10.1145/3483899.3483905},
abstract = {Search-based algorithms have been successfully applied in the Product Line Architecture (PLA) optimization using the seminal approach called Multi-Objective Approach for Product-Line Architecture Design (MOA4PLA). This approach produces a set of alternative PLA designs intending to improve the different factors being optimized. Currently, the MOA4PLA uses the NSGA-II algorithm, a multi-objective evolutionary algorithm (MOEA) that can optimize several architectural properties simultaneously. Despite the promising results, studying the best values for the algorithm parameters is essential to obtain even better results. This is also crucial to ease the adoption of MOA4PLA by newcomers or non-expert companies willing to start using search-based software engineering to PLA design. Three crossover operators for the PLA design optimization were proposed recently. However, reference values for parameters have not been defined for PLA design optimization using crossover operators. In this context, the objective of this work is conducting an experimental study to discover which are the most effective crossover operators and the best values to configure the MOEA parameters, such as population size, number of generations, and mutation and crossover rates. A quantitative analysis based on quality indicators and statistical tests was performed using four PLA designs to determine the most suitable parameter values to the search-based algorithm. Empirical results pointed out the best combination of crossover operators and the most suitable values to configure MOA4PLA.},
booktitle = {Proceedings of the 15th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {software product line, software architecture, recombination operators, Multi-objective evolutionary algorithm},
location = {Joinville, Brazil},
series = {SBCARS '21}
}

@inproceedings{10.1145/2791060.2791107,
author = {Ji, Wenbin and Berger, Thorsten and Antkiewicz, Michal and Czarnecki, Krzysztof},
title = {Maintaining feature traceability with embedded annotations},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791107},
doi = {10.1145/2791060.2791107},
abstract = {Features are commonly used to describe functional and nonfunctional aspects of software. To effectively evolve and reuse features, their location in software assets has to be known. However, locating features is often difficult given their crosscutting nature. Once implemented, the knowledge about a feature's location quickly deteriorates, requiring expensive recovering of these locations. Manually recording and maintaining traceability information is generally considered expensive and error-prone. In this paper, we argue to the contrary and hypothesize that such information can be effectively embedded into software assets, and that arising costs will be amortized by the benefits of this information later during development. We test this hypothesis in a study where we simulate the development of a product line of cloned/forked projects using a lightweight code annotation approach. We identify annotation evolution patterns and measure the cost and benefit of these annotations. Our results show that not only the cost of adding annotations, but also that of maintaining them is small compared to the actual development cost. Embedding the annotations into assets significantly reduced the maintenance cost because they naturally co-evolve with the assets. Our results also show that a majority of these annotations provides a benefit for feature-related code maintenance tasks, such as feature propagation and migrating clones into a platform.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {61–70},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3382025.3414985,
author = {Halin, Axel and Nuttinck, Alexandre and Acher, Mathieu and Devroey, Xavier and Perrouin, Gilles and Baudry, Benoit},
title = {Test them all, is it worth it? assessing configuration sampling on the JHipster web development stack},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414985},
doi = {10.1145/3382025.3414985},
abstract = {This is an extended abstract of the article: Axel Halin, Alexandre Nuttinck, Mathieu Acher, Xavier Devroey, Gilles Perrouin, and Benoit Baudry. 2018. Test them all, is it worth it? Assessing configuration sampling on the JHipster Web development stack. In Empirical Software Engineering (17 Jul 2018). https://doi.org/10.1007/s10664-018-9635-4.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {12},
numpages = {1},
keywords = {variability-intensive systems, software testing, configuration sampling, case study, JHipster},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3336294.3336319,
author = {Carvalho, Luiz and Garcia, Alessandro and Assun\c{c}\~{a}o, Wesley K. G. and Bonif\'{a}cio, Rodrigo and Tizzei, Leonardo P. and Colanzi, Thelma Elita},
title = {Extraction of Configurable and Reusable Microservices from Legacy Systems: An Exploratory Study},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336319},
doi = {10.1145/3336294.3336319},
abstract = {Microservices is an emerging industrial technique to promote better modularization and management of small and autonomous services. Microservice architecture is widely used to overcome the limitations of monolithic legacy systems, such as limited maintainability and reusability. Migration to a microservice architecture is increasingly becoming the focus of academic research. However, there is little knowledge on how microservices are extracted from legacy systems in practice. Among these limitations, there is a lack of understanding if variability is considered useful along the microservice extraction from a configurable system. In order to address this gap, we performed an exploratory study composed of two phases. Firstly, we conducted an online survey with 26 specialists that contributed to the migration of existing systems to a microservice architecture. Secondly, we performed individual interviews with seven survey participants. A subset of the participants (13 out of 26) dealt with systems with variability during the extraction, which stated that variability is a key criterion for structuring the microservices. Moreover, variability in the legacy system is usually implemented with simple mechanisms. Finally, initial evidence points out that microservices extraction can increase software customization.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {26–31},
numpages = {6},
keywords = {software variability, microservice customization, microservice architecture, architecture migration},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2791060.2791072,
author = {Patel, Sachin and Shah, Vipul},
title = {Automated testing of software-as-a-service configurations using a variability language},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791072},
doi = {10.1145/2791060.2791072},
abstract = {The benefits offered by cloud technologies have compelled enterprises to adopt the Software-as-a-Service (SaaS) model for their enterprise software needs. A SaaS has to be configured or customized to suit the specific requirements of every enterprise that subscribes to it. IT service providers have to deal with the problem of testing many such configurations created for different enterprises. The software gets upgraded periodically and the configurations need to be tested on an ongoing basis to ensure business continuity. In order to run the testing organization efficiently, it is imperative that the test cycle is automated. Developing automated test scripts for a large number of configurations is a non-trivial task because differences across them may range from a few user interface changes to business process level changes. We propose an approach that combines the benefits of model driven engineering and variability modeling to address this issue. The approach comprises of the Enterprise Software Test Modeling Language to model the test cases. We use the Common Variability Language to model variability in the test cases and apply model transformations on a base model to generate a test model for each configuration. These models are used to generate automated test scripts for all the configurations. We describe the test modelling language and an experiment which shows that the approach can be used to automatically generate variations in automated test scripts.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {253–262},
numpages = {10},
keywords = {variability specification, test automation, software-as-a-service, model based testing, enterprise software testing},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2648511.2648528,
author = {Barreiros, Jorge and Moreira, Ana},
title = {A cover-based approach for configuration repair},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648528},
doi = {10.1145/2648511.2648528},
abstract = {Feature models are often used to describe variability and commonality in Software Product Lines, specifying admissible configurations of valid products. However, invalid configurations may arise in some scenarios. These include feature model evolution that invalidates pre-existing products or collaborative configuration by multiple stakeholders with conflicting goals, among others. This problem has been acknowledged in the literature and some techniques for configuration repair have already been proposed. However, common optimization criteria such as proximity between original and repaired configurations can result in a significant number of alternative repair possibilities, easily attaining thousands of alternatives for models of practical dimension. Consequently, rather than just efficiently providing an exhaustive list of possibilities, an approach that specifically addresses this issue should be able to offer the user a manageable and comprehensible view of the configuration problems and potential repair options. We offer a novel approach for configuration repair, based on partitioning and cover analysis, with high performance and generating high quality solutions, which allows efficient identification and presentation of multiple competing repairs.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {157–166},
numpages = {10},
keywords = {software product lines, feature modeling, configuration repair, configuration diagnosis, configuration},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2791060.2791114,
author = {Beuche, Danilo and Hellebrand, Robert},
title = {Using pure::variants across the product line lifecycle},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791114},
doi = {10.1145/2791060.2791114},
abstract = {The paper describes a demonstration of pure::variants, a commercial tool for variant and variability management for product lines. The demonstration shows how flexible product line (PL) architectures can be built, tested and maintained by using the modeling and integration capabilities provided by pure::variants [2].},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {352–354},
numpages = {3},
keywords = {tools, software product lines, feature modelling},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2791060.2791066,
author = {Dhungana, Deepak and Falkner, Andreas and Haselb\"{o}ck, Alois and Schreiner, Herwig},
title = {Smart factory product lines: a configuration perspective on smart production ecosystems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791066},
doi = {10.1145/2791060.2791066},
abstract = {Smart production aims to increase the flexibility of the production processes and be more efficient in the use of resources. Two important pillars of this initiative are "smart products" and "smart factories". From the perspective of product line engineering, these can be seen as two product lines (product line of factories and product line of goods) that need to be integrated for a common systems engineering approach. In this paper, we look at this problem from the perspective of configuration technologies, outline the research challenges in this area and illustrate our vision using an industrial example. The factory product line goes hand-in-hand with the product line of the products to be manufactured. Future research in product line engineering needs to consider an ecosystem of a multitude of stakeholders - e.g., factory component vendors, product designers, factory owners/operators and end-consumers.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {201–210},
numpages = {10},
keywords = {smart production, smart product, smart factory, product line of factories, product and production configuration},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2647908.2655973,
author = {Cordy, Maxime and Willemart, Marco and Dawagne, Bruno and Heymans, Patrick and Schobbens, Pierre-Yves},
title = {An extensible platform for product-line behavioural analysis},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655973},
doi = {10.1145/2647908.2655973},
abstract = {Software Product-Line (SPL) model checking has reached an adequate level of efficiency and expressiveness to be applied on real-world cases. Yet a major challenge remains: model checkers should consist of black-box tools that do not require in-depth expertise to be used. In particular, it is essential to provide engineers with easy-to-learn languages to model both the behaviour of their SPL and the properties to check. In this paper, we propose a framework to build customized product-line verifiers modularly. Our extensible architecture allows one to plug new modelling languages or verifications algorithms without modifying other parts of it. It also provides means of representing and reasoning on variability that can facilitate the development of other SPL quality assurance techniques. We illustrate the benefits of our approach by detailing how we created a new domain-specific SPL modelling language and linked it to our tool.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {102–109},
numpages = {8},
keywords = {tool, software product lines, model checking, features},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.5555/1753235.1753285,
author = {Schmid, Klaus and van der Linden, Frank},
title = {Introducing and optimizing software product lines using the FEF},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {This tutorial provides an introduction to the basic concepts of software product line engineering: business orientation, variability management, architecture-driven development, and the two-lifecycle approach.Based on this introduction the Families Evaluation Framework (FEF) is described. This is a systematic approach to evaluating a product line organization and its development approach. The material in this tutorial also provides the basis for the tutorial on inner source software product lines.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {311},
numpages = {1},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2491627.2491635,
author = {Henard, Christopher and Papadakis, Mike and Perrouin, Gilles and Klein, Jacques and Traon, Yves Le},
title = {Multi-objective test generation for software product lines},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491635},
doi = {10.1145/2491627.2491635},
abstract = {Software Products Lines (SPLs) are families of products sharing common assets representing code or functionalities of a software product. These assets are represented as features, usually organized into Feature Models (FMs) from which the user can configure software products. Generally, few features are sufficient to allow configuring millions of software products. As a result, selecting the products matching given testing objectives is a difficult problem.The testing process usually involves multiple and potentially conflicting testing objectives to fulfill, e.g. maximizing the number of optional features to test while at the same time both minimizing the number of products and minimizing the cost of testing them. However, most approaches for generating products usually target a single objective, like testing the maximum amount of feature interactions. While focusing on one objective may be sufficient in certain cases, this practice does not reflect real-life testing situations.The present paper proposes a genetic algorithm to handle multiple conflicting objectives in test generation for SPLs. Experiments conducted on FMs of different sizes demonstrate the effectiveness, feasibility and practicality of the introduced approach.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {62–71},
numpages = {10},
keywords = {test generation, software product lines, multi-objective optimization, genetic algorithms, feature models},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2791060.2791070,
author = {Liang, Jia Hui and Ganesh, Vijay and Czarnecki, Krzysztof and Raman, Venkatesh},
title = {SAT-based analysis of large real-world feature models is easy},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791070},
doi = {10.1145/2791060.2791070},
abstract = {Modern conflict-driven clause-learning (CDCL) Boolean SAT solvers provide efficient automatic analysis of real-world feature models (FM) of systems ranging from cars to operating systems. It is well-known that solver-based analysis of real-world FMs scale very well even though SAT instances obtained from such FMs are large, and the corresponding analysis problems are known to be NP-complete. To better understand why SAT solvers are so effective, we systematically studied many syntactic and semantic characteristics of a representative set of large real-world FMs. We discovered that a key reason why large real-world FMs are easy-to-analyze is that the vast majority of the variables in these models are unrestricted, i.e., the models are satisfiable for both true and false assignments to such variables under the current partial assignment. Given this discovery and our understanding of CDCL SAT solvers, we show that solvers can easily find satisfying assignments for such models without too many backtracks relative to the model size, explaining why solvers scale so well. Further analysis showed that the presence of unrestricted variables in these real-world models can be attributed to their high-degree of variability. Additionally, we experimented with a series of well-known nonbacktracking simplifications that are particularly effective in solving FMs. The remaining variables/clauses after simplifications, called the core, are so few that they are easily solved even with backtracking, further strengthening our conclusions. We explain the connection between our findings and backdoors, an idea posited by theorists to explain the power of SAT solvers. This connection strengthens our hypothesis that SAT-based analysis of FMs is easy. In contrast to our findings, previous research characterizes the difficulty of analyzing randomly-generated FMs in terms of treewidth. Our experiments suggest that the difficulty of analyzing real-world FMs cannot be explained in terms of treewidth.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {91–100},
numpages = {10},
keywords = {feature model, SAT-based analysis},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2648511.2648550,
author = {Dillon, Michael and Rivera, Jorge and Darbin, Rowland},
title = {A methodical approach to product line adoption},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648550},
doi = {10.1145/2648511.2648550},
abstract = {The evolution of the U.S. Army's Live Training Transformation (LT2) product line of combat training systems, including the move by the Army to consolidate management of the product line under a single contracting team, has provided a natural experiment that validates the hypothesis that product line engineering practices are more effective than traditional software engineering practices, and has demonstrated which product line adoption approaches are more successful than others. By analyzing this natural experiment, the product line team has been able to apply a methodical approach to product line adoption across the development organization and successfully adopt second generation product line processes. This paper explores that methodical approach. It will enumerate the steps that led to successes and explore the contributing factors and unintended consequences of failures along the way. Additionally this paper will explore how this approach is being employed to extend the LT2 product line beyond software.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {340–349},
numpages = {10},
keywords = {variation points, software product lines, second generation product line engineering, product portfolio, product line governance, product line engineering, product line adoption, product configurator, product baselines, feature profiles, feature modeling, feature constraints hierarchical product lines, bill-of-features},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2499777.2500715,
author = {Ishida, Yuzo},
title = {Scalable variability management for enterprise applications with data model driven development},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500715},
doi = {10.1145/2499777.2500715},
abstract = {Unlike embedded systems, some of enterprise systems are evolved over the decades. The predictability of requirements is a key to success in building reusable assets however it is very hard to predict future business context changes, which are driving factors of requirements. Thus, both functional and context variability must be managed in order to satisfy ever-changing requirements. Scalability does matter for enterprise systems in two aspects. One aspect comes from data volume. Once data become big, it is difficult to maintain performance requirements without de-normalizing database schema. Since database de-normalization is driven by non-functional properties, a model driven approach is not feasible if the model cannot express such properties. Another aspect comes from the unpredictability of future functional requirements. A functional decomposition of enterprise systems usually introduces ever-increasing complexity among systems' interactions due to cross-cutting requirements across functional systems. This paper reflects our empirical studies in data intensive large enterprise systems such as retail and telecommunication industries with industry independent application framework to separate functional and non-functional concerns. Our variability management technique is based on database schema modeling, which can be evolved incrementally in scaling an enterprise system with both data and functional aspects.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {90–93},
numpages = {4},
keywords = {type theory, relational algebra, quality attributes, higher-order simple predicate logic, core assets},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/2499777.2499782,
author = {Beuche, Danilo},
title = {Modeling and building product lines with pure::variants},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499782},
doi = {10.1145/2499777.2499782},
abstract = {The paper describes a demonstration of pure::variants, a commercial tool for variant and variability management for product lines. The demonstration shows how flexible product line (PL) architectures can be built by using the modeling capabilities provided by pure::variants [2].},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {147–149},
numpages = {3},
keywords = {tools, software product lines, feature modelling},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/2362536.2362563,
author = {Heider, Wolfgang and Rabiser, Rick and Gr\"{u}nbacher, Paul and Lettner, Daniela},
title = {Using regression testing to analyze the impact of changes to variability models on products},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362563},
doi = {10.1145/2362536.2362563},
abstract = {Industrial product lines are typically maintained for a long time and evolve continuously to address changing requirements and new technologies. Already derived products often have to be re-derived after such changes to benefit from new and updated features. Product line engineers thus frequently need to analyze the impact of changes to variability models to prevent unexpected changes of re-derived products. In this paper we present a tool-supported approach that informs engineers about the impacts of variability model changes on existing products. Regression tests are used to determine whether existing product configurations and generated product outputs can be re-derived without unexpected effects. We evaluate the feasibility of the approach based on changes observed in a real-world software product line. More specifically, we show how our approach helps engineers performing specific evolution tasks to analyze the change impacts on existing products. We also evaluate the performance and scalability of our approach. Our results show that variability change impact analyses can be automated using model regression testing and can help reducing the gap between domain engineering and application engineering.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {196–205},
numpages = {10},
keywords = {variability models, regression testing, product line evolution},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/1629716.1629729,
author = {Liebig, J\"{o}rg and Apel, Sven and Lengauer, Christian and Leich, Thomas},
title = {RobbyDBMS: a case study on hardware/software product line engineering},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629729},
doi = {10.1145/1629716.1629729},
abstract = {The development of a highly configurable data management system is a challenging task, especially if it is to be implemented on an embedded system that provides limited resources. We present a case study of such a data management system, called RobbyDBMS, and give it a feature-oriented design. In our case study, we evaluate the system's efficiency and variability. We pay particular attention to the interaction between the features of the data management system and the components of the underlying embedded platform. We also propose an integrated development process covering both hardware and software.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {63–68},
numpages = {6},
keywords = {FeatureC++, domain engineering, feature oriented software development, hardware product lines, software product lines},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@inproceedings{10.1145/2019136.2019177,
author = {Abbas, Nadeem and Andersson, Jesper and Weyns, Danny},
title = {Knowledge evolution in autonomic software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019177},
doi = {10.1145/2019136.2019177},
abstract = {We describe ongoing work in knowledge evolution management for autonomic software product lines. We explore how an autonomic product line may benefit from new knowledge originating from different source activities and artifacts at run time. The motivation for sharing run-time knowledge is that products may self-optimize at run time and thus improve quality faster compared to traditional software product line evolution. We propose two mechanisms that support knowledge evolution in product lines: online learning and knowledge sharing. We describe two basic scenarios for runtime knowledge evolution that involves these mechanisms. We evaluate online learning and knowledge sharing in a small product line setting that shows promising results.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {36},
numpages = {8},
keywords = {software product-lines, software design, self-adaptation, product-line management, online learning, knowledge sharing},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2491627.2491629,
author = {Clements, Paul and Krueger, Charles and Shepherd, James and Winkler, Andrew},
title = {A PLE-based auditing method for protecting restricted content in derived products},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491629},
doi = {10.1145/2491627.2491629},
abstract = {Many organizations that produce a portfolio of products for different customers need to ensure that sensitive or restricted content that may appear in some products must not appear in others. Examples of this need include complying with statutes in different countries of sale, protection of intellectual property developed specifically for one customer, and more. For organizations operating under these requirements and producing their products under a product line engineering paradigm that relies on automation in product derivation, there is a need for a method to ensure that the content restrictions have been met in the derived products. This paper describes an auditing method that meets this need. It was created for use in the Second Generation Product Line Engineering approach that is being applied by Lockheed Martin in their AEGIS ship combat system product line.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {218–226},
numpages = {9},
keywords = {variation points, software product lines, second generation product line engineering, product portfolio, product line engineering, product derivation, product configurator, product baselines, product audit, hierarchical product lines, feature profiles, feature modeling, bill-of-features},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2648511.2648524,
author = {Quinton, Cl\'{e}ment and Pleuss, Andreas and Berre, Daniel Le and Duchien, Laurence and Botterweck, Goetz},
title = {Consistency checking for the evolution of cardinality-based feature models},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648524},
doi = {10.1145/2648511.2648524},
abstract = {Feature-models (fms) are a widely used approach to specify the commonalities and variability in variable systems and software product lines. Various works have addressed edits to fms for fm evolution and tool support to ensure consistency of fms. An important extension to fms are feature cardinalities and related constraints, as extensively used e.g., when modeling variability of cloud computing environments. Since cardinality-based fms pose additional complexity, additional support for evolution and consistency checking with respect to feature cardinalities would be desirable, but has not been addressed yet. In this paper, we discuss common cardinality-based fm edits and resulting inconsistencies based on experiences with fms in cloud domain. We introduce tool-support for automated inconsistency detection and explanation based on an off-the-shelf solver. We demonstrate the feasibility of the approach by an empirical evaluation showing the performance of the tool.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {122–131},
numpages = {10},
keywords = {feature model, edit, consistency, cardinality},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2791060.2791089,
author = {Abbas, Nadeem and Andersson, Jesper},
title = {Harnessing variability in product-lines of self-adaptive software systems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791089},
doi = {10.1145/2791060.2791089},
abstract = {This work studies systematic reuse in the context of self-adaptive software systems. In our work, we realized that managing variability for such platforms is different compared to traditional platforms, primarily due to the run-time variability and system uncertainties. Motivated by the fact that recent trends show that self-adaptation will be used more often in future system generation and that software reuse state-of-practice or research do not provide sufficient support, we have investigated the problems and possibly resolutions in this context. We have analyzed variability for these systems, using a systematic reuse prism, and identified a research gap in variability management. The analysis divides variability handling into four activities: (1) identify variability, (2) constrain variability, (3) implement variability, and (4) manage variability. Based on the findings we envision a reuse framework for the specific domain and present an example framework that addresses some of the identified challenges. We argue that it provides basic support for engineering self-adaptive software systems with systematic reuse. We discuss some important avenues of research for achieving the vision.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {191–200},
numpages = {10},
keywords = {variability analysis, software reuse, self-adaptive software systems},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2499777.2499781,
author = {Cordy, Maxime and Classen, Andreas and Heymans, Patrick and Schobbens, Pierre-Yves and Legay, Axel},
title = {ProVeLines: a product line of verifiers for software product lines},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499781},
doi = {10.1145/2499777.2499781},
abstract = {Software Product Lines (SPLs) are families of similar software products built from a common set of features. As the number of products of an SPL is potentially exponential in the number of its features, the model checking problem is harder than for single software. A practical way to face this exponential blow-up is to reuse common behaviour between products. We previously introduced Featured Transition Systems (FTS), a mathematical model that serves as a basis for efficient SPL model checking techniques. In this paper, we present ProVeLines, a product line of verifiers for SPLs that incorporates the results of over three years of research on formal verification of SPLs. Being itself a product line, our tool is flexible and extensible, and offers a wide range of solutions for SPL modelling and verification.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {141–146},
numpages = {6},
keywords = {tool, software product lines, model checking, features},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/2019136.2019150,
author = {Serajzadeh, Hadi and Shams, Fereidoon},
title = {The application of swarm intelligence in service-oriented product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019150},
doi = {10.1145/2019136.2019150},
abstract = {Changing markets and environments has made the ability to rapidly adapt to these changes a necessity in software systems. However the costs of changing and adapting systems to new requirements still remains an unsolved issue. In this context service-oriented software product lines were introduced with the aim to combine the reusability of software product line with the flexibility of service-oriented architecture. Although this approach helps build flexible software systems with high levels of reuse, certain issues are raised. The main issue is the complexity that a service-oriented product line will face. Developing systems from internal and external assets, taking into consideration the variety and number of these assets, can cause problems in deciding which asset is best suited for the system. To help solve these issues we propose the use of approaches based on artificial intelligence. In this paper we show how swarm intelligence can be used in service-oriented product lines to reduce complexity and find optimal solutions for the development of software systems. We also present an example of the application of swarm intelligence in finding the optimal product for a service-oriented product line.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {12},
numpages = {7},
keywords = {swarm intelligence, service-oriented product line, optimization},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3129790.3129818,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green software development and research with the HADAS toolkit},
year = {2017},
isbn = {9781450352178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129790.3129818},
doi = {10.1145/3129790.3129818},
abstract = {Energy is a critical resource, and designing a sustainable software architecture is a non-trivial task. Developers require energy metrics that support sustainable software architectures reflecting quality attributes such as security, reliability, performance, etc., identifying what are the concerns that impact more in the energy consumption. A variability model of different designs and implementations of an energy model should exist for this task, as well as a service that stores and compares the experimentation results of energy and time consumption of each concern, finding out what is the most eco-efficient solution. The experimental measurements are performed by energy experts and researchers that share the energy model and metrics in a collaborative repository. HADAS confronts these tasks modelling and reasoning with the variability of energy consuming concerns for different energy contexts, connecting HADAS variability model with its energy efficiency collaborative repository, establishing a Software Product Line (SPL) service. Our main goal is to help developers to perform sustainability analyses finding out the eco-friendliest architecture configurations. A HADAS toolkit prototype is implemented based on a Clafer model and Choco solver, and it has been tested with several case studies.},
booktitle = {Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings},
pages = {205–211},
numpages = {7},
keywords = {variability, software product line, repository, optimisation, metrics, energy efficiency, clafer, CVL},
location = {Canterbury, United Kingdom},
series = {ECSA '17}
}

@inproceedings{10.1145/2364412.2364439,
author = {Vale, Tassio and Figueiredo, Gustavo Bittencourt and de Almeida, Eduardo Santana and de Lemos Meira, Silvio Romero},
title = {A study on service identification methods for software product lines},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364439},
doi = {10.1145/2364412.2364439},
abstract = {The combination of service-orientation and software product line engineering, called Service-Oriented Product Line Engineering (SOPLE) have received attention by researchers and practitioners in the last years, and these areas can address issues of each other. One service-orientation issue is service identification. It consists of determining candidate services to a service-oriented environment based on pre-existing software artifacts, e.g., business process, source code, and so on. In order to provide a systematic identification of services, there are many available service identification methods in the literature, regarding different understanding of services, goals, and techniques. Due to this heterogeneity, this paper presents an in-depth comparison of service identification methods as well as a recommendation of the most suitable ones in the SOPLE context. This work can help the decision making of the most suitable method according to stakeholders' needs.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {156–163},
numpages = {8},
keywords = {software product lines, service-oriented product lines, service-oriented computing, service identification},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3427423.3427450,
author = {Haris, M Syauqi and Kurniawan, Tri Astoto},
title = {Automated requirement sentences extraction from software requirement specification document},
year = {2021},
isbn = {9781450376051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427423.3427450},
doi = {10.1145/3427423.3427450},
abstract = {In the requirement reuse and natural language document-based Software Product Line (SPL) domain analysis, requirement sentences of the requirement document are the primary concern. Most studies conducted in this research area have document preprocessing stage in their methods that is a manual process to separate requirement sentences and non-requirement sentences from the document. This manual labor process might be tedious and error-prone since it will need much time and expert intervention to make this process completely done. In this paper, we present a method to automate requirement sentence extraction from the Software Requirement Specification (SRS) document by leveraging Natural Language Processing (NLP) approach and requirement boilerplate sentence patterns. Conducted experiments in this research show this method has such accuracy from 64% to 100% on precision value and recall value in the range of 64% to 89%.},
booktitle = {Proceedings of the 5th International Conference on Sustainable Information Engineering and Technology},
pages = {142–147},
numpages = {6},
keywords = {software requirement reuse, software product line, requirement boilerplate, natural language processing, domain analysis},
location = {Malang, Indonesia},
series = {SIET '20}
}

@inproceedings{10.1145/2364412.2364434,
author = {Helvensteijn, Michiel},
title = {Dynamic delta modeling},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364434},
doi = {10.1145/2364412.2364434},
abstract = {Abstract Delta Modeling (ADM) offers an algebraic description of how a (software) product line may be built so that every product can be automatically derived by structured reuse of code. In traditional application engineering a single valid feature configuration is chosen, which does not change during the lifetime of the product. However, there are many useful applications for product lines that change their configuration at run time. We present a new technique for generating efficient dynamic product lines from their static counterparts. We use Mealy machines for their dynamic reconfiguration. Furthermore, we posit that monitoring some features will be more expensive than monitoring others, and present techniques for minimizing the cost of monitoring the system. We stay in the abstract setting of ADM but the techniques can be instantiated to any concrete domain. We illustrate them through the example of a mobile application for Android, which dynamically reconfigures a devices operating profile based on environmental factors.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {127–134},
numpages = {8},
keywords = {profile management, optimization, mealy machines, dynamic product lines, delta modeling},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791082,
author = {Hotz, Lothar and Wang, Yibo and Riebisch, Matthias and G\"{o}tz, Olaf and Lackhove, Josef},
title = {Evaluation across multiple views for variable automation systems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791082},
doi = {10.1145/2791060.2791082},
abstract = {Automation systems in industry are often software-intensive systems consisting of software and hardware components. During their development several engineers of different disciplines are involved, such as mechanical, electrical and software engineering. Each engineer focuses on specific system aspects to be developed. To enable an efficient development, product lines especially with feature models for variability modeling are promising technologies. In order to reduce the complexity of both feature models and development process, views on feature models can be applied. The use of views for filtering purposes constitutes an established method. However, views also enable further options missing in current approaches, such as evaluations regarding requirements, including non-functional ones. This paper presents an approach for evaluation across multiple views to enable collaborative development for developers who focus on different system aspects. We validate our approach by applying it in an industrial project for the planning of flying saws.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {311–315},
numpages = {5},
keywords = {product lines, multi-criteria evaluation, feature model, consistency check, configuration, automation systems},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2362536.2362560,
author = {Lettner, Daniela and Vierhauser, Michael and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {Supporting end users with business calculations in product configuration},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362560},
doi = {10.1145/2362536.2362560},
abstract = {Business calculations like break-even, return on investment, or cost are essential in many domains to support decision making while configuring products. For instance, customers and sales people need to estimate and compare the business value of different product variants. Some product line approaches provide initial support, e.g., by defining quality attributes in relation to features. However, an approach that allows domain engineers to easily define business calculations together with variability models is still lacking. In product configuration, calculation results need to be instantly presented to end users after making configuration choices. Further, due to the often high number of calculations, the presentation of calculation results to end users can be challenging. These challenges cannot be addressed by integrating off-the-shelf applications performing the calculations with product line tools. We thus present an approach based on dedicated calculation models that are related to variability models. Our approach seamlessly integrates business calculations with product configuration and provides support for formatting calculations and calculation results. We use the DOPLER tool suite to deploy calculations together with variability models to end users in product configuration. We evaluate the expressiveness and practical relevance of the approach by investigating the development of business calculations for 15 product lines from the domain of industrial automation.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {171–180},
numpages = {10},
keywords = {variability models, product configuration, business calculations},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791073,
author = {Lachmann, Remo and Lity, Sascha and Lischke, Sabrina and Beddig, Simon and Schulze, Sandro and Schaefer, Ina},
title = {Delta-oriented test case prioritization for integration testing of software product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791073},
doi = {10.1145/2791060.2791073},
abstract = {Software product lines have potential to allow for mass customization of products. Unfortunately, the resulting, vast amount of possible product variants with commonalities and differences leads to new challenges in software testing. Ideally, every product variant should be tested, especially in safety-critical systems. However, due to the exponentially increasing number of product variants, testing every product variant is not feasible. Thus, new concepts and techniques are required to provide efficient SPL testing strategies exploiting the commonalities of software artifacts between product variants to reduce redundancy in testing. In this paper, we present an efficient integration testing approach for SPLs based on delta modeling. We focus on test case prioritization. As a result, only the most important test cases for every product variant are tested, reducing the number of executed test cases significantly, as testing can stop at any given point because of resource constraints while ensuring that the most important test cases have been covered. We present the general concept and our evaluation results. The results show a measurable reduction of executed test cases compared to single-software testing approaches.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {81–90},
numpages = {10},
keywords = {test case prioritization, regression testing, delta-oriented software product lines, architecture-based testing},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2791060.2791065,
author = {Gregg, Susan P. and Scharadin, Rick and Clements, Paul},
title = {The more you do, the more you save: the superlinear cost avoidance effect of systems product line engineering},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791065},
doi = {10.1145/2791060.2791065},
abstract = {Product lines that use automated tools to configure shared assets (e.g., software or requirements or test cases or user documentation) based on product descriptions have long been known to bring about substantial development cost avoidance when compared to clone-and-own or product-specific development techniques. Now, however, it can be shown that the cost avoidance for configuring multiple shared assets is superlinear -- that is, the overall cost avoidance exceeds the sum of the that brought about by working with each of the shared assets in isolation. That is, a product line that configures (for example) requirements and code will avoid more cost than the sum of code-based plus requirements-based cost avoidance. In addition, we also observe a superlinear effect in terms of the number of products in the portfolio as well. This paper explores why these effects occur, and presents analytical and empirical evidence for their existence from one of the largest and most successful product lines in the literature, the AEGIS Weapon System. The result may lead to new insight into the economics of product line engineering in the systems engineering realm.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {303–310},
numpages = {8},
keywords = {variation points, systems and software product lines, second generation product line engineering, product line measurement, product line engineering, product line economics, product derivation, product configurator, feature modeling, AEGIS},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2648511.2648525,
author = {Stein, Jacob and Nunes, Ingrid and Cirilo, Elder},
title = {Preference-based feature model configuration with multiple stakeholders},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648525},
doi = {10.1145/2648511.2648525},
abstract = {Feature model configuration is known to be a hard, error-prone and time-consuming activity. This activity gets even more complicated when it involves multiple stakeholders in the configuration process. Research work has proposed approaches to aid multi-stakeholder feature model configuration, but they rely on systematic processes that constraint decisions of some of the stakeholders. In this paper, we propose a novel approach to improve the multi-stakeholder configuration process, considering stakeholders' preferences expressed through both hard and soft constraints. Based on such preferences, we recommend different product configurations using different strategies from the social choice theory. We conducted an empirical study to evaluate the effectiveness of our strategies with respect to individual stakeholder satisfaction and fairness among all stakeholders. Results indicate that particular strategies perform best with respect to these aspects.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {132–141},
numpages = {10},
keywords = {social choice, preferences, feature model configuration},
location = {Florence, Italy},
series = {SPLC '14}
}

@proceedings{10.1145/3629527,
title = {ICPE '24 Companion: Companion of the 15th ACM/SPEC International Conference on Performance Engineering},
year = {2024},
isbn = {9798400704451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to present the ICPE 2024 workshops program. ICPE workshops extend the main conference by providing a forum to foster discussion on hot and emerging topics from the broad field of performance engineering. They offer a highly dynamic venue to exchange ideas, establish new collaborations, and bootstrap debates on novel techniques, methodologies, and their associated early research results. Workshops feature various presentation formats, including research paper presentations, panel discussions, and keynote talks. Through these presentations and discussions with peer researchers, ICPE workshops help shape future research and identify promising research directions for performance engineering.},
location = {London, United Kingdom}
}

@inproceedings{10.1145/2791060.2791067,
author = {Yue, Tao and Ali, Shaukat and Selic, Bran},
title = {Cyber-physical system product line engineering: comprehensive domain analysis and experience report},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791067},
doi = {10.1145/2791060.2791067},
abstract = {Cyber-Physical Systems (CPSs) are the future generation of highly connected embedded systems having applications in diverse domains including Oil and Gas. Employing Product Line Engineering (PLE) is believed to bring potential benefits with respect to reduced cost, higher productivity, higher quality, and faster time-to-market. However, relatively few industrial field studies are reported regarding the application of PLE to develop large-scale systems, and more specifically CPSs. In this paper, we report about our experiences and insights gained from investigating the application of model-based PLE at a large international organization developing subsea production systems (typical CPSs) to manage the exploitation of oil and gas production fields. We report in this paper 1) how two systematic domain analyses (on requirements engineering and product configuration/derivation) were conducted to elicit CPS PLE requirements and challenges, 2) key results of the domain analysis (commonly observed in other domains), and 3) our initial experience of developing and applying two Model Based System Engineering (MBSE) PLE solution to address some of the requirements and challenges elicited during the domain analyses.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {338–347},
numpages = {10},
keywords = {requirements engineering, product line engineering (PLE), model based system engineering, domain analysis, cyber physical system (CPS)},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3377930.3390215,
author = {Silva, Diego Fernandes da and Okada, Luiz Fernando and Colanzi, Thelma Elita and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Enhancing search-based product line design with crossover operators},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3390215},
doi = {10.1145/3377930.3390215},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA designing has been formulated as a multi-objective optimization problem and successfully solved by a state-of-the-art search-based approach. However, the majority of empirical studies optimize PLA designs without applying one of the fundamental genetic operators: the crossover. An operator for PLA design, named Feature-driven Crossover, was proposed in a previous study. In spite of the promising results, this operator occasionally generated incomplete solutions. To overcome these limitations, this paper aims to enhance the search-based PLA design optimization by improving the Feature-driven Crossover and introducing a novel crossover operator specific for PLA design. The proposed operators were evaluated in two well-studied PLA designs, using three experimental configurations of NSGA-II in comparison with a baseline that uses only mutation operators. Empirical results show the usefulness and efficiency of the presented operators on reaching consistent solutions. We also observed that the two operators complement each other, leading to PLA design solutions with better feature modularization than the baseline experiment.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {1250–1258},
numpages = {9},
keywords = {software product line, software architecture, recombination operators, multi-objective evolutionary algorithm},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.5555/1753235.1753238,
author = {White, Jules and Dougherty, Brian and Schmidt, Doulas C. and Benavides, David},
title = {Automated reasoning for multi-step feature model configuration problems},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {The increasing complexity and cost of software-intensive systems has led developers to seek ways of increasing software reusability. One software reuse approach is to develop a Software Product-line (SPL), which is a reconfigurable software architecture that can be reused across projects. Creating configurations of the SPL that meets arbitrary requirements is hard.Existing research has focused on techniques that produce a configuration of the SPL in a single step. This paper provides three contributions to the study of multi-step configuration for SPLs. First, we present a formal model of multi-step SPL configuration and map this model to constraint satisfaction problems (CSPs). Second, we show how solutions to these CSP configuration problem CSPs can be derived automatically with a constraint solver. Third, we present empirical results demonstrating that our CSP-based technique can solve multi-step configuration problems involving hundreds of features in seconds.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {11–20},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2362536.2362557,
author = {Elsner, Christoph},
title = {Light-weight tool support for staged product derivation},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362557},
doi = {10.1145/2362536.2362557},
abstract = {Tool support that checks for configuration errors and generates product parts from configurations can significantly improve on product derivation in product line engineering. Up to now, however, derivation tools commonly disregard the staged derivation process. They do not restrict configuration consistency checks to process entities such as configuration stages, stakeholders, or build tasks. As a result, constraints that are only valid for certain process entities must either be checked permanently, leading to false positive errors, or one must refrain from defining them at all.This paper contributes a light-weight approach to provide tailored tool support for staged product derivation. Compared to previous approaches, it is not tied to a single configuration mechanism (e.g., feature modeling), and also accounts for the stakeholders involved and the build tasks that generate product parts. First, the product line engineer describes the derivation process in a concise model. Then, based on constraint checks on the configuration (e.g., a feature model configuration) that are linked to the modeled entities, comprehensive tool support can be provided: Configuration actions can be guided and restricted depending on the configuring stakeholder in a fine-grained manner, and constraints attached to a build task will only be checked if it actually shall be executed. Finally, in combination with previous work, the paper provides evidence that the approach is applicable to legacy product lines in a light-weight manner and that it technically scales to thousands of constraint checks.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {146–155},
numpages = {10},
keywords = {tool support, staged product derivation, product line},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791096,
author = {F\'{e}derle, \'{E}dipo Luis and do Nascimento Ferreira, Thiago and Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {OPLA-tool: a support tool for search-based product line architecture design},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791096},
doi = {10.1145/2791060.2791096},
abstract = {The Product Line Architecture (PLA) design is a complex task, influenced by many factors such as feature modularization and PLA extensibility, which are usually evaluated according to different metrics. Hence, the PLA design is an optimization problem and problems like that have been successfully solved in the Search-Based Software Engineering (SBSE) area, by using metaheuristics such as Genetic Algorithm. Considering this fact, this paper introduces a tool named OPLA-Tool, conceived to provide computer support to a search-based approach for PLA design. OPLA-Tool implements all the steps necessary to use multi-objective optimization algorithms, including PLA transformations and visualization through a graphical interface. OPLA-Tool receives as input a PLA at the class diagram level, and produces a set of good alternative diagrams in terms of cohesion, feature modularization and reduction of crosscutting concerns.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {370–373},
numpages = {4},
keywords = {search-based software engineering, product line architecture design, multi-objective evolutionary algorithms},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3425174.3425211,
author = {Ferreira, Thiago do Nascimento and Vergilio, Silvia Regina and Kessentini, Marouane},
title = {Applying Many-objective Algorithms to the Variability Test of Software Product Lines},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425211},
doi = {10.1145/3425174.3425211},
abstract = {The problem known as Variability Test of Software Product Line (VTSPL) is related to the selection of the most representative products for the SPL testing. This is an optimization problem because a possible exponential number of products can be derived from the SPL variability model, such as the Feature Model (FM). In the literature many works are dedicated to this research subject, each one applying a different search-based algorithm and using distinct criteria. However, there is no study encompassing all these criteria at the same time. To this end, this paper investigates the use of two Many-Objective Evolutionary Algorithms (MaOEAs). We apply the algorithm NSGA-III, widely used for many-objective algorithms, and the algorithm PCA-NSGA-II, a reduction dimensionality algorithm, which uses the Principal-Component Analysis (PCA) in combination with NSGA-II, to evaluate the objectives used in the literature for the VTSPL problem. PCA-NSGA-II reduces the search space dimensionality by eliminating the redundant objectives. The analysis shows the importance of some objectives such as the number of alive mutants, similarity between products, and unselected features. NSGA-III reaches the best results regarding the quality indicators for all instances, but taking a longer time. Besides, PCA-NSGA-II can find different solutions in the search space that are not found by NSGA-III.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {11–20},
numpages = {10},
keywords = {many-objective problems, dimensionality reduction, Software product line testing},
location = {Natal, Brazil},
series = {SAST '20}
}

@inproceedings{10.5555/1753235.1753247,
author = {Chen, Lianping and Ali Babar, Muhammad and Ali, Nour},
title = {Variability management in software product lines: a systematic review},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Variability Management (VM) in Software Product Line (SPL) is a key activity that usually affects the degree to which a SPL is successful. SPL community has spent huge amount of resources on developing various approaches to dealing with variability related challenges over the last decade. To provide an overview of different aspects of the proposed VM approaches, we carried out a systematic literature review of the papers reporting VM in SPL. This paper presents and discusses the findings from this systematic literature review. The results reveal the chronological backgrounds of various approaches over the history of VM research, and summarize the key issues that drove the evolution of different approaches. This study has also identified several gaps that need to be filled by future efforts in this line of research.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {81–90},
numpages = {10},
keywords = {software product lines, systematic reviews, variability management},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.5555/1753235.1753254,
author = {Parra, Carlos and Blanc, Xavier and Duchien, Laurence},
title = {Context awareness for dynamic service-oriented product lines},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {This paper presents a Context-Aware Dynamic Software Product Line (DSPL) for building service oriented applications and adapting them at runtime in accordance with their using context. This DSPL, named CAPucine for Context-Aware Service-Oriented Product Line is based on two different processes for product derivation. The first process uses assets that represent features of the product family. The assets, represented as models, get composed and transformed in order to generate the product. The second process relates to dynamic adaptation. This process introduces context-aware assets that operate at runtime. These context-aware assets contain three kinds of data: the context when the assets can be modified, the place where the assets must be applied and the change that must be performed. The realization of these context-aware assets combines two runtime platforms. On the one hand,COSMOS is a context-aware framework connected to the environment by the use of sensors. On the other hand FraSCAti is a Service Component Architecture (SCA) platform with dynamic properties that enables to bind and unbind components at runtime. CAPucine allows designing and processing context-aware applications based on an SCA platform which is dynamic, introspectable, and reconfigurable in accordance with the context environment.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {131–140},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2362536.2362566,
author = {Kircher, Michael and Hofman, Peter},
title = {Combining systematic reuse with Agile development: experience report},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362566},
doi = {10.1145/2362536.2362566},
abstract = {This paper documents the experiences of Siemens Healthcare in mastering challenges when transitioning a large-scale dispersed platform development organization to Agile. Product Line Engineering aims at increasing productivity through reuse, but since strategic reuse requires up-front decisions, is also seen as heavy weight and process driven. Agile development on the other hand is perceived as lightweight, change friendly, but at the same time neglecting long term strategic planning. With this paper we want to report on our experience combining both approaches, PLE for strategic reuse and agile principles for achieving steady progress while still leveraging the long-term benefits. The key was to build the foundation on the common best practice of 'feature-orientation' present in flavors in both disciplines. Feature-orientation allowed merging both disciplines into a holistic approach that blends the benefits of product line engineering with those of Agility -- resulting in improved product delivery, as well as employee and customer satisfaction.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {215–219},
numpages = {5},
keywords = {lean, hierarchical platform, Agile},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2364412.2364449,
author = {Helvensteijn, Michiel},
title = {Abstract delta modeling: my research plan},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364449},
doi = {10.1145/2364412.2364449},
abstract = {Software product lines are sets of software programs with well defined commonalities and variabilities that are distinguished by which features they support. There is need of a way to organize the underlying code to clearly link features on the feature modeling level to code artifacts on the implementation level, without code duplication or overspecification, so we can support automated product derivation. Existing approaches are still lacking in one way or another. My answer to this problem is delta modeling. My thesis will approach delta modeling from an abstract algebraic perspective called Abstract Delta Modeling. It will give a thorough formal treatment of the subject and extend it in several directions. A workflow for building a product line from scratch, a way to model dynamic product lines as well as plenty of practical examples and case studies.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {217–224},
numpages = {8},
keywords = {type systems, product lines, modal logic, dynamic product lines, development workflow, delta modeling, PhD thesis},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2491627.2491647,
author = {Murashkin, Alexandr and Antkiewicz, Micha\l{} and Rayside, Derek and Czarnecki, Krzysztof},
title = {Visualization and exploration of optimal variants in product line engineering},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491647},
doi = {10.1145/2491627.2491647},
abstract = {The decision-making process in Product Line Engineering (PLE) is often concerned with variant qualities such as cost, battery life, or security. Pareto-optimal variants, with respect to a set of objectives such as minimizing a variant's cost while maximizing battery life and security, are variants in which no single quality can be improved without sacrificing other qualities. We propose a novel method and a tool for visualization and exploration of a multi-dimensional space of optimal variants (i.e., a Pareto front). The visualization method is an integrated, interactive, and synchronized set of complementary views onto a Pareto front specifically designed to support PLE scenarios, including: understanding differences among variants and their positioning with respect to quality dimensions; solving trade-offs; selecting the most desirable variants; and understanding the impact of changes during product line evolution on a variant's qualities. We present an initial experimental evaluation showing that the visualization method is a good basis for supporting these PLE scenarios.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {111–115},
numpages = {5},
keywords = {visualization, product line engineering, pareto front, optimal variant, feature modeling, exploration, clafer, ClaferMoo visualizer, ClaferMoo},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3422392.3422498,
author = {Freire, Willian Marques and Massago, Mamoru and Zavadski, Arthur Cattaneo and Malachini, Aline Maria and Amaral, Miotto and Colanzi, Thelma Elita},
title = {OPLA-Tool v2.0: a Tool for Product Line Architecture Design Optimization},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422498},
doi = {10.1145/3422392.3422498},
abstract = {The Multi-objective Optimization Approach for Product Line Architecture Design (MOA4PLA) is the seminal approach that successfully optimizes Product Line Architecture (PLA) design using search algorithms. The tool named OPLA-Tool was developed in order to automate the use of MOA4PLA. Over time, the customization of the tool to suit the needs of new research and application scenarios led to several problems. The main problems identified in the original version of OPLA-Tool are environment configuration, maintainability and usability problems, and PLA design modeling and visualization. Such problems motivated the development of a new version of this tool: OPLA-Tool v2.0, presented in this work. In this version, those problems were solved by the source code refactoring, migration to a web-based graphical user interface (GUI) and inclusion of a new support tool for PLA modeling and visualization. Furthermore, OPLA-Tool v2.0 has new functionalities, such as new objective functions, new search operators, intelligent interaction with users during the optimization process, multi-user authentication and simultaneous execution of several experiments to PLA optimization. Such a new version of OPLA-Tool is an important achievement to PLA design optimization as it provides an easier and more complete way to automate this task.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {818–823},
numpages = {6},
keywords = {Software product line, multi-objective evolutionary algorithms, product line architecture},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.5555/1753235.1753251,
author = {Bosch, Jan},
title = {From software product lines to software ecosystems},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Software product line companies increasingly expand their platform outside their organizational boundaries, in effect transitioning to a software ecosystem approach. In this paper, we discuss the emerging trend of software ecosystems and provide a overview of the key concepts and implications of adopting a software ecosystem approach. We define the notion of software ecosystems and introduce a taxonomy. Finally, we explore the implications of software ecosystems to the way companies build software.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {111–119},
numpages = {9},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/3168365.3170425,
author = {Lienhardt, Michael and Damiani, Ferruccio and Donetti, Simone and Paolini, Luca},
title = {Multi Software Product Lines in the Wild},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3170425},
doi = {10.1145/3168365.3170425},
abstract = {Modern software systems are often built from customizable and inter-dependent components. Such customizations usually define which features are offered by the components, and may depend on backend components being configured in a specific way. As such system become very large, with a huge number of possible configurations and complex dependencies between components, maintenance and ensuring the consistency of such systems is a challenge.In this paper, we propose a Multi Software Product Line model to capture the complexity of such systems and pave the way to formal studies on them. We applied and implemented our model on a full Linux Distribution of almost 40,000 interconnected components and 3 million features, and present some initial analysis we did on this model.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {89–96},
numpages = {8},
keywords = {Variability Modeling, Software Product Line, Multi Software Product Line, Linux Distribution, Configurable Software, Composition},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/2648511.2648533,
author = {Simidchieva, Borislava I. and Osterweil, Leon J.},
title = {Generation, composition, and verification of families of human-intensive systems},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648533},
doi = {10.1145/2648511.2648533},
abstract = {Software products are rarely developed without providing different sets of features to better meet varying user needs, whether through tiered products as part of a product line or different subscription levels for software as a service (SaaS). Software product line approaches for generating and maintaining a family of different variants of software products address such needs for variation quite well. Real-world human-intensive systems (HISs) display similar needs for families of variants. A key contribution of this paper is to show how many of these needs can be rigorously and systematically addressed by adapting established techniques from system and software product line engineering (SPLE).In this paper, we present an approach for creating such families by explicitly modeling variation in HISs. We focus on two kinds of variation we have previously described in other work---functional detail variation and service variation. We describe a prototype system that is able to meet the need for these kinds of variation within an existing modeling framework and present a case study of the application of our prototype system to generate a family in an HIS from the domain of elections. Our approach also demonstrates how to perform model-checking of this family to discover whether any variants in the family may violate specified system requirements.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {207–216},
numpages = {10},
keywords = {system variation, software product lines, process families},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2499777.2500716,
author = {Saller, Karsten and Lochau, Malte and Reimund, Ingo},
title = {Context-aware DSPLs: model-based runtime adaptation for resource-constrained systems},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500716},
doi = {10.1145/2499777.2500716},
abstract = {Dynamic Software Product Lines (DSPLs) provide a promising approach for planning and applying runtime reconfiguration scenarios to adaptive software systems. However, applying DSPLs in the vital domain of highly context-aware systems, e.g., mobile devices, is obstructed by the inherently limited resources being insufficient to handle large, constrained (re-)configurations spaces. To tackle these drawbacks, we propose a novel model-based approach for designing DSPLs in a way that allows for a trade-off between precomputation of reconfiguration scenarios at development time and on-demand evolution at runtime. Therefore, we (1) enrich feature models with context information to reason about potential context changes, and (2) specify context-aware reconfiguration processes on the basis of a scalable transition system incorporating state space abstractions and incremental refinement at runtime. We illustrate our concepts by means of a smartphone case study and present an implementation and evaluation considering different trade-off metrics.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {106–113},
numpages = {8},
keywords = {state space reduction, feature models, contexts, adaptive systems, DSPL},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/2364412.2364435,
author = {Saller, Karsten and Oster, Sebastian and Sch\"{u}rr, Andy and Schroeter, Julia and Lochau, Malte},
title = {Reducing feature models to improve runtime adaptivity on resource limited devices},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364435},
doi = {10.1145/2364412.2364435},
abstract = {Mobile devices like smartphones are getting increasingly important in our daily lifes. They are used in various environments and have to dynamically adapt themselves accordingly in order to provide an optimal runtime behavior. Naturally, adapting to continuously changing environmental conditions is a challenging task because mobile devices are always limited in their resources and have to adapt in real-time. In this paper, we introduce an approach that enables resource limited devices to adapt to changing conditions using dynamic software product lines techniques. Therefore, feature models are reduced to a specific hardware context before installing the adaptive mobile application on the device. This reduces the amount of possible configurations that are compatible with the device and, thereby, minimizes the costs and the duration of an adaptation during runtime.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {135–142},
numpages = {8},
keywords = {feature models, dynamic software product lines, context-awareness, adaptive systems},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2491627.2491650,
author = {Xu, Zhihong and Cohen, Myra B. and Motycka, Wayne and Rothermel, Gregg},
title = {Continuous test suite augmentation in software product lines},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491650},
doi = {10.1145/2491627.2491650},
abstract = {Software Product Line (SPL) engineering offers several advantages in the development of families of software products. There is still a need, however, to generate test cases for individual products in product lines more efficiently. In this paper we propose an approach, CONTESA, for generating test cases for SPLs using test suite augmentation. Instead of generating test cases for products independently, our approach generates new test cases for products in an order that allows it to build on test cases created for products tested earlier. In this work, we use a genetic algorithm to generate test cases, targeting branches not yet covered in each product, although other algorithms and coverage criteria could be utilized. We have evaluated CONTESA on two non-trivial SPLs, and have shown that CONTESA is more efficient and effective than an approach that generates test cases for products independently. A further evaluation shows that CONTESA is more effective at achieving coverage, and reveals as many faults as an existing feature-based testing approach.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {52–61},
numpages = {10},
keywords = {test generation, software testing, software product lines},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2499777.2500712,
author = {Kolokolov, Viktor and Baumann, Paul and Santini, Silvia and Ruehl, Stefan T. and Verclas, Stephan A. W.},
title = {Flexible development of variable software features for mobile business applications},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500712},
doi = {10.1145/2499777.2500712},
abstract = {With recent advances in development and deployment of mobile business applications (MBAs) based on the hybrid Web approach (hybrid MBAs) enterprises around the world well recognize new potentials to mobilize their business processes (BPs). Variability has a natural appearance in complex environments of different enterprises, where even similar BPs can have varying facets on the cross-enterprise scale. Yet, despite this fact current development tools for hybrid MBAs are lacking systematic variability management. Further, the literature on this particular technological landscape is scarce. We highlight in this paper emerging importance of this research field and describe its context and a research methodology. We propose an SPL-based approach to tackle considerable variabilities of hybrid MBAs.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {67–73},
numpages = {7},
keywords = {variability modeling, software product lines, mobile business applications, hybrid web},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3447545.3451177,
author = {Canales, Felipe and Hecht, Geoffrey and Bergel, Alexandre},
title = {Optimization of Java Virtual Machine Flags using Feature Model and Genetic Algorithm},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451177},
doi = {10.1145/3447545.3451177},
abstract = {Optimizing the Java Virtual Machine (JVM) options in order to get the best performance out of a program for production is a challenging and time-consuming task. HotSpot, the Oracle's open-source Java VM implementation offers more than 500 options, called flags, that can be used to tune the JVM's compiler, garbage collector (GC), heap size and much more. In addition to being numerous, these flags are sometimes poorly documented and create a need of benchmarking to ensure that the flags and their associated values deliver the best performance and stability for a particular program to execute.Auto-tuning approaches have already been proposed in order to mitigate this burden. However, in spite of increasingly sophisticated search techniques allowing for powerful optimizations, these approaches take little account of the underlying complexities of JVM flags. Indeed, dependencies and incompatibilities between flags are non-trivial to express, which if not taken into account may lead to invalid or spurious flag configurations that should not be considered by the auto-tuner.In this paper, we propose a novel model, inspired by the feature model used in Software Product Line, which takes the complexity of JVM's flags into account. We then demonstrate the usefulness of this model, using it as an input of a Genetic Algorithm (GA) to optimize the execution times of DaCapo Benchmarks.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {183–186},
numpages = {4},
keywords = {optimization, java virtual machine, genetic algorithm, feature model, auto-tuning},
location = {Virtual Event, France},
series = {ICPE '21}
}

@inproceedings{10.1145/3168365.3168378,
author = {Carbonnel, Jessie and Huchard, Marianne and Nebut, Cl\'{e}mentine},
title = {Towards the Extraction of Variability Information to Assist Variability Modelling of Complex Product Lines},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168378},
doi = {10.1145/3168365.3168378},
abstract = {Software product line engineering gathers a set of methods that rely on systematic reuse and mass customisation to reduce the development time and cost of a set of similar software systems. Boolean feature models are the de facto standard used to represent product line variability in terms of features, a feature being a distinguishable characteristic of one or several softwares. The extractive adoption of a product line from a set of individually developed softwares requires to extract variability information from a collection of software descriptions to model their variability. With the appearance of more and more complex software systems, software product line engineering faces new challenges including variability extraction and modelling. Extensions of boolean feature models, as multi-valued attributes or UML-like cardinalities have since been proposed to support variability modelling in complex product lines. In this paper, we propose research directions to address the issue of extracting more complex variability information, as a part of extended feature models synthesis from software descriptions. We consider the capabilities of Formal Concept Analysis, a mathematical framework for knowledge discovery, along with two of its extensions called Pattern Structures and Relational Concept Analysis, to answer this problematic. These frameworks bring theoretical foundations to complex variability extraction algorithms.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {113–120},
numpages = {8},
keywords = {Variability Extraction, Software Product Line, Reverse Engineering},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/2019136.2019141,
author = {Ryssel, Uwe and Ploennigs, Joern and Kabitzsch, Klaus},
title = {Extraction of feature models from formal contexts},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019141},
doi = {10.1145/2019136.2019141},
abstract = {For economical reasons, the creation of feature oriented software should include previously created products and should not be done from scratch. To speed up this migration process, feature models have to be generated automatically from existing product variants. This work presents an approach based on formal concept analysis that analyzes incidence matrices containing matching relations as input and creates feature models as output. The resulting feature models describe exactly the given input variants. The introduced novel optimized approach performs this transformation in reasonable time even for large product libraries.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {4},
numpages = {8},
keywords = {formal concept analysis, feature models},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2362536.2362551,
author = {Tischer, Christian and Boss, Birgit and M\"{u}ller, Andreas and Thums, Andreas and Acharya, Rajneesh and Schmid, Klaus},
title = {Developing long-term stable product line architectures},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362551},
doi = {10.1145/2362536.2362551},
abstract = {Product lines are usually built for the long term in order to repay the initial investment. While long-term stable software systems are already hard, if they are developed individually, it is even harder for complete product lines. At the time a new product line is created, the details of future product line characteristics are typically not known, no matter how well and detailed scoping and planning is done. Thus, any product line needs to evolve and adapt over time to incorporate new customer requirements as well as new technology constraints.Stability of the product line architecture is very important to the successful long-term evolution of a product line. In this paper, we discuss how a form of domain decomposition, which we call conceptual architecture, can be used to guide product line engineering towards long-term viability. We will illustrate this approach in the context of a large-scale product line development and analyze the evolution properties of the product line. Transferability of the approach is suggested to other embedded software systems that drive mature, well-understood physical control system.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {86–95},
numpages = {10},
keywords = {software product lines, software architecture, scoping, multi product lines, AUTOSAR},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1145/3300148,
author = {Li, Miqing and Yao, Xin},
title = {Quality Evaluation of Solution Sets in Multiobjective Optimisation: A Survey},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3300148},
doi = {10.1145/3300148},
abstract = {Complexity and variety of modern multiobjective optimisation problems result in the emergence of numerous search techniques, from traditional mathematical programming to various randomised heuristics. A key issue raised consequently is how to evaluate and compare solution sets generated by these multiobjective search techniques. In this article, we provide a comprehensive review of solution set quality evaluation. Starting with an introduction of basic principles and concepts of set quality evaluation, this article summarises and categorises 100 state-of-the-art quality indicators, with the focus on what quality aspects these indicators reflect. This is accompanied in each category by detailed descriptions of several representative indicators and in-depth analyses of their strengths and weaknesses. Furthermore, issues regarding attributes that indicators possess and properties that indicators are desirable to have are discussed, in the hope of motivating researchers to look into these important issues when designing quality indicators and of encouraging practitioners to bear these issues in mind when selecting/using quality indicators. Finally, future trends and potential research directions in the area are suggested, together with some guidelines on these directions.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {26},
numpages = {38},
keywords = {performance assessment, multobjective optimisation, multi-criteria optimisation, metric, metaheuristic, measure, indicator, heuristic, exact method, evolutionary algorithms, Quality evaluation}
}

@inproceedings{10.1145/2647908.2655977,
author = {El Yamany, Ahmed Eid and Shaheen, Mohamed and Sayyad, Abdel Salam},
title = {OPTI-SELECT: an interactive tool for user-in-the-loop feature selection in software product lines},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655977},
doi = {10.1145/2647908.2655977},
abstract = {Opti-Select is an Interactive Multi-objective feature analysis and optimization tool for software product lines configuration and feature models optimization based on an innovative UIL (User-In-the-loop) idea. In this tool, the experience of system analysts and stakeholders are merged with optimization techniques and algorithms.Opti-Select interactive tool is an integrated set of techniques providing step by step feature model and attribute configuration, selecting and excluding features, solution set optimization, and user interaction utilities that can all together reach satisfactory set of solutions that fits stakeholder preferences.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {126–129},
numpages = {4},
keywords = {user-in-the-loop (UIL), software product lines, search-based software engineering, product line engineering, optimal variant, optimal feature selection, multi-objective optimization, modeling, features, feature models, feature modeling, exploration, Pareto front visualization},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2647908.2655961,
author = {Seidl, Christoph and Domachowska, Irena},
title = {Teaching variability engineering to cognitive psychologists},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655961},
doi = {10.1145/2647908.2655961},
abstract = {In research of cognitive psychology, experiments to measure cognitive processes may be run in many similar yet slightly different configurations. Variability engineering offers techniques to handle variable configurations both conceptually and technically. However, these techniques are largely unknown to cognitive psychologists so that experiment configurations are specified informally or too coarse grain. This is problematic, because it becomes difficult to get an overview of paradigm configurations used in the so far conducted experiments. Variability engineering techniques provide, i.a., concise notations for capturing variability in software and can also be used to express the configurable nature of a wide range of experiments in cognitive psychology. Furthermore, it enables cognitive psychologists to structure configuration knowledge, to identify suitably similar experiment setups and to more efficiently identify individual configuration options as relevant reasons for a particular effect in the outcome of an experiment. In this paper, we present experiences with teaching variability engineering to cognitive psychologists along with a suitable curriculum.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {16–23},
numpages = {8},
keywords = {variability engineering, teaching, feature model, cognitive psychology},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3555776.3578613,
author = {T\"{e}rnava, Xhevahire and Acher, Mathieu and Combemale, Benoit},
title = {Specialization of Run-time Configuration Space at Compile-time: An Exploratory Study},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3578613},
doi = {10.1145/3555776.3578613},
abstract = {Numerous software systems are highly configurable through runtime options (e.g., command-line parameters). Users can tune some of the options to meet various functional and non-functional requirements such as footprint, security, or execution time. However, some options are never set for a given system instance, and their values remain the same whatever the use cases of the system. Herein, we design a controlled experiment in which the system's run-time configuration space can be specialized at compile-time and combinations of options can be removed on demand. We perform an in-depth study of the well-known x264 video encoder and quantify the effects of its specialization to its non-functional properties, namely on binary size, attack surface, and performance while ensuring its validity. Our exploratory study suggests that the configurable specialization of a system has statistically significant benefits on most of its analysed non-functional properties, which benefits depend on the number of the debloated options. While our empirical results and insights show the importance of removing code related to unused run-time options to improve software systems, an open challenge is to further automate the specialization process.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1459–1468},
numpages = {10},
keywords = {unused variability, debloating, performance, program specialization},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.5555/1753235.1753265,
author = {Weston, Nathan and Chitchyan, Ruzanna and Rashid, Awais},
title = {A framework for constructing semantically composable feature models from natural language requirements},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Software Product Line Engineering (SPLE) requires the construction of feature models from large, unstructured and heterogeneous documents, and the reliable derivation of product variants from the resulting model. This can be an arduous task when performed manually, and can be error-prone in the presence of a change in requirements. In this paper we introduce a tool suite which automatically processes natural-language requirements documents into a candidate feature model, which can be refined by the requirements engineer. The framework also guides the process of identifying variant concerns and their composition with other features. We also provide language support for specifying semantic variant feature compositions which are resilient to change. We show that feature models produced by this framework compare favourably with those produced by domain experts by application to a real-life industrial example.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {211–220},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2362536.2362544,
author = {Dietrich, Christian and Tartler, Reinhard and Schr\"{o}der-Preikschat, Wolfgang and Lohmann, Daniel},
title = {A robust approach for variability extraction from the Linux build system},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362544},
doi = {10.1145/2362536.2362544},
abstract = {With more than 11,000 optional and alternative features, the Linux kernel is a highly configurable piece of software. Linux is generally perceived as a textbook example for preprocessor-based product derivation, but more than 65 percent of all features are actually handled by the build system. Hence, variability-aware static analysis tools have to take the build system into account.However, extracting variability information from the build system is difficult due to the declarative and turing-complete make language. Existing approaches based on text processing do not cover this challenges and tend to be tailored to a specific Linux version and architecture. This renders them practically unusable as a basis for variability-aware tool support -- Linux is a moving target!We describe a robust approach for extracting implementation variability from the Linux build system. Instead of extracting the variability information by a text-based analysis of all build scripts, our approach exploits the build system itself to produce this information. As our results show, our approach is robust and works for all versions and architectures from the (git-)history of Linux.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {21–30},
numpages = {10},
keywords = {static analysis, maintenance, kbuild, configurability, build systems, VAMOS, Linux},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2019136.2019176,
author = {Gomaa, Hassan and Hashimoto, Koji},
title = {Dynamic software adaptation for service-oriented product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019176},
doi = {10.1145/2019136.2019176},
abstract = {This paper describes a dynamic software adaptation approach and environment for service-oriented product lines. The approach uses a dynamic feature model and product line architecture for a family of service-oriented architectures (SOA), in which a member of the SOA can be dynamically adapted to a different member of the family at run-time. The approach integrates software product line and feature modeling concepts with SOA and dynamic software adaptation concepts. The approach is validated using a case study of a dynamic service-oriented product line.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {35},
numpages = {8},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1145/3528100,
author = {Cheng, Jiezhu and Gao, Cuiyun and Zheng, Zibin},
title = {HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3528100},
doi = {10.1145/3528100},
abstract = {Modern software systems are usually highly configurable, providing users with customized functionality through various configuration options. Understanding how system performance varies with different option combinations is important to determine optimal configurations that meet specific requirements. Due to the complex interactions among multiple options and the high cost of performance measurement under a huge configuration space, it is challenging to study how different configurations influence the system performance. To address these challenges, we propose HINNPerf, a novel hierarchical interaction neural network for performance prediction of configurable systems. HINNPerf employs the embedding method and hierarchic network blocks to model the complicated interplay between configuration options, which improves the prediction accuracy of the method. In addition, we devise a hierarchical regularization strategy to enhance the model robustness. Empirical results on 10 real-world configurable systems show that our method statistically significantly outperforms state-of-the-art approaches by achieving average 22.67% improvement in prediction accuracy. In addition, combined with the Integrated Gradients method, the designed hierarchical architecture provides some insights about the interaction complexity and the significance of configuration options, which might help users and developers better understand how the configurable system works and efficiently identify significant options affecting the performance.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {46},
numpages = {30},
keywords = {machine learning, deep neural network, highly configurable systems, Software performance prediction}
}

@inproceedings{10.1145/2019136.2019159,
author = {Otsuka, Jun and Kawarabata, Kouichi and Iwasaki, Takashi and Uchiba, Makoto and Nakanishi, Tsuneo and Hisazumi, Kenji},
title = {Small inexpensive core asset construction for large gainful product line development: developing a communication system firmware product line},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019159},
doi = {10.1145/2019136.2019159},
abstract = {Product line development of communication system firmware with more than 2,000 features was performed in a large-scale project that involved more than 300 engineers (at a maximum) across four distributed sites. However, since intense demands to reduce development costs and time made it prohibitive to construct core assets for all those identified features, the project screened a limited number of the features, for which core assets were constructed, and then performed partial application of product line engineering. Nevertheless, when compared with previously engineered derivative developments, when the second product of the product line was released, it was clear that the project had achieved significant improvements in quality, as well as reductions in development costs and time requirements. Automatic code generation also contributed to those improvements.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {20},
numpages = {5},
keywords = {product line, feature modeling, core assets, case study},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2019136.2019161,
author = {Pleuss, Andreas and Rabiser, Rick and Botterweck, Goetz},
title = {Visualization techniques for application in interactive product configuration},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019161},
doi = {10.1145/2019136.2019161},
abstract = {In product line engineering (PLE) a major challenge is the complexity of artifacts that have to be handled. In real-world product lines, variability models can become large and complex comprising thousands of elements with hundreds of non-trivial dependencies. Visual and interactive techniques aim to reduce the (cognitive) complexity and support the user during challenging PLE tasks like product configuration. There are many visualization techniques described in the literature -- e.g., in Software Visualization -- and some isolated techniques have been applied in PLE tools. Nevertheless, the full potential of visualization in the context of PLE has not been exploited so far. This paper provides an overview of (1) available visualization techniques and criteria to judge their benefits and drawbacks for product configuration, (2) which have been applied in product configuration in PLE, and (3) which could be beneficial to support product configuration. We propose a research agenda for future work in visual and interactive PLE techniques.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {22},
numpages = {8},
keywords = {software visualization, product line engineering, product configuration},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3194078.3194082,
author = {Pukhkaiev, Dmytro and G\"{o}tz, Sebastian},
title = {BRISE: energy-efficient benchmark reduction},
year = {2018},
isbn = {9781450357326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194078.3194082},
doi = {10.1145/3194078.3194082},
abstract = {A considerable portion of research activities in computer science heavily relies on the process of benchmarking, e.g., to evaluate a hypothesis in an empirical study. The goal is to reveal how a set of independent variables (factors) influences one or more dependent variables. With a vast number of factors or a high amount of factors' values (levels), this process becomes time- and energy-consuming. Current approaches to lower the benchmarking effort suffer from two deficiencies: (1) they focus on reducing the number of factors and, hence, are inapplicable to experiments with only two factors, but a vast number of levels and (2) being adopted from, e.g., combinatorial optimization they are designed for a different search space structure and, thus, can be very wasteful. This paper provides an approach for benchmark reduction, based on adaptive instance selection and multiple linear regression. We evaluate our approach using four empirical studies, which investigate the effect made by dynamic voltage and frequency scaling in combination with dynamic concurrency throttling on the energy consumption of a computing system (parallel compression, sorting, and encryption algorithms as well as database query processing). Our findings show the effectiveness of the approach. We can save 78% of benchmarking effort, while the result's quality decreases only by 3 pp, due to using only a near-optimal configuration.},
booktitle = {Proceedings of the 6th International Workshop on Green and Sustainable Software},
pages = {23–30},
numpages = {8},
keywords = {non-functional properties, fractional factorial design, benchmarking, adaptive instance selection, active learning},
location = {Gothenburg, Sweden},
series = {GREENS '18}
}

@inproceedings{10.1145/2364412.2364424,
author = {Gnesi, Stefania and Petrocchi, Marinella},
title = {Towards an executable algebra for product lines},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364424},
doi = {10.1145/2364412.2364424},
abstract = {We propose the Controlled Language for Software Product Lines CL4SPL with the twofold aim of ensuring simplicity of use for product line engineers and safe translations to executable languages amenable for automated verification. We show an implementation of CL4SPL in Maude, a well-known rewrite engine, thus allowing formal analyses over product families specified with CL4SPL. We illustrate our approach with a toy family of coffee machines.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {66–73},
numpages = {8},
keywords = {variability, product families, automated analysis, Maude},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1145/3611663,
author = {Oh, Jeho and Batory, Don and Heradio, Rub\'{e}n},
title = {Finding Near-optimal Configurations in Colossal Spaces with Statistical Guarantees},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3611663},
doi = {10.1145/3611663},
abstract = {A Software Product Line (SPL) is a family of similar programs. Each program is defined by a unique set of features, called a configuration, that satisfies all feature constraints. “What configuration achieves the best performance for a given workload?” is the SPLOptimization (SPLO) challenge. SPLO is daunting: just 80 unconstrained features yield 1024 unique configurations, which equals the estimated number of stars in the universe. We explain (a) how uniform random sampling and random search algorithms solve SPLO more efficiently and accurately than current machine-learned performance models and (b) how to compute statistical guarantees on the quality of a returned configuration; i.e., it is within x% of optimal with y% confidence.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {7},
numpages = {36},
keywords = {Software product lines, configuration optimization, product spaces, machine learning, uniform random sampling, random search, order statistics}
}

@inproceedings{10.1145/2771783.2771808,
author = {Tan, Tian Huat and Xue, Yinxing and Chen, Manman and Sun, Jun and Liu, Yang and Dong, Jin Song},
title = {Optimizing selection of competing features via feedback-directed evolutionary algorithms},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2771808},
doi = {10.1145/2771783.2771808},
abstract = {Software that support various groups of customers usually require complicated configurations to attain different functionalities. To model the configuration options, feature model is proposed to capture the commonalities and competing variabilities of the product variants in software family or Software Product Line (SPL). A key challenge for deriving a new product is to find a set of features that do not have inconsistencies or conflicts, yet optimize multiple objectives (e.g., minimizing cost and maximizing number of features), which are often competing with each other. Existing works have attempted to make use of evolutionary algorithms (EAs) to address this problem. In this work, we incorporated a novel feedback-directed mechanism into existing EAs. Our empirical results have shown that our method has improved noticeably over all unguided version of EAs on the optimal feature selection. In particular, for case studies in SPLOT and LVAT repositories, the feedback-directed Indicator-Based EA (IBEA) has increased the number of correct solutions found by 72.33% and 75%, compared to unguided IBEA. In addition, by leveraging a pre-computed solution, we have found 34 sound solutions for Linux X86, which contains 6888 features, in less than 40 seconds.},
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {246–256},
numpages = {11},
keywords = {evolutionary algorithms, Software product line, SAT solvers},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@inproceedings{10.5555/1753235.1753245,
author = {Cetina, Carlos and Haugen, \O{}ystein and Zhang, Xiaorui and Fleurey, Franck and Pelechano, Vicente},
title = {Strategies for variability transformation at run-time},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {More and more approaches propose to use Software Product Lines (SPLs) modelling techniques to implement dynamic adaptive systems. The resulting Dynamic Software Product Lines (DSPLs) present new challenges since the variability transformations used to derive alternative configurations have to be intensively used at runtime. This paper proposes to use the Common Variability Language (CVL) for modelling runtime variability and evaluates a set of alternative strategies for implementing the associated variability transformations. All the proposed strategies have been implemented and evaluated on the case-study of a smart-home system. Results show that the proposed strategies provide the same reconfiguration service with significant differences in quality-of-service.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {61–70},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.5555/1753235.1753263,
author = {Than Tun, Thein and Boucher, Quentin and Classen, Andreas and Hubaux, Arnaud and Heymans, Patrick},
title = {Relating requirements and feature configurations: a systematic approach},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {A feature model captures various possible configurations of products within a product family. When configuring a product, several features are selected and composed. Selecting features at the program level has a general limitation of not being able to relate the resulting configuration to its requirements. As a result, it is difficult to decide whether a given configuration of features is optimal. An optimal configuration satisfies all stakeholder requirements and quantitative constraints, while ensuring that there is no extraneous feature in it. In relating requirements and feature configurations, we use the description of the problem world context in which the software is designed to operate as the intermediate description between them. The advantage of our approach is that feature selection can be done at the requirements level, and an optimal program level configuration can be generated from the requirements selected. Our approach is illustrated with a real-life problem of configuring a satellite communication software. The use of an existing tool to support our approach is also discussed.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {201–210},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2647908.2655957,
author = {Murguzur, Aitor and Capilla, Rafael and Trujillo, Salvador and Ortiz, \'{O}scar and Lopez-Herrejon, Roberto E.},
title = {Context variability modeling for runtime configuration of service-based dynamic software product lines},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655957},
doi = {10.1145/2647908.2655957},
abstract = {In emerging domains such as Cloud-based Industrial Control Systems (ICSs) and SCADA systems where data-intensive and high performance computing are needed, a higher degree of flexibility is being demanded to meet new stakeholder requirements, context changes and intrinsic complexity. In this light, Dynamic Software Product Lines (DSPLs) provide a way to build self-managing systems exploiting traditional product line engineering concepts at runtime. Although context-awareness is widely perceived to be a first-class concern in such runtime variability mechanisms, existing approaches do not provide the necessary level of formalization to model and enact context variability for DSPLs. This is crucial for operational analytics processes since variant configuration could differ from context to context depending on diverse data values linked to context features and cross-tree constraints in a feature model. In this paper, we propose a context variability modeling approach, demonstrate its applicability and usability via a wind farm use case, and present the fundamental building blocks of a framework for enabling context variability in service-based DSPLs which provide Workflow as a Service (WFaaS).},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {2–9},
numpages = {8},
keywords = {process variability, data-aware systems, context variability, context awareness},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2647908.2655965,
author = {Holthusen, S\"{o}nke and Wille, David and Legat, Christoph and Beddig, Simon and Schaefer, Ina and Vogel-Heuser, Birgit},
title = {Family model mining for function block diagrams in automation software},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655965},
doi = {10.1145/2647908.2655965},
abstract = {Automation systems are mostly individual highly customized system variants, consisting both of hardware and software. In order to reduce development effort, it is a common practice to use a clone-and-own approach by modifying an existing variant to fit the changed requirements of a new variant. The information about the commonalities and differences between those variants is usually not well documented and leads to problems in maintenance, testing and evolution. To alleviate these problems, in this paper, we present an improved version of a family mining approach for automatically discovering commonality and variability between related system variants. We apply this approach to function block diagrams used to develop automation software and show its feasibility by a manufacturing case study.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {36–43},
numpages = {8},
keywords = {software engineering, re-engineering, family mining, automation software},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2019136.2019152,
author = {Kozuka, Nobuaki and Ishida, Yuzo},
title = {Building a product line architecture for variant-rich enterprise applications using a data-oriented approach},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019152},
doi = {10.1145/2019136.2019152},
abstract = {IT industry in Japan has grown by providing specific made-to-order enterprise applications for various industries. Most of enterprise applications are built upon relational database management system (RDBMS), which takes the responsibility of keeping data integrity and data manipulation. However, data explosion in recent years especially in retail and telecommunication industries makes IT industry difficult to satisfy quality attributes such as scalability, availability and data consistency with traditional development techniques. From the beginning of this century, NRI has built and refined product line architecture as a primary core asset for such data intensive industries, which have very rich variations in functional and nonfunctional requirements of their enterprise applications. This paper summarizes key criteria to build such an architecture based on our ten years experience in developing dozens of mission critical IT systems as product families for those industries.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {14},
numpages = {6},
keywords = {relational database management system, quality attributes, product line architecture, enterprise applications, data oriented approach, data intensiveness, core asset development},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2648511.2648514,
author = {Seidl, Christoph and Schaefer, Ina and A\ss{}mann, Uwe},
title = {Integrated management of variability in space and time in software families},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648514},
doi = {10.1145/2648511.2648514},
abstract = {Software product lines (SPLs) and software ecosystems (SECOs) encompass a family of closely related software systems in terms of common and variable assets that are configured to concrete products (variability in space). Over the course of time, variable assets of SPLs and especially SECOs are subject to change in order to meet new requirements as part of software evolution (variability in time). Even though both dimensions of variability have to be handled simultaneously, e.g., as not all customers upgrade their respective products immediately or completely, there currently is no approach that can create variants with a selection of variable assets in various versions. In this paper, we introduce an integrated approach to manage variability in space and time in software families using Hyper Feature Models (HFMs) with feature versions and combine them with an extension of the transformational variability realization mechanism delta modeling. This allows derivation of concrete software systems from an SPL or SECO configuring both functionality (features) as well as versions.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {22–31},
numpages = {10},
keywords = {variability, software product lines, software ecosystems, hyper feature models (HFMs), evolution, delta modeling},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.5555/1753235.1753241,
author = {John, Isabel and Eisenbarth, Michael},
title = {A decade of scoping: a survey},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Scoping can be defined as the process of deciding in which parts of an organization's products, features and domains systematic reuse is economically useful. It generally is the first phase in product line engineering. For a decade now scoping has been recognized as a discipline of it's own in product line engineering. So it's time to look at what has been done in scoping in the last years and what is still to be done. In this survey, we identify and characterize existing scoping approaches with the main goal to derive open areas and research questions for further research in scoping. We analyze and compare existing approaches and derive open and partially addressed research questions that can be tackled by researchers in product line engineering in the next years.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {31–40},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2647908.2655969,
author = {ter Beek, Maurice H. and Mazzanti, Franco},
title = {VMC: recent advances and challenges ahead},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655969},
doi = {10.1145/2647908.2655969},
abstract = {The variability model checker VMC accepts a product family specified as a Modal Transition System (MTS) with additional variability constraints. Consequently, it offers behavioral variability analyses over both the family and its valid product behavior. This ranges from product derivation and simulation to efficient on-the-fly model checking of logical properties expressed in a variability-aware version of action-based CTL. In this paper, we first explain the reasons and assumptions underlying the choice for a modeling and analysis framework based on MTSs. Subsequently, we present recent advances on proving inheritance of behavioral analysis properties from a product family to its valid products. Finally, we illustrate challenges remaining for the future.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {70–77},
numpages = {8},
keywords = {behavioral variability, model checking, product families},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/1655925.1656013,
author = {Alsawalqah, Hamad I. and Abotsi, Komi S. and Lee, Dan Hyung},
title = {An automated mechanism for organizing and retrieving core asset artifacts for product derivation in SPL},
year = {2009},
isbn = {9781605587103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1655925.1656013},
doi = {10.1145/1655925.1656013},
abstract = {Software Product Line, SPL, is a software development strategy in which products are developed from a common set of core assets in a prescribed way with product specific features to satisfy specific market segment [1]. The SPL development process is carried out in two phases: the first phase is about building core assets called domain engineering, which has gained a lot of researchers' attention. The second step is about instantiating the specifics of the products by adding to the common part the specific features that identify the product from the other application engineering. For large and complex domains, it is argued that organizing and retrieving the development of artifacts from the core asset required by the application under development is a way of shortening the application development time, thus reduces the time to market. In this paper, we propose an automation mechanism for organizing the core assets using feature based organization to divide the customized domain feature model based on the application features and their dependencies. When that retrieval step where the artifacts are represented by relations that inherit the dependencies between the features in each division of the feature model, takes place, the final result is a set of development artifacts with their traceability links to be customized based on the application variability model and integrated with the application specific artifacts. To demonstrate our work, we applied this mechanism on a watch, a case study in the digital watch domain.},
booktitle = {Proceedings of the 2nd International Conference on Interaction Sciences: Information Technology, Culture and Human},
pages = {480–485},
numpages = {6},
keywords = {digital watch, feature model, ontology, product derivation, software product line},
location = {Seoul, Korea},
series = {ICIS '09}
}

@inproceedings{10.1145/2647908.2655958,
author = {Eichelberger, Holger and Schmid, Klaus},
title = {Resource-optimizing adaptation for big data applications},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655958},
doi = {10.1145/2647908.2655958},
abstract = {The resource requirements of Big Data applications may vary dramatically over time, depending on changes in the context. If resources should not be defined for the maximum case, but available resources are mostly static, there is a need to adapt resource usage by modifying the processing behavior. The QualiMaster project researches such an approach for the analysis of systemic risks in the financial markets.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {10–11},
numpages = {2},
keywords = {QualiMaster, adaptive systems, financial markets, resource adaptation, stream-processing, systematic-risks},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2019136.2019146,
author = {Gerlach, Simon},
title = {Improving efficiency when deriving numerous products from software product lines simultaneously},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019146},
doi = {10.1145/2019136.2019146},
abstract = {In-car infotainment systems must allow for product differentiation and the adaption to the needs of different markets. Product line approaches are applied because large numbers of different product variants need to be developed simultaneously. During development, updated versions of each projected product variant need to be derived from the product line assets repeatedly. Current build tools create each of the numerous product variants one after another. Accordingly, the creation process can take much time. This paper presents an approach to abbreviate this creation process based on the fact that multiple product variants created at once can have parts in common. To benefit from this optimization potential the workflow that creates an individual product variant is subdivided into multiple fragments. Whenever a set of such product variants needs to be created, an optimization algorithm then calculates an individual execution order of the fragments for this set. This order minimizes the total execution time by a systematic reuse of workflow fragment's results for the creation of multiple different product variants.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {9},
numpages = {4},
keywords = {application engineering, automotive, product configuration, product derivation, software product lines},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3084100.3084109,
author = {Hohl, Philipp and Ghofrani, Javad and M\"{u}nch, J\"{u}rgen and Stupperich, Michael and Schneider, Kurt},
title = {Searching for common ground: existing literature on automotive agile software product lines},
year = {2017},
isbn = {9781450352703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3084100.3084109},
doi = {10.1145/3084100.3084109},
abstract = {The digital transformation of the automotive industry has a significant impact on how development processes need to be organized in future. Dynamic market and technological environments require capabilities to react on changes and to learn fast. Agile methods are a promising approach to address these needs but they are not tailored to the specific characteristics of the automotive domain like product line development. Although, there have been efforts to apply agile methods in the automotive domain for many years, significant and widespread adoptions have not yet taken place. The goal of this literature review is to gain an overview and a better understanding of agile methods for embedded software development in the automotive domain, especially with respect to product line development. A mapping study was conducted to analyze the relation between agile software development, embedded software development in the automotive domain and software product line development. Three research questions were defined and 68 papers were evaluated. The study shows that agile and product line development approaches tailored for the automotive domain are not yet fully explored in the literature. Especially, literature on the combination of agile and product line development is rare. Most of the examined combinations are customizations of generic approaches or approaches stemming from other domains. Although, only few approaches for combining agile and software product line development in the automotive domain were found, these findings were valuable for identifying research gaps and provide insights into how existing approaches can be combined, extended and tailored to suit the characteristics of the automotive domain.},
booktitle = {Proceedings of the 2017 International Conference on Software and System Process},
pages = {70–79},
numpages = {10},
keywords = {agile methods, automotive, literature review, software product line},
location = {Paris, France},
series = {ICSSP '17}
}

@inproceedings{10.5555/1753235.1753270,
author = {Slegers, Walter J.},
title = {Building automotive product lines around managed interfaces},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {TomTom is extending its current business of portable navigation devices into the embedded automotive navigation domain. Portable navigation devices have a high pace of innovation and moderate diversity. Automotive devices traditionally have a slower pace of innovation and high diversity. Their integration in the vehicle needs to comply with formal and intrusive automotive requirements.How can both worlds be combined, offering an increased innovation and reduced lead time in the automotive domain? We introduced an architectural decoupling with explicit management of interfaces to support a product line approach with systematic reuse across business units enabling an increase of diversity in these different market segments.This paper describes business, architecture, organization, and process aspects of this approach with special attention to the architecture and the management of interfaces.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {257–264},
numpages = {8},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2684200.2684314,
author = {Murwantara, I Made and Bordbar, Behzad and Minku, Leandro L.},
title = {Measuring Energy Consumption for Web Service Product Configuration},
year = {2014},
isbn = {9781450330015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684200.2684314},
doi = {10.1145/2684200.2684314},
abstract = {Because of the economies of scale that Cloud provides, there is great interest in hosting web services on the Cloud. Web services are created from components such as Database Management Systems and HTTP servers. There is a wide variety of components that can be used to configure a web service. The choice of components influences the performance and energy consumption. Most current research in the web service technologies focuses on system performance, and only small number of researchers give attention to energy consumption. In this paper, we propose a method to select the web service configurations which reduce energy consumption. Our method has capabilities to manage feature configuration and predict energy consumption of web service systems. To validate, we developed a technique to measure energy consumption of several web service configurations running in a Virtualized environment. Our approach allows Cloud companies to provide choices of web service technology that consumes less energy.},
booktitle = {Proceedings of the 16th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {224–228},
numpages = {5},
keywords = {Energy Aware, Machine Learning, Software Product Line, Web System},
location = {Hanoi, Viet Nam},
series = {iiWAS '14}
}

@inproceedings{10.1145/2499777.2499779,
author = {Antkiewicz, Micha\l{} and B\k{a}k, Kacper and Murashkin, Alexandr and Olaechea, Rafael and Liang, Jia Hui (Jimmy) and Czarnecki, Krzysztof},
title = {Clafer tools for product line engineering},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499779},
doi = {10.1145/2499777.2499779},
abstract = {Clafer is a lightweight yet expressive language for structural modeling: feature modeling and configuration, class and object modeling, and metamodeling. Clafer Tools is an integrated set of tools based on Clafer. In this paper, we describe some product-line variability modeling scenarios of Clafer Tools from the viewpoints of product-line owner, product-line engineer, and product engineer.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {130–135},
numpages = {6},
keywords = {Clafer, ClaferIG, ClaferMOO, ClaferMOO visualizer, ClaferWiki, clafer configurator},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.5555/3712729.3712796,
author = {Xie, Shufang and Zhang, Tao and Rose, Oliver and Uhlig, Tobias and Vollack, Bj\"{o}rn},
title = {Mixed Energy and Production Scheduling in an Eco-Industrial Park},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {In recent times, eco-industrial parks (EIP) have taken on a significant role in addressing environmental challenges and supporting sustainable practices. To enhance the efficient utilization of energy within the park and ensure the achievement of production targets among its members, the implementation of energy and production scheduling is imperative. This paper addresses this optimization challenge by formulating it into a constraint programming (CP) model. The optimized scheduling solutions generated by the CP model will be directly used to guide the operations within the simulation environment of the EIP. The paper compares the CP results with the results of another method which we developed in our previous research stage. The outcomes of CP solver demonstrate its effectiveness in optimizing energy utilization and meeting production targets. This research contributes to developing a practical decision support system for resolving real-life scheduling problems in industry parks.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {810–821},
numpages = {12},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@inproceedings{10.5555/1753235.1753249,
author = {Montagud, Sonia and Abrah\~{a}o, Silvia},
title = {Gathering current knowledge about quality evaluation in software product lines},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Recently, a number of methods and techniques for assessing the quality of software product lines have been proposed. However, to the best of our knowledge, there is no study which summarizes all the existing evidence about them. This paper presents a systematic review that investigates what methods and techniques have been employed (in the last 10 years) to evaluate the quality of software product lines and how they were employed. A total of 39 research papers have been reviewed from an initial set of 1388 papers. The results show that 25% of the papers reported evaluations at the Design phase of the Domain Engineering phase. The most widely used mechanism for modeling quality attributes was extended feature models and the most evaluated artifact was the base architecture. In addition, the results of the review have identified several research gaps. Specifically, 77% of the papers employed case studies as a "proof of concept" whereas 23% of the papers did not perform any type of validation. Our results are particularly relevant in positioning new research activities and in the selection of quality evaluation methods or techniques that best fit a given purpose.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {91–100},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/3030207.3030226,
author = {Stefan, Petr and Horky, Vojtech and Bulej, Lubomir and Tuma, Petr},
title = {Unit Testing Performance in Java Projects: Are We There Yet?},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3030226},
doi = {10.1145/3030207.3030226},
abstract = {Although methods and tools for unit testing of performance exist for over a decade, anecdotal evidence suggests unit testing of performance is not nearly as common as unit testing of functionality. We examine this situation in a study of GitHub projects written in Java, looking for occurrences of performance evaluation code in common performance testing frameworks. We quantify the use of such frameworks, identifying the most relevant performance testing approaches, and describe how we adjust the design of our SPL performance testing framework to follow these conclusions.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {401–412},
numpages = {12},
keywords = {jmh, open source, performance unit testing, spl, survey},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@inproceedings{10.5555/1753235.1753274,
author = {Pech, Daniel and Knodel, Jens and Carbon, Ralf and Schitter, Clemens and Hein, Dirk},
title = {Variability management in small development organizations: experiences and lessons learned from a case study},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Product line practices promise to reduce development and maintenance efforts, to improve the productivity and to reduce the time to market by systematic reuse of commonalities and variabilities. However, in order to reap the fruits of exploiting those, an upfront investment is required. This paper presents a case study, which analyzes the cost-benefit ratio for one product line discipline -- variability management. Wikon GmbH -- a small German development organization evolving a product line of remote monitoring and controlling devices -- switched from manual, file-based conditional compilation to tool-supported decision models. We discuss experiences made and show that the break-even was reached with the 4th product derivation.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {285–294},
numpages = {10},
keywords = {decision model, evolution, product line engineering, software architecture, variability management},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/3696409.3700196,
author = {Jiang, Tao and Hou, Feng and Wang, Yi},
title = {Multimodal Energy Prompting for Video Salient Object Detection},
year = {2024},
isbn = {9798400712739},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696409.3700196},
doi = {10.1145/3696409.3700196},
abstract = {Video salient object detection (VSOD) identifies the most eye-catching objects in videos and extracts crucial information from complex scenes. However, existing VSOD methods primarily utilize spatio-temporal information and seldom deeply exploit optical flow and depth for additional information. We introduce an innovative network named MEPNet, which leverages the learned knowledge from the Segment Anything Model (SAM) alongside a multimodal energy prompt. We first devise a Spectrogram Energy Generator (SEG) to extract the spectrogram energy from the optical flow and depth. Unlike existing methods that directly fuse multimodality information, we use the extracted spectrogram energy to prompt the SAM encoder to capture the dynamic nature of videos more efficiently. Additionally, we propose a Circular High-frequency Filter (CHF) to enhance RGB modality details with an adaptive circular mask that is more intuitive and avoids the drawbacks of hand-crafted design. Experiments on five VSOD benchmark datasets demonstrate that our network significantly outperforms existing methods, highlighting the effectiveness of integrating multimodality prompts and advanced filtering techniques.},
booktitle = {Proceedings of the 6th ACM International Conference on Multimedia in Asia},
articleno = {35},
numpages = {8},
keywords = {Video salient object detection, Prompt learning, Multimodality, Spectrogram energy},
location = {
},
series = {MMAsia '24}
}

@inproceedings{10.5555/1753235.1753257,
author = {Carbon, Ralf and Adam, Sebastian and Uchida, Takayuki},
title = {Towards a product line approach for office devices: facilitating customization of office devices at Ricoh Co. Ltd.},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Like in many other domains, customization is one of the major challenges for providers of office devices like Ricoh Co. Ltd. The specific challenge is the integration of office devices with the workflows and existing office infrastructures of customers. Hence, an approach to support customization of office devices has been developed by Ricoh and Fraunhofer IESE. The approach builds upon product line engineering and focuses on improving application engineering to support customization based on the workflows and office infrastructures of specific customers. The key ideas of the approach are flexibility concepts on architecture level that support recurring types of customizations and a requirements engineering process that is driven by the workflows of individual customers. In this paper, the approach developed in cooperation with Ricoh is presented. A case study illustrates the applicability of the concepts and the overall approach.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {151–160},
numpages = {10},
keywords = {application engineering, flexibility, product line architecture, product line engineering, service orientation},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/3626246.3653378,
author = {Pavlenko, Anna and Cahoon, Joyce and Zhu, Yiwen and Kroth, Brian and Nelson, Michael and Carter, Andrew and Liao, David and Wright, Travis and Camacho-Rodr\'{\i}guez, Jes\'{u}s and Saur, Karla},
title = {Vertically Autoscaling Monolithic Applications with CaaSPER: Scalable Container-as-a-Service Performance Enhanced Resizing Algorithm for the Cloud},
year = {2024},
isbn = {9798400704222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626246.3653378},
doi = {10.1145/3626246.3653378},
abstract = {Kubernetes has emerged as a prominent open-source platform for managing cloud applications, including stateful databases. These monolithic applications rely on vertical scaling, adjusting CPU cores based on load fluctuations. However, our analysis of Kubernetes-based Database-as-a-Service (DBaaS) offerings at Microsoft revealed that many customers consistently over-provision resources for peak workloads, neglecting cost-saving opportunities through resource scale-down. We found that there is a gap in the ability of existing vertical autoscaling tools to minimize resource slack and respond promptly to throttling, leading to increased costs and impacting crucial metrics such as throughput and availability.To address this challenge, we propose CaaSPER, a vertical autoscaling algorithm that blends reactive and proactive strategies. By dynamically adjusting CPU resources, CaaSPER minimizes resource slack, maintains optimal CPU utilization, and reduces throttling. Importantly, customers have the flexibility to prioritize either cost savings or high performance based on their preferences. Extensive testing demonstrates that CaaSPER effectively reduces throttling and keeps CPU utilization within target levels. CaaSPER is designed to be application-agnostic and platform-agnostic, with potential for extension to other applications requiring vertical autoscaling.},
booktitle = {Companion of the 2024 International Conference on Management of Data},
pages = {241–254},
numpages = {14},
keywords = {containers, kubernetes, resource optimization, vertical auto-scaling},
location = {Santiago AA, Chile},
series = {SIGMOD '24}
}

@inproceedings{10.1145/2188286.2188304,
author = {Tawhid, Rasha and Petriu, Dorina},
title = {User-friendly approach for handling performance parameters during predictive software performance engineering},
year = {2012},
isbn = {9781450312028},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2188286.2188304},
doi = {10.1145/2188286.2188304},
abstract = {A Software Product Line (SPL) is a set of similar software systems that share a common set of features. Instead of building each product from scratch, SPL development takes advantage of the reusability of the core assets shared among the SPL members. In this work, we integrate performance analysis in the early phases of SPL development process, applying the same reusability concept to the performance annotations. Instead of annotating from scratch the UML model of every derived product, we propose to annotate the SPL model once with generic performance annotations. After deriving the model of a product from the family model by an automatic transformation, the generic performance annotations need to be bound to concrete product-specific values provided by the developer. Dealing manually with a large number of performance annotations, by asking the developer to inspect every diagram in the generated model and to extract these annotations is an error-prone process. In this paper we propose to automate the collection of all generic parameters from the product model and to present them to the developer in a user-friendly format (e.g., a spreadsheet per diagram, indicating each generic parameter together with guiding information that helps the user in providing concrete binding values). There are two kinds of generic parametric annotations handled by our approach: product-specific (corresponding to the set of features selected for the product) and platform-specific (such as device choices, network connections, middleware, and runtime environment). The following model transformations for (a) generating a product model with generic annotations from the SPL model, (b) building the spreadsheet with generic parameters and guiding information, and (c) performing the actual binding are all realized in the Atlas Transformation Language (ATL).},
booktitle = {Proceedings of the 3rd ACM/SPEC International Conference on Performance Engineering},
pages = {109–120},
numpages = {12},
keywords = {atl, marte, model-driven development, performance completion, performance model, spl, uml},
location = {Boston, Massachusetts, USA},
series = {ICPE '12}
}

@inproceedings{10.5555/1753235.1753267,
author = {Mendonca, Marcilio and W\k{a}sowski, Andrzej and Czarnecki, Krzysztof},
title = {SAT-based analysis of feature models is easy},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Feature models are a popular variability modeling notation used in product line engineering. Automated analyses of feature models, such as consistency checking and interactive or offline product selection, often rely on translating models to propositional logic and using satisfiability (SAT) solvers.Efficiency of individual satisfiability-based analyses has been reported previously. We generalize and quantify these studies with a series of independent experiments. We show that previously reported efficiency is not incidental. Unlike with the general SAT instances, which fall into easy and hard classes, the instances induced by feature modeling are easy throughout the spectrum of realistic models. In particular, the phenomenon of phase transition is not observed for realistic feature models.Our main practical conclusion is a general encouragement for researchers to continued development of SAT-based methods to further exploit this efficiency in future.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {231–240},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/3660515.3661325,
author = {Mei\ss{}ner, Simon and Degbelo, Auriol},
title = {User Performance Modelling for Spatial Entities Comparison with Geodashboards: Using View Quality and Distractor as Concepts},
year = {2024},
isbn = {9798400706516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660515.3661325},
doi = {10.1145/3660515.3661325},
abstract = {Geodashboards are increasingly available but there is still a lack of understanding about the design elements that contribute to a positive user experience. This work addresses that gap with a focus on the task of comparing spatial entities. The study explored ways of quantitatively modelling distraction and view quality during the use of geodashboards in question-answering scenarios. Eight types of questions related to the comparison of spatial entities were considered by varying the question-answering scenarios along three dimensions: number of comparison targets, comparison action, and map interaction operand. An exploratory study showed that an exponential model of distractors yields the best results for time and accuracy modelling. These initial findings can serve as a building block for researchers developing theoretical models for the computational design of geodashboards.},
booktitle = {Companion Proceedings of the 16th ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
pages = {7–14},
numpages = {8},
keywords = {geodashboard, mathematical models, user experience modelling, visual search},
location = {Cagliari, Italy},
series = {EICS '24 Companion}
}

@article{10.1145/3039207,
author = {Hirzel, Martin and Schneider, Scott and Gedik, Bu\u{g}ra},
title = {SPL: An Extensible Language for Distributed Stream Processing},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0164-0925},
url = {https://doi.org/10.1145/3039207},
doi = {10.1145/3039207},
abstract = {Big data is revolutionizing how all sectors of our economy do business, including telecommunication, transportation, medical, and finance. Big data comes in two flavors: data at rest and data in motion. Processing data in motion is stream processing. Stream processing for big data analytics often requires scale that can only be delivered by a distributed system, exploiting parallelism on many hosts and many cores. One such distributed stream processing system is IBM Streams. Early customer experience with IBM Streams uncovered that another core requirement is extensibility, since customers want to build high-performance domain-specific operators for use in their streaming applications. Based on these two core requirements of distribution and extensibility, we designed and implemented the Streams Processing Language (SPL). This article describes SPL with an emphasis on the language design, distributed runtime, and extensibility mechanism. SPL is now the gateway for the IBM Streams platform, used by our customers for stream processing in a broad range of application domains.},
journal = {ACM Trans. Program. Lang. Syst.},
month = mar,
articleno = {5},
numpages = {39},
keywords = {Stream processing}
}

@inproceedings{10.1145/2647908.2655980,
author = {Samih, Hamza and Bogusch, Ralf},
title = {MPLM - MaTeLo product line manager: [relating variability modelling and model-based testing]},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655980},
doi = {10.1145/2647908.2655980},
abstract = {The diversity of requirements elicited from different customers leads to the development of many variants. Furthermore, compliance with safety standards as mandated for safety-critical systems requires high test efforts for each variant. Model-based testing aims to reduce test efforts by automatically generating test cases from test models.In this paper, we introduce variability management to usage models, a widely used model-based testing formalism. We present an approach that allows to derive usage model variants from a desired set of features and thus generate test cases for each variant. The approach is integrated in the industrial model-based testing tool chain MaTeLo and exemplified using an industrial case study from the aerospace domain.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {138–142},
numpages = {5},
keywords = {MaTeLo, OVM, feature, model-based testing, model-based testing tool, product line engineering, product line manager, product line requirements, product line usage model, usage model variant, variability, variability model, variant},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3180155.3180159,
author = {Krieter, Sebastian and Th\"{u}m, Thomas and Schulze, Sandro and Schr\"{o}ter, Reimar and Saake, Gunter},
title = {Propagating configuration decisions with modal implication graphs},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180159},
doi = {10.1145/3180155.3180159},
abstract = {Highly-configurable systems encompass thousands of interdependent configuration options, which require a non-trivial configuration process. Decision propagation enables a backtracking-free configuration process by computing values implied by user decisions. However, employing decision propagation for large-scale systems is a time-consuming task and, thus, can be a bottleneck in interactive configuration processes and analyses alike. We propose modal implication graphs to improve the performance of decision propagation by precomputing intermediate values used in the process. Our evaluation results show a significant improvement over state-of-the-art algorithms for 120 real-world systems.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {898–909},
numpages = {12},
keywords = {configuration, decision propagation, software product line},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3458817.3476197,
author = {Shu, Tong and Guo, Yanfei and Wozniak, Justin and Ding, Xiaoning and Foster, Ian and Kurc, Tahsin},
title = {Bootstrapping in-situ workflow auto-tuning via combining performance models of component applications},
year = {2021},
isbn = {9781450384421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458817.3476197},
doi = {10.1145/3458817.3476197},
abstract = {In an in-situ workflow, multiple components such as simulation and analysis applications are coupled with streaming data transfers. The multiplicity of possible configurations necessitates an auto-tuner for workflow optimization. Existing auto-tuning approaches are computationally expensive because many configurations must be sampled by running the whole workflow repeatedly in order to train the auto-tuner surrogate model or otherwise explore the configuration space. To reduce these costs, we instead combine the performance models of component applications by exploiting the analytical workflow structure, selectively generating test configurations to measure and guide the training of a machine learning workflow surrogate model. Because the training can focus on well-performing configurations, the resulting surrogate model can achieve high prediction accuracy for good configurations despite training with fewer total configurations. Experiments with real applications demonstrate that our approach can identify significantly better configurations than other approaches for a fixed computer time budget.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {28},
numpages = {15},
keywords = {auto-tuning, bootstrapping, component model combination, in-situ workflow},
location = {St. Louis, Missouri},
series = {SC '21}
}

@inproceedings{10.1145/3510003.3510094,
author = {He, Haochen and Jia, Zhouyang and Li, Shanshan and Yu, Yue and Zhou, Chenglong and Liao, Qing and Wang, Ji and Liao, Xiangke},
title = {Multi-intention-aware configuration selection for performance tuning},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510094},
doi = {10.1145/3510003.3510094},
abstract = {Automatic configuration tuning helps users who intend to improve software performance. However, the auto-tuners are limited by the huge configuration search space. More importantly, they focus only on performance improvement while being unaware of other important user intentions (e.g., reliability, security). To reduce the search space, researchers mainly focus on pre-selecting performance-related parameters which requires a heavy stage of dynamically running under different configurations to build performance models. Given that other important user intentions are not paid attention to, we focus on guiding users in pre-selecting performance-related parameters in general while warning about side-effects on non-performance intentions. We find that the configuration document often, if it does not always, contains rich information about the parameters' relationship with diverse user intentions, but documents might also be long and domain-specific.In this paper, we first conduct a comprehensive study on 13 representative software containing 7,349 configuration parameters, and derive six types of ways in which configuration parameters may affect non-performance intentions. Guided by this study, we design SafeTune, a multi-intention-aware method that preselects important performance-related parameters and warns about their side-effects on non-performance intentions. Evaluation on target software shows that SafeTune correctly identifies 22--26 performance-related parameters that are missed by state-of-the-art tools but have significant performance impact (up to 14.7x). Furthermore, we illustrate eight representative cases to show that SafeTune can effectively prevent real-world and critical side-effects on other user intentions.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1431–1442},
numpages = {12},
keywords = {non-performance property, performance tuning, user intention},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/2499777.2500721,
author = {Seidl, Christoph and Schaefer, Ina and A\ss{}mann, Uwe},
title = {Variability-aware safety analysis using delta component fault diagrams},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500721},
doi = {10.1145/2499777.2500721},
abstract = {Component Fault Diagrams (CFD) allow the specification of fault propagation paths, which is employed for the design of safety-critical systems as well as their certification. Even though families of safety-critical systems exist with many similar, yet not equal, variants there is no dedicated variability mechanism for CFDs to reuse commonalities of all family members and to alter only variable parts. In this paper, we present a variability representation approach for CFDs based on delta modeling that allows to transform an initial CFD within a closed or open variant space. Furthermore, we provide delta-aware analysis techniques for CFDs in order to analyse multiple variants efficiently. We show the feasibility of our approach by means of an example scenario based on the personal home robot TurtleBot using a prototypical implementation of our concepts.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {2–9},
numpages = {8},
keywords = {component fault diagrams, delta modeling, minimum cut set, safety, software fault trees, variability},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3278122.3278123,
author = {Nieke, Michael and Mauro, Jacopo and Seidl, Christoph and Th\"{u}m, Thomas and Yu, Ingrid Chieh and Franzke, Felix},
title = {Anomaly analyses for feature-model evolution},
year = {2018},
isbn = {9781450360456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278122.3278123},
doi = {10.1145/3278122.3278123},
abstract = {Software Product Lines (SPLs) are a common technique to capture families of software products in terms of commonalities and variabilities. On a conceptual level, functionality of an SPL is modeled in terms of features in Feature Models (FMs). As other software systems, SPLs and their FMs are subject to evolution that may lead to the introduction of anomalies (e.g., non-selectable features). To fix such anomalies, developers need to understand the cause for them. However, for large evolution histories and large SPLs, explanations may become very long and, as a consequence, hard to understand. In this paper, we present a method for anomaly detection and explanation that, by encoding the entire evolution history, identifies the evolution step of anomaly introduction and explains which of the performed evolution operations lead to it. In our evaluation, we show that our method significantly reduces the complexity of generated explanations.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {188–201},
numpages = {14},
keywords = {Anomalies, Evolution, Evolution Operation, Explanation, Feature Model, Software Product Line},
location = {Boston, MA, USA},
series = {GPCE 2018}
}

@inproceedings{10.5555/2662572.2662583,
author = {Karimpour, Reza and Ruhe, Guenther},
title = {Bi-criteria genetic search for adding new features into an existing product line},
year = {2013},
isbn = {9781467362849},
publisher = {IEEE Press},
abstract = {Software product line evolution involves decisions like finding which products are better candidates for realizing new feature requests. In this paper, we propose a solution for finding trade-off evolution alternatives for products while balancing between overall value and product integrity. The purpose of this study is to support product managers with feature selection for an existing product line. For this purpose, first, the feature model of the product line is encoded into a single binary encoding. Then we employ a bi-criteria genetic search algorithm, NSGA-II, to find the possible alternatives with different value and product integrity. From the proposed set of trade-off alternatives, the product line manager can select the solutions that best fit with the concerns of their preference. The implementation has been initially evaluated by two product line configurations.},
booktitle = {Proceedings of the 1st International Workshop on Combining Modelling and Search-Based Software Engineering},
pages = {34–38},
numpages = {5},
keywords = {evolution, feature model, software product line},
location = {San Francisco, California},
series = {CMSBSE '13}
}

@inproceedings{10.1145/3324884.3416620,
author = {Dorn, Johannes and Apel, Sven and Siegmund, Norbert},
title = {Mastering uncertainty in performance estimations of configurable software systems},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416620},
doi = {10.1145/3324884.3416620},
abstract = {Understanding the influence of configuration options on performance is key for finding optimal system configurations, system understanding, and performance debugging. In prior research, a number of performance-influence modeling approaches have been proposed, which model a configuration option's influence and a configuration's performance as a scalar value. However, these point estimates falsely imply a certainty regarding an option's influence that neglects several sources of uncertainty within the assessment process, such as (1) measurement bias, (2) model representation and learning process, and (3) incomplete data. This leads to the situation that different approaches and even different learning runs assign different scalar performance values to options and interactions among them. The true influence is uncertain, though. There is no way to quantify this uncertainty with state-of-the-art performance modeling approaches. We propose a novel approach, P4, based on probabilistic programming that explicitly models uncertainty for option influences and consequently provides a confidence interval for each prediction of a configuration's performance alongside a scalar. This way, we can explain, for the first time, why predictions may cause errors and which option's influences may be unreliable. An evaluation on 12 real-world subject systems shows that P4's accuracy is in line with the state of the art while providing reliable confidence intervals, in addition to scalar predictions.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {684–696},
numpages = {13},
keywords = {P4, configurable software systems, performance-influence modeling, probabilistic programming},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1109/ASE56229.2023.00091,
author = {Xia, Yuanjie and Ding, Zishuo and Shang, Weiyi},
title = {CoMSA: A Modeling-Driven Sampling Approach for Configuration Performance Testing},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00091},
doi = {10.1109/ASE56229.2023.00091},
abstract = {Highly configurable systems enable customers to flexibly configure the systems in diverse deployment environments. The flexibility of configurations also poses challenges for performance testing. On one hand, there exist a massive number of possible configurations; while on the other hand, the time and resources are limited for performance testing, which is already a costly process during software development. Modeling the performance of configurations is one of the solutions to reduce the cost of configuration performance testing. Although prior research proposes various modeling and sampling techniques to build configuration performance models, the sampling approaches used in the model typically do not consider the accuracy of the performance models, leading to potential suboptimal performance modeling results in practice. In this paper, we present a modeling-driven sampling approach (CoMSA) to improve the performance modeling of highly configurable systems. The intuition of CoMSA is to select samples based on their uncertainties to the performance models. In other words, the configurations that have the more uncertain performance prediction results by the performance models are more likely to be selected as further training samples to improve the model. CoMSA is designed by considering both scenarios where 1) the software projects do not have historical performance testing results (cold start) and 2) there exist historical performance testing results (warm start). We evaluate the performance of our approach in four subjects, namely LRZIP, LLVM, x264, and SQLite. Through the evaluation result, we can conclude that our sampling approaches could highly enhance the accuracy of the prediction models and the efficiency of configuration performance testing compared to other baseline sampling approaches.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1352–1363},
numpages = {12},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/2351676.2351678,
author = {Harman, Mark and Langdon, William B. and Jia, Yue and White, David R. and Arcuri, Andrea and Clark, John A.},
title = {The GISMOE challenge: constructing the pareto program surface using genetic programming to find better programs (keynote paper)},
year = {2012},
isbn = {9781450312042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351676.2351678},
doi = {10.1145/2351676.2351678},
abstract = {Optimising programs for non-functional properties such as speed, size, throughput, power consumption and bandwidth can be demanding; pity the poor programmer who is asked to cater for them all at once! We set out an alternate vision for a new kind of software development environment inspired by recent results from Search Based Software Engineering (SBSE). Given an input program that satisfies the functional requirements, the proposed programming environment will automatically generate a set of candidate program implementations, all of which share functionality, but each of which differ in their non-functional trade offs. The software designer navigates this diverse Pareto surface of candidate implementations, gaining insight into the trade offs and selecting solutions for different platforms and environments, thereby stretching beyond the reach of current compiler technologies. Rather than having to focus on the details required to manage complex, inter-related and conflicting, non-functional trade offs, the designer is thus freed to explore, to understand, to control and to decide rather than to construct.},
booktitle = {Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1–14},
numpages = {14},
keywords = {Compilation, Genetic Programming, Non-functional Properties, Pareto Surface, SBSE, Search Based Optimization},
location = {Essen, Germany},
series = {ASE '12}
}

@inproceedings{10.1145/3550355.3552411,
author = {Hentze, Marc and Sundermann, Chico and Th\"{u}m, Thomas and Schaefer, Ina},
title = {Quantifying the variability mismatch between problem and solution space},
year = {2022},
isbn = {9781450394666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550355.3552411},
doi = {10.1145/3550355.3552411},
abstract = {A software product line allows to derive individual software products based on a configuration. As the number of configurations is an indicator for the general complexity of a software product line, automatic #SAT analyses have been proposed to provide this information. However, the number of configurations does not need to match the number of derivable products. Due to this mismatch, using the number of configurations to reason about the software complexity (i.e., the number of derivable products) of a software product line can lead to wrong assumptions during implementation and testing. How to compute the actual number of derivable products, however, is unknown. In this paper, we mitigate this problem and present a concept to derive a solution-space feature model which allows to reuse existing #SAT analyses for computing the number of derivable products of a software product line. We apply our concept to a total of 119 subsystems of three industrial software product lines. The results show that the derivation scales for real world software product lines and confirm the mismatch between the number of configurations and the number of products.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems},
pages = {322–333},
numpages = {12},
keywords = {product lines, solution-space analyses, variability mismatch},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3640310.3674090,
author = {Restrepo, Camilo Correa and Robin, Jacques and Mazo, Raul},
title = {Extensions and Scalability Experiments of a Generic Model-Driven Architecture for Variability Model Reasoning},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674090},
doi = {10.1145/3640310.3674090},
abstract = {Until recently, the state-of-the-art of Software Product Line (SPL) configuration and verification automation consisted of a collection of ad-hoc approaches tightly coupling a single input Variability Modeling Language (VML) with a single constraint solver. To remedy this situation, a novel generic model-driven architecture was then proposed that enables using a variety of VMLs and solvers. The key ideas of this proposal were (a) the use of a standard logical language (CLIF) as a pivot between VMLs and solvers, and (b) the use of a standard data exchange format (JSON) to explicilty and declaratively specify the abstract syntax and semantics of the VMLs to be used in an SPL engineering project and the automated reasoning task to be performed by the solvers.In this article, we overcome the limitations of this initial proposal in three key ways: (1) we add the ability to reason on textual or hybrid VMLs, rather than only on diagrammatic VMLs, enhancing the versatility of the architecture on the input side; (2) we enable the use of solvers from a third paradigm, enhancing the versatility of the architecture on the output side; and, (3) we present the results of scalability performance experiments of an implementation of this architecture. These results have been achieved without significantly altering the architecture, demonstrating its agnosticism with respect to specific VMLs and solvers. It also shows that it can underlie the implementation of practical variability reasoning tools that scale up to real sized variability model analysis and configuration needs.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {126–137},
numpages = {12},
keywords = {Automated Reasoning, Configuration Automation, Generic Architecture, Software Product Lines},
location = {Linz, Austria},
series = {MODELS '24}
}

@inproceedings{10.1145/3589335.3651918,
author = {Huang, Feihu and Yi, Peiyu and Li, Shan and Xu, Haiwen},
title = {Data Quality-based Gradient Optimization for Recurrent Neural Networks},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3651918},
doi = {10.1145/3589335.3651918},
abstract = {Time series forecasting holds significant value in various application scenarios. However, existing forecasting methods primarily focus on optimizing model architecture while neglecting the substantial impact of data quality on model learning. In this study, we aim to enhance model performance by optimizing data utilization based on data quality and propose a Data Quality-based Gradient Optimization (DQGO) method to facilitate training of recurrent neural networks. Firstly, we define sample quality as the matching degree between samples and model, and suggest using the attention entropy to calculate the sample quality through an attention mechanism. Secondly, we optimize the model's gradient vector by giving different weights to samples with different quality. Through experiments conducted on six datasets, the results demonstrate that DQGO significantly improves LSTM's performance. In certain cases, it even surpasses the state-of-the-art models.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {1496–1501},
numpages = {6},
keywords = {data quality, gradient optimization, recurrent neural network, time series},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/2783258.2783270,
author = {Yan, Feng and Ruwase, Olatunji and He, Yuxiong and Chilimbi, Trishul},
title = {Performance Modeling and Scalability Optimization of Distributed Deep Learning Systems},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2783270},
doi = {10.1145/2783258.2783270},
abstract = {Big deep neural network (DNN) models trained on large amounts of data have recently achieved the best accuracy on hard tasks, such as image and speech recognition. Training these DNNs using a cluster of commodity machines is a promising approach since training is time consuming and compute-intensive. To enable training of extremely large DNNs, models are partitioned across machines. To expedite training on very large data sets, multiple model replicas are trained in parallel on different subsets of the training examples with a global parameter server maintaining shared weights across these replicas. The correct choice for model and data partitioning and overall system provisioning is highly dependent on the DNN and distributed system hardware characteristics. These decisions currently require significant domain expertise and time consuming empirical state space exploration.This paper develops performance models that quantify the impact of these partitioning and provisioning decisions on overall distributed system performance and scalability. Also, we use these performance models to build a scalability optimizer that efficiently determines the optimal system configuration that minimizes DNN training time. We evaluate our performance models and scalability optimizer using a state-of-the-art distributed DNN training framework on two benchmark applications. The results show our performance models estimate DNN training time with high estimation accuracy and our scalability optimizer correctly chooses the best configurations, minimizing the training time of distributed DNNs.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1355–1364},
numpages = {10},
keywords = {deep learning, distributed system, optimization, performance modeling, scalability},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}

@inproceedings{10.1145/2701319.2701325,
author = {Devroey, Xavier and Perrouin, Gilles and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Covering SPL Behaviour with Sampled Configurations: An Initial Assessment},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701325},
doi = {10.1145/2701319.2701325},
abstract = {Structural approaches to Software Product Lines (SPL) testing (such as pairwise testing) have gained momentum as they are able to scale to larger SPLs described as feature diagrams (FD). However, these methods are agnostic with respect to behaviour: the sampled configurations have thus no reason to satisfy any given behavioural criterion. In this paper, we investigate the behavioural coverage of two structural testing criteria: pairwise and similarity. To do so, we modelled four SPLs in terms of feature diagrams and associated featured transitions systems (FTSs). We then computed state, action and transition coverage for a set of generated configurations. Preliminary results indicate that for relatively small variability models with few cross-tree constraints, structural coverage-driven tools tend to cover large parts of behaviour with less than 8 configurations. Though structural coverage cannot be used directly as a replacement for behavioural driven SPL test generation, opportunities to mix structural and behavioural coverage for efficient and effective SPL testing do exist.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {59–66},
numpages = {8},
keywords = {Featured Transition System, SPL Testing, Structural Coverage},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@inproceedings{10.1145/3622748.3622750,
author = {Arasaki, Caio and Wolschick, Lucas and Freire, Willian and Amaral, Aline},
title = {Feature selection in an interactive search-based PLA design approach},
year = {2023},
isbn = {9798400709524},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622748.3622750},
doi = {10.1145/3622748.3622750},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line (SPL). PLA design can be formulated as an interactive optimization problem with many conflicting factors. Incorporating Decision Makers’ (DM) preferences during the search process may help the algorithms find more adequate solutions for their profiles. Interactive approaches allow the DM to evaluate solutions, guiding the optimization according to their preferences. However, this brings up human fatigue problems caused by the excessive amount of interactions and solutions to evaluate. A common strategy to prevent this problem is limiting the number of interactions and solutions evaluated by the DM. Machine Learning (ML) models were also used to learn how to evaluate solutions according to the DM profile and replace them after some interactions. Feature selection performs an essential task as non-relevant and/or redundant features used to train the ML model can reduce the accuracy and comprehensibility of the hypotheses induced by ML algorithms. This work aims to select features of an ML model used to prevent human fatigue in an interactive search-based PLA design approach. We applied four selectors and through results we were able to reduce 30% of features, obtaining an accuracy of 99%.},
booktitle = {Proceedings of the 17th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Feature Selection, Interactive search-based Software Engineering, Machine Learning},
location = {Campo Grande, Brazil},
series = {SBCARS '23}
}

@inproceedings{10.1145/2695664.2695909,
author = {Burity, Tha\'{\i}s and Elias, Gledson},
title = {A quantitative, evidence-based approach for recommending software modules},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695909},
doi = {10.1145/2695664.2695909},
abstract = {In distributed software product line projects, dependencies between components influence on communication and coordination needs among their respective development teams. As an alternative to reduce such needs, it seems interesting to cluster tightly-coupled components into loosely-coupled modules as long as each module is developed by a single team. In such a context, considering that numerous clustering possibilities exist, this paper presents a quantitative, evidence-based approach for recommending software modules by clustering software components. The proposed approach has a threefold foundation: quantitative measures that characterize dependencies between components; a search-based clustering algorithm to recommend modules; and a quantitative measure that characterize dependencies between modules, which can guide the allocation of development teams to modules.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1449–1456},
numpages = {8},
keywords = {architectural metrics, design structure matrix, global software development, search based software engineering, simulated annealing, software product line},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/302405.302409,
author = {DeBaud, Jean-Marc and Schmid, Klaus},
title = {A systematic approach to derive the scope of software product lines},
year = {1999},
isbn = {1581130740},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/302405.302409},
doi = {10.1145/302405.302409},
booktitle = {Proceedings of the 21st International Conference on Software Engineering},
pages = {34–43},
numpages = {10},
keywords = {domain engineering, product line scoping, reuse economic models, software product line},
location = {Los Angeles, California, USA},
series = {ICSE '99}
}

@inproceedings{10.1145/3023956.3023962,
author = {Nieke, Michael and Engel, Gil and Seidl, Christoph},
title = {DarwinSPL: an integrated tool suite for modeling evolving context-aware software product lines},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023962},
doi = {10.1145/3023956.3023962},
abstract = {Software Product Lines (SPLs) are an approach for large-scale reuse for software families by means of variabilities and commonalities. We consider three dimensions of variability representing sources of software systems to behave differently: configuration as spatial variability, dependence on surroundings as contextual variability and evolution as temporal variability. The three dimensions of variability strongly correlate: Contextual variability changes the set of possible configurations in spatial variability. Temporal variability captures changes of spatial and contextual variability over the course of time. However, currently, there is no tool support for integrated modeling of these three dimensions of variability. In this paper, we present DarwinSPL, a tool suite supporting integrated definition of spatial, contextual and temporal variability. With DarwinSPL, spatial variability is modeled as feature models with constraints. Additionally, we are able to capture the current context and its impact on functionality of the SPL. Moreover, by providing support for temporal variability, DarwinSPL supports performing arbitrary evolutionary changes to spatial and contextual variability and tracking of previous evolution and planning future evolution of SPLs. We show the feasibility of DarwinSPL by performing a case study adapted from our industrial partner in the automotive domain.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {92–99},
numpages = {8},
keywords = {DarwinSPL, context-aware, evolution, software product line, temporal feature model, tool suite},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@article{10.1145/2580950,
author = {Th\"{u}m, Thomas and Apel, Sven and K\"{a}stner, Christian and Schaefer, Ina and Saake, Gunter},
title = {A Classification and Survey of Analysis Strategies for Software Product Lines},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2580950},
doi = {10.1145/2580950},
abstract = {Software-product-line engineering has gained considerable momentum in recent years, both in industry and in academia. A software product line is a family of software products that share a common set of features. Software product lines challenge traditional analysis techniques, such as type checking, model checking, and theorem proving, in their quest of ensuring correctness and reliability of software. Simply creating and analyzing all products of a product line is usually not feasible, due to the potentially exponential number of valid feature combinations. Recently, researchers began to develop analysis techniques that take the distinguishing properties of software product lines into account, for example, by checking feature-related code in isolation or by exploiting variability information during analysis. The emerging field of product-line analyses is both broad and diverse, so it is difficult for researchers and practitioners to understand their similarities and differences. We propose a classification of product-line analyses to enable systematic research and application. Based on our insights with classifying and comparing a corpus of 123 research articles, we develop a research agenda to guide future research on product-line analyses.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {6},
numpages = {45},
keywords = {Product-line analysis, model checking, program family, software analysis, software product line, static analysis, theorem proving, type checking}
}

@inproceedings{10.5555/2820518.2820523,
author = {Hashimoto, Masatomo and Terai, Masaaki and Maeda, Toshiyuki and Minami, Kazuo},
title = {Extracting facts from performance tuning history of scientific applications for predicting effective optimization patterns},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {To improve performance of large-scale scientific applications, scientists or tuning experts make various empirical attempts to change compiler options, program parameters or even the syntactic structure of programs. Those attempts followed by performance evaluation are repeated until satisfactory results are obtained. The task of performance tuning requires a great deal of time and effort. On account of combinatorial explosion of possible attempts, scientists/tuning experts have a tendency to make decisions on what to be explored just based on their intuition or good sense of tuning. We advocate evidence-based performance tuning (EBT) that facilitates the use of database of facts extracted from tuning histories of applications to guide the exploration of the search space. However, in general, performance tuning is conducted as transient tasks without version control systems. Tuning histories may lack explicit facts about what kind of program transformation contributed to the better performance or even about the chronological order of the source code snapshots. For reconstructing the missing information, we employ a state-of-the-art fine-grained change pattern identification tool for inferring applied transformation patterns only from an unordered set of source code snapshots. The extracted facts are intended to be stored and queried for further data mining. This paper reports on experiments of tuning pattern identification followed by predictive model construction conducted for a few scientific applications tuned for the K supercomputer.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {13–23},
numpages = {11},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1145/3691620.3695321,
author = {Gropengie\ss{}er, Uwe and Liphardt, Julian and Matth\'{e}, Michael and M\"{u}hlh\"{a}user, Max},
title = {Feature Model Slicing for Real-time Selection of Mission-critical Edge Application},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695321},
doi = {10.1145/3691620.3695321},
abstract = {At first glance, running mission-critical applications at the edge appears to be an opportunity to benefit from scalability and reusability. The low latency to the edge makes it particularly interesting for mission-critical applications. The hardware heterogeneity of the edge, coupled with the strict requirement for the execution time of a mission-critical application, creates the need for flexible application control and, at the same time, increases the complexity of modeling such systems. With its Feature Models (FMs), software product line engineering offers a modeling option for various alternative compositions of an application. However, the calculation of valid configurations takes too long for the dynamic adaptation of an application flow of a mission-critical application. This paper presents an approach for slicing FMs to support mission-critical applications. Our approach supports the strict requirements on the execution time of mission-critical applications.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {2446–2447},
numpages = {2},
keywords = {software product lines, feature model, approximate computing, edge computing},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3580305.3599316,
author = {Zhang, Chenkang and Shi, Wanli and Luo, Lei and Gu, Bin},
title = {Doubly Robust AUC Optimization against Noisy and Adversarial Samples},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599316},
doi = {10.1145/3580305.3599316},
abstract = {Area under the ROC curve (AUC) is an important and widely used metric in machine learning especially for imbalanced datasets. In current practical learning problems, not only adversarial samples but also noisy samples seriously threaten the performance of learning models. Nowadays, there have been a lot of research works proposed to defend the adversarial samples and noisy samples separately. Unfortunately, to the best of our knowledge, none of them with AUC optimization can secure against the two kinds of harmful samples simultaneously. To fill this gap and also address the challenge, in this paper, we propose a novel doubly robust dAUC optimization (DRAUC) algorithm. Specifically, we first exploit the deep integration of self-paced learning and adversarial training under the framework of AUC optimization, and provide a statistical upper bound to the AUC adversarial risk. Inspired by the statistical upper bound, we propose our optimization objective followed by an efficient alternatively stochastic descent algorithm, which can effectively improve the performance of learning models by guarding against adversarial samples and noisy samples. Experimental results on several standard datasets demonstrate that our DRAUC algorithm has better noise robustness and adversarial robustness than the state-of-the-art algorithms.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3195–3205},
numpages = {11},
keywords = {adversarial training, auc optimization, self-paced learning},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/2556624.2556628,
author = {Lengauer, Philipp and Bitto, Verena and Angerer, Florian and Gr\"{u}nbacher, Paul and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {Where has all my memory gone? determining memory characteristics of product variants using virtual-machine-level monitoring},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556628},
doi = {10.1145/2556624.2556628},
abstract = {Non-functional properties such as memory footprint have recently gained importance in software product line research. However, determining the memory characteristics of individual features and product variants is extremely challenging. We present an approach that supports the monitoring of memory characteristics of individual features at the level of Java virtual machines. Our approach provides extensions to Java virtual machines to track memory allocations and deal-locations of individual features based on a feature-to-code mapping. The approach enables continuous monitoring at the level of features to detect anomalies such as memory leaks, excessive memory consumption, or abnormal garbage collection times in product variants. We provide an evaluation of our approach based on different product variants of the DesktopSearcher product line. Our experiment with different program inputs demonstrates the feasibility of our technique.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {13},
numpages = {8},
keywords = {Java, feature-oriented software development, memory footprint, monitoring},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@inproceedings{10.1145/3452296.3472913,
author = {Namyar, Pooria and Supittayapornpong, Sucha and Zhang, Mingyang and Yu, Minlan and Govindan, Ramesh},
title = {A throughput-centric view of the performance of datacenter topologies},
year = {2021},
isbn = {9781450383837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452296.3472913},
doi = {10.1145/3452296.3472913},
abstract = {While prior work has explored many proposed datacenter designs, only two designs, Clos-based and expander-based, are generally considered practical because they can scale using commodity switching chips. Prior work has used two different metrics, bisection bandwidth and throughput, for evaluating these topologies at scale. Little is known, theoretically or practically, how these metrics relate to each other. Exploiting characteristics of these topologies, we prove an upper bound on their throughput, then show that this upper bound better estimates worst-case throughput than all previously proposed throughput estimators and scales better than most of them. Using this upper bound, we show that for expander-based topologies, unlike Clos, beyond a certain size of the network, no topology can have full throughput, even if it has full bisection bandwidth; in fact, even relatively small expander-based topologies fail to achieve full throughput. We conclude by showing that using throughput to evaluate datacenter performance instead of bisection bandwidth can alter conclusions in prior work about datacenter cost, manageability, and reliability.},
booktitle = {Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
pages = {349–369},
numpages = {21},
keywords = {Clos topologies, data centers, network management, throughput},
location = {Virtual Event, USA},
series = {SIGCOMM '21}
}

@inproceedings{10.1145/3489849.3489948,
author = {Lebiedz, Jacek and Wiszniewski, Bogdan},
title = {CAVE applications: from craft manufacturing to product line engineering},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489948},
doi = {10.1145/3489849.3489948},
abstract = {Product line engineering model is suitable for engineering related software products in an efficient manner, taking advantage of their similarities while managing their differences. Our feature driven software product line (SPL) solution based on that model allows for instantiation of different CAVE products based on the set of core assets and driven by a set of common VR features with the minimal budget and time to market.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {57},
numpages = {2},
keywords = {VR application features, core assets, production stations},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3417113.3423000,
author = {de Macedo, Jo\~{a}o and Alo\'{\i}sio, Jo\~{a}o and Gon\c{c}alves, Nelson and Pereira, Rui and Saraiva, Jo\~{a}o},
title = {Energy wars - Chrome vs. Firefox: which browser is more energy efficient?},
year = {2021},
isbn = {9781450381284},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417113.3423000},
doi = {10.1145/3417113.3423000},
abstract = {This paper presents a preliminary study on the energy consumption of two popular web browsers. In order to properly measure the energy consumption of both environments, we simulate the usage of various applications, which the goal to mimic typical user interactions and usage.Our preliminary results show interesting findings based on observation, such as what type of interactions generate high peaks of energy consumption, and which browser is overall the most efficient. Our goal with this preliminary study is to show to users how very different the efficiency of web browsers can be, and may serve with advances in this area of study.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {159–165},
numpages = {7},
keywords = {energy efficiency, green software, web browsers},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/2668930.2688051,
author = {Hork\'{y}, Vojt\v{e}ch and Libi\v{c}, Peter and Marek, Luk\'{a}\v{s} and Steinhauser, Antonin and T\r{u}ma, Petr},
title = {Utilizing Performance Unit Tests To Increase Performance Awareness},
year = {2015},
isbn = {9781450332484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668930.2688051},
doi = {10.1145/2668930.2688051},
abstract = {Many decisions taken during software development impact the resulting application performance. The key decisions whose potential impact is large are usually carefully weighed. In contrast, the same care is not used for many decisions whose individual impact is likely to be small -- simply because the costs would outweigh the benefits. Developer opinion is the common deciding factor for these cases, and our goal is to provide the developer with information that would help form such opinion, thus preventing performance loss due to the accumulated effect of many poor decisions.Our method turns performance unit tests into recipes for generating performance documentation. When the developer selects an interface and workload of interest, relevant performance documentation is generated interactively. This increases performance awareness -- with performance information available alongside standard interface documentation, developers should find it easier to take informed decisions even in situations where expensive performance evaluation is not practical. We demonstrate the method on multiple examples, which show how equipping code with performance unit tests works.},
booktitle = {Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
pages = {289–300},
numpages = {12},
keywords = {java, javadoc, performance awareness, performance documentation, performance testing},
location = {Austin, Texas, USA},
series = {ICPE '15}
}

@inproceedings{10.1145/2465478.2465495,
author = {Klatt, Benjamin and K\"{u}ster, Martin},
title = {Improving product copy consolidation by architecture-aware difference analysis},
year = {2013},
isbn = {9781450321266},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2465478.2465495},
doi = {10.1145/2465478.2465495},
abstract = {Software product lines (SPL) are a well-known concept to efficiently develop product variants. However, migrating customised product copies to a product line is still a labour-intensive challenge due to the required comprehension of differences among the implementations and SPL design decisions. Most existing SPL approaches are focused on forward engineering. Only few aim to handle SPL evolution, but even those lack support of variability reverse engineering, which is necessary for migrating product copies to a product line. In this paper, we present our continued concept on using component architecture information to enhance a variability reverse engineering process. Including this information particularly improves the difference identification as well as the variation point analysis and -aggregation steps. We show how the concept can be applied by providing an illustrating example.},
booktitle = {Proceedings of the 9th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {117–122},
numpages = {6},
keywords = {component architecture, reverse engineering, software product line},
location = {Vancouver, British Columbia, Canada},
series = {QoSA '13}
}

@inproceedings{10.1145/3023956.3023959,
author = {Ochoa, Lina and Pereira, Juliana Alves and Gonz\'{a}lez-Rojas, Oscar and Castro, Harold and Saake, Gunter},
title = {A survey on scalability and performance concerns in extended product lines configuration},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023959},
doi = {10.1145/3023956.3023959},
abstract = {Product lines have been employed as a mass customisation method that reduces production costs and time-to-market. Multiple product variants are represented in a product line, however the selection of a particular configuration depends on stakeholders' functional and non-functional requirements. Methods like constraint programming and evolutionary algorithms have been used to support the configuration process. They consider a set of product requirements like resource constraints, stakeholders' preferences, and optimization objectives. Nevertheless, scalability and performance concerns start to be an issue when facing large-scale product lines and runtime environments. Thus, this paper presents a survey that analyses strengths and drawbacks of 21 approaches that support product line configuration. This survey aims to: i) evidence which product requirements are currently supported by studied methods; ii) how scalability and performance is considered in existing approaches; and iii) point out some challenges to be addressed in future research.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {5–12},
numpages = {8},
keywords = {configuration, literature review, performance, product line, product requirements, scalability, survey},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/1985484.1985490,
author = {Stallinger, Fritz and Neumann, Robert and Schossleitner, Robert and Kriener, Stephan},
title = {Migrating towards evolving software product lines: challenges of an SME in a core customer-driven industrial systems engineering context},
year = {2011},
isbn = {9781450305846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985484.1985490},
doi = {10.1145/1985484.1985490},
abstract = {In this paper we identify key challenges a medium-sized software organization is facing in migrating towards Software Product Line Engineering (SPLE). The software engineering context of the company is characterized by a two-fold access to the market - core customer driven product enhancement and product development for a broader, anonymous market - and the embedding of software engineering in multi-disciplinary systems and solutions engineering.Based on a characterization of the business, the software product subject to migration towards SPLE, and the goals and background of the SPLE initiative, seven key challenges with respect to the migration are identified. These challenges relate to process diversity in the face of multiple reuse approaches; the management of requirements and variability; the integration of requirements traceability and variability management; legacy software and discipline vs. software-specific modularization; integration with systems engineering; costing and pricing models; and project vs. product documentation.},
booktitle = {Proceedings of the 2nd International Workshop on Product Line Approaches in Software Engineering},
pages = {20–24},
numpages = {5},
keywords = {sme, software engineering, software product line, software product migration, systems engineering},
location = {Waikiki, Honolulu, HI, USA},
series = {PLEASE '11}
}

@inproceedings{10.5555/3643142.3643343,
author = {Lechner, Mihaela and Roman, Alexander and Uhlig, Tobias and Rose, Oliver and Mayer, Thomas},
title = {Simulation-Based Optimization of Air Force Mission Planning},
year = {2024},
isbn = {9798350369663},
publisher = {IEEE Press},
abstract = {Military planning operations deal with highly dynamic environments and a variety of complex optimization challenges. In order to support decision-makers in this process, innovative concepts are required that can automatically generate applicable solutions for certain aspects of mission planning. Such instruments can simplify the planning process, reduce risks, and lower operating costs. This paper presents a simulation-based optimization framework that addresses three problems in the context of aerial warfare planning: task assignment, scheduling, and route planning. These problems are tackled with interconnected heuristics based on either greedy approaches or genetic algorithms. Additionally, hierarchical task networks are employed to incorporate domain knowledge in form of tactical doctrines into the solution. Our simulation results confirm the viability of the proposed approach for small to medium-sized scenarios. However, further investigation with regard to the evaluation function and the simulation environment is required.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2427–2438},
numpages = {12},
location = {San Antonio, Texas, USA},
series = {WSC '23}
}

@inproceedings{10.1145/2556624.2556637,
author = {Machado, Ivan do Carmo and Santos, Alcemir Rodrigues and Cavalcanti, Yguarat\~{a} Cerqueira and Trzan, Eduardo Gomes and de Souza, Marcio Magalh\~{a}es and de Almeida, Eduardo Santana},
title = {Low-level variability support for web-based software product lines},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556637},
doi = {10.1145/2556624.2556637},
abstract = {The Web systems domain has faced an increasing number of devices, browsers, and platforms to cope with, driving software systems to be more flexible to accomodate them. Software product line (SPL) engineering can be used as a strategy to implement systems capable of handling such a diversity. To this end, automated tool support is almost indispensable. However, current tool support gives more emphasis to modeling variability in the problem domain, over the support of variability at the solution domain. There is a need for mapping the variability between both abstraction levels, so as to determine what implementation impact a certain variability has. In this paper, we propose the FeatureJS, a FeatureIDE extension aiming at Javascript and HTML support for SPL engineering. The tool combines feature-oriented programming and preprocessors, as a strategy to map variability at source code with the variability modeled at a higher level of abstraction. We carried out a preliminary evaluation with an industrial project, aiming to characterize the capability of the tool to handle SPL engineering in the Web systems domain.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {15},
numpages = {8},
keywords = {Eclipse plugin, FeatureIDE, feature composition, feature oriented software development, software product line engineering, web systems domain},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@inproceedings{10.1145/3634713.3634725,
author = {G\"{u}thing, Lukas and Bittner, Paul Maximilian and Schaefer, Ina and Th\"{u}m, Thomas},
title = {Explaining Edits to Variability Annotations in Evolving Software Product Lines},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634725},
doi = {10.1145/3634713.3634725},
abstract = {Software is subject to changes and revisions during its development life cycle. For configurable software systems, changes may be made to functionality of source code as well as variability information such as code-to-feature mappings. To explain how code-to-feature mappings change in edits made to configurable software, we relate the mappings before and after an edit in terms of the sets of variants they denote. We prove our explanations to be complete and unambiguous, meaning that every pair of code-to-feature mappings is explained in terms of exactly one relation. Based on a graph formalism, we provide an algorithm for fast detection of relations during commits to version control. In an initial study, we detect relations between feature annotations in 42 real-world software product-line repositories to better understand typical changes in the evolution of configurable software. We demonstrate that our formalism can be automated and that analyzing a commit requires only 135 ms on average.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {93–102},
numpages = {10},
keywords = {software evolution, software product lines, software variability},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.1145/3652628.3652665,
author = {Wan, Yaping and Peng, Zhihui and Chen, Huajuan and Yang, Wangda},
title = {MRAbF: MapReduce Resource Allocation Optimization Algorithm Based on Fair Policy},
year = {2024},
isbn = {9798400708831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652628.3652665},
doi = {10.1145/3652628.3652665},
abstract = {In the era of big data, data storage and computation have become a mainstream issue. Hadooop, as a distributed computing framework capable of handling large-scale datasets, the computing performance of its computing component MapReduce greatly affects the efficiency of big data processing. However, the current MapReduce computing component has the problem of uneven resource allocation during the Map computing phase. By analyzing the calculation process of MapReduce in the article, it is concluded that two main resource allocation parameters, mapreduce. task. io. sport. mb and mapreduce. map. sport. spool. percentage, affect the computational performance of the Map phase. Thus, a MapReduce resource allocation optimization algorithm MRAbF based on fairness strategy was proposed. By comparing the WordCount calculation experiment with the native MapReduce, the optimized MapReduce calculation performance can be improved by 4.8% to 17.2%.},
booktitle = {Proceedings of the 4th International Conference on Artificial Intelligence and Computer Engineering},
pages = {216–222},
numpages = {7},
location = {Dalian, China},
series = {ICAICE '23}
}

@inproceedings{10.1145/2647508.2647512,
author = {Koscielny, Jonathan and Holthusen, S\"{o}nke and Schaefer, Ina and Schulze, Sandro and Bettini, Lorenzo and Damiani, Ferruccio},
title = {DeltaJ 1.5: delta-oriented programming for Java 1.5},
year = {2014},
isbn = {9781450329262},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647508.2647512},
doi = {10.1145/2647508.2647512},
abstract = {Delta-oriented programming (DOP) is a modular, yet flexible approach to implement software product lines. In DOP, a product line is implemented by a set of deltas, which are containers of modifications to a program. A delta-oriented product line is specified by its code base, i.e., the set of delta modules, and a product line declaration specifying the set of possible product variants. In this paper, we present DOP for Java 1.5 extending previous proof-of-concept realizations of DOP for simple core Java-like languages. The novel prototypical implementation DeltaJ 1.5 provides full integrated access to the object-oriented features of Java. The extensions include delta operations to fully integrate the Java package system, to declare and modify interfaces, to explicitly change the inheritance hierarchy, to access nested types and enum types, to alter field declarations, and to unambiguously remove overloaded methods. Furthermore, we improve the specification of the product line declaration by providing a separate language. We have evaluated DeltaJ 1.5 using a case study.},
booktitle = {Proceedings of the 2014 International Conference on Principles and Practices of Programming on the Java Platform: Virtual Machines, Languages, and Tools},
pages = {63–74},
numpages = {12},
keywords = {delta-oriented programming, program generation, software product line},
location = {Cracow, Poland},
series = {PPPJ '14}
}

@inproceedings{10.1145/3652620.3688216,
author = {Fadhlillah, Hafiyyan Sayyid and Greiner, Sandra and Feichtinger, Kevin and Rabiser, Rick and Zoitl, Alois},
title = {Managing Variability of Cyber-Physical Production Systems: Towards Consistency Management},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688216},
doi = {10.1145/3652620.3688216},
abstract = {Engineering Cyber-Physical Production Systems (CPPSs) involves several different disciplines, where team members range from mechanical, electrical, and automation engineers, to control software engineers. When developing variability-intensive software systems such as CPPSs, engineers create heterogeneous engineering artifacts of varying granularity, structure, and level of abstraction in the problem and solution space, e.g., CAD drawings, delta models, and control software artifacts. Managing consistency among these heterogeneous artifacts is essential during the development and maintenance of these systems to reduce development costs and runtime failures. Software product line engineering provides approaches to manage the variability of heterogeneous artifacts. However, these approaches must be adapted and extended to manage consistency in CPPSs and address the additional multidimensional challenges in CPPSs. In this short paper, we outline these challenges, motivate them using a case study, and discuss potential solutions to manage the consistency of engineering artifacts expressing CPPS control software variability. We thereby lay the grounds for a deeper understanding of possible inconsistencies and exploring new methods for managing consistency in control software variability in CPPSs.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {945–949},
numpages = {5},
keywords = {cyber-physical production systems engineering, heterogeneous multi-modeling, software modeling consistency},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/2851553.2851569,
author = {Hork\'{y}, Vojt\v{e}ch and Kotr\v{c}, Jaroslav and Libi\v{c}, Peter and T\r{u}ma, Petr},
title = {Analysis of Overhead in Dynamic Java Performance Monitoring},
year = {2016},
isbn = {9781450340809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851553.2851569},
doi = {10.1145/2851553.2851569},
abstract = {In production environments, runtime performance monitoring is often limited to logging of high level events. More detailed measurements, such as method level tracing, tend to be avoided because their overhead can disrupt execution. This limits the information available to developers when solving performance issues at code level. One approach that reduces the measurement disruptions is dynamic performance monitoring, where the measurement instrumentation is inserted and removed as needed. Such selective monitoring naturally reduces the aggregate overhead, but also introduces transient overhead artefacts related to insertion and removal of instrumentation. We experimentally analyze this overhead in Java, focusing in particular on the measurement accuracy, the character of the transient overhead, and the longevity of the overhead artefacts.Among other results, we show that dynamic monitoring requires time from seconds to minutes to deliver stable measurements, that the instrumentation can both slow down and speed up the execution, and that the overhead artefacts can persist beyond the monitoring period.},
booktitle = {Proceedings of the 7th ACM/SPEC on International Conference on Performance Engineering},
pages = {275–286},
numpages = {12},
keywords = {dynamic instrumentation, java, performance measurement overhead},
location = {Delft, The Netherlands},
series = {ICPE '16}
}

@inproceedings{10.1145/2577080.2577095,
author = {Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha and Baier, Christel},
title = {Probabilistic model checking for energy analysis in software product lines},
year = {2014},
isbn = {9781450327725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2577080.2577095},
doi = {10.1145/2577080.2577095},
abstract = {In a software product line (SPL), a collection of software products is defined by their commonalities in terms of features rather than explicitly specifying all products one-by-one. Several verification techniques were adapted to establish temporal properties of SPLs. Symbolic and family-based model checking have been proven to be successful for tackling the combinatorial blow-up arising when reasoning about several feature combinations. However, most formal verification approaches for SPLs presented in the literature focus on the static SPLs, where the features of a product are fixed and cannot be changed during runtime. This is in contrast to dynamic SPLs, allowing to adapt feature combinations of a product dynamically after deployment.The main contribution of the paper is a compositional modeling framework for dynamic SPLs, which supports probabilistic and nondeterministic choices and allows for quantitative analysis. We specify the feature changes during runtime within an automata-based coordination component, enabling to reason over strategies how to trigger dynamic feature changes for optimizing various quantitative objectives, e.g., energy or monetary costs and reliability. For our framework there is a natural and conceptually simple translation into the input language of the prominent probabilistic model checker PRISM. This facilitates the application of PRISM's powerful symbolic engine to the operational behavior of dynamic SPLs and their family-based analysis against various quantitative queries. We demonstrate feasibility of our approach by a case study issuing an energy-aware bonding network device.},
booktitle = {Proceedings of the 13th International Conference on Modularity},
pages = {169–180},
numpages = {12},
keywords = {dynamic features, energy analysis, probabilistic model checking, software product lines},
location = {Lugano, Switzerland},
series = {MODULARITY '14}
}

@inproceedings{10.1145/2814251.2814263,
author = {Ochoa, Lina and Gonz\'{a}lez-Rojas, Oscar and Th\"{u}m, Thomas},
title = {Using decision rules for solving conflicts in extended feature models},
year = {2015},
isbn = {9781450336864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814251.2814263},
doi = {10.1145/2814251.2814263},
abstract = {Software Product Line Engineering has introduced feature modeling as a domain analysis technique used to represent the variability of software products and decision-making scenarios. We present a model-based transformation approach to solve conflicts among configurations performed by different stakeholders on feature models. We propose the usage of a domain-specific language named CoCo to specify attributes as non-functional properties of features, and to describe business-related decision rules in terms of costs, time, and human resources. These specifications along with the stakeholders' configurations and the feature model are transformed into a constraint programming problem, on which decision rules are executed to find a non-conflicting set of solution configurations that are aligned to business objectives. We evaluate CoCo's compositionality and model complexity simplification while using a set of motivating decision scenarios.},
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Software Language Engineering},
pages = {149–160},
numpages = {12},
keywords = {Domain engineering, conflicting configurations, constraint satisfaction problem, domain-specific language, extended feature model, model transformation chain},
location = {Pittsburgh, PA, USA},
series = {SLE 2015}
}

@inproceedings{10.1109/ASE.2013.6693103,
author = {Pohl, Richard and Stricker, Vanessa and Pohl, Klaus},
title = {Measuring the structural complexity of feature models},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693103},
doi = {10.1109/ASE.2013.6693103},
abstract = {The automated analysis of feature models (FM) is based on SAT, BDD, and CSP - known NP-complete problems. Therefore, the analysis could have an exponential worst-case execution time. However, for many practical relevant analysis cases, state-of-the-art (SOTA) analysis tools quite successfully master the problem of exponential worst-case execution time based on heuristics. So far, however, very little is known about the structure of FMs that cause the cases in which the execution time (hardness) for analyzing a given FM increases unpredictably for SOTA analysis tools. In this paper, we propose to use width measures from graph theory to characterize the structural complexity of FMs as a basis for an estimation of the hardness of analysis operations on FMs with SOTA analysis tools. We present an experiment that we use to analyze the reasonability of graph width measures as metric for the structural complexity of FMs and the hardness of FM analysis. Such a complexity metric can be used as a basis for a unified method to systematically improve SOTA analysis tools.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {454–464},
numpages = {11},
keywords = {automated analysis, feature model, performance measurement, software product line},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@inproceedings{10.1145/2884781.2884823,
author = {Schr\"{o}ter, Reimar and Krieter, Sebastian and Th\"{u}m, Thomas and Benduhn, Fabian and Saake, Gunter},
title = {Feature-model interfaces: the highway to compositional analyses of highly-configurable systems},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884823},
doi = {10.1145/2884781.2884823},
abstract = {Today's software systems are often customizable by means of load-time or compile-time configuration options. These options are typically not independent and their dependencies can be specified by means of feature models. As many industrial systems contain thousands of options, the maintenance and utilization of feature models is a challenge for all stakeholders. In the last two decades, numerous approaches have been presented to support stakeholders in analyzing feature models. Such analyses are commonly reduced to satisfiability problems, which suffer from the growing number of options. While first attempts have been made to decompose feature models into smaller parts, they still require to compose all parts for analysis. We propose the concept of a feature-model interface that only consists of a subset of features, typically selected by experts, and hides all other features and dependencies. Based on a formalization of feature-model interfaces, we prove compositionality properties. We evaluate feature-model interfaces using a three-month history of an industrial feature model from the automotive domain with 18,616 features. Our results indicate performance benefits especially under evolution as often only parts of the feature model need to be analyzed again.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {667–678},
numpages = {12},
keywords = {compositionality, configurable software, feature model, modularity, software product line, variability modeling},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/2188286.2188345,
author = {Bulej, Lubom\'{\i}r and Bure\v{s}, Tom\'{a}\v{s} and Keznikl, Jaroslav and Koubkov\'{a}, Alena and Podzimek, Andrej and T\r{u}ma, Petr},
title = {Capturing performance assumptions using stochastic performance logic},
year = {2012},
isbn = {9781450312028},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2188286.2188345},
doi = {10.1145/2188286.2188345},
abstract = {Compared to functional unit testing, automated performance testing is difficult, partially because correctness criteria are more difficult to express for performance than for functionality. Where existing approaches rely on absolute bounds on the execution time, we aim to express assertions on code performance in relative, hardware-independent terms. To this end, we introduce Stochastic Performance Logic (SPL), which allows making statements about relative method performance. Since SPL interpretation is based on statistical tests applied to performance measurements, it allows (for a special class of formulas) calculating the minimum probability at which a particular SPL formula holds. We prove basic properties of the logic and present an algorithm for SAT-solver-guided evaluation of SPL formulas, which allows optimizing the number of performance measurements that need to be made. Finally, we propose integration of SPL formulas with Java code using higher-level performance annotations, for performance testing and documentation purposes.},
booktitle = {Proceedings of the 3rd ACM/SPEC International Conference on Performance Engineering},
pages = {311–322},
numpages = {12},
keywords = {performance testing, regression benchmarking},
location = {Boston, Massachusetts, USA},
series = {ICPE '12}
}

@inproceedings{10.1145/3023956.3023968,
author = {Mjeda, Anila and Wasala, Asanka and Botterweck, Goetz},
title = {Decision spaces in product lines, decision analysis, and design exploration: an interdisciplinary exploratory study},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023968},
doi = {10.1145/3023956.3023968},
abstract = {Context. From recent works on product properties resulting from configurations and the optimisation of these properties, one comes quickly to more complex challenges such as multi-objective optimisation, conflicting objectives, multiple stakeholders, and conflict resolution. The intuition is that Software Product Line Engineering (SPLE) can draw from other disciplines that deal with decision spaces and complex decision scenarios.Objectives. We aim to (1) explore links to such disciplines, (2) systematise and compare concepts, and (3) identify opportunities, where SPLE approaches can be enriched.Method. We undertake an exploratory study: Starting from common SPLE activities and artefacts, we identify aspects where we expect to find corresponding counterparts in other disciplines. We focus on Multiple Criteria Decision Analysis (MCDA), Multi-Objective Optimisation (MOO), and Design Space Exploration (DSE), and perform a comparison of the key concepts.Results. The resulting comparison relates SPLE activities and artefacts to concepts from MCDA, MOO, and DSE and identifies areas where SPLE approaches can be enriched. We also provide examples of existing work at the intersections of SPLE with the other fields. These findings are aimed to foster the conversation on research opportunities where SPLE can draw techniques from other disciplines dealing with complex decision scenarios.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {68–75},
numpages = {8},
keywords = {decision modelling, design-space exploration, multi-criteria decision analysis, multi-objective optimisation},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@article{10.1007/s00165-017-0432-4,
author = {Chrszon, Philipp and Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha and Baier, Christel},
title = {ProFeat: feature-oriented engineering for family-based probabilistic model checking},
year = {2018},
issue_date = {Jan 2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {1},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-017-0432-4},
doi = {10.1007/s00165-017-0432-4},
abstract = {The concept of features provides an elegant way to specify families of systems. Given a base system, features encapsulate additional functionalities that can be activated or deactivated to enhance or restrict the base system’s behaviors. Features can also facilitate the analysis of families of systems by exploiting commonalities of the family members and performing an all-in-one analysis, where all systems of the family are analyzed at once on a single family model instead of one-by-one. Most prominent, the concept of features has been successfully applied to describe and analyze (software) product lines. We present the tool ProFeat that supports the feature-oriented engineering process for stochastic systems by probabilistic model checking. To describe families of stochastic systems, ProFeat extends models for the prominent probabilistic model checker Prism by feature-oriented concepts, including support for probabilistic product lines with dynamic feature switches, multi-features and feature attributes. ProFeat provides a compact symbolic representation of the analysis results for each family member obtained by Prism to support, e.g., model repair or refinement during feature-oriented development. By means of several case studies we show how ProFeat eases family-based quantitative analysis and compare one-by-one and all-in-one analysis approaches.},
journal = {Form. Asp. Comput.},
month = jan,
pages = {45–75},
numpages = {31},
keywords = {Feature-oriented systems, Probabilistic model checking, Software product line analysis}
}

@article{10.1145/2000799.2000803,
author = {Dehlinger, Josh and Lutz, Robyn R.},
title = {Gaia-PL: A Product Line Engineering Approach for Efficiently Designing Multiagent Systems},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/2000799.2000803},
doi = {10.1145/2000799.2000803},
abstract = {Agent-oriented software engineering (AOSE) has provided powerful and natural, high-level abstractions in which software developers can understand, model and develop complex, distributed systems. Yet, the realization of AOSE partially depends on whether agent-based software systems can achieve reductions in development time and cost similar to other reuse-conscious development methods. Specifically, AOSE does not adequately address requirements specifications as reusable assets. Software product line engineering is a reuse technology that supports the systematic development of a set of similar software systems through understanding, controlling, and managing their common, core characteristics and their differing variation points. In this article, we present an extension to the Gaia AOSE methodology, named Gaia-PL (Gaia-Product Line), for agent-based distributed software systems that enables requirements specifications to be easily reused. We show how our methodology uses a product line perspective to promote reuse in agent-based software systems early in the development life cycle so that software assets can be reused throughout system development and evolution. We also present results from an application to show how Gaia-PL provided reuse that reduced the design and development effort for a large, multiagent system.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {17},
numpages = {27},
keywords = {Agent-oriented software engineering, software product line engineering}
}

@inproceedings{10.1145/1982185.1982522,
author = {Mohabbati, Bardia and Hatala, Marek and Ga\v{s}evi\'{c}, Dragan and Asadi, Mohsen and Bo\v{s}kovi\'{c}, Marko},
title = {Development and configuration of service-oriented systems families},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982522},
doi = {10.1145/1982185.1982522},
abstract = {Software Product Lines (SPLs) are families of software systems which share a common sets of feature and are developed through common set of core assets in order to promotes software reusability, mass customization, reducing cost, time-to-market and improving the quality of the product. SPLs are sets (i.e., families) of software applications developed as a whole for a specific business domain. Particular applications are derived from software families by selecting the desired features through configuration process. Traditionally, SPLs are implemented with systematically developed components, shared by members of the SPLs and reused every time a new application is derived. In this paper, we propose an approach to the development and configuration of Service-Oriented SPLs in which services are used as reusable assets and building blocks of implementation. Our proposed approach also suggests prioritization of family features according to stakeholder's non-functional requirements (NFRs) and preferences. Priorities of NFRs are used to filter the most important features of the family, which is performed by Stratified Analytic Hierarchical Process (S-AHP). The priorities also are used further for the selection of appropriate services implementation for business processes realizing features. We apply Mixed Integer Linear Programming to find the optimal service selection within the constraints boundaries specified by stakeholders.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {1606–1613},
numpages = {8},
keywords = {feature-oriented development, optimization, service selection, service-oriented architecture, software product line},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@inproceedings{10.1145/1868688.1868690,
author = {Siegmund, Norbert and Rosenm\"{u}ller, Marko and Apel, Sven},
title = {Automating energy optimization with features},
year = {2010},
isbn = {9781450302081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868688.1868690},
doi = {10.1145/1868688.1868690},
abstract = {Mobile devices such as cell phones and notebooks rely on battery power supply. For these systems, optimizing the power consumption is important to increase the system's lifetime. However, this is hard to achieve because energy-saving functions often depend on the hardware, and operating systems. The diversity of hardware components and operating systems makes the implementation time consuming and difficult. We propose an approach to automate energy optimization of programs by implementing energy-saving functionality as modular, separate implementation units (e.g., feature modules or aspects). These units are bundled as energy features into an energy-optimization feature library. Based on aspect-oriented and feature-oriented programming, we discuss different techniques to compose the source code of a client program and the implementation units of the energy features.},
booktitle = {Proceedings of the 2nd International Workshop on Feature-Oriented Software Development},
pages = {2–9},
numpages = {8},
keywords = {energy consumption, feature-oriented programming, software product lines},
location = {Eindhoven, The Netherlands},
series = {FOSD '10}
}

@article{10.5555/3586589.3586854,
author = {Necoara, Ion and Singh, Nitesh Kumar},
title = {Stochastic subgradient projection methods for composite optimization with functional constraints},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {In this paper we consider optimization problems with stochastic composite objective function subject to (possibly) infinite intersection of constraints. The objective function is expressed in terms of expectation operator over a sum of two terms satisfying a stochastic bounded gradient condition, with or without strong convexity type properties. In contrast to the classical approach, where the constraints are usually represented as intersection of simple sets, in this paper we consider that each constraint set is given as the level set of a convex but not necessarily differentiable function. Based on the exibility offered by our general optimization model we consider a stochastic subgradient method with random feasibility updates. At each iteration, our algorithm takes a stochastic proximal (sub)gradient step aimed at minimizing the objective function and then a subsequent subgradient step minimizing the feasibility violation of the observed random constraint. We analyze the convergence behavior of the proposed algorithm for diminishing stepsizes and for the case when the objective function is convex or has a quadratic functional growth, unifying the nonsmooth and smooth cases. We prove sublinear convergence rates for this stochastic subgradient algorithm, which are known to be optimal for subgradient methods on this class of problems. When the objective function has a linear least-square form and the constraints are polyhedral, it is shown that the algorithm converges linearly. Numerical evidence supports the effectiveness of our method in real problems.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {265},
numpages = {35},
keywords = {stochastic optimization, convex functional constraints, stochastic subgradient, rate of convergence, constrained least-squares, robust/sparse svm}
}

@inproceedings{10.1145/3131151.3131152,
author = {Filho, Helson L. Jakubovski and Lima, Jackson A. Prado and Vergilio, Silvia R.},
title = {Automatic Generation of Search-Based Algorithms Applied to the Feature Testing of Software Product Lines},
year = {2017},
isbn = {9781450353267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131151.3131152},
doi = {10.1145/3131151.3131152},
abstract = {The selection of products for the variability testing of Feature Models (FMs) is a complex task impacted by many factors. To solve this problem, Multi-Objective Evolutionary Algorithms (MOEAs) have been successfully used in the field known as Search-Based Software Engineering (SBSE). However, the design of a search-based approach is not an easy task for the software engineer, who can find some difficulties such as: the choice and configuration of the best MOEAs, the choice of the best search operators to be implemented, and so on. In addition to this, existing approaches are dependent on the problem domain and do not allow reuse. In this way the use of Hyper-Heuristic (HH) can help to obtain more generic and reusable search-based approaches, and because of this is considered a trend in the SBSE field. Following this trend and to contribute to reduce the software engineer's efforts, this work explores the use of a hyper-heuristic for automatic generation of MOEAs to select test products from the FM, considering three factors: pairwise coverage, mutation score and cost, given by the number of products. The HH is based on a grammar that represents the elements, parameters and components of existing MOEAs and implements evolutionary operators, such as crossover and mutation, suitable for selection problems. In this way, it can be reused for other similar software engineering problems. Evaluation results show that the proposed approach obtains results that are better or statistically equivalent than similar approaches found in the literature.},
booktitle = {Proceedings of the XXXI Brazilian Symposium on Software Engineering},
pages = {114–123},
numpages = {10},
keywords = {Hyper-Heuristics, Search-Based Software Engineering, Software Product Line Testing},
location = {Fortaleza, CE, Brazil},
series = {SBES '17}
}

@inproceedings{10.1145/2110147.2110166,
author = {M\ae{}rsk-M\o{}ller, Hans Martin and J\o{}rgensen, Bo N\o{}rregaard},
title = {Cardinality-dependent variability in orthogonal variability models},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110166},
doi = {10.1145/2110147.2110166},
abstract = {During our work on developing and running a software product line for eco-sustainable greenhouse-production software tools, which currently have three products members we have identified a need for extending the notation of the Orthogonal Variability Model (OVM) to support what we refer to as cardinality range dependencies. The cardinality-range-dependency type enables expressing that the binding of a certain number of variants to a variation point can influence variability in other places in the model. In other words, we acknowledge that variability can be influenced, not necessarily by the specific variants being bound, but by their sheer numbers.This paper contributes with an extension to the meta-model underlying the OVM notation, suggesting a notation for the new type of dependency and shows its applicability. The specific case, which initially required this extension, will work as running example throughout the paper and underline the need for the extension. Finally, the paper evaluates and discusses the general applicability of the proposed notation extension and future perspectives.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {165–172},
numpages = {8},
keywords = {documentation, orthogonal variability model (OVM), software product line engineering, variability modeling language},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@inproceedings{10.1145/3019612.3019768,
author = {Arcaini, Paolo and Trubiani, Catia},
title = {Collaborative development of feature models and evaluation of performance bounds},
year = {2017},
isbn = {9781450344869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019612.3019768},
doi = {10.1145/3019612.3019768},
abstract = {The development of feature models enables configurable software systems, however in a collaborative environment different designers may specify and select software and/or hardware features in a conflicting way. Starting from multiple feature models developed by the different designers, we produce some artefacts, such as a merged or a diff feature model representing the commonalities and differences, respectively, to support the designers in achieving an agreement on a shared feature model. For this last model we compute a partial order of system products based on the inclusion relation to get the maximal and minimal sets of common features. The performance evaluation of the software products is optimized, since bounds are obtained analyzing only the performance of the maximal and minimal elements of the proposed order. We applied the approach to an illustrative case study and we found that our technique is able to correctly provide performance bounds while minimizing the number of evaluations, thus saving computational time.},
booktitle = {Proceedings of the Symposium on Applied Computing},
pages = {1162–1167},
numpages = {6},
keywords = {collaborative development, feature models, model-based performance analysis},
location = {Marrakech, Morocco},
series = {SAC '17}
}

@inproceedings{10.1145/2110147.2110154,
author = {Steffens, Michaela and Oster, Sebastian and Lochau, Malte and Fogdal, Thomas},
title = {Industrial evaluation of pairwise SPL testing with MoSo-PoLiTe},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110154},
doi = {10.1145/2110147.2110154},
abstract = {Testing Software Product Lines is a very challenging task due to variability. Frequently, approaches such as combinatorial testing are used to generate representative sets of products for testing purposes instead of testing each individual product of the SPL under test. In this contribution we present the results of applying the MoSo-PoLiTe framework at Danfoss Power Electronics A/S to calculate a representative set of product configurations for black box testing purposes. Within this evaluation we use MoSo-PoLiTe's pairwise configuration selection component on the basis of a feature model. This component implements a heuristics finding a minimal subset of configurations covering 100% T-wise feature interaction. According to the best of our knowledge, this is the first publication providing industrial results about pairwise SPL testing.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {55–62},
numpages = {8},
keywords = {combinatorial testing, evaluation, feature model-based testing, pairwise testing, product lines},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@inproceedings{10.5555/2666064.2666077,
author = {Leitner, Andrea and Wei\ss{}, Reinhold and Kreiner, Christian},
title = {Optimizing problem space representations through domain multi-modeling},
year = {2012},
isbn = {9781467317511},
publisher = {IEEE Press},
abstract = {This work states that there is a need for an optimized problem space representation for heterogeneous domains. We identify two modeling paradigms widely used in practice: Domain-Specific Modeling (DSM) and Feature-Oriented Domain Modeling (FODM). Each modeling paradigm favors different domain characteristics. Especially the fact that software often is embedded either in a system or in a process and, therefore, is strongly influenced by its environment enforces the demand for a combined representation.We propose a concept for a multi-modeling approach based on existing technology. Multi-modeling means the combination of the two main modeling paradigms to represent a heterogeneous domain. The major benefit of the approach is the reduction of representation complexity by optimizing the representation of single subdomains. This will be shown on one representative case study from the automotive domain. Another advantage is the improved stakeholder communication because of familiar notations. A discussion of limitations shows potential for future work.},
booktitle = {Proceedings of the Third International Workshop on Product LinE Approaches in Software Engineering},
pages = {49–52},
numpages = {4},
keywords = {automotive software product line, binding times, domain modeling, model-based development, software product line engineering, variant-rich component model},
location = {Zurich, Switzerland},
series = {PLEASE '12}
}

@inproceedings{10.1145/3361525.3361544,
author = {Ni, Xiang and Schneider, Scott and Pavuluri, Raju and Kaus, Jonathan and Wu, Kun-Lung},
title = {Automating Multi-level Performance Elastic Components for IBM Streams},
year = {2019},
isbn = {9781450370097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361525.3361544},
doi = {10.1145/3361525.3361544},
abstract = {Streaming applications exhibit abundant opportunities for pipeline parallelism, data parallelism and task parallelism. Prior work in IBM Streams introduced an elastic threading model that sought the best performance by automatically tuning the number of threads. In this paper, we introduce the ability to automatically discover where that threading model is profitable. However this introduces a new challenge: we have separate performance elastic mechanisms that are designed with different objectives, leading to potential negative interactions and unintended performance degradation. We present our experiences in overcoming these challenges by showing how to coordinate separate but interfering elasticity mechanisms to maxmize performance gains with stable and fast parallelism exploitation. We first describe an elastic performance mechanism that automatically adapts different threading models to different regions of an application. We then show a coherent ecosystem for coordinating this threading model elasticty with thread count elasticity. This system is an online, stable multi-level elastic coordination scheme that adapts different regions of a streaming application to different threading models and number of threads. We implemented this multi-level coordination scheme in IBM Streams and demonstrated that it (a) scales to over a hundred threads; (b) can improve performance by an order of magnitude on two different processor architectures when an application can benefit from multiple threading models; and (c) achieves performance comparable to hand-optimized applications but with much fewer threads.},
booktitle = {Proceedings of the 20th International Middleware Conference},
pages = {163–175},
numpages = {13},
keywords = {Stream processing, elastic scheduling, runtime},
location = {Davis, CA, USA},
series = {Middleware '19}
}

@inproceedings{10.1145/2976767.2976804,
author = {Fang, Miao and Leyh, Georg and Doerr, Joerg and Elsner, Christoph},
title = {Multi-variability modeling and realization for software derivation in industrial automation management},
year = {2016},
isbn = {9781450343213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976767.2976804},
doi = {10.1145/2976767.2976804},
abstract = {The systems of industrial automation management (IAM) are in the domain of information systems. IAM systems have software components that support manufacturing processes. The operational parts of IAM coordinate highly plug-compatible hardware devices. These functions lead to process and topology variability, which result in development and reuse challenges for software engineers in practice. This paper presents an approach aiming at improving the development and derivation of one IAM software family within Siemens. The approach integrates feature modeling with domain-specific modeling languages (DSMLs) for variability representation. Moreover, by combining code generation techniques, the configuration of variability models can be used to automate the software derivation. We report on a case study of applying the approach in practice. The outcome shows the enhancement of variability representation by introducing DSMLs and the improvement on automating software derivation. Finally, we present the lessons learned during the execution of this case study.},
booktitle = {Proceedings of the ACM/IEEE 19th International Conference on Model Driven Engineering Languages and Systems},
pages = {2–12},
numpages = {11},
keywords = {code generation, domain-specific modeling, model-based engineering, software derivation, software product line, variability modeling},
location = {Saint-malo, France},
series = {MODELS '16}
}

@inproceedings{10.1145/2451617.2451619,
author = {Kowal, Matthias and Schulze, Sandro and Schaefer, Ina},
title = {Towards efficient SPL testing by variant reduction},
year = {2013},
isbn = {9781450318679},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2451617.2451619},
doi = {10.1145/2451617.2451619},
abstract = {Testing software systems plays a pivotal role for quality, reliability, and safety of such systems. Several approaches exist that provide efficient algorithms to test one software system. However, in the context of variable software systems, called software product lines (SPLs), testing has to deal with potentially thousands of variants. Unfortunately, current approaches do not scale to this problem and thus testing SPLs efficiently is a challenging task. In this paper, we propose an approach to reduce the test set by explicitly modeling information about shared resources and communication in feature models. As a result, we can figure out features that interact with each other and thus are more likely to cause problems. We show with a small case study that our approach reduces both, the features under test as well as the time for computing all feature combinations to be tested.},
booktitle = {Proceedings of the 4th International Workshop on Variability &amp; Composition},
pages = {1–6},
numpages = {6},
keywords = {feature models, software product lines, testing},
location = {Fukuoka, Japan},
series = {VariComp '13}
}

@inproceedings{10.1145/2786805.2786845,
author = {Siegmund, Norbert and Grebhahn, Alexander and Apel, Sven and K\"{a}stner, Christian},
title = {Performance-influence models for highly configurable systems},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786845},
doi = {10.1145/2786805.2786845},
abstract = {Almost every complex software system today is configurable. While configurability has many benefits, it challenges performance prediction, optimization, and debugging. Often, the influences of individual configuration options on performance are unknown. Worse, configuration options may interact, giving rise to a configuration space of possibly exponential size. Addressing this challenge, we propose an approach that derives a performance-influence model for a given configurable system, describing all relevant influences of configuration options and their interactions. Our approach combines machine-learning and sampling heuristics in a novel way. It improves over standard techniques in that it (1) represents influences of options and their interactions explicitly (which eases debugging), (2) smoothly integrates binary and numeric configuration options for the first time, (3) incorporates domain knowledge, if available (which eases learning and increases accuracy), (4) considers complex constraints among options, and (5) systematically reduces the solution space to a tractable size. A series of experiments demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {284–294},
numpages = {11},
keywords = {Performance-influence models, machine learning, sampling},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/3180155.3180257,
author = {Xue, Yinxing and Li, Yan-Fu},
title = {Multi-objective integer programming approaches for solving optimal feature selection problem: a new perspective on multi-objective optimization problems in SBSE},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180257},
doi = {10.1145/3180155.3180257},
abstract = {The optimal feature selection problem in software product line is typically addressed by the approaches based on Indicator-based Evolutionary Algorithm (IBEA). In this study we first expose the mathematical nature of this problem --- multi-objective binary integer linear programming. Then, we implement/propose three mathematical programming approaches to solve this problem at different scales. For small-scale problems (roughly less than 100 features), we implement two established approaches to find all exact solutions. For medium-to-large problems (roughly, more than 100 features), we propose one efficient approach that can generate a representation of the entire Pareto front in linear time complexity. The empirical results show that our proposed method can find significantly more non-dominated solutions in similar or less execution time, in comparison with IBEA and its recent enhancement (i.e., IBED that combines IBEA and Differential Evolution).},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1231–1242},
numpages = {12},
keywords = {multi-objective integer programming (MOIP), multi-objective optimization (MOO), optimal feature selection problem},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3510003.3510200,
author = {Dubslaff, Clemens and Weis, Kallistos and Baier, Christel and Apel, Sven},
title = {Causality in configurable software systems},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510200},
doi = {10.1145/3510003.3510200},
abstract = {Detecting and understanding reasons for defects and inadvertent behavior in software is challenging due to their increasing complexity. In configurable software systems, the combinatorics that arises from the multitude of features a user might select from adds a further layer of complexity. We introduce the notion of feature causality, which is based on counterfactual reasoning and inspired by the seminal definition of actual causality by Halpern and Pearl. Feature causality operates at the level of system configurations and is capable of identifying features and their interactions that are the reason for emerging functional and non-functional properties. We present various methods to explicate these reasons, in particular well-established notions of responsibility and blame that we extend to the feature-oriented setting. Establishing a close connection of feature causality to prime implicants, we provide algorithms to effectively compute feature causes and causal explications. By means of an evaluation on a wide range of configurable software systems, including community benchmarks and real-world systems, we demonstrate the feasibility of our approach: We illustrate how our notion of causality facilitates to identify root causes, estimate the effects of features, and detect feature interactions.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {325–337},
numpages = {13},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3593663.3593677,
author = {Willert, Nico and Eriksson, Janik},
title = {Towards a feature-based didactic framework for generating individualized programming tasks for an e-learning environment},
year = {2023},
isbn = {9781450399562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593663.3593677},
doi = {10.1145/3593663.3593677},
abstract = {Adaptive programming tasks are a promising approach for personalized learning that adapts to each student’s unique needs and abilities. However, developing effective adaptive programming tasks can be challenging, particularly when it comes to selecting the appropriate changes and adapting the difficulty of the exercise. In this paper, we propose a model for tracking student knowledge and adapting programming exercises to guide the selection and implementation of task features. Our model combines aspects of cognitive load, computational thinking and feature-oriented software product line engineering to identify core and optional features, so that they can be used in conjunction to adapt to the specific needs and abilities of each student. We provide an overview over the insights gained from an exploratory study with students. To support the creation process of feature-based programming tasks, we present an approach using a template-based generator.},
booktitle = {Proceedings of the 5th European Conference on Software Engineering Education},
pages = {246–255},
numpages = {10},
keywords = {adaptivity, assessment, computational thinking, instructional design},
location = {Seeon/Bavaria, Germany},
series = {ECSEE '23}
}

@inproceedings{10.1145/3382494.3410677,
author = {Shu, Yangyang and Sui, Yulei and Zhang, Hongyu and Xu, Guandong},
title = {Perf-AL: Performance Prediction for Configurable Software through Adversarial Learning},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410677},
doi = {10.1145/3382494.3410677},
abstract = {Context: Many software systems are highly configurable. Different configuration options could lead to varying performances of the system. It is difficult to measure system performance in the presence of an exponential number of possible combinations of these options.Goal: Predicting software performance by using a small configuration sample.Method: This paper proposes Perf-AL to address this problem via adversarial learning. Specifically, we use a generative network combined with several different regularization techniques (L1 regularization, L2 regularization and a dropout technique) to output predicted values as close to the ground truth labels as possible. With the use of adversarial learning, our network identifies and distinguishes the predicted values of the generator network from the ground truth value distribution. The generator and the discriminator compete with each other by refining the prediction model iteratively until its predicted values converge towards the ground truth distribution.Results: We argue that (i) the proposed method can achieve the same level of prediction accuracy, but with a smaller number of training samples. (ii) Our proposed model using seven real-world datasets show that our approach outperforms the state-of-the-art methods. This help to further promote software configurable performance.Conclusion: Experimental results on seven public real-world datasets demonstrate that PERF-AL outperforms state-of-the-art software performance prediction methods.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {16},
numpages = {11},
keywords = {Software performance prediction, adversarial learning, configurable systems, regularization},
location = {Bari, Italy},
series = {ESEM '20}
}

@inproceedings{10.1145/3451421.3451427,
author = {Liu, Xiaoli and Li, Jiali and Cao, Peng},
title = {SP-MTFL: A self paced multi-task feature learning method for cognitive performance predicting of Alzheimer's disease},
year = {2021},
isbn = {9781450389686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3451421.3451427},
doi = {10.1145/3451421.3451427},
abstract = {Machine learning algorithms and multivariate data analysis methods have been widely utilized in the field of Alzheimer's disease (AD) research in recent years. Predicting cognitive performance of subjects from neuroimage measures and identifying relevant imaging biomarkers are important research topics in the study of Alzheimer's disease. Multi-task based feature learning (MTFL) have been widely studied to select a discriminative feature subset from MRI features, and improve the performance by incorporating inherent correlations among multiple clinical cognitive measures. Inspired by the fact that humans often learn from easy concepts to hard ones in the cognitive process, we propose a self-paced multi-task feature learning framework that attempts to learn the tasks by simultaneously taking into consideration the complexities of both tasks and instances per task in this study. Experimental results on ADNI are provided, and the comparison results demonstrate the effectiveness of our approach and show that our approach outperforms the state-of-the-art methods.},
booktitle = {The Fourth International Symposium on Image Computing and Digital Medicine},
pages = {23–27},
numpages = {5},
keywords = {Alzheimer's disease, Machine learning, Self-paced learning, multi-task learning, regression},
location = {Shenyang, China},
series = {ISICDM 2020}
}

@inproceedings{10.5555/2874916.2874975,
author = {Mittal, Saurabh and Ruth, Mark and Pratt, Annabelle and Lunacek, Monte and Krishnamurthy, Dheepak and Jones, Wesley},
title = {A system-of-systems approach for integrated energy systems modeling and simulation},
year = {2015},
isbn = {9781510810594},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Energy systems integration combines energy carriers, including electricity, with infrastructures, to maximize efficiency and minimize waste. In order to study systems at a variety of physical scales---from individual buildings to distribution systems---interconnected through these energy infrastructures, NREL is developing an Integrated Energy System Model (IESM), with an initial focus on the electricity system. Today's electricity grid is the most complex system ever built---and the future grid is likely to be even more complex because it will incorporate distributed energy resources (DERs) such as wind, solar, and various other sources of generation and energy storage. The complexity is further augmented by the possible evolution to new retail market structures that would provide incentives to owners of DERs to support the grid. The IESM can be used to understand and test the impact of new retail market structures and technologies such as DERs, demand-response equipment, and energy management systems on the system's ability to provide reliable electricity to all customers. The IESM is composed of a power flow simulator (GridLAB-D), building and appliance models including home energy management system implemented using either GAMS or Pyomo, a market layer, and is able to include hardware-in-the-loop simulation (testing appliances such as air conditioners, dishwashers, etc.). The IESM is a system-of-systems (SoS) simulator wherein the constituent systems are brought together in a virtual testbed. We will describe an SoS approach for developing a distributed simulation environment. We will elaborate on the methodology and the control mechanisms used in the co-simulation illustrated by a case study.},
booktitle = {Proceedings of the Conference on Summer Computer Simulation},
pages = {1–10},
numpages = {10},
keywords = {DEVS, GridLAB-D, co-simulation, discrete-event simulation, integrated energy systems, optimization, smart grid, system-of-systems},
location = {Chicago, Illinois},
series = {SummerSim '15}
}

@inproceedings{10.1145/1960275.1960283,
author = {Schaefer, Ina and Bettini, Lorenzo and Damiani, Ferruccio},
title = {Compositional type-checking for delta-oriented programming},
year = {2011},
isbn = {9781450306058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1960275.1960283},
doi = {10.1145/1960275.1960283},
abstract = {Delta-oriented programming is a compositional approach to flexibly implementing software product lines. A product line is represented by a code base and a product line declaration. The code base consists of a set of delta modules specifying modifications to object-oriented programs. The product line declaration provides the connection of the delta modules with the product features. This separation increases the reusability of delta modules. In this paper, we provide a foundation for compositional type checking of delta-oriented product lines of Java programs by presenting a minimal core calculus for delta-oriented programming. The calculus is equipped with a constraint-based type system that allows analyzing each delta module in isolation, such that that also the results of the analysis can be reused. By combining the analysis results for the delta modules with the product line declaration it is possible to establish that all the products of the product line are well-typed according to the Java type system.},
booktitle = {Proceedings of the Tenth International Conference on Aspect-Oriented Software Development},
pages = {43–56},
numpages = {14},
keywords = {java, software product line, type system},
location = {Porto de Galinhas, Brazil},
series = {AOSD '11}
}

@article{10.1109/TNET.2021.3056772,
author = {Ruby, Rukhsana and Zhong, Shuxin and ElHalawany, Basem M. and Luo, Hanjiang and Wu, Kaishun},
title = {SDN-Enabled Energy-Aware Routing in Underwater Multi-Modal Communication Networks},
year = {2021},
issue_date = {June 2021},
publisher = {IEEE Press},
volume = {29},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3056772},
doi = {10.1109/TNET.2021.3056772},
abstract = {Despite extensive research efforts, underwater sensor networks (UWSNs) still suffer from serious performance issues due to their inefficient and uncoordinated channel access and resource management. For example, due to the lack of holistic knowledge on the network resources, existing decentralized routing protocols fail to provide globally optimal performance. On the other hand, Software Defined Networking (SDN), as a promising paradigm to provide prominent centralized solutions, can be employed to address the aforementioned issues in UWSNs. Indeed, SDN brings unprecedented opportunities to improve the network performance through the development of advanced algorithms at controllers. In this paper, we study the routing problem in such a network with new features including centralized route decision, global network-state awareness, seamless route discovery while considering the optimization of several long-term global performance metrics. We formulate the entire routing problem of a multi-modal UWSN as an optimization problem while considering the interference phenomenon of ad hoc scenarios and some long-term global performance metrics of an ideal routing protocol. Our formulated problem nicely captures all possible flexibilities of a sensor node no matter it has the full-duplex or half-duplex functionality. Upon the formulation, we recognize the NP-hard nature of the problem for all possible scenarios. We adopt a rounding technique based on the convex programming relaxation concept to solve the formulated routing problem that considers full-duplex scenarios, whereas we solve the problem for half-duplex scenarios using a greedy method upon interpreting it as a submodular function maximization problem. Through extensive simulation via our Python-based in-house simulator, we verify that our proposed globally optimal routing scheme always outperforms three existing decentralized routing protocols (each of these protocols are selected from each of three prominent protocol types, i.e., flooding, cross-layer information and adaptive machine learning based, respectively) in terms of reliability, latency, energy efficiency, lifetime and fairness.},
journal = {IEEE/ACM Trans. Netw.},
month = feb,
pages = {965–978},
numpages = {14}
}

@inproceedings{10.1145/3510466.3511272,
author = {Kuiter, Elias and Kn\"{u}ppel, Alexander and Bordis, Tabea and Runge, Tobias and Schaefer, Ina},
title = {Verification Strategies for Feature-Oriented Software Product Lines},
year = {2022},
isbn = {9781450396042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510466.3511272},
doi = {10.1145/3510466.3511272},
abstract = {Highly-customizable software systems in form of software product lines are becoming increasingly relevant for safety-critical systems, in which the correctness of software is a major concern. To ensure the correct behavior of a software product line, each product can be verified in isolation—however, this strategy quickly becomes infeasible for a large number of products. In this paper, we propose proof plans, a novel strategy for verifying feature-oriented software product lines based on partial proofs. Our technique splits the verification task into small proofs that can be reused across method variants, which gives rise to a wider spectrum of verification strategies for software product lines. We describe applications of our technique and evaluate one of them on a case study by comparing it with established verification strategies.},
booktitle = {Proceedings of the 16th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {12},
numpages = {9},
keywords = {Deductive Verification, Proof Reuse, Software Product Lines},
location = {Florence, Italy},
series = {VaMoS '22}
}

@inproceedings{10.1145/303008.303063,
author = {Bayer, Joachim and Flege, Oliver and Knauber, Peter and Laqua, Roland and Muthig, Dirk and Schmid, Klaus and Widen, Tanya and DeBaud, Jean-Marc},
title = {PuLSE: a methodology to develop software product lines},
year = {1999},
isbn = {1581131011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/303008.303063},
doi = {10.1145/303008.303063},
booktitle = {Proceedings of the 1999 Symposium on Software Reusability},
pages = {122–131},
numpages = {10},
keywords = {domain engineering, domain-specific software architecture, software product line},
location = {Los Angeles, California, USA},
series = {SSR '99}
}

@inproceedings{10.1145/2405136.2405145,
author = {Almeida, Andr\'{e} and Cavalcante, Everton and Batista, Thais and Lopes, Frederico and Delicato, Flavia C. and Pires, Paulo F. and Alves, Gustavo and Cacho, N\'{e}lio},
title = {Towards an SPL-based monitoring middleware strategy for cloud computing applications},
year = {2012},
isbn = {9781450316088},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2405136.2405145},
doi = {10.1145/2405136.2405145},
abstract = {Cloud-based applications are composed of services offered by distinct third-party cloud providers. The selection of the proper cloud services that fit the application needs is based on cloud-related information, i.e. properties of the services such as price, availability, response time, among others. Typically, applications rely on a middleware that abstracts away the burden of direct dealing with underlying mechanisms for service selection and communication with the cloud providers. In this context, in a previous work we already discussed the benefits of using the software product lines (SPL) paradigm for representing alternative cloud services and their properties, which is suitable for the process of choosing the proper services to compose the application. As most cloud-related information are dynamic and may change any time during the application execution, the continuous monitoring of such information is essential to ensure that the deployed application is composed of cloud services that adhere to the application requirements. In this paper we present an SPL-based monitoring middleware strategy to continuously monitoring the dynamic properties of cloud services used by an application.},
booktitle = {Proceedings of the 10th International Workshop on Middleware for Grids, Clouds and e-Science},
articleno = {9},
numpages = {6},
keywords = {cloud computing, monitoring, monitoring strategy, selection, software product lines},
location = {Montreal, Quebec, Canada},
series = {MGC '12}
}

@inproceedings{10.1145/1987875.1987886,
author = {Belategi, Lorea and Sagardui, Goiuria and Etxeberria, Leire},
title = {Model based analysis process for embedded software product lines},
year = {2011},
isbn = {9781450307307},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1987875.1987886},
doi = {10.1145/1987875.1987886},
abstract = {Nowadays, embedded system development is increasing its complexity dealing with quality, cost and time-to-market among others. Quality attributes are an important issue to consider in embedded software development where time issues may be critical. Development paradigms such as Model Driven Development and Software Product Lines can be an adequate alternative to traditional software development and validation methods due to the characteristics of embedded systems. But for a proper validation and verification based on MARTE model analysis, all variability issues and critical quality attributes that take part in analysis must be properly modelled and managed. Therefore, a model analysis process for Model Driven Embedded Software Product Lines has been defined as some process lacks have been found.},
booktitle = {Proceedings of the 2011 International Conference on Software and Systems Process},
pages = {53–62},
numpages = {10},
keywords = {model based analysis process, model driven development, performance, quality attributes, schedulability, software product line},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSSP '11}
}

@inproceedings{10.1145/3350768.3350774,
author = {Souza, Iuri Santos and Machado, Ivan and Seaman, Carolyn and Gomes, Gecynalda and Chavez, Christina and de Almeida, Eduardo Santana and Masiero, Paulo},
title = {Investigating Variability-aware Smells in SPLs: An Exploratory Study},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3350774},
doi = {10.1145/3350768.3350774},
abstract = {Variability-aware smell is a concept referring to artifact shortcomings in the context of highly-configurable systems that can degrade aspects such as program comprehension, maintainability, and evolvability. To the best of our knowledge, there is very little evidence that variability-aware smells exist in Software Product Lines (SPLs). This work presents an exploratory study that investigated (I) evidence that variability-aware smells exist in SPLs and (II) new types of variability-aware smell not yet documented in the literature based on a quantitative study with open source SPL projects. We collected quantitative data to generate reliable research evidence, by performing feature model and source code inspections on eleven open-source SPL projects. Our findings revealed that (1) instances of variability-aware smells exist in open-source SPL projects and (2) feature information presented significant associations with variability-aware smells. Furthermore, (3) the study presented six new types of variability-aware smells.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {367–376},
numpages = {10},
keywords = {Empirical Study, Exploratory Study, Software Product Lines, Variability-Aware Smells},
location = {Salvador, Brazil},
series = {SBES '19}
}

@inproceedings{10.1145/3023956.3023961,
author = {Lity, Sascha and Al-Hajjaji, Mustafa and Th\"{u}m, Thomas and Schaefer, Ina},
title = {Optimizing product orders using graph algorithms for improving incremental product-line analysis},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023961},
doi = {10.1145/3023956.3023961},
abstract = {The individual analysis of each product of a software product line (SPL) leads to redundant analysis steps due to the inherent commonality. Therefore, incremental SPL analyses exploit commonalities and focus on the differences between products to reduce the analysis effort. However, existing techniques are influenced by the order in which products are analyzed. The more similar subsequently analyzed products are, the greater is the potential reduction of the overall analysis effort as similar products imply less differences to be analyzed. Hence, an order of products, where the total number of differences is minimized, facilitates incremental SPL analyses. In this paper, we apply graph algorithms to determine optimized product orders. We capture products as nodes in a graph, where solution-space information defines edge weights between product nodes. We adopt existing heuristics for finding an optimal solution of the traveling salesperson problem to determine a path in the product graph with minimal costs. A path represents an optimized product order w.r.t. minimized differences between all products. We realize a prototype of our approach and evaluate its applicability and performance showing a significant optimization compared to standard and random orders.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {60–67},
numpages = {8},
keywords = {delta-oriented software product lines, graph algorithms, product orders},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/3663529.3663851,
author = {Zellmer, Philipp and Kr\"{u}ger, Jacob and Leich, Thomas},
title = {Decision Making for Managing Automotive Platforms: An Interview Survey on the State-of-Practice},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663851},
doi = {10.1145/3663529.3663851},
abstract = {The automotive industry is changing due to digitization, a growing focus on software, and the increasing use of electronic control units. Consequently, automotive engineering is shifting from hardware-focused towards software-focused platform concepts to address these challenges. This shift includes adopting and integrating methods like electrics/electronics platforms, software product-line engineering, and product generation. Although these concepts are well-known in their respective research fields and different industries, there is limited research on their practical effectiveness and issues—particularly when implementing and using these concepts for modern automotive platforms. The lack of research and practical experiences challenges particularly decision makers, who cannot build on reliable evidence or techniques. In this paper, we address this gap by reporting on the state-of-practice of supporting the decision making for managing automotive electrics/electronics platforms, which integrate hardware, software, and electrics/electronics artifacts. For this purpose, we conducted 26 interviews with experts from the automotive domain. We derived questions from a previous mapping study in which we collected current research on product-structuring concepts, aiming to derive insights on the consequent practical challenges and requirements. Specifically, we contribute an overview of the requirements and criteria for (re)designing the decision-making process for managing electrics/electronics platforms within the automotive domain from the practitioners’ view. Through this, we aim to assist practitioners in managing electrics/electronics platforms, while also providing starting points for future research on a real-world problem.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {318–328},
numpages = {11},
keywords = {automotive, cyber-physical system, decision making, electrics/electronics, life-cycle management, platform management, product line, product structuring concept},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/3425269.3425278,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Supporting user preferences in search-based product line architecture design using Machine Learning},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425278},
doi = {10.1145/3425269.3425278},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA design requires intensive human effort as it involves several conflicting factors. In order to support this task, an interactive search-based approach, automated by a tool named OPLA-Tool, was proposed in a previous work. Through this tool the software architect evaluates the generated solutions during the optimization process. Considering that evaluating PLA is a complex task and search-based algorithms demand a high number of generations, the evaluation of all solutions in all generations cause human fatigue. In this work, we incorporated in OPLA-Tool a Machine Learning (ML) model to represent the architect in some moments during the optimization process aiming to decrease the architect's effort. Through the execution of a quantiqualitative exploratory study it was possible to demonstrate the reduction of the fatigue problem and that the solutions produced at the end of the process, in most cases, met the architect's needs.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Human-computer interaction, Machine Learning, Product Line Architecture},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1145/3235830.3235834,
author = {Escobar, Juan Jos\'{e} and Ortega, Julio and D\'{\i}az, Antonio Francisco and Gonz\'{a}lez, Jes\'{u}s and Damas, Miguel},
title = {Speedup and Energy Analysis of EEG Classification for BCI Tasks on CPU-GPU Clusters},
year = {2018},
isbn = {9781450365314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3235830.3235834},
doi = {10.1145/3235830.3235834},
abstract = {Many data mining applications on bioinformatics and bioengineering require solving problems with different profiles from the point of view of their implicit parallelism. In this context, heterogeneous architectures comprised by interconnected nodes with multiple multi-core microprocessors and accelerators, such as vector processors, Graphics Processing Units (GPUs), or Field-Programmable Gate Arrays would constitute suitable platforms that offer the possibility of not only to accelerate the running time of the applications, but also to optimize the energy consumption. In this paper, we analyze the speedups and energy consumption of a parallel multiobjective approach for feature selection and classification of electroencephalograms in Brain Computing Interface tasks, by considering different implementation alternatives in a heterogeneous CPU-GPU cluster. The procedure is able to take advantage of parallelism through message-passing among the CPU-GPU nodes of the cluster (through shared-memory and thread-level parallelism in the CPU cores, and data-level parallelism and thread-level parallelism in the GPU). The experimental results show high code accelerations and high energy-savings: running times between 1.4 and 5.3% of the sequential time and energy consumptions between 5.9 and 11.6% of the energy consumed by the sequential execution.},
booktitle = {Proceedings of the 6th International Workshop on Parallelism in Bioinformatics},
pages = {33–43},
numpages = {11},
keywords = {BCI Tasks, Distributed Programming, EEG Classification, Energy-aware Computing, Heterogeneous Cluster, Parallelism},
location = {Barcelona, Spain},
series = {PBio 2018}
}

@inproceedings{10.1145/3196398.3196407,
author = {Laaber, Christoph and Leitner, Philipp},
title = {An evaluation of open-source software microbenchmark suites for continuous performance assessment},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196407},
doi = {10.1145/3196398.3196407},
abstract = {Continuous integration (CI) emphasizes quick feedback to developers. This is at odds with current practice of performance testing, which predominantely focuses on long-running tests against entire systems in production-like environments. Alternatively, software microbenchmarking attempts to establish a performance baseline for small code fragments in short time. This paper investigates the quality of microbenchmark suites with a focus on suitability to deliver quick performance feedback and CI integration. We study ten open-source libraries written in Java and Go with benchmark suite sizes ranging from 16 to 983 tests, and runtimes between 11 minutes and 8.75 hours. We show that our study subjects include benchmarks with result variability of 50% or higher, indicating that not all benchmarks are useful for reliable discovery of slowdowns. We further artificially inject actual slowdowns into public API methods of the study subjects and test whether test suites are able to discover them. We introduce a performance-test quality metric called the API benchmarking score (ABS). ABS represents a benchmark suite's ability to find slowdowns among a set of defined core API methods. Resulting benchmarking scores (i.e., fraction of discovered slowdowns) vary between 10% and 100% for the study subjects. This paper's methodology and results can be used to (1) assess the quality of existing microbenchmark suites, (2) select a set of tests to be run as part of CI, and (3) suggest or generate benchmarks for currently untested parts of an API.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {119–130},
numpages = {12},
keywords = {Go, Java, continuous integration, empirical study, microbenchmarking, software performance testing},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.5555/2675983.2676315,
author = {Gutenschwager, Kai and Rabe, Markus and Sari, Mehmet Umut and Fechteler, Till},
title = {A data model for carbon footprint simulation in consumer goods supply chains},
year = {2013},
isbn = {9781479920778},
publisher = {IEEE Press},
abstract = {CO2 efficiency is currently a popular topic in supply chain management. Most approaches are based on the Life Cycle Assessment (LCA) which usually exploits data from a static database. This approach is effective when estimating the carbon footprint of products or groups of products in general. Simulation has been a proper method for metering the effectiveness of logistics systems, and could thus be expected to also support the analysis of CO2 efficiency in supply chains (SC) when combined with an LCA database. However, research shows that this combination does not deliver reliable results when the target of the study is improvement of the logistics in the SC. The paper demonstrates the shortcomings of the LCA-analogous approach and proposes a data model that enables discrete event simulation of SC logistics including its impact on the carbon footprint that is under development in the e-SAVE joint project funded by the European Commission.},
booktitle = {Proceedings of the 2013 Winter Simulation Conference: Simulation: Making Decisions in a Complex World},
pages = {2677–2688},
numpages = {12},
location = {Washington, D.C.},
series = {WSC '13}
}

@inproceedings{10.1145/2602576.2602585,
author = {Etxeberria, Leire and Trubiani, Catia and Cortellessa, Vittorio and Sagardui, Goiuria},
title = {Performance-based selection of software and hardware features under parameter uncertainty},
year = {2014},
isbn = {9781450325769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602576.2602585},
doi = {10.1145/2602576.2602585},
abstract = {Configurable software systems allow stakeholders to derive variants by selecting software and/or hardware features. Performance analysis of feature-based systems has been of large interest in the last few years, however a major research challenge is still to conduct such analysis before achieving full knowledge of the system, namely under a certain degree of uncertainty. In this paper we present an approach to analyze the correlation between selection of features embedding uncertain parameters and system performance. In particular, we provide best and worst case performance bounds on the basis of selected features and, in cases of wide gaps among these bounds, we carry on a sensitivity analysis process aimed at taming the uncertainty of parameters. The application of our approach to a case study in the e-health domain demonstrates how to support stakeholders in the identification of system variants that meet performance requirements.},
booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {23–32},
numpages = {10},
keywords = {feature selection, performance analysis, software architectures, uncertainty},
location = {Marcq-en-Bareul, France},
series = {QoSA '14}
}

@inproceedings{10.1145/1138474.1138485,
author = {Yoshimura, Kentaro and Ganesan, Dharmalingam and Muthig, Dirk},
title = {Assessing merge potential of existing engine control systems into a product line},
year = {2006},
isbn = {1595934022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1138474.1138485},
doi = {10.1145/1138474.1138485},
abstract = {Engine Control Systems (ECS) for automobiles have many variants for many manufactures and several markets. To improve their development efficiency, exploiting ECS commonalities and predicting their variability are mandatory. The concept of software product line engineering meets this ECS business background. However, we should carefully investigate the expected technical, economical, and organizational effects of introducing the strategy into existing products. Thereafter, a strategy will be derived systematically and realize the desired benefits.This paper reports an experience with the up-front investigation performed for Hitachi's ECS. We focus on the approach to plan the migration of the existing family of individual systems into a future product line. The approach assesses potential ways of merging software from existing variants and eventually defines a procedure for performing the migration. To get a high quality strategy, we integrate the approach of software measurement, the expertise of software architects, and reverse engineering techniques.},
booktitle = {Proceedings of the 2006 International Workshop on Software Engineering for Automotive Systems},
pages = {61–67},
numpages = {7},
keywords = {clone detection and classification, engine control systems, reverse engineering, software product line},
location = {Shanghai, China},
series = {SEAS '06}
}

@inproceedings{10.5555/3320516.3320905,
author = {Stoldt, Johannes and Prell, Bastian and Schlegel, Andreas and Putz, Matthias},
title = {Applications for models of renewable energy sources and energy storages in material flow simulation},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {The increasing reliance on volatile renewable energy sources in the European Union raises questions regarding the future mechanisms of the energy markets. Energy-intensive production industries are particularly expected to take a more active role by shaping their energy demand according to the availability of wind and sun. Hence, they will need to align their production processes with external energy market signals. This paper presents an application example for a Siemens Plant Simulation extension that makes holistic material flow and energy flow studies of factories possible. The so-called eniBRIC class library provides functionalities for investigating the flow of energy between infrastructure and production equipment. Since its latest update, it can also be used to model renewable energy sources as well as energy storages. A case study of the E3-Research Factory showcases the features of eniBRIC and provides an outlook on future research in the field of energy-flexible production.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {3287–3298},
numpages = {12},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@article{10.1145/1183236.1183272,
author = {Pe\~{n}a, Joaquin and Hinchey, Michael G. and Ruiz-Cort\'{e}s, Antonio},
title = {Multi-agent system product lines: challenges and benefits},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183272},
doi = {10.1145/1183236.1183272},
journal = {Commun. ACM},
month = dec,
pages = {82–84},
numpages = {3}
}

@inproceedings{10.1145/3634713.3634714,
author = {Kodetzki, Maximilian and Bordis, Tabea and Runge, Tobias and Schaefer, Ina},
title = {Partial Proofs to Optimize Deductive Verification of Feature-Oriented Software Product Lines},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634714},
doi = {10.1145/3634713.3634714},
abstract = {Software product lines (SPLs) are a technique to efficiently develop families of software products. Code is implemented in functional features which are composed to individual software variants. SPLs are oftentimes used in safety-critical systems, which is why functional correctness is more important than ever. As an advanced approach, deductive verification offers the possibility to verify the behaviour of software against a formal specification. When deductive verification is applied for SPLs, it meets the challenges of an SPLs variability. Since most verification approaches do not scale for variant-rich product lines, we take up existing approaches of reuse of proof parts to develop our concept of partial proofs. We split proofs into a feature-specific and a product-specific part. The feature-specific part is only proven once for all products enabling advanced proof reuse. We implement our concept of partial proofs in the tool VarCorC and evaluate it on three case studies. We found that both the number of proof steps and the verification time can be reduced by using partial proofs. Further, we determine a trend of increasing improvements of verification costs for large-scale SPLs.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {17–26},
numpages = {10},
keywords = {deductive verification, formal methods, software product lines},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@article{10.1145/1968587.1968607,
author = {Geetha, D. Evangelin and Kumar, T.V. Suresh and Kanth, K. Rajani},
title = {Framework for hybrid performance prediction process model: use case performance engineering approach},
year = {2011},
issue_date = {May 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/1968587.1968607},
doi = {10.1145/1968587.1968607},
abstract = {The dynamic behavior of distributed systems requires that their performance characteristics be determined rigorously, preferably in the early stages of software engineering process. Evaluation of the performance at the end of software development leads to increase in the cost of design change. To compare design alternatives or to identify system bottlenecks, quantitative system analysis must be carried out from the early stages of the software development life cycle. In this paper we describe a process model, Hybrid Performance Prediction Process Model that allows modeling and evaluating distributed systems with the explicit goal of assessing performance of the software system during feasibility study. The use case performance engineering approach proposed in this paper exploits use case model and provides flexibility to integrate the software performance prediction process with software engineering process. We use an e-parking application to demonstrate various elements in our framework. The performance metrics are obtained and analyzed by considering two software architectures. Sensitivity analysis on the behavior of resources is carried out. This analysis helps to determine the capacity of the execution environment to obtain the defined performance objectives.},
journal = {SIGSOFT Softw. Eng. Notes},
month = may,
pages = {1–15},
numpages = {15},
keywords = {4+1 view model, hybrid performance prediction process model, multitier architecture, simulation model, software performance prediction, unified modeling language, use case performance engineering}
}

@article{10.1145/2897760,
author = {Hierons, Robert M. and Li, Miqing and Liu, Xiaohui and Segura, Sergio and Zheng, Wei},
title = {SIP: Optimal Product Selection from Feature Models Using Many-Objective Evolutionary Optimization},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/2897760},
doi = {10.1145/2897760},
abstract = {A feature model specifies the sets of features that define valid products in a software product line. Recent work has considered the problem of choosing optimal products from a feature model based on a set of user preferences, with this being represented as a many-objective optimization problem. This problem has been found to be difficult for a purely search-based approach, leading to classical many-objective optimization algorithms being enhanced either by adding in a valid product as a seed or by introducing additional mutation and replacement operators that use an SAT solver. In this article, we instead enhance the search in two ways: by providing a novel representation and by optimizing first on the number of constraints that hold and only then on the other objectives. In the evaluation, we also used feature models with realistic attributes, in contrast to previous work that used randomly generated attribute values. The results of experiments were promising, with the proposed (SIP) method returning valid products with six published feature models and a randomly generated feature model with 10,000 features. For the model with 10,000 features, the search took only a few minutes.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {17},
numpages = {39},
keywords = {Product selection}
}

@inproceedings{10.1145/2110147.2110158,
author = {Lopez-Herrejon, Roberto E. and Egyed, Alexander},
title = {Towards fixing inconsistencies in models with variability},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110158},
doi = {10.1145/2110147.2110158},
abstract = {Recent years have witnessed a convergence between research in SPL and Model-Driven Engineering (MDE) that leverages the complementary capabilities that both paradigms can offer. A crucial factor for the success of MDE is the availability of effective support for detecting and fixing inconsistencies among model elements. The importance of such support is attested by the extensive literature devoted to the topic. However, when coupled with variability, the research focus has been devoted to inconsistency detection, while leaving the important issue of fixing the inconsistency largely unaddressed. In this research-in-progress paper, we explore one of the issues that variability raises for inconsistency fixing. Namely, in which features to locate the fixes. We compute what is the minimal number of fixes and use it as a baseline to compare fixes obtained with a heuristic based on feature model analysis and random approaches. Our work highlights the pros and cons of both approaches and suggests how they could be addressed.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {93–100},
numpages = {8},
keywords = {consistency, consistency checking, feature oriented software development, model, safe composition, software product line, variability},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@inproceedings{10.1145/2608628.2608661,
author = {Meng, Lingchuan and Johnson, Jeremy},
title = {High performance implementation of the TFT},
year = {2014},
isbn = {9781450325011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2608628.2608661},
doi = {10.1145/2608628.2608661},
abstract = {This paper reports on a high-performance implementation of the truncated Fourier transform (TFT). A general Cooley-Tukey like algorithm for the TFT is developed that allows the implementation to automatically adapt to the memory hierarchy. Then the algorithm introduces a small relaxation for larger transform sizes which trades off slightly higher arithmetic cost for improved data flow which allows full vectorization and parallelization. The implementation is automatically derived and tuned using the SPIRAL system for code generation and adaptation. The resulting arbitrary-size TFT library smooths out the staircase performance associated with power-of-two modular FFT implementations while retaining the performance associated with state-of-the-art FFT libraries. This provides significant performance improvement over approaches that pad to the next power of two even when using high-performance FFT libraries.},
booktitle = {Proceedings of the 39th International Symposium on Symbolic and Algebraic Computation},
pages = {328–334},
numpages = {7},
keywords = {automatic performance tuning, parallelization, truncated fourier transform, vectorization},
location = {Kobe, Japan},
series = {ISSAC '14}
}

@inproceedings{10.1145/1551722.1551730,
author = {Laguna, Miguel A. and Finat, Javier and Gonz\'{a}lez, Jos\'{e} A.},
title = {Mobile health monitoring and smart sensors: a product line approach},
year = {2009},
isbn = {9781605583983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1551722.1551730},
doi = {10.1145/1551722.1551730},
abstract = {The evolution of the population pyramid in developed countries, with an increasing proportion of aged people introduces new challenges to the public and private assistance services. A form of improving these services while keeping controlled the associated costs is to use remote continuous assistance. Wireless sensors allow obtaining real-time information of health parameters in a non-intrusive way. The determination of alert values for these parameters and the computing possibilities of the current mobile devices can facilitate a faster intervention which will minimize risks linked to delays in medical assistance. However, the diversity of risk situations is a factor that increases costs as many similar but not exactly identical products will be necessary now and in the future. We aim to solve this problem using an approach of software product lines, as multiple options can be easily incorporated to each final product implementation. This article presents the product line generic architecture and some examples of application, using wireless sensors connected to a central station by means of a smart phone, which is able to detect alarm situations.},
booktitle = {Proceedings of the 2009 Euro American Conference on Telematics and Information Systems: New Opportunities to Increase Digital Citizenship},
articleno = {8},
numpages = {8},
keywords = {remote health monitoring, sensor, software product line},
location = {Prague, Czech Republic},
series = {EATIS '09}
}

@inproceedings{10.1145/3141848.3141853,
author = {Schuster, Sven and Seidl, Christoph and Schaefer, Ina},
title = {Towards a development process for maturing Delta-oriented software product lines},
year = {2017},
isbn = {9781450355186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141848.3141853},
doi = {10.1145/3141848.3141853},
abstract = {A Software Product Line (SPL) exploits reuse-in-the-large to enable customization by explicitly modeling commonalities and variabilities of closely related software systems. Delta-Oriented Programming (DOP) is a flexible implementation approach to SPL engineering, which transforms an existing core product to another desired product by applying transformation operations. By capturing product alterations related to configurable functionality within delta modules, DOP closely resembles a natural process of software development, which proves beneficial in early stages of development. However, increasing complexity for a growing SPL in later development stages caused by the invasiveness of DOP drastically impairs maintenance and extensibility. Hence, a process utilizing the invasiveness of DOP in early development stages and restricting it in later stages would allow developers to mature growing delta-oriented SPLs. Moreover, ever-increasing complexity requires means to migrate into less invasive development approaches that are more suited for large-scale configurable applications. To this end, we propose a development process for delta-oriented SPLs including explicit variability points, metrics and refactorings as well as a semi-automatic reengineering of a delta-oriented SPL into a development approach based on blackbox-components. In this paper, we sketch this development process with its constituents and point out required research essential for successfully maturing a delta-oriented SPL.},
booktitle = {Proceedings of the 8th ACM SIGPLAN International Workshop on Feature-Oriented Software Development},
pages = {41–50},
numpages = {10},
keywords = {Delta-Oriented Programming, Software Product Lines},
location = {Vancouver, BC, Canada},
series = {FOSD 2017}
}

@article{10.1145/1183236.1183264,
author = {Batory, Don and Benavides, David and Ruiz-Cortes, Antonio},
title = {Automated analysis of feature models: challenges ahead},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183264},
doi = {10.1145/1183236.1183264},
journal = {Commun. ACM},
month = dec,
pages = {45–47},
numpages = {3}
}

@article{10.1145/1183236.1183239,
author = {Bichler, Martin and Kalagnanam, Jayant R.},
title = {Software frameworks for advanced procurement auction markets},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183239},
doi = {10.1145/1183236.1183239},
abstract = {A range of versatile auction formats are coming that allow more flexibility in specifying demand and supply.},
journal = {Commun. ACM},
month = dec,
pages = {104–108},
numpages = {5}
}

@inproceedings{10.1145/3338906.3338974,
author = {Ne\v{s}i\'{c}, Damir and Kr\"{u}ger, Jacob and St\u{a}nciulescu, undefinedtefan and Berger, Thorsten},
title = {Principles of feature modeling},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338974},
doi = {10.1145/3338906.3338974},
abstract = {Feature models are arguably one of the most intuitive and successful notations for modeling the features of a variant-rich software system. Feature models help developers to keep an overall understanding of the system, and also support scoping, planning, development, variant derivation, configuration, and maintenance activities that sustain the system's long-term success. Unfortunately, feature models are difficult to build and evolve. Features need to be identified, grouped, organized in a hierarchy, and mapped to software assets. Also, dependencies between features need to be declared. While feature models have been the subject of three decades of research, resulting in many feature-modeling notations together with automated analysis and configuration techniques, a generic set of principles for engineering feature models is still missing. It is not even clear whether feature models could be engineered using recurrent principles. Our work shows that such principles in fact exist. We analyzed feature-modeling practices elicited from ten interviews conducted with industrial practitioners and from 31 relevant papers. We synthesized a set of 34 principles covering eight different phases of feature modeling, from planning over model construction, to model maintenance and evolution. Grounded in empirical evidence, these principles provide practical, context-specific advice on how to perform feature modeling, describe what information sources to consider, and highlight common characteristics of feature models. We believe that our principles can support researchers and practitioners enhancing feature-modeling tooling, synthesis, and analyses techniques, as well as scope future research.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {62–73},
numpages = {12},
keywords = {Feature models, modeling principles, software product lines},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3168365.3168374,
author = {Arcaini, Paolo and Gargantini, Angelo and Radavelli, Marco},
title = {An evolutionary process for product-driven updates of feature models},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168374},
doi = {10.1145/3168365.3168374},
abstract = {Feature models are a widely used modeling notation for variability and commonality management in software product line (SPL) engineering. In order to keep an SPL and its feature model aligned, feature models must be changed by including/excluding new features and products, either because faults in the model are found or to reflect the normal evolution of the SPL. The modification of the feature model able to satisfy these change requirements can be complex and error-prone. In this paper, we present a method that is able to automatically update a feature model in order to satisfy a given update request. Our method is based on an evolutionary algorithm and it iteratively applies structure-preserving mutations to the original model, until the model is completely updated. We evaluate the process on real-world feature models. Although our approach does not guarantee to completely update all possible feature models, empirical analysis shows that, on average, more than 80% of requested changes are applied.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {67–74},
numpages = {8},
keywords = {feature models, mutation, search-based software engineering, software product lines},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/1621607.1621633,
author = {Sanen, Frans and Truyen, Eddy and Joosen, Wouter},
title = {Mapping problem-space to solution-space features: a feature interaction approach},
year = {2009},
isbn = {9781605584942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1621607.1621633},
doi = {10.1145/1621607.1621633},
abstract = {Mapping problem-space features into solution-space features is a fundamental configuration problem in software product line engineering. A configuration problem is defined as generating the most optimal combination of software features given a requirements specification and given a set of configuration rules. Current approaches however provide little support for expressing complex configuration rules between problem and solution space that support incomplete requirements specifications. In this paper, we propose an approach to model complex configuration rules based on a generalization of the concept of problem-solution feature interactions. These are interactions between solution-space features that only arise in specific problem contexts. The use of an existing tool to support our approach is also discussed: we use the DLV answer set solver to express a particular configuration problem as a logic program whose answer set corresponds to the optimal combinations of solution-space features. We motivate and illustrate our approach with a case study in the field of managing dynamic adaptations in distributed software, where the goal is to generate an optimal protocol for accommodating a given adaptation.},
booktitle = {Proceedings of the Eighth International Conference on Generative Programming and Component Engineering},
pages = {167–176},
numpages = {10},
keywords = {DLV, configuration knowledge, default logic, distributed runtime adaptation, problem-solution feature interactions, software product line engineering},
location = {Denver, Colorado, USA},
series = {GPCE '09}
}

@inproceedings{10.5555/3400397.3400545,
author = {Poeting, Moritz and Prell, Bastian and Rabe, Markus and Uhlig, Tobias and Wenzel, Sigrid},
title = {Considering energy-related factors in the simulation of logistics systems},
year = {2020},
isbn = {9781728132839},
publisher = {IEEE Press},
abstract = {Traditionally, aspects such as emissions and energy consumption have to be taken into account for environmental and economic reasons when it comes to transport. In other areas of logistics, such as production logistics and intralogistics, the energy aspect is also becoming increasingly important. Existing literature has been recently reviewed in a contribution of the Arbeitsgemeinschaft Simulation (ASIM) to the Winter Simulation Conference 2018 (Uhlig et al. 2018) to develop a map of common approaches and best practices for manufacturing and logistics systems. In the paper presented here, as a complement we are focusing on the application of energy simulation in logistics to give a comprehensive overview and present exemplary case studies. Furthermore, we show a classification of approaches to combine energy aspects with simulation. Finally, we will discuss open questions and future trends in this field of research.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1849–1858},
numpages = {10},
location = {National Harbor, Maryland},
series = {WSC '19}
}

@article{10.1145/3176644,
author = {Xiang, Yi and Zhou, Yuren and Zheng, Zibin and Li, Miqing},
title = {Configuring Software Product Lines by Combining Many-Objective Optimization and SAT Solvers},
year = {2018},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3176644},
doi = {10.1145/3176644},
abstract = {A feature model (FM) is a compact representation of the information of all possible products from software product lines. The optimal feature selection involves the simultaneous optimization of multiple (usually more than three) objectives in a large and highly constrained search space. By combining our previous work on many-objective evolutionary algorithm (i.e., VaEA) with two different satisfiability (SAT) solvers, this article proposes a new approach named SATVaEA for handling the optimal feature selection problem. In SATVaEA, an FM is simplified with the number of both features and constraints being reduced greatly. We enhance the search of VaEA by using two SAT solvers: one is a stochastic local search--based SAT solver that can quickly repair infeasible configurations, whereas the other is a conflict-driven clause-learning SAT solver that is introduced to generate diversified products. We evaluate SATVaEA on 21 FMs with up to 62,482 features, including two models with realistic values for feature attributes. The experimental results are promising, with SATVaEA returning 100% valid products on almost all FMs. For models with more than 10,000 features, the search in SATVaEA takes only a few minutes. Concerning both effectiveness and efficiency, SATVaEA significantly outperforms other state-of-the-art algorithms.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {14},
numpages = {46},
keywords = {Optimal feature selection, many-objective optimization, satisfiability (SAT) solvers, vector angle--based evolutionary algorithm (VaEA)}
}

@inproceedings{10.5555/3320516.3320904,
author = {Wenzel, Sigrid and Peter, Tim and Stoldt, Johannes and Schlegel, Andreas and Uhlig, Tobias and J\'{o}svai, J\'{a}nos},
title = {Considering energy in the simulation of manufacturing systems},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {In recent years, environmental aspects became one of the key interests in manufacturing. Accordingly, simulation studies had to include factors like energy or emissions. This paper aims to provide a comprehensive introduction to the state of the art in modeling of energy and emissions in simulation of manufacturing systems. We review existing literature to develop a landscape of common approaches and best practices. Typical goals and objectives of the reviewed simulation projects are summarized. Furthermore, we will evaluate the structure and life cycle phases of the examined manufacturing systems and look into the requirements and implementation of respective simulation studies. Finally, we will discuss open questions and future trends in this field of research.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {3275–3286},
numpages = {12},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@inproceedings{10.5555/3320516.3320857,
author = {Rabe, Markus and Ammouriova, Majsa and Schmitt, Dominik},
title = {Improving the performance of a logistics assistance system for materials trading networks by grouping similar actions},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {Decision makers (DMs) for logistics networks (LNWs) have the complex task of maintaining their networks in good conditions while internal and external demands are changing. Therefore, the DMs need to identify promising actions in order to adapt to the LNW's changing state, e.g., increasing the stock level of stock keeping units (SKUs). The authors have developed a logistics assistance system (LAS) that automatically alters the LNW's model, for improving it under changing conditions, by applying actions and evaluating their effects on the LNW's performance. Promising actions are suggested to the DM. As the LNW grows in size, the number of potential actions increases and therefore, the response time of the LAS increases as well under the additional computational burden. In this paper, the authors describe a novel concept for reducing the number of actions by grouping similar actions together, leading to faster convergence and shorter response time of the LAS.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {2861–2872},
numpages = {12},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@inproceedings{10.1145/2884781.2884821,
author = {Devroey, Xavier and Perrouin, Gilles and Papadakis, Mike and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Featured model-based mutation analysis},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884821},
doi = {10.1145/2884781.2884821},
abstract = {Model-based mutation analysis is a powerful but expensive testing technique. We tackle its high computation cost by proposing an optimization technique that drastically speeds up the mutant execution process. Central to this approach is the Featured Mutant Model, a modelling framework for mutation analysis inspired by the software product line paradigm. It uses behavioural variability models, viz., Featured Transition Systems, which enable the optimized generation, configuration and execution of mutants. We provide results, based on models with thousands of transitions, suggesting that our technique is fast and scalable. We found that it outperforms previous approaches by several orders of magnitude and that it makes higher-order mutation practically applicable.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {655–666},
numpages = {12},
keywords = {featured transition systems, mutation analysis, variability},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/3132498.3133834,
author = {Santos, Marcelo C. B. and Colanzi, Thelma E. and Amaral, Aline M. M. M. and OliveiraJr, Edson},
title = {Preliminary study on the correlation of objective functions to optimize product-line architectures},
year = {2017},
isbn = {9781450353250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132498.3133834},
doi = {10.1145/3132498.3133834},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line (SPL). The Multi-Objective Approach for PLA Design (MOA4PLA) aims at optimizing the PLA design by using search algorithms easing the design activity. From an original PLA, MOA4PLA automatically obtains alternative designs to improve the original one in terms of the objectives selected for optimization. The use of search algorithms is an incipient research topic, which includes several open research questions. The evaluation model of MOA4PLA is composed of various objective functions, which use software metrics to evaluate different factors that influence on the PLA design. However, the simultaneous optimization of all objective functions is a computationally complex task. In this sense, it is worthwhile to investigate the possible correlation between objective functions because the discovery of correlated functions allows to reduce the number of objectives to be optimized by the search algorithm. Hence, in this paper we perform a preliminary study to investigate the correlation among five objective functions related to metrics that provide indicators on conventional architectural properties, such as coupling, cohesion and size. To accomplish the objective of this paper, four controlled experiments were carried out with four different PLA designs. Empirical results provide preliminary evidence that two pairs of functions are positively correlated and two other pairs of functions are negatively correlated. From such findings, several guidelines were derived to help architects to both reduce and select the objectives related to conventional architectural properties to be tackled during the PLA design optimization.},
booktitle = {Proceedings of the 11th Brazilian Symposium on Software Components, Architectures, and Reuse},
articleno = {11},
numpages = {10},
keywords = {correlation study, product-line architecture, search-based software engineering},
location = {Fortaleza, Cear\'{a}, Brazil},
series = {SBCARS '17}
}

@inproceedings{10.1007/978-3-642-36757-1_11,
author = {Zhang, Xiaorui and M\o{}ller-Pedersen, Birger},
title = {Towards correct product derivation in model-driven product lines},
year = {2012},
isbn = {9783642367564},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-36757-1_11},
doi = {10.1007/978-3-642-36757-1_11},
abstract = {In a product line model, the product line developer often specifies not only high-level domain features but also their low-level realization steps. We see two challenges against deriving and intended products with respect to the specification of feature realizations:1 The developer is not provided with immediate feedback on the realization steps at design time.2 How to ensure that the realization steps are consistent with high-level features.The Common Variability Language (CVL) is a generic language for modeling variability and the CVL tool can be used for product line development. We propose two extensions to the CVL tool to address the aforementioned challenges:1 A simulator that simulates the feature realizations and visualizes the resulting product model at design time.2 A consistency checker that checks if the realizations are consistent with high-level features.We illustrate these two added procedures by applying them to the development of a train control product line. A tool prototype is implemented and used for evaluation.},
booktitle = {Proceedings of the 7th International Conference on System Analysis and Modeling: Theory and Practice},
pages = {179–197},
numpages = {19},
keywords = {common variability language, model-driven software product line, product derivation},
location = {Innsbruck, Austria},
series = {SAM'12}
}

@inproceedings{10.1145/3377024.3377031,
author = {El-Sharkawy, Sascha and Krafczyk, Adam and Schmid, Klaus},
title = {Fast static analyses of software product lines: an example with more than 42,000 metrics},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377031},
doi = {10.1145/3377024.3377031},
abstract = {Context: Software metrics, as one form of static analyses, is a commonly used approach in software engineering in order to understand the state of a software system, in particular to identify potential areas prone to defects. Family-based techniques extract variability information from code artifacts in Software Product Lines (SPLs) to perform static analysis for all available variants. Many different types of metrics with numerous variants have been defined in literature. When counting all metrics including such variants, easily thousands of metrics can be defined. Computing all of them for large product lines can be an extremely expensive process in terms of performance and resource consumption.Objective: We address these performance and resource challenges while supporting customizable metric suites, which allow running both, single system and variability-aware code metrics.Method: In this paper, we introduce a partial parsing approach used for the efficient measurement of more than 42,000 code metric variations. The approach covers variability information and restricts parsing to the relevant parts of the Abstract Syntax Tree (AST).Conclusions: This partial parsing approach is designed to cover all relevant information to compute a broad variety of variability-aware code metrics on code artifacts containing annotation-based variability, e.g., realized with C-preprocessor statements. It allows for the flexible combination of single system and variability-aware metrics, which is not supported by existing tools. This is achieved by a novel representation of partially parsed product line code artifacts, which is tailored to the computation of the metrics. Our approach consumes considerably less resources, especially when computing many metric variants in parallel.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {8},
numpages = {9},
keywords = {AST, SPL, abstract syntax trees, feature models, implementation, metrics, software product lines, variability models},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@article{10.1145/3204459,
author = {Chen, Tao and Li, Ke and Bahsoon, Rami and Yao, Xin},
title = {FEMOSAA: Feature-Guided and Knee-Driven Multi-Objective Optimization for Self-Adaptive Software},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3204459},
doi = {10.1145/3204459},
abstract = {Self-Adaptive Software (SAS) can reconfigure itself to adapt to the changing environment at runtime, aiming to continually optimize conflicted nonfunctional objectives (e.g., response time, energy consumption, throughput, cost, etc.). In this article, we present Feature-guided and knEe-driven Multi-Objective optimization for Self-Adaptive softwAre (FEMOSAA), a novel framework that automatically synergizes the feature model and Multi-Objective Evolutionary Algorithm (MOEA) to optimize SAS at runtime. FEMOSAA operates in two phases: at design time, FEMOSAA automatically transposes the engineers’ design of SAS, expressed as a feature model, to fit the MOEA, creating new chromosome representation and reproduction operators. At runtime, FEMOSAA utilizes the feature model as domain knowledge to guide the search and further extend the MOEA, providing a larger chance for finding better solutions. In addition, we have designed a new method to search for the knee solutions, which can achieve a balanced tradeoff. We comprehensively evaluated FEMOSAA on two running SAS: One is a highly complex SAS with various adaptable real-world software under the realistic workload trace; another is a service-oriented SAS that can be dynamically composed from services. In particular, we compared the effectiveness and overhead of FEMOSAA against four of its variants and three other search-based frameworks for SAS under various scenarios, including three commonly applied MOEAs, two workload patterns, and diverse conflicting quality objectives. The results reveal the effectiveness of FEMOSAA and its superiority over the others with high statistical significance and nontrivial effect sizes.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {5},
numpages = {50},
keywords = {Feature model, multi-objective evolutionary algorithm, multi-objective optimization, performance engineering, search-based software engineering, self-adaptive system}
}

@inproceedings{10.5555/3172795.3172831,
author = {Masri, Samer Al and Bhuiyan, Nazim Uddin and Nadi, Sarah and Gaudet, Matthew},
title = {Software variability through C++ static polymorphism: a case study of challenges and open problems in eclipse OMR},
year = {2017},
publisher = {IBM Corp.},
address = {USA},
abstract = {Software Product Line Engineering (SPLE) creates configurable platforms that can be used to efficiently produce similar, and yet different, product variants. SPLs are typically modular such that it is easy to connect different blocks of code together, creating different variations of the product. There are many variability implementation mechanisms to achieve an SPL. This paper shows how static polymorphism can be used to implement variability, through a case study of IBM's open-source Eclipse OMR project. We discuss the current open problems and challenges this variability implementation mechanism raises and highlight technology gaps for reasoning about variability in OMR. We then suggest steps to close these gaps.},
booktitle = {Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering},
pages = {285–291},
numpages = {7},
location = {Markham, Ontario, Canada},
series = {CASCON '17}
}

@inproceedings{10.1109/ASE.2015.16,
author = {Kowal, Matthias and Tschaikowski, Max and Tribastone, Mirco and Schaefer, Ina},
title = {Scaling size and parameter spaces in variability-aware software performance models},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.16},
doi = {10.1109/ASE.2015.16},
abstract = {In software performance engineering, what-if scenarios, architecture optimization, capacity planning, run-time adaptation, and uncertainty management of realistic models typically require the evaluation of many instances. Effective analysis is however hindered by two orthogonal sources of complexity. The first is the infamous problem of state space explosion---the analysis of a single model becomes intractable with its size. The second is due to massive parameter spaces to be explored, but such that computations cannot be reused across model instances. In this paper, we efficiently analyze many queuing models with the distinctive feature of more accurately capturing variability and uncertainty of execution rates by incorporating general (i.e., non-exponential) distributions. Applying product-line engineering methods, we consider a family of models generated by a core that evolves into concrete instances by applying simple delta operations affecting both the topology and the model's parameters. State explosion is tackled by turning to a scalable approximation based on ordinary differential equations. The entire model space is analyzed in a family-based fashion, i.e., at once using an efficient symbolic solution of a super-model that subsumes every concrete instance. Extensive numerical tests show that this is orders of magnitude faster than a naive instance-by-instance analysis.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {407–417},
numpages = {11},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1145/2851613.2851959,
author = {Noorian, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Quality-centric feature model configuration using goal models},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851959},
doi = {10.1145/2851613.2851959},
abstract = {In software product line engineering, a feature model represents the possible configuration space and can be customized based on the stakeholders' needs. Considering the complexity of feature models in addition to the diversity of the stake-holders' expectations, the configuration process is viewed as a complex optimization problem. In this paper, we propose a holistic approach for the configuration process that seeks to satisfy the stakeholders' requirements as well as the feature models' structural and integrity constraints. Here, we model stakeholders' functional and non-functional needs and their preferences using requirement engineering goal models. We formalize the structure of the feature model, the stake-holders' objectives, and their preferences in the form of an integer linear program to automatically perform feature selection.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1296–1299},
numpages = {4},
keywords = {configuration process, feature model, goal model},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1109/ICSE.2019.00113,
author = {Ha, Huong and Zhang, Hongyu},
title = {DeepPerf: performance prediction for configurable software with deep sparse neural network},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00113},
doi = {10.1109/ICSE.2019.00113},
abstract = {Many software systems provide users with a set of configuration options and different configurations may lead to different runtime performance of the system. As the combination of configurations could be exponential, it is difficult to exhaustively deploy and measure system performance under all possible configurations. Recently, several learning methods have been proposed to build a performance prediction model based on performance data collected from a small sample of configurations, and then use the model to predict system performance under a new configuration. In this paper, we propose a novel approach to model highly configurable software system using a deep feedforward neural network (FNN) combined with a sparsity regularization technique, e.g. the L1 regularization. Besides, we also design a practical search strategy for automatically tuning the network hyperparameters efficiently. Our method, called DeepPerf, can predict performance values of highly configurable software systems with binary and/or numeric configuration options at much higher prediction accuracy with less training data than the state-of-the art approaches. Experimental results on eleven public real-world datasets confirm the effectiveness of our approach.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1095–1106},
numpages = {12},
keywords = {deep sparse feedforward neural network, highly configurable systems, software performance prediction, sparsity regularization},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/2935323.2935326,
author = {Henriksen, Troels and Larsen, Ken Friis and Oancea, Cosmin E.},
title = {Design and GPGPU performance of Futhark's redomap construct},
year = {2016},
isbn = {9781450343848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935323.2935326},
doi = {10.1145/2935323.2935326},
abstract = {This paper presents and evaluates a novel second-order operator, named 'redomap', that stems from 'map'-'reduce' compositions in the context of the purely-functional array language Futhark, which is aimed at efficient GPGPU execution. Main contributions are: First, we demonstrate an aggressive fusion technique that is centered on the 'redomap' operator. Second, we present a compilation technique for 'redomap' that efficiently sequentializes the excess parallelism and ensures coalesced access to global memory, even for non-commutative 'reduce' operators. Third, a detailed performance evaluation shows that Futhark's automatically generated code matches or exceeds performance of hand-tuned Thrust code. Our evaluation infrastructure is publicly available and we encourage replication and verification of our results.},
booktitle = {Proceedings of the 3rd ACM SIGPLAN International Workshop on Libraries, Languages, and Compilers for Array Programming},
pages = {17–24},
numpages = {8},
keywords = {GPGPU, autoparallelization, functional language, map-reduce},
location = {Santa Barbara, CA, USA},
series = {ARRAY 2016}
}

@inproceedings{10.5555/3106050.3106059,
author = {Al-Hajjaji, Mustafa and Lity, Sascha and Lachmann, Remo and Th\"{u}m, Thomas and Schaefer, Ina and Saake, Gunter},
title = {Delta-oriented product prioritization for similarity-based product-line testing},
year = {2017},
isbn = {9781538628034},
publisher = {IEEE Press},
abstract = {Testing every product of a software product line (SPL) is often not feasible due to the exponential number of products in the number of features. Thus, the order in which products are tested matters, because it can increase the early rate of fault detection. Several approaches have been proposed to prioritize products based on configuration similarity. However, current approaches are oblivious to solution-space differences among products, because they consider only problem-space information. With delta modeling, we incorporate solution-space information in product prioritization to improve the effectiveness of SPL testing. Deltas capture the differences between products facilitating the reasoning about product similarity. As a result, we select the most dissimilar product to the previously tested ones, in terms of deltas, to be tested next. We evaluate the effectiveness of our approach using an SPL from the automotive domain showing an improvement in the effectiveness of SPL testing.},
booktitle = {Proceedings of the 2nd International Workshop on Variability and Complexity in Software Design},
pages = {34–40},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {VACE '17}
}

@inproceedings{10.5555/2820656.2820662,
author = {Chitchyan, Ruzanna and Noppen, Joost and Groher, Iris},
title = {What can software engineering do for sustainability: case of software product lines},
year = {2015},
publisher = {IEEE Press},
abstract = {Sustainable living, i.e., living within the bounds of the available environmental, social, and economic resources, is the focus of many present-day social and scientific discussions. But what does sustainability mean within the context of Software Product Line Engineering (SPLE)? And what does SPLE do for sustainable living? In this paper we take the first step towards identification of the sustainability-related characteristics relevant to SPLE. The paper also discusses how the key areas of interest to the current SPL community (as reflected by what is measured and optimised in SPLs today) relate to these sustainability characteristics.},
booktitle = {Proceedings of the Fifth International Workshop on Product LinE Approaches in Software Engineering},
pages = {11–14},
numpages = {4},
location = {Florence, Italy},
series = {PLEASE '15}
}

@inproceedings{10.1145/3368826.3377923,
author = {Shaikhha, Amir and Schleich, Maximilian and Ghita, Alexandru and Olteanu, Dan},
title = {Multi-layer optimizations for end-to-end data analytics},
year = {2020},
isbn = {9781450370479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368826.3377923},
doi = {10.1145/3368826.3377923},
abstract = {We consider the problem of training machine learning models over multi-relational data. The mainstream approach is to first construct the training dataset using a feature extraction query over input database and then use a statistical software package of choice to train the model. In this paper we introduce Iterative Functional Aggregate Queries (IFAQ), a framework that realizes an alternative approach. IFAQ treats the feature extraction query and the learning task as one program given in the IFAQ's domain-specific language, which captures a subset of Python commonly used in Jupyter notebooks for rapid prototyping of machine learning applications. The program is subject to several layers of IFAQ optimizations, such as algebraic transformations, loop transformations, schema specialization, data layout optimizations, and finally compilation into efficient low-level C++ code specialized for the given workload and data.  We show that a Scala implementation of IFAQ can outperform mlpack, Scikit, and TensorFlow by several orders of magnitude for linear regression and regression tree models over several relational datasets.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {145–157},
numpages = {13},
keywords = {In-Database Machine Learning, Multi-Query Optimization, Query Compilation},
location = {San Diego, CA, USA},
series = {CGO '20}
}

@inproceedings{10.1145/3603287.3651199,
author = {Shatnawi, Hazim and Saquer, Jamil},
title = {Encoding Feature Models in Neo4j Graph Database},
year = {2024},
isbn = {9798400702372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603287.3651199},
doi = {10.1145/3603287.3651199},
abstract = {This study introduces an innovative approach to encoding and analyzing feature models within the Network Exploration and Optimization for Java (Neo4j) graph database, significantly enhancing the management of complex Software Product Lines (SPLs). We present a comparative analysis of traditional loading techniques against Neo4j's batch importer and the Awesome Procedures on Cypher (APOC) library, demonstrating the superior efficiency and effectiveness of our proposed methods, especially in handling large datasets. Our methodology extends beyond mere encoding; it capitalizes on Neo4j's Graph Data Science (GDS) library, employing Depth-First Search (DFS) and other advanced traversal techniques to navigate and manipulate these complex structures. The findings reveal not only a significant enhancement in the processing and analysis of feature models but also underscore the potential for more sophisticated SPL management strategies. By integrating innovative loading techniques, encoding strategies, and GDS traversal methods, this study lays a robust foundation for future advancements in the field.},
booktitle = {Proceedings of the 2024 ACM Southeast Conference},
pages = {157–166},
numpages = {10},
keywords = {Cypher, data science, feature model, graph data science library, graph traversals, load data in Neo4j, performance measurement},
location = {Marietta, GA, USA},
series = {ACMSE '24}
}

@inproceedings{10.1145/1147249.1147252,
author = {Kolb, Ronny and Muthig, Dirk},
title = {Making testing product lines more efficient by improving the testability of product line architectures},
year = {2006},
isbn = {1595934596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1147249.1147252},
doi = {10.1145/1147249.1147252},
abstract = {Product line engineering is a recent approach to software development that has shown to enable organizations to achieve significant reductions in development and maintenance cost as well as time-to-market of increasingly complex software systems. Yet, the testing process has not kept up with these reductions and the relative cost for testing product lines is actually becoming higher than in traditional single system development. Also, testing often cannot keep pace with accelerated development in product line engineering due to technical and organizational issues. This paper advocates that testing of product lines can be made more efficient and effective by considering testability already during architectural design. It explores the relationship between testability and product line architecture and discusses the importance of high testability for reducing product line testing effort and achieving required coverage criteria. The paper also outlines a systematic approach that will support product line organizations in improving and evaluating testability of product lines at the architectural level.},
booktitle = {Proceedings of the ISSTA 2006 Workshop on Role of Software Architecture for Testing and Analysis},
pages = {22–27},
numpages = {6},
keywords = {architecture, design, evaluation, software product line, testability, testing},
location = {Portland, Maine},
series = {ROSATEA '06}
}

@inproceedings{10.1145/1385486.1385488,
author = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Schirmeier, Horst and Sincero, Julio and Apel, Sven and Leich, Thomas and Spinczyk, Olaf and Saake, Gunter},
title = {FAME-DBMS: tailor-made data management solutions for embedded systems},
year = {2008},
isbn = {9781595939647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1385486.1385488},
doi = {10.1145/1385486.1385488},
abstract = {Data management functionality is not only needed in large-scale server systems, but also in embedded systems. Resource restrictions and heterogeneity of hardware, however, complicate the development of data management solutions for those systems. In current practice, this typically leads to the redevelopment of data management because existing solutions cannot be reused and adapted appropriately. In this paper, we present our ongoing work on FAME-DBMS, a research project that explores techniques to implement highly customizable data management solutions, and illustrate how such systems can be created with a software product line approach. With this approach a concrete instance of a DBMS is derived by composing features of the DBMS product line that are needed for a specific application scenario. This product derivation process is getting complex if a large number of features is available. Furthermore, in embedded systems also non-functional properties, e.g., memory consumption, have to be considered when creating a DBMS instance. To simplify the derivation process we present approaches for its automation.},
booktitle = {Proceedings of the 2008 EDBT Workshop on Software Engineering for Tailor-Made Data Management},
pages = {1–6},
numpages = {6},
location = {Nantes, France},
series = {SETMDM '08}
}

@inproceedings{10.5555/2819009.2819244,
author = {Rubin, Julia and Botterweck, Goetz and Pleuss, Andreas and Weiss, David},
title = {5th international workshop on product line approaches in software engineering PLE for a sustainable society (PLEASE 2015)},
year = {2015},
publisher = {IEEE Press},
abstract = {This paper summarizes the motivation, objectives, and format of the 5th International Workshop on Product LinE Approaches in Software Engineering (PLEASE15). The main goal of the PLEASE workshop series is to encourage and promote the adoption of Software Product Line Engineering. This year's edition focuses on the link between software product line engineering (SPLE) and new challenges posed by emerging societal trends. Towards this end, we invited reports on (1) opportunities posed by societal challenges for SPLE research and practice and (2) concrete solutions exemplifying application of SPLE techniques to societal challenges.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {989–990},
numpages = {2},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/3669721.3674529,
author = {Shao, Mengxue and Lu, Yang and Xu, Xice},
title = {Noise Abatement Flight Procedure Design for Helicopter Approach based on Acoustic Mode},
year = {2024},
isbn = {9798400710025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3669721.3674529},
doi = {10.1145/3669721.3674529},
abstract = {For the problem of noise abatement flight procedure, a method for predicting helicopter approach noise footprints based on acoustic mode is proposed. A segmented approach procedure for noise reduction has been studied. The core steps of the method include the construction of the acoustic modal coefficient database and the prediction of noise footprints in the acoustic modal domain. First, derive the acoustic modal expansion model of rotor noise, mapping the time-domain sound pressure on the holographic surface to the acoustic modes. In order to make a quick call, a database with the advance ratio and the tip-path-plane angle as the characteristic parameters is constructed. Furthermore, the acoustic modal coefficients corresponding to the flight parameters in the discrete trajectory elements are estimated from the database. Predict the noise footprint of the approach in the acoustic modal domain. The AS350 helicopter was used as the object for simulation analysis. Simulation results show that: Under steady-state conditions, the flight procedure with advance ratio and/or flight path angle segments can reduce the approach noise footprint level by up to 13 dB. The method proposed in this paper has important potential in noise abatement flight procedure design, and the feasibility of reducing noise in the target area through flight optimization is preliminarily verified.},
booktitle = {Proceedings of the 2024 3rd International Symposium on Intelligent Unmanned Systems and Artificial Intelligence},
pages = {165–170},
numpages = {6},
keywords = {Acoustic mode, Helicopter, Near-field acoustic holography, Noise footprint, noise abatement procedure design},
location = {Qingdao, China},
series = {SIUSAI '24}
}

@inproceedings{10.1145/2790282.2790292,
author = {Meng, Lingchuan and Johnson, Jeremy},
title = {High performance implementation of the inverse TFT},
year = {2015},
isbn = {9781450335997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2790282.2790292},
doi = {10.1145/2790282.2790292},
abstract = {The inverse truncated Fourier transform (ITFT) is a key component in the fast polynomial and large integer algorithms introduced by van der Hoeven. This paper reports a high performance implementation of the ITFT which poses additional challenges compared to that of the forward transform. A general-radix variant of the ITFT algorithm is developed to allow the implementation to automatically adapt to the memory hierarchy. Then a parallel ITFT algorithm is developed that trades off small arithmetic cost for full vectorization and improved multi-threaded parallelism. The algorithms are automatically generated and tuned to produce an arbitrary-size ITFT library. The new algorithms and the implementation smooths out the staircase performance associated with power-of-two modular FFT implementations, and provide significant performance improvement over zero-padding approaches even when high-performance FFT libraries are used.},
booktitle = {Proceedings of the 2015 International Workshop on Parallel Symbolic Computation},
pages = {87–94},
numpages = {8},
location = {Bath, United Kingdom},
series = {PASCO '15}
}

@inproceedings{10.5555/3466184.3466321,
author = {Rabe, Markus and Chicaiza-Vaca, Jorge and Tordecilla, Rafael D. and Juan, Angel A.},
title = {A simulation-optimization approach for locating automated parcel lockers in urban logistics operations},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Experts propose using an automated parcel locker (APL) for improving urban logistics operations. However, deciding the location of these APLs is not a trivial task, especially when considering a multi-period horizon under uncertainty. Based on a case study developed in Dortmund, Germany, we propose a simulation-optimization approach that integrates a system dynamics simulation model with a multi-period capacitated facility location problem (CFLP). First, we built the causal-loop and stock-flow diagrams to show the APL system's main components and interdependencies. Then, we formulated a multi-period CFLP model to provide the optimal number of APLs to be installed in each period. Finally, Monte Carlo simulation was used to estimate the cost and reliability level for different scenarios with random demands. In our experiments, only one solution reaches a 100% reliability level, with a total cost of 2.7 million euros. Nevertheless, if the budget is lower, our approach offers other good alternatives.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1230–1241},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@inproceedings{10.1145/3251104,
author = {Langdon, William B. and Petke, Justyna and White, David R.},
title = {Session details: Genetic Improvement 2015 Workshop},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251104},
doi = {10.1145/3251104},
abstract = {It is our great pleasure to welcome you to the first international workshop on the Genetic Improvement of Software -- GI-2015, held at GECCO'15. Our goal was to bring together research from across the globe to exchange ideas on using optimisation techniques, particularly evolutionary computation such as genetic programming, to improve existing software. We invited short position papers to encourage the discussion of new ideas and recent work in addition to longer and more concrete submissions. The call for participation invited GI work on automatic bug-fixing; improving functionality; improving non-functional properties such as efficiency, memory and energy consumption; "plastic surgery" by transplanting functionality from other existing code to host software; and automatically specialising generic software for dedicated tasks. As you will see, we have accepted papers in most of these areas as well as papers on improving the nascent genetic improvement tools in use, improving parallel code, GI's relationship with software product lines (SPL), improving security and GI for embedded systems.We had submissions from Asia, Europe and both North and South America. They were exactly evenly split between full-length submissions (8) and two page position papers (8).},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@inproceedings{10.5555/2337223.2337243,
author = {Siegmund, Norbert and Kolesnikov, Sergiy S. and K\"{a}stner, Christian and Apel, Sven and Batory, Don and Rosenm\"{u}ller, Marko and Saake, Gunter},
title = {Predicting performance via automated feature-interaction detection},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Customizable programs and program families provide user-selectable features to allow users to tailor a program to an application scenario. Knowing in advance which feature selection yields the best performance is difficult because a direct measurement of all possible feature combinations is infeasible. Our work aims at predicting program performance based on selected features. However, when features interact, accurate predictions are challenging. An interaction occurs when a particular feature combination has an unexpected influence on performance. We present a method that automatically detects performance-relevant feature interactions to improve prediction accuracy. To this end, we propose three heuristics to reduce the number of measurements required to detect interactions. Our evaluation consists of six real-world case studies from varying domains (e.g., databases, encoding libraries, and web servers) using different configuration techniques (e.g., configuration files and preprocessor flags). Results show an average prediction accuracy of 95%.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {167–177},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1145/3302333.3302340,
author = {Krieter, Sebastian and Thiem, Tobias and Leich, Thomas},
title = {Using Dynamic Software Product Lines to Implement Adaptive SGX-enabled Systems},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302340},
doi = {10.1145/3302333.3302340},
abstract = {In the light of computational outsourcing and external data storage, data protection and trusted execution become increasingly important. Novel hardware such as Intel's Software Guard extensions (SGX) attempts to provide a solution to protect data and computations from unauthorized access and manipulation, even against attackers with physical access to a machine. However, the current generation of SGX limits the protected memory space that can be efficiently used to 128 MiB, which must be shared between data and binary code. Thus, we propose to use a software product line approach to tailor an application's binary code in such a way that it can be updated during runtime, with the goal to only store relevant features in the protected memory at a given time. We provide a prototypical implementation that enables basic support for loading and unloading features during runtime and evaluate our prototype in terms of execution times against non-adaptive execution.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {9},
numpages = {9},
keywords = {Intel Software Guard Extensions, Runtime Adaptation, Software Product Lines},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@article{10.1109/TASLP.2018.2889927,
author = {Zhu, Qiaoxi and Coleman, Philip and Qiu, Xiaojun and Wu, Ming and Yang, Jun and Burnett, Ian},
title = {Robust Personal Audio Geometry Optimization in the SVD-Based Modal Domain},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2889927},
doi = {10.1109/TASLP.2018.2889927},
abstract = {Personal audio generates sound zones in a shared space to provide private and personalized listening experiences with minimized interference between consumers. Regularization has been commonly used to increase the robustness of such systems against potential perturbations in the sound reproduction. However, the performance is limited by the system geometry such as the number and location of the loudspeakers and controlled zones. This paper proposes a geometry optimization method to find the most geometrically robust approach for personal audio amongst all available candidate system placements. The proposed method aims to approach the most “natural” sound reproduction so that the solo control of the listening zone coincidently accompanies the preferred quiet zone. Being formulated in the SVD-based modal domain, the method is demonstrated by applications in three typical personal audio optimizations, i.e., the acoustic contrast control, the pressure matching, and the planarity control. Simulation results show that the proposed method can obtain the system geometry with better avoidance of “occlusion,” improved robustness to regularization, and improved broadband equalization.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {610–620},
numpages = {11}
}

@inproceedings{10.1145/378795.378860,
author = {Xiong, Jianxin and Johnson, Jeremy and Johnson, Robert and Padua, David},
title = {SPL: a language and compiler for DSP algorithms},
year = {2001},
isbn = {1581134142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/378795.378860},
doi = {10.1145/378795.378860},
abstract = {We discuss the design and implementation of a compiler that translates formulas representing signal processing transforms into efficient C or Fortran programs. The formulas are represented in a language that we call SPL, an acronym from Signal Processing Language. The compiler is a component of the SPIRAL system which makes use of formula transformations and intelligent search strategies to automatically generate optimized digital signal processing (DSP) libraries. After a discussion of the translation and optimization techniques implemented in the compiler, we use SPL formulations of the fast Fourier transform (FFT) to evaluate the compiler. Our results show that SPIRAL, which can be used to implement many classes of algorithms, produces programs that perform as well as “hard-wired” systems like FFTW.},
booktitle = {Proceedings of the ACM SIGPLAN 2001 Conference on Programming Language Design and Implementation},
pages = {298–308},
numpages = {11},
location = {Snowbird, Utah, USA},
series = {PLDI '01}
}

@inproceedings{10.1145/3194133.3194143,
author = {Olaechea, Rafael and Atlee, Joanne and Legay, Axel and Fahrenberg, Uli},
title = {Trace checking for dynamic software product lines},
year = {2018},
isbn = {9781450357159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194133.3194143},
doi = {10.1145/3194133.3194143},
abstract = {A key objective of self-adaptive systems is to continue to provide optimal quality of service when the environment changes. A dynamic software product line (DSPL) can benefit from knowing how its various product variants would have performed (in terms of quality of service) with respect to the recent history of inputs. We propose a family-based analysis that simulates all the product variants of a DSPL simultaneously, at runtime, on recent environmental inputs to obtain an estimate of the quality of service that each one of the product variants would have had, provided it had been executing. We assessed the efficiency of our DSPL analysis compared to the efficiency of analyzing each product individually on three case studies. We obtained mixed results due to the explosion of quality-of-service values for the product variants of a DSPL. After introducing a simple data abstraction on the values of quality-of- service variables, our DSPL analysis is between 1.4 and 7.7 times faster than analyzing the products one at a time.},
booktitle = {Proceedings of the 13th International Conference on Software Engineering for Adaptive and Self-Managing Systems},
pages = {69–75},
numpages = {7},
location = {Gothenburg, Sweden},
series = {SEAMS '18}
}

@inproceedings{10.1109/ICSE.2017.58,
author = {Behringer, Benjamin and Palz, Jochen and Berger, Thorsten},
title = {PEoPL: projectional editing of product lines},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.58},
doi = {10.1109/ICSE.2017.58},
abstract = {The features of a software product line---a portfolio of system variants---can be realized using various implementation techniques (a.k.a., variability mechanisms). Each technique represents the software artifacts of features differently, typically classified into annotative (e.g., C preprocessor) and modular representations (e.g., feature modules), each with distinct advantages and disadvantages. Annotative representations are easy to realize, but annotations clutter source code and hinder program comprehension. Modular representations support comprehension, but are difficult to realize. Most importantly, to engineer feature artifacts, developers need to choose one representation and adhere to it for evolving and maintaining the same artifacts.We present PEoPL, an approach to combine the advantages of annotative and modular representations. When engineering a feature artifact, developers can choose the most-suited representation and even use different representations in parallel. PEoPL relies on separating a product line into an internal and external representation, the latter by providing editable projections used by the developers. We contribute a programming-language-independent internal representation of variability, five editable projections reflecting different variability representations, a supporting IDE, and a tailoring of PEoPL to Java. We evaluate PEoPL's expressiveness, scalability, and flexibility in eight Java-based product lines, finding that all can be realized, that projections are feasible, and that variant computation is fast (&lt;45ms on average for our largest subject Berkeley DB).},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {563–574},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@inproceedings{10.5555/2820518.2820528,
author = {Moura, Irineu and Pinto, Gustavo and Ebert, Felipe and Castor, Fernando},
title = {Mining energy-aware commits},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Over the last years, energy consumption has become a first-class citizen in software development practice. While energy-efficient solutions on lower-level layers of the software stack are well-established, there is convincing evidence that even better results can be achieved by encouraging practitioners to participate in the process. For instance, previous work has shown that using a newer version of a concurrent data structure can yield a 2.19x energy savings when compared to the old associative implementation [75]. Nonetheless, little is known about how much software engineers are employing energy-efficient solutions in their applications and what solutions they employ for improving energy-efficiency. In this paper we present a qualitative study of "energy-aware commits". Using Github as our primary data source, we perform a thorough analysis on an initial sample of 2,189 commits and carefully curate a set of 371 energy-aware commits spread over 317 real-world non-trivial applications. Our study reveals that software developers heavily rely on low-level energy management approaches, such as frequency scaling and multiple levels of idleness. Also, our findings suggest that ill-chosen energy saving techniques can impact the correctness of an application. Yet, we found what we call "energy-aware interfaces", which are means for clients (e.g., developers or end-users) to save energy in their applications just by using a function, abstracting away the low-level implementation details.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {56–67},
numpages = {12},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.5555/2818754.2818819,
author = {Henard, Christopher and Papadakis, Mike and Harman, Mark and Le Traon, Yves},
title = {Combining multi-objective search and constraint solving for configuring large software product lines},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Software Product Line (SPL) feature selection involves the optimization of multiple objectives in a large and highly constrained search space. We introduce SATIBEA, that augments multi-objective search-based optimization with constraint solving to address this problem, evaluating it on five large real-world SPLs, ranging from 1,244 to 6,888 features with respect to three different solution quality indicators and two diversity metrics. The results indicate that SATIBEA statistically significantly outperforms the current state-of-the-art (p &lt; 0.01) for all five SPLs on all three quality indicators and with maximal effect size (\^{A}12 = 1.0). We also present results that demonstrate the importance of combining constraint solving with search-based optimization and the significant improvement SATIBEA produces over pure constraint solving. Finally, we demonstrate the scalability of SATIBEA: within less than half an hour, it finds thousands of constraint-satisfying optimized software products, even for the largest SPL considered in the literature to date.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {517–528},
numpages = {12},
location = {Florence, Italy},
series = {ICSE '15}
}

@article{10.1145/3428225,
author = {Shahin, Ramy and Chechik, Marsha},
title = {Automatic and efficient variability-aware lifting of functional programs},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428225},
doi = {10.1145/3428225},
abstract = {A software analysis is a computer program that takes some representation of a software product as input and produces some useful information about that product as output. A software product line encompasses many software product variants, and thus existing analyses can be applied to each of the product variations individually, but not to the entire product line as a whole. Enumerating all product variants and analyzing them one by one is usually intractable due to the combinatorial explosion of the number of product variants with respect to product line features. Several software analyses (e.g., type checkers, model checkers, data flow analyses) have been redesigned/re-implemented to support variability. This usually requires a lot of time and effort, and the variability-aware version of the analysis might have new errors/bugs that do not exist in the original one. Given an analysis program written in a functional language based on PCF, in this paper we present two approaches to transforming (lifting) it into a semantically equivalent variability-aware analysis. A light-weight approach (referred to as shallow lifting) wraps the analysis program into a variability-aware version, exploring all combinations of its input arguments. Deep lifting, on the other hand, is a program rewriting mechanism where the syntactic constructs of the input program are rewritten into their variability-aware counterparts. Compositionally this results in an efficient program semantically equivalent to the input program, modulo variability. We present the correctness criteria for functional program lifting, together with correctness proof sketches of shallow lifting. We evaluate our approach on a set of program analyses applied to the BusyBox C-language product line.},
journal = {Proc. ACM Program. Lang.},
month = nov,
articleno = {157},
numpages = {27},
keywords = {Lifting, PCF, Program Rewriting, Software Product Lines, Variability-aware Programming}
}

@article{10.1145/2382756.2382783,
author = {do Carmo Machado, Ivan and McGregor, John D. and Santana de Almeida, Eduardo},
title = {Strategies for testing products in software product lines},
year = {2012},
issue_date = {November 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/2382756.2382783},
doi = {10.1145/2382756.2382783},
abstract = {The software product line engineering strategy enables the achievement of significant improvements in quality through reuse of carefully crafted software assets across multiple products. However, high levels of quality in the software product line assets, which are used to create products, must be accompanied by effective and efficient test strategies for the products in the software product line. The goal of this study is to understand which strategies for testing products in software product lines have been reported in the literature, enabling discussions on the significant issues, and also pointing out further research directions. A systematic literature review was carried out that identified two hundred seventy-three papers, published from the years 1998 and early in 2012. From such a set of papers, a systematic selection resulted in forty-one relevant papers. The analysis of the reported strategies comprised two important aspects: the selection of products for testing, and the actual test of products. The findings showed a range of strategies, dealing with both aspects, but few empirical evaluations of their effectiveness have been performed, which limits the inferences that can be drawn.},
journal = {SIGSOFT Softw. Eng. Notes},
month = nov,
pages = {1–8},
numpages = {8},
keywords = {software product lines, software testing, systematic review}
}

@proceedings{10.1145/3571788,
title = {VaMoS '23: Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Odense, Denmark}
}

@inproceedings{10.1145/2973839.2973842,
author = {Lima, Crescencio and Chavez, Christina},
title = {A Systematic Review on Metamodels to Support Product Line Architecture Design},
year = {2016},
isbn = {9781450342018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2973839.2973842},
doi = {10.1145/2973839.2973842},
abstract = {Product Line Architecture (PLA) design is a key activity for developing successful Software Product Line (SPL) projects. PLA design is a difficult task, mostly due to the complexity of the software systems that SPLs deal with, and their variabilities. Metamodels have been used to support the representation of assets that compose a PLA, SPL variability and the relationships among them. The goal of this study is to characterize the use of metamodeling on PLA design, aiming to identify the main characteristics of metamodels, the elements used for PLA and variability representation and trace the evolution of metamodels. We conducted a systematic literature review to identify the primary studies on the use of metamodels in PLA Design. Thirty-five studies that proposed metamodels to support PLA design were selected. The review main findings are: (i) it is difficult to identify the existence of research trends because the number of publication varies and metamodels lack standardization; (ii) several metamodels support feature representation; (iii) the majority of studies addressed variability representation with variation points in UML diagrams; and, (iv) five evolution lines that describe how metamodels evolved over the years were identified.},
booktitle = {Proceedings of the XXX Brazilian Symposium on Software Engineering},
pages = {13–22},
numpages = {10},
keywords = {Metamodels, Product Line Architecture, Software Product Lines, Systematic Literature Review, Variability},
location = {Maring\'{a}, Brazil},
series = {SBES '16}
}

@inproceedings{10.1145/1629716.1629724,
author = {Elsner, Christoph and Lohmann, Daniel and Schr\"{o}der-Preikschat, Wolfgang},
title = {Product derivation for solution-driven product line engineering},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629724},
doi = {10.1145/1629716.1629724},
abstract = {Solution-driven product line engineering is a project business where products are created for each customer individually. Although reuse of results from former projects is widely done, configuration and integration of the results currently is often a manual, time-consuming, and error-prone task and needs considerable knowledge about implementation details.In this paper, we elaborate and approach the challenges when giving automated support for product derivation (i.e., product configuration and generation) in a large-scale solution-driven product line context. Our PLiC approach resembles the fact that, in practice, the domain of a large product line is divided into sub-domains. A PLiC (product line component) packages all results (configuration, generation, and implementation assets) of a sub-domain and offers interfaces for configuration and generation. With our approach we tackle the challenges of using multiple and different types of configuration models and text files, give support for automated product generation, and integrate feature modeling to support application engineering as an extensive development task.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {35–41},
numpages = {7},
keywords = {feature modeling, software product line development, solution-driven software development},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@article{10.1145/3659101,
author = {Tian, Huan and Tang, Jiewen and Li, Jun and Sha, Zhibing and Yang, Fan and Cai, Zhigang and Liao, Jianwei},
title = {Modeling Retention Errors of 3D NAND Flash for Optimizing Data Placement},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {1084-4309},
url = {https://doi.org/10.1145/3659101},
doi = {10.1145/3659101},
abstract = {Considering 3D NAND flash has a new property of process variation (PV), which causes different raw bit error rates (RBER) among different layers of the flash block. This article builds a mathematical model for estimating the retention errors of flash cells, by considering the factor of layer-to-layer PV in 3D NAND flash memory, as well as the factors of program/erase (P/E) cycle and retention time of data. Then, it proposes classifying the layers of flash block in 3D NAND flash memory into profitable and unprofitable categories, according to the error correction overhead. After understanding the retention error variation of different layers in 3D NAND flash, we design a mechanism of data placement, which maps the write data onto a suitable layer of flash block, according to the data hotness and the error correction overhead of layers, to boost read performance of 3D NAND flash. The experimental results demonstrate that our proposed retention error estimation model can yield a R2 value of 0.966 on average, verifying the accuracy of the model. Based on the estimated retention error rates of layers, the proposed data placement mechanism can noticeably reduce the read latency by 29.8% on average, compared with state-of-the-art methods against retention errors for 3D NAND flash memory.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = jun,
articleno = {62},
numpages = {24},
keywords = {Solid-state drivers, 3D flash memories, ECC, reliability, modeling, layer RBER variation}
}

@inproceedings{10.1145/2829875.2829894,
author = {Amin, Navya and Gross, Thomas and Rosenthal, Susanne and Borschbach, Markus},
title = {Blind Source Separation Performance based on Microphone Sensitivity and Orientation within Interaction Devices},
year = {2015},
isbn = {9781450334631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2829875.2829894},
doi = {10.1145/2829875.2829894},
abstract = {The use of smartphones has tremendously increased in the last decade. The coupling of a mobile phone with multiple sensors has expanded the usage of smartphones into a wider area. A basic research on pre-requisites of Blind Source Separation (BSS) to be used in interaction devices is performed. Our focus of research is the application of signal processing techniques for BSS of audio signals captured using interaction devices like the smartphones with the final aim of recovering the spatial listening experience of persons with reduced hearing abilities using the recently proposed app-based system. With this app, the user is able to interactively enhance speech or sound sources in this surroundings using the interactive interface on his smartphone. The sensitivity and orientation of the smartphone microphones (mics) play a very important role to achieve a better quality of BSS and hence is one of the prime pre-requisites for BSS. Therefore, a benchmarking regarding the interdependence of the different internal and external mics as well as the orientation of the smartphones relative to the sources on the separation performance is discussed in this paper. Four smartphones are used to examine the influence of mic sensitivity on the BSS quality of captured signals. Further external mics with smartphones are tested for their influence on BSS. The captured audio signals are separated using PARAllel FACtor analysis (PARAFAC) based BSS algorithm.},
booktitle = {Proceedings of the XVI International Conference on Human Computer Interaction},
articleno = {26},
numpages = {8},
keywords = {BSS, External Microphones, Internal Microphones, Microphone sensitivity, Smartphone},
location = {Vilanova i la Geltru, Spain},
series = {Interacci\'{o}n '15}
}

@inproceedings{10.1145/2420942.2420944,
author = {Olaechea, Rafael and Stewart, Steven and Czarnecki, Krzysztof and Rayside, Derek},
title = {Modelling and multi-objective optimization of quality attributes in variability-rich software},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420944},
doi = {10.1145/2420942.2420944},
abstract = {Variability-rich software, such as software product lines, offers optional and alternative features to accommodate varying needs of users. Designers of variability-rich software face the challenge of reasoning about the impact of selecting such features on the quality attributes of the resulting software variant. Attributed feature models have been proposed to model such features and their impact on quality attributes, but existing variability modelling languages and tools have limited or no support for such models and the complex multi-objective optimization problem that arises. This paper presents ClaferMoo, a language and tool that addresses these shortcomings. ClaferMoo uses type inheritance to modularize the attribution of features in feature models and allows specifying multiple optimization goals. We evaluate an implementation of the language on a set of attributed feature models from the literature, showing that the optimization infrastructure can handle small-scale feature models with about a dozen features within seconds.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {2},
numpages = {6},
keywords = {multi-objective optimization, software product lines},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@inproceedings{10.1145/974044.974089,
author = {Wu, Xiuping and Woodside, Murray},
title = {Performance modeling from software components},
year = {2004},
isbn = {1581136730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/974044.974089},
doi = {10.1145/974044.974089},
abstract = {When software products are assembled from pre-defined components, performance prediction should be based on the components also. This supports rapid model-building, using previously calibrated sub-models or "performance components", in sync with the construction of the product. The specification of a performance component must be tied closely to the software component specification, but it also includes performance related parameters (describing workload characteristics and demands), and it abstracts the behaviour of the component in various ways (for reasons related to practical factors in performance analysis). A useful set of abstractions and parameters are already defined for layered performance modeling. This work extends them to accommodate software components, using a new XML-based language called Component-Based Modeling Language (CBML). With CBML, compatible components can be inserted into slots provided in a hierarchical component specification based on the UML component model.},
booktitle = {Proceedings of the 4th International Workshop on Software and Performance},
pages = {290–301},
numpages = {12},
keywords = {CBML, LQN, generative programming, layered queue model, performance prediction, software component, software performance, submodel},
location = {Redwood Shores, California},
series = {WOSP '04}
}

@inproceedings{10.1109/ICSE-NIER.2019.00028,
author = {Trubiani, Catia and Apel, Sven},
title = {PLUS: performance learning for uncertainty of software},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2019.00028},
doi = {10.1109/ICSE-NIER.2019.00028},
abstract = {Uncertainty is particularly critical in software performance engineering when it relates to the values of important parameters such as workload, operational profile, and resource demand, because such parameters inevitably affect the overall system performance. Prior work focused on monitoring the performance characteristics of software systems while considering influence of configuration options. The problem of incorporating uncertainty as a first-class concept in the software development process to identify performance issues is still challenging. The PLUS (Performance Learning for Uncertainty of Software) approach aims at addressing these limitations by investigating the specification of a new class of performance models capturing how the different uncertainties underlying a software system affect its performance characteristics. The main goal of PLUS is to answer a fundamental question in the software performance engineering domain: How to model the variable configuration options (i.e., software and hardware resources) and their intrinsic uncertainties (e.g., resource demand, processor speed) to represent the performance characteristics of software systems? This way, software engineers are exposed to a quantitative evaluation of their systems that supports them in the task of identifying performance critical configurations along with their uncertainties.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {77–80},
numpages = {4},
keywords = {machine learning, uncertainty},
location = {Montreal, Quebec, Canada},
series = {ICSE-NIER '19}
}

@inproceedings{10.1109/ICSE.2019.00112,
author = {Kaltenecker, Christian and Grebhahn, Alexander and Siegmund, Norbert and Guo, Jianmei and Apel, Sven},
title = {Distance-based sampling of software configuration spaces},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00112},
doi = {10.1109/ICSE.2019.00112},
abstract = {Configurable software systems provide a multitude of configuration options to adjust and optimize their functional and non-functional properties. For instance, to find the fastest configuration for a given setting, a brute-force strategy measures the performance of all configurations, which is typically intractable. Addressing this challenge, state-of-the-art strategies rely on machine learning, analyzing only a few configurations (i.e., a sample set) to predict the performance of other configurations. However, to obtain accurate performance predictions, a representative sample set of configurations is required. Addressing this task, different sampling strategies have been proposed, which come with different advantages (e.g., covering the configuration space systematically) and disadvantages (e.g., the need to enumerate all configurations). In our experiments, we found that most sampling strategies do not achieve a good coverage of the configuration space with respect to covering relevant performance values. That is, they miss important configurations with distinct performance behavior. Based on this observation, we devise a new sampling strategy, called distance-based sampling, that is based on a distance metric and a probability distribution to spread the configurations of the sample set according to a given probability distribution across the configuration space. This way, we cover different kinds of interactions among configuration options in the sample set. To demonstrate the merits of distance-based sampling, we compare it to state-of-the-art sampling strategies, such as t-wise sampling, on 10 real-world configurable software systems. Our results show that distance-based sampling leads to more accurate performance models for medium to large sample sets.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1084–1094},
numpages = {11},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1109/AST.2017.7,
author = {Al-Hajjaji, Mustafa and Kr\"{u}ger, Jacob and Schulze, Sandro and Leich, Thomas and Saake, Gunter},
title = {Efficient product-line testing using cluster-based product prioritization},
year = {2017},
isbn = {9781538615485},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/AST.2017.7},
doi = {10.1109/AST.2017.7},
abstract = {A software product-line comprises a set of products that share a common set of features. These features can be reused to customize a product to satisfy specific needs of certain customers or markets. As the number of possible products increases exponentially for new features, testing all products is infeasible. Existing testing approaches reduce their effort by restricting the number of products (sampling) and improve their effectiveness by considering the order of tests (prioritization). In this paper, we propose a cluster-based prioritization technique to sample similar products with respect to the feature selection. We evaluate our approach using feature models of different sizes and show that cluster-based prioritization can enhance the effectiveness of product-line testing.},
booktitle = {Proceedings of the 12th International Workshop on Automation of Software Testing},
pages = {16–22},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {AST '17}
}

@inproceedings{10.1145/1159733.1159762,
author = {Denger, Christian and Kolb, Ronny},
title = {Testing and inspecting reusable product line components: first empirical results},
year = {2006},
isbn = {1595932186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1159733.1159762},
doi = {10.1145/1159733.1159762},
abstract = {In recent years, product line development has increasingly received attention in industry as it enables software-developing organizations to reduce both cost and time of developing and maintaining increasingly complex systems as well as to address the demands for individually customized products. Successful product line development requires high quality of reusable artifacts in order to achieve the promised benefits. The unique issues of quality assurance in the context of systematic reuse, however, have not been quantitatively investigated so far. This paper describes a first empirical study comparing the two defect detection techniques, code inspections and functional testing, in the context of product line development. The primary goal of the study was to initially investigate the defect finding potential of the techniques on reusable software components with common and variant features. The major findings of the study are that the two techniques identified different types of defects on variants of a reusable component. Inspections are on average 66.39% more effective and need on average 36.84% less effort to detect a defect We found that both the testing and inspection techniques applied in the experiment were ineffective in identifying variant-specific defects. Overall, the results indicate that the standard quality assurance techniques seem to be insufficient to address special characteristics of reusable components.},
booktitle = {Proceedings of the 2006 ACM/IEEE International Symposium on Empirical Software Engineering},
pages = {184–193},
numpages = {10},
keywords = {controlled experiment, functional testing, inspection, quality assurance, reusable components, software product line},
location = {Rio de Janeiro, Brazil},
series = {ISESE '06}
}

@article{10.1145/2211616.2211617,
author = {K\"{a}stner, Christian and Apel, Sven and Th\"{u}m, Thomas and Saake, Gunter},
title = {Type checking annotation-based product lines},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2211616.2211617},
doi = {10.1145/2211616.2211617},
abstract = {Software product line engineering is an efficient means of generating a family of program variants for a domain from a single code base. However, because of the potentially high number of possible program variants, it is difficult to test them all and ensure properties like type safety for the entire product line. We present a product-line-aware type system that can type check an entire software product line without generating each variant in isolation. Specifically, we extend the Featherweight Java calculus with feature annotations for product-line development and prove formally that all program variants generated from a well typed product line are well typed. Furthermore, we present a solution to the problem of typing mutually exclusive features. We discuss how results from our formalization helped implement our own product-line tool CIDE for full Java and report of our experience with detecting type errors in four existing software product line implementations.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {14},
numpages = {39},
keywords = {#ifdef, CFJ, CIDE, Featherweight Java, conditional compilation, software product lines, type system}
}

@inproceedings{10.1145/3267183.3267185,
author = {Perissato, Eduardo G. and Neto, Jo\~{a}o Choma and Colanzi, Thelma E. and Oizumi, Willian and Garcia, Alessandro},
title = {On Identifying Architectural Smells in Search-based Product Line Designs},
year = {2018},
isbn = {9781450365543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267183.3267185},
doi = {10.1145/3267183.3267185},
abstract = {The Product Line Architecture (PLA) defines a common design for all products derived from the software product line. PLA design is a high-effort task since it is influenced by several factors, such as feature modularity, PLA variability and extensibility. Given the complexity and variety of such influential factors, the use of search-based algorithms have recently been explored to derive PLA designs in the seminal approach named Multi-Objective Approach for Product-Line Architecture Design (MOA4PLA). MOA4PLA produces a set of alternative PLA designs that improve the different optimized factors. Besides the optimization of these factors, the obtained PLA designs should ideally have no architectural smell. An architectural smell may harm not only the PLA variability and extensibility, but also other non-functional attributes. However, no previous study was performed to investigate if and to what extent state-of-the-art search-based approaches, such as MOA4PLA, adversely introduce architectural smells in resulting designs. Thus, we performed an exploratory investigation about the presence of architectural smells in a set of PLA designs obtained from MOA4PLA. Such an investigation was also useful to derive guidelines for: (i) improving the resulting search-based PLA designs, or (ii) preventing upfront the emergence of architectural smells in PLA designs automatically obtained with search-based approaches.},
booktitle = {Proceedings of the VII Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {13–22},
numpages = {10},
keywords = {Architectural Smells, Product Line Design, Search-based Software Engineering},
location = {Sao Carlos, Brazil},
series = {SBCARS '18}
}

@inproceedings{10.1145/1062455.1062552,
author = {Schmid, Klaus and John, Isabel and Kolb, Ronny and Meier, Gerald},
title = {Introducing the puLSE approach to an embedded system population at testo AG},
year = {2005},
isbn = {1581139632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062455.1062552},
doi = {10.1145/1062455.1062552},
abstract = {Over the last few years, product line engineering has become a major theme in software engineering research, and is increasingly becoming a central topic of software engineering practice in the embedded domain.Migrating towards a product line approach is not an easy feat. It is even less so, if it is done under tight technology constraints in an embedded environment. It becomes even more difficult if the transition directly aims at integrating two product families into a single product population. In this paper, we discuss our experiences with a project where we successfully dealt with these difficulties and achieved a successful product line transition. In our paper we strongly emphasize the role of technology transfer, as many facets of product line know-how had to be transferred to guarantee a complete transition to product line engineering. From the experiences of this project many lessons learned can be deduced, which can be transferred to different environments.},
booktitle = {Proceedings of the 27th International Conference on Software Engineering},
pages = {544–552},
numpages = {9},
keywords = {product line introduction, software product line, systematic software reuse, technology transfer},
location = {St. Louis, MO, USA},
series = {ICSE '05}
}

@inproceedings{10.1145/2742647.2742658,
author = {Zhang, Li and Pathak, Parth H. and Wu, Muchen and Zhao, Yixin and Mohapatra, Prasant},
title = {AccelWord: Energy Efficient Hotword Detection through Accelerometer},
year = {2015},
isbn = {9781450334945},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2742647.2742658},
doi = {10.1145/2742647.2742658},
abstract = {Voice control has emerged as a popular method for interacting with smart-devices such as smartphones, smartwatches etc. Popular voice control applications like Siri and Google Now are already used by a large number of smartphone and tablet users. A major challenge in designing a voice control application is that it requires continuous monitoring of user?s voice input through the microphone. Such applications utilize hotwords such as "Okay Google" or "Hi Galaxy" allowing them to distinguish user?s voice command and her other conversations. A voice control application has to continuously listen for hotwords which significantly increases the energy consumption of the smart-devices.To address this energy efficiency problem of voice control, we present AccelWord in this paper. AccelWord is based on the empirical evidence that accelerometer sensors found in today?s mobile devices are sensitive to user?s voice. We also demonstrate that the effect of user?s voice on accelerometer data is rich enough so that it can be used to detect the hotwords spoken by the user. To achieve the goal of low energy cost but high detection accuracy, we combat multiple challenges, e.g. how to extract unique signatures of user?s speaking hotwords only from accelerometer data and how to reduce the interference caused by user?s mobility.We finally implement AccelWord as a standalone application running on Android devices. Comprehensive tests show AccelWord has hotword detection accuracy of 85% in static scenarios and 80% in mobile scenarios. Compared to the microphone based hotword detection applications such as Google Now and Samsung S Voice, AccelWord is 2 times more energy efficient while achieving the accuracy of 98% and 92% in static and mobile scenarios respectively.},
booktitle = {Proceedings of the 13th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {301–315},
numpages = {15},
keywords = {accelerometer, accelword, energy, hotword detection, measurement},
location = {Florence, Italy},
series = {MobiSys '15}
}

@article{10.1145/2638550,
author = {Wu, Lisa and Polychroniou, Orestis and Barker, Raymond J. and Kim, Martha A. and Ross, Kenneth A.},
title = {Energy Analysis of Hardware and Software Range Partitioning},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {0734-2071},
url = {https://doi.org/10.1145/2638550},
doi = {10.1145/2638550},
abstract = {Data partitioning is a critical operation for manipulating large datasets because it subdivides tasks into pieces that are more amenable to efficient processing. It is often the limiting factor in database performance and represents a significant fraction of the overall runtime of large data queries. This article measures the performance and energy of state-of-the-art software partitioners, and describes and evaluates a hardware range partitioner that further improves efficiency.The software implementation is broken into two phases, allowing separate analysis of the partition function computation and data shuffling costs. Although range partitioning is commonly thought to be more expensive than simpler strategies such as hash partitioning, our measurements indicate that careful data movement and optimization of the partition function can allow it to approach the throughput and energy consumption of hash or radix partitioning.For further acceleration, we describe a hardware range partitioner, or HARP, a streaming framework that offers a seamless execution environment for this and other streaming accelerators, and a detailed analysis of a 32nm physical design that matches the throughput of four to eight software threads while consuming just 6.9% of the area and 4.3% of the power of a Xeon core in the same technology generation.},
journal = {ACM Trans. Comput. Syst.},
month = aug,
articleno = {8},
numpages = {24},
keywords = {Accelerator, data partitioning, microarchitecture, specialized functional unit, streaming data}
}

@inproceedings{10.1145/3624007.3624058,
author = {Medeiros, Raul and D\'{\i}az, Oscar and Benavides, David},
title = {Unleashing the Power of Implicit Feedback in Software Product Lines: Benefits Ahead},
year = {2023},
isbn = {9798400704062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624007.3624058},
doi = {10.1145/3624007.3624058},
abstract = {Software Product Lines (SPLs) facilitate the development of a complete range of software products through systematic reuse. Reuse involves not only code but also the transfer of knowledge gained from one product to others within the SPL. This transfer includes bug fixing, which, when encountered in one product, affects the entire SPL portfolio. Similarly, feedback obtained from the usage of a single product can inform beyond that product to impact the entire SPL portfolio. Specifically, implicit feedback refers to the automated collection of data on software usage or execution, which allows for the inference of customer preferences and trends. While implicit feedback is commonly used in single-product development, its application in SPLs has not received the same level of attention. This paper promotes the investigation of implicit feedback in SPLs by identifying a set of SPL activities that can benefit the most from it. We validate this usefulness with practitioners using a questionnaire-based approach (n=8). The results provide positive insights into the advantages and practical implications of adopting implicit feedback at the SPL level.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {113–121},
numpages = {9},
keywords = {Code generation, Implicit feedback, Software Product Lines, User behavior},
location = {Cascais, Portugal},
series = {GPCE 2023}
}

@article{10.1109/TASLP.2014.2344862,
author = {Defraene, Bruno and Van Waterschoot, Toon and Diehl, Moritz and Moonen, Marc},
title = {Embedded-optimization-based loudspeaker precompensation using a hammerstein loudspeaker model},
year = {2014},
issue_date = {November 2014},
publisher = {IEEE Press},
volume = {22},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2344862},
doi = {10.1109/TASLP.2014.2344862},
abstract = {This paper presents an embedded-optimization-based loudspeaker precompensation algorithm using a Hammerstein loudspeaker model, i.e. a cascade of a memoryless nonlinearity and a linear finite impulse response filter. The loudspeaker precompensation consists in a per-frame signal optimization. In order to minimize the perceptible distortion incurred in the loudspeaker, a psychoacoustically motivated optimization criterion is proposed. The resulting per-frame signal optimization problems are solved efficiently using first-order optimization methods. Depending on the invertibility and the smoothness of the memoryless nonlinearity, different first-order optimization methods are proposed and their convergence properties are analyzed. Objective evaluation experiments using synthetic loudspeaker models and real loudspeakers show that the proposed loudspeaker precompensation algorithm provides a significant audio quality improvement, especially so at high playback levels.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1648–1659},
numpages = {12},
keywords = {embedded optimization, gradient optimization, hammerstein model, loudspeaker precompensation, sound perception}
}

@inproceedings{10.5555/3320516.3320858,
author = {Rabe, Markus and Ammouriova, Majsa and Schmitt, Dominik},
title = {Utilizing domain-specific information for the optimization of logistics networks},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {Continuously maintaining a logistics network (LNW) in good condition is a challenging task for decision makers. For purposes of improving an LNW's performance, promising actions need to be identified, such as the centralization of a stock keeping unit (SKU). In order to support the decision maker, the authors have developed a logistics assistance system (LAS) based on discrete-event simulation. With an increasing size of the LNW, the response time of such an LAS increases exponentially. In this paper, the authors present an approach for utilizing domain-specific information to guide the search for promising actions and, therefore, reduce the LAS's response time. The given examples show that the LAS's response time can be decreased. For example, the approach reduces the number of iterations needed by an evolutionary algorithm to converge.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {2873–2884},
numpages = {12},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@article{10.1109/TASLP.2018.2821903,
author = {Fu, Szu-Wei and Wang, Tao-Wei and Tsao, Yu and Lu, Xugang and Kawai, Hisashi},
title = {End-to-End Waveform Utterance Enhancement for Direct Evaluation Metrics Optimization by Fully Convolutional Neural Networks},
year = {2018},
issue_date = {September 2018},
publisher = {IEEE Press},
volume = {26},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2821903},
doi = {10.1109/TASLP.2018.2821903},
abstract = {Speech enhancement model is used to map a noisy speech to a clean speech. In the training stage, an objective function is often adopted to optimize the model parameters. However, in the existing literature, there is an inconsistency between the model optimization criterion and the evaluation criterion for the enhanced speech. For example, in measuring speech intelligibility, most of the evaluation metric is based on a short-time objective intelligibility STOI measure, while the frame based mean square error MSE between estimated and clean speech is widely used in optimizing the model. Due to the inconsistency, there is no guarantee that the trained model can provide optimal performance in applications. In this study, we propose an end-to-end utterance-based speech enhancement framework using fully convolutional neural networks FCN to reduce the gap between the model optimization and the evaluation criterion. Because of the utterance-based optimization, temporal correlation information of long speech segments, or even at the entire utterance level, can be considered to directly optimize perception-based objective functions. As an example, we implemented the proposed FCN enhancement framework to optimize the STOI measure. Experimental results show that the STOI of a test speech processed by the proposed approach is better than conventional MSE-optimized speech due to the consistency between the training and the evaluation targets. Moreover, by integrating the STOI into model optimization, the intelligibility of human subjects and automatic speech recognition system on the enhanced speech is also substantially improved compared to those generated based on the minimum MSE criterion.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1570–1584},
numpages = {15}
}

@inproceedings{10.5555/1129601.1129621,
author = {Staszewski, R. B. and Muhammad, K. and Leipold, D.},
title = {Digital RF processor (DRP/spl trade/) for cellular phones},
year = {2005},
isbn = {078039254X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {RF circuits for multi-GHz frequencies have recently migrated to low-cost digital deep-submicron CMOS processes. Unfortunately, this process environment, which is optimized only for digital logic and SRAM memory, is extremely unfriendly for conventional analog and HF designs. We present fundamental techniques recently developed that transform the RF and analog circuit design complexity to digital domain for a wireless RF transceiver, so that it enjoys the benefits of digital approach, such as process node scaling and design automation. All-digital phase locked loop, all-digital control of phase and amplitude of a polar transmitter, and direct HF sampling techniques allow great flexibility in reconfigurable radio design. Digital signal processing concepts are used to help relieve analog design complexity, allowing one to reduce cost and power consumption in a reconfigurable design environment. Software layers are defined to enable these architectures to develop an efficient software defined radio. VHDL hardware description language is universally used throughout this SoC. The ideas presented have been used in Texas Instruments to develop two generations of commercial digital RF processors: a single-chip Bluetooth radio and a single-chip GSM radio.},
booktitle = {Proceedings of the 2005 IEEE/ACM International Conference on Computer-Aided Design},
pages = {122–129},
numpages = {8},
location = {San Jose, CA},
series = {ICCAD '05}
}

@article{10.5555/3122009.3242055,
author = {Patrascu, Andrei and Necoara, Ion},
title = {Nonasymptotic convergence of stochastic proximal point methods for constrained convex optimization},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {A popular approach for solving stochastic optimization problems is the stochastic gradient descent (SGD) method. Although the SGD iteration is computationally cheap and its practical performance may be satisfactory under certain circumstances, there is recent evidence of its convergence difficulties and instability for unappropriate choice of parameters. To avoid some of the drawbacks of SGD, stochastic proximal point (SPP) algorithms have been recently considered. We introduce a new variant of the SPP method for solving stochastic convex problems subject to (in)finite intersection of constraints satisfying a linear regularity condition. For the newly introduced SPP scheme we prove new nonasymptotic convergence results. In particular, for convex Lipschitz continuous objective functions, we prove nonasymptotic convergence rates in terms of the expected value function gap of order O(1/k1/2), where k is the iteration counter. We also derive better nonasymptotic convergence rates in terms of expected quadratic distance from the iterates to the optimal solution for smooth strongly convex objective functions, which in the best case is of order O(1/k). Since these convergence rates can be attained by our SPP algorithm only under some natural restrictions on the stepsize, we also introduce a restarting variant of SPP that overcomes these difficulties and derive the corresponding nonasymptotic convergence rates. Numerical evidence supports the effectiveness of our methods in real problems.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {7204–7245},
numpages = {42},
keywords = {intersection of convex constraints, nonasymptotic convergence analysis, rates of convergence, stochastic convex optimization, stochastic proximal point}
}

@inproceedings{10.1145/2892664.2892686,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Towards the dynamic reconfiguration of quality attributes},
year = {2016},
isbn = {9781450340335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2892664.2892686},
doi = {10.1145/2892664.2892686},
abstract = {There are some Quality Attributes (QAs) whose variability is addressed through functional variability in the software architecture. Separately modelling the variability of these QAs from the variability of the base functionality of the application has many advantages (e.g., a better reusability), and facilitates the reconfiguration of the QA variants at runtime. Many factors may vary the QA functionality: variations in the user preferences and usage needs; variations in the non-functional QAs; variations in resources, hardware, or even in the functionality of the base application, that directly affect the product's QAs. In this paper, we aim to elicit the relationships and dependencies between the functionalities required to satisfy the QAs and all those factors that can provoke a reconfiguration of the software architecture at runtime. We follow an approach in which the variability of the QAs is modelled separately from the base application functionality, and propose a dynamic approach to reconfigure the software architecture based on those reconfiguration criteria.},
booktitle = {Companion Proceedings of the 15th International Conference on Modularity},
pages = {131–136},
numpages = {6},
keywords = {Quality attributes, SPL, reconfiguration, software architecture, variability},
location = {M\'{a}laga, Spain},
series = {MODULARITY Companion 2016}
}

@inproceedings{10.1145/3205455.3205640,
author = {Shakya, Siddhartha and Poon, Kin and Ouali, Anis},
title = {A GA based network optimization tool for passive in-building distributed antenna systems},
year = {2018},
isbn = {9781450356183},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205455.3205640},
doi = {10.1145/3205455.3205640},
abstract = {With an explosive increase in data traffic over recent years, it has become increasingly difficult to rely on outdoor base stations to support the traffic generated indoors mainly due to the penetration issue of wireless signals. Mobile operators have investigated different options to provide adequate capacity and good in-building coverage such as by deploying femtocells, Wi-Fi off-load or in-building distributed antenna systems (IB-DAS). A passive IB-DAS extends indoor coverage by connecting antennas to a base station through coaxial cables and passive components. This paper focuses on automated design of IB-DAS based on the real world requirements of a telecom service provider. A Genetic Algorithm (GA) is derived for this purpose, giving consideration to different factors, such as minimizing cabling and passive splitter costs, reducing power spillage and power deviation between the required and supplied power for antennas. The solution representation of the problem and the customized genetic operators to assist the evolution are described. The experimental results showing the effectiveness of the GA model on a number of different scenarios are also presented. The built model is incorporated into a software tool, which is being trialled by our industrial partner, delivering encouraging results, saving cost and design time.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1371–1378},
numpages = {8},
keywords = {GA, antennas, distributed antenna system, mixed integer linear program, splitters},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@inproceedings{10.1145/3192366.3192420,
author = {Steindorfer, Michael J. and Vinju, Jurgen J.},
title = {To-many or to-one? all-in-one! efficient purely functional multi-maps with type-heterogeneous hash-tries},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192420},
doi = {10.1145/3192366.3192420},
abstract = {An immutable multi-map is a many-to-many map data structure with expected fast insert and lookup operations. This data structure is used for applications processing graphs or many-to-many relations as applied in compilers, runtimes of programming languages, or in static analysis of object-oriented systems. Collection data structures are assumed to carefully balance execution time of operations with memory consumption characteristics and need to scale gracefully from a few elements to multiple gigabytes at least. When processing larger in-memory data sets the overhead of the data structure encoding itself becomes a memory usage bottleneck, dominating the overall performance.  In this paper we propose AXIOM, a novel hash-trie data structure that allows for a highly efficient and type-safe multi-map encoding by distinguishing inlined values of singleton sets from nested sets of multi-mappings. AXIOM strictly generalizes over previous hash-trie data structures by supporting the processing of fine-grained type-heterogeneous content on the implementation level (while API and language support for type-heterogeneity are not scope of this paper). We detail the design and optimizations of AXIOM and further compare it against state-of-the-art immutable maps and multi-maps in Java, Scala and Clojure. We isolate key differences using microbenchmarks and validate the resulting conclusions on a case study in static analysis. AXIOM reduces the key-value storage overhead by 1.87x; with specializing and inlining across collection boundaries it improves by 5.1x.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {283–295},
numpages = {13},
keywords = {Data structures, JVM, functional programming, graph, hashtable, many-to-many relation, multi-map, optimization, performance, persistent data structures},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@inproceedings{10.1145/2660190.2662115,
author = {Baller, Hauke and Lochau, Malte},
title = {Towards incremental test suite optimization for software product lines},
year = {2014},
isbn = {9781450329804},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660190.2662115},
doi = {10.1145/2660190.2662115},
abstract = {The design of an appropriate test suite for software testing is a challenging task. It requires a suitable tradeoff between effectiveness, e.g., a sufficient amount of test cases to satisfy the test goals of a given coverage criterion, and efficiency, e.g., a redundancy-reduced selection of test cases. Recent test suite optimization approaches, therefore, usually require an explicit enumeration of existing test cases to select from. The test suite design for covering entire software product lines is even more problematic as the dependency between test cases, test goals and product configurations has to be taken into account. Due to the exponential number of configurations w.r.t. the number of features, an explicit enumeration of all products for optimizing a product-line test suite is impractible. To tackle this problem, we propose an incremental test suite optimization approach for product-line testing that does not require an explicit representation of the set of configurations under test, but rather uses a symbolic representation in terms of feature constraints. The approach is illustrated by means of a running example.},
booktitle = {Proceedings of the 6th International Workshop on Feature-Oriented Software Development},
pages = {30–36},
numpages = {7},
keywords = {model-based software engineering, software product lines, test case generation},
location = {V\"{a}ster\r{a}s, Sweden},
series = {FOSD '14}
}

@inproceedings{10.1145/3128473.3128479,
author = {Lima, Jackson A. Prado and Vergilio, Silvia R.},
title = {A Multi-objective optimization approach for selection of second order mutant generation strategies},
year = {2017},
isbn = {9781450353021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3128473.3128479},
doi = {10.1145/3128473.3128479},
abstract = {The use of Higher-Order Mutants (HOMs) presents some advantages concerning the traditional use of First-Order Mutants (FOMs). HOMs can better simulate real and subtle faults, reduce the number of generated mutants and test cases, and so on. However, the HOM space is potentially huge, and an efficient strategy to generate the best HOMs is fundamental. In the literature different strategies were proposed and evaluated, mainly to generate Second-Order Mutants (SOMs), but none has been proved to perform better in different situations. Due to this, the selection of the best strategy is an important task. Most times a lot of experiments need to be conducted. To help the tester in this task and to allow the use of HOMs in practice, this paper proposes a hyper-heuristic approach. Such approach is based on NSGA-II and uses the selection method Choice Function to automatically choose among different Low-Level Heuristics (LLHs), which, in this case, are search-operators related to existing SOM generation strategies. The performance of each LLH is related to some objectives such as the number of SOMs generated, the capacity to capture subtler faults and replace the constituent FOMs. In comparison with existing strategies, our approach obtained better results considering the used objectives, and statistically equivalent results considering mutation score with respect to the FOMs.},
booktitle = {Proceedings of the 2nd Brazilian Symposium on Systematic and Automated Software Testing},
articleno = {6},
numpages = {10},
keywords = {Higher-Order Mutation, Multi-objective optimization, Mutation Testing},
location = {Fortaleza, Brazil},
series = {SAST '17}
}

@inproceedings{10.1109/ASE.2011.6100068,
author = {Pohl, Richard and Lauenroth, Kim and Pohl, Klaus},
title = {A performance comparison of contemporary algorithmic approaches for automated analysis operations on feature models},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100068},
doi = {10.1109/ASE.2011.6100068},
abstract = {The formalization of variability models (e.g. feature models) is a prerequisite for the automated analysis of these models. The efficient execution of the analysis operations depends on the selection of well-suited solver implementations. Regarding feature models, on the one hand, the formalization with Boolean expressions enables the use of SAT or BDD solvers. On the other hand, feature models can be transformed into a Constraint-Satisfaction Problem (CSP) in order to use CSP solvers for validation. This paper presents a performance comparison regarding nine contemporary high-performance solvers, three for each base problem structure (BDD, CSP, and SAT). Four operations on 90 feature models are run on each solver. The results will in turn clear the way for new improvements regarding the automatic verification of software product lines, since the efficient execution of analysis operations is essential to such automatic verification approaches.},
booktitle = {Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {313–322},
numpages = {10},
series = {ASE '11}
}

@inproceedings{10.1145/1950413.1950448,
author = {Kenter, Tobias and Plessl, Christian and Platzner, Marco and Kauschke, Michael},
title = {Performance estimation framework for automated exploration of CPU-accelerator architectures},
year = {2011},
isbn = {9781450305549},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1950413.1950448},
doi = {10.1145/1950413.1950448},
abstract = {In this paper we present a fast and fully automated approach for studying the design space when interfacing reconfigurable accelerators with a CPU. Our challenge is, that a reasonable evaluation of architecture parameters requires a hardware/software partitioning that makes best use of each given architecture configuration. Therefore we developed a framework based on the LLVM infrastructure that performs this partitioning with high-level estimation of the runtime on the target architecture utilizing profiling information and code analysis. By making use of program characteristics also during the partitioning process, we improve previous results for various benchmarks and especially for growing interface latencies between CPU and accelerator.},
booktitle = {Proceedings of the 19th ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
pages = {177–180},
numpages = {4},
keywords = {design space exploration, hardware/software partitioning, llvm, performance estimation},
location = {Monterey, CA, USA},
series = {FPGA '11}
}

@inproceedings{10.1145/2593735.2593739,
author = {Kugele, Stefan and Pucea, Gheorghe},
title = {Model-based optimization of automotive E/E-architectures},
year = {2014},
isbn = {9781450328470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593735.2593739},
doi = {10.1145/2593735.2593739},
abstract = {In this paper we present a generic framework to enable constraint-based automotive E/E-architecture optimization using a domain-specific language. The quality of today's automotive E/E-architectures is highly influenced by the mapping of software to executing hardware components: the so-called deployment problem. First, we introduce a holistic architectural model facilitating a seamless model-based development from requirements management to deployment, which is the focus of this work. Second, we introduce our domain-specific constraint and optimization language AAOL (Automotive Architecture Optimization Language) capable to express a wide range of deployment-relevant problems. Third, we present a generic, i.e., solver-independent framework currently supporting multi-objective evolutionary algorithms (MOEA). We investigate the feasibility of the approach by dint of a case study taken from the literature.},
booktitle = {Proceedings of the 6th International Workshop on Constraints in Software Testing, Verification, and Analysis},
pages = {18–29},
numpages = {12},
keywords = {Model-based optimization, automotive E/E-architecture, constraint satisfaction problem, domain-specific languages},
location = {Hyderabad, India},
series = {CSTVA 2014}
}

@inproceedings{10.1109/ASE.2019.00123,
author = {Reichelt, David Georg and K\"{u}hne, Stefan and Hasselbring, Wilhelm},
title = {PeASS: a tool for identifying performance changes at code level},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00123},
doi = {10.1109/ASE.2019.00123},
abstract = {We present PeASS (Performance Analysis of Software System versions), a tool for detecting performance changes at source code level that occur between different code versions. By using PeASS, it is possible to identify performance regressions that happened in the past to fix them.PeASS measures the performance of unit tests in different source code versions. To achieve statistic rigor, measurements are repeated and analyzed using an agnostic t-test. To execute a minimal amount of tests, PeASS uses a regression test selection.We evaluate PeASS on a selection of Apache Commons projects and show that 81% of all unit test covered performance changes can be found by PeASS. A video presentation is available at https://www.youtube.com/watch?v=RORFEGSCh6Y and PeASS can be downloaded from https://github.com/DaGeRe/peass.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1146–1149},
numpages = {4},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.5555/2337223.2337468,
author = {Colanzi, Thelma Elita},
title = {Search based design of software product lines architectures},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {The Product-Line Architecture (PLA) is the main artifact of a Software Product Line (SPL). However, obtaining a modular, extensible and reusable PLA is a people-intensive and non-trivial task, related to different and possible conflicting factors. Hence, the PLA design is a hard problem and to find the best architecture can be formulated as an optimization problem with many factors. Similar Software Engineering problems have been efficiently solved by search-based algorithms in the field known as Search-based Software Engineering. The existing approaches used to optimize software architecture are not suitable since they do not encompass specific characteristics of SPL. To easy the SPL development and to automate the PLA design this work introduces a multi-objective optimization approach to the PLA design. The approach is now being implemented by using evolutionary algorithms. Empirical studies will be performed to validate the neighborhood operators, SPL measures and search algorithms chosen. Finally, we intend to compare the results of the proposed approach with PLAs designed by human architects.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {1507–1510},
numpages = {4},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1145/2568225.2568267,
author = {Salay, Rick and Famelis, Michalis and Rubin, Julia and Di Sandro, Alessio and Chechik, Marsha},
title = {Lifting model transformations to product lines},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568267},
doi = {10.1145/2568225.2568267},
abstract = {Software product lines and model transformations are two techniques used in industry for managing the development of highly complex software. Product line approaches simplify the handling of software variants while model transformations automate software manipulations such as refactoring, optimization, code generation, etc. While these techniques are well understood independently, combining them to get the benefit of both poses a challenge because most model transformations apply to individual models while model-level product lines represent sets of models. In this paper, we address this challenge by providing an approach for automatically ``lifting'' model transformations so that they can be applied to product lines. We illustrate our approach using a case study and evaluate it through a set of experiments.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {117–128},
numpages = {12},
keywords = {Model Driven Engineering, Model Transformations, Software Product Lines},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.5555/3042094.3042184,
author = {Rosen, Scott and Salemi, Peter and Wickham, Brian and Williams, Ashley and Harvey, Christine and Catlett, Erin and Taghiyeh, Sajjad and Xu, Jie},
title = {Parallel empirical stochastic branch and bound for large-scale discrete optimization via simulation},
year = {2016},
isbn = {9781509044849},
publisher = {IEEE Press},
abstract = {Real-life simulation optimization applications often deal with large-scale simulation models that are time-consuming to execute. Parallel computing environments, such as high performance computing clusters and cloud computing services, provide the computing power needed to scale to such applications. In this paper, we show how the Empirical Stochastic Branch and Bound algorithm, an effective globally convergent random search algorithm for discrete optimization via simulation, can be adapted to a high-performance computing environment to effectively utilize the power of parallelism. We propose a master-worker structure driven by MITRE's Goal-Directed Grid-Enabled Simulation Experimentation Environment. Numerical experiments with the popular Ackley benchmark test function and a real-world simulation called runwaySimulator demonstrate the number of cores needed to achieve a good scaled efficiency of parallel empirical stochastic branch and bound for increasing levels of simulation run times.},
booktitle = {Proceedings of the 2016 Winter Simulation Conference},
pages = {626–637},
numpages = {12},
location = {Arlington, Virginia},
series = {WSC '16}
}

@article{10.1145/3708527,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Marchezan, Luciano and Arkoh, Lawrence and Egyed, Alexander and Ramler, Rudolf},
title = {Contemporary Software Modernization: Strategies, Driving Forces, and Research Opportunities},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708527},
doi = {10.1145/3708527},
abstract = {Software modernization is a common activity in software engineering, since technologies advance, requirements change, and business models evolve. Differently from conventional software evolution (e.g., adding new features, enhancing performance, or adapting to new requirements), software modernization involves re-engineering entire legacy systems (e.g., changing the technology stack, migrating to a new architecture style, or programming paradigms). Given the pervasive nature of software today, modernizing legacy systems is paramount to provide customers with competitive and innovative products and services, while keeping companies profitable. Despite the prevalent discussion of software modernization in gray literature, and the many papers in the literature, there is no work presenting a “big picture” of contemporary software modernization, describing challenges, and providing a well-defined research agenda. The goal of this work is to describe the state of the art in software modernization in the past 10 years. We collect the state of the art by performing a rapid review (searching five digital libraries), identifying potential 3,460 studies, leading to a final set of 127. We analyzed these studies to understand which strategies are employed, the driving forces that lead organizations to modernize their systems, and the challenges that need to be addressed. The results show that studies in the last 10 years have explored eight strategies for modernizing legacy systems, namely cloudification, architecture redesign, moving to a new programming language, targeting reuse optimization, software modernization for new hardware integration, practices to leverage automation, database modernization, and digital transformation. Modernization is triggered by 14 driving forces, with the most common ones being reducing operational costs, improving performance and scalability, and reducing complexity. In addition, based on the analysis of existing literature, we present a detailed discussion of research opportunities in this field. The main challenges are providing tooling support, followed by defining a modernization process and considering better evaluation metrics. The main contribution of our work is to equip practitioners and researchers with knowledge of the current state of contemporary software modernization so that they are aware of practices and challenges to be addressed when deciding to modernize legacy systems.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {Software Evolution, Software Migration, Re-designing, Re-engineering}
}

@article{10.1109/TASLP.2016.2585864,
author = {Zorila, Tudor-Catalin and Stylianou, Yannis and Ishihara, Tatsuma and Akamine, Masami and Zorila, Tudor-Catalin and Stylianou, Yannis and Ishihara, Tatsuma and Akamine, Masami and Ishihara, Tatsuma and Stylianou, Yannis and Akamine, Masami and Zorila, Tudor-Catalin},
title = {Near and Far Field Speech-in-Noise Intelligibility Improvements Based on a Time–Frequency Energy Reallocation Approach},
year = {2016},
issue_date = {October 2016},
publisher = {IEEE Press},
volume = {24},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2585864},
doi = {10.1109/TASLP.2016.2585864},
abstract = {An algorithm designed to enhance the intelligibility of speech signals before they are presented in noisy environments was evaluated. The processed and unprocessed speech had the same root-mean square level. Spectral energy was redistributed to increase the signal-to-noise ratio SNR in the mid- and high-frequency bands, while the softer segments of speech were increased in level by applying time-domain dynamic range compression. Noise level adaptation was introduced to increase the subjective quality of signals at high SNRs. Evaluations were conducted both in near field headphones and in far field outdoor conditions using listeners with normal hearing and two types of background. The results showed: a In the near field test, the proposed algorithm yielded significant intelligibility improvements relative to the unprocessed speech for both stationary and nonstationary backgrounds; b In the far field test, the proposed algorithm increased the intelligibility of unprocessed speech by a factor of seven at 200-m distance from the sound source; and c When the background level did not alter intelligibility, listeners preferred the quality of the speech processed by the noise-dependent version of the algorithm.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1808–1818},
numpages = {11}
}

@inproceedings{10.1145/2897845.2897856,
author = {Meng, Guozhu and Xue, Yinxing and Mahinthan, Chandramohan and Narayanan, Annamalai and Liu, Yang and Zhang, Jie and Chen, Tieming},
title = {Mystique: Evolving Android Malware for Auditing Anti-Malware Tools},
year = {2016},
isbn = {9781450342339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897845.2897856},
doi = {10.1145/2897845.2897856},
abstract = {In the arms race of attackers and defenders, the defense is usually more challenging than the attack due to the unpredicted vulnerabilities and newly emerging attacks every day. Currently, most of existing malware detection solutions are individually proposed to address certain types of attacks or certain evasion techniques. Thus, it is desired to conduct a systematic investigation and evaluation of anti-malware solutions and tools based on different attacks and evasion techniques. In this paper, we first propose a meta model for Android malware to capture the common attack features and evasion features in the malware. Based on this model, we develop a framework, MYSTIQUE, to automatically generate malware covering four attack features and two evasion features, by adopting the software product line engineering approach. With the help of MYSTIQUE, we conduct experiments to 1) understand Android malware and the associated attack features as well as evasion techniques; 2) evaluate and compare the 57 off-the-shelf anti-malware tools, 9 academic solutions and 4 App market vetting processes in terms of accuracy in detecting attack features and capability in addressing evasion. Last but not least, we provide a benchmark of Android malware with proper labeling of contained attack and evasion features.},
booktitle = {Proceedings of the 11th ACM on Asia Conference on Computer and Communications Security},
pages = {365–376},
numpages = {12},
keywords = {android feature model, defense capability, evolutionary algorithm, malware generation},
location = {Xi'an, China},
series = {ASIA CCS '16}
}

@inproceedings{10.5555/2667025.2667027,
author = {Siegmund, Norbert and Mory, Maik and Feigenspan, Janet and Saake, Gunter and Nykolaychuk, Mykhaylo and Schumann, Marco},
title = {Interoperability of non-functional requirements in complex systems},
year = {2012},
isbn = {9781467318532},
publisher = {IEEE Press},
abstract = {Heterogeneity of embedded systems leads to the development of variable software, such as software product lines. From such a family of programs, stakeholders select the specific variant that satisfies their functional requirements. However, different functionality exposes different non-functional properties of these variants. Especially in the embedded-system domain, non-functional requirements are vital, because resources are scarce. Hence, when selecting an appropriate variant, we have to fulfill also non-functional requirements. Since more systems are interconnected, the challenge is to find a variant that additionally satisfies global nonfunctional (or quality) requirements. In this paper, we advert the problem of achieving interoperability of non-functional requirements among multiple interacting systems using a real-world scenario. Furthermore, we show an approach to find optimal variants for multiple systems that reduces computation effort by means of a stepwise configuration process.},
booktitle = {Proceedings of the Second International Workshop on Software Engineering for Embedded Systems},
pages = {2–8},
numpages = {7},
location = {Zurich, Switzerland},
series = {SEES '12}
}

@inproceedings{10.1145/1868688.1868694,
author = {Kuhlemann, Martin and Sturm, Martin},
title = {Patching product line programs},
year = {2010},
isbn = {9781450302081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868688.1868694},
doi = {10.1145/1868688.1868694},
abstract = {Software product line engineering is one approach to implement sets of related programs efficiently. Software product lines (SPLs) can be implemented using code transformations which are combined in order to generate a program. A code transformation may add functionality to a program or may alter its structure. Though implemented with less effort, a single malfunctioning SPL program is harder to patch because patches must effect the SPL transformations which the program was generated from. In this paper, we present a new approach to patch programs of a transformation-based SPL. We demonstrate the feasibility of this approach using a prototype.},
booktitle = {Proceedings of the 2nd International Workshop on Feature-Oriented Software Development},
pages = {33–40},
numpages = {8},
location = {Eindhoven, The Netherlands},
series = {FOSD '10}
}

@inproceedings{10.1145/1456659.1456662,
author = {Chapman, Mark and van der Merwe, Alta},
title = {Contemplating systematic software reuse in a project-centric company},
year = {2008},
isbn = {9781605582863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1456659.1456662},
doi = {10.1145/1456659.1456662},
abstract = {Systematic software reuse is still the most promising strategy for increasing productivity and improving quality in the software industry. Although it is simple in concept, successful software reuse implementation is difficult in practice. A reason put forward for this is the dependence of software reuse on the context in which it is implemented. This paper describes an interpretive case study aimed at investigating the potential for the implementation of systematic software reuse in a project-centric company. The study confirmed the need for systematic software reuse and identified the reuse issues that could present challenges. The study also revealed a number of problems relating to the project-centric structure for which systematic reuse provides potential solutions.},
booktitle = {Proceedings of the 2008 Annual Research Conference of the South African Institute of Computer Scientists and Information Technologists on IT Research in Developing Countries: Riding the Wave of Technology},
pages = {16–26},
numpages = {11},
keywords = {action research, ethnography, interpretive case study, project-centric, software product line engineering, software product lines, software reuse, systematic software reuse},
location = {Wilderness, South Africa},
series = {SAICSIT '08}
}

@inproceedings{10.1145/3610579.3611091,
author = {Spieck, Jan and Sixdenier, Pierre-Louis and Esper, Khalil and Wildermann, Stefan and Teich, J\"{u}rgen},
title = {Hybrid Genetic Reinforcement Learning for Generating Run-Time Requirement Enforcers},
year = {2023},
isbn = {9798400703188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610579.3611091},
doi = {10.1145/3610579.3611091},
abstract = {When designing embedded systems, engineers have to consider non-functional requirements, such as real-time or energy consumption constraints. To enforce or counteract any potential violation of such constraints, feedback-based control techniques can be applied, e.g., adapting the degree of parallelism or changing the DVFS settings of the resources. Of particular interest here are formal techniques for proving that the developed controllers either never lead to a violation of a given set of non-functional requirements or minimize the probability of such violations occurring. In the context of run-time requirement enforcement, it has been shown that either property can be described as one or a set of verification goals of a given or generated enforcement strategy. In this paper, we propose a design space exploration (DSE) methodology to determine a Pareto-optimal set of verifiable FSM-based feedback-based enforcers for a given set of verification goals. A major problem encountered here is that formally checking a set of verification goals can be quite time-intensive and, as a consequence, may lead to intolerably high exploration times. As a remedy, this paper proposes a hybrid DSE methodology based on a combination of multi-objective evolutionary algorithm search and reinforcement learning (RL).In particular, RL is used in each iteration of the evolutionary algorithm as a local search strategy to efficiently identify and fill gaps of diversity in the front of non-dominated solutions. It is shown that this leads to drastic reductions in exploration time. In three case studies, we compare the proposed approach with state-of-the-art methods and demonstrate considerably smaller optimization times alongside its capability to generate controllers exhibiting higher probabilities of satisfying a given set of requirement verification goals, as verified by model checkers.},
booktitle = {Proceedings of the 21st ACM-IEEE International Conference on Formal Methods and Models for System Design},
pages = {23–35},
numpages = {13},
keywords = {control, runtime requirement enforcement, verification, reinforcement learning, design space exploration, MPSoC},
location = {Hamburg, Germany},
series = {MEMOCODE '23}
}

@inproceedings{10.1145/2491956.2491976,
author = {Bodden, Eric and Tol\^{e}do, T\'{a}rsis and Ribeiro, M\'{a}rcio and Brabrand, Claus and Borba, Paulo and Mezini, Mira},
title = {SPLLIFT: statically analyzing software product lines in minutes instead of years},
year = {2013},
isbn = {9781450320146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491956.2491976},
doi = {10.1145/2491956.2491976},
abstract = {A software product line (SPL) encodes a potentially large variety of software products as variants of some common code base. Up until now, re-using traditional static analyses for SPLs was virtually intractable, as it required programmers to generate and analyze all products individually. In this work, however, we show how an important class of existing inter-procedural static analyses can be transparently lifted to SPLs. Without requiring programmers to change a single line of code, our approach SPLLIFT automatically converts any analysis formulated for traditional programs within the popular IFDS framework for inter-procedural, finite, distributive, subset problems to an SPL-aware analysis formulated in the IDE framework, a well-known extension to IFDS. Using a full implementation based on Heros, Soot, CIDE and JavaBDD, we show that with SPLLIFT one can reuse IFDS-based analyses without changing a single line of code. Through experiments using three static analyses applied to four Java-based product lines, we were able to show that our approach produces correct results and outperforms the traditional approach by several orders of magnitude.},
booktitle = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {355–364},
numpages = {10},
keywords = {context sensitive, flow sensitive, inter-procedural static analysis, software product lines},
location = {Seattle, Washington, USA},
series = {PLDI '13}
}

@inproceedings{10.1145/1837154.1837157,
author = {Siegmund, Norbert and Feigenspan, Janet and Soffner, Michael and Fruth, Jana and K\"{o}ppen, Veit},
title = {Challenges of secure and reliable data management in heterogeneous environments},
year = {2010},
isbn = {9781605589923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1837154.1837157},
doi = {10.1145/1837154.1837157},
abstract = {Ubiquitous computing is getting more important since requirements for complex systems grow fast. In these systems, embedded devices have to fulfill different tasks. They have to monitor the environment, store data, communicate with other devices, and react to user input. In addition to this complexity, quality issues such as security and reliability have to be considered, as well, due to their increasing use in life critical application scenarios. Finally, different devices with different application goals are used, which results in interoperability problems. In this paper, we highlight challenges for interoperability, data management, and security, which arise with complex systems. Furthermore, we present approaches to overcome different problems and how an integrated solution can be realized using software product line techniques.},
booktitle = {Proceedings of the First International Workshop on Digital Engineering},
pages = {17–24},
numpages = {8},
keywords = {data management, digital engineering, security, software product lines},
location = {Magdeburg, Germany},
series = {IWDE '10}
}

@inproceedings{10.1145/2485922.2485944,
author = {Wu, Lisa and Barker, Raymond J. and Kim, Martha A. and Ross, Kenneth A.},
title = {Navigating big data with high-throughput, energy-efficient data partitioning},
year = {2013},
isbn = {9781450320795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2485922.2485944},
doi = {10.1145/2485922.2485944},
abstract = {The global pool of data is growing at 2.5 quintillion bytes per day, with 90% of it produced in the last two years alone [24]. There is no doubt the era of big data has arrived. This paper explores targeted deployment of hardware accelerators to improve the throughput and energy efficiency of large-scale data processing. In particular, data partitioning is a critical operation for manipulating large data sets. It is often the limiting factor in database performance and represents a significant fraction of the overall runtime of large data queries.To accelerate partitioning, this paper describes a hardware accelerator for range partitioning, or HARP, and a hardware-software data streaming framework. The streaming framework offers a seamless execution environment for streaming accelerators such as HARP. Together, HARP and the streaming framework provide an order of magnitude improvement in partitioning performance and energy. A detailed analysis of a 32nm physical design shows 7.8 times the throughput of a highly optimized and optimistic software implementation, while consuming just 6.9% of the area and 4.3% of the power of a single Xeon core in the same technology generation.},
booktitle = {Proceedings of the 40th Annual International Symposium on Computer Architecture},
pages = {249–260},
numpages = {12},
keywords = {accelerator, data partitioning, microarchitecture, specialized functional unit, streaming data},
location = {Tel-Aviv, Israel},
series = {ISCA '13}
}

@inproceedings{10.1145/1960275.1960284,
author = {Kim, Chang Hwan Peter and Batory, Don S. and Khurshid, Sarfraz},
title = {Reducing combinatorics in testing product lines},
year = {2011},
isbn = {9781450306058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1960275.1960284},
doi = {10.1145/1960275.1960284},
abstract = {A Software Product Line (SPL) is a family of programs where each program is defined by a unique combination of features. Testing or checking properties of an SPL is hard as it may require the examination of a combinatorial number of programs. In reality, however, features are often irrelevant for a given test - they augment, but do not change, existing behavior, making many feature combinations unnecessary as far as testing is concerned. In this paper we show how to reduce the amount of effort in testing an SPL. We represent an SPL in a form where conventional static program analysis techniques can be applied to find irrelevant features for a test. We use this information to reduce the combinatorial number of SPL programs to examine.},
booktitle = {Proceedings of the Tenth International Conference on Aspect-Oriented Software Development},
pages = {57–68},
numpages = {12},
keywords = {feature oriented programming, software product lines, static analysis, testing},
location = {Porto de Galinhas, Brazil},
series = {AOSD '11}
}

@inproceedings{10.1145/2993236.2993252,
author = {Rothberg, Valentin and Dietrich, Christian and Ziegler, Andreas and Lohmann, Daniel},
title = {Towards scalable configuration testing in variable software},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993252},
doi = {10.1145/2993236.2993252},
abstract = {Testing a software product line such as Linux implies building the source with different configurations. Manual approaches to generate configurations that enable code of interest are doomed to fail due to the high amount of variation points distributed over the feature model, the build system and the source code. Research has proposed various approaches to generate covering configurations, but the algorithms show many drawbacks related to run-time, exhaustiveness and the amount of generated configurations. Hence, analyzing an entire Linux source can yield more than 30 thousand configurations and thereby exceeds the limited budget and resources for build testing.  In this paper, we present an approach to fill the gap between a systematic generation of configurations and the necessity to fully build software in order to test it. By merging previously generated configurations, we reduce the number of necessary builds and enable global variability-aware testing. We reduce the problem of merging configurations to finding maximum cliques in a graph. We evaluate the approach on the Linux kernel, compare the results to common practices in industry, and show that our implementation scales even when facing graphs with millions of edges.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {156–167},
numpages = {12},
keywords = {Configurability, Linux, Sampling, Software Product Lines, Software Testing},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@inproceedings{10.1145/2491411.2491459,
author = {Kim, Chang Hwan Peter and Marinov, Darko and Khurshid, Sarfraz and Batory, Don and Souto, Sabrina and Barros, Paulo and D'Amorim, Marcelo},
title = {SPLat: lightweight dynamic analysis for reducing combinatorics in testing configurable systems},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491459},
doi = {10.1145/2491411.2491459},
abstract = {Many programs can be configured through dynamic and/or static selection of configuration variables. A software product line (SPL), for example, specifies a family of programs where each program is defined by a unique combination of features. Systematically testing SPL programs is expensive as it can require running each test against a combinatorial number of configurations. Fortunately, a test is often independent of many configuration variables and need not be run against every combination. Configurations that are not required for a test can be pruned from execution. This paper presents SPLat, a new way to dynamically prune irrelevant configurations: the configurations to run for a test can be determined during test execution by monitoring accesses to configuration variables. SPLat achieves an optimal reduction in the number of configurations and is lightweight compared to prior work that used static analysis and heavyweight dynamic execution. Experimental results on 10 SPLs written in Java show that SPLat substantially reduces the total test execution time in many cases. Moreover, we demonstrate the scalability of SPLat by applying it to a large industrial code base written in Ruby on Rails.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {257–267},
numpages = {11},
keywords = {Automated testing, Configurable Systems, Efficiency, Software Product Lines},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inproceedings{10.5555/2693848.2694323,
author = {Salemi, Peter and Nelson, Barry L. and Staum, Jeremy},
title = {Discrete optimization via simulation using gaussian markov random fields},
year = {2014},
publisher = {IEEE Press},
abstract = {We construct a discrete optimization via simulation (DOvS) procedure using discrete Gaussian Markov random fields (GMRFs). Gaussian random fields (GRFs) are used in DOvS to balance exploration and exploitation. They enable computation of the expected improvement (EI) due to running the simulation to evaluate a feasible point of the optimization problem. Existing methods use GRFs with a continuous domain, which leads to dense covariance matrices, and therefore can be ill-suited for large-scale problems due to slow and ill-conditioned numerical computations. The use of GMRFs leads to sparse precision matrices, on which several sparse matrix techniques can be applied. To allocate the simulation effort throughout the procedure, we introduce a new EI criterion that incorporates the uncertainty in stochastic simulation by treating the value at the current optimal point as a random variable.},
booktitle = {Proceedings of the 2014 Winter Simulation Conference},
pages = {3809–3820},
numpages = {12},
location = {Savannah, Georgia},
series = {WSC '14}
}

@inproceedings{10.1145/3458817.3476160,
author = {Shang, Honghui and Li, Fang and Zhang, Yunquan and Liu, Ying and Zhang, Libo and Wu, Mingchuan and Wu, Yangjun and Wei, Di and Cui, Huimin and Liu, Xin and Wang, Fei and Ye, Yuxi and Gao, Yingxiang and Ni, Shuang and Chen, Xin and Chen, Dexun},
title = {Accelerating all-electron ab initio simulation of raman spectra for biological systems},
year = {2021},
isbn = {9781450384421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458817.3476160},
doi = {10.1145/3458817.3476160},
abstract = {Raman spectroscopy provides chemical and compositional information that can serve as a structural fingerprint for various materials. Therefore, simulations of Raman spectra, including both quantum perturbation analyses and ground-state calculations are of significant interest. However, highly accurate full quantum mechanical (QM) simulations of Raman spectra have previously been confined to small systems. For large systems such as biological materials, the computational cost of full QM simulations is extremely high, and their extension to such systems remains challenging. In the work described here, by employing robust new algorithms and advances in implementation for the many-core architectures, we are able to perform fast, accurate, and massively parallel full ab initio simulations of the Raman spectra of biological systems with excellent strong and weak scaling, thereby providing a starting point for applying QM approaches to structural studies of such systems.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {41},
numpages = {15},
keywords = {all-electron, biological systems, many-core processor, quantum mechanics, scalability},
location = {St. Louis, Missouri},
series = {SC '21}
}

@inproceedings{10.1145/2110147.2110160,
author = {Schroeter, Julia and Cech, Sebastian and G\"{o}tz, Sebastian and Wilke, Claas and A\ss{}mann, Uwe},
title = {Towards modeling a variable architecture for multi-tenant SaaS-applications},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110160},
doi = {10.1145/2110147.2110160},
abstract = {A widespread business model in cloud computing is to offer software as a service (SaaS) over the Internet. Such applications are often multi-tenant aware, which means that multiple tenants share hardware and software resources of the same application instance. However, SaaS stakeholders have different or even contradictious requirements and interests: For a user, the application's quality and non-functional properties have to be maximized (e.g., choosing the fastest available algorithm for a computation at runtime). In contrast, a resource or application provider is interested in minimizing the operating costs while maximizing his profit. Finally, tenants are interested in offering a customized functionality to their users. To identify an optimal compromise for all these objectives, multiple levels of variability have to be supported by reference architectures for multi-tenant SaaS applications. In this paper, we identify requirements for such a runtime architecture addressing the individual interests of all involved stakeholders. Furthermore, we show how our existing architecture for dynamically adaptive applications can be extended for the development and operation of multi-tenant applications.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {111–120},
numpages = {10},
keywords = {auto-tuning, multi-tenancy, self-optimization, software-as-a-service, variability modeling},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@inproceedings{10.1145/2430502.2430511,
author = {Kolesnikov, Sergiy S. and Apel, Sven and Siegmund, Norbert and Sobernig, Stefan and K\"{a}stner, Christian and Senkaya, Semah},
title = {Predicting quality attributes of software product lines using software and network measures and sampling},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430511},
doi = {10.1145/2430502.2430511},
abstract = {Software product-line engineering aims at developing families of related products that share common assets to provide customers with tailor-made products. Customers are often interested not only in particular functionalities (i.e., features), but also in non-functional quality attributes, such as performance, reliability, and footprint. Measuring quality attributes of all products of a product line usually does not scale. In this research-in-progress report, we propose a systematic approach aiming at efficient and scalable prediction of quality attributes of products. To this end, we establish predictors for certain categories of quality attributes (e.g., a predictor for high memory consumption) based on software and network measures, and receiver operating characteristic analysis. We use these predictors to guide a sampling process that takes the assets of a product line as input and determines the products that fall into the category denoted by the given predictor (e.g., products with high memory consumption). We propose to use predictors to make the process of finding "acceptable" products more efficient. We discuss and compare several strategies to incorporate predictors in the sampling process.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {6},
numpages = {5},
keywords = {metrics, prediction, quality attributes, sampling, software product lines},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@inproceedings{10.1145/3581783.3612113,
author = {Wu, Yixuan and Chen, Jintai and Yan, Jiahuan and Zhu, Yiheng and Chen, Danny Z. and Wu, Jian},
title = {GCL: Gradient-Guided Contrastive Learning for Medical Image Segmentation with Multi-Perspective Meta Labels},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612113},
doi = {10.1145/3581783.3612113},
abstract = {Since annotating medical images for segmentation tasks commonly incurs expensive costs, it is highly desirable to design an annotation-efficient method to alleviate the annotation burden. Recently, contrastive learning has exhibited a great potential in learning robust representations to boost downstream tasks with limited labels. In medical imaging scenarios, ready-made meta labels (i.e., specific attribute information of medical images) inherently reveal semantic relationships among images, which have been used to define positive pairs in previous work. However, the multi-perspective semantics revealed by various meta labels are usually incompatible and can incur intractable "semantic contradiction" when combining different meta labels. In this paper, we tackle the issue of "semantic contradiction" in a gradient-guided manner using our proposed Gradient Mitigator method, which systematically unifies multi-perspective meta labels to enable a pre-trained model to attain a better high-level semantic recognition ability. Moreover, we emphasize that the fine-grained discrimination ability is vital for segmentation-oriented pre-training, and develop a novel method called Gradient Filter to dynamically screen pixel pairs with the most discriminating power based on the magnitude of gradients. Comprehensive experiments on four medical image segmentation datasets verify that our new method GCL: (1) learns informative image representations and considerably boosts segmentation performance with limited labels, and (2) shows promising generalizability on out-of-distribution datasets.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {463–471},
numpages = {9},
keywords = {medical pre-training, multi-perspective meta labels, optimization},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/1774088.1774566,
author = {Ballance, Robert A. and Cook, Jonathan},
title = {Monitoring MPI programs for performance characterization and management control},
year = {2010},
isbn = {9781605586397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1774088.1774566},
doi = {10.1145/1774088.1774566},
abstract = {Monitoring distributed programs on high performance supercomputers is a challenging task, yet it is essential for the proper administration of the machines and for users to understand what their program is doing on production runs. To this end, we created a flexible monitoring capability for a major class of scientific applications, programs using MPI, that efficiently gathers information from the distributed program and collects it at a central point. This data can then be used to both understand application-centric issues and system-centric issues; and for improvement, administration, and maintenance of both the complex applications producing important scientific results and the complex systems that execute them.},
booktitle = {Proceedings of the 2010 ACM Symposium on Applied Computing},
pages = {2305–2310},
numpages = {6},
keywords = {scientific applications, software monitoring},
location = {Sierre, Switzerland},
series = {SAC '10}
}

@inproceedings{10.1145/3624007.3624060,
author = {Correa Restrepo, Camilo and Robin, Jacques and Mazo, Raul},
title = {Generating Constraint Programs for Variability Model Reasoning: A DSL and Solver-Agnostic Approach},
year = {2023},
isbn = {9798400704062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624007.3624060},
doi = {10.1145/3624007.3624060},
abstract = {Verifying and configuring large Software Product Lines (SPL) requires automation tools. Current state-of-the-art approaches involve translating variability models into a formalism accepted as input by a constraint solver. There are currently no standards for variability modeling languages (VML). There is also a variety of constraint solver input languages. This has resulted in a multiplication of ad-hoc architectures and tools specialized for a single pair of VML and solver, fragmenting the SPL community. To overcome this limitation, we propose a novel architecture based on model-driven code generation, where the syntax and semantics of VMLs can be declaratively specified as data, and a standard, human-readable, formal pivot language is used between the VML and the solver input language. This architecture is the first to be fully generic by being agnostic to both VML and the solver paradigm. To validate the genericity of the approach, we have implemented a prototype tool together with declarative specifications for the syntax and semantics of two different VMLs and two different solver families. One VML is for classic, static SPL, and the other for run-time reconfigurable dynamic SPL with soft constraints to be optimized during configuration. The two solver families are Constraint Satisfaction Programs (CSP) and Constraint Logic Programs (CLP).},
booktitle = {Proceedings of the 22nd ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {138–152},
numpages = {15},
keywords = {Automated Reasoning, Configuration Automation, Generic Architecture, Software Product Lines},
location = {Cascais, Portugal},
series = {GPCE 2023}
}

@inproceedings{10.1145/1409720.1409748,
author = {Nestor, Daren and Thiel, Steffen and Botterweck, Goetz and Cawley, Ciar\'{a}n and Healy, Patrick},
title = {Applying visualisation techniques in software product lines},
year = {2008},
isbn = {9781605581125},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1409720.1409748},
doi = {10.1145/1409720.1409748},
abstract = {Software product lines of industrial size can easily incorporate thousands of variation points. This scale of variability can become extremely complex to manage resulting in a product development process that bears significant costs. One technique that can be applied beneficially in this context is visualisation. Visualisation is widely used in software engineering and has proven useful to amplify human cognition in data intensive applications. Adopting this technique in software product line engineering can help stakeholders in supporting essential work tasks and in enhancing their understanding of large and complex product lines.The research presented in this paper describes an integrated meta-model and research tool that employs visualisation techniques to address significant software product line tasks such as variability management and product derivation. Examples of the tasks are described and the ways in which these tasks can be further supported by utilising visualisation techniques are explained.},
booktitle = {Proceedings of the 4th ACM Symposium on Software Visualization},
pages = {175–184},
numpages = {10},
keywords = {feature configuration, interaction, software product lines, visualisation},
location = {Ammersee, Germany},
series = {SoftVis '08}
}

@inproceedings{10.1145/2188286.2188347,
author = {Dayarathna, Miyuru and Suzumura, Toyotaro},
title = {Hirundo: a mechanism for automated production of optimized data stream graphs},
year = {2012},
isbn = {9781450312028},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2188286.2188347},
doi = {10.1145/2188286.2188347},
abstract = {Stream programs have to be crafted carefully to maximize the performance gain that can be obtained from stream processing environments. Manual fine tuning of a stream program is a very difficult process which requires considerable amount of programmer time and expertise. In this paper we present Hirundo, which is a mechanism for automatically generating optimized stream programs that are tailored for the environment they run. Hirundo analyzes, identifies the structure of a stream program, and transforms it to many different sample programs with same semantics using the notions of Tri-Operator Transformation, Transformer Blocks, and Operator Blocks Fusion. Then it uses empirical optimization information to identify a small subset of generated sample programs that could deliver high performance. It runs the selected sample programs in the run-time environment for a short period of time to obtain their performance information. Hirundo utilizes these information to output a ranked list of optimized stream programs that are tailored for a particular run-time environment. Hirundo has been developed using Python as a prototype application for optimizing SPADE programs, which run on System S stream processing run-time. Using three example real world stream processing applications we demonstrate effectiveness of our approach, and discuss how well it generalizes for automatic stream program performance optimization.},
booktitle = {Proceedings of the 3rd ACM/SPEC International Conference on Performance Engineering},
pages = {335–346},
numpages = {12},
keywords = {data-intensive computing, fault tolerance, performance optimization, scalability, stream processing},
location = {Boston, Massachusetts, USA},
series = {ICPE '12}
}

@inproceedings{10.5555/318773.319262,
author = {Bayer, Joachim and Girard, Jean-Fran\c{c}ois and W\"{u}rthner, Martin and DeBaud, Jean-Marc and Apel, Martin},
title = {Transitioning legacy assets to a product line architecture},
year = {1999},
isbn = {3540665382},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A successful software system evolves over time, but this evolution often occurs in an ad-hoc fashion. One approach to structure system evolution is the concept of software product lines where a core architecture supports a variety of application contexts. However, in practice, the high cost and high risks of redevelopment as well as the substantial investments made to develop the existing systems most often mandate significant leverage of the legacy assets. Yet, there is little guidance in the literature on how to transition legacy assets into a product line set-up.In this paper, we present RE-PLACE, an approach developed to support the transition of existing software assets towards a product line architecture while taking into account anticipated new system variants. We illustrate this approach with its application in an industrial setting.},
booktitle = {Proceedings of the 7th European Software Engineering Conference Held Jointly with the 7th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {446–463},
numpages = {18},
keywords = {architecture recovery, domain-specific software architecture, reengineering, reuse, software product line},
location = {Toulouse, France},
series = {ESEC/FSE-7}
}

@inproceedings{10.1145/2897053.2897058,
author = {Sharifloo, Amir Molzam and Metzger, Andreas and Quinton, Cl\'{e}ment and Baresi, Luciano and Pohl, Klaus},
title = {Learning and evolution in dynamic software product lines},
year = {2016},
isbn = {9781450341875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897053.2897058},
doi = {10.1145/2897053.2897058},
abstract = {A Dynamic Software Product Line (DSPL) aims at managing run-time adaptations of a software system. It is built on the assumption that context changes that require these adaptations at run-time can be anticipated at design-time. Therefore, the set of adaptation rules and the space of configurations in a DSPL are predefined and fixed at design-time. Yet, for large-scale and highly distributed systems, anticipating all relevant context changes during design-time is often not possible due to the uncertainty of how the context may change. Such design-time uncertainty therefore may mean that a DSPL lacks adaptation rules or configurations to properly reconfigure itself at run-time. We propose an adaptive system model to cope with design-time uncertainty in DSPLs. This model combines learning of adaptation rules with evolution of the DSPL configuration space. It takes particular account of the mutual dependencies between evolution and learning, such as using feedback from unsuccessful learning to trigger evolution. We describe concrete steps for learning and evolution to show how such feedback can be exploited. We illustrate the use of such a model with a running example from the cloud computing domain.},
booktitle = {Proceedings of the 11th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {158–164},
numpages = {7},
keywords = {adaptation, dynamic software product lines, evolution, machine learning},
location = {Austin, Texas},
series = {SEAMS '16}
}

@inproceedings{10.1145/2739480.2754720,
author = {Assun\c{c}\~{a}o, Wesley K.G. and Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Vergilio, Silvia R. and Egyed, Alexander},
title = {Extracting Variability-Safe Feature Models from Source Code Dependencies in System Variants},
year = {2015},
isbn = {9781450334723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739480.2754720},
doi = {10.1145/2739480.2754720},
abstract = {To effectively cope with increasing customization demands, companies that have developed variants of software systems are faced with the challenge of consolidating all the variants into a Software Product Line, a proven development paradigm capable of handling such demands. A crucial step in this challenge is to reverse engineer feature models that capture all the required feature combinations of each system variant. Current research has explored this task using propositional logic, natural language, and search-based techniques. However, using knowledge from the implementation artifacts for the reverse engineering task has not been studied. We propose a multi-objective approach that not only uses standard precision and recall metrics for the combinations of features but that also considers variability-safety, i.e. the property that, based on structural dependencies among elements of implementation artifacts, asserts whether all feature combinations of a feature model are in fact well-formed software systems. We evaluate our approach with five case studies and highlight its benefits for the software engineer.},
booktitle = {Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1303–1310},
numpages = {8},
keywords = {feature models, multi-objective evolutionary algorithms, reverse engineering},
location = {Madrid, Spain},
series = {GECCO '15}
}

@inproceedings{10.1145/2993236.2993253,
author = {Al-Hajjaji, Mustafa and Krieter, Sebastian and Th\"{u}m, Thomas and Lochau, Malte and Saake, Gunter},
title = {IncLing: efficient product-line testing using incremental pairwise sampling},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993253},
doi = {10.1145/2993236.2993253},
abstract = {A software product line comprises a family of software products that share a common set of features. It enables customers to compose software systems from a managed set of features. Testing every product of a product line individually is often infeasible due to the exponential number of possible products in the number of features. Several approaches have been proposed to restrict the number of products to be tested by sampling a subset of products achieving sufficient combinatorial interaction coverage. However, existing sampling algorithms do not scale well to large product lines, as they require a considerable amount of time to generate the samples. Moreover, samples are not available until a sampling algorithm completely terminates. As testing time is usually limited, we propose an incremental approach of product sampling for pairwise interaction testing (called IncLing), which enables developers to generate samples on demand in a step-wise manner. Furthermore, IncLing uses heuristics to efficiently achieve pairwise interaction coverage with a reasonable number of products. We evaluated IncLing by comparing it against existing sampling algorithms using feature models of different sizes. The results of our approach indicate efficiency improvements for product-line testing.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {144–155},
numpages = {12},
keywords = {Software product lines, combinatorial interaction testing, model-based testing, sampling},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@inproceedings{10.1145/3430524.3440620,
author = {Morales Gonz\'{a}lez, Rafael and Marzo, Asier and Freeman, Euan and Frier, William and Georgiou, Orestis},
title = {UltraPower: Powering Tangible &amp; Wearable Devices with Focused Ultrasound},
year = {2021},
isbn = {9781450382137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430524.3440620},
doi = {10.1145/3430524.3440620},
abstract = {Wireless power transfer creates new opportunities for interaction with tangible and wearable devices, by freeing designers from the constraints of an integrated power source. We explore the use of focused ultrasound as a means of transferring power to a distal device, transforming passive props into dynamic active objects. We analyse the ability to transfer power from an ultrasound array commonly used for mid-air haptic feedback and investigate the practical challenges of ultrasonic power transfer (e.g., receiving and rectifying energy from sound waves). We also explore the ability to power electronic components and multimodal actuators such as lights, speakers and motors. Finally, we describe exemplar wearable and tangible device prototypes that are activated by UltraPower, illustrating the potential applications of this novel technology.},
booktitle = {Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction},
articleno = {1},
numpages = {13},
keywords = {Energy, Tangible Device, Ultrasound, Wearable Device, Wireless Power Transfer},
location = {Salzburg, Austria},
series = {TEI '21}
}

@inproceedings{10.1145/2814228.2814229,
author = {Arzt, Steven and Nadi, Sarah and Ali, Karim and Bodden, Eric and Erdweg, Sebastian and Mezini, Mira},
title = {Towards secure integration of cryptographic software},
year = {2015},
isbn = {9781450336888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814228.2814229},
doi = {10.1145/2814228.2814229},
abstract = {While cryptography is now readily available to everyone and can, provably, protect private information from attackers, we still frequently hear about major data leakages, many of which are due to improper use of cryptographic mechanisms. The problem is that many application developers are not cryptographic experts. Even though high-quality cryptographic APIs are widely available, programmers often select the wrong algorithms or misuse APIs due to a lack of understanding. Such issues arise with both simple operations such as encryption as well as with complex secure communication protocols such as SSL. In this paper, we provide a long-term solution that helps application developers integrate cryptographic components correctly and securely by bridging the gap between cryptographers and application developers. Our solution consists of a software product line (with an underlying feature model) that automatically identifies the correct cryptographic algorithms to use, based on the developer's answers to high-level questions in non-expert terminology. Each feature (i.e., cryptographic algorithm) maps into corresponding Java code and a usage protocol describing API restrictions. By composing the user's selected features, we automatically synthesize a secure code blueprint and a usage protocol that corresponds to the selected usage scenario. Since the developer may change the application code over time, we use the usage protocols to statically analyze the program and ensure that the correct use of the API is not violated over time.},
booktitle = {2015 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (Onward!)},
pages = {1–13},
numpages = {13},
keywords = {API protocols, Software product lines, cryptography, typestate analysis},
location = {Pittsburgh, PA, USA},
series = {Onward! 2015}
}

@inproceedings{10.1145/2491411.2491455,
author = {Davril, Jean-Marc and Delfosse, Edouard and Hariri, Negar and Acher, Mathieu and Cleland-Huang, Jane and Heymans, Patrick},
title = {Feature model extraction from large collections of informal product descriptions},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491455},
doi = {10.1145/2491411.2491455},
abstract = {Feature Models (FMs) are used extensively in software product line engineering to help generate and validate individual product configurations and to provide support for domain analysis. As FM construction can be tedious and time-consuming, researchers have previously developed techniques for extracting FMs from sets of formally specified individual configurations, or from software requirements specifications for families of existing products. However, such artifacts are often not available. In this paper we present a novel, automated approach for constructing FMs from publicly available product descriptions found in online product repositories and marketing websites such as SoftPedia and CNET. While each individual product description provides only a partial view of features in the domain, a large set of descriptions can provide fairly comprehensive coverage. Our approach utilizes hundreds of partial product descriptions to construct an FM and is described and evaluated against antivirus product descriptions mined from SoftPedia.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {290–300},
numpages = {11},
keywords = {Domain Analysis, Feature Models, Product Lines},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inproceedings{10.1145/1531542.1531590,
author = {Jahanian, Ali and Saheb Zamani, Morteza},
title = {Improved performance and yield with chip master planning design methodology},
year = {2009},
isbn = {9781605585222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1531542.1531590},
doi = {10.1145/1531542.1531590},
abstract = {Mis-prediction is a dominant problem in nano-scale design that may diminish the quality of physical design algorithms or may even result in failing the design cycle convergence. In this paper, a new planning methodology is presented in which a masterplan of the chip is constructed in early levels of physical design and the rest of succeeding physical design stags operate considering this masterplan. The proposed planning design flow is used to wire planning and buffer resource planning in order to compare with conventional contributions. Experimental results show the considerable improvements in terms of performance, timing yield and buffer usage.},
booktitle = {Proceedings of the 19th ACM Great Lakes Symposium on VLSI},
pages = {185–190},
numpages = {6},
keywords = {chip planning, highway on chip, interconnect planning},
location = {Boston Area, MA, USA},
series = {GLSVLSI '09}
}

@inproceedings{10.1109/ICSE.2009.5070526,
author = {Th\"{u}m, Thomas and Batory, Don and K\"{a}stner, Christian},
title = {Reasoning about edits to feature models},
year = {2009},
isbn = {9781424434534},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSE.2009.5070526},
doi = {10.1109/ICSE.2009.5070526},
abstract = {Features express the variabilities and commonalities among programs in a software product line (SPL). A feature model defines the valid combinations of features, where each combination corresponds to a program in an SPL. SPLs and their feature models evolve over time. We classify the evolution of a feature model via modifications as refactorings, specializations, generalizations, or arbitrary edits. We present an algorithm to reason about feature model edits to help designers determine how the program membership of an SPL has changed. Our algorithm takes two feature models as input (before and after edit versions), where the set of features in both models are not necessarily the same, and it automatically computes the change classification. Our algorithm is able to give examples of added or deleted products and efficiently classifies edits to even large models that have thousands of features.},
booktitle = {Proceedings of the 31st International Conference on Software Engineering},
pages = {254–264},
numpages = {11},
series = {ICSE '09}
}

@inproceedings{10.1145/1985793.1985838,
author = {Classen, Andreas and Heymans, Patrick and Schobbens, Pierre-Yves and Legay, Axel},
title = {Symbolic model checking of software product lines},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1985838},
doi = {10.1145/1985793.1985838},
abstract = {We study the problem of model checking software product line (SPL) behaviours against temporal properties. This is more difficult than for single systems because an SPL with n features yields up to 2n individual systems to verify. As each individual verification suffers from state explosion, it is crucial to propose efficient formalisms and heuristics.We recently proposed featured transition systems (FTS), a compact representation for SPL behaviour, and defined algorithms for model checking FTS against linear temporal properties. Although they showed to outperform individual system verifications, they still face a state explosion problem as they enumerate and visit system states one by one.In this paper, we tackle this latter problem by using symbolic representations of the state space. This lead us to consider computation tree logic (CTL) which is supported by the industry-strength symbolic model checker NuSMV. We first lay the foundations for symbolic SPL model checking by defining a feature-oriented version of CTL and its dedicated algorithms. We then describe an implementation that adapts the NuSMV language and tool infrastructure. Finally, we propose theoretical and empirical evaluations of our results. The benchmarks show that for certain properties, our algorithm is over a hundred times faster than model checking each system with the standard algorithm.},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {321–330},
numpages = {10},
keywords = {features, software product lines, specification},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@inproceedings{10.1145/2695664.2695743,
author = {Garcia, Cleiton and Paludo, Marco and Malucelli, Andreia and Reinehr, Sheila},
title = {A software process line for service-oriented applications},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695743},
doi = {10.1145/2695664.2695743},
abstract = {The management of processes and systems is a complex and time-consuming activity for organizations and also an ongoing Information Technology (IT) challenge. Among the different approaches for bringing flexibility to the business processes and systems are Service-Oriented Architecture (SOA) and Business Process Management (BPM). The SOA approach has become popular providing services and interfaces, enabling integration of heterogeneous and distributed platforms and BPM leverages the cycles of improvements, control and evaluation of business processes. BPM and SOA should work together aiming at improving business processes and evolving systems architecture. One main problem to apply BPM and SOA is the lack of established processes and this work proposes a software process line in order to simplify variability control and enable the instantiation of new development process applying BPM and SOA. It also aims at developing an environment to support the proposed software process line in order to automate the process, integrating industrial tools with one specifically developed to perform the transformation of UMA models into BPM notation. The main contribution of this work is the definition of the software process line for engineering service-oriented products. It is highly relevant to software industry since software process lines lacks experiments, practices and tools.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1680–1687},
numpages = {8},
keywords = {BPM, SOA, SPL, software process lines, web-based services},
location = {Salamanca, Spain},
series = {SAC '15}
}

@article{10.1109/TASLP.2013.2290502,
author = {Cosentino, Stefano and Falk, Tiago H. and McAlpine, David and Marquardt, Torsten},
title = {Cochlear Implant Filterbank Design and Optimization: A Simulation Study},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2290502},
doi = {10.1109/TASLP.2013.2290502},
abstract = {Cochlear implants (CIs) are devices capable of restoring hearing function in profoundly-deaf patients to an acceptable degree of performance. An essential processing step in any cochlear implant is frequency analysis, which is usually performed via banks of filters. Here, we simulate and test the suitability of different filters and filterbank architectures for CIs with respect to their performance in speech intelligibility. Four different filters were implemented in an established model of CI hearing, the tone-excited vocoder, namely: GTF (Gammatone Filter), DAPGF (Differentiated All-Pole GTF), OZGF (One-Zero GTF) and BUTF (Butterworth). Three filterbank parameters, the filter order ( N), the filter quality factor ( Q) and the number of channels ( Ch), and their combinations were tested using objective and subjective metrics. Simulation results show that all filters tested are suitable for CI implementation, but that the choice of Q and N parameter values is crucial. For most conditions, optimal ( N,Q) combinations were within few units away from the combination (2, 4).},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {347–353},
numpages = {7}
}

@article{10.1145/3514233,
author = {Chen, Tao and Li, Miqing},
title = {The Weights Can Be Harmful: Pareto Search versus Weighted Search in Multi-objective Search-based Software Engineering},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3514233},
doi = {10.1145/3514233},
abstract = {In presence of multiple objectives to be optimized in Search-Based Software Engineering (SBSE), Pareto search has been commonly adopted. It searches for a good approximation of the problem’s Pareto-optimal solutions, from which the stakeholders choose the most preferred solution according to their preferences. However, when clear preferences of the stakeholders (e.g., a set of weights that reflect relative importance between objectives) are available prior to the search, weighted search is believed to be the first choice, since it simplifies the search via converting the original multi-objective problem into a single-objective one and enables the search to focus on what only the stakeholders are interested in.This article questions such a “weighted search first” belief. We show that the weights can, in fact, be harmful to the search process even in the presence of clear preferences. Specifically, we conduct a large-scale empirical study that consists of 38 systems/projects from three representative SBSE problems, together with two types of search budget and nine sets of weights, leading to 604 cases of comparisons. Our key finding is that weighted search reaches a certain level of solution quality by consuming relatively less resources at the early stage of the search; however, Pareto search is significantly better than its weighted counterpart the majority of the time (up to 77% of the cases), as long as we allow a sufficient, but not unrealistic search budget. This is a beneficial result, as it discovers a potentially new “rule-of-thumb” for the SBSE community: Even when clear preferences are available, it is recommended to always consider Pareto search by default for multi-objective SBSE problems, provided that solution quality is more important. Weighted search, in contrast, should only be preferred when the resource/search budget is limited, especially for expensive SBSE problems. This, together with other findings and actionable suggestions in the article, allows us to codify pragmatic and comprehensive guidance on choosing weighted and Pareto search for SBSE under the circumstance that clear preferences are available. All code and data can be accessed at .},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {5},
numpages = {40},
keywords = {Search-based software engineering, multi-objective optimization, pareto optimization, quality evaluation, quality indicator, user preference, configurable systems, adaptive systems, self-adaptive systems}
}

@inproceedings{10.1145/3458359.3458363,
author = {Tang, Ke and Xu Ding, Jing and Jian Lei, Mei and Zeng, Zhen and Di Guo, Jun and Ling Zeng, Qiu},
title = {An Electromagnetic Immunity Test Scheme for Vehicle Audio System},
year = {2021},
isbn = {9781450389020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458359.3458363},
doi = {10.1145/3458359.3458363},
abstract = {With the emergence of in-vehicle electronic products and the rapid development of Telematics, the electromagnetic environment faced by vehicles has become more complex than ever, and the electromagnetic immunity of in-vehicle electronic products has a crucial impact on the driver's user experience and even road safety. So electromagnetic susceptibility (EMS) testing has become an urgent issue for the industry to face and solve. Besides EMS test, acoustic measurement should be paid attention to in-vehicle multimedia equipment audio performance. In this paper, an electromagnetic compatibility (EMC) test scheme for vehicle level audio quality is proposed. The EMC test platform is verified, the audio anti-interference test system is placed in an AF shielding box to test the different reverberation effects of the information received by human ear, and the Perceptual Objective Hearing Quality Assessment (POLQA) is introduced. The EMS test results show that it is feasible to use the MOS values of POLQA to evaluate the whole vehicle audio electromagnetic immunity.},
booktitle = {Proceedings of the 2021 10th International Conference on Informatics, Environment, Energy and Applications},
pages = {11–16},
numpages = {6},
keywords = {Audio quality, Automotive EMC, Electric vehicle, Vehicle acoustical environments},
location = {Xi'an, China},
series = {IEEA '21}
}

@inproceedings{10.1145/3368474.3368488,
author = {Takahashi, Daisuke and Franchetti, Franz},
title = {FFTE on SVE: SPIRAL-Generated Kernels},
year = {2020},
isbn = {9781450372367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368474.3368488},
doi = {10.1145/3368474.3368488},
abstract = {In this paper we propose an implementation of the fast Fourier transform (FFT) targeting the ARM Scalable Vector Extension (SVE). We performed automatic vectorization via a compiler and an explicit vectorization through code generation by SPIRAL for FFT kernels, and compared the performance. We show that the explicit vectorization of SPIRAL generated code improves performance significantly. Performance results of FFTs on RIKEN's Fugaku processor simulator are reported. With the ARM compiler SPIRAL-generated FFT kernels written in SVE intrinsic are up to 3.16 times faster than FFT kernels of FFTE written in Fortran and up to 5.62 times faster than SPIRAL-generated FFT kernels written in C.},
booktitle = {Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region},
pages = {114–122},
numpages = {9},
keywords = {ARM SVE, FFT, SPIRAL, vectorization},
location = {Fukuoka, Japan},
series = {HPCAsia '20}
}

@inproceedings{10.1145/3295500.3356138,
author = {Li, Zhihao and Jia, Haipeng and Zhang, Yunquan and Chen, Tun and Yuan, Liang and Cao, Luning and Wang, Xiao},
title = {AutoFFT: a template-based FFT codes auto-generation framework for ARM and X86 CPUs},
year = {2019},
isbn = {9781450362290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295500.3356138},
doi = {10.1145/3295500.3356138},
abstract = {The discrete Fourier transform (DFT) is widely used in scientific and engineering computation. This paper proposes a template-based code generation framework named AutoFFT that can automatically generate high-performance fast Fourier transform (FFT) codes. AutoFFT employs the Cooley-Tukey FFT algorithm, which exploits the symmetric and periodic properties of the DFT matrix as the outer parallelization framework. To further reduce the number of floating-point operations of butterflies, we explore more symmetric and periodic properties of the DFT matrix and formulate two optimized calculation templates for prime and power-of-two radices. To fully exploit hardware resources, we encapsulate a series of optimizations in an assembly template optimizer. Given any DFT problem, AutoFFT automatically generates C FFT kernels using these two templates and transfers them to efficient assembly codes using the template optimizer. Experiments show that AutoFFT outperforms FFTW, ARMPL, and Intel MKL on average across all FFT types on ARMv8 and Intel x86-64 processors.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {25},
numpages = {15},
keywords = {AutoFFT, DFT, FFT, code generation, template},
location = {Denver, Colorado},
series = {SC '19}
}

@inproceedings{10.1145/3368089.3409749,
author = {Shahbazian, Arman and Karthik, Suhrid and Brun, Yuriy and Medvidovic, Nenad},
title = {eQual: informing early design decisions},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409749},
doi = {10.1145/3368089.3409749},
abstract = {When designing a software system, architects make a series of design decisions that directly impact the system's quality. The number of available design alternatives grows rapidly with system size, creating an enormous space of intertwined design concerns that renders manual exploration impractical. We present eQual, a model-driven technique for simulation-based assessment of architectural designs. While it is not possible to guarantee optimal decisions so early in the design process, eQual improves decision quality. eQual is effective in practice because it (1) limits the amount of information the architects have to provide and (2) adapts optimization algorithms to effectively explore massive spaces of design alternatives. We empirically demonstrate that eQual yields designs whose quality is comparable to a set of systems' known optimal designs. A user study shows that, compared to the state-of-the-art, engineers using eQual produce statistically significantly higher-quality designs with a large effect size, are statistically significantly more confident in their designs, and find eQual easier to use.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1039–1051},
numpages = {13},
keywords = {Software design, design analysis, design decisions, optimization},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3350768.3351993,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Towards the support of user preferences in search-based product line architecture design: an exploratory study},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3351993},
doi = {10.1145/3350768.3351993},
abstract = {Software Product Lines (SPLs) is a reuse approach in which a family of products is generalized in a common architecture that can be adapted to different clients. The Product Line Architecture (PLA) is one of the most important artifacts of a SPL. PLA design requires great human effort as it involves several factors that are usually in conflict. To ease this task, PLA design can be formulated as an optimization problem with many factors, i.e, as a multi-objective optimization problem. In this context, the MOA4PLA approach was proposed to optimize PLA design using search algorithms and metrics specific to the context. This approach supported by OPLA-Tool has already been used in several works demonstrating its applicability. However, MOA4PLA does not take into account aspects that are subjective, such as the preferences of a particular Decision Maker (DM). To do so, this paper presents a proposal to incorporate the user preferences in the optimization process performed by MOA4PLA, through an interactive process in which the DM subjectively evaluates the solutions in processing time. Thus, the solutions generated can be better suited to the DM's needs or preferences. In order to allow the user interaction, modifications were made in MOA4PLA and implemented in the OPLA-Tool. Aiming at an initial validation of the proposal, an exploratory study was carried out, composed of two experiments: a qualitative and a quantitative. These experiments were realized with the participation of a software architect. Empirical results pointed out that the proposed interactive process enables the generation of PLAs that are in accordance with the architect's preferences. Another significant contribution are the lessons learned on how to improve the interactive process.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {387–396},
numpages = {10},
keywords = {Human-computer interaction, Multi-Objective Optimization, Product Line Architecture},
location = {Salvador, Brazil},
series = {SBES '19}
}

@inproceedings{10.1145/3562939.3565653,
author = {Uchihashi, Ryoto and Otsuka, Takumi and Murakami, Yuya and Yoshizawa, Ayaka and Kawashima, Takuya and Yamaguchi, Kaito and Ono, Genta and Matsuhashi, Tsukina and Yamada, Saki and Waguri, Manoka and Kamiyama, Youichi and Watanabe, Keita},
title = {TeleStick: Video Recording and Playing System Optimized for Tactile Interaction using a Stick},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565653},
doi = {10.1145/3562939.3565653},
abstract = {TeleStick is a system that records and replicates tactile experience and visual information using common types of camera and video monitor environment. TeleStick uses a stick-type device with a tactile microphone attached to a camera so that it is consistently visible in the video, and records video and with tactile and audio information using stereo 2ch. The users can feel as if they were in the video as they watch it while holding a stick-type device with a speaker and a vibrator.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {74},
numpages = {2},
keywords = {Camera, Cross-multimodal, Interaction, Tactile, Visual},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@article{10.1145/3714458,
author = {Li, Menglu and Ahmadiadli, Yasaman and Zhang, Xiao-Ping},
title = {A Survey on Speech Deepfake Detection},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3714458},
doi = {10.1145/3714458},
abstract = {The availability of smart devices leads to an exponential increase in multimedia content. However, advancements in deep learning have also enabled the creation of highly sophisticated Deepfake content, including speech Deepfakes, which pose a serious threat by generating realistic voices and spreading misinformation. To combat this, numerous challenges have been organized to advance speech Deepfake detection techniques. In this survey, we systematically analyze more than 200 papers published up to March 2024. We provide a comprehensive review of each component in the detection pipeline, including model architectures, optimization techniques, generalizability, evaluation metrics, performance comparisons, available datasets, and open source availability. For each aspect, we assess recent progress and discuss ongoing challenges. In addition, we explore emerging topics such as partial Deepfake detection, cross-dataset evaluation, and defenses against adversarial attacks, while suggesting promising research directions. This survey not only identifies the current state of the art to establish strong baselines for future experiments but also offers clear guidance for researchers aiming to enhance speech Deepfake detection systems.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {165},
numpages = {38},
keywords = {Deepfake, speech synthesis, speech Deepfake detection, spoofing countermeasures, ASV}
}

@inproceedings{10.1145/3583781.3590267,
author = {Chen, Xinda and Fu, Rongliang and Huang, Junying and Cao, Huawei and Zhang, Zhimin and Ye, Xiaochun and Ho, Tsung-Yi and Fan, Dongrui},
title = {JRouter: A Multi-Terminal Hierarchical Length-Matching Router under Planar Manhattan Routing Model for RSFQ Circuits},
year = {2023},
isbn = {9798400701252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583781.3590267},
doi = {10.1145/3583781.3590267},
abstract = {Superconducting rapid single-flux-quantum (RSFQ) logic has shown great potential for high-energy-efficient computing systems. To ensure correct operations at ultra-high frequencies, it is necessary to incorporate length-matching constraints into the routing problem. Existing routing algorithms, however, can only address 2-pin connections or support the conventional horizontal/vertical routing model, which substantially limits the optimization space for routing solutions. This paper presents JRouter, an RSFQ router that considers the two-layer planar Manhattan routing model while simultaneously coping with splitter (SPL) placement and length-matching multi-terminal routing. JRouter contains a track-assignment-based initial routing that minimizes the initial routing width while avoiding conflicts in the horizontal constraint graph. Moreover, JRouter implements an SPL-tree-based hierarchical routing with an iterative maximum-flow-based formulation to insert the detours for multi-terminal routing. A routing region extension algorithm is also developed to insert the detours for unsatisfied connections. According to the experimental results, JRouter achieves an average routing width reduction of 35.71% and 22.46% on a 16-bit RSFQ Sklansky adder compared to Kito's and Kou's routing algorithms. For randomly generated benchmarks, JRouter reduces the routing width by an average of 38.77%, 38.20%, 21.65%, and 7.01% compared to Kito's, Kou's, and two of Yan's routing algorithms, respectively, while maintaining reasonable runtime.},
booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2023},
pages = {515–520},
numpages = {6},
keywords = {length-matching, routing, rsfq, superconducting logic},
location = {Knoxville, TN, USA},
series = {GLSVLSI '23}
}

@article{10.1109/TASLP.2019.2912123,
author = {Cauchi, Benjamin and Siedenburg, Kai and Santos, Joao F. and Falk, Tiago H. and Doclo, Simon and Goetze, Stefan},
title = {Non-Intrusive Speech Quality Prediction Using Modulation Energies and LSTM-Network},
year = {2019},
issue_date = {July 2019},
publisher = {IEEE Press},
volume = {27},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2912123},
doi = {10.1109/TASLP.2019.2912123},
abstract = {Many signal processing algorithms have been proposed to improve the quality of speech recorded in the presence of noise and reverberation. Perceptual measures, i.e., listening tests, are usually considered the most reliable way to evaluate the quality of speech processed by such algorithms but are costly and time-consuming. Consequently, speech enhancement algorithms are often evaluated using signal-based measures, which can be either intrusive or non-intrusive. As the computation of intrusive measures requires a reference signal, only non-intrusive measures can be used in applications for which the clean speech signal is not available. However, many existing non-intrusive measures correlate poorly with the perceived speech quality, particularly when applied over a wide range of algorithms or acoustic conditions. In this paper, we propose a novel non-intrusive measure of the quality of processed speech that combines modulation energy features and a recurrent neural network using long short-term memory cells. We collected a dataset of perceptually evaluated signals representing several acoustic conditions and algorithms and used this dataset to train and evaluate the proposed measure. Results show that the proposed measure yields higher correlation with perceptual speech quality than that of benchmark intrusive and non-intrusive measures when considering various categories of algorithms. Although the proposed measure is sensitive to mismatch between training and testing, results show that it is a useful approach to evaluate specific algorithms over a wide range of acoustic conditions and may, thus, become particularly useful for real-time selection of speech enhancement algorithm settings.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1151–1163},
numpages = {13}
}

@inproceedings{10.1145/1953355.1953361,
author = {Millo, Jean-Vivien and Mohalik, Swarup K. and Ramesh, S.},
title = {Integrated analysis of software product lines: a constraint based framework for consistency, liveness, and commonness checking},
year = {2011},
isbn = {9781450305594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1953355.1953361},
doi = {10.1145/1953355.1953361},
abstract = {Software Product Line (SPL) is a software development framework to jointly design a family of closely related software products in an efficient and cost-effective manner. In order to separate the concerns and handle complexity, designers usually project the SPL along different perspectives such as feature, architecture and behaviour. Each perspective deals with variability of a set of artifacts and variability constraints among them. SPL designers attempt to ensure the consistency of the individual perspectives and the SPL as a whole. They are also interested in finding the elements common to all products and the live elements (used in at least one product).In the literature, most of the works focus on a single perspective and address the above-mentioned problems within single perspectives. There have also been attempts to express the variability of different perspectives within the feature perspective. However, since the different perspectives have different intents, coercing them into a single perspective may result in unnatural constructs in the feature perspective. Hence, it is better to keep the perspectives separate. However, in any SPL, the perspectives are closely related through an implementability relation or through constraints arising from design or business reasons. We call this the traceability aspect, which mandates an integrated analysis of the different perspectives.In this paper, we propose a constraint-based framework where variability and traceability constraints can be uniformly expressed, at the same time keeping the different intents of perspectives intact. We describe how the consistency, liveness, and commonness problems can be reduced to problems of constraint solving. Through a realistic case study, we provide some evidence that the constraint-based framework is expressive and scalable to large SPLs.},
booktitle = {Proceedings of the 4th India Software Engineering Conference},
pages = {41–50},
numpages = {10},
keywords = {commonness, consistency, liveness, traceability, variability},
location = {Thiruvananthapuram, Kerala, India},
series = {ISEC '11}
}

@inproceedings{10.1145/2517208.2517214,
author = {Kramer, Dean and Oussena, Samia and Komisarczuk, Peter and Clark, Tony},
title = {Using document-oriented GUIs in dynamic software product lines},
year = {2013},
isbn = {9781450323734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517208.2517214},
doi = {10.1145/2517208.2517214},
abstract = {Dynamic Software Product Line (DSPL) Engineering has gained interest through its promise of being able to unify software adaptation whereby software adaptation can be realised at compile time and runtime. While previous work has enabled program logic adaptation by the use of language extensions and platform support, little attention has been placed on Graphical User Interface (GUI) variability. Different design patterns including the Model View Controller are commonly used in GUI implementation, with GUI documents being used for declaring the GUI. To handle dynamic GUI variability currently, the developer needs to implement GUI refinements using multiple techniques. This paper proposes a solution for dealing with GUI document variability, statically and dynamically, in a unified way. In our approach, we currently use a compile time method for producing GUI variants, and code transformations to handle these variants within the application at runtime. To avoid GUI duplicates, only GUI variants that are unique, and related to a valid product configuration, are produced. To validate our approach, we implemented tool support to enable this for Android based applications.},
booktitle = {Proceedings of the 12th International Conference on Generative Programming: Concepts &amp; Experiences},
pages = {85–94},
numpages = {10},
keywords = {dynamic software product lines, graphical user interfaces},
location = {Indianapolis, Indiana, USA},
series = {GPCE '13}
}

@article{10.1145/3507907,
author = {Rust, Pierre and Picard, Gauthier and Ramparany, Fano},
title = {Resilient Distributed Constraint Reasoning to Autonomously Configure and Adapt IoT Environments},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/3507907},
doi = {10.1145/3507907},
abstract = {In this article, we investigate multi-agent techniques to install autonomy and adaptation in IoT-based smart environment settings, like smart home scenarios. We particularly make use of the smart environment configuration problem (SECP) framework, and map it to a distributed optimization problem (DCOP). This consists in enabling smart objects to coordinate and self-configure as to meet both user-defined requirements and energy efficiency, by operating a distributed constraint reasoning process over a computation graph. As to cope with the dynamics of the environment and infrastructure (e.g., by adding or removing devices), we also specify the k-resilient distribution of graph-structured computations supporting agent decisions, over dynamic and physical multi-agent systems. We implement a self-organizing distributed repair method, based on a distributed constraint optimization algorithm to adapt the distribution as to ensure the system still performs collective decisions and remains resilient to upcoming changes. We provide a full stack of mechanisms to install resilience in operating stateless DCOP solution methods, which results in a robust approach using a fast DCOP algorithm to repair any stateless DCOP solution methods at runtime. We experimentally evaluate the performances of these techniques when operating stateless DCOP algorithms to solve SECP instances.},
journal = {ACM Trans. Internet Technol.},
month = nov,
articleno = {100},
numpages = {31},
keywords = {IoT, smart home, DCOP, resilience, adaptation}
}

@inproceedings{10.1145/3183399.3183405,
author = {Li, Miqing and Chen, Tao and Yao, Xin},
title = {A critical review of: "a practical guide to select quality indicators for assessing pareto-based search algorithms in search-based software engineering": essay on quality indicator selection for SBSE},
year = {2018},
isbn = {9781450356626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183399.3183405},
doi = {10.1145/3183399.3183405},
abstract = {This paper presents a critical review of the work published at ICSE'2016 on a practical guide of quality indicator selection for assessing multiobjective solution sets in search-based software engineering (SBSE). This review has two goals. First, we aim at explaining why we disagree with the work at ICSE'2016 and why the reasons behind this disagreement are important to the SBSE community. Second, we aim at providing a more clarified guide of quality indicator selection, serving as a new direction on this particular topic for the SBSE community. In particular, we argue that it does matter which quality indicator to select, whatever in the same quality category or across different categories. This claim is based upon the fundamental goal of multiobjective optimisation --- supplying the decision-maker a set of solutions which are the most consistent with their preferences.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {17–20},
numpages = {4},
keywords = {multiobjective optimisation, quality assessment, quality indicator selection, search-based software engineering},
location = {Gothenburg, Sweden},
series = {ICSE-NIER '18}
}

@inproceedings{10.5555/762761.762818,
author = {Baumgartner, Gerald and Bernholdt, David E. and Cociorva, Daniel and Harrison, Robert and Hirata, So and Lam, Chi-Chung and Nooijen, Marcel and Pitzer, Russell and Ramanujam, J. and Sadayappan, P.},
title = {A high-level approach to synthesis of high-performance codes for quantum chemistry},
year = {2002},
isbn = {076951524X},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
abstract = {This paper discusses an approach to the synthesis of high-performance parallel programs for a class of computations encountered in quantum chemistry and physics. These computations are expressible as a set of tensor contractions and arise in electronic structure modeling. An overview is provided of the synthesis system, that transforms a high-level specification of the computation into high-performance parallel code, tailored to the characteristics of the target architecture. An example from computational chemistry is used to illustrate how different code structures are generated under different assumptions of available memory on the target computer.},
booktitle = {Proceedings of the 2002 ACM/IEEE Conference on Supercomputing},
pages = {1–10},
numpages = {10},
location = {Baltimore, Maryland},
series = {SC '02}
}

@inproceedings{10.1145/3183713.3190662,
author = {Begoli, Edmon and Camacho-Rodr\'{\i}guez, Jes\'{u}s and Hyde, Julian and Mior, Michael J. and Lemire, Daniel},
title = {Apache Calcite: A Foundational Framework for Optimized Query Processing Over Heterogeneous Data Sources},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3190662},
doi = {10.1145/3183713.3190662},
abstract = {Apache Calcite is a foundational software framework that provides query processing, optimization, and query language support to many popular open-source data processing systems such as Apache Hive, Apache Storm, Apache Flink, Druid, and MapD. The goal of this paper is to formally introduce Calcite to the broader research community, brie y present its history, and describe its architecture, features, functionality, and patterns for adoption. Calcite's architecture consists of a modular and extensible query optimizer with hundreds of built-in optimization rules, a query processor capable of processing a variety of query languages, an adapter architecture designed for extensibility, and support for heterogeneous data models and stores (relational, semi-structured, streaming, and geospatial). This exible, embeddable, and extensible architecture is what makes Calcite an attractive choice for adoption in big-data frameworks. It is an active project that continues to introduce support for the new types of data sources, query languages, and approaches to query processing and optimization.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {221–230},
numpages = {10},
keywords = {apache calcite, data management, modular query optimization, query algebra, relational semantics, storage adapters},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.5555/1129601.1129704,
author = {Li, Xin and Le, Jiayong and Pileggi, L. T. and Strojwas, A.},
title = {Projection-based performance modeling for inter/intra-die variations},
year = {2005},
isbn = {078039254X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Large-scale process fluctuations in nano-scale IC technologies suggest applying high-order (e.g., quadratic) response surface models to capture the circuit performance variations. Fitting such models requires significantly more simulation samples and solving much larger linear equations. In this paper, we propose a novel projection-based extraction approach, PROBE, to efficiently create quadratic response surface models and capture both inter-die and intra-die variations with affordable computation cost. PROBE applies a novel projection scheme to reduce the response surface modeling cost (i.e., both the required number of samples and the linear equation size) and make the modeling problem tractable even for large problem sizes. In addition, a new implicit power iteration algorithm is developed to find the optimal projection space and solve for the unknown model coefficients. Several circuit examples from both digital and analog circuit modeling applications demonstrate that PROBE can generate accurate response surface models while achieving up to 12/spl times/ speedup compared with the traditional methods.},
booktitle = {Proceedings of the 2005 IEEE/ACM International Conference on Computer-Aided Design},
pages = {721–727},
numpages = {7},
location = {San Jose, CA},
series = {ICCAD '05}
}

@inproceedings{10.1145/2866614.2866616,
author = {Schulze, Sandro and Schulze, Michael and Ryssel, Uwe and Seidl, Christoph},
title = {Aligning Coevolving Artifacts Between Software Product Lines and Products},
year = {2016},
isbn = {9781450340199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2866614.2866616},
doi = {10.1145/2866614.2866616},
abstract = {Software product lines (SPLs) play a pivotal role for developing a vast amount of related programs efficiently and with high quality. To this end, the SPL engineering process is separated into two levels: domain engineering (DE), which captures variability and development artifacts of the entire SPL, and application engineering (AE), which encompasses a variant-specific subset of the aforementioned artifacts. In the industrial practice of evolving an SPL, it is common that evolution is performed on both levels, which may affect the same artifacts (e.g., code, models) in different ways due to changes on the product line (DE) and the variant level (AE). As a result, conflicts may arise that have to be solved properly to guarantee correctness and validity of the affected artifacts. In this paper, we propose a methodology for resolving such conflicts to ensure correctness and consistency among artifacts while minimizing manual effort. Our method is comprehensive in two ways: First, we consider all kinds of artifacts (code and non-code) that may be subject to evolutionary changes in both DE and AE. Second, we also take into account that changing one particular artifact (e.g., a requirement) may require further changes to other artifacts of the same level. This way, our method reflects common industrial practices in SPL development and, thus, provides benefits for efficiently evolving real-world SPLs.},
booktitle = {Proceedings of the 10th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {9–16},
numpages = {8},
keywords = {SPL engineering, software evolution},
location = {Salvador, Brazil},
series = {VaMoS '16}
}

@inproceedings{10.1109/CHASE.2017.55,
author = {Thiyagaraja, Shanti R. and Dantu, Ram and Shrestha, Pradhumna L. and Thompson, Mark A. and Smith, Christopher},
title = {Optimized and secured transmission and retrieval of vital signs from remote devices},
year = {2017},
isbn = {9781509047215},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CHASE.2017.55},
doi = {10.1109/CHASE.2017.55},
abstract = {Smartphones and other mobile platforms provide a low cost and easily accessible method of monitoring patient health, and aid healthcare professionals in early detection of disease. Immediate access to the gathered data is an essential factor in effective patient care. But the current processes used for patients' vital data collection is slow and error prone. This undermines the advantages of remote monitoring that mobile platforms for health monitoring provide. In this paper, we propose to upload the patient health information to the Cloud. We investigate three different models to transfer data from the smartphone to the Cloud---perform all computations in the smartphone, perform all computations in the Cloud, and divide the computations between the smartphone and the Cloud. The second approach was found to be infeasible due to very high latency in data transfer with a delay of 2.84 seconds at an upload speed of 2500 KBytes per second.In order to protect the privacy of patients, it is required by law that the data gathered from remote monitoring by using mobile platforms must be kept private, and be secured before uploading to the Cloud. This paper explores the use of prominent public key encryption algorithms and their performance on a mobile device to securely transmit confidential electronic personal health information to the Cloud. We analyze performance of three common public key encryption schemes -RSA, Diffie-Hellman, and ECC. It is shown that 160 bit key size in ECC scheme provides the same level of security that a 1024 bit key size does in RSA and Diffie-Hellman. Further, the encryption and decryption time required by ECC is three times less than the other two schemes. Hence, ECC not only requires a smaller key size to provide the same level of security, but also faster encryption and decryption times as compared to the other two schemes. This makes ECC algorithms suitable to be implemented in resource constrained mobile platforms. We also compared ECC curves from three different standards - NIST, SECG, and Brainpool - to determine the optimum ECC curve, and key size to encrypt data in the mobile phone platform. It is shown that the Brainpool curve performed better than the other two standards when the key size is less than 521 bits. We also measured the latency of uploading encrypted data in a wide variety of WiFi and mobile networks.},
booktitle = {Proceedings of the Second IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies},
pages = {25–30},
numpages = {6},
keywords = {cloud computing, mobile health, performance evaluation, security},
location = {Philadelphia, Pennsylvania},
series = {CHASE '17}
}

@inproceedings{10.1109/PESOS.2009.5068815,
author = {Mietzner, Ralph and Metzger, Andreas and Leymann, Frank and Pohl, Klaus},
title = {Variability modeling to support customization and deployment of multi-tenant-aware Software as a Service applications},
year = {2009},
isbn = {9781424437160},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/PESOS.2009.5068815},
doi = {10.1109/PESOS.2009.5068815},
abstract = {More and more companies are offering their software by following the Software as a Service (SaaS) model. The promise of the SaaS model is to exploit economies of scale on the provider side by hosting multiple customers (or tenants) on the same hardware and software infrastructure. However, to attract a significant number of tenants, SaaS applications have to be customizable to fulfill the varying functional and quality requirements of individual tenants. In this paper, we describe how variability modeling techniques from software product line engineering can support SaaS providers in managing the variability of SaaS applications and their requirements. Specifically, we propose using explicit variability models to systematically derive customization and deployment information for individual SaaS tenants. We also demonstrate how variability models could be used to systematically consider information about already deployed SaaS applications for efficiently deploying SaaS applications for new tenants. We illustrate our approach by a running example for a meeting planning application.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Principles of Engineering Service Oriented Systems},
pages = {18–25},
numpages = {8},
series = {PESOS '09}
}

@inproceedings{10.1145/1282280.1282282,
author = {Yu, Ning and Vu, Khanh and Hua, Kien A.},
title = {An in-memory relevance feedback technique for high-performance image retrieval systems},
year = {2007},
isbn = {9781595937339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1282280.1282282},
doi = {10.1145/1282280.1282282},
abstract = {Content-based image retrieval with relevant feedback has been widely adopted as the query model of choice for improved effectiveness in image retrieval. The effectiveness of this solution, however, depends on the efficiency of the feedback mechanism. Current methods rely on searching the database, stored on disks, in each round of relevance feedback. This strategy incurs long delay making relevance feedback less friendly to the user, especially for very large databases. Thus, scalability is a limitation of existing solutions. In this paper, we propose an in-memory relevance feedback technique to substantially reduce the delay associated with feedback processing, and therefore improve system usability. Our new data-independent dimensionality-reduction technique is used to compress the metadata to build a small in-memory database to support relevance feedback operations with minimal disk accesses. We compare the performance of this approach with conventional relevance feedback techniques in terms of computation efficiency and retrieval accuracy. The results indicate that the new technique substantially reduces response time for user feedback while maintaining the quality of the retrieval.},
booktitle = {Proceedings of the 6th ACM International Conference on Image and Video Retrieval},
pages = {9–16},
numpages = {8},
keywords = {dimension reduction, in memory relevance feedback},
location = {Amsterdam, The Netherlands},
series = {CIVR '07}
}

@inproceedings{10.1145/3510455.3512792,
author = {Randrianaina, Georges Aaron and Khelladi, Djamel Eddine and Zendra, Olivier and Acher, Mathieu},
title = {Towards incremental build of software configurations},
year = {2022},
isbn = {9781450392242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510455.3512792},
doi = {10.1145/3510455.3512792},
abstract = {Building software is a crucial task to compile, test, and deploy software systems while continuously ensuring quality. As software is more and more configurable, building multiple configurations is a pressing need, yet, costly and challenging to instrument. The common practice is to independently build (a.k.a., clean build) a software for a subset of configurations. While incremental build has been considered for software evolution and relatively small modifications of the source code, it has surprisingly not been considered for software configurations. In this vision paper, we formulate the hypothesis that incremental build can reduce the cost of exploring the configuration space of software systems. We detail how we apply incremental build for two real-world application scenarios and conduct a preliminary evaluation on two case studies, namely x264 and Linux Kernel. For x264, we found that one can incrementally build configurations in an order such that overall build time is reduced. Nevertheless, we could not find any optimal order with the Linux Kernel, due to a high distance between random configurations. Therefore, we show it is possible to control the process of generating configurations: we could reuse commonality and gain up to 66% of build time compared to only clean builds.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {101–105},
numpages = {5},
keywords = {build system, highly configurable system, incremental build},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-NIER '22}
}

@inproceedings{10.1145/3129676.3129687,
author = {Mofrad, Asieh Abolpour and Yazidi, Anis and Hammer, Hugo Lewi},
title = {Solving Stochastic Point Location Problem in a Dynamic Environment with Weak Estimation},
year = {2017},
isbn = {9781450350273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129676.3129687},
doi = {10.1145/3129676.3129687},
abstract = {The Stochastic Point Location (SPL) problem introduced by Oommen [7] can be summarized as searching for an unknown point in the interval under a possibly faulty feedback. The search is performed via a Learning Mechanism (LM) (algorithm) that interacts with a stochastic environment which in turn informs it about the direction of the search. Since the environment is stochastic, the guidance for directions could be faulty. The first solution to the SPL problem which was pioneered by Oommen [7] two decades ago relies on discretizing the search interval and performing a controlled random walk on it. The state of the random walk at each step is considered to be the estimation of the point location. The convergence of the latter simplistic estimation strategy is proved for an infinite resolution. However, the latter strategy yields rather poor accuracy for low resolutions. In this paper, we present sophisticated tracking methods that outperform Oommen strategy [7]. Our methods revolve around tracking some key statistical properties of the underlying random walk using the family of weak estimators. Furthermore, we address the settings where the point location is non-stationary, i.e. LM is searching with uncertainty for a (possibly moving) point in an interval. In such settings, asymptotic results are no longer applicable. Simulation results show that the proposed methods outperform Oommen method for estimating point location by reducing the estimated error up to 75%.},
booktitle = {Proceedings of the International Conference on Research in Adaptive and Convergent Systems},
pages = {30–35},
numpages = {6},
keywords = {Random walk, Stochastic learning weak estimator (SLWE), Stochastic point location (SPL)},
location = {Krakow, Poland},
series = {RACS '17}
}

@inproceedings{10.1145/1529282.1529388,
author = {Bure\v{s}, Tom\'{a}\v{s} and Hn\v{e}tynka, Petr and Malohlava, Michal},
title = {Using a product line for creating component systems},
year = {2009},
isbn = {9781605581668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1529282.1529388},
doi = {10.1145/1529282.1529388},
abstract = {Component systems have become a wide-spread technology and found their place in several application domains. Each component system has its specifics and particularities that reflect its focus and the application domain it is intended for. Although important, the diversity of component systems leads to a number of problems including having different tools for each systems, unnecessary duplication of functionality and problems with integration when several domains are to be targeted. Based on categorization of component application domains, we propose a "meta-component system", which provides a software product line for creating custom component systems. We focus especially on the deployment and execution environment, which is where most diversities are found. We demonstrate the usage of the "meta-component system" and propose how it is to be realized by two core concepts of SOFA 2, namely connector generator and microcomponents.},
booktitle = {Proceedings of the 2009 ACM Symposium on Applied Computing},
pages = {501–508},
numpages = {8},
keywords = {component systems, generative programming, product line engineering, runtime environment},
location = {Honolulu, Hawaii},
series = {SAC '09}
}

@inproceedings{10.1145/2463372.2463545,
author = {Wang, Shuai and Ali, Shaukat and Gotlieb, Arnaud},
title = {Minimizing test suites in software product lines using weight-based genetic algorithms},
year = {2013},
isbn = {9781450319638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463372.2463545},
doi = {10.1145/2463372.2463545},
abstract = {Test minimization techniques aim at identifying and eliminating redundant test cases from test suites in order to reduce the total number of test cases to execute, thereby improving the efficiency of testing. In the context of software product line, we can save effort and cost in the selection and minimization of test cases for testing a specific product by modeling the product line. However, minimizing the test suite for a product requires addressing two potential issues: 1) the minimized test suite may not cover all test requirements compared with the original suite; 2) the minimized test suite may have less fault revealing capability than the original suite. In this paper, we apply weight-based Genetic Algorithms (GAs) to minimize the test suite for testing a product, while preserving fault detection capability and testing coverage of the original test suite. The challenge behind is to define an appropriate fitness function, which is able to preserve the coverage of complex testing criteria (e.g., Combinatorial Interaction Testing criterion). Based on the defined fitness function, we have empirically evaluated three different weight-based GAs on an industrial case study provided by Cisco Systems, Inc. Norway. We also presented our results of applying the three weight-based GAs on five existing case studies from the literature. Based on these case studies, we conclude that among the three weight-based GAs, Random-Weighted GA (RWGA) achieved significantly better performance than the other ones.},
booktitle = {Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation},
pages = {1493–1500},
numpages = {8},
keywords = {fault detection capability, feature pairwise coverage, test minimization, weight-based gas},
location = {Amsterdam, The Netherlands},
series = {GECCO '13}
}

@inproceedings{10.1145/2991041.2991053,
author = {Alidra, Abdelghani and Kimour, Mohamed Tahar},
title = {Prototyping Software Product Lines analysis with Pharo},
year = {2016},
isbn = {9781450345248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2991041.2991053},
doi = {10.1145/2991041.2991053},
abstract = {Software Product Lines (SPLs) are an emerging software engineering paradigm that aims to optimize software development costs and time to market trough systematic development of reusable of core assets. At the heart of SPL engineering is variability modelling. Feature models (FM) are a common way to model variability and reason about it. Examples of reasoning are for instance checking that at least one product is represented by a given FM (satisfiability) or finding the product that best fits a given set of requirements. In practice however, such operations are often complex and time consuming. In order to address these challenges, we introduce in the present article the notion of transitive dependency between features and show how it can be used as the basis for efficient analysis and automatic reasoning on feature models. We exploit this new concept to implement a first platform for prototyping and reasoning on large SPLs in Pharo. Finally we illustrate the efficiency of our proposal on the problem of features selection optimisation.},
booktitle = {Proceedings of the 11th Edition of the International Workshop on Smalltalk Technologies},
articleno = {12},
numpages = {11},
keywords = {Feature models, Software Product Lines, analysis environment, automatic reasoning},
location = {Prague, Czech Republic},
series = {IWST'16}
}

@inproceedings{10.1145/2377836.2377842,
author = {Gamez, Nadia and Romero, Daniel and Fuentes, Lidia and Rouvoy, Romain and Duchien, Laurence},
title = {Constraint-based self-adaptation of wireless sensor networks},
year = {2012},
isbn = {9781450315661},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2377836.2377842},
doi = {10.1145/2377836.2377842},
abstract = {In recent years, the Wireless Sensor Networks (WSNs) have become a useful mechanism to monitor physical phenomena in environments. The sensors that make part of these long-lived networks have to be reconfigured according to context changes in order to preserve the operation of the network. Such reconfigurations require to consider the distributed nature of the sensor nodes as well as their resource scarceness. Therefore, self-adaptations for WSNs have special requirements comparing with traditional information systems. In particular, the reconfiguration of the WSN requires a trade-off between critical dimensions for this kind of networks and devices, such as resource consumption or reconfiguration cost. Thus, in this paper, we propose to exploit Constraint-Satisfaction Problem (CSP) techniques in order to find a suitable configuration for self-adapting WSNs, modelled using a Dynamic Software Product Line (DSPL), when the context changes. We exploit CSP modeling to find a compromise between contradictory dimensions. To illustrate our approach, we use an Intelligent Transportation System scenario. This case study enables us to show the advantages of obtaining suitable and optimized configurations for self-adapting WSNs.},
booktitle = {Proceedings of the 2nd International Workshop on Adaptive Services for the Future Internet and 6th International Workshop on Web APIs and Service Mashups},
pages = {20–27},
numpages = {8},
keywords = {constraint-satisfaction problem, dynamic software product lines, self-adaptation, wireless sensor networks},
location = {Bertinoro, Italy},
series = {WAS4FI-Mashups '12}
}

@inproceedings{10.1109/ASE.2011.6100118,
author = {Soltani, Samaneh and Asadi, Mohsen and Hatala, Marek and Gasevic, Dragan and Bagheri, Ebrahim},
title = {Automated planning for feature model configuration based on stakeholders' business concerns},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100118},
doi = {10.1109/ASE.2011.6100118},
abstract = {In Software Product Line Engineering, concrete products of a family can be generated through a configuration process over a feature model. The configuration process selects features from the feature model according to the stakeholders' requirements. Selecting the right set of features for one product from all the available features in the feature model is a cumbersome task because 1) the stakeholders may have diverse business concerns and limited resources that they can spend on a product and 2) features may have negative and positive contributions on different business concern. Many configurations techniques have been proposed to facilitate software developers' tasks through automated product derivation. However, most of the current proposals for automatic configuration are not devised to cope with business oriented requirements and stakeholders' resource limitations. We propose a framework, which employs an artificial intelligence planning technique to automatically select suitable features that satisfy the stakeholders' business concerns and resource limitations. We also provide tooling support to facilitate the use of our framework.},
booktitle = {Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {536–539},
numpages = {4},
series = {ASE '11}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00122,
author = {Rosiak, Kamil},
title = {Extractive multi product-line engineering},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00122},
doi = {10.1109/ICSE-Companion52605.2021.00122},
abstract = {Cloning is a general approach to create new functionality within variants as well as new system variants. It is a fast, flexible, intuitive, and economical approach to evolve systems in the short run. However, in the long run, the maintenance effort increases. A common solution to this problem is the extraction of a product line from a set of cloned variants. This process requires a detailed analysis of variants to extract variability information. However, clones within a variant are usually not considered in the process, but are also a cause for unsustainable software. This thesis proposes an extractive multi product-line engineering approach to re-establish the sustainable development of software variants. We propose an approach to re-engineer intra-system and inter-system clones into reusable, configurable components stored in an integrated platform and synthesize a matching multilayer feature model.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {263–265},
numpages = {3},
keywords = {clone detection, multi product-line, refactoring, variability mining},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@article{10.1145/3647640,
author = {Jord\~{a}o, Rodolfo and Becker, Matthias and Sander, Ingo},
title = {IDeSyDe: Systematic Design Space Exploration via Design Space Identification},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3647640},
doi = {10.1145/3647640},
abstract = {Design space exploration (DSE) is a key activity in embedded design processes, where a mapping between applications and platforms that meets the process design requirements must be found. Finding such mappings is very challenging due to the complexity of modern embedded platforms and applications. DSE tools aid in this challenge by potentially covering sections of the design space that could be unintuitive to designers, leading to more optimised designs. Despite this potential benefit, DSE tools remain relatively niche in the embedded industry. A significant obstacle hindering their wider adoption is integrating such tools into embedded design processes.We present two contributions that address this integration issue. First, we present the design space identification (DSI) approach for systematically constructing DSE solutions that are modular and tuneable. Modularity means that DSE solutions can be reused to construct other DSE solutions, while tuneability means that the most specific DSE solution is chosen for the target DSE problem. Moreover, DSI enables transparent cooperation between exploration algorithms. Second, we present IDeSyDe, an extensible DSE framework for DSE solutions based on DSI. IDeSyDe allows extensions to be developed in different programming languages in a manner compliant with the DSI approach.We showcase the relevance of these contributions through five different case studies. The case study evaluations showed that non-exploration DSI procedures create overheads, which are marginal compared to the exploration algorithms. Empirically, most evaluations average 2% of the total DSE request. More importantly, the case studies have shown that IDeSyDe indeed provides a modular and incremental framework for constructing DSE solutions. In particular, the last case study required minimal extensions over the previous case studies so that support for a new application type was added to IDeSyDe.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = sep,
articleno = {87},
numpages = {45},
keywords = {Design space exploration, design space identification, embedded system design}
}

@inproceedings{10.5555/2050655.2050659,
author = {Drago, Mauro Luigi and Ghezzi, Carlo and Mirandola, Raffaela},
title = {Towards quality driven exploration of model transformation spaces},
year = {2011},
isbn = {9783642244841},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Verifying that a software system has certain nonfunctional properties is a primary concern in many engineering fields. Although several model-driven approaches exist to predict quality attributes from system models, they still lack the proper level of automation envisioned by Model Driven Software Development. When a potential issue concerning non-functional properties is discovered, the identification of a solution is still entirely up to the engineer and to his/her experience. This paper presents QVT-Rational, our multi-modeling solution to automate the detection-solution loop. We leverage and extend existing model transformation techniques with constructs to elicit the space of the alternative solutions and to bind quality properties to them. Our framework is highly customizable, it supports the definition of nonfunctional requirements and provides an engine to automatically explore the solution space. We evaluate our approach by applying it to two well-known software engineering problems -- Object-Relational Mapping and components allocation -- and by showing how several solutions that satisfy given performance requirements can be automatically identified.},
booktitle = {Proceedings of the 14th International Conference on Model Driven Engineering Languages and Systems},
pages = {2–16},
numpages = {15},
keywords = {feedback provisioning, model transformations},
location = {Wellington, New Zealand},
series = {MODELS'11}
}

@inproceedings{10.1145/2335484.2335506,
author = {Hirzel, Martin},
title = {Partition and compose: parallel complex event processing},
year = {2012},
isbn = {9781450313155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2335484.2335506},
doi = {10.1145/2335484.2335506},
abstract = {Complex event processing uses patterns to detect composite events in streams of simple events. Typically, the events are logically partitioned by some key. For instance, the key can be the stock symbol in stock quotes, the author in tweets, the vehicle in transportation, or the patient in health-care. Composite event patterns often become meaningful only after partitioning. For instance, a pattern over stock quotes is typically meaningful over quotes for the same stock symbol. This paper proposes a pattern syntax and translation scheme organized around the notion of partitions. Besides making patterns meaningful, partitioning also benefits performance, since different keys can be processed in parallel. We have implemented partitioned parallel complex event processing as an extension to IBM's System S high-performance streaming platform. Our experiments with several benchmarks from finance and social media demonstrate processing speeds of up to 830,000 events per second, and substantial speedups for expensive patterns parallelized on multi-core machines as well as multi-machine clusters. Partitioning the event stream before detecting composite events makes event processing both more intuitive and parallel.},
booktitle = {Proceedings of the 6th ACM International Conference on Distributed Event-Based Systems},
pages = {191–200},
numpages = {10},
keywords = {CEP, SPL, automata, composite events, parallelism, pattern matching, regular expressions, stream processing},
location = {Berlin, Germany},
series = {DEBS '12}
}

@proceedings{10.1145/3663529,
title = {FSE 2024: Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to FSE 2024, the ACM International Conference on the Foundations of Software Engineering (FSE) 2024. The conference now has a shorter name! FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {Porto de Galinhas, Brazil}
}

@inproceedings{10.1145/781131.781134,
author = {Lamb, Andrew A. and Thies, William and Amarasinghe, Saman},
title = {Linear analysis and optimization of stream programs},
year = {2003},
isbn = {1581136625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/781131.781134},
doi = {10.1145/781131.781134},
abstract = {As more complex DSP algorithms are realized in practice, there is an increasing need for high-level stream abstractions that can be compiled without sacrificing efficiency. Toward this end, we present a set of aggressive optimizations that target linear sections of a stream program. Our input language is StreamIt, which represents programs as a hierarchical graph of autonomous filters. A filter is linear if each of its outputs can be represented as an affine combination of its inputs. Linearity is common in DSP components; examples include FIR filters, expanders, compressors, FFTs and DCTs.We demonstrate that several algorithmic transformations, traditionally hand-tuned by DSP experts, can be completely automated by the compiler. First, we present a linear extraction analysis that automatically detects linear filters from the C-like code in their work function. Then, we give a procedure for combining adjacent linear filters into a single filter, as well as for translating a linear filter to operate in the frequency domain. We also present an optimization selection algorithm, which finds the sequence of combination and frequency transformations that will give the maximal benefit.We have completed a fully-automatic implementation of the above techniques as part of the StreamIt compiler, and we demonstrate a 450% performance improvement over our benchmark suite.},
booktitle = {Proceedings of the ACM SIGPLAN 2003 Conference on Programming Language Design and Implementation},
pages = {12–25},
numpages = {14},
keywords = {DSP, FFT, StreamIt, algebraic simplification, embedded, linear systems, optimization, stream programming},
location = {San Diego, California, USA},
series = {PLDI '03}
}

@proceedings{10.1145/3634713,
title = {VaMoS '24: Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bern, Switzerland}
}

@inproceedings{10.1145/3672919.3673000,
author = {Sun, Ziwei and Hua, Zexi and Li, Hengchao},
title = {A Training Strategy of Flying Bird Object Detection Model Based on Improved Self-Paced Learning Algorithm},
year = {2024},
isbn = {9798400718212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672919.3673000},
doi = {10.1145/3672919.3673000},
abstract = {In order to avoid the impact of hard samples on the training process of the Flying Bird Object Detection model (FBOD model, in our previous work, we designed the FBOD model according to the characteristics of flying bird objects in surveillance video), the Self-Paced Learning method with Easy Sample Prior Based on Confidence (SPL-ESP-BC), a new model training strategy, is proposed. Firstly, the loss-based Minimizer Function in Self-Paced Learning (SPL) is improved, and the confidence-based Minimizer Function is proposed, which makes it more suitable for one-class object detection tasks. Secondly, to give the model the ability to judge easy and hard samples at the early stage of training by using the SPL strategy, an SPL strategy with Easy Sample Prior (ESP) is proposed. The FBOD model is trained using the standard training method with easy samples first, then the SPL method with all samples is used to train it. Combining the strategy of the ESP and the Minimizer Function based on confidence, the SPL-ESP-BC model training strategy is proposed. Using this strategy to train the FBOD model can make it to learn the characteristics of the flying bird object in the surveillance video better, from easy to hard. The experimental results show that compared with the standard training method that does not distinguish between easy and hard samples, the AP50 of the FBOD model trained by the SPL-ESP-BC is increased by 2.1%.},
booktitle = {Proceedings of the 2024 3rd International Conference on Cyber Security, Artificial Intelligence and Digital Economy},
pages = {444–450},
numpages = {7},
location = {Nanjing, China},
series = {CSAIDE '24}
}

@inproceedings{10.1145/2245276.2245370,
author = {Parra, Carlos and Romero, Daniel and Mosser, S\'{e}bastien and Rouvoy, Romain and Duchien, Laurence and Seinturier, Lionel},
title = {Using constraint-based optimization and variability to support continuous self-adaptation},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2245370},
doi = {10.1145/2245276.2245370},
abstract = {Self-adaptation is one of the upcoming paradigms that accurately tackles nowadays systems complexity. In this context, Dynamic Software Product Lines model the intrinsic variability of a family of systems, and dynamically support their reconfiguration according to updated context. However, when several configurations are available for the same context, making a decision about the right one is a hard challenge: further dimensions such as QoS are needed to enrich the decision making process. In this paper, we propose to combine variability with Constraint-Satisfaction Problem techniques to face this challenge. The approach is illustrated and validated with a context-driven system used to support the control of a home through mobile devices.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {486–491},
numpages = {6},
location = {Trento, Italy},
series = {SAC '12}
}

@inproceedings{10.1145/2851141.2851148,
author = {De Matteis, Tiziano and Mencagli, Gabriele},
title = {Keep calm and react with foresight: strategies for low-latency and energy-efficient elastic data stream processing},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851148},
doi = {10.1145/2851141.2851148},
abstract = {This paper addresses the problem of designing scaling strategies for elastic data stream processing. Elasticity allows applications to rapidly change their configuration on-the-fly (e.g., the amount of used resources) in response to dynamic workload fluctuations. In this work we face this problem by adopting the Model Predictive Control technique, a control-theoretic method aimed at finding the optimal application configuration along a limited prediction horizon in the future by solving an online optimization problem. Our control strategies are designed to address latency constraints, using Queueing Theory models, and energy consumption by changing the number of used cores and the CPU frequency through the Dynamic Voltage and Frequency Scaling (DVFS) support available in the modern multicore CPUs. The proactive capabilities, in addition to the latency- and energy-awareness, represent the novel features of our approach. To validate our methodology, we develop a thorough set of experiments on a high-frequency trading application. The results demonstrate the high-degree of flexibility and configurability of our approach, and show the effectiveness of our elastic scaling strategies compared with existing state-of-the-art techniques used in similar scenarios.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {13},
numpages = {12},
keywords = {DVFS, data stream processing, elasticity, model predictive control, multicore programming},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3293534,
author = {Liando, Jansen C. and Gamage, Amalinda and Tengourtius, Agustinus W. and Li, Mo},
title = {Known and Unknown Facts of LoRa: Experiences from a Large-scale Measurement Study},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1550-4859},
url = {https://doi.org/10.1145/3293534},
doi = {10.1145/3293534},
abstract = {Long Range (LoRa) is a Low-power Wide-area Network technology designed for the Internet of Things. In recent years, it has gained significant momentum among industrial and research communities. Patented by Semtech, LoRa makes use of chirp spread spectrum modulation to deliver data with promises of long battery life, far-reaching communication distances, and a high node density at the cost of data rate. In this article, we conduct a series of experiments to verify the claims made by Semtech on LoRa technology. Our results show that LoRa is capable of communicating over 10km under line-of-sight environments. However, under non-line-of-sight environments, LoRa’s performance is severely affected by obstructions such as buildings and vegetations. Moreover, the promise of prolonged battery life requires extreme tuning of parameters. Last, a LoRa gateway supports up to 6,000 nodes with PRR requirement of &gt;70%. This study also explores the relationship between LoRa transmission parameters and proposes an algorithm to determine optimal settings in terms of coverage and power consumption under non-line-of-sight environments. It further investigates the impact of LoRa Wide-area Networks on energy consumption and network capacity along with implementation of a LoRa medium access mechanism and possible gains brought forth by implementing such a mechanism.},
journal = {ACM Trans. Sen. Netw.},
month = feb,
articleno = {16},
numpages = {35},
keywords = {LoRa, Network measurement, internet of things, low-power wide-area network, network performance analysis, sensor networks, wide-area networks}
}

@inproceedings{10.1145/2517208.2517228,
author = {Ofenbeck, Georg and Rompf, Tiark and Stojanov, Alen and Odersky, Martin and P\"{u}schel, Markus},
title = {Spiral in scala: towards the systematic construction of generators for performance libraries},
year = {2013},
isbn = {9781450323734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517208.2517228},
doi = {10.1145/2517208.2517228},
abstract = {Program generators for high performance libraries are an appealing solution to the recurring problem of porting and optimizing code with every new processor generation, but only few such generators exist to date. This is due to not only the difficulty of the design, but also of the actual implementation, which often results in an ad-hoc collection of standalone programs and scripts that are hard to extend, maintain, or reuse. In this paper we ask whether and which programming language concepts and features are needed to enable a more systematic construction of such generators. The systematic approach we advocate extrapolates from existing generators: a) describing the problem and algorithmic knowledge using one, or several, domain-specific languages (DSLs), b) expressing optimizations and choices as rewrite rules on DSL programs, c) designing data structures that can be configured to control the type of code that is generated and the data representation used, and d) using autotuning to select the best-performing alternative. As a case study, we implement a small, but representative subset of Spiral in Scala using the Lightweight Modular Staging (LMS) framework. The first main contribution of this paper is the realization of c) using type classes to abstract over staging decisions, i.e. which pieces of a computation are performed immediately and for which pieces code is generated. Specifically, we abstract over different complex data representations jointly with different code representations including generating loops versus unrolled code with scalar replacement - a crucial and usually tedious performance transformation. The second main contribution is to provide full support for a) and d) within the LMS framework: we extend LMS to support translation between different DSLs and autotuning through search.},
booktitle = {Proceedings of the 12th International Conference on Generative Programming: Concepts &amp; Experiences},
pages = {125–134},
numpages = {10},
keywords = {abstraction over staging, data representation, scalar replacement, selective precomputation, synthesis},
location = {Indianapolis, Indiana, USA},
series = {GPCE '13}
}

@article{10.1145/3034827,
author = {Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz},
title = {CASE Tool Support for Variability Management in Software Product Lines},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3034827},
doi = {10.1145/3034827},
abstract = {Software product lines (SPL) aim at reducing time-to-market and increasing software quality through extensive, planned reuse of artifacts. An essential activity in SPL is variability management, i.e., defining and managing commonality and variability among member products. Due to the large scale and complexity of today's software-intensive systems, variability management has become increasingly complex to conduct. Accordingly, tool support for variability management has been gathering increasing momentum over the last few years and can be considered a key success factor for developing and maintaining SPLs. While several studies have already been conducted on variability management, none of these analyzed the available tool support in detail. In this work, we report on a survey in which we analyzed 37 existing variability management tools identified using a systematic literature review to understand the tools’ characteristics, maturity, and the challenges in the field. We conclude that while most studies on variability management tools provide a good motivation and description of the research context and challenges, they often lack empirical data to support their claims and findings. It was also found that quality attributes important for the practical use of tools such as usability, integration, scalability, and performance were out of scope for most studies.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {14},
numpages = {45},
keywords = {Software engineering, computer-aided software engineering, software variability}
}

@inproceedings{10.1145/2884781.2884861,
author = {Tan, Tian Huat and Chen, Manman and Sun, Jun and Liu, Yang and Andr\'{e}, \'{E}tienne and Xue, Yinxing and Dong, Jin Song},
title = {Optimizing selection of competing services with probabilistic hierarchical refinement},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884861},
doi = {10.1145/2884781.2884861},
abstract = {Recently, many large enterprises (e.g., Netflix, Amazon) have decomposed their monolithic application into services, and composed them to fulfill their business functionalities. Many hosting services on the cloud, with different Quality of Service (QoS) (e.g., availability, cost), can be used to host the services. This is an example of competing services. QoS is crucial for the satisfaction of users. It is important to choose a set of services that maximize the overall QoS, and satisfy all QoS requirements for the service composition. This problem, known as optimal service selection, is NP-hard. Therefore, an effective method for reducing the search space and guiding the search process is highly desirable. To this end, we introduce a novel technique, called Probabilistic Hierarchical Refinement (ProHR). ProHR effectively reduces the search space by removing competing services that cannot be part of the selection. ProHR provides two methods, probabilistic ranking and hierarchical refinement, that enable smart exploration of the reduced search space. Unlike existing approaches that perform poorly when QoS requirements become stricter, ProHR maintains high performance and accuracy, independent of the strictness of the QoS requirements. ProHR has been evaluated on a publicly available dataset, and has shown significant improvement over existing approaches.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {85–95},
numpages = {11},
location = {Austin, Texas},
series = {ICSE '16}
}

@article{10.1109/TASLP.2020.3040033,
author = {Son, Phan Le},
title = {On the Design of Sparse Arrays With Frequency-Invariant Beam Pattern},
year = {2020},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3040033},
doi = {10.1109/TASLP.2020.3040033},
abstract = {Beamformer performs spatial filtering to preserve the desired signal while suppressing interfering signals and noise arriving from directions other than the direction of interest. However, the beam pattern of the conventional beamformer is dependent on the frequency of the signal. It is common to use dense and uniform arrays for a broadband signal to achieve some essential performances together, such as frequency-invariant, white noise gain, directivity factor, front-to-back ratio, etc. Recently, the interest in sparse arrays is growing, mainly due to the capacity to reduce the number of sensors. Nevertheless, in general, finding a suitable sparse array layout is still a challenging task. Many studies have focused on optimization procedures to seek the sparse array deployment. This paper presents an alternative approach to determine the location of sensors. Starting with a weight spectrum of a virtual uniform array, some techniques are used, such as analyzing the weight spectrum to determine the critical sensors, applying the clustering technique to group the sensors into the different groups, and selecting the representative sensors for each group. After the sparse array deployment is specified, the optimization technique is applied to find the beamformer coefficients. The proposed method helps to save the computation time in the design phase, and its beamformer performance outperforms other state-of-the-art methods in several aspects.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {226–238},
numpages = {13}
}

@inproceedings{10.1145/3677333.3678153,
author = {Qararyah, Fareed and Azhar, Muhammad Waqar and Maleki, Mohammad Ali and Trancoso, Pedro},
title = {Fusing Depthwise and Pointwise Convolutions for Efficient Inference on GPUs},
year = {2024},
isbn = {9798400718021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677333.3678153},
doi = {10.1145/3677333.3678153},
abstract = {Depthwise and pointwise convolutions have fewer parameters and perform fewer operations than standard convolutions. As a result, they have become increasingly used in various compact DNNs, including convolutional neural networks (CNNs) and vision transformers (ViTs). However, they have a lower compute-to-memory-access ratio than standard convolutions, making their memory accesses often the performance bottleneck. This paper explores fusing depthwise and pointwise convolutions to overcome the memory access bottleneck. The focus is on fusing these operators on GPUs. The prior art on GPU-based fusion suffers from one or more of the following: (1) fusing either a convolution with an element-wise or multiple non-convolutional operators, (2) not explicitly optimizing for memory accesses, (3) not supporting depthwise convolutions. This paper proposes Fused Convolutional Modules (FCMs), a set of novel fused depthwise and pointwise GPU kernels. FCMs significantly reduce pointwise and depthwise convolutions memory accesses, improving execution time and energy efficiency. To evaluate the trade-offs associated with fusion and determine which convolutions are beneficial to fuse and the optimal FCM parameters, we propose FusePlanner. FusePlanner consists of cost models to estimate the memory accesses of depthwise, pointwise, and FCM kernels given GPU characteristics. Our experiments on three GPUs using representative CNNs and ViTs demonstrate that FCMs save up to 83% of the memory accesses and achieve speedups of up to 3.7x compared to cuDNN. Complete model implementations of various CNNs using our modules outperform TVMs’ achieving speedups of up to 1.8x and saving up to two-thirds of the energy. FCM and FusePlanner implementations are open source: https://github.com/fqararyah/Fusing_DW_and_PW_on_GPUs},
booktitle = {Workshop Proceedings of the 53rd International Conference on Parallel Processing},
pages = {58–67},
numpages = {10},
keywords = {CNN, GPU, depthwise convolution, layer fusion, pointwise convolution, vision transformer},
location = {Gotland, Sweden},
series = {ICPP Workshops '24}
}

@inproceedings{10.1145/3627673.3679740,
author = {Zhou, Dongming and Pang, Zhengbin and Li, Wei},
title = {Learning Cross-modal Knowledge Reasoning and Heuristic-prompt for Visual-language Navigation},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3679740},
doi = {10.1145/3627673.3679740},
abstract = {Visual language navigation is an exciting and challenging multi-modal task. Most existing research focuses on the fusion of visual features and semantic space, which ignoring the importance of local highlight features and semantic knowledge alignment in images for agent navigation. Therefore, this paper proposes a novel visual language model combining Knowledge-augmented Reasoning and Soft-Prompt (KRSP) learning. First, we perform fine-grained processing of local regions in the image and to map context image features and text knowledge to the same common sub-space. We focus on regional knowledge to increase the model reasoning ability. Next, soft-prompt learning aligns keywords and sub-visual information in instruction features to solve the path mismatch problem in coarse-grained instructions. We use a large-scale pre-training model CoCoOp to collect highly matched soft action prompts into a unified instruction set. Finally, we propose a general cross-modal feature alignment loss function. The potential semantic correlation between sub-visual information and instruction space is closer through the penalty mechanism of the alignment function. This paper verifies the method effectiveness on the R2R and REVERIE datasets, and the experimental results show that KRSP achieves state-of-the-art performance. Among them, the KRSP of SPL evaluation metric increased by 4.5% in unseen scenarios.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {3453–3462},
numpages = {10},
keywords = {fine-grained alignment, pre-training model, prompt learning, visual-language navigation},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3001867.3001869,
author = {Schuster, Sven and Nieke, Michael and Schaefer, Ina},
title = {Name resolution strategies in variability realization languages for software product lines},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001869},
doi = {10.1145/3001867.3001869},
abstract = {Software Product Lines (SPLs) exploit reuse-in-the-large to enable customization by explicitly modeling commonalities and variabilities of closely related software systems. Different approaches exist to enable SPL development and product creation by using modular languages, such as Feature-Oriented Programming (FOP) (compositional) or Delta-Oriented Programming (DOP) (transformational). These languages incorporate, e.g., object-oriented languages, adding a layer of variability. Creating a variabilityaware Abstract Syntax Graph (ASG), i.e., an ASG that contains the complete variability of the SPL, facilitates family-based analyses and is essential for supporting developers during SPL development. To this end, name resolution has to be performed. However, name resolution for these languages is a challenge as multiple declarations for the same element may occur in different modules. In this paper, we propose four name resolution strategies for compositional and transformational SPL realization languages and discuss their benefits and drawbacks, categorized by relevant application scenarios of the ASG.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {11–17},
numpages = {7},
keywords = {Abstract Syntax Graph, Delta-Oriented Programming, Feature-Oriented Programming, Software Product Lines, name resolution},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@proceedings{10.1145/3540250,
title = {ESEC/FSE 2022: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of all members of the organizing committee, we are delighted to welcome everyone to the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2022. The event continues the long, distinguished ESEC/FSE tradition of presenting the most innovative research, and facilitating interactions between scientists and engineers who are passionate about advancing the theory and practice of software engineering.},
location = {Singapore, Singapore}
}

@proceedings{10.1145/3680121,
title = {CoNEXT '24: Proceedings of the 20th International Conference on emerging Networking EXperiments and Technologies},
year = {2024},
isbn = {9798400711084},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 20th edition of the ACM Conference on Emerging Networking Experiment and Technologies (ACM CoNEXT 2024). CoNEXT is a premier and highly selective venue in computer networking. The first edition of the conference was organized in Toulouse in 2005, and this year we are in Los Angeles.CoNEXT employs a hybrid model with two submission deadlines (December and June). Two types of papers can be submitted to CoNEXT: (i) long papers presenting significant and novel research results on emerging computer networks and applications, and (ii) short papers for contributions whose novelty and impact show the same technical excellence, but whose description fits within 6 pages. CoNEXT employs a rigorous review process, including two rounds of reviews by TPC members, online discussions, and a TPC meeting. The accepted long papers are published in the journal Proceedings of the ACM on Networking (PACMNET) while the short papers appear in this conference proceedings.A total of 82 papers were submitted to the December 2023 deadline. 63 of these were long papers, and 19 were short papers. Five long papers were directly accepted and appeared in the June 2024 issue of PACMNET. Another five long papers submitted in December 2023 were revised by the authors and appeared in the September 2024 issue of PACMNET. Three short papers submitted in December 2023 were revised by the authors, and appear in these conference proceedings.},
location = {Los Angeles, CA, USA}
}

@article{10.1145/3376921,
author = {Renner, Bernd-Christian and Heitmann, Jan and Steinmetz, Fabian},
title = {ahoi: Inexpensive, Low-power Communication and Localization for Underwater Sensor Networks and μAUVs},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1550-4859},
url = {https://doi.org/10.1145/3376921},
doi = {10.1145/3376921},
abstract = {The recent development of small, cheap AUVs enables a plethora of underwater near- and inshore applications. Among these are monitoring of wind parks, detection of pollution sources, water-quality inspection, and the support of divers during disaster management. These tasks profit from online reporting, control, and AUV swarm interaction; yet they require underwater communication. Unfortunately, commercial devices are prohibitively expensive and typically closed-source, hampering their application in affordable products and research. Therefore, we developed the open-source ahoi acoustic modem. It is (i)&nbsp;small enough to be carried by micro AUVs, (ii)&nbsp;consumes little enough energy to not diminish operation times of its host, (iii)&nbsp;comes at an attractive unit cost below $600, (iv)&nbsp;can reliably communicate at distances of 150 m and more, and (v)&nbsp;supports ranging without additional hardware. Due to its modular build, the modem can be customized and is suitable as research platform to analyze, e.g., MAC and routing protocols. We conducted extensive real-world studies and present results of communication range, packet reception rate, ranging accuracy, and efficient and reliable self-localization. Finally, we draw conclusions regarding acoustic communication, ranging, and localization with inexpensive and low-power devices that go beyond a particular device. Our study, hence, encompasses general insights, observations, and recommendations.},
journal = {ACM Trans. Sen. Netw.},
month = jan,
articleno = {18},
numpages = {46},
keywords = {AUV, Acoustic, ahoi, communication, localization, modem, swarm, underwater}
}

@article{10.1109/TNET.2024.3366166,
author = {Chiang, Sheng-Hao and Wang, Chih-Hang and Yang, De-Nian and Liao, Wanjiun and Chen, Wen-Tsuen},
title = {Online Multicast Traffic Engineering for Multi-View Videos With View Synthesis in SDN},
year = {2024},
issue_date = {Aug. 2024},
publisher = {IEEE Press},
volume = {32},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2024.3366166},
doi = {10.1109/TNET.2024.3366166},
abstract = {Multi-view videos (MVV) have emerged to provide users with immersively interactive experiences with 3D multimedia content. Compared with traditional 2D videos, MVV offers multiple view angles to avoid generating occluded regions from a single viewpoint and allows users to receive different view angles, which consume much higher bandwidth. In this paper, we leverage multicast and view synthesis to effectively reduce the number of transmitted views and total bandwidth consumption in software-defined content delivery networks (SD-CDN). By exploiting the SDN architecture, SD-CDN can optimize traffic engineering and the selection of multi-view sources to serve users. We formulate a new optimization problem, Online Multicast with View Synthesis (OMVS), prove the NP-hardness, and design an online algorithm with the ideas of View Popularity Cost Ratio, View Watching Possibility, and synthesis tree, to achieve the tightest competitive ratio. The experiment on real networks and implementation in an experimental SDN manifest that the proposed algorithm outperforms state-of-the-art algorithms regarding the total cost, bandwidth consumption, synthesis quality, and link and node utilization.},
journal = {IEEE/ACM Trans. Netw.},
month = feb,
pages = {2778–2793},
numpages = {16}
}

@inproceedings{10.5555/3522802.3522999,
author = {Flores-Garc\'{\i}a, Erik and Jeong, Yongkuk and Wiktorsson, Magnus and Liu, Sichao and Wang, Lihui and Kim, GooYoung},
title = {Digital twin-based services for smart production logistics},
year = {2022},
publisher = {IEEE Press},
abstract = {Digital Twin (DT)-based services including Industrial Internet of Things (IIoT) are essential for achieving the vision of Smart Production Logistics and enhancing manufacturing competitiveness. DT-based services combining IIoT provide real-time location of materials and optimization of resources for addressing mass customization and fluctuating market demand. However, literature applying IIoT and achieving DT-based services in Smart Production Logistics (SPL) is scarce. Accordingly, the purpose of this study is to analyze the combined use of DT-based services and IIoT in SPL. We propose a framework combining DT-based services and IIoT for the real-time location and optimization of material handling. The study draws results from an SPL demonstrator based on a case in the automotive industry applying the proposed framework. The results show improvement in the delivery, makespan, and distance travelled during material handling. The study provides critical insight for managers responsible for improving the delivery of materials and information inside a factory.},
booktitle = {Proceedings of the Winter Simulation Conference},
articleno = {233},
numpages = {12},
location = {Phoenix, Arizona},
series = {WSC '21}
}

@inproceedings{10.1145/1967486.1967572,
author = {Souer, Jurriaan and Joor, Dirk-Jan},
title = {An approach to identify commonalities in web application engineering for a web content management system},
year = {2010},
isbn = {9781450304214},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1967486.1967572},
doi = {10.1145/1967486.1967572},
abstract = {The process of Web applications engineering can be complex and time consuming. We argue that Web engineering based on a standardized platform with reusable components is a logical next step in the evolution of Web application development. One popular platform to create Web applications is called a Web Content Management Systems (WCMS) which allows organizations to develop Web applications in a time and resource efficient way. This paper presents a method to identify software commonalities in WCMS-based Web applications to improve the software product for future implementations based on feature modeling and e-business models. The resulting method provides insight in relevant e-business models and their corresponding functionalities. Moreover, this paper shows how these commonalities can be identified and how that could influence the software product line. The approach has been applied in a practical case study of a series of Web application engineering projects within the publishing vertical market. We have validated the approach with experts within the case study company and found that the approach is useful in aiding requirements engineers in the Web application engineering process and product managers in the software product management process.},
booktitle = {Proceedings of the 12th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {558–565},
numpages = {8},
keywords = {software product lines, web content management system, web engineering},
location = {Paris, France},
series = {iiWAS '10}
}

@article{10.1145/3709159,
author = {P\"{a}\ss{}ler, Juliane and ter Beek, Maurice H. and Damiani, Ferruccio and Dubslaff, Clemens and Johnsen, Einar Broch and Tapia Tarifa, Silvia Lizeth},
title = {Feature-Oriented Modelling and Analysis of a Self-Adaptive Robotic System},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0934-5043},
url = {https://doi.org/10.1145/3709159},
doi = {10.1145/3709159},
abstract = {Improved autonomy in robotic systems is needed for innovation in, e.g., the marine sector. Autonomous robots that are let loose in hazardous environments, such as underwater, need to handle uncertainties that stem from both their environment and internal state. While self-adaptation is crucial to cope with these uncertainties, bad decisions may cause the robot to get lost or even to cause severe environmental damage. Autonomous, self-adaptive robots that operate in uncontrolled environments full of uncertainties need to be reliable! Since these uncertainties are hard to replicate in test deployments, we need methods to formally analyse self-adaptive robots operating in uncontrolled environments. In this paper, we show how feature-oriented techniques can be used to formally model and analyse self-adaptive robotic systems in the presence of such uncertainties. Self-adaptive systems can be organised as two-layered systems with a managed subsystem handling the domain concerns and a managing subsystem implementing the adaptation logic. We consider a case study of an autonomous underwater vehicle (AUV) for pipeline inspection, in which the managed subsystem of the AUV is modelled as a family of systems, where each family member corresponds to a valid configuration of the AUV which can be seen as an operating mode of the AUV’s behaviour. The managing subsystem of the AUV is modelled as a control layer that is capable of dynamically switching between such valid configurations, depending on both environmental and internal uncertainties. These uncertainties are captured in a probabilistic and highly configurable model. Our modelling approach allows us to exploit powerful formal methods for feature-oriented systems, which we illustrate by analysing safety properties, energy consumption, and multi-objective properties, as well as performing parameter synthesis to analyse to what extent environmental conditions affect the AUV. The case study is realised in the probabilistic feature-oriented modelling language and verification tool ProFeat, and in particular exploits family-based probabilistic and parametric model checking.},
note = {Just Accepted},
journal = {Form. Asp. Comput.},
month = jan,
keywords = {Feature models, Probabilistic model checking, Parametric model checking, Self-adaptive systems, Cyber-physical systems, Robotics}
}

@proceedings{10.1145/3611643,
title = {ESEC/FSE 2023: Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to ESEC/FSE 2023, the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ESEC/FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. ESEC/FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {San Francisco, CA, USA}
}

@inproceedings{10.5555/261693.261706,
author = {Gebotys, Catherine H.},
title = {An efficient model for DSP code generation: performance, code size, estimated energy},
year = {1997},
isbn = {0818679492},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper presents a model for simultaneous instruction selection, compaction, and register allocation. An arc mapping model along with logical propositions is used to create an optimization model. Code is generated in fast cpu times and is optimized for minimum code size, maximum performance or estimated energy dissipation. Code generated for realistic DSP applications provide performance and code size improvements from 1.09 up to 2.18 times for the TMS320C2x processor compared to previous research and a commercial compiler. In all examples up to 106 instructions are generated in under one cpu minute. This research is important for industry since DSP code can be efficiently generated with constraints on code size, performance, energy dissipation.},
booktitle = {Proceedings of the 10th International Symposium on System Synthesis},
pages = {41–47},
numpages = {7},
location = {Antwerp, Belgium},
series = {ISSS '97}
}

@proceedings{10.1145/3643667,
title = {Q-SE 2024: Proceedings of the 5th ACM/IEEE International Workshop on Quantum Software Engineering},
year = {2024},
isbn = {9798400705700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 5th International Workshop on Quantum Software Engineering (Q-SE 2024), co-located with ICSE 2024, provides a platform for researchers and practitioners to discuss challenges in developing quantum software in high-level quantum languages, novel solutions to build correct methods for testing quantum programs, executing quantum software, developing best practices, and creating a research roadmap of quantum software engineering.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3503161.3547852,
author = {Dang, Ronghao and Shi, Zhuofan and Wang, Liuyi and He, Zongtao and Liu, Chengju and Chen, Qijun},
title = {Unbiased Directed Object Attention Graph for Object Navigation},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547852},
doi = {10.1145/3503161.3547852},
abstract = {Object navigation tasks require agents to locate specific objects in unknown environments based on visual information. Previously, graph convolutions were used to implicitly explore the relationships between objects. However, due to differences in visibility among objects, it is easy to generate biases in object attention. Thus, in this paper, we propose a directed object attention (DOA) graph to guide the agent in explicitly learning the attention relationships between objects, thereby reducing the object attention bias. In particular, we use the DOA graph to perform unbiased adaptive object attention (UAOA) on the object features and unbiased adaptive image attention (UAIA) on the raw images, respectively. To distinguish features in different branches, a concise adaptive branch energy distribution (ABED) method is proposed. We assess our methods on the AI2-Thor dataset. Compared with the state-of-the-art (SOTA) method, our method reports 7.4%, 8.1% and 17.6% increase in success rate (SR), success weighted by path length (SPL) and success weighted by action efficiency (SAE), respectively.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {3617–3627},
numpages = {11},
keywords = {object attention bias, object attention graph, object navigation},
location = {Lisboa, Portugal},
series = {MM '22}
}

@article{10.1145/2528412,
author = {Hirzel, Martin and Soul\'{e}, Robert and Schneider, Scott and Gedik, Bu\u{g}ra and Grimm, Robert},
title = {A catalog of stream processing optimizations},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/2528412},
doi = {10.1145/2528412},
abstract = {Various research communities have independently arrived at stream processing as a programming model for efficient and parallel computing. These communities include digital signal processing, databases, operating systems, and complex event processing. Since each community faces applications with challenging performance requirements, each of them has developed some of the same optimizations, but often with conflicting terminology and unstated assumptions. This article presents a survey of optimizations for stream processing. It is aimed both at users who need to understand and guide the system’s optimizer and at implementers who need to make engineering tradeoffs. To consolidate terminology, this article is organized as a catalog, in a style similar to catalogs of design patterns or refactorings. To make assumptions explicit and help understand tradeoffs, each optimization is presented with its safety constraints (when does it preserve correctness?) and a profitability experiment (when does it improve performance?). We hope that this survey will help future streaming system builders to stand on the shoulders of giants from not just their own community.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {46},
numpages = {34},
keywords = {Stream processing, optimizations}
}

@article{10.1145/3701731,
author = {Han, Kai and Wang, Jin and Shi, Yunhui and Cai, Hanqin and Ling, Nam and Yin, Baocai},
title = {WTDUN: Wavelet Tree-Structured Sampling and Deep Unfolding Network for Image Compressed Sensing},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3701731},
doi = {10.1145/3701731},
abstract = {Deep unfolding networks have gained increasing attention in the field of compressed sensing (CS) owing to their theoretical interpretability and superior reconstruction performance. However, most existing deep unfolding methods often face the following issues: (1) they learn directly from single-channel images, leading to a simple feature representation that does not fully capture complex features; and (2) they treat various image components uniformly, ignoring the characteristics of different components. To address these issues, we propose a novel wavelet-domain deep unfolding framework named WTDUN, which operates directly on the multi-scale wavelet sub-bands. Our method utilizes the intrinsic sparsity and multi-scale structure of wavelet coefficients to achieve a tree-structured sampling and reconstruction, effectively capturing and highlighting the most important features within images. Specifically, the design of tree-structured reconstruction aims to capture the inter-dependencies among the multi-scale sub-bands, enabling the identification of both fine and coarse features, which can lead to a marked improvement in reconstruction quality. Furthermore, a wavelet domain adaptive sampling method is proposed to greatly improve the sampling capability, which is realized by assigning measurements to each wavelet sub-band based on its importance. Unlike pure deep learning methods that treat all components uniformly, our method introduces a targeted focus on important sub-bands, considering their energy and sparsity. This targeted strategy lets us capture key information more efficiently while discarding less important information, resulting in a more effective and detailed reconstruction. Extensive experimental results on various datasets validate the superior performance of our proposed method.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = dec,
articleno = {33},
numpages = {22},
keywords = {Compressed sensing, deep unfolding, wavelet tree}
}

@inproceedings{10.1145/3131473.3131486,
author = {Kazdaridis, Giannis and Keranidis, Stratos and Symeonidis, Polychronis and Dias, Paulo Sousa and Gon\c{c}alves, Pedro and Loureiro, Bruno and Gjanci, Petrika and Petrioli, Chiara},
title = {EVERUN: Enabling Power Consumption Monitoring in Underwater Networking Platforms},
year = {2017},
isbn = {9781450351478},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131473.3131486},
doi = {10.1145/3131473.3131486},
abstract = {The energy restricted nature of underwater sensor networks directly affects the expected lifetime of autonomous deployments and limits the capabilities for long term underwater monitoring. Towards the goal of developing energy-efficient protocols and algorithms, researchers and equipment vendors require in-depth understanding of the power consumption characteristics of underwater hardware when deployed in-field. In this work, we introduce the EVERUN power monitoring framework, consisting of hardware and software components that were integrated with real equipment of the SUNRISE testbed facilities. Through the execution of a wide set of experiments under realistic conditions, we highlighted the limitations of model-based energy evaluation tools and characterized the energy efficiency performance of key protocols and mechanisms. The accuracy of the collected power data, along with the interesting derived findings, verified the applicability of our approach in evaluating the energy efficiency performance of proposed solutions.},
booktitle = {Proceedings of the 11th Workshop on Wireless Network Testbeds, Experimental Evaluation &amp; CHaracterization},
pages = {83–90},
numpages = {8},
keywords = {energy efficiency, power consumption monitorin, testbed experimentation, underwater networking},
location = {Snowbird, Utah, USA},
series = {WiNTECH '17}
}

@inproceedings{10.1145/1842752.1842769,
author = {Weyns, Danny and Capilla, Rafael},
title = {Current and emerging topics in software architecture (ECSA 2010 Workshops Summary)},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842769},
doi = {10.1145/1842752.1842769},
abstract = {Since 2004 in St. Andrews (Scotland, U.K.), ECSA the European Conference on Software Architecture (formerly EWSA, the European Workshop on Software Architecture) has been considered as an important meeting point for researchers and practitioners on the topic of software architecture. ECSA has matured from a workshop format to a full software engineering conference in the subfield of software architecture.This year, ECSA has become more ambitious and expanded its scope and schedule up to four full days. The program includes a series of tutorials, a doctoral mentoring program, and four full-day workshops. New and existing software challenges have led to a variety of trends in software architecture research, which makes the conference and workshops more attractive and promotes the discussion on current and emerging topics.Based on the scientific and technical interest of the topics, the innovativeness of workshop topics, and the capacity of the conference workshop program, the workshop co-chairs selected four workshops from the nine submitted proposals. We summarize the aims and goals of each workshop and the contributions accepted for the four workshops:• 2nd International Workshop on Software Ecosystems (EcoSys). Piers Campbell, Faheem Ahmed, Jan Bosch, Sliger Jansen.• 1st International Workshop on Measurability of Security in Software Architectures (MeSSa). Reijo Savola, Teemu Kranst\'{e}n, Antti Evesti.• 8th Nordic Workshop on Model Driven Software Engineering (NW-MODE). Andrzej Wasowski, Dragos Truscan, Ludwik Kuzniarz.• 1st International Workshop on Variability in Software Product Line Architectures (VARI-ARCH). Alexander Helleboogh, Paris Avgeriou, Nelis Boucke, Patryck Heymans.The ECSA 2010 Workshop co-chairs would like to thanks all workshop organizers for their effort and enthusiasm to attract submission in different software architecture research topics and make the ECSA 2010 workshops a success.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {59–62},
numpages = {4},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@inproceedings{10.1145/3442391.3442409,
author = {G\"{o}ttmann, Hendrik and Bacher, Isabelle and Gottwald, Nicolas and Lochau, Malte},
title = {Static Analysis Techniques for Efficient Consistency Checking of Real-Time-Aware DSPL Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442409},
doi = {10.1145/3442391.3442409},
abstract = {Dynamic Software Product Lines (DSPL) have recently gained momentum as integrated engineering methodology for (self-)adaptive software. DSPL enhance statically configurable software by enabling run-time reconfiguration to facilitate continuous adaptations to changing environmental contexts. In a previous work, we presented a model-based methodology for specifying and automatically analyzing real-time constraints of reconfiguration decisions in a feature-oriented and compositional way. Internally, we translate real-time-aware DSPL specifications into timed automata serving as input for off-the-shelf model checkers like Uppaal for automatically checking semantic consistency properties. However, due to the very high computational complexity of model checking timed automata, those consistency checks suffer from scalability problems thus obstructing practical applications of the proposed approach. In this paper, we tackle this issue by investigating various kinds of static-analysis techniques that (1) aim to avoid expensive model checker calls by statically detecting certain classes of inconsistencies beforehand and otherwise (2) perform model reduction by detecting and merging equivalence states prior to model checker calls. The results of our experimental evaluation show very promising performance improvements achievable by those techniques, especially by the model-reduction approach.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {17},
numpages = {9},
keywords = {Dynamic Software Product Lines, Reconfiguration Decisions, Timed Automata},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.1109/ICSE-NIER52604.2021.00026,
author = {Shahin, Ramy},
title = {Towards modal software engineering},
year = {2021},
isbn = {9780738133249},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER52604.2021.00026},
doi = {10.1109/ICSE-NIER52604.2021.00026},
abstract = {In this paper we introduce the notion of Modal Software Engineering: automatically turning sequential, deterministic programs into semantically equivalent programs efficiently operating on inputs coming from multiple overlapping worlds. We are drawing an analogy between modal logics, and software application domains where multiple sets of inputs (multiple worlds) need to be processed efficiently. Typically those sets highly overlap, so processing them independently would involve a lot of redundancy, resulting in lower performance, and in many cases intractability. Three application domains are presented: reasoning about feature-based variability of Software Product Lines (SPLs), probabilistic programming, and approximate programming.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {86–90},
numpages = {5},
location = {Virtual Event, Spain},
series = {ICSE-NIER '21}
}

@inproceedings{10.1145/3636534.3649345,
author = {Zhang, Tianfang and Phan, Huy and Tang, Zijie and Shi, Cong and Wang, Yan and Yuan, Bo and Chen, Yingying},
title = {Inaudible Backdoor Attack via Stealthy Frequency Trigger Injection in Audio Spectrogram},
year = {2024},
isbn = {9798400704895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636534.3649345},
doi = {10.1145/3636534.3649345},
abstract = {Deep learning-enabled Voice User Interfaces (VUIs) have surpassed human-level performance in acoustic perception tasks. However, the significant cost associated with training these models compels users to rely on third-party data or outsource training services. Such emerging trends have drawn substantial attention to training-phase attacks, particularly backdoor attacks. Such attacks implant hidden trigger patterns (e.g., tones, environmental sounds) into the model during training, thereby manipulating the model's predictions in the inference phase. However, existing backdoor attacks can be easily undermined in practice as the inserted triggers are audible. Users may notice such attacks when listening to the training data and remaining alert for suspicious sounds. In this work, we present a novel audio backdoor attack that exploits completely inaudible triggers in the frequency domain of the audio spectrograms. Specifically, we optimize the trigger to be a frequency-domain pattern with the energy below the noise floor (e.g., background and hardware noises) at any given frequency, thereby rendering the trigger inaudible. To realize such attacks, we design a strategy that automatically generates inaudible triggers in the spectrum supported by commodity playback devices (e.g., smartphones and laptops). We further develop optimization techniques to enhance the trigger's robustness against speech content and onset variations. Experiments on hotword and speaker recognition indicate that our attack can achieve attack success rates of more than 98.2% and 81.0% under digital and physical attack scenarios. The results also demonstrate the trigger's inaudibility with a Signal-to-Noise Ratio (SNR) less than -3.54 dB against background noises. We further verify that our attack can successfully bypass state-of-the-art backdoor defense strategies based on learning and audio processing.},
booktitle = {Proceedings of the 30th Annual International Conference on Mobile Computing and Networking},
pages = {31–45},
numpages = {15},
keywords = {inaudible attack, audio backdoor attack, frequency injection, audio spectrogram},
location = {Washington D.C., DC, USA},
series = {ACM MobiCom '24}
}

@inproceedings{10.1109/MODELS-C.2019.00077,
author = {Kaur, Navpreet and Famelis, Michalis},
title = {Towards reasoning about product lines with design choices},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00077},
doi = {10.1109/MODELS-C.2019.00077},
abstract = {While designing changes to Software Product Lines (SPLs), engineers may need to consider many alternative SPL designs. In the absence of enough information to pick an appropriate SPL design, they face design-time uncertainty about how to make the appropriate design choices. The combination of the two dimensions (variability and design choices) leads to Software Product Lines with Design Choices (SPLDCs). We propose Tyson, an Alloy-based domain-specific language for modelling SPLDCs and reasoning about their structural properties. We illustrate the applicability and feasibility of Tyson with a worked example, showing the kind of nuanced feedback necessary for meaningful analysis of SPLs with design choices.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems Companion},
pages = {483–492},
numpages = {10},
location = {Munich, Germany},
series = {MODELS '19 Companion}
}

@article{10.1109/TASLP.2014.2387385,
author = {Hua, Guang and Goh, Jonathan and Thing, Vrizlynn L. L.},
title = {Time-spread echo-based audio watermarking with optimized imperceptibility and robustness},
year = {2015},
issue_date = {February 2015},
publisher = {IEEE Press},
volume = {23},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2387385},
doi = {10.1109/TASLP.2014.2387385},
abstract = {We present a time-spread echo-based audio watermarking scheme with optimized imperceptibility and robustness. Specifically, convex optimization based finite-impulse-response (FIR) filter design is utilized to obtain the optimal echo filter coefficients. The desired power spectrum of the echo filter is shaped by the proposed maximum power spectral margin (MPSM) and the absolute threshold of hearing (ATH) of human auditory system (HAS) to ensure the optimal imperceptibility. Meanwhile, the auto-correlation function of the echo filter coefficients is specified as the constraint in the problem formulation, which controls the robustness in terms of watermark detection. In this way, a joint optimization of imperceptibility and robustness can be quantitatively performed. As a result, the proposed watermarking scheme is superior to existing solutions such as the ones based on pseudo noise (PN) sequence or modified pseudo noise (MPN) sequence. Note that the designed echo kernel is also highly secure in that only with the same filter coefficients can one successfully detect the watermark. Experimental results are provided to evaluate the imperceptibility and robustness of the proposed watermarking scheme.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {227–239},
numpages = {13},
keywords = {FIR filter design, audio watermarking, convex optimization, time-spread echo}
}

@inproceedings{10.1145/1353482.1353496,
author = {Chakravarthy, Venkat and Regehr, John and Eide, Eric},
title = {Edicts: implementing features with flexible binding times},
year = {2008},
isbn = {9781605580449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1353482.1353496},
doi = {10.1145/1353482.1353496},
abstract = {In a software product line, the binding time of a feature is the time at which one decides to include or exclude a feature from a product. Typical binding site implementations are intended to support a single binding time only, e.g., compile time or run time. Sometimes, however, a product line must support features with variable binding times. For instance, a product line may need to include both embedded system configurations, in which features are selected and optimized early, and desktop configurations, in which client programs choose features on demand.We present a new technique for implementing the binding sites of features that require flexible binding times. Our technique combines design patterns and aspect-oriented programming: a pattern encapsulates the variation point, and targeted aspects---called edicts---set the binding times of the pattern participants. We describe our approach and demonstrate its usefulness by creating a middleware product line capable of serving the desktop and embedded domains. Our product line is based on JacORB, a middleware platform with many dynamically configurable features. By using edicts to select features at compile time, we create a version of JacORB more suited to resource-constrained environments. By configuring four JacORB subsystems via edicts, we achieve a 32.2% reduction in code size. Our examples show that our technique effectively modularizes binding-time concerns, supporting both compile-time optimization and run-time flexibility as needed.},
booktitle = {Proceedings of the 7th International Conference on Aspect-Oriented Software Development},
pages = {108–119},
numpages = {12},
location = {Brussels, Belgium},
series = {AOSD '08}
}

@inproceedings{10.1145/3510003.3510190,
author = {Randrianaina, Georges Aaron and T\"{e}rnava, Xhevahire and Khelladi, Djamel Eddine and Acher, Mathieu},
title = {On the benefits and limits of incremental build of software configurations: an exploratory study},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510190},
doi = {10.1145/3510003.3510190},
abstract = {Software projects use build systems to automate the compilation, testing, and continuous deployment of their software products. As software becomes increasingly configurable, the build of multiple configurations is a pressing need, but expensive and challenging to implement. The current state of practice is to build independently (a.k.a., clean build) a software for a subset of configurations. While incremental build has been studied for software evolution and relatively small changes of the source code, it has surprisingly not been considered for software configurations. In this exploratory study, we examine the benefits and limits of building software configurations incrementally, rather than always building them cleanly. By using five real-life configurable systems as subjects, we explore whether incremental build works, outperforms a sequence of clean builds, is correct w.r.t. clean build, and can be used to find an optimal ordering for building configurations. Our results show that incremental build is feasible in 100% of the times in four subjects and in 78% of the times in one subject. In average, 88.5% of the configurations could be built faster with incremental build while also finding several alternatives faster incremental builds. However, only 60% of faster incremental builds are correct. Still, when considering those correct incremental builds with clean builds, we could always find an optimal order that is faster than just a collection of clean builds with a gain up to 11.76%.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1584–1596},
numpages = {13},
keywords = {build systems, configurable software systems, configuration build},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/2576768.2598305,
author = {Lopez-Herrejon, Roberto Erick and Javier Ferrer, Javier and Chicano, Francisco and Haslinger, Evelyn Nicole and Egyed, Alexander and Alba, Enrique},
title = {A parallel evolutionary algorithm for prioritized pairwise testing of software product lines},
year = {2014},
isbn = {9781450326629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2576768.2598305},
doi = {10.1145/2576768.2598305},
abstract = {Software Product Lines (SPLs) are families of related software systems, which provide different feature combinations. Different SPL testing approaches have been proposed. However, despite the extensive and successful use of evolutionary computation techniques for software testing, their application to SPL testing remains largely unexplored. In this paper we present the Parallel Prioritized product line Genetic Solver (PPGS), a parallel genetic algorithm for the generation of prioritized pairwise testing suites for SPLs. We perform an extensive and comprehensive analysis of PPGS with 235 feature models from a wide range of number of features and products, using 3 different priority assignment schemes and 5 product prioritization selection strategies. We also compare PPGS with the greedy algorithm prioritized-ICPL. Our study reveals that overall PPGS obtains smaller covering arrays with an acceptable performance difference with prioritized-ICPL.},
booktitle = {Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {1255–1262},
numpages = {8},
keywords = {combinatorial interaction testing, feature models, pairwise testing, software product lines},
location = {Vancouver, BC, Canada},
series = {GECCO '14}
}

@inproceedings{10.1145/3593434.3593436,
author = {Ramos-Vidal, Delfina},
title = {Reengineering legacy document information systems: Challenges and solutions},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593434.3593436},
doi = {10.1145/3593434.3593436},
abstract = {Since internet applications have reached a satisfactory level of maturity, large information systems have been developed to manage and facilitate access to documents. Simultaneously, there was an enormous international effort to digitise documents, enabling access via the internet. This endeavour facilitated the access of researchers to extensive document repositories and libraries, while also aiding companies in organising their documents. Two decades later, these vast databases are reasonably clean and well-organised, although the software used to manage and feed them is gradually becoming obsolete. Therefore, it is imperative to continuously reengineer the software to maintain optimal functionality. Furthermore, after the initial effort to digitise documents and create the initial metadata, it is reasonable to augment the metadata information pertaining to the documents. As such, two necessities are apparent: improving support for reengineering legacy document information systems and enabling data model updates and schema evolution to accommodate new information. Our goal is to automate the reengineering process as a whole.},
booktitle = {Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
pages = {286–291},
numpages = {6},
keywords = {automated development, document information systems, schema evolution, software reengineering},
location = {Oulu, Finland},
series = {EASE '23}
}

@inproceedings{10.1109/ICSE.2007.36,
author = {Trujillo, Salvador and Batory, Don and Diaz, Oscar},
title = {Feature Oriented Model Driven Development: A Case Study for Portlets},
year = {2007},
isbn = {0769528287},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSE.2007.36},
doi = {10.1109/ICSE.2007.36},
abstract = {Model Driven Development (MDD) is an emerging paradigm for software construction that uses models to specify programs, and model transformations to synthesize executables. Feature Oriented Programming (FOP) is a paradigm for software product lines where programs are synthesized by composing features. Feature Oriented Model Driven Development (FOMDD) is a blend of FOP and MDD that shows how products in a software product line can be synthesized in an MDD way by composing features to create models, and then transforming these models into executables. We present a case study of FOMDD on a product line of portlets, which are components of web portals. We reveal mathematical properties of portlet synthesis that helped us to validate the correctness of our abstractions, tools, and specifications, as well as optimize portlet synthesis.},
booktitle = {Proceedings of the 29th International Conference on Software Engineering},
pages = {44–53},
numpages = {10},
series = {ICSE '07}
}

@article{10.1145/3701229,
author = {Wang, Guolong and Wu, Xun and Tu, Xun and Liu, Zhaoyuan and Yan, Junchi},
title = {Unsupervised Video Moment Retrieval with Knowledge-Based Pseudo-Supervision Construction},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3701229},
doi = {10.1145/3701229},
abstract = {Video moment retrieval locates a specified moment by a sentence query. Recent approaches have made remarkable advancements with large-scale video-sentence annotations. These annotations require extensive human labor and expertise, leading to the need for unsupervised fashion. Generating pseudo-supervision from videos is an effective strategy. With the power of the large-scale pre-trained model, we introduce knowledge into constructing pseudo-supervision. The main technical challenge is improving pseudo-supervision diversity and alleviating noise brought by external knowledge. To address these problems, we propose two Knowledge-Based Pseudo-Supervision Construction (KPSC) strategies: KPSC-P and KPSC-F. They all follow two steps: generating diverse samples and alleviating knowledge chaos. The main difference is that the former first learns a representation space with prompt tuning, while the latter directly utilizes data information. KPSC-P has two modules: (1) Proposal Prompt (PP): Generate temporal proposals; (2) Verb Prompt (VP): Generate pseudo-queries with noun-verb patterns. KPSC-F also has two modules: (1) Captioner: Generating candidate queries; (2) Filter: Alleviating knowledge chaos. Thus, our KPSC involves two attempts to extract knowledge from pre-trained models. Extensive experiments show that our attempts outperform the existing unsupervised methods on two public datasets (Charades-STA and ActivityNet-Captions) and perform on par with several methods using stronger supervision.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {23},
numpages = {26},
keywords = {Knowledge Introduction, Video Moment Retrieval, Pseudo-Supervision Construction}
}

@inproceedings{10.1145/2556624.2556635,
author = {Devroey, Xavier and Perrouin, Gilles and Cordy, Maxime and Schobbens, Pierre-Yves and Legay, Axel and Heymans, Patrick},
title = {Towards statistical prioritization for software product lines testing},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556635},
doi = {10.1145/2556624.2556635},
abstract = {Software Product Lines (SPLs) are inherently difficult to test due to the combinatorial explosion of the number of products to consider. To reduce the number of products to test, sampling techniques such as combinatorial interaction testing have been proposed. They usually start from a feature model and apply a coverage criterion (e.g. pairwise feature interaction or dissimilarity) to generate tractable, fault-finding, lists of configurations to be tested. Prioritization can also be used to sort/generate such lists, optimizing coverage criteria or weights assigned to features. However, current sampling/prioritization techniques barely take product behaviour into account. We explore how ideas of statistical testing, based on a usage model (a Markov chain), can be used to extract configurations of interest according to the likelihood of their executions. These executions are gathered in featured transition systems, compact representation of SPL behaviour. We discuss possible scenarios and give a prioritization procedure validated on a web-based learning management software.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {10},
numpages = {7},
keywords = {SPL testing, prioritization, statistical testing},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@inproceedings{10.1145/941350.941369,
author = {Li, Li (Erran) and Sinha, Prasun},
title = {Throughput and energy efficiency in topology-controlled multi-hop wireless sensor networks},
year = {2003},
isbn = {1581137648},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/941350.941369},
doi = {10.1145/941350.941369},
abstract = {In the context of multi-hop wireless networks, various topology control algorithms have been proposed to adapt the transmission range of nodes based on local information while maintaining a connected topology. These algorithms are particularly suited for deployment in sensor networks which typically consist of energy constrained sensors. Sensor nodes should support power adaptation in order to use the benefits of topology control for energy conservation. In this paper, we design a framework for evaluating the performance of topology control algorithms using overall network throughput, and total energy consumption per packet delivered, as the metrics. Our goal is to identify the scenarios in which topology control improves the network performance. We supplement our analysis with ns2 simulations using the cone-based topology control algorithm [10, 19].Based on our analysis and simulations, we find that link layer retransmissions are essential with topology control to avoid throughput degradation due to increase in number of hops in lightly loaded networks. In heavily loaded networks, the throughput can be improved by a factor up to k2, where k is the average factor of reduction in transmission range using topology control. Studies of energy consumption reveal that improvements of up to $k^4$ can be obtained using topology control. However, these improvements decrease as the traffic pattern shifts from local (few hop connections) to non-local (hop lengths of the order of the diameter of the network). These results can be used to guide the deployment of topology control algorithms in sensor networks.},
booktitle = {Proceedings of the 2nd ACM International Conference on Wireless Sensor Networks and Applications},
pages = {132–140},
numpages = {9},
keywords = {ad-hoc networks, sensor networks, topology control, wireless networks},
location = {San Diego, CA, USA},
series = {WSNA '03}
}

@inproceedings{10.1145/2364474.2364488,
author = {Fl\ae{}n\o{} Werk, Michael and Ahnfelt-R\o{}nne, Joakim and Larsen, Ken Friis},
title = {An embedded DSL for stochastic processes: research article},
year = {2012},
isbn = {9781450315777},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364474.2364488},
doi = {10.1145/2364474.2364488},
abstract = {We present a domain specific language embedded in Haskell for specifying stochastic processes, called SPL;. It is designed with the goal of matching the notation used in mathematical finance, where the price of a financial contract is specified using stochastic processes and distributions.SPL; is declarative in the sense that it is agnostic of the choice of discretization and of the computational model. We provide an implementation of SPL; that performs Monte Carlo simulation using GPGPU, and we present data indicating that this gives a 100x speedup compared to hand-written sequential C, and that the speedup scales linearly with the number of available cores.},
booktitle = {Proceedings of the 1st ACM SIGPLAN Workshop on Functional High-Performance Computing},
pages = {93–102},
numpages = {10},
keywords = {code generation, embedded domain specific language, gpgpu, haskell, monte-carlo simulation, opencl, stochastic processes},
location = {Copenhagen, Denmark},
series = {FHPC '12}
}

@inproceedings{10.1145/3205455.3205513,
author = {Chen, Tao and Li, Miqing and Yao, Xin},
title = {On the effects of seeding strategies: a case for search-based multi-objective service composition},
year = {2018},
isbn = {9781450356183},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205455.3205513},
doi = {10.1145/3205455.3205513},
abstract = {Service composition aims to search a composition plan of candidate services that produces the optimal results with respect to multiple and possibly conflicting Quality-of-Service (QoS) attributes, e.g., latency, throughput and cost. This leads to a multi-objective optimization problem for which evolutionary algorithm is a promising solution. In this paper, we investigate different ways of injecting knowledge about the problem into the Multi-Objective Evolutionary Algorithm (MOEA) by seeding. Specifically, we propose four alternative seeding strategies to strengthen the quality of the initial population for the MOEA to start working with. By using the real-world WS-DREAM dataset, we conduced experimental evaluations based on 9 different workflows of service composition problems and several metrics. The results confirm the effectiveness and efficiency of those seeding strategies. We also observed that, unlike the discoveries for other problem domains, the implication of the number of seeds on the service composition problems is minimal, for which we investigated and discussed the possible reasons.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1419–1426},
numpages = {8},
keywords = {evolutionary algorithm, multi-objective optimization, search-based software engineering, seeding strategy, service composition},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@inproceedings{10.1145/1023833.1023847,
author = {Beltrame, Giovanni and Palermo, Gianluca and Sciuto, Donatella and Silvano, Cristina},
title = {Plug-in of power models in the StepNP exploration platform: analysis of power/performance trade-offs},
year = {2004},
isbn = {1581138903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1023833.1023847},
doi = {10.1145/1023833.1023847},
abstract = {In this paper, we propose a power/performance estimation layer designed for StepNP, a system-level architecture simulation and exploration platform for Network Processors and Multi-Processor Systems-on-Chip (MP-SoCs). The first goal of our work is to plug-in PIRATE, a parameterizable Network on-Chip in the StepNP platform, to support a fast exploration of on-chip interconnection networks. Up to now, StepNP does not provide any energy profiling, so our second goal is to dynamically plug-in power models of the different system components to provide power estimates quickly. The proposed power/performance exploration framework is based on a power characterization methodology and a system-level simulator to dynamically profile the given network application. This framework is intended to be used at different levels of the design, considering several levels of accuracy and taking full advantage of the StepNP performance profiling features. Experimental results are provided for the exploration of an ARM-based MP-SOC including a configurable NoC-IP executing an IPv4 forwarding application.},
booktitle = {Proceedings of the 2004 International Conference on Compilers, Architecture, and Synthesis for Embedded Systems},
pages = {85–92},
numpages = {8},
keywords = {low-power design, multiprocessor, network on chip, platform based design},
location = {Washington DC, USA},
series = {CASES '04}
}

@inproceedings{10.1145/258492.258512,
author = {Ranganathan, Parthasarathy and Pai, Vijay S. and Adve, Sarita V.},
title = {Using speculative retirement and larger instruction windows to narrow the performance gap between memory consistency models},
year = {1997},
isbn = {0897918908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/258492.258512},
doi = {10.1145/258492.258512},
booktitle = {Proceedings of the Ninth Annual ACM Symposium on Parallel Algorithms and Architectures},
pages = {199–210},
numpages = {12},
location = {Newport, Rhode Island, USA},
series = {SPAA '97}
}

@inproceedings{10.1145/1403375.1403436,
author = {Rimolo-Donadio, Renato and Schuster, Christian and Gu, Xiaoxiong and Kwark, Young H. and Ritter, Mark B.},
title = {Analysis and optimization of the recessed probe launch for high frequency measurements of PCB interconnects},
year = {2008},
isbn = {9783981080131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1403375.1403436},
doi = {10.1145/1403375.1403436},
abstract = {Measurements of internal printed circuit board (PCB) structures such as striplines and vias face the problem of launching clean test signals into the device under test (DUT). Traditionally, coaxial connectors or surface probing with high frequency microprobes are used to provide interfaces to test equipment. Both approaches have to be carefully optimized in order to give adequate results for the multi-GHz range. This paper discusses a different access technique, the recessed probe launch (RPL), which was previously used by the authors for measurements up to 40 GHz. Full-wave 3D electromagnetic modeling is applied to analyze the parasitics of the proposed launch technique and to find strategies for its optimization. Comparison to measurement shows that the models are able to predict the major physics of the launch but several details still need to be explored, e.g. accurate modeling of the microprobes, material parameters, and network analyzer calibration.},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe},
pages = {252–255},
numpages = {4},
location = {Munich, Germany},
series = {DATE '08}
}

@inproceedings{10.5555/850995.855824,
author = {Balzer, R.},
title = {Supporting product line development},
year = {1996},
isbn = {0818677252},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The author considers how the maturation of the software development field has been inextricably tied to its adoption of a product line orientation. Only by specializing in a particular application domain can software development organizations develop the expertise required for that domain, develop cost effective products, and establish a market niche. He argues that software development processes are fundamentally flawed because maintenance, including enhancement and evolution, is performed on source code. This source code has been hand optimized by programmers and that optimization has spread information and built up implicit dependencies among the parts.},
booktitle = {Proceedings of the 10th International Software Process Workshop},
pages = {29},
keywords = {cost effective products, market niche, optimization, software development organizations, software development processes, software engineering, software maintenance, software product line development support, source code},
series = {ISPW '96}
}

@inproceedings{10.1145/3489517.3530562,
author = {Gu, Jiaqi and Zhu, Hanqing and Feng, Chenghao and Jiang, Zixuan and Liu, Mingjie and Zhang, Shuhan and Chen, Ray T. and Pan, David Z.},
title = {ADEPT: automatic differentiable DEsign of photonic tensor cores},
year = {2022},
isbn = {9781450391429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489517.3530562},
doi = {10.1145/3489517.3530562},
abstract = {Photonic tensor cores (PTCs) are essential building blocks for optical artificial intelligence (AI) accelerators based on programmable photonic integrated circuits. PTCs can achieve ultra-fast and efficient tensor operations for neural network (NN) acceleration. Current PTC designs are either manually constructed or based on matrix decomposition theory, which lacks the adaptability to meet various hardware constraints and device specifications. To our best knowledge, automatic PTC design methodology is still unexplored. It will be promising to move beyond the manual design paradigm and "nurture" photonic neurocomputing with AI and design automation. Therefore, in this work, for the first time, we propose a fully differentiable framework, dubbed ADEPT, that can efficiently search PTC designs adaptive to various circuit footprint constraints and foundry PDKs. Extensive experiments show superior flexibility and effectiveness of the proposed ADEPT framework to explore a large PTC design space. On various NN models and benchmarks, our searched PTC topology outperforms prior manually-designed structures with competitive matrix representability, 2\texttimes{}-30\texttimes{} higher footprint compactness, and better noise robustness, demonstrating a new paradigm in photonic neural chip design. The code of ADEPT is available at link using the TorchONN library.},
booktitle = {Proceedings of the 59th ACM/IEEE Design Automation Conference},
pages = {937–942},
numpages = {6},
location = {San Francisco, California},
series = {DAC '22}
}

@article{10.1145/2220365.2220368,
author = {Joisha, Pramod G. and Schreiber, Robert S. and Banerjee, Prithviraj and Boehm, Hans-J. and Chakrabarti, Dhruva R.},
title = {On a Technique for Transparently Empowering Classical Compiler Optimizations on Multithreaded Code},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {0164-0925},
url = {https://doi.org/10.1145/2220365.2220368},
doi = {10.1145/2220365.2220368},
abstract = {A large body of data-flow analyses exists for analyzing and optimizing sequential code. Unfortunately, much of it cannot be directly applied on parallel code, for reasons of correctness. This article presents a technique to automatically, aggressively, yet safely apply sequentially-sound data-flow transformations, without change, on shared-memory programs. The technique is founded on the notion of program references being “siloed” on certain control-flow paths. Intuitively, siloed references are free of interference from other threads within the confines of such paths. Data-flow transformations can, in general, be unblocked on siloed references.The solution has been implemented in a widely used compiler. Results on benchmarks from SPLASH-2 show that performance improvements of up to 41% are possible, with an average improvement of 6% across all the tested programs over all thread counts.},
journal = {ACM Trans. Program. Lang. Syst.},
month = jun,
articleno = {9},
numpages = {42},
keywords = {Data-flow analysis, parallel-program optimization}
}

@inproceedings{10.5555/285730.285780,
author = {Lam, Jimmy and Delosme, Jean-Marc},
title = {Performance of a new annealing schedule},
year = {1988},
isbn = {0818688645},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
abstract = {A new simulated annealing schedule has been developed; its application to the standard cell placement and the traveling salesman problems results in a two to twenty-four times speedup over annealing schedules currently available in the literature. Since it uses only statistical quantities, the annealing schedule is applicable to general combinatorial optimization problems.},
booktitle = {Proceedings of the 25th ACM/IEEE Design Automation Conference},
pages = {306–311},
numpages = {6},
location = {Atlantic City, New Jersey, USA},
series = {DAC '88}
}

@inproceedings{10.1145/1062455.1062551,
author = {Verlage, Martin and Kiesgen, Thomas},
title = {Five years of product line engineering in a small company},
year = {2005},
isbn = {1581139632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062455.1062551},
doi = {10.1145/1062455.1062551},
abstract = {In 1999, a new team at MARKET MAKER Software AG began to develop a software product line for managing and displaying stock market data and financial market news. The basic idea was to use web technology in all applications for delivering services to customers. It soon turned out that the company had to change both the processes and the organization. This report summarizes the changes made and the lessons learned over the past five years, when the product line idea was introduced into a small company which faced the pressure to quickly market the first product line instances.},
booktitle = {Proceedings of the 27th International Conference on Software Engineering},
pages = {534–543},
numpages = {10},
keywords = {SME, experience report, product line engineering, project management},
location = {St. Louis, MO, USA},
series = {ICSE '05}
}

@article{10.1145/505532.505551,
author = {Butler, Greg},
title = {Generative techniques for product lines},
year = {2001},
issue_date = {November 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/505532.505551},
doi = {10.1145/505532.505551},
abstract = {A software product line leverages the knowledge of one or more domains in order to achieve short time-to-market, cost savings, and high quality software. The highest level of reuse comes by using domain-specific languages or visual builders to describe a member of the product line, and to generate the member from the description. Generative techniques can help us to capture the configuration knowledge for a product line and use it to generate concrete family members. This workshop focuses on technical issues of product lines, rather than economic issues.},
journal = {SIGSOFT Softw. Eng. Notes},
month = nov,
pages = {74–76},
numpages = {3}
}

@inproceedings{10.3115/100964.1138530,
author = {Zue, Victor},
title = {Prosody, performance evaluation, databases, and ISAT},
year = {1989},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/100964.1138530},
doi = {10.3115/100964.1138530},
abstract = {This session began with a one-hour invited tutorial on prosody given by Dr. Janet Pierrehumbert of AT&amp;T Bell Laboratories, which was followed by several short reports on database collection and performance evaluation activities dealing with various aspects of the spoken speech system development effort. Issues and discussions on methods of formal evaluation were particularly relevant for natural language processing, where formal evaluation is in many respects an elusive goal.},
booktitle = {Proceedings of the Workshop on Speech and Natural Language},
pages = {5–36},
numpages = {32},
location = {Philadelphia, Pennsylvania},
series = {HLT '89}
}

@inproceedings{10.1145/2488222.2488268,
author = {Schneider, Scott and Hirzel, Martin and Gedik, Bu\u{g}ra},
title = {Tutorial: stream processing optimizations},
year = {2013},
isbn = {9781450317580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2488222.2488268},
doi = {10.1145/2488222.2488268},
abstract = {This tutorial starts with a survey of optimizations for streaming applications. The survey is organized as a catalog that introduces uniform terminology and a common categorization of optimizations across disciplines, such as data management, programming languages, and operating systems. After this survey, the tutorial continues with a deep-dive into the fission optimization, which automatically transforms streaming applications for data-parallelism. Fission helps an application improve its throughput by taking advantage of multiple cores in a machine, or, in the case of a distributed streaming engine, multiple machines in a cluster. While the survey of optimizations covers a wide range of work from the literature, the in-depth discussion of fission relies more heavily on the presenters' own research and experience in the area. The tutorial concludes with a discussion of open research challenges in the field of stream processing optimizations.},
booktitle = {Proceedings of the 7th ACM International Conference on Distributed Event-Based Systems},
pages = {249–258},
numpages = {10},
keywords = {data parallelism, fission, stream processing},
location = {Arlington, Texas, USA},
series = {DEBS '13}
}

@article{10.1145/3644391,
author = {Yao, Rujing and Wu, Ou},
title = {A Taxonomy for Learning with Perturbation and Algorithms},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3644391},
doi = {10.1145/3644391},
abstract = {Weighting strategy prevails in machine learning. For example, a common approach in robust machine learning is to exert low weights on samples which are likely to be noisy or quite hard. This study summarizes another less-explored strategy, namely, perturbation. Various incarnations of perturbation have been utilized but it has not been explicitly revealed. Learning with perturbation is called perturbation learning and a systematic taxonomy is constructed for it in this study. In our taxonomy, learning with perturbation is divided on the basis of the perturbation targets, directions, inference manners, and granularity levels. Many existing learning algorithms including some classical ones can be understood with the constructed taxonomy. Alternatively, these algorithms share the same component, namely, perturbation in their procedures. Furthermore, a family of new learning algorithms can be obtained by varying existing learning algorithms with our taxonomy. Specifically, three concrete new learning algorithms are proposed for robust machine learning. Extensive experiments on image classification and text sentiment analysis verify the effectiveness of the three new algorithms. Learning with perturbation can also be used in other various learning scenarios, such as imbalanced learning, clustering, regression, and so on.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {129},
numpages = {38},
keywords = {Sample weighting, perturbation, robust machine learning, learning taxonomy}
}

@inproceedings{10.1145/1858996.1859009,
author = {Vierhauser, Michael and Gr\"{u}nbacher, Paul and Egyed, Alexander and Rabiser, Rick and Heider, Wolfgang},
title = {Flexible and scalable consistency checking on product line variability models},
year = {2010},
isbn = {9781450301169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858996.1859009},
doi = {10.1145/1858996.1859009},
abstract = {The complexity of product line variability models makes it hard to maintain their consistency over time regardless of the modeling approach used. Engineers thus need support for detecting and resolving inconsistencies. We describe experiences of applying a tool-supported approach for incremental consistency checking on variability models. Our approach significantly improves the overall performance and scalability compared to batch-oriented techniques and allows providing immediate feedback to modelers. It is extensible as new consistency constraints can easily be added. Furthermore, the approach is flexible as it is not limited to variability models and it also checks the consistency of the models with the underlying code base of the product line. We report the results of a thorough evaluation based on real-world product line models and discuss lessons learned.},
booktitle = {Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering},
pages = {63–72},
numpages = {10},
keywords = {incremental consistency checking, lessons learned, memory consumption, model consistency, performance, software product lines, variability models},
location = {Antwerp, Belgium},
series = {ASE '10}
}

@article{10.1145/3701732,
author = {Chen, Zan and Wang, Tao and Li, Jun and Guo, Wenlong and Feng, Yuanjing and Qian, Xueming and Hou, Xingsong},
title = {Discard Significant Bits of Compressed Sensing: A Robust Image Coding for Resource-Limited Contexts},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3701732},
doi = {10.1145/3701732},
abstract = {Compressed sensing (CS) provides a robust and simple framework for compressing images in resource-constrained environments. However, CS-based image coding schemes often have poor rate-distortion (R-D) performance, particularly due to the quantization process. Our research indicates that leveraging the image prior enables the estimation of most significant bits (MSBs) from least significant bits (LSBs), which provides a quantization strategy to improve R-D performance without increasing coding complexity. That is discarding MSBs of measurements, and only transmitting LSBs to the decoder side. At the decoder side, we reconstruct images by solving an inverse-quantization set-constrained CS optimization problem. Our approach further employs a tailored designed deep denoiser as the proximal operator to enhance the reconstructed image quality. Extensive experimental results demonstrate that the proposed scheme achieves satisfactory performance, with promising R-D results (PSNR gains over 1.71 dB than JPEG at 0.50 bpp compression ratio), and robust bit error and loss resilience (reconstructed 29.98 dB even with 50% bit loss at 0.50 bpp compression ratio), meanwhile having lower encoding complexity (less than half encoding time of CCSDS-IDC).},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = dec,
articleno = {31},
numpages = {25},
keywords = {Compressive sensing, Quantization, Image prior, Robust image coding}
}

@inproceedings{10.1145/1449913.1449918,
author = {Mendonca, Marcilio and Wasowski, Andrzej and Czarnecki, Krzysztof and Cowan, Donald},
title = {Efficient compilation techniques for large scale feature models},
year = {2008},
isbn = {9781605582672},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1449913.1449918},
doi = {10.1145/1449913.1449918},
abstract = {Feature modeling is used in generative programming and software product-line engineering to capture the common and variable properties of programs within an application domain. The translation of feature models to propositional logics enabled the use of reasoning systems, such as BDD engines, for the analysis and transformation of such models and interactive configurations. Unfortunately, the size of a BDD structure is highly sensitive to the variable ordering used in its construction and an inappropriately chosen ordering may prevent the translation of a feature model into a BDD representation of a tractable size. Finding an optimal order is NP-hard and has for long been addressed by using heuristics.We review existing general heuristics and heuristics from the hardware circuits domain and experimentally show that they are not effective in reducing the size of BDDs produced from feature models. Based on that analysis we introduce two new heuristics for compiling feature models to BDDs. We demonstrate the effectiveness of these heuristics using publicly available and automatically generated models. Our results are directly applicable in construction of feature modeling tools.},
booktitle = {Proceedings of the 7th International Conference on Generative Programming and Component Engineering},
pages = {13–22},
numpages = {10},
keywords = {configuration, feature modeling, formal verification, model-driven development, software-product lines},
location = {Nashville, TN, USA},
series = {GPCE '08}
}

@article{10.1109/TASLP.2024.3378099,
author = {Leer, Peter and Jensen, Jesper and Tan, Zheng-Hua and \O{}stergaard, Jan and Bramsl\o{}w, Lars},
title = {How to Train Your Ears: Auditory-Model Emulation for Large-Dynamic-Range Inputs and Mild-to-Severe Hearing Losses},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3378099},
doi = {10.1109/TASLP.2024.3378099},
abstract = {Advanced auditory models are useful in designing signal-processing algorithms for hearing-loss compensation or speech enhancement. Such auditory models provide rich and detailed descriptions of the auditory pathway, and might allow for individualization of signal-processing strategies, based on physiological measurements. However, these auditory models are often computationally demanding, requiring significant time to compute. To address this issue, previous studies have explored the use of deep neural networks to emulate auditory models and reduce inference time. While these deep neural networks offer impressive efficiency gains in terms of computational time, they may suffer from uneven emulation performance as a function of auditory-model frequency-channels and input sound pressure level, making them unsuitable for many tasks. In this study, we demonstrate that the conventional machine-learning optimization objective used in existing state-of-the-art methods is the primary source of this limitation. Specifically, the optimization objective fails to account for the frequency- and level-dependencies of the auditory model, caused by a large input dynamic range and different types of hearing losses emulated by the auditory model. To overcome this limitation, we propose a new optimization objective that explicitly embeds the frequency- and level-dependencies of the auditory model. Our results show that this new optimization objective significantly improves the emulation performance of deep neural networks across relevant input sound levels and auditory-model frequency channels, without increasing the computational load during inference. Addressing these limitations is essential for advancing the application of auditory models in signal-processing tasks, ensuring their efficacy in diverse scenarios.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {2006–2020},
numpages = {15}
}

@inproceedings{10.1145/3689484.3690735,
author = {Ghallab, Karim and Ziadi, Tewfik and Chalal, Zaak},
title = {An Extensible Feature-Oriented Approach for Fine-Grained Code Quality Analysis},
year = {2024},
isbn = {9798400712111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689484.3690735},
doi = {10.1145/3689484.3690735},
abstract = {Assessing code quality is crucial for effective software maintenance and evolution. Traditional tools like SonarQube offer valuable insights at the application level but lack the granularity needed for detailed, feature-specific analysis. This paper emphasizes the importance of feature-oriented code quality analysis, often overlooked by mainstream tools due to the challenge of correlating high-level feature descriptions with low-level code implementations. To tackle this issue, we leverage existing feature location techniques to introduce a novel approach enabling granular analysis tailored to specific application features. We discuss the motivations for this approach, highlighting its potential to improve precision in enhancement and maintenance strategies. Additionally, this paper introduces a tool-based approach known as InsightMapper. We also present a study demonstrating the benefits of this method through the analysis of two case studies, featuring a recognized benchmark in the feature location domain.},
booktitle = {Proceedings of the 23rd ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {15–28},
numpages = {14},
keywords = {ArgoUml, Code Quality Analysis, Feature Location, Feature-Oriented, InsightMapper, Mobioos Forge, Software Product Lines, SonarQube, eShopOnContainers},
location = {Pasadena, CA, USA},
series = {GPCE '24}
}

@inproceedings{10.1145/3320326.3320374,
author = {Moutai, Fatima-zahra and Hsaini, Sara and Azzouzi, Salma and Charaf, My El Hassan},
title = {Security Testing Approach for IaaS Infrastructure},
year = {2019},
isbn = {9781450366458},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320326.3320374},
doi = {10.1145/3320326.3320374},
abstract = {Cloud computing represents a new computing model that poses many demanding security issues at all levels. The variety of delivery models presents different security challenges depending on the model and the quality of service (QoS) requirements of consumers. Confidentiality, integrity, and availability are key concerns for cloud computing providers and consumers. In this paper, we present a novel secure architecture-Based Conformance Testing in order to check not only if the behavior of the cloud environment is conforms to its specification but also if respects some security policies. The proposed approach shows how to cope with problems encountered in cloud distributed testing using distributed testing rules.},
booktitle = {Proceedings of the 2nd International Conference on Networking, Information Systems &amp; Security},
articleno = {42},
numpages = {5},
keywords = {Cloud Computing, Distributed Testing, IaaS, Security, Security policies, Testing Rules},
location = {Rabat, Morocco},
series = {NISS '19}
}

@inproceedings{10.1145/3168365.3168377,
author = {Ananieva, Sofia and Klare, Heiko and Burger, Erik and Reussner, Ralf},
title = {Variants and Versions Management for Models with Integrated Consistency Preservation},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168377},
doi = {10.1145/3168365.3168377},
abstract = {Modern software systems are often developed and maintained by describing them in several modeling and programming languages. To reduce complexity and improve understandability of such systems, models represent specific views on the system. These views have semantic interrelations (e.g., by sharing common or dependent information) that need to be kept consistent during evolution of the system. Apart from that, modern systems need to run in many different contexts and be highly configurable to satisfy the demand for fully customizable products. Such variable systems often comprise various dependencies from which inconsistencies may arise. Combining solutions for consistency management with variants and versions management, however, comes with many challenges.In this research-in-progress paper, we introduce the VaVe approach which makes variants and versions management aware of automated consistency preservation in the context of multi-view modeling. We explain core features of the approach and reason about its benefits and limitations.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {3–10},
numpages = {8},
keywords = {Delta-Based Consistency Preservation, Software Product Lines, Variability Management},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/3474624.3476010,
author = {Ferreira, Thiago do Nascimento and Vergilio, Silvia Regina and Kessentini, Marouane},
title = {Implementing Search-Based Software Engineering Approaches with Nautilus},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3476010},
doi = {10.1145/3474624.3476010},
abstract = {Search-Based Software Engineering (SBSE) approaches adopt search-based techniques to solve Software Engineering (SE) optimization problems. Among these techniques, evolutionary algorithms are the most popular and successfully used, such as multi-objective evolutionary algorithms. However, some challenges still need to be addressed. Firstly, SE problems are complex and commonly impacted by many conflicting factors. In this context, the use of many-objective algorithms is necessary. Secondly, the users very often do not recognise the found solutions as feasible because these solutions are usually not generated considering the users’ needs and preferences. Thus, to deal properly with this situation, preference-based algorithms should be applied. Moreover, there are some practical issues regarding the choice of operators, evaluation of algorithms and visualization of solutions. Existing frameworks do not provide support to address these challenges. To overcome these limitations, we present Nautilus, an open-source Java web-platform tool that works with plugins to ease the addition of new problem instances, implementation of search operators and different multi and many-objective optimization algorithms, guided (or not) by human participation. This paper describes Nautilus-NRP, an extension implemented to address the Next Release Problem (NRP). NRP refers to the selection of requirements to be implemented in the next release of a software and is used to illustrate Nautilus’ main functionalities and how it can be extended to solve a SE problem. Link for the video: https://youtu.be/2dbwslTrvhg.},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {303–308},
numpages = {6},
keywords = {many-objective optimization, next release problem, preference-based algorithms},
location = {Joinville, Brazil},
series = {SBES '21}
}

@inproceedings{10.1145/2627373.2627376,
author = {Stojanov, Alen and Ofenbeck, Georg and Rompf, Tiark and P\"{u}schel, Markus},
title = {Abstracting Vector Architectures in Library Generators: Case Study Convolution Filters},
year = {2014},
isbn = {9781450329378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2627373.2627376},
doi = {10.1145/2627373.2627376},
abstract = {We present FGen, a program generator for high performance convolution operations (finite-impulse-response filters). The generator uses an internal mathematical DSL to enable structural optimization at a high level of abstraction. We use FGen as a testbed to demonstrate how to provide modular and extensible support for modern SIMD vector architectures in a DSL-based generator. Specifically, we show how to combine staging and generic programming with type classes to abstract over both the data type (real or complex) and the target architecture (e.g., SSE or AVX) when mapping DSL expressions to C code with explicit vector intrinsics. Benchmarks shows that the generated code is highly competitive with commercial libraries.},
booktitle = {Proceedings of ACM SIGPLAN International Workshop on Libraries, Languages, and Compilers for Array Programming},
pages = {14–19},
numpages = {6},
keywords = {Abstraction, Domain Specific Language, Performance, Program Generation, SIMD Vector Extensions},
location = {Edinburgh, United Kingdom},
series = {ARRAY'14}
}

@inproceedings{10.1145/3510003.3510053,
author = {Xiang, Yi and Huang, Han and Zhou, Yuren and Li, Sizhe and Luo, Chuan and Lin, Qingwei and Li, Miqing and Yang, Xiaowei},
title = {Search-based diverse sampling from real-world software product lines},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510053},
doi = {10.1145/3510003.3510053},
abstract = {Real-world software product lines (SPLs) often encompass enormous valid configurations that are impossible to enumerate. To understand properties of the space formed by all valid configurations, a feasible way is to select a small and valid sample set. Even though a number of sampling strategies have been proposed, they either fail to produce diverse samples with respect to the number of selected features (an important property to characterize behaviors of configurations), or achieve diverse sampling but with limited scalability (the handleable configuration space size is limited to 1013). To resolve this dilemma, we propose a scalable diverse sampling strategy, which uses a distance metric in combination with the novelty search algorithm to produce diverse samples in an incremental way. The distance metric is carefully designed to measure similarities between configurations, and further diversity of a sample set. The novelty search incrementally improves diversity of samples through the search for novel configurations. We evaluate our sampling algorithm on 39 real-world SPLs. It is able to generate the required number of samples for all the SPLs, including those which cannot be counted by sharpSAT, a state-of-the-art model counting solver. Moreover, it performs better than or at least competitively to state-of-the-art samplers regarding diversity of the sample set. Experimental results suggest that only the proposed sampler (among all the tested ones) achieves scalable diverse sampling.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1945–1957},
numpages = {13},
keywords = {distance metric, diverse sampling, novelty search, software product lines},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@article{10.1145/3492762,
author = {Sobhy, Dalia and Minku, Leandro and Bahsoon, Rami and Kazman, Rick},
title = {Continuous and Proactive Software Architecture Evaluation: An IoT Case},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3492762},
doi = {10.1145/3492762},
abstract = {Design-time evaluation is essential to build the initial software architecture to be deployed. However, experts’ assumptions made at design-time are unlikely to remain true indefinitely in systems that are characterized by scale, hyperconnectivity, dynamism, and uncertainty in operations (e.g. IoT). Therefore, experts’ design-time decisions can be challenged at run-time. A continuous architecture evaluation that systematically assesses and intertwines design-time and run-time decisions is thus necessary. This paper proposes the first proactive approach to continuous architecture evaluation of the system leveraging the support of simulation. The approach evaluates software architectures by not only tracking their performance over time, but also forecasting their likely future performance through machine learning of simulated instances of the architecture. This enables architects to make cost-effective informed decisions on potential changes to the architecture. We perform an IoT case study to show how machine learning on simulated instances of architecture can fundamentally guide the continuous evaluation process and influence the outcome of architecture decisions. A series of experiments is conducted to demonstrate the applicability and effectiveness of the approach. We also provide the architect with recommendations on how to best benefit from the approach through choice of learners and input parameters, grounded on experimentation and evidence.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {46},
numpages = {54},
keywords = {Continuous evaluation, software architecture evaluation, time series forecasting, IoT}
}

@inproceedings{10.1145/2464996.2464998,
author = {Liu, Chenyang and Jamal, Muhammad Hasan and Kulkarni, Milind and Prakash, Arun and Pai, Vijay},
title = {Exploiting domain knowledge to optimize parallel computational mechanics codes},
year = {2013},
isbn = {9781450321303},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2464996.2464998},
doi = {10.1145/2464996.2464998},
abstract = {An important emerging problem domain in computational science and engineering is the development of multi-scale computational methods for complex problems in mechanics that span multiple spatial and temporal scales. An attractive approach to solving these problems is recursive decomposition: the problem is broken up into a tree of loosely coupled sub-problems which can be solved independently and then coupled back together to obtain the desired solution. However, a particular problem can be solved in myriad ways by coupling the sub-problems together in different tree orders. As we argue in this paper, the space of possible orders is vast, the performance gap between an arbitrary order and the best order is potentially quite large, and the likelihood that a domain scientist can find the best order to solve a problem on a particular machine is vanishingly small. In this paper, we present a system that uses domain-specific knowledge captured in computational libraries to optimize code written in a conventional language (C). The system generates efficient coupling orders to solve computational mechanics problems using recursive decomposition. Our system adopts the inspector-executor paradigm, where the problem is inspected and a novel heuristic finds an effective implementation based on domain properties evaluated by a cost model. The derived implementation is then executed by a parallel run-time system (Cilk) which achieves optimal parallel performance. We demonstrate that our cost model is highly correlated with actual application runtime, that our proposed technique outperforms non-decomposed and non-multiscale methods. The code generated by the heuristic also outperforms alternate scheduling strategies, as well as over 99% of randomly-generated recursive decompositions sampled from the space of possible solutions.},
booktitle = {Proceedings of the 27th International ACM Conference on International Conference on Supercomputing},
pages = {25–36},
numpages = {12},
keywords = {domain-specific optimization, multi-scale method, recursive decomposition},
location = {Eugene, Oregon, USA},
series = {ICS '13}
}

@article{10.1145/3377871,
author = {Besta, Maciej and Fischer, Marc and Ben-Nun, Tal and Stanojevic, Dimitri and Licht, Johannes De Fine and Hoefler, Torsten},
title = {Substream-Centric Maximum Matchings on FPGA},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1936-7406},
url = {https://doi.org/10.1145/3377871},
doi = {10.1145/3377871},
abstract = {Developing high-performance and energy-efficient algorithms for maximum matchings is becoming increasingly important in social network analysis, computational sciences, scheduling, and others. In this work, we propose the first maximum matching algorithm designed for FPGAs; it is energy-efficient and has provable guarantees on accuracy, performance, and storage utilization. To achieve this, we forego popular graph processing paradigms, such as vertex-centric programming, that often entail large communication costs. Instead, we propose a substream-centric approach, in which the input stream of data is divided into substreams processed independently to enable more parallelism while lowering communication costs. We base our work on the theory of streaming graph algorithms and analyze 14 models and 28 algorithms. We use this analysis to provide theoretical underpinning that matches the physical constraints of FPGA platforms. Our algorithm delivers high performance (more than 4\texttimes{} speedup over tuned parallel CPU variants), low memory, high accuracy, and effective usage of FPGA resources. The substream-centric approach could easily be extended to other algorithms to offer low-power and high-performance graph processing on FPGAs.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = apr,
articleno = {8},
numpages = {33},
keywords = {Graph computations, energy-efficient graph processing, streaming graph processing}
}

@inproceedings{10.1145/2896839.2896845,
author = {Karim, Muhammad Rezaul and Al Alam, S. M. Didar and Kabeer, Shaikh Jeeshan and Ruhe, G\"{u}nther and Baluta, Basil and Mahmud, Shafquat},
title = {Applying data analytics towards optimized issue management: an industrial case study},
year = {2016},
isbn = {9781450341547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896839.2896845},
doi = {10.1145/2896839.2896845},
abstract = {This document describes our experience of applying data analytics at Plexina, a leading IT company working in the healthcare domain. The main goal of the project was to identify factors currently affecting issue management and to make analytics based suggestions for optimizing the process. Various statistical and machine learning techniques were applied on a data set extracted from six releases of Plexina, containing more than 666 issues. Statistical techniques successfully identified the various factors that leads to estimation inaccuracy related to issues as well as identified the hidden relationships existing among various variables. The employed predictive analytic models was also successful to some extent, in predicting effort estimation related inaccuracy associated with the issues. The insights provided by the entire data analytics study can be of great help to product managers or the developers to make more informed decisions. In addition, the guidelines presented in this paper based on the lessons learnt can be applied to other data analytics and academia-industry collaboration project.},
booktitle = {Proceedings of the 4th International Workshop on Conducting Empirical Studies in Industry},
pages = {7–13},
numpages = {7},
keywords = {case study, data analytics, industry-academia collaboration, issue management},
location = {Austin, Texas},
series = {CESI '16}
}

@inproceedings{10.1145/1926385.1926457,
author = {Joisha, Pramod G. and Schreiber, Robert S. and Banerjee, Prithviraj and Boehm, Hans J. and Chakrabarti, Dhruva R.},
title = {A technique for the effective and automatic reuse of classical compiler optimizations on multithreaded code},
year = {2011},
isbn = {9781450304900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1926385.1926457},
doi = {10.1145/1926385.1926457},
abstract = {A large body of data-flow analyses exists for analyzing and optimizing sequential code. Unfortunately, much of it cannot be directly applied on parallel code, for reasons of correctness. This paper presents a technique to automatically, aggressively, yet safely apply sequentially-sound data-flow transformations, without change, on shared-memory programs. The technique is founded on the notion of program references being "siloed" on certain control-flow paths. Intuitively, siloed references are free of interference from other threads within the confines of such paths. Data-flow transformations can, in general, be unblocked on siloed references.The solution has been implemented in a widely used compiler. Results on benchmarks from SPLASH-2 show that performance improvements of up to 41% are possible, with an average improvement of 6% across all the tested programs over all thread counts.},
booktitle = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {623–636},
numpages = {14},
keywords = {data-flow analysis, parallel-program optimization},
location = {Austin, Texas, USA},
series = {POPL '11}
}

@article{10.1145/264645.264658,
author = {Kieras, David E. and Wood, Scott D. and Meyer, David E.},
title = {Predictive engineering models based on the EPIC architecture for a multimodal high-performance human-computer interaction task},
year = {1997},
issue_date = {Sept. 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1073-0516},
url = {https://doi.org/10.1145/264645.264658},
doi = {10.1145/264645.264658},
abstract = {Engineering models of human performance permit some aspects of usability of interface designs to be predicted from an analysis of the task, and thus they can replace to some extent expensive user-testing data. We successfully predicted human performance in telephone operator tasks with engineering models constructed in the EPIC (Executive Process-Interactive Control) architecture for human information processing, which is especially suited for modeling multimodal, complex tasks, and has demonstrated success in other task domains. Several models were constructed on an a priori basis to represent different hypotheses about how operators coordinate their activities to produce rapid task performance. The models predicted  the total time with useful accuracy and clarified  some important properties of the task. The best model was based directly on the GOMS analysis of the task and made simple assumptions about the operator's task strategy, suggesting that EPIC models are a feasible approach to predicting performance in multimodal high-performance tasks.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = sep,
pages = {230–275},
numpages = {46},
keywords = {cognitive models, usability engineering}
}

@inproceedings{10.1145/3613904.3642613,
author = {Li, Ke and Zhang, Ruidong and Chen, Siyuan and Chen, Boao and Sakashita, Mose and Guimbretiere, Francois and Zhang, Cheng},
title = {EyeEcho: Continuous and Low-power Facial Expression Tracking on Glasses},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642613},
doi = {10.1145/3613904.3642613},
abstract = {In this paper, we introduce EyeEcho, a minimally-obtrusive acoustic sensing system designed to enable glasses to continuously monitor facial expressions. It utilizes two pairs of speakers and microphones mounted on glasses, to emit encoded inaudible acoustic signals directed towards the face, capturing subtle skin deformations associated with facial expressions. The reflected signals are processed through a customized machine-learning pipeline to estimate full facial movements. EyeEcho samples at 83.3 Hz with a relatively low power consumption of 167mW. Our user study involving 12 participants demonstrates that, with just four minutes of training data, EyeEcho achieves highly accurate tracking performance across different real-world scenarios, including sitting, walking, and after remounting the devices. Additionally, a semi-in-the-wild study involving 10 participants further validates EyeEcho’s performance in naturalistic scenarios while participants engage in various daily activities. Finally, we showcase EyeEcho’s potential to be deployed on a commercial-off-the-shelf (COTS) smartphone, offering real-time facial expression tracking.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {319},
numpages = {24},
keywords = {Acoustic Sensing, Eye-mounted Wearable, Facial Expression Tracking, Low-power},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3127479.3131621,
author = {Traub, Jonas and Bre\ss{}, Sebastian and Rabl, Tilmann and Katsifodimos, Asterios and Markl, Volker},
title = {Optimized on-demand data streaming from sensor nodes},
year = {2017},
isbn = {9781450350280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127479.3131621},
doi = {10.1145/3127479.3131621},
abstract = {Real-time sensor data enables diverse applications such as smart metering, traffic monitoring, and sport analysis. In the Internet of Things, billions of sensor nodes form a sensor cloud and offer data streams to analysis systems. However, it is impossible to transfer all available data with maximal frequencies to all applications. Therefore, we need to tailor data streams to the demand of applications.We contribute a technique that optimizes communication costs while maintaining the desired accuracy. Our technique schedules reads across huge amounts of sensors based on the data-demands of a huge amount of concurrent queries. We introduce user-defined sampling functions that define the data-demand of queries and facilitate various adaptive sampling techniques, which decrease the amount of transferred data. Moreover, we share sensor reads and data transfers among queries. Our experiments with real-world data show that our approach saves up to 87% in data transmissions.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
pages = {586–597},
numpages = {12},
keywords = {adaptive sampling, on-demand streaming, oversampling, real-time analysis, sensor data, sensor sharing, user-defined sampling},
location = {Santa Clara, California},
series = {SoCC '17}
}

@inproceedings{10.1145/2983323.2983807,
author = {Carbone, Paris and Traub, Jonas and Katsifodimos, Asterios and Haridi, Seif and Markl, Volker},
title = {Cutty: Aggregate Sharing for User-Defined Windows},
year = {2016},
isbn = {9781450340731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983323.2983807},
doi = {10.1145/2983323.2983807},
abstract = {Aggregation queries on data streams are evaluated over evolving and often overlapping logical views called windows. While the aggregation of periodic windows were extensively studied in the past through the use of aggregate sharing techniques such as Panes and Pairs, little to no work has been put in optimizing the aggregation of very common, non-periodic windows. Typical examples of non-periodic windows are punctuations and sessions which can implement complex business logic and are often expressed as user-defined operators on platforms such as Google Dataflow or Apache Storm. The aggregation of such non-periodic or user-defined windows either falls back to expensive, best-effort aggregate sharing methods, or is not optimized at all.In this paper we present a technique to perform efficient aggregate sharing for data stream windows, which are declared as user-defined functions (UDFs) and can contain arbitrary business logic. To this end, we first introduce the concept of User-Defined Windows (UDWs), a simple, UDF-based programming abstraction that allows users to programmatically define custom windows. We then define semantics for UDWs, based on which we design Cutty, a low-cost aggregate sharing technique. Cutty improves and outperforms the state of the art for aggregate sharing on single and multiple queries. Moreover, it enables aggregate sharing for a broad class of non-periodic UDWs. We implemented our techniques on Apache Flink, an open source stream processing system, and performed experiments demonstrating orders of magnitude of reduction in aggregation costs compared to the state of the art.},
booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
pages = {1201–1210},
numpages = {10},
keywords = {data stream aggregation, data stream optimisation, data stream processing, data stream windows, data streams, data structures, databases, functional programming, operator sharing, programming models, user-defined functions},
location = {Indianapolis, Indiana, USA},
series = {CIKM '16}
}

@inproceedings{10.1145/502585.502623,
author = {Nie, Zaiqing and Kambhampati, Subbarao},
title = {Joint optimization of cost and coverage of query plans in data integration},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502623},
doi = {10.1145/502585.502623},
abstract = {Existing approaches for optimizing queries in data integration use decoupled strategies--attempting to optimize coverage and cost in two separate phases. Since sources tend to have a variety of access limitations, such phased optimization of cost and coverage can unfortunately lead to expensive planning as well as highly inefficient plans. In this paper we present techniques for joint optimization of cost and coverage of the query plans. Our algorithms search in the space of parallel query plans that support multiple sources for each subgoal conjunct. The refinement of the partial plans takes into account the potential parallelism between source calls, and the binding compatibilities between the sources included in the plan. We start by introducing and motivating our query plan representation. We then briefly review how to compute the cost and coverage of a parallel plan. Next, we provide both a System-R style query optimization algorithm as well as a greedy local search algorithm for searching in the space of such query plans. Finally we present a simulation study that demonstrates that the plans generated by our approach will be significantly better, both in terms of planning cost, and in terms of plan execution cost, compared to the existing approaches.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {223–230},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/2420942.2420943,
author = {Bo\v{s}kovi\'{c}, Marko and Mussbacher, Gunter and Ga\v{s}evi\'{c}, Dragan and Bagheri, Ebrahim},
title = {The Fourth International Workshop on Non-functional System Properties in Domain Specific Modeling Languages (NFPinDSML2012)},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420943},
doi = {10.1145/2420942.2420943},
abstract = {The International Workshop on Non-functional System Properties in Domain Specific Modeling Languages (NFPinDSML) series traditionally takes place as part of the Satellite Events of the ACM/IEEE International Conference on Model Driven Engineering Languages and Systems (MODELS). Traditionally, NFPinDSML gathers researchers and practitioners interested in the estimation and evaluation of system quality and their integration in Domain Specific Modeling Languages and Model Driven Engineering in general. This paper is the summary of the fourth NFPinDSML workshop which was affiliated with MODELS 2012.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {1},
numpages = {2},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@article{10.1145/3660641,
author = {Diffallah, Zhor and Ykhlef, Hadjer and Bouarfa, Hafida},
title = {Teacher-Student Framework for Polyphonic Semi-supervised Sound Event Detection: Survey and Empirical Analysis},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3660641},
doi = {10.1145/3660641},
abstract = {Polyphonic sound event detection refers to the task of automatically identifying sound events occurring simultaneously in an auditory scene. Due to the inherent complexity and variability of real-world auditory scenes, building robust detectors for polyphonic sound event detection poses a significant challenge. The task becomes furthermore challenging without sufficient annotated data to develop sound event detection systems under a supervised learning regime. In this article, we explore the recent developments in polyphonic sound event detection, with a particular emphasis on the application of Teacher-Student techniques within the semi-supervised learning paradigm. Unlike previous works, we have consolidated and organized the fragmented literature on Teacher-Student techniques for polyphonic sound event detection. By examining the latest research, categorizing Teacher-Student approaches, and conducting an empirical study to assess the performance of each approach, this survey offers valuable insights and practical guidance for researchers and practitioners in the field. Our findings highlight the potential benefits of utilizing multiple learners, ensuring consistent predictions, and making thoughtful choices regarding perturbation strategies.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {90},
numpages = {44},
keywords = {Polyphonic sound event detection, Teacher-Student framework, semi-supervised learning}
}

@inproceedings{10.1145/1066677.1067012,
author = {Blake, M. Brian and Cleary, Kevin and Ranjan, Sohan R. and Ibanez, Luis and Gary, Kevin},
title = {Use case-driven component specification: a medical applications perspective to product line development},
year = {2005},
isbn = {1581139640},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066677.1067012},
doi = {10.1145/1066677.1067012},
abstract = {Modular and flexible software components can be useful for reuse across a class of domain-specific applications or product lines. By varying the composition of components suited to a particular product line, an assortment of applications can be developed to support differing operational needs. A top-down approach to the design components for a specific application may be effective, however a more evolutionary approach is needed to support the specification of components suited for a class of applications. In addition, such evolutionary approaches require support for the knowledge transfer that must occur from domain experts, who are not software experts, to skilled software engineers. By combining concepts from Software Product Line Development (SPLD) and other evolutionary design techniques, a new, use case-driven approach has been created called Component-Based Product Line Analysis and Design (C-PLAD). This approach was used to develop components in the domain of image-guided surgery applications.},
booktitle = {Proceedings of the 2005 ACM Symposium on Applied Computing},
pages = {1470–1477},
numpages = {8},
keywords = {component specifications, generation of component-based systems, medical domain, software lifecycle},
location = {Santa Fe, New Mexico},
series = {SAC '05}
}

@inproceedings{10.1109/CCGrid.2014.25,
author = {Almeida, Andr\'{e} and Dantas, Francisco and Cavalcante, Everton and Batista, Thais},
title = {A branch-and-bound algorithm for autonomic adaptation of multi-cloud applications},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.25},
doi = {10.1109/CCGrid.2014.25},
abstract = {Adaptation is an important concern in cloud-based applications composed of services provided by different cloud providers since cloud services can suffer from Quality of Services (QoS) fluctuations. Other conditions that can also trigger an adaptation process at runtime are the unavailability of services or the violation of user-defined policies. Moreover, the detection and reaction on such changes must be done in an autonomic way, without the need of user intervention. This paper presents a dynamic adaptation approach for multi-cloud applications supported by a Branch-and-Bound (B&amp;B) algorithm in order to optimize the adaptation process itself when selecting the services to be deployed within the application. Computational experiments comparing the B&amp;B algorithm with another algorithm that evaluates all possible configurations for adapting an application showed that the B&amp;B algorithm is faster than the previous version. This new algorithm brings benefits to the scalability of the adaptation process, which can deal with large configurations of multi-cloud applications composed by a plethora of cloud services.},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {315–323},
numpages = {9},
keywords = {branch-and-bound algorithm, dynamic adaptation, multi-cloud applications, optimization, scalability},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@inproceedings{10.1145/3442391.3442403,
author = {Michelon, Gabriela K. and Linsbauer, Lukas and Assun\c{c}\~{a}o, Wesley K.G. and Fischer, Stefan and Egyed, Alexander},
title = {A Hybrid Feature Location Technique for Re-engineeringSingle Systems into Software Product Lines},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442403},
doi = {10.1145/3442391.3442403},
abstract = {Software product lines (SPLs) are known for improving productivity and reducing time-to-market through the systematic reuse of assets. SPLs are adopted mainly by re-engineering existing system variants. Feature location techniques (FLTs) support the re-engineering process by mapping the variants’ features to their implementation. However, such FLTs do not perform well when applied to single systems. In this way, there is a lack of FLTs to aid the re-engineering process of a single system into an SPL. In this work, we present a hybrid technique that consists of two complementary types of analysis: i) a dynamic analysis by runtime monitoring traces of scenarios in which features of the system are exercised individually, and ii) a static analysis for refining overlapping traces. We evaluate our technique on three subject systems by computing the common metrics used in FL research. We thus computed Precision, Recall, and F-Score at the line- and method-level of source code. In addition to that, one of the systems has a ground truth available, which we also used for comparing results. Results show that our FLT reached an average of 68-78% precision and 72-81% recall on two systems at the line-level, and 67-65% precision and 68-48% recall at the method-level. In these systems, most of the implementation can be covered by the exercise of the features. For the largest system, our technique reached a precision of up to 99% at the line-level, 94% at the method-level, and 44% when comparing to traces. However, due to its size, it was difficult to reach high code coverage during execution, and thus the recall obtained was on average of 28% at the line-level, 25% at the method-level, and 30% when comparing to traces. The main contribution of this work is a hybrid FLT, its publicly available implementation, and a replication package for comparisons and future studies.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {11},
numpages = {9},
keywords = {feature location, runtime monitoring, software reuse, traceability},
location = {Krems, Austria},
series = {VaMoS '21}
}

@article{10.1145/2898354,
author = {Andreetta, Christian and B\'{e}got, Vivien and Berthold, Jost and Elsman, Martin and Henglein, Fritz and Henriksen, Troels and Nordfang, Maj-Britt and Oancea, Cosmin E.},
title = {FinPar: A Parallel Financial Benchmark},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/2898354},
doi = {10.1145/2898354},
abstract = {Commodity many-core hardware is now mainstream, but parallel programming models are still lagging behind in efficiently utilizing the application parallelism. There are (at least) two principal reasons for this. First, real-world programs often take the form of a deeply nested composition of parallel operators, but mapping the available parallelism to the hardware requires a set of transformations that are tedious to do by hand and beyond the capability of the common user. Second, the best optimization strategy, such as what to parallelize and what to efficiently sequentialize, is often sensitive to the input dataset and therefore requires multiple code versions that are optimized differently, which also raises maintainability problems.This article presents three array-based applications from the financial domain that are suitable for gpgpu execution. Common benchmark-design practice has been to provide the same code for the sequential and parallel versions that are optimized for only one class of datasets. In comparison, we document (1) all available parallelism via nested map-reduce functional combinators, in a simple Haskell implementation that closely resembles the original code structure, (2) the invariants and code transformations that govern the main trade-offs of a data-sensitive optimization space, and (3) report target cpu and multiversion gpgpu code together with an evaluation that demonstrates optimization trade-offs and other difficulties. We believe that this work provides useful insight into the language constructs and compiler infrastructure capable of expressing and optimizing such applications, and we report in-progress work in this direction.},
journal = {ACM Trans. Archit. Code Optim.},
month = jun,
articleno = {18},
numpages = {27},
keywords = {Data-parallel functional language, fission, fusion, strength reduction}
}

@inproceedings{10.1145/3551349.3556899,
author = {Fernandez-Amoros, David and Heradio, Ruben and Mayr-Dorn, Christoph and Egyed, Alexander},
title = {Scalable Sampling of Highly-Configurable Systems: Generating Random Instances of the Linux Kernel},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556899},
doi = {10.1145/3551349.3556899},
abstract = {Software systems are becoming increasingly configurable. A paradigmatic example is the Linux kernel, which can be adjusted for a tremendous variety of hardware devices, from mobile phones to supercomputers, thanks to the thousands of configurable features it supports. In principle, many relevant problems on configurable systems, such as completing a partial configuration to get the system instance that consumes the least energy or optimizes any other quality attribute, could be solved through exhaustive analysis of all configurations. However, configuration spaces are typically colossal and cannot be entirely computed in practice. Alternatively, configuration samples can be analyzed to approximate the answers. Generating those samples is not trivial since features usually have inter-dependencies that constrain the configuration space. Therefore, getting a single valid configuration by chance is extremely unlikely. As a result, advanced samplers are being proposed to generate random samples at a reasonable computational cost. However, to date, no sampler can deal with highly configurable complex systems, such as the Linux kernel. This paper proposes a new sampler that does scale for those systems, based on an original theoretical approach called extensible logic groups. The sampler is compared against five other approaches. Results show our tool to be the fastest and most scalable one.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {89},
numpages = {12},
keywords = {Kconfig, SAT, binary decision diagrams, configurable systems, random sampling, software product lines, variability modeling},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@article{10.1109/TASLP.2023.3288411,
author = {Malik, Arsalan and Agarwal, Nipun and Settibhaktini, Harshavardhan and Chintanpalli, Ananthakrishna},
title = {Predicting Level-Dependent Changes in Concurrent Vowel Scores Using the 2D-CNN Models},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3288411},
doi = {10.1109/TASLP.2023.3288411},
abstract = {Differences in fundamental frequencies (F0s) are an important cue for segregating multiple speakers. However, the ability to avail this cue for identification varies with sound levels. For different-and same-F0 conditions, the identification scores of both vowels increased from low- to mid-levels and then reduced at higher levels for younger adults with normal hearing (YNH). These subjects benefited from the F0 difference; however, this benefit varied across the levels. The current study aims to develop a deep-neural-network (DNN) model that can predict the level-dependent changes in concurrent-vowel scores for YNH subjects. This DNN-model includes two-dimensional convolutional neural networks (2D-CNNs) in parallel, with the same architecture, to predict the concurrent-vowel scores. The input layer was neural responses of the auditory-nerve model to concurrent vowels. The 2D-CNN models were trained and validated using the subsets of the concurrent vowels from 50, 65, and 75 dB SPL for both F0 conditions, using the batch gradient descent algorithm. The trained model was then fine-tuned with a single epoch. The 2D-CNN models were evaluated against vowel levels (25 to 85 dB SPL) and F0 conditions. Compared with previous models, the current model accurately predicts the level-dependent changes in concurrent vowel scores for different-and same-F0 conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {2558–2566},
numpages = {9}
}

@article{10.1145/3617946.3617958,
author = {Arcaini, Paolo and Miranskyy, Andriy},
title = {Report of the Fourth International Workshop on Quantum Software Engineering (Q-SE 2023)},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/3617946.3617958},
doi = {10.1145/3617946.3617958},
abstract = {The Fourth International Workshop on Quantum Software Engineering (Q-SE 2023), co-located with the 45th International Conference on Software Engineering (ICSE 2023), was held on May 14, 2023 in a hybrid manner in Melbourne, Australia, and online. This report presents the workshop structure, the keynote speech, and the themes of the presented papers.},
journal = {SIGSOFT Softw. Eng. Notes},
month = oct,
pages = {64–65},
numpages = {2}
}

@inproceedings{10.1145/2658761.2658772,
author = {Hess, Benjamin and Gross, Thomas R. and P\"{u}schel, Markus},
title = {Automatic locality-friendly interface extension of numerical functions},
year = {2014},
isbn = {9781450331616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658761.2658772},
doi = {10.1145/2658761.2658772},
abstract = {Raising the level of abstraction is a key concern of software engineering, and libraries (either used directly or as a target of a program generation system) are a successful technique to raise programmer productivity and to improve software quality. Unfortunately successful libraries may contain functions that may not be general enough. For example, many numeric performance libraries contain functions that work on one- or higher-dimensional arrays. A problem arises if a program wants to invoke such a function on a non-contiguous subarray (e.g., in C the column of a matrix or a subarray of an image). If the library developer did not foresee this scenario, the client program must include explicit copy steps before and after the library function call, incurring a possibly high performance penalty. A better solution would be an enhanced library function that allows for the desired access pattern. Exposing the access pattern allows the compiler to optimize for the intended usage scenario(s). As we do not want the library developer to generate all interesting versions manually, we present a tool that takes a library function written in C and generates such a customized function for typical accesses. We describe the approach, discuss limitations, and report on the performance. As example access patterns we consider those most common in numerical applications: striding and block striding, general permutations, as well as scaling. We evaluate the tool on various library functions including filters, scans, reductions, sorting, FFTs, and linear algebra operations. The automatically generated custom version is in most cases significantly faster than using individual steps, offering speed-ups that are typically in the range of 1.2-1.8x.},
booktitle = {Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences},
pages = {83–92},
numpages = {10},
keywords = {Libraries, components, interface extension, performance, preprocessors, programming language features interaction, software product lines},
location = {V\"{a}ster\r{a}s, Sweden},
series = {GPCE 2014}
}

@article{10.1145/3280848,
author = {Pereira, Fernando Magno Quint\~{a}o and Leobas, Guilherme Vieira and Gamati\'{e}, Abdoulaye},
title = {Static Prediction of Silent Stores},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3280848},
doi = {10.1145/3280848},
abstract = {A store operation is called “silent” if it writes in memory a value that is already there. The ability to detect silent stores is important, because they might indicate performance bugs, might enable code optimizations, and might reveal opportunities of automatic parallelization, for instance. Silent stores are traditionally detected via profiling tools. In this article, we depart from this methodology and instead explore the following question: is it possible to predict silentness by analyzing the syntax of programs? The process of building an answer to this question is interesting in itself, given the stochastic nature of silent stores, which depend on data and coding style. To build such an answer, we have developed a methodology to classify store operations in terms of syntactic features of programs. Based on such features, we develop different kinds of predictors, some of which go much beyond what any trivial approach could achieve. To illustrate how static prediction can be employed in practice, we use it to optimize programs running on nonvolatile memory systems.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {44},
numpages = {26},
keywords = {Silent stores, code optimization, machine learning, nonvolatile memory, static analysis}
}

@inproceedings{10.1145/3275245.3275261,
author = {Campos, Denivan and Lima, Crescencio and do Carmo Machado, Ivan},
title = {MERCI: A Method to Evaluate Combinatorial Interaction Testing Tools for Software Product Lines},
year = {2018},
isbn = {9781450365659},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3275245.3275261},
doi = {10.1145/3275245.3275261},
abstract = {Testing a system is a routine activity, and it plays an important role in the software quality assurance process. However, testing highly-configurable systems, such as Software Product Lines (SPL), is a rather complex activity, due to the presence of variability in its engineering process, which increases the number of product configurations to test. The underlying idea to make testing feasible in SPL engineering is to select a small but representative subset of products to test, by employing techniques such as combinatorial interaction testing (CIT). This paper presents Method to Evaluate Combinatorial Interaction (MERCI), a novel method to evaluate the adequacy of existing CIT tools for SPL engineering, with respect to three measures: defect detection, test coverage, and test execution length. We carried out an empirical evaluation to compare four CIT tools: ACTS, CATS, PICTMaster and VPTag. The results show that the method may serve as an affordable strategy to evaluate how the CIT tools could behave in an SPL testing scenario.},
booktitle = {Proceedings of the XVII Brazilian Symposium on Software Quality},
pages = {151–159},
numpages = {9},
keywords = {Combinatorial Interaction Testing, Software Product Lines, Software Testing Strategies, Testing Tools},
location = {Curitiba, Brazil},
series = {SBQS '18}
}

@inproceedings{10.1145/2739482.2768422,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Assun\c{c}\~{a}o, Wesley K.G. and Fischer, Stefan and Vergilio, Silvia R. and Egyed, Alexander},
title = {Genetic Improvement for Software Product Lines: An Overview and a Roadmap},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2768422},
doi = {10.1145/2739482.2768422},
abstract = {Software Product Lines (SPLs) are families of related software systems that provide different combinations of features. Extensive research and application attest to the significant economical and technological benefits of employing SPL practices. However, there are still several challenges that remain open. Salient among them is reverse engineering SPLs from existing variants of software systems and their subsequent evolution. In this paper, we aim at sketching connections between research on these open SPL challenges and ongoing work on Genetic Improvement. Our hope is that by drawing such connections we can spark the interest of both research communities on the exciting synergies at the intersection of these subject areas.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {823–830},
numpages = {8},
keywords = {evolutionary algorithms, genetic improvement, genetic programming, software product lines, variability},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@inproceedings{10.1145/106972.107002,
author = {Hall, C. Brian and O'Brien, Kevin},
title = {Performance characteristics of architectural features of the IBM RISC System/6000},
year = {1991},
isbn = {0897913809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/106972.107002},
doi = {10.1145/106972.107002},
booktitle = {Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {303–309},
numpages = {7},
location = {Santa Clara, California, USA},
series = {ASPLOS IV}
}

@inproceedings{10.5555/381473.381482,
author = {Bosch, Jan},
title = {Software product lines: organizational alternatives},
year = {2001},
isbn = {0769510507},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Software product lines enjoy increasingly wide adoption in the software industry. Most authors focus on the technical and process aspects and assume an organizational model consisting of a domain engineering unit and several application engineering units. In our cooperation with several software development organizations applying software product line principles, we have identified several other organizational models that are employed as well. In this article, we present a number of organizational alternatives, organized around four main models, i.e. development department, business units, domain engineering unit and hierarchical domain engineering units. For each model, its characteristics, applicability and advantages and disadvantages are discussed, as well as an example. Based on an analysis of these models, we present three factors that influence the choice of the organizational model, i.e. product-line assets, the responsibility levels and the type of organizational units.},
booktitle = {Proceedings of the 23rd International Conference on Software Engineering},
pages = {91–100},
numpages = {10},
location = {Toronto, Ontario, Canada},
series = {ICSE '01}
}

@inproceedings{10.1145/3634713.3634732,
author = {Acher, Mathieu},
title = {A Demonstration of End-User Code Customization Using Generative AI},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634732},
doi = {10.1145/3634713.3634732},
abstract = {Producing a variant of code is highly challenging, particularly for individuals unfamiliar with programming. This demonstration introduces a novel use of generative AI to aid end-users in customizing code. We first describe how generative AI can be used to customize code through prompts and instructions, and further demonstrate its potential in building end-user tools for configuring code. We showcase how to transform an undocumented, technical, low-level TikZ into a user-friendly, configurable, Web-based customization tool written in Python, HTML, CSS, and JavaScript and itself configurable. We discuss how generative AI can support this transformation process and traditional variability engineering tasks, such as identification and implementation of features, synthesis of a template code generator, and development of end-user configurators. We believe it is a first step towards democratizing variability programming, opening a path for end-users to adapt code to their needs.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {139–145},
numpages = {7},
keywords = {LLM, code synthesis, customization, end-user programming, generative AI, generator, software product lines, variability},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.5555/3721488.3721814,
author = {Huynh, Sandy and Kim, Clarissa},
title = {Origami Echo: Adaptable Acoustics},
year = {2025},
publisher = {IEEE Press},
abstract = {Classrooms often function as multipurpose spaces for educational activities and events, such as musical performances and presentations hosted by campus organizations. Poor acoustics in these environments can affect speech intelligibility and diminish auditory experiences. Origami Echo addresses these challenges through adaptable acoustic panels and a human-robot interaction (HRI) element-a magic wand interface. The system, inspired by active and variable acoustic principles, allows users to dynamically modify the sound profile of a room, ensuring seamless transitions between diverse applications. Using intuitive gestures with the wand, users can interact with the system while achieving acoustic optimization. Through a Research-through-Design (RtD) approach, the project demonstrated a 25 percent improvement in sound clarity and reduced reverberation time from 0.9 seconds to the ANSI-recommended 0.6 seconds.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1922–1925},
numpages = {4},
keywords = {acoustic panels, active acoustics, classroom acoustics, multipurpose spaces, reverberation time, sound clarity, variable acoustics},
location = {Melbourne, Australia},
series = {HRI '25}
}

@inproceedings{10.1145/375212.375269,
author = {Gacek, Critina and Anastasopoules, Michalis},
title = {Implementing product line variabilities},
year = {2001},
isbn = {1581133588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375212.375269},
doi = {10.1145/375212.375269},
abstract = {Software product lines have numerous members. Thus, a product line infrastructure must cover various systems. This is the significant difference to usual software systems and the reason for additional requirements on the various assets present during software product line engineering. It is imperative that they support the description of the product line as a whole, as well as its instantiation for the derivation of individual products.Literature has already addressed how to create and instantiate generic product line assets, such as domain models and architectures to generate instance specific ones [1, 2, 3], yet little attention has been given on how to actually deal with this genericity at the code level.This paper addresses the issue of handling product line variability at the code level. To this end various implementation approaches are examined with respect to their use in a product line context.},
booktitle = {Proceedings of the 2001 Symposium on Software Reusability: Putting Software Reuse in Context},
pages = {109–117},
numpages = {9},
keywords = {implementation approaches, implementing variabilities, product line variability, software product lines, traceability},
location = {Toronto, Ontario, Canada},
series = {SSR '01}
}

@inproceedings{10.1145/276304.276333,
author = {Subramanian, Subbu N. and Venkataraman, Shivakumar},
title = {Cost-based optimization of decision support queries using transient-views},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276333},
doi = {10.1145/276304.276333},
abstract = {Next generation decision support applications, besides being capable of processing huge amounts of data, require the ability to integrate and reason over data from multiple, heterogeneous data sources. Often, these data sources differ in a variety of aspects such as their data models, the query languages they support, and their network protocols. Also, typically they are spread over a wide geographical area. The cost of processing decision support queries in such a setting is quite high. However, processing these queries often involves redundancies such as repeated access of same data source and multiple execution of similar processing sequences. Minimizing these redundancies would significantly reduce the query processing cost. In this paper, we (1) propose an architecture for processing complex decision support queries involving multiple, heterogeneous data sources; (2) introduce the notion of transient-views — materialized views that exist only in the context of execution of a query — that is useful for minimizing the redundancies involved in the execution of these queries; (3) develop a cost-based algorithm that takes a query plan as input and generates an optimal “covering plan”, by minimizing redundancies in the original plan; (4) validate our approach by means of an implementation of the algorithms and a detailed performance study based on TPC-D benchmark queries on a commercial database system; and finally, (5) compare and contrast our approach with work in related areas, in particular, the areas of answering queries using views and optimization using common sub-expressions. Our experiments demonstrate the practicality and usefulness of transient-views in significantly improving the performance of decision support queries.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {319–330},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@inproceedings{10.1145/3368089.3409675,
author = {Siegmund, Norbert and Ruckel, Nicolai and Siegmund, Janet},
title = {Dimensions of software configuration: on the configuration context in modern software development},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409675},
doi = {10.1145/3368089.3409675},
abstract = {With the rise of containerization, cloud development, and continuous integration and delivery, configuration has become an essential aspect not only to tailor software to user requirements, but also to configure a software system’s environment and infrastructure. This heterogeneity of activities, domains, and processes blurs the term configuration, as it is not clear anymore what tasks, artifacts, or stakeholders are involved and intertwined. However, each re- search study and each paper involving configuration places their contributions and findings in a certain context without making the context explicit. This makes it difficult to compare findings, translate them to practice, and to generalize the results. Thus, we set out to evaluate whether these different views on configuration are really distinct or can be summarized under a common umbrella. By interviewing practitioners from different domains and in different roles about the aspects of configuration and by analyzing two qualitative studies in similar areas, we derive a model of configuration that provides terminology and context for research studies, identifies new research opportunities, and allows practitioners to spot possible challenges in their current tasks. Although our interviewees have a clear view about configuration, it substantially differs due to their personal experience and role. This indicates that the term configuration might be overloaded. However, when taking a closer look, we see the interconnections and dependencies among all views, arriving at the conclusion that we need to start considering the entire spectrum of dimensions of configuration.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {338–349},
numpages = {12},
keywords = {Dimensions of software configuration, configuration management and life cycle, developer study, variability},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1145/3654439,
author = {Gavidia-Calderon, Carlos and Kordoni, Anastasia and Bennaceur, Amel and Levine, Mark and Nuseibeh, Bashar},
title = {The IDEA of Us: An Identity-Aware Architecture for Autonomous Systems},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3654439},
doi = {10.1145/3654439},
abstract = {Autonomous systems, such as drones and rescue robots, are increasingly used during emergencies. They deliver services and provide situational awareness that facilitate emergency management and response. To do so, they need to interact and cooperate with humans in their environment. Human behaviour is uncertain and complex, so it can be difficult to reason about it formally. In this article, we propose IDEA: an adaptive software architecture that enables cooperation between humans and autonomous systems, by leveraging the social identity approach. This approach establishes that group membership drives human behaviour. Identity and group membership are crucial during emergencies, as they influence cooperation among survivors. IDEA systems infer the social identity of surrounding humans, thereby establishing their group membership. By reasoning about groups, we limit the number of cooperation strategies the system needs to explore. IDEA systems select a strategy from the equilibrium analysis of game-theoretic models that represent interactions between group members and the IDEA system. We demonstrate our approach using a search-and-rescue scenario, in which an IDEA rescue robot optimises evacuation by collaborating with survivors. Using an empirically validated agent-based model, we show that the deployment of the IDEA system can reduce median evacuation time by 13.6%.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {164},
numpages = {38},
keywords = {Autonomous systems, game theory, social identity, agent-based modelling}
}

@article{10.1145/3487921,
author = {Hezavehi, Sara M. and Weyns, Danny and Avgeriou, Paris and Calinescu, Radu and Mirandola, Raffaela and Perez-Palacin, Diego},
title = {Uncertainty in Self-adaptive Systems: A Research Community Perspective},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/3487921},
doi = {10.1145/3487921},
abstract = {One of the primary drivers for self-adaptation is ensuring that systems achieve their goals regardless of the uncertainties they face during operation. Nevertheless, the concept of uncertainty in self-adaptive systems is still insufficiently understood. Several taxonomies of uncertainty have been proposed, and a substantial body of work exists on methods to tame uncertainty. Yet, these taxonomies and methods do not fully convey the research community’s perception on what constitutes uncertainty in self-adaptive systems and on the key characteristics of the approaches needed to tackle uncertainty. To understand this perception and learn from it, we conducted a survey comprising two complementary stages in which we collected the views of 54 and 51 participants, respectively. In the first stage, we focused on current research and development, exploring how the concept of uncertainty is understood in the community and how uncertainty is currently handled in the engineering of self-adaptive systems. In the second stage, we focused on directions for future research to identify potential approaches to dealing with unanticipated changes and other open challenges in handling uncertainty in self-adaptive systems. The key findings of the first stage are: (a) an overview of uncertainty sources considered in self-adaptive systems, (b) an overview of existing methods used to tackle uncertainty in concrete applications, (c) insights into the impact of uncertainty on non-functional requirements, (d) insights into different opinions in the perception of uncertainty within the community and the need for standardised uncertainty-handling processes to facilitate uncertainty management in self-adaptive systems. The key findings of the second stage are: (a) the insight that over 70% of the participants believe that self-adaptive systems can be engineered to cope with unanticipated change, (b) a set of potential approaches for dealing with unanticipated change, (c) a set of open challenges in mitigating uncertainty in self-adaptive systems, in particular in those with safety-critical requirements. From these findings, we outline an initial reference process to manage uncertainty in self-adaptive systems. We anticipate that the insights on uncertainty obtained from the community and our proposed reference process will inspire valuable future research on self-adaptive systems.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = dec,
articleno = {10},
numpages = {36},
keywords = {Self-adaptation, uncertainty, uncertainty models, uncertainty methods, unanticipated change, uncertainty challenges, survey}
}

@article{10.5555/3455716.3455938,
author = {Weinshall, Daphna and Amir, Dan},
title = {Theory of curriculum learning, with convex loss functions},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Curriculum Learning is motivated by human cognition, where teaching often involves gradually exposing the learner to examples in a meaningful order, from easy to hard. Although methods based on this concept have been empirically shown to improve performance of several machine learning algorithms, no theoretical analysis has been provided even for simple cases. To address this shortfall, we start by formulating an ideal definition of difficulty score - the loss of the optimal hypothesis at a given datapoint. We analyze the possible contribution of curriculum learning based on this score in two convex problems - linear regression, and binary classification by hinge loss minimization. We show that in both cases, the convergence rate of SGD optimization decreases monotonically with the difficulty score, in accordance with earlier empirical results. We also prove that when the difficulty score is fixed, the convergence rate of SGD optimization is monotonically increasing with respect to the loss of the current hypothesis at each point. We discuss how these results settle some confusion in the literature where two apparently opposing heuristics are reported to improve performance: curriculum learning in which easier points are given priority, vs hard data mining where the more difficult points are sought out.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {222},
numpages = {19},
keywords = {curriculum learning, linear regression, hinge loss minimization}
}

@article{10.1145/3524301,
author = {Kuo, Hsuan-Chi and Chen, Jianyan and Mohan, Sibin and Xu, Tianyin},
title = {Set the configuration for the heart of the OS: on the practicality of operating system kernel debloating},
year = {2022},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/3524301},
doi = {10.1145/3524301},
abstract = {This paper presents a study on the practicality of operating system (OS) kernel debloating, that is, reducing kernel code that is not needed by the target applications. Despite their significant benefits regarding security (attack surface reduction) and performance (fast boot time and reduced memory footprints), the state-of-the-art OS kernel debloating techniques are not widely adopted in practice, especially in production environments. We identify the limitations of existing kernel debloating techniques that hinder their practical adoption, such as both accidental and essential ones. To understand these limitations, we build an advanced debloating framework named Cozart that enables us to conduct a number of experiments on different types of OS kernels (such as Linux and the L4 microkernel) with a wide variety of applications (such as HTTPD, Memcached, MySQL, NGINX, PHP, and Redis). Our experimental results reveal the challenges and opportunities in making OS kernel debloating practical. We share these insights and our experience to shed light on addressing the limitations of kernel debloating techniques in future research and development efforts.},
journal = {Commun. ACM},
month = apr,
pages = {101–109},
numpages = {9}
}

@inproceedings{10.1145/581339.581416,
author = {Bratthall, Lars G. and van der Geest, Robert and Hofmann, Holger and Jellum, Edgar and Korendo, Zbigniew and Martinez, Robert and Orkisz, Michal and Zeidler, Christian and Andersson, Johan S},
title = {Integrating hundred's of products through one architecture: the industrial IT architecture},
year = {2002},
isbn = {158113472X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/581339.581416},
doi = {10.1145/581339.581416},
abstract = {During the last few years, software product line engineering has gained significant interest as a way for creating software products faster and cheaper. But what architecture is needed to integrate huge amounts of products, from different product lines? This paper describes such an architecture and its support processes and tools. Through cases, it is illustrated how the architecture is used to integrate new --- and old --- products in such diverse integration projects as vessel motion control, airport baggage handling systems, pulp&amp;paper and oil&amp;gas, in a very large organization. However, in a large organization it is a challenge to make everyone follow an architecture. Steps taken to ensure global architectural consistency are presented. It is concluded that a single architecture can be used to unify development in a huge organization, where the distributed development practices otherwise may prohibit integration of various products.},
booktitle = {Proceedings of the 24th International Conference on Software Engineering},
pages = {604–614},
numpages = {11},
location = {Orlando, Florida},
series = {ICSE '02}
}

@proceedings{10.1145/3640310,
title = {MODELS '24: Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Linz, Austria}
}

@inproceedings{10.1145/3639477.3639750,
author = {Wang, Zikuan and Liu, Bohan and Zhan, Zeye and Zhang, He and Li, Gongyuan},
title = {An Ethnographic Study on the CI of A Large Scale Project},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639750},
doi = {10.1145/3639477.3639750},
abstract = {Continuous Integration (CI) is the foundation for achieving rapid iteration and short-cycle delivery. To achieve CI, a series of best practices and solutions have been proposed, which are referred to as patterns. However, there is a natural contradiction between the speed and continuity pursued by CI and the ever-expanding project scale and complexity. Various factors such as project size, outdated system architecture, complex organizational structure, or limited server resources can all lead to deviations from patterns in CI practices, resulting in anti-patterns. We conducted an ethnographic research to investigate the current state, anti-patterns, and challenges in resolving anti-patterns of the CI process within a large communication project at a globally leading IT company. We conducted a deep observation and participation in the project for seven months and conducted multiple rounds of interviews with related developers in the company. The project adopts a CI pipeline that has a three-level hierarchical structure. We evaluated the company's software development practices based on the pattern list. We identified three anti-patterns that contradicted the patterns listed, and we also discovered three new anti-patterns that were not on the list. Further, we analyzed the challenges of solving these anti-patterns. Additionally, we found seven better practices and analyzed why they are better.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {287–297},
numpages = {11},
keywords = {continuous integration, ethnographic study, anti-patterns, best practice},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3634713.3634715,
author = {B\"{o}hm, Sabrina and Krieter, Sebastian and He\ss{}, Tobias and Th\"{u}m, Thomas and Lochau, Malte},
title = {Incremental Identification of T-Wise Feature Interactions},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634715},
doi = {10.1145/3634713.3634715},
abstract = {Developers of configurable software use the concept of selecting and deselecting features to create different variants of a software product. In this context, one of the most challenging aspects is to identify unwanted interactions between those features. Due to the combinatorial explosion of the number of potentially interacting features, it is currently an open question how to systematically identify a particular feature interaction that causes a specific fault in a set of software products. In this paper, we propose an incremental approach to identify such t-wise feature interactions based on testing additional configurations in a black-box setting. We present the algorithm Inciident, which generates and selects new configurations based on a divide-and-conquer strategy to efficiently identify the feature interaction with a preferably minimal number of configurations. We evaluate our approach by considering simulated and real interactions of different sizes for 48 real-world feature models. Our results show that on average, Inciident requires 80&nbsp;% less configurations to identify an interaction than using randomly selected configurations.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {27–36},
numpages = {10},
keywords = {Configurable Systems, Feature Interaction, Feature-Model Analysis, Software Product Lines},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.1145/3148173.3148189,
author = {Bercea, Gheorghe-Teodor and Bertolli, Carlo and Jacob, Arpith C. and Eichenberger, Alexandre and Bataev, Alexey and Rokos, Georgios and Sung, Hyojin and Chen, Tong and O'Brien, Kevin},
title = {Implementing implicit OpenMP data sharing on GPUs},
year = {2017},
isbn = {9781450355650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148173.3148189},
doi = {10.1145/3148173.3148189},
abstract = {OpenMP is a shared memory programming model which supports the offloading of target regions to accelerators such as NVIDIA GPUs. The implementation in Clang/LLVM aims to deliver a generic GPU compilation toolchain that supports both the native CUDA C/C++ and the OpenMP device offloading models. There are situations where the semantics of OpenMP and those of CUDA diverge. One such example is the policy for implicitly handling local variables. In CUDA, local variables are implicitly mapped to thread local memory and thus become private to a CUDA thread. In OpenMP, due to semantics that allow the nesting of regions executed by different numbers of threads, variables need to be implicitly shared among the threads of a contention group.In this paper we introduce a re-design of the OpenMP device data sharing infrastructure that is responsible for the implicit sharing of local variables in the Clang/LLVM toolchain. We introduce a new data sharing infrastructure that lowers implicitly shared variables to the shared memory of the GPU.We measure the amount of shared memory used by our scheme in cases that involve scalar variables and statically allocated arrays. The evaluation is carried out by offloading to K40 and P100 NVIDIA GPUs. For scalar variables the pressure on shared memory is relatively low, under 26% of shared memory utilization for the K40, and does not negatively impact occupancy. The limiting occupancy factor in that case is register pressure. The data sharing scheme offers the users a simple memory model for controlling the implicit allocation of device shared memory.},
booktitle = {Proceedings of the Fourth Workshop on the LLVM Compiler Infrastructure in HPC},
articleno = {5},
numpages = {12},
keywords = {Clang, OpenMP, data sharing, shared memory},
location = {Denver, CO, USA},
series = {LLVM-HPC'17}
}

@inproceedings{10.1145/3613424.3623787,
author = {Liu, Zeshi and Chen, Shuo and Qu, Peiyao and Liu, Huanli and Niu, Minghui and Ying, Liliang and Ren, Jie and Tang, Guangming and You, Haihang},
title = {SUSHI: Ultra-High-Speed and Ultra-Low-Power Neuromorphic Chip Using Superconducting Single-Flux-Quantum Circuits},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623787},
doi = {10.1145/3613424.3623787},
abstract = {The rapid single-flux-quantum (RSFQ) superconducting technology is highly promising due to its ultra-high-speed computation with ultra-low-power consumption, making it an ideal solution for the post-Moore era. In superconducting technology, information is encoded and processed based on pulses that resemble the neuronal pulses present in biological neural systems. This has led to a growing research focus on implementing neuromorphic processing using superconducting technology. However, current research on superconducting neuromorphic processing does not fully leverage the advantages of superconducting circuits due to incomplete neuromorphic design and approach. Although they have demonstrated the benefits of using superconducting technology for neuromorphic hardware, their designs are mostly incomplete, with only a few components validated, or based solely on simulation. This paper presents SUSHI (Superconducting neUromorphic proceSsing cHIp) to fully leverage the potential of superconducting neuromorphic processing. Based on three guiding principles and our architectural and methodological designs, we address existing challenges and enables the design of verifiable and fabricable superconducting neuromorphic chips. We fabricate and verify a chip of SUSHI using superconducting circuit technology. Successfully obtaining the correct inference results of a complete neural network on the chip, this is the first instance of neural networks being completely executed on a superconducting chip to the best of our knowledge. Our evaluation shows that using approximately 105 Josephson junctions, SUSHI achieves a peak neuromorphic processing performance of 1,355 giga-synaptic operations per second (GSOPS) and a power efficiency of 32,366 GSOPS per Watt (GSOPS/W). This power efficiency outperforms the state-of-the-art neuromorphic chips TrueNorth and Tianjic by 81 and 50 times, respectively.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {614–627},
numpages = {14},
keywords = {Neuromorphic, Single-Flux-Quantum, Spiking Neural Networks, Superconducting},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/2854038.2854043,
author = {Sui, Yulei and Di, Peng and Xue, Jingling},
title = {Sparse flow-sensitive pointer analysis for multithreaded programs},
year = {2016},
isbn = {9781450337786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2854038.2854043},
doi = {10.1145/2854038.2854043},
abstract = {For C programs, flow-sensitivity is important to enable pointer analysis to achieve highly usable precision. Despite significant recent advances in scaling flow-sensitive pointer analysis sparsely for sequential C programs, relatively little progress has been made for multithreaded C programs. In this paper, we present FSAM, a new Flow-Sensitive pointer Analysis that achieves its scalability for large Multithreaded C programs by performing sparse analysis on top of a series of thread interference analysis phases. We evaluate FSAM with 10 multithreaded C programs (with more than 100K lines of code for the largest) from Phoenix-2.0, Parsec-3.0 and open-source applications. For two programs, raytrace and x264, the traditional data-flow-based flow-sensitive pointer analysis is un- scalable (under two hours) but our analysis spends just under 5 minutes on raytrace and 9 minutes on x264. For the rest, our analysis is 12x faster and uses 28x less memory.},
booktitle = {Proceedings of the 2016 International Symposium on Code Generation and Optimization},
pages = {160–170},
numpages = {11},
keywords = {Flow-Sensitivity, Pointer Analysis, Sparse Analysis},
location = {Barcelona, Spain},
series = {CGO '16}
}

@inproceedings{10.1145/3571788.3571791,
author = {Di Sandro, Alessio and Shahin, Ramy and Chechik, Marsha},
title = {Adding Product-Line Capabilities to Your Favourite Modeling Language},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571788.3571791},
doi = {10.1145/3571788.3571791},
abstract = {Software product lines are commonly adopted in industry to manage the development of complex families of software systems. Software engineering activities use models at their core, and extending a modeling language to support product lines is an expensive task. Moreover, many useful techniques and analyses such as model querying and model refactoring, are defined at the level of individual products. Before they can take advantage of a product line representation, they need to be lifted, i.e., reengineered to handle variability in a product line. Not only is this process non-trivial, it needs to be redone for each modeling language. In this paper, we propose an Eclipse-based framework, MMINT-PL, for creating and managing annotative product lines of software models in a language-agnostic way. Our framework allows extending any modeling language with product line capabilities, and facilitates lifting of a variety of modeling activities to the product line level. We also demonstrate how to use MMINT-PL to lift the Viatra Query Language.},
booktitle = {Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {3–12},
numpages = {10},
keywords = {Product lines, modeling languages, queries., variability},
location = {Odense, Denmark},
series = {VaMoS '23}
}

@article{10.5555/3322706.3361993,
author = {Glimsdal, Sondre and Granmo, Ole-Christoffer},
title = {Thompson sampling guided stochastic searching on the line for deceptive environments with applications to root-finding problems},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {The multi-armed bandit problem forms the foundation for solving a wide range of online stochastic optimization problems through a simple, yet effective mechanism. One simply casts the problem as a gambler who repeatedly pulls one out of N slot machine arms, eliciting random rewards. Learning of reward probabilities is then combined with reward maximization, by carefully balancing reward exploration against reward exploitation. In this paper, we address a particularly intriguing variant of the multi-armed bandit problem, referred to as the Stochastic Point Location (SPL) problem. The gambler is here only told whether the optimal arm (point) lies to the "left" or to the "right" of the arm pulled, with the feedback being erroneous with probability 1 - π. This formulation thus targets optimization in continuous action spaces with both informative and deceptive feedback. To tackle this class of problems, we formulate a compact and scalable Bayesian representation of the solution space that simultaneously captures both the location of the optimal arm as well as the probability of receiving correct feedback. We further introduce the accompanying Thompson Sampling guided Stochastic Point Location (TS-SPL) scheme for balancing exploration against exploitation. By learning π, TS-SPL also supports deceptive environments that are lying about the direction of the optimal arm. This, in turn, allows us to address the fundamental Stochastic Root Finding (SRF) problem. Empirical results demonstrate that our scheme deals with both deceptive and informative environments, significantly outperforming competing algorithms both for SRF and SPL.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1910–1933},
numpages = {24},
keywords = {deceptive environment, probabilistic bisection search, searching on the line, stochastic point location, thompson sampling}
}

@proceedings{10.1145/3559712,
title = {SBCARS '22: Proceedings of the 16th Brazilian Symposium on Software Components, Architectures, and Reuse},
year = {2022},
isbn = {9781450397452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Uberlandia, Brazil}
}

@inproceedings{10.1145/3205651.3208239,
author = {Escobar, Juan Jos\'{e} and Ortega, Julio and D\'{\i}az, Antonio Francisco and Gonz\'{a}lez, Jes\'{u}s and Damas, Miguel},
title = {Multi-objective feature selection for EEG classification with multi-level parallelism on heterogeneous CPU-GPU clusters},
year = {2018},
isbn = {9781450357647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205651.3208239},
doi = {10.1145/3205651.3208239},
abstract = {The present trend in the development of computer architectures that offer improvements in both performance and energy efficiency has provided clusters with interconnected nodes including multiple multi-core microprocessors and accelerators. In these so-called heterogeneous computers, the applications can take advantage of different parallelism levels according to the characteristics of the architectures in the platform. Thus, the applications should be properly programmed to reach good efficiencies, not only with respect to the achieved speedups but also taking into account the issues related to energy consumption. In this paper we provide a multi-objective evolutionary algorithm for feature selection in electroencephalogram (EEG) classification, which can take advantage of parallelism at multiple levels: among the CPU-GPU nodes interconnected in the cluster (through message-passing), and inside these nodes (through shared-memory thread-level parallelism in the CPU cores, and data-level parallelism and thread-level parallelism in the GPU). The procedure has been experimentally evaluated in performance and energy consumption and shows statistically significant benefits for feature selection: speedups of up to 73 requiring only a 6% of the energy consumed by the sequential code.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1862–1869},
numpages = {8},
keywords = {EEG multi-objective feature selection, distributed master-worker procedure, energy-aware computing, heterogeneous platform, parallel programming, subpopulation-based genetic algorithm},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@proceedings{10.1145/3664647,
title = {MM '24: Proceedings of the 32nd ACM International Conference on Multimedia},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are delighted to welcome you to Melbourne, Australia for ACM Multimedia 2024, the 32nd ACM International Conference on Multimedia. ACM Multimedia is the premier international conference series in the area of multimedia within the field of computer science. Since 1993, ACM Multimedia has been bringing together worldwide researchers and practitioners from academia and industry to present their innovative research and to discuss recent advancements in multimedia.For the first time since the end of the COVID-19 pandemic, this year's conference returns to the Asia-Pacific region and resumes as a full-fledged, inperson event. With no travel restrictions or significant visa challenges, we are excited to once again experience the warmth of face-to-face gatherings, where we can reconnect with colleagues and friends.The enthusiasm and support from the community have been incredible. ACM Multimedia 2024 received over 4,300 main conference submissions, accepting more than 1,100 papers (please refer to the TPC Chairs' message for details). In addition, 10 Grand Challenges were selected from 22 submissions, 18 workshops from 30 submissions, and 8 tutorials from 13 proposals. We've prepared an exciting five-day program: workshops, grand challenges, and tutorials will be held on the 1st and 5th days, with the main conference occupying the middle three days. All accepted papers will be accessible online prior to the conference, and we are working to ensure proceedings are available through the ACM Digital Library around the conference period.This year's conference features three distinguished academic keynote speeches, several prestigious SIGMM award talks, a panel discussion on Generative AI in Multimedia, a refreshed Brave New Idea (BNI) session, and our inaugural industry program.The opening keynote will be delivered by Prof. Pascale Fung from HKUST, a Fellow of AAAI, ACL, and IEEE. Her talk will explore the pressing topic of Agents in the Large Language Model (LLM) Era. Prof. Judy Kay from the University of Sydney, a renowned expert in HCI, user modeling, and ubiquitous computing, will give the second keynote on how to empower individuals to harness and control their multimodal data. The final academic keynote will be presented by Prof. Jiebo Luo from the University of Rochester, a Fellow of ACM, AAAI, IEEE, SPIE, and IAPR, as well as a member of Academia Europaea and the US National Academy of Inventors. He will discuss leveraging LLMs as social multimedia analysis engines.This year, we continue using OpenReview to ensure an open and transparent review process. Thanks to the exceptional efforts of the technical program committee, every paper received at least three reviews before the review announcement. The BNI track has also revamped its review process to align with the main conference, promoting visionary papers. Additionally, we are excited to introduce the industry program to ACM Multimedia for the first time, featuring industry keynote speeches, expert talks, and demonstrations (please refer to the industry chairs' message for further details).We are also committed to making the conference inclusive and accessible. To support students with financial constraints, we have awarded travel grants to at least 25 students from the ACM Multimedia 2024 budget, with an additional 20+ students receiving SIGMM travel grants. Over 20 local students have also been recruited as volunteers, benefiting from complimentary registration. Furthermore, we have arranged childcare facilities to accommodate attendees with young children. A welcome reception will take place on the 2nd day of the conference, followed by a gala dinner on the 3rd day, featuring exciting cultural performances.We hope you find this year's program engaging and thought-provoking and that it offers valuable opportunities to exchange ideas with fellow researchers and practitioners from around the globe. We also encourage you to take time to explore the beautiful city of Melbourne and its surrounding regions.},
location = {Melbourne VIC, Australia}
}

@inproceedings{10.1145/3658271.3658325,
author = {Pereira, Aurisl\^{a}nia and Bezerra, Carla and Coutinho, Emanuel},
title = {Decomposition of Reliability Requirements for Self-Adaptive Systems Using the NFR Framework},
year = {2024},
isbn = {9798400709968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658271.3658325},
doi = {10.1145/3658271.3658325},
abstract = {Context: Self-adaptive systems (SAS) can change their behavior at runtime according to their needs and problems, presenting dynamic and complex operations. Problem: One way to help ensure the quality of SAS is to correctly address non-functional requirements (NFRs), such as security, usability, and reliability. However, dealing with NFRs is naturally a difficult task, as they have some problems related to their relationships with other NFRs or their sub-characteristics. Solution: SASs have characteristics directly linked to NFRs or quality characteristics that must be adequately maintained for the system to work. One way to map the conflicts of RNFs is using SIGs (Softgoal Interdependency Graph), basic units that represent non-functional requirements or generic and flexible goals. Aiming to support the implementation of NFRs in SAS and contribute to improving the quality assurance of these systems, this work aims to propose a reliability SIG for self-adaptive systems. IS Theory: General systems theory, specifically the interfaces between different parts of systems and solutions that communicate, exchanging information. These parts can be systems from different institutions that require integration, consisting of Information Systems. Method: We conducted a systematic mapping of the literature to identify which NFRs are most used in evaluating SASs. Next, the reliability NFRs decomposition step was performed, generating a reliability SIG for SAS as a product, which was evaluated by specialists in SAS. Summary of Results: As the main results, we obtain that: (i) reliability was the most cited characteristic among the 31 selected studies, (ii) availability was the most cited in the works, and (iii) reliability significantly influences the properties and performance of SAS, in addition to impact service quality. The SIG developed throughout this study can be used as a support base for developers or requirements analysts interested in understanding the relationships between the reliability NFR in SASs. Contributions and Impact in the IS area: The results contribute to SI in bringing discussions about quality of SAS is to correctly address non-functional requirements (NFRs). The main contribution is a systematic mapping and SIG.},
booktitle = {Proceedings of the 20th Brazilian Symposium on Information Systems},
articleno = {53},
numpages = {10},
keywords = {Non-functional requirements, Self-adaptive system, Software quality.},
location = {Juiz de Fora, Brazil},
series = {SBSI '24}
}

@article{10.1145/3688836,
author = {Luo, Chuan and Song, Jianping and Zhao, Qiyuan and Sun, Binqi and Chen, Junjie and Zhang, Hongyu and Lin, Jinkun and Hu, Chunming},
title = {Solving the t-Wise Coverage Maximum Problem via Effective and Efficient Local Search-Based Sampling},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3688836},
doi = {10.1145/3688836},
abstract = {To meet the increasing demand for customized software, highly configurable systems become essential in practice. Such systems offer many options to configure, and ensuring the reliability of these systems is critical. A widely used evaluation metric for testing these systems is  (t) -wise coverage, where  (t)  represents testing strength, and its value typically ranges from 2 to 6. It is crucial to design effective and efficient methods for generating test suites that achieve high  (t) -wise coverage. However, current state-of-the-art methods need to generate large test suites for achieving high  (t) -wise coverage. In this work, we propose a novel method called LS-Sampling-Plus that can efficiently generate test suites with high  (t) -wise coverage for  (2leq tleq 6)  while being smaller in size compared to existing state-of-the-art methods. LS-Sampling-Plus incorporates many core algorithmic techniques, including two novel scoring functions, a dynamic mechanism for updating sampling probabilities, and a validity-guaranteed systematic search method. Our experiments on various practical benchmarks show that LS-Sampling-Plus can achieve higher  (t) -wise coverage than current state-of-the-art methods, through building a test suite of the same size. Moreover, our evaluations indicate the effectiveness of all core algorithmic techniques of LS-Sampling-Plus. Furthermore, LS-Sampling-Plus exhibits better scalability and fault detection capability than existing state-of-the-art methods.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {13},
numpages = {64},
keywords = {Highly Configurable Software Systems, Software Testing, Local Search}
}

@inproceedings{10.1109/CGO.2009.33,
author = {Voronenko, Yevgen and de Mesmay, Fr\'{e}d\'{e}ric and P\"{u}schel, Markus},
title = {Computer Generation of General Size Linear Transform Libraries},
year = {2009},
isbn = {9780769535760},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CGO.2009.33},
doi = {10.1109/CGO.2009.33},
abstract = {The development of high-performance libraries has become extraordinarily difficult due to multiple processor cores, vector instruction sets, and deep memory hierarchies. Often, the library has to be reimplemented and reoptimized, when a new platform is released. In this paper we show how to automatically generate general input-size libraries for the domain of linear transforms. The input to our generator is a formal specification of the transform and the recursive algorithms the library should use; the output is a library that supports general input size, is vectorized and multithreaded, provides an adaptation mechanism for the memory hierarchy, and has excellent performance, comparable to or better than the best human-written libraries. Further, we show that our library generator enables various customizations; one example is the generation of Java libraries.},
booktitle = {Proceedings of the 7th Annual IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {102–113},
numpages = {12},
keywords = {DFT, Linear transform, SIMD vector instructions, automatic performance tuning, discrete Fourier transform, domain-specific language, multithreading, program generation},
series = {CGO '09}
}

@inproceedings{10.1145/3503161.3547960,
author = {Shao, Jianjian and Wu, Zhenqian and Luo, Yuanyan and Huang, Shudong and Pu, Xiaorong and Ren, Yazhou},
title = {Self-Paced Label Distribution Learning for In-The-Wild Facial Expression Recognition},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547960},
doi = {10.1145/3503161.3547960},
abstract = {Label distribution learning (LDL) has achieved great progress in facial expression recognition (FER), where the generating label distribution is a key procedure for LDL-based FER. However, many existing researches have shown the common problem with noisy samples in FER, especially on in-the-wild datasets. This issue may lead to generating unreliable label distributions (which can be seen as label noise), and will further negatively affect the FER model. To this end, we propose a play-and-plug method of self-paced label distribution learning (SPLDL) for in-the-wild FER. Specifically, a simple yet efficient label distribution generator is adopted to generate label distributions to guide label distribution learning. We then introduce self-paced learning (SPL) paradigm and develop a novel self-paced label distribution learning strategy, which considers both classification losses and distribution losses. SPLDL first learns easy samples with reliable label distributions and gradually steps to complex ones, effectively suppressing the negative impact introduced by noisy samples and unreliable label distributions. Extensive experiments on in-the-wild FER datasets (emphi.e., RAF-DB and AffectNet) based on three backbone networks demonstrate the effectiveness of the proposed method.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {161–169},
numpages = {9},
keywords = {facial expression recognition, label distribution learning, self-paced learning.},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1109/ICSE-NIER52604.2021.00013,
author = {Kehrer, Timo and Th\"{u}m, Thomas and Schulthei\ss{}, Alexander and Bittner, Paul Maximilian},
title = {Bridging the gap between clone-and-own and software product lines},
year = {2021},
isbn = {9780738133249},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER52604.2021.00013},
doi = {10.1109/ICSE-NIER52604.2021.00013},
abstract = {Software is often released in multiple variants to meet all customer requirements. While software product lines address this need by advocating the development of an integrated software platform, practitioners frequently rely on ad-hoc reuse based on a principle which is known as clone-and-own. This practice avoids high up-front investments, as new variants of a software family are created by simply copying and adapting an existing variant, but maintenance costs explode once a critical number of variants is reached. With our research project VariantSync, we aim to bridge the gap between clone-and-own and product lines by combining the minimal overhead and flexibility of clone-and-own with the systematic handling of variability in software product lines. The key idea is to transparently integrate product-line concepts with variant management facilities known from version control systems in order to automatically synchronize a set of evolving variants. We believe that VariantSync has the potential to change the way how practitioners develop multi-variant software systems for which it is hard to foresee which variants will be added in the future.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {21–25},
numpages = {5},
keywords = {clone-and-own, configuration management, feature traceability, software product lines},
location = {Virtual Event, Spain},
series = {ICSE-NIER '21}
}

@article{10.1145/3532182,
author = {Arrieta, Aitor and Valle, Pablo and Agirre, Joseba A. and Sagardui, Goiuria},
title = {Some Seeds Are Strong: Seeding Strategies for Search-based Test Case Selection},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3532182},
doi = {10.1145/3532182},
abstract = {The time it takes software systems to be tested is usually long. Search-based test selection has been a widely investigated technique to optimize the testing process. In this article, we propose a set of seeding strategies for the test case selection problem that generates the initial population of Pareto-based multi-objective algorithms, with the goals of (1) helping to find an overall better set of solutions and (2) enhancing the convergence of the algorithms. The seeding strategies were integrated with four state-of-the-art multi-objective search algorithms and applied into two contexts where regression-testing is paramount: (1) Simulation-based testing of Cyber-physical Systems and (2) Continuous Integration. For the first context, we evaluated our approach by using six fitness function combinations and six independent case studies, whereas in the second context, we derived a total of six fitness function combinations and employed four case studies. Our evaluation suggests that some of the proposed seeding strategies are indeed helpful for solving the multi-objective test case selection problem. Specifically, the proposed seeding strategies provided a higher convergence of the algorithms towards optimal solutions in 96% of the studied scenarios and an overall cost-effectiveness with a standard search budget in 85% of the studied scenarios.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {17},
numpages = {47},
keywords = {Test case selection, search-based software testing, regression testing}
}

@proceedings{10.1145/3639478,
title = {ICSE-Companion '24: Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ICSE is the leading and, by far, the largest conference in Software Engineering, attracting researchers, practitioners, and students worldwide. ICSE2024 is co-located with 11 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Lisbon, Portugal}
}

@article{10.1109/TNET.2024.3407783,
author = {Yang, Huanqi and Li, Zhenjiang and Luo, Chengwen and Wei, Bo and Xu, Weitao},
title = {InaudibleKey2.0: Deep Learning-Empowered Mobile Device Pairing Protocol Based on Inaudible Acoustic Signals},
year = {2024},
issue_date = {Oct. 2024},
publisher = {IEEE Press},
volume = {32},
number = {5},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2024.3407783},
doi = {10.1109/TNET.2024.3407783},
abstract = {The increasing proliferation of Internet-of-Things (IoT) devices in daily life has rendered secure Device-to-Device (D2D) communication increasingly crucial. Achieving secure D2D communication necessitates key agreement between various IoT devices without prior knowledge. Despite existing literature proposing numerous approaches, they exhibit limitations such as low key generation rates and short pairing distances. In this paper, we present InaudibleKey2.0, an inaudible acoustic signal based key generation protocol for mobile devices. Based on acoustic channel reciprocity, InaudibleKey2.0 exploits the acoustic channel frequency response of two legitimate devices as a shared secret for key generation. To significantly enhance performance, InaudibleKey2.0 incorporates novel technologies, including a deep learning-enabled channel prediction model for improved channel reciprocity, a quantization model for increased key generation rates, and a transformer-based reconciliation method for augmented key agreement rates. We conduct comprehensive experiments to evaluate InaudibleKey2.0 in diverse real-world environments. In comparison to state-of-the-art solutions, InaudibleKey2.0 achieves 1.3–9.1 times improvement in key generation rates, 3.2–44 times extension in pairing distances, and 1.2–16 times reduction in information reconciliation counts. Security analysis substantiates that InaudibleKey2.0 is resilient to numerous malicious attacks. Furthermore, we implement InaudibleKey2.0 on modern smartphones and resource-limited IoT devices. The results indicate that it is energy-efficient and can operate on both powerful and resource-limited IoT devices without causing excessive resource consumption.},
journal = {IEEE/ACM Trans. Netw.},
month = jun,
pages = {4160–4174},
numpages = {15}
}

@article{10.1145/3472796,
author = {H\"{u}ckelheim, Jan and Hasco\"{e}t, Laurent},
title = {Source-to-Source Automatic Differentiation of OpenMP Parallel Loops},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {1},
issn = {0098-3500},
url = {https://doi.org/10.1145/3472796},
doi = {10.1145/3472796},
abstract = {This article presents our work toward correct and efficient automatic differentiation of OpenMP parallel worksharing loops in forward and reverse mode. Automatic differentiation is a method to obtain gradients of numerical programs, which are crucial in optimization, uncertainty quantification, and machine learning. The computational cost to compute gradients is a common bottleneck in practice. For applications that are parallelized for multicore CPUs or GPUs using OpenMP, one also wishes to compute the gradients in parallel. We propose a framework to reason about the correctness of the generated derivative code, from which we justify our OpenMP extension to the differentiation model. We implement this model in the automatic differentiation tool Tapenade and present test cases that are differentiated following our extended differentiation procedure. Performance of the generated derivative programs in forward and reverse mode is better than sequential, although our reverse mode often scales worse than the input programs.},
journal = {ACM Trans. Math. Softw.},
month = feb,
articleno = {7},
numpages = {32},
keywords = {Automatic differentiation, OpenMP, shared-memory parallel, multicore}
}

@inproceedings{10.1145/3377024.3377026,
author = {Kenner, Andy and Dassow, Stephan and Lausberger, Christian and Kr\"{u}ger, Jacob and Leich, Thomas},
title = {Using variability modeling to support security evaluations: virtualizing the right attack scenarios},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377026},
doi = {10.1145/3377024.3377026},
abstract = {A software system's security is constantly threatened by vulnerabilities that result from faults in the system's design (e.g., unintended feature interactions) and which can be exploited with attacks. While various databases summarize information on vulnerabilities and other security issues for many software systems, these databases face severe limitations. For example, the information's quality is unclear, often only semi-structured, and barely connected to other information. Consequently, it can be challenging for any security-related stakeholder to extract and understand what information is relevant, considering that most systems exist in different variants and versions. To tackle this problem, we propose to design vulnerability feature models that represent the vulnerabilities of a system and enable developers to virtualize corresponding attack scenarios. In this paper, we report a first case study on Mozilla Firefox for which we extracted vulnerabilities and used them to virtualize vulnerable instances in Docker. To this end, we focused on extracting information from available databases and on evaluating the usability of the results. Our findings indicate several problems with the extraction that complicate modeling, understanding, and testing of vulnerabilities. Nonetheless, the databases provide a valuable foundation for our technique, which we aim to extend with automatic synthesis and analyses of feature models, as well as virtualization for attack scenarios in future work.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {10},
numpages = {9},
keywords = {attack scenarios, docker-container, exploit, feature model, software architecture, variability model, vulnerability},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@article{10.1145/3582571,
author = {Xu, Qinghua and Ali, Shaukat and Yue, Tao},
title = {Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3582571},
doi = {10.1145/3582571},
abstract = {Anomaly detection is critical to ensure the security of cyber-physical systems (CPS). However, due to the increasing complexity of attacks and CPS themselves, anomaly detection in CPS is becoming more and more challenging. In our previous work, we proposed a digital twin-based anomaly detection method, called ATTAIN, which takes advantage of both historical and real-time data of CPS. However, such data vary significantly in terms of difficulty. Therefore, similar to human learning processes, deep learning models (e.g., ATTAIN) can benefit from an easy-to-difficult curriculum. To this end, in this paper, we present a novel approach, named digitaL twin-based Anomaly deTecTion wIth Curriculum lEarning (LATTICE), which extends ATTAIN by introducing curriculum learning to optimize its learning paradigm. LATTICE attributes each sample with a difficulty score, before being fed into a training scheduler. The training scheduler samples batches of training data based on these difficulty scores such that learning from easy to difficult data can be performed. To evaluate LATTICE, we use five publicly available datasets collected from five real-world CPS testbeds. We compare LATTICE with ATTAIN and two other state-of-the-art anomaly detectors. Evaluation results show that LATTICE outperforms the three baselines and ATTAIN by 0.906%-2.367% in terms of the F1 score. LATTICE also, on average, reduces the training time of ATTAIN by 4.2% on the five datasets and is on par with the baselines in terms of detection delay time.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {113},
numpages = {32},
keywords = {Cyber-physical system, digital twin, curriculum learning, deep learning, anomaly detection}
}

@inproceedings{10.1145/3664475.3664734,
author = {Zicarelli, David},
title = {Advances in Real Time Audio Rendering - Part 2},
year = {2024},
isbn = {9798400706837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664475.3664734},
doi = {10.1145/3664475.3664734},
abstract = {We originally referred to "real-time" audio systems to draw a distinction with "non-real-time" systems where a series of audio samples is entirely determined and computed in advance, originally because computers were not fast enough to perform the needed mathematical calculations. If you can't listen to the sound as it is produced, you won't be able to change it live and know what you're changing. Thus, the desire to turn a computer into something more like a "musical instrument" was a primary motivation in the development of real-time audio systems.},
booktitle = {ACM SIGGRAPH 2024 Courses},
articleno = {7},
numpages = {16},
location = {Denver, CO, USA},
series = {SIGGRAPH Courses '24}
}

@inproceedings{10.1145/3412382.3458260,
author = {Xu, Weitao and Li, Zhenjiang and Xue, Wanli and Yu, Xiaotong and Wei, Bo and Wang, Jia and Luo, Chengwen and Li, Wei and Zomaya, Albert Y.},
title = {InaudibleKey: Generic Inaudible Acoustic Signal based Key Agreement Protocol for Mobile Devices},
year = {2021},
isbn = {9781450380980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412382.3458260},
doi = {10.1145/3412382.3458260},
abstract = {Secure Device-to-Device (D2D) communication is becoming increasingly important with the ever-growing number of Internet-of-Things (IoT) devices in our daily life. To achieve secure D2D communication, the key agreement between different IoT devices without any prior knowledge is becoming desirable. Although various approaches have been proposed in the literature, they suffer from a number of limitations, such as low key generation rate and short pairing distance. In this paper, we present InaudibleKey, an inaudible acoustic signal based key generation protocol for mobile devices. Based on acoustic channel reciprocity, InaudibleKey exploits the acoustic channel frequency response of two legitimate devices as a common secret to generating keys. InaudibleKey employs several novel technologies to significantly improve its performance. We conduct extensive experiments to evaluate the proposed system in different real environments. Compared to state-of-the-art works, InaudibleKey improves key generation rate by 3 times, extends pairing distance by 3.2 times, and reduces information reconciliation counts by 2.5 times. Security analysis demonstrates that InaudibleKey is resilient to a number of malicious attacks. We also implement InaudibleKey on modern smartphones and resource-limited IoT devices. Results show that it is energy-efficient and can run on both powerful and resource-limited IoT devices without incurring excessive resource consumption.},
booktitle = {Proceedings of the 20th International Conference on Information Processing in Sensor Networks (Co-Located with CPS-IoT Week 2021)},
pages = {106–118},
numpages = {13},
keywords = {Acoustic signal, Device pairing, Key generation, Mobile devices},
location = {Nashville, TN, USA},
series = {IPSN '21}
}

@inproceedings{10.1145/3517077.3517098,
author = {Yin, Yiyang and Wang, Ruofan and Wang, Haodong and Shi, Lianshuan and Wang, Wei},
title = {Electroencephalogram Recognition of Alzheimer's Brain with SecVibratPSO-SVM Frame},
year = {2022},
isbn = {9781450387408},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517077.3517098},
doi = {10.1145/3517077.3517098},
abstract = {In order to investigate the abnormalities of brain connectivity in Alzheimer's disease (AD), and to improve the EEG recognition rate of AD, brain network of linear coherence was constructed to use background electroencephalogram (EEG) signals from patients with AD patients and normal elderly control group. In this study, we extracted 8 brain network features from each frequency band(Delta, Theta, Alpha and Beta). Statistical analysis was used to investigate whether there were significant differences between the characteristics of the AD group and the control group, and further support vector machine (SVM) classification analysis was conducted to identify the differences between the two groups. As an intelligent optimization algorithm, particle swarm optimization (PSO) and its improved algorithm (SecvibratPSO) provide a new possibility for effective feature screening of brain networks. PSO and SecvibratPSO were applied to screen feature combinations of brain networks in single band and in the cross band combinations. The simulation results showed that the two algorithms could determine the optimal feature combination of brain network and improved the EEG recognition rate of AD, but the accuracy of the SecvibratPSO algorithm was higher, reaching 0.8891. These results indicated that the SecvibratPSO algorithm is an effective method to identify the abnormal topological structure of AD brain network.},
booktitle = {Proceedings of the 2022 7th International Conference on Multimedia and Image Processing},
pages = {131–136},
numpages = {6},
keywords = {Alzheimer's disease(AD), Coherent network, Electroencephalogram(EEG), Particle swarm optimization(PSO), SecVibratPSO, Support vector machine (SVM)},
location = {Tianjin, China},
series = {ICMIP '22}
}

@article{10.1145/3633783,
author = {Zhao, Han and Yang, Xu and Deng, Cheng},
title = {Parameter-Agnostic Deep Graph Clustering},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3633783},
doi = {10.1145/3633783},
abstract = {Deep graph clustering, efficiently dividing nodes into multiple disjoint clusters in an unsupervised manner, has become a crucial tool for analyzing ubiquitous graph data. Existing methods have acquired impressive clustering effects by optimizing the clustering network under the parametric condition—predefining the true number of clusters (Ktr). However, Ktr is inaccessible in pure unsupervised scenarios, in which existing methods are incapable of inferring the number of clusters (K), causing limited feasibility. This article proposes the first Parameter-Agnostic Deep Graph Clustering method (PADGC), which consists of two core modules: K-guidence clustering and topological-hierarchical inference, to infer K efficiently and gain impressive clustering predictions. Specifically, K-guidence clustering is employed to optimize the cluster assignments and discriminative embeddings in a mutual promotion manner under the latest updated K, even though K may deviate from Ktr. In turn, such optimized cluster assignments are utilized to explore more accurate K in the topological-hierarchical inference, which can split the dispersive clusters and merge the coupled ones. In this way, these two modules are complementarily optimized until generating the final convergent K and discriminative cluster assignments. Extensive experiments on several benchmarks, including graphs and images, can demonstrate the superiority of our method. The mean values of our inferred K, in 11 out of 12 datasets, deviates from Ktr by less than 1. Our method can also achieve competitive clustering effects with existing parametric deep graph clustering.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {66},
numpages = {20},
keywords = {Parameter-agnostic graph clustering, topological-hierarchical inference, K-guidence clustering, deep graph clustering.}
}

@article{10.1145/3715002,
author = {Blasco, Daniel and Iglesias, Antonio and Echeverr\'{\i}a, Jorge and P\'{e}rez, Francisca and Cetina, Carlos},
title = {Introducing Phylogenetics in Search-based Software Engineering: Phylogenetics-aware SBSE},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715002},
doi = {10.1145/3715002},
abstract = {Phylogenetics studies the relationships, in terms of biological history and kinship, of a set of taxa (e.g., species). We argue that in Search-based Software Engineering (SBSE), the individuals of an evolutionary computation-driven population could be considered as taxa for which the leverage of Phylogenetic Inference might be beneficial. In this work, we present our Phylogenetics-aware SBSE approach. Our approach introduces a novel Phylogenetic Operation to promote results which are sufficiently aligned (in terms of lineage) with a certain reference given by the domain expert. Our approach is evaluated in two heterogeneous industrial case studies: Procedural Content Generation from Game Software Engineering, and Feature Location from Software Maintenance. The results are analyzed using quality-of-the-solution and acceptance-by-developers measurements. We performed a statistical analysis to determine whether the impact on the results is significant compared to baselines that do not leverage Phylogenetics. The results show that our approach significantly outperforms two baselines in both case studies. Furthermore, two focus groups confirmed the acceptance of our approach and stressed that solution acceptance may make the difference in industrial environments. Our work has the potential to motivate a new breed of research work on Phylogenetics awareness to produce better results in Software Engineering.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Phylogenetics, Search-based Software Engineering, Evolutionary Computation, Game Software Engineering, Procedural Content Generation, Software Maintenance, Feature Location, Model-Driven Engineering}
}

@article{10.1145/3631405,
author = {Demirel, Berken Utku and Dang, Ting and Al-Naimi, Khaldoon and Kawsar, Fahim and Montanari, Alessandro},
title = {Unobtrusive Air Leakage Estimation for Earables with In-ear Microphones},
year = {2024},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
url = {https://doi.org/10.1145/3631405},
doi = {10.1145/3631405},
abstract = {Earables (in-ear wearables) are gaining increasing attention for sensing applications and healthcare research thanks to their ergonomy and non-invasive nature. However, air leakages between the device and the user's ear, resulting from daily activities or wearing variabilities, can decrease the performance of applications, interfere with calibrations, and reduce the robustness of the overall system. Existing literature lacks established methods for estimating the degree of air leaks (i.e., seal integrity) to provide information for the earable applications. In this work, we proposed a novel unobtrusive method for estimating the air leakage level of earbuds based on an in-ear microphone. The proposed method aims to estimate the magnitude of distortions, reflections, and external noise in the ear canal while excluding the speaker output by learning the speaker-to-microphone transfer function which allows us to perform the task unobtrusively. Using the obtained residual signal in the ear canal, we extract three features and deploy a machine-learning model for estimating the air leakage level. We investigated our system under various conditions to validate its robustness and resilience against the motion and other artefacts. Our extensive experimental evaluation shows that the proposed method can track air leakage levels under different daily activities."The best computer is a quiet, invisible servant."~Mark Weiser},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jan,
articleno = {156},
numpages = {29},
keywords = {ear canal sealing, transfer function approximation, unobtrusive}
}

@inproceedings{10.1145/3503161.3548283,
author = {Wu, Siying and Fu, Xueyang and Wu, Feng and Zha, Zheng-Jun},
title = {Cross-modal Semantic Alignment Pre-training for Vision-and-Language Navigation},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548283},
doi = {10.1145/3503161.3548283},
abstract = {Vision-and-Language Navigation needs an agent to navigate to a target location by progressively grounding and following the relevant instruction conditioning on its memory and current observation. Existing works utilize the cross-modal transformer to pass the message between visual modality and textual modality. However, they are still limited to mining the fine-grained matching between the underlying components of trajectories and instructions. Inspired by the significant progress achieved by large-scale pre-training methods, in this paper, we propose CSAP, a new method of Cross-modal Semantic Alignment Pre-training for Vision-and-Language Navigation. It is designed to learn the alignment from trajectory-instruction pairs through two novel tasks, including trajectory-conditioned masked fragment modeling and contrastive semantic-alignment modeling. Specifically, the trajectory-conditioned masked fragment modeling encourages the agent to extract useful visual information to reconstruct the masked fragment. The contrastive semantic-alignment modeling is designed to align the visual representation with corresponding phrase embeddings. By showing experimental results on the benchmark dataset, we demonstrate that transformer architecture-based navigation agent pre-trained with our proposed CSAP outperforms existing methods on both SR and SPL scores.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {4233–4241},
numpages = {9},
keywords = {semantic alignment, vision-and-language navigation, visual-and-language pre-training},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/2866614.2866620,
author = {Mauro, Jacopo and Nieke, Michael and Seidl, Christoph and Yu, Ingrid Chieh},
title = {Context Aware Reconfiguration in Software Product Lines},
year = {2016},
isbn = {9781450340199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2866614.2866620},
doi = {10.1145/2866614.2866620},
abstract = {Software Product Lines (SPLs) are a mechanism for large-scale reuse where families of related software systems are represented in terms of commonalities and variabilities, e.g., using Feature Models (FMs). While FMs define all possible configurations of the SPL, when considering dynamic SPLs not every possible configuration may be valid in all possible contexts. Unfortunately, common FMs can not capture this context dependence. In this paper, we remedy this problem by extending attributed FMs with Validity Formulas (VFs) that constrain the selection of a particular feature to a specific context and that are located directly within the FM. We provide a reconfiguration engine that checks if the active configuration is valid in the current context and, if not, computes how to reconfigure it. Furthermore, we present our implementation and demonstrate its feasibility within a case study derived from scenarios of our industry partner in the automotive domain.},
booktitle = {Proceedings of the 10th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {41–48},
numpages = {8},
keywords = {Context, Feature Models, Software Product Lines, Variability},
location = {Salvador, Brazil},
series = {VaMoS '16}
}

@inproceedings{10.1145/3238147.3240466,
author = {Cashman, Mikaela and Cohen, Myra B. and Ranjan, Priya and Cottingham, Robert W.},
title = {Navigating the maze: the impact of configurability in bioinformatics software},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240466},
doi = {10.1145/3238147.3240466},
abstract = {The bioinformatics software domain contains thousands of applications for automating tasks such as the pairwise alignment of DNA sequences, building and reasoning about metabolic models or simulating growth of an organism. Its end users range from sophisticated developers to those with little computational experience. In response to their needs, developers provide many options to customize the way their algorithms are tuned. Yet there is little or no automated help for the user in determining the consequences or impact of the options they choose. In this paper we describe our experience working with configurable bioinformatics tools. We find limited documentation and help for combining and selecting options along with variation in both functionality and performance. We also find previously undetected faults. We summarize our findings with a set of lessons learned, and present a roadmap for creating automated techniques to interact with bioinformatics software. We believe these will generalize to other types of scientific software.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {757–767},
numpages = {11},
keywords = {bioinformatics, configurability, software testing},
location = {Montpellier, France},
series = {ASE '18}
}

@inproceedings{10.1145/3708557.3716329,
author = {Duarte, Carlos and Costa, Miguel and Seixas Pereira, Let\'{\i}cia and Guerreiro, Jo\~{a}o},
title = {Expanding Automated Accessibility Evaluations: Leveraging Large Language Models for Heading-Related Barriers},
year = {2025},
isbn = {9798400714092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708557.3716329},
doi = {10.1145/3708557.3716329},
abstract = {Ensuring digital resources are accessible to all users, including those with disabilities, is critical in today’s digital landscape. The growing volume of online content has intensified the need for automated accessibility evaluations to ensure compliance with accessibility guidelines. Yet, existing automated tools are limited in scope, being unable to identify many types of accessibility barriers. Recent advances in AI, particularly large language models (LLMs), offer opportunities to expand the range of automated accessibility checks. This work explores the ability of LLMs to detect accessibility barriers related to web page headings. We developed targeted prompts to help LLMs identify them and evaluated the effectiveness of three models – Llama 3.1, GPT-4o, and GPT-4o mini – in multiple versions of a reference webpage, each featuring different heading-related barriers. Findings reveal that model performance depends on barrier type: Llama 3.1 stands out at detecting structural issues like heading appropriateness and hierarchy, GPT-4o is better at identifying accessible names and semantic substitutions, while GPT-4o mini is the most versatile, handling complex structural modifications and labelling. This study highlights LLM’s potential in advancing web accessibility evaluation and bridging gaps in automated assessments.},
booktitle = {Companion Proceedings of the 30th International Conference on Intelligent User Interfaces},
pages = {39–42},
numpages = {4},
keywords = {Web Accessibility, LLM, Accessibility Evaluation, Headings, Llama, GPT},
location = {
},
series = {IUI '25 Companion}
}

@article{10.1145/3678546,
author = {Zhou, Juntao and Li, Yijie and Wang, Yida and Ding, Dian and Lu, Yu and Chen, Yi-Chao and Xue, Guangtao},
title = {Visar: Projecting Virtual Sound Spots for Acoustic Augmented Reality Using Air Nonlinearity},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
url = {https://doi.org/10.1145/3678546},
doi = {10.1145/3678546},
abstract = {Augmented reality that integrates virtual content in real-world surroundings has attracted lots of concentration as the growing trend of the metaverse. Acoustic augmented reality (AAR) applications have proliferated due to readily available earphones and speakers. AAR can provide omnidirectional engagement through the all-around sense of spatial information. Most existing AAR approaches offer immersive experiences by playing binaural spatial audios according to head-related transfer functions (HRTF). These involve complex modeling and require the user to wear a headphone. Air nonlinearity that can reproduce audible sounds from ultrasound offers opportunities to achieve device-free and omnidirectional sound source projection in AAR. This paper proposes Visar, a device-free virtual sound spots projection system leveraging air nonlinearity. Visar achieves simultaneous tracking and sound spot generation while suppressing unintended audio leakages caused by grating lobes and nonlinear effects in mixing lobes through optimization. Considering multi-user scenarios, Visar also proposed a multi-spot scheduling scheme to mitigate the mutual interference between the spots. Extensive experiments show the tracking error is 7.83cm and the orientation estimation error is 10.06°, respectively, envisioning the considerable potential of Visar in AAR applications.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {147},
numpages = {30},
keywords = {Acoustic augmented reality, Air nonlinearity, Sound source projection}
}

@inproceedings{10.1145/2544137.2544155,
author = {Spampinato, Daniele G. and P\"{u}schel, Markus},
title = {A Basic Linear Algebra Compiler},
year = {2018},
isbn = {9781450326704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2544137.2544155},
doi = {10.1145/2544137.2544155},
abstract = {Many applications in media processing, control, graphics, and other domains require efficient small-scale linear algebra computations. However, most existing high performance libraries for linear algebra, such as ATLAS or Intel MKL are more geared towards large-scale problems (matrix sizes in the hundreds and larger) and towards specific interfaces (e.g., BLAS). In this paper we present LGen: a compiler for small-scale, basic linear algebra computations. The input to LGen is a fixed-size linear algebra expression; the output is a corresponding C function optionally including intrinsics to efficiently use SIMD vector extensions. LGen generates code using two levels of mathematical domain-specific languages (DSLs). The DSLs are used to perform tiling, loop fusion, and vectorization at a high level of abstraction, before the final code is generated. In addition, search is used to select among alternative generated implementations. We show benchmarks of code generated by LGen against Intel MKL and IPP as well as against alternative generators, such as the C++ template-based Eigen and the BTO compiler. The achieved speed-up is typically about a factor of two to three.},
booktitle = {Proceedings of Annual IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {23–32},
numpages = {10},
keywords = {Basic linear algebra, DSL, Program synthesis, SIMD vectorization, Small matrices, Tiling},
location = {Orlando, FL, USA},
series = {CGO '14}
}

@proceedings{10.1145/3564719,
title = {GPCE 2022: Proceedings of the 21st ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2022},
isbn = {9781450399203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 21st ACM SIGPLAN International Conference on Generative Programming: Concept &amp; Experiences (GPCE 2022) held on December 6th and 7th, 2022 in Auckland, New Zealand. GPCE is the premiere venue for researchers and practitioners interested in techniques that use program generation, domain-specific languages, and component deployment to increase programmer productivity, improve software quality, and shorten the time-to-market of software products. In addition to exploring cutting-edge techniques of generative software, GPCE seeks to foster cross-fertilization between software engineering and programming language.},
location = {Auckland, New Zealand}
}

@inproceedings{10.1145/3698038.3698512,
author = {Zhang, Xinmin and He, Qiang and Fan, Hao and Wu, Song},
title = {Faascale: Scaling MicroVM Vertically for Serverless Computing with Memory Elasticity},
year = {2024},
isbn = {9798400712869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698038.3698512},
doi = {10.1145/3698038.3698512},
abstract = {This paper quantitatively analyses the potential of vertical scaling MicroVMs in serverless computing. Our analysis shows that under real-world serverless workloads, vertical scaling can significantly improve execution performance and resource utilization. However, we also find that the memory scaling of MicroVMs is the bottleneck that hinders vertical scaling from reaching the performance ceiling. We propose Faascale, a novel mechanism that efficiently scales the memory of MicroVMs for serverless applications. Faascale employs a series of techniques to tackle this bottleneck: 1) it sizes up/down the memory for a MicroVM by blocks that bind with a function instance instead of general pages; and 2) it pre-populates physical memory for function instances to reduce the delays introduced by the lazy-population. Compared with existing memory scaling mechanisms, Faascale improves the memory scaling efficiency by 2 to 3 orders of magnitude. We implement Faascale on Amazon Firecracker to evaluate its gains for the serverless platform. The results of experiments conducted on eight serverless benchmark functions demonstrate that compared with horizontal scaling strategies based the state-of-the-art snapshots technique, Faascale reduces time for cold-starting MicroVMs by 89.01% and functions execution time by 23.93% on average.},
booktitle = {Proceedings of the 2024 ACM Symposium on Cloud Computing},
pages = {196–212},
numpages = {17},
keywords = {Cold Start, MicroVMs, Serverless Computing, Vertical Scaling},
location = {Redmond, WA, USA},
series = {SoCC '24}
}

@proceedings{10.1145/3689031,
title = {EuroSys '25: Proceedings of the Twentieth European Conference on Computer Systems},
year = {2025},
isbn = {9798400711961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are delighted to welcome you to EuroSys 2025, the 20th edition of the European Conference on Computer Systems! We are excited to host EuroSys 2025 in the modern and dynamic city of Rotterdam, Netherlands. This year's EuroSys is very special as it is co-located (for the first time) with the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 2025). We hope you will enjoy an excellent technical program, engaging discussions, and networking opportunities in this vibrant city known for its innovative architecture, bustling port, and rich cultural scene.},
location = {Rotterdam, Netherlands}
}

@article{10.14778/2078324.2078325,
author = {Pavlo, Andrew and Jones, Evan P. C. and Zdonik, Stanley},
title = {On predictive modeling for optimizing transaction execution in parallel OLTP systems},
year = {2011},
issue_date = {October 2011},
publisher = {VLDB Endowment},
volume = {5},
number = {2},
issn = {2150-8097},
url = {https://doi.org/10.14778/2078324.2078325},
doi = {10.14778/2078324.2078325},
abstract = {A new emerging class of parallel database management systems (DBMS) is designed to take advantage of the partitionable workloads of on-line transaction processing (OLTP) applications [23, 20]. Transactions in these systems are optimized to execute to completion on a single node in a shared-nothing cluster without needing to coordinate with other nodes or use expensive concurrency control measures [18]. But some OLTP applications cannot be partitioned such that all of their transactions execute within a single-partition in this manner. These distributed transactions access data not stored within their local partitions and subsequently require more heavy-weight concurrency control protocols. Further difficulties arise when the transaction's execution properties, such as the number of partitions it may need to access or whether it will abort, are not known beforehand. The DBMS could mitigate these performance issues if it is provided with additional information about transactions. Thus, in this paper we present a Markov model-based approach for automatically selecting which optimizations a DBMS could use, namely (1) more efficient concurrency control schemes, (2) intelligent scheduling, (3) reduced undo logging, and (4) speculative execution. To evaluate our techniques, we implemented our models and integrated them into a parallel, main-memory OLTP DBMS to show that we can improve the performance of applications with diverse workloads.},
journal = {Proc. VLDB Endow.},
month = oct,
pages = {85–96},
numpages = {12}
}

@inproceedings{10.1145/1529282.1529336,
author = {Op 't Land, Martin and Zwitzer, Hans and Ensink, Paul and Lebel, Quentin},
title = {Towards a fast enterprise ontology based method for post merger integration},
year = {2009},
isbn = {9781605581668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1529282.1529336},
doi = {10.1145/1529282.1529336},
abstract = {Our research program aims at finding and testing method components for deciding on and implementing organizational splits and mergers. We tested a method to timely detect design &amp; migration issues of a Post Merger Integration. Using actors from Enterprise Ontology as organization building blocks for two large operationally merging airliners, experts systematically listed per actor (a) its organizational implementations, (b) its Quality of Business and (c) its IT implementations. The drafted DEMO Construction Model appeared to be the first neutral and shared language for describing the essence of the business. Also the results needed for decision making (a) were experienced as a necessary and sufficient validation of operational integrity and (b) were delivered fast, yielding a high Return On Modeling Effort.},
booktitle = {Proceedings of the 2009 ACM Symposium on Applied Computing},
pages = {245–252},
numpages = {8},
keywords = {DEMO, PMI, enterprise networks, enterprise ontology, merging IT systems, merging organizations, migration issues, operational integrity, post merger integration, return on modeling effort},
location = {Honolulu, Hawaii},
series = {SAC '09}
}

@inproceedings{10.1145/2581122.2544155,
author = {Spampinato, Daniele G. and P\"{u}schel, Markus},
title = {A Basic Linear Algebra Compiler},
year = {2014},
isbn = {9781450326704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2581122.2544155},
doi = {10.1145/2581122.2544155},
abstract = {Many applications in media processing, control, graphics, and other domains require efficient small-scale linear algebra computations. However, most existing high performance libraries for linear algebra, such as ATLAS or Intel MKL are more geared towards large-scale problems (matrix sizes in the hundreds and larger) and towards specific interfaces (e.g., BLAS). In this paper we present LGen: a compiler for small-scale, basic linear algebra computations. The input to LGen is a fixed-size linear algebra expression; the output is a corresponding C function optionally including intrinsics to efficiently use SIMD vector extensions. LGen generates code using two levels of mathematical domain-specific languages (DSLs). The DSLs are used to perform tiling, loop fusion, and vectorization at a high level of abstraction, before the final code is generated. In addition, search is used to select among alternative generated implementations. We show benchmarks of code generated by LGen against Intel MKL and IPP as well as against alternative generators, such as the C++ template-based Eigen and the BTO compiler. The achieved speed-up is typically about a factor of two to three.},
booktitle = {Proceedings of Annual IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {23–32},
numpages = {10},
keywords = {Basic linear algebra, DSL, Program synthesis, SIMD vectorization, Small matrices, Tiling},
location = {Orlando, FL, USA},
series = {CGO '14}
}

@proceedings{10.1145/3622748,
title = {SBCARS '23: Proceedings of the 17th Brazilian Symposium on Software Components, Architectures, and Reuse},
year = {2023},
isbn = {9798400709524},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Campo Grande, Brazil}
}

@inproceedings{10.1145/3365438.3410963,
author = {Alwidian, Sanaa and Amyot, Daniel},
title = {"Union is power": analyzing families of goal models using union models},
year = {2020},
isbn = {9781450370196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365438.3410963},
doi = {10.1145/3365438.3410963},
abstract = {A goal model family is a set of related goal models that conform to the same metamodel, with commonalities and variabilities between models. Such families stem from the evolution of initial models into several versions over time and/or the variation of models over the space dimension (e.g., products). In contexts where there are several versions/variations of a goal model, analyzing a set of related models with typical similarities, one model at a time, often involves redundant computations and may require repeated user assistance (e.g., for interactive analysis) and laborious activities. This paper proposes the use of union models as first-class artifacts to analyze families of goal models, in order to improve performance of language-specific analysis procedures. The paper empirically evaluates the performance gain resulting from adapting (or lifting) an existing analysis technique specific to the Goal-oriented Requirement Language (GRL) to a family of GRL models, all at once using a union model, compared to analyzing individual models. Our experiments show, based on the use of the IBM CPLEX optimizer, the usefulness and performance gains of using union models to perform a computationally expensive analysis, namely quantitative backward propagation, on families of GRL models.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
pages = {252–262},
numpages = {11},
keywords = {analysis, backward propagation, goal modeling, model family},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@inproceedings{10.1145/3377555.3377889,
author = {Castro-Perez, David and Yoshida, Nobuko},
title = {Compiling first-order functions to session-typed parallel code},
year = {2020},
isbn = {9781450371209},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377555.3377889},
doi = {10.1145/3377555.3377889},
abstract = {Building correct and efficient message-passing parallel programs still poses many challenges. The incorrect use of message-passing constructs can introduce deadlocks, and a bad task decomposition will not achieve good speedups. Current approaches focus either on correctness or efficiency, but limited work has been done on ensuring both. In this paper, we propose a new parallel programming framework, PAlg, which is a first-order language with participant annotations that ensures deadlock-freedom by construction. PAlg programs are coupled with an abstraction of their communication structure, a global type from the theory of multiparty session types (MPST). This global type serves as an output for the programmer to assess the efficiency of their achieved parallelisation. PAlg is implemented as an EDSL in Haskell, from which we: 1. compile to low-level message-passing C code; 2. compile to sequential C code, or interpret as sequential Haskell functions; and, 3. infer the communication protocol followed by the compiled message-passing program. We use the properties of global types to perform message reordering optimisations to the compiled C code. We prove the extensional equivalence of the compiled code, as well as protocol compliance. We achieve linear speedups on a shared-memory 12-core machine, and a speedup of 16 on a 2-node, 24-core NUMA.},
booktitle = {Proceedings of the 29th International Conference on Compiler Construction},
pages = {143–154},
numpages = {12},
keywords = {arrows, multiparty session types, parallelism},
location = {San Diego, CA, USA},
series = {CC 2020}
}

@inproceedings{10.1145/2695664.2695875,
author = {Almeida, Andr\'{e} and Bencomo, Nelly and Batista, Thais and Cavalcante, Everton and Dantas, Francisco},
title = {Dynamic decision-making based on NFR for managing software variability and configuration selection},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695875},
doi = {10.1145/2695664.2695875},
abstract = {Due to dynamic variability, identifying the specific conditions under which non-functional requirements (NFRs) are satisfied may be only possible at runtime. Therefore, it is necessary to consider the dynamic treatment of relevant information during the requirements specifications. The associated data can be gathered by monitoring the execution of the application and its underlying environment to support reasoning about how the current application configuration is fulfilling the established requirements. This paper presents a dynamic decision-making infrastructure to support both NFRs representation and monitoring, and to reason about the degree of satisfaction of NFRs during runtime. The infrastructure is composed of: (i) an extended feature model aligned with a domain-specific language for representing NFRs to be monitored at runtime; (ii) a monitoring infrastructure to continuously assess NFRs at runtime; and (iii) a flexible decision-making process to select the best available configuration based on the satisfaction degree of the NRFs. The evaluation of the approach has shown that it is able to choose application configurations that well fit user NFRs based on runtime information. The evaluation also revealed that the proposed infrastructure provided consistent indicators regarding the best application configurations that fit user NFRs. Finally, a benefit of our approach is that it allows us to quantify the level of satisfaction with respect to NFRs specification.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1376–1382},
numpages = {7},
keywords = {SPLs, monitoring, non-functional requirements, variability},
location = {Salamanca, Spain},
series = {SAC '15}
}

@proceedings{10.1145/3624007,
title = {GPCE 2023: Proceedings of the 22nd ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2023},
isbn = {9798400704062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 22nd ACM SIGPLAN International Conference on Generative Programming: Concepts &amp; Experiences (GPCE’23). GPCE is the premiere venue for researchers and practitioners interested in techniques that use program generation to increase programmer productivity, improve software quality, and shorten the time-to-market of software products. In addition to exploring cutting-edge techniques of generative software, GPCE seeks to foster cross-fertilization between the programming languages research communities.},
location = {Cascais, Portugal}
}

@article{10.1145/3635308,
author = {Huang, Hui and Xiao, Di and Liang, Jia},
title = {Secure Low-complexity Compressive Sensing with Preconditioning Prior Regularization Reconstruction},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1551-6857},
url = {https://doi.org/10.1145/3635308},
doi = {10.1145/3635308},
abstract = {Compressive sensing (CS), a breakthrough technology in image processing, provides a privacy-preserving layer and image reconstruction while performing sensing and recovery processes, respectively. Unfortunately, it still faces high-complexity, low-security, and low-quality reconstruction challenges during image processing. Therefore, this article presents a secure low-complexity CS scheme with preconditioning prior regularization reconstruction. More specifically, the original image is compressed by a low-complexity LFSR-based sparse circulant matrix to obtain measurements. It is worth noting that measurements achieve preliminary distribution equalization through the Tanh sequence to acquire processed measurements. Furthermore, the privacy-preserving edge processing for processed measurements can achieve high security. Finally, preconditioning prior regularization CS reconstruction is designed to improve reconstruction performance. Simulation results and analyses demonstrate that the proposed scheme can achieve low-complexity sampling, high security, and superior reconstruction performance.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jan,
articleno = {116},
numpages = {22},
keywords = {Compressive sensing, sparse circulant matrix, joint quantization and diffusion, preconditioning prior regularization reconstruction}
}

@article{10.1145/3660774,
author = {Lyu, Jun and Li, Shanshan and Zhang, He and Yang, Lanxin and Liu, Bohan and Rigger, Manuel},
title = {Towards Efficient Build Ordering for Incremental Builds with Multiple Configurations},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660774},
doi = {10.1145/3660774},
abstract = {Software products have many configurations to meet different environments and diverse needs. Building software with multiple software configurations typically incurs high costs in terms of build time and computing resources. Incremental builds could reuse intermediate artifacts if configuration settings affect only a portion of the build artifacts. The efficiency gains depend on the strategic ordering of the incremental builds as the order influences which build artifacts can be reused. Deriving an efficient order is challenging and an open problem, since it is infeasible to reliably determine the degree of re-use and time savings before an actual build. In this paper, we propose an approach, called BUDDI—BUild Declaration DIstance, for C-based and Make-based projects to derive an efficient order for incremental builds from the static information provided by the build scripts (i.e., Makefile). The core strategy of BUDDI is to measure the distance between the build declarations of configurations and predict the build size of a configuration from the build targets and build commands in each configuration. Since some artifacts could be reused in the subsequent builds if there is a close distance between the build scripts for different configurations. We implemented BUDDI as an automated tool called BuddiPlanner and evaluated it on 20 popular open-source projects, by comparing it to a baseline that randomly selects a build order. The experimental results show that the order created by BuddiPlanner outperforms 96.5% (193/200) of the random build orders in terms of build time and reduces the build time by an average of 305.94s (26%) compared to the random build orders, with a median saving of 64.88s (28%). BuddiPlanner demonstrates its potential to relieve practitioners of excessive build times and computational resource burdens caused by building multiple software configurations.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {67},
numpages = {24},
keywords = {Build Tool, Incremental Builds, Software Configurations}
}

@inproceedings{10.1145/2534645.2534649,
author = {Coetzee, Peter and Jarvis, Stephen},
title = {CRUCIBLE: towards unified secure on- and off-line analytics at scale},
year = {2013},
isbn = {9781450325066},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2534645.2534649},
doi = {10.1145/2534645.2534649},
abstract = {The burgeoning field of data science benefits from the application of a variety of analytic models and techniques to the oft-cited problems of large volume, high velocity data rates, and significant variety in data structure and semantics. Many approaches make use of common analytic techniques in either a streaming or batch processing paradigm.This paper presents progress in developing a framework for the analysis of large-scale datasets using both of these pools of techniques in a unified manner. This includes: (1) a Domain Specific Language (DSL) for describing analyses as a set of Communicating Sequential Processes, fully integrated with the Java type system, including an Integrated Development Environment (IDE) and a compiler which builds idiomatic Java; (2) a runtime model for execution of an analytic in both streaming and batch environments; and (3) a novel approach to automated management of cell-level security labels, applied uniformly across all runtimes.The paper concludes with a demonstration of the successful use of this system with a sample workload developed in (1), and an analysis of the performance characteristics of each of the runtimes described in (2).},
booktitle = {Proceedings of the 2013 International Workshop on Data-Intensive Scalable Computing Systems},
pages = {43–48},
numpages = {6},
location = {Denver, Colorado},
series = {DISCS-2013}
}

@inproceedings{10.1145/3106237.3106273,
author = {Oh, Jeho and Batory, Don and Myers, Margaret and Siegmund, Norbert},
title = {Finding near-optimal configurations in product lines by random sampling},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106273},
doi = {10.1145/3106237.3106273},
abstract = {Software Product Lines (SPLs) are highly configurable systems. This raises the challenge to find optimal performing configurations for an anticipated workload. As SPL configuration spaces are huge, it is infeasible to benchmark all configurations to find an optimal one. Prior work focused on building performance models to predict and optimize SPL configurations. Instead, we randomly sample and recursively search a configuration space directly to find near-optimal configurations without constructing a prediction model. Our algorithms are simpler and have higher accuracy and efficiency.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {61–71},
numpages = {11},
keywords = {finding optimal configurations, searching configuration spaces, software product lines},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/3332165.3347882,
author = {Morales, Rafael and Marzo, Asier and Subramanian, Sriram and Mart\'{\i}nez, Diego},
title = {LeviProps: Animating Levitated Optimized Fabric Structures using Holographic Acoustic Tweezers},
year = {2019},
isbn = {9781450368162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332165.3347882},
doi = {10.1145/3332165.3347882},
abstract = {LeviProps are tangible structures used to create interactive mid-air experiences. They are composed of an acoustically- transparent lightweight piece of fabric and attached beads that act as levitated anchors. This combination enables real- time 6 Degrees-of-Freedom control of levitated structures which are larger and more diverse than those possible with previous acoustic manipulation techniques. LeviProps can be used as free-form interactive elements and as projection surfaces. We developed an authoring tool to support the creation of LeviProps. Our tool considers the outline of the prop and the user constraints to compute the optimum locations for the anchors (i.e. maximizing trapping forces), increasing prop stability and maximum size. The tool produces a final LeviProp design which can be fabricated following a simple procedure. This paper explains and evaluates our approach and showcases example applications, such as interactive storytelling, games and mid-air displays.},
booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {651–661},
numpages = {11},
keywords = {design methods, fabrication, levitation, mid-air uis, tools},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@article{10.1145/3627157,
author = {Wan, Zhijing and Wang, Zhixiang and Chung, Cheukting and Wang, Zheng},
title = {A Survey of Dataset Refinement for Problems in Computer Vision Datasets},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3627157},
doi = {10.1145/3627157},
abstract = {Large-scale datasets have played a crucial role in the advancement of computer vision. However, they often suffer from problems such as class imbalance, noisy labels, dataset bias, or high resource costs, which can inhibit model performance and reduce trustworthiness. With the advocacy of data-centric research, various data-centric solutions have been proposed to solve the dataset problems mentioned above. They improve the quality of datasets by re-organizing them, which we call dataset refinement. In this survey, we provide a comprehensive and structured overview of recent advances in dataset refinement for problematic computer vision datasets.1 Firstly, we summarize and analyze the various problems encountered in large-scale computer vision datasets. Then, we classify the dataset refinement algorithms into three categories based on the refinement process: data sampling, data subset selection, and active learning. In addition, we organize these dataset refinement methods according to the addressed data problems and provide a systematic comparative description. We point out that these three types of dataset refinement have distinct advantages and disadvantages for dataset problems, which informs the choice of the data-centric method appropriate to a particular research objective. Finally, we summarize the current literature and propose potential future research topics.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {172},
numpages = {34},
keywords = {Dataset refinement, data sampling, subset selection, active learning}
}

@inproceedings{10.1145/288548.289061,
author = {Ranjan, Rajeev K. and Singhal, Vigyan and Somenzi, Fabio and Brayton, Robert K.},
title = {On the optimization power of retiming and resynthesis transformations},
year = {1998},
isbn = {1581130082},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/288548.289061},
doi = {10.1145/288548.289061},
booktitle = {Proceedings of the 1998 IEEE/ACM International Conference on Computer-Aided Design},
pages = {402–407},
numpages = {6},
location = {San Jose, California, USA},
series = {ICCAD '98}
}

@inproceedings{10.1145/3062341.3062366,
author = {Schneider, Scott and Wu, Kun-Lung},
title = {Low-synchronization, mostly lock-free, elastic scheduling for streaming runtimes},
year = {2017},
isbn = {9781450349888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3062341.3062366},
doi = {10.1145/3062341.3062366},
abstract = {We present the scalable, elastic operator scheduler in IBM Streams 4.2. Streams is a distributed stream processing system used in production at many companies in a wide range of industries. The programming language for Streams, SPL, presents operators, tuples and streams as the primary abstractions. A fundamental SPL optimization is operator fusion, where multiple operators execute in the same process. Streams 4.2 introduces automatic submission-time fusion to simplify application development and deployment. However, potentially thousands of operators could then execute in the same process, with no user guidance for thread placement. We needed a way to automatically figure out how many threads to use, with arbitrarily sized applications on a wide variety of hardware, and without any input from programmers. Our solution has two components. The first is a scalable operator scheduler that minimizes synchronization, locks and global data, while allowing threads to execute any operator and dynamically come and go. The second is an elastic algorithm to dynamically adjust the number of threads to optimize performance, using the principles of trusted measurements to establish trends. We demonstrate our scheduler's ability to scale to over a hundred threads, and our elasticity algorithm's ability to adapt to different workloads on an Intel Xeon system with 176 logical cores, and an IBM Power8 system with 184 logical cores.},
booktitle = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {648–661},
numpages = {14},
keywords = {elastic, lock-free, runtime scheduling, stream processing},
location = {Barcelona, Spain},
series = {PLDI 2017}
}

@proceedings{10.1145/3652024,
title = {ISMM 2024: Proceedings of the 2024 ACM SIGPLAN International Symposium on Memory Management},
year = {2024},
isbn = {9798400706158},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is with great pleasure that we welcome you to the 2024 ACM SIGPLAN International Symposium on Memory Management (ISMM '24)! This is the 23rd event in the ISMM series. We expanded the scope of ISMM this year and encouraged submissions and participation from related fields such as computer architecture and operating systems in addition to the programming languages community. The call for papers motivated submissions of work in the following areas.},
location = {Copenhagen, Denmark}
}

@inproceedings{10.1145/3194760.3194761,
author = {Kessel, Marcus and Atkinson, Colin},
title = {Integrating reuse into the rapid, continuous software engineering cycle through test-driven search},
year = {2018},
isbn = {9781450357456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194760.3194761},
doi = {10.1145/3194760.3194761},
abstract = {Today's advanced agile practices such as Continuous Integration and Test-Driven Development support a wide range of software development activities to facilitate the rapid delivery of high-quality software. However, the reuse of pre-existing, third-party software components is not one of them. Software reuse is still primarily perceived as a time-consuming, unsystematic and ultimately, "discontinuous" activity even though it aims to deliver the same basic benefits as continuous software engineering - namely, a reduction in the time and effort taken to deliver quality software. However, the increasingly central role of testing in continuous software engineering offers a way of addressing this problem by exploiting the new generation of test-driven search engines that can harvest components based on tests. This search technology not only exploits artifacts that have already been created as part of the continuous testing process to harvest components, it returns results that have a high likelihood of being fit for purpose and thus of being worth reusing. In this paper, we propose to augment continuous software engineering with the rapid, continuous reuse of software code units by integrating the test-driven mining of software artifact repositories into the continuous integration process. More specifically, we propose to use tests written as part of the Test-First Development approach to perform test-driven searches for matching functionality while developers are working on their normal development activities. We discuss the idea of rapid, continuous code reuse based on recent advances in our test-driven search platform and elaborate on scenarios for its application in the future.},
booktitle = {Proceedings of the 4th International Workshop on Rapid Continuous Software Engineering},
pages = {8–11},
numpages = {4},
keywords = {rapid continuous code reuse, rapid continuous integration, test-driven development, test-driven reuse, test-driven search},
location = {Gothenburg, Sweden},
series = {RCoSE '18}
}

@inproceedings{10.1145/3637528.3672069,
author = {Karimi Monsefi, Amin and Karisani, Payam and Zhou, Mengxi and Choi, Stacey and Doble, Nathan and Ji, Heng and Parthasarathy, Srinivasan and Ramnath, Rajiv},
title = {Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3672069},
doi = {10.1145/3637528.3672069},
abstract = {Standard modern machine-learning-based imaging methods have faced challenges in medical applications due to the high cost of dataset construction and, thereby, the limited labeled training data available. Additionally, upon deployment, these methods are usually used to process a large volume of data on a daily basis, imposing a high maintenance cost on medical facilities. In this paper, we introduce a new neural network architecture, termed LoGoNet, with a tailored self-supervised learning (SSL) method to mitigate such challenges. LoGoNet integrates a novel feature extractor within a U-shaped architecture, leveraging Large Kernel Attention (LKA) and a dual encoding strategy to capture both long-range and short-range feature dependencies adeptly. This is in contrast to existing methods that rely on increasing network capacity to enhance feature extraction. This combination of novel techniques in our model is especially beneficial in medical image segmentation, given the difficulty of learning intricate and often irregular body organ shapes, such as the spleen. Complementary, we propose a novel SSL method tailored for 3D images to compensate for the lack of large labeled datasets. Our method combines masking and contrastive learning techniques within a multi-task learning framework and is compatible with both Vision Transformer (ViT) and CNN-based models. We demonstrate the efficacy of our methods in numerous tasks across two standard datasets (i.e., BTCV and MSD). Benchmark comparisons with eight state-of-the-art models highlight LoGoNet's superior performance in both inference time and accuracy. Code available at: https://github.com/aminK8/Masked-LoGoNet.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1348–1359},
numpages = {12},
keywords = {dual-encoder, image segmentation, medical imaging, multi-task learning, self-supervised learning},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3669940.3707286,
author = {Cai, Xuran and Goharshady, Amir Kafshdar and Hitarth, S. and Lam, Chun Kit},
title = {Faster Chaitin-like Register Allocation via Grammatical Decompositions of Control-Flow Graphs},
year = {2025},
isbn = {9798400706981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3669940.3707286},
doi = {10.1145/3669940.3707286},
abstract = {It is well-known that control-flow graphs (CFGs) of structured programs are sparse. This sparsity has been previously formalized in terms of graph parameters such as treewidth and pathwidth and used to design faster parameterized algorithms for numerous compiler optimization, model checking and program analysis tasks.In this work, we observe that the known graph sparsity parameters fail to exactly capture the kind of sparsity exhibited by CFGs. For example, while all structured CFGs have a treewidth of at most 7, not every graph with a treewidth of 7 or less is realizable as a CFG. As a result, current parameterized algorithms are solving the underlying graph problems over a more general family of graphs than the CFGs.To address this problem, we design a new but natural concept of graph decomposition based on a grammar that precisely captures the set of graphs that can be realized as CFGs of programs. We show that our notion of decomposition enables the same type of dynamic programming algorithms that are often used in treewidth/pathwidth-based methods. As two concrete applications, using our grammatical decomposition of CFGs, we provide asymptotically more efficient algorithms for two variants of the classical problem of register allocation as defined by Chaitin, i.e. assigning program variables to a limited number of registers such that variables with intersecting lifetimes are not assigned to the same register. Note that Chaitin's formulation of register allocation does not allow live-range splitting. Our algorithms are asymptotically faster not only in comparison with the non-parameterized solutions for these problems, but also compared to the state-of-the-art treewidth/pathwidth-based approaches in the literature. For minimum-cost register allocation over a fixed number of registers, we provide an algorithm with a runtime of O(|G| ⋅ |𝕈| 5 ⋅ r) where |G| is the size of the program, 𝕈 is the set of program variables and r is the number of registers. In contrast, the previous treewidth-based algorithm had a runtime of O(|G| ⋅ |𝕈| 16 ⋅ r). For the decision problem of spill-free register allocation, our algorithm's runtime is O(|G| ⋅ r5 ⋅ r + 5) whereas the previous works had a runtime of O(|G| ⋅ r16 ⋅ r).Finally, we provide extensive experimental results on spill-free register allocation, showcasing the scalability of our approach in comparison to previous state-of-the-art methods. Most notably, our approach can handle real-world instances with up to 20 registers, whereas previous works could only scale to 8. This is a significant improvement since most ubiquitous architectures, such as the x86 family, have 16 registers. For such architectures, our approach is the first-ever exact algorithm that scales up to solve the real-world instances of spill-free register allocation.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1},
pages = {463–477},
numpages = {15},
keywords = {control-flow graphs, graph decompositions, register allocation, sparsity},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@article{10.1145/3700599,
author = {Francis, Anthony and P\'{e}rez-D’Arpino, Claudia and Li, Chengshu and Xia, Fei and Alahi, Alexandre and Alami, Rachid and Bera, Aniket and Biswas, Abhijat and Biswas, Joydeep and Chandra, Rohan and Chiang, Hao-Tien Lewis and Everett, Michael and Ha, Sehoon and Hart, Justin and How, Jonathan P. and Karnan, Haresh and Lee, Tsang-Wei Edward and Manso, Luis J. and Mirsky, Reuth and Pirk, S\"{o}ren and Singamaneni, Phani Teja and Stone, Peter and Taylor, Ada V. and Trautman, Peter and Tsoi, Nathan and V\'{a}zquez, Marynel and Xiao, Xuesu and Xu, Peng and Yokoyama, Naoki and Toshev, Alexander and Mart\'{\i}n-Mart\'{\i}n, Roberto},
title = {Principles and Guidelines for Evaluating Social Robot Navigation Algorithms},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
url = {https://doi.org/10.1145/3700599},
doi = {10.1145/3700599},
abstract = {A major challenge to deploying robots widely is navigation in human-populated environments, commonly referred to as social robot navigation. While the field of social navigation has advanced tremendously in recent years, the fair evaluation of algorithms that tackle social navigation remains hard because it involves not just robotic agents moving in static environments but also dynamic human agents and their perceptions of the appropriateness of robot behavior. In contrast, clear, repeatable, and accessible benchmarks have accelerated progress in fields like computer vision, natural language processing and traditional robot navigation by enabling researchers to fairly compare algorithms, revealing limitations of existing solutions and illuminating promising new directions. We believe the same approach can benefit social navigation. In this article, we pave the road toward common, widely accessible, and repeatable benchmarking criteria to evaluate social robot navigation. Our contributions include (a) a definition of a socially navigating robot as one that respects the principles of safety, comfort, legibility, politeness, social competency, agent understanding, proactivity, and responsiveness to context, (b) guidelines for the use of metrics, development of scenarios, benchmarks, datasets, and simulators to evaluate social navigation, and (c) a design of a social navigation metrics framework to make it easier to compare results from different simulators, robots, and datasets.},
journal = {J. Hum.-Robot Interact.},
month = feb,
articleno = {34},
numpages = {65},
keywords = {social robotics, robot navigation, datasets, benchmarks, simulators}
}

@article{10.1109/TCBB.2019.2961667,
author = {Huang, Hai-Hui and Liang, Yong},
title = {A Novel Cox Proportional Hazards Model for High-Dimensional Genomic Data in Cancer Prognosis},
year = {2019},
issue_date = {Sept.-Oct. 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {5},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2961667},
doi = {10.1109/TCBB.2019.2961667},
abstract = {The Cox proportional hazards model is a popular method to study the connection between feature and survival time. Because of the high-dimensionality of genomic data, existing Cox models trained on any specific dataset often generalize poorly to other independent datasets. In this paper, we suggest a novel strategy for the Cox model. This strategy is included a new learning technique, self-paced learning (SPL), and a new gene selection method, SCAD-Net penalty. The SPL method is adopted to aid to build a more accurate prediction with its built-in mechanism of learning from easy samples first and adaptively learning from hard samples. The SCAD-Net penalty has fixed the problem of the SCAD method without an inherent mechanism to fuse the prior graphical information. We combined the SPL with the SCAD-Net penalty to the Cox model (SSNC). The simulation shows that the SSNC outperforms the benchmark in terms of prediction and gene selection. The analysis of a large-scale experiment across several cancer datasets shows that the SSNC method not only results in higher prediction accuracies but also identifies markers that satisfactory stability across another validation dataset. The demo code for the proposed method is provided in supplemental file.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = dec,
pages = {1821–1830},
numpages = {10}
}

@article{10.1145/3654668,
author = {Huang, Junjian and Ren, Hao and Liu, Shulin and Liu, Yong and Lv, Chuanlu and Lu, Jiawen and Xie, Changyong and Lu, Hong},
title = {Real-Time Attentive Dilated U-Net for Extremely Dark Image Enhancement},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {8},
issn = {1551-6857},
url = {https://doi.org/10.1145/3654668},
doi = {10.1145/3654668},
abstract = {Images taken under low-light conditions suffer from poor visibility, color distortion, and graininess, all of which degrade the image quality and hamper the performance of downstream vision tasks, such as object detection and instance segmentation in the field of autonomous driving, making low-light enhancement an indispensable basic component of high-level visual tasks. Low-light enhancement aims to mitigate these issues, and has garnered extensive attention and research over several decades. The primary challenge in low-light image enhancement arises from the low signal-to-noise ratio caused by insufficient lighting. This challenge becomes even more pronounced in near-zero lux conditions, where noise overwhelms the available image information. Both traditional image signal processing pipeline and conventional low-light image enhancement methods struggle in such scenarios. Recently, deep neural networks have been used to address this challenge. These networks take unmodified RAW images as input and produce the enhanced sRGB images, forming a deep learning based image signal processing pipeline. However, most of these networks are computationally expensive and thus far from practical use. In this article, we propose a lightweight model called attentive dilated U-Net (ADU-Net) to tackle this issue. Our model incorporates several innovative designs, including an asymmetric U-shape architecture, dilated residual modules for feature extraction, and attentive fusion modules for feature fusion. The dilated residual modules provide strong representative capability, whereas the attentive fusion modules effectively leverage low-level texture information and high-level semantic information within the network. Both modules employ a lightweight design but offer significant performance gains. Extensive experiments demonstrate that our method is highly effective, achieving an excellent balance between image quality and computational complexity—that is, taking less than 4ms for a high-definition 4K image on a single GTX 1080Ti GPU and yet maintaining competitive visual quality. Furthermore, our method exhibits pleasing scalability and generalizability, highlighting its potential for widespread applicability.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jun,
articleno = {231},
numpages = {19},
keywords = {Low-light image enhancement, image signal processing, convolutional neural networks, attention mechanism}
}

@article{10.1145/3706423,
author = {Zheng, Yanwei and Li, Yaling and Li, Changrui and Zhang, Taiqi and Zou, Yifei and Yu, Dongxiao},
title = {Learning Attribute Attention and Retrospect Location for Instance Object Navigation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1936-1955},
url = {https://doi.org/10.1145/3706423},
doi = {10.1145/3706423},
abstract = {Visual object navigation, a classical problem in embodied intelligence tasks, requires agents to find a specified target using only the first-person view of visual information. A number of methods that search only for an object in the same category are known as category navigation. This is different from the real world where objects are labelled as “whose”. Then, the navigation task of distinguishing objects by using attributes is closer to practicality, which is called instance-level object navigation. However, the current instance-level object navigation approaches have some limitations. Firstly, different attribute information are jumbled together and the fine-grained feature relationships are ignored, resulting weakened discriminative ability. Secondly, the explored trajectories have insufficient correlation with the navigation target, resulting in degraded memory ability. In this paper, we present a novel cascade architecture from a fresh perspective to solve those limitations. Our approach has two main techniques: Object-Attribute Attention Graph (OAAG) and Objective Retrospect and Location Module (ORLM). OAAG consists of Object-aware Graph (OAG) which is created to encode the dynamically relations between all instances, and Attribute-Attention Graph (AAG) which assigns different attention weight to different attributes. ORLM makes it possible for the agent to review the region it explores and enhance the relevant memorization of the target. We connect them as the final model output and input it to the deep reinforcement learning A3C framework. We evaluate these two technologies on the AI2-THOR simulator. Experimental studies have shown that our approach outperforms other related works and achieves the state-of-the-art results on three specific tasks (Instance-Localization, Instance-Navigation and Category-Localization). The project is available at https://github.com/visee-sdu/Instance-Navigation.},
note = {Just Accepted},
journal = {J. Data and Information Quality},
month = jan,
keywords = {Instance-level Object Navigation, Attribute Attention, Objective Retrospect and Location}
}

@article{10.1145/3729270,
author = {Fani, Davoud and Beheshti-Shirazi, Aliasghar and Ghanbari, Mohammad and Rezaei, Esmatollah},
title = {On Temporal Smoothness of Video Reconstruction Quality in the DCVS via Non-Uniform Sampling},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1551-6857},
url = {https://doi.org/10.1145/3729270},
doi = {10.1145/3729270},
abstract = {To the distributed video coding approach, which focuses on fully or partially shifting the computational complexity from video encoder to the decoder, the simplicity and highly compact sampling in emerging compressive sensing appear to be very efficient tools. So, the distributed compressive video sensing (DCVS) has attracted much attentions in the video coding community by applying constant high measurement rate (MR) for key frames sampling and constant low MR for non-key frames sampling. According to use of constant and different MRs for the key and non-key frames sampling, severe and undesirable fluctuations in quality of reconstructed video frames is a common unresolved shortcoming in the DCVS, which negatively affects the users’ visual experience. To suppress sharp and undesirable quality fluctuations, group of picture (GOP)-level non-uniform MR allocation models are proposed in this paper for the key and non-key frames at the encoder of the DCVS. This enhances visual quality without incurring noticeable computational cost to the encoder. A new multi-step reconstruction scheme is also proposed at the decoder exploiting spatial-temporal information in the reconstruction process with a tolerable computational complexity and remarkable reconstruction performance. It compensates for quality degradations, which may be caused by non-uniform MR allocation, to successive GOPs to reach high average quality and temporal smoothness of quality at the same time. Extensive experiments on different video sequences show that not only desirable high average reconstruction quality is maintained, but severe and undesirable quality fluctuations are also well suppressed. Hence, the users’ perceived quality is highly promoted, while the compression ratio does not exceed a certain target by restricting average MR to reach target MR in the long-term.},
note = {Just Accepted},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = apr,
keywords = {distributed compressive video sensing, non-uniform sampling, perceived quality, temporal smoothness of quality, visual quality}
}

@article{10.1145/3712288,
author = {Zhang, Qian and Guo, Kaiyi and Yang, Yifei and Wang, Dong},
title = {WearSE: Enabling Streaming Speech Enhancement on Eyewear Using Acoustic Sensing},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
url = {https://doi.org/10.1145/3712288},
doi = {10.1145/3712288},
abstract = {Smart eyewear has rapidly evolved in recent years, yet its mobile and in-the-wild characteristics often make voice interactions on such devices susceptible to external interferences. In this paper, we introduce WearSE, a system that utilizes acoustic signals emitted and received by speakers and microphones mounted on eyewear to perceive facial movements during speech, achieving multimodal speech enhancement. WearSE incorporates three key designs to meet the high demands for real-time operation and robustness on smart eyewear. First, considering the frequent use in mobile scenarios, we design a sensing-enhanced network to amplify the capability of acoustic sensing, eliminating dynamic multipath interferences. Second, we develop a lightweight speech enhancement network that enhances both the amplitude and phase of the speech spectrum. Through a casual network design, computational demands are significantly reduced, ensuring real-time operation on mobile devices. Third, addressing the scarcity of paired data, we design a memory-based back-translation mechanism to generate pseudo-acoustic sensing data using a large amount of publicly available speech data for network training. We construct a prototype system and extensively evaluate WearSE through experiments. In multi-speaker scenarios, our approach exhibits much better performance than pure audio speech enhancement methods. Comparisons with commercial smart eyewear also demonstrate that WearSE significantly surpasses existing noise reduction algorithms in these devices. The audio demo of WearSE is available on https://github.com/WearSE/wearse.github.io.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = mar,
articleno = {25},
numpages = {30},
keywords = {back-translation, cascaded network, sensing-enhanced network, smart eyewear, streaming speech enhancement}
}

@proceedings{10.1145/2993236,
title = {GPCE 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.1145/3510466.3510484,
author = {Ratzenb\"{o}ck, Michael and Gr\"{u}nbacher, Paul and Assun\c{c}ao, Wesley K. G. and Egyed, Alexander and Linsbauer, Lukas},
title = {Refactoring Product Lines by Replaying Version Histories},
year = {2022},
isbn = {9781450396042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510466.3510484},
doi = {10.1145/3510466.3510484},
abstract = {When evolving software product lines, new features are added over time and existing features are revised. Engineers also decide to merge different features or split features in other cases. Such refactoring tasks are difficult when using manually maintained feature-to-code mappings. Intensional version control systems such as ECCO overcome this issue with automatically computed feature-to-code mappings. Furthermore, they allow creating variants that have not been explicitly committed before. However, such systems are still rarely used compared to extensional version control systems like Git, which keep track of the evolution history by assigning revisions to states of a system. This paper presents an approach combining both extensional and intensional version control systems, which relies on the extensional version control system Git to store versions. Developers selectively tag existing versions to describe the evolution at the level of features. Our approach then automatically replays the evolution history to create a repository of the intensional variation control system ECCO. The approach contributes to research on refactoring features of existing product lines and migrating existing systems to product lines. We provide an initial evaluation of the approach regarding correctness and performance based on an existing system.},
booktitle = {Proceedings of the 16th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {8},
numpages = {10},
keywords = {feature-level evolution, refactoring, version control systems},
location = {Florence, Italy},
series = {VaMoS '22}
}

@article{10.1145/3626954,
author = {Yasar, Mohammad Samin and Islam, Md Mofijul and Iqbal, Tariq},
title = {IMPRINT: Interactional Dynamics-aware Motion Prediction in Teams using Multimodal Context},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
url = {https://doi.org/10.1145/3626954},
doi = {10.1145/3626954},
abstract = {Robots are moving from working in isolation to working with humans as a part of human-robot teams. In such situations, they are expected to work with multiple humans and need to understand and predict the team members’ actions. To address this challenge, in this work, we introduce IMPRINT, a multi-agent motion prediction framework that models the interactional dynamics and incorporates the multimodal context (e.g., data from RGB and depth sensors and skeleton joint positions) to accurately predict the motion of all the agents in a team. In IMPRINT, we propose an Interaction module that can extract the intra-agent and inter-agent dynamics before fusing them to obtain the interactional dynamics. Furthermore, we propose a Multimodal Context module that incorporates multimodal context information to improve multi-agent motion prediction. We evaluated IMPRINT by comparing its performance on human-human and human-robot team scenarios against state-of-the-art methods. The results suggest that IMPRINT outperformed all other methods over all evaluated temporal horizons. Additionally, we provide an interpretation of how IMPRINT incorporates the multimodal context information from all the modalities during multi-agent motion prediction. The superior performance of IMPRINT provides a promising direction to integrate motion prediction with robot perception and enable safe and effective human-robot collaboration.},
journal = {J. Hum.-Robot Interact.},
month = aug,
articleno = {43},
numpages = {29},
keywords = {Datasets, neural networks, gaze detection, text tagging}
}

@inproceedings{10.1145/3474085.3475471,
author = {Huang, Zongmo and Ren, Yazhou and Pu, Xiaorong and He, Lifang},
title = {Non-Linear Fusion for Self-Paced Multi-View Clustering},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475471},
doi = {10.1145/3474085.3475471},
abstract = {With the advance of the multi-media and multi-modal data, multi-view clustering (MVC) has drawn increasing attentions recently. In this field, one of the most crucial challenges is that the characteristics and qualities of different views usually vary extensively. Therefore, it is essential for MVC methods to find an effective approach that handles the diversity of multiple views appropriately. To this end, a series of MVC methods focusing on how to integrate the loss from each view have been proposed in the past few years. Among these methods, the mainstream idea is assigning weights to each view and then combining them linearly. In this paper, inspired by the effectiveness of non-linear combination in instance learning and the auto-weighted approaches, we propose Non-Linear Fusion for Self-Paced Multi-View Clustering (NSMVC), which is totally different from the the conventional linear-weighting algorithms. In NSMVC, we directly assign different exponents to different views according to their qualities. By this way, the negative impact from the corrupt views can be significantly reduced. Meanwhile, to address the non-convex issue of the MVC model, we further define a novel regularizer-free modality of Self-Paced Learning (SPL), which fits the proposed non-linear model perfectly. Experimental results on various real-world data sets demonstrate the effectiveness of the proposed method.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {3211–3219},
numpages = {9},
keywords = {multi-view clustering, non-linear fusion, self-paced learning},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3664647.3681640,
author = {Hong, Haodong and Wang, Sen and Huang, Zi and Wu, Qi and Liu, Jiajun},
title = {Navigating Beyond Instructions: Vision-and-Language Navigation in Obstructed Environments},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3681640},
doi = {10.1145/3664647.3681640},
abstract = {Real-world navigation often involves dealing with unexpected obstructions such as closed doors, moved objects, and unpredictable entities. However, mainstream Vision-and-Language Navigation (VLN) tasks typically assume instructions perfectly align with the fixed and predefined navigation graphs without any obstructions. This assumption overlooks potential discrepancies in actual navigation graphs and given instructions, which can cause major failures for both indoor and outdoor agents. To address this issue, we integrate diverse obstructions into the R2R dataset by modifying both the navigation graphs and visual observations, introducing an innovative dataset and task, R2R with UNexpected Obstructions (R2R-UNO). R2R-UNO contains various types and numbers of path obstructions to generate instruction-reality mismatches for VLN research. Experiments on R2R-UNO reveal that state-of-the-art VLN methods inevitably encounter significant challenges when facing such mismatches, indicating that they rigidly follow instructions rather than navigate adaptively. Therefore, we propose a novel method called ObVLN (Obstructed VLN), which includes a curriculum training strategy and virtual graph construction to help agents effectively adapt to obstructed environments. Empirical results show that ObVLN not only maintains robust performance in unobstructed scenarios but also achieves a substantial performance advantage with unexpected obstructions. The source code is available at https://github.com/honghd16/ObstructedVLN.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {7639–7648},
numpages = {10},
keywords = {embodied agents, object insertion, vision-and-language navigation},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@inproceedings{10.1145/2307636.2307676,
author = {Borchert, Christoph and Lohmann, Daniel and Spinczyk, Olaf},
title = {CiAO/IP: a highly configurable aspect-oriented IP stack},
year = {2012},
isbn = {9781450313018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2307636.2307676},
doi = {10.1145/2307636.2307676},
abstract = {Internet protocols are constantly gaining relevance for the domain of mobile and embedded systems. However, building complex network protocol stacks for small resource-constrained devices is more than just porting a reference implementation. Due to the cost pressure in this area especially the memory footprint has to be minimized. Therefore, embedded TCP/IP implementations tend to be statically configurable with respect to the concrete application scenario. This paper describes our software engineering approach for building CiAO/IP - a tailorable TCP/IP stack for small embedded systems, which pushes the limits of static configurability while retaining source code maintainability. Our evaluation results show that CiAO/IP thereby outperforms both lwIP and uIP in terms of code size (up to 90% less than uIP), throughput (up to 20% higher than lwIP), energy consumption (at least 40% lower than uIP) and, most importantly, tailorability.},
booktitle = {Proceedings of the 10th International Conference on Mobile Systems, Applications, and Services},
pages = {435–448},
numpages = {14},
keywords = {aop, aspect-oriented programming, aspectc++, embedded systems, internet protocol, network protocol stacks, operating systems, tcp/ip},
location = {Low Wood Bay, Lake District, UK},
series = {MobiSys '12}
}

@inproceedings{10.1145/3240508.3240648,
author = {Zheng, Xiaoju and Zha, Zheng-Jun and Zhuang, Liansheng},
title = {A Feature-Adaptive Semi-Supervised Framework for Co-saliency Detection},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240648},
doi = {10.1145/3240508.3240648},
abstract = {Co-saliency detection, which refers to the discovery of common salient foreground regions in a group of relevant images, has attracted increasing attention due to its widespread applications in many vision tasks. Existing methods assemble features from multiple views toward a comprehensive representation, however overlook the efficacy disparity among various features in detecting co-saliency. This paper proposes a novel feature-adaptive semi-supervised (FASS) framework for co-saliency detection, which seamlessly integrates multi-view feature learning, graph structure optimization and co-saliency prediction in a unified solution. In particular, the FASS exploits the efficacy disparity of multi-view features at both view and element levels by a joint formulation of view-wise feature weighting and element-wise feature selection, leading to an effective representation robust to feature noise and redundancy as well as adaptive to the task at hand. It predicts co-saliency map by optimizing co-saliency label prorogation over a graph of both labeled and unlabeled image regions. The graph structure is optimized jointly with feature learning and co-saliency prediction to precisely characterize underlying correlation among regions. The FASS is thus able to produce satisfactory co-saliency map based on the effective exploration of multi-view features as well as inter-region correlation. Extensive experiments on three benchmark datasets, i.e., iCoseg, Cosal2015 and MSRC, have demonstrated that the proposed FASS outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {959–966},
numpages = {8},
keywords = {co-saliency detection, graph optimization, multi-view feature, semi-supervised learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@article{10.1109/TASLP.2021.3065730,
author = {Shi, Dongyuan and Gan, Woon-Seng and Lam, Bhan and Wen, Shulin and Shen, Xiaoyi},
title = {Optimal Output-Constrained Active Noise Control Based on Inverse Adaptive Modeling Leak Factor Estimate},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3065730},
doi = {10.1109/TASLP.2021.3065730},
abstract = {Output saturation, mainly caused by the power amplifier, is a critical issue influencing the performance and stability of an adaptive system, such as in active noise control. In this paper, a quadratically constrained quadratic program (QCQP) is defined to achieve optimal control under the averaging-output-power constraint, which ensures the output of the system operates linearly and hence, avoids the output saturation. To solve this QCQP problem recursively in practice, this paper utilizes one of the leaky-based filtered-x least mean square algorithm with an optimal leak factor. However, this method only can be applied when the statistical feature of the control signal with maximum output-power is known, which is difficult to obtain in practice. Hence, by incorporating the adaptive inverse modeling technique, we can derive a practical estimation of the optimal leaky factor, which is applicable to different noise types. Furthermore, as the optimal output-constraint control forces the output to operate linearly, the nonlinear amplifier model is not required for the leak factor estimate. The simulation of the proposed algorithm is carried out on measured nonlinear paths to validate its efficacy.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {1256–1269},
numpages = {14}
}

@inproceedings{10.1145/3387940.3391474,
author = {Brings, Jennifer and Daun, Marian},
title = {Towards automated safety analysis for architectures of dynamically forming networks of cyber-physical systems},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391474},
doi = {10.1145/3387940.3391474},
abstract = {Dynamically forming networks of cyber-physical systems are becoming increasingly widespread in manufacturing, transportation, automotive, avionics and more domains. The emergence of future internet technology and the ambition for ever closer integration of different systems leads to highly collaborative cyber-physical systems. Such cyber-physical systems form networks to provide additional functions, behavior, and benefits the individual systems cannot provide on their own. As safety is a major concern of systems from these domains, there is a need to provide adequate support for safety analyses of these collaborative cyber-physical systems. This support must explicitly consider the dynamically formed networks of cyber-physical systems. This is a challenging task as the configurations of these cyber-physical system networks (i.e. the architecture of the super system the individual system joins) can differ enormously depending on the actual systems joining a cyber-physical system network. Furthermore, the configuration of the network heavily impacts the adaptations performed by the individual systems and thereby impacting the architecture not only of the system network but of all individual systems involved. As existing safety analysis techniques, however, are not meant for supporting such an array of potential system network configurations the individual system will have to be able to cope with at runtime, we propose automated support for safety analysis for these systems that considers the configuration of the system network. Initial evaluation results from the application to industrial case examples show that the proposed support can aid in the detection of safety defects.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {258–265},
numpages = {8},
keywords = {cyber-physical system, safety analysis, system architecture},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@proceedings{10.1145/3613424,
title = {MICRO '23: Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Toronto, ON, Canada}
}

@inproceedings{10.1145/3511808.3557696,
author = {Lu, Yihang and Zheng, Xuan and Lu, Jitao and Wang, Rong and Nie, Feiping and Li, Xuelong},
title = {Self-Paced and Discrete Multiple Kernel k-Means},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557696},
doi = {10.1145/3511808.3557696},
abstract = {Multiple Kernel K-means (MKKM) uses various kernels from different sources to improve clustering performance. However, most of the existing models are non-convex, which is prone to be stuck into bad local optimum, especially with noise and outliers. To address the issue, we propose a novel Self-Paced and Discrete Multiple Kernel K-Means (SPD-MKKM). It learns the MKKM model in a meaningful order by progressing both samples and kernels from easy to complex, which is beneficial to avoid bad local optimum. In addition, whereas existing methods optimize in two stages: learning the relaxation matrix and then finding the discrete one by extra discretization, our work can directly gain the discrete cluster indicator matrix without extra process. What's more, a well-designed alternative optimization is employed to reduce the overall computational complexity via using the coordinate descent technique. Finally, thorough experiments performed on real-world datasets illustrated the excellence and efficacy of our method.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {4284–4288},
numpages = {5},
keywords = {clustering, multiple kernel k-means, self-paced learning},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@article{10.5555/3546258.3546440,
author = {Klink, Pascal and Abdulsamad, Hany and Belousov, Boris and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
title = {A probabilistic interpretation of self-paced learning with applications to reinforcement learning},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Across machine learning, the use of curricula has shown strong empirical potential to improve learning from data by avoiding local optima of training objectives. For reinforcement learning (RL), curricula are especially interesting, as the underlying optimization has a strong tendency to get stuck in local optima due to the exploration-exploitation trade-off. Recently, a number of approaches for an automatic generation of curricula for RL have been shown to increase performance while requiring less expert knowledge compared to manually designed curricula. However, these approaches are seldomly investigated from a theoretical perspective, preventing a deeper understanding of their mechanics. In this paper, we present an approach for automated curriculum generation in RL with a clear theoretical underpinning. More precisely, we formalize the well-known self-paced learning paradigm as inducing a distribution over training tasks, which trades off between task complexity and the objective to match a desired task distribution. Experiments show that training on this induced distribution helps to avoid poor local optima across RL algorithms in different tasks with uninformative rewards and challenging exploration requirements.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {182},
numpages = {52},
keywords = {curriculum learning, reinforcement learning, self-paced learning, tempered inference, rl-as-inference}
}

@proceedings{10.1145/3669940,
title = {ASPLOS '25: Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1},
year = {2025},
isbn = {9798400706981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are delighted to introduce the first volume of the ASPLOS proceedings for 2025. The conference is in its third year of an experiment with a three-deadline structure: authors can submit to any of three separate review cycles handled by a single year-long program committee. This volume includes papers from the first two review cycles, which had submission deadlines in the spring and summer of 2024. We combined the two cycles because submission volumes in the spring cycle were disproportionately small.This volume contains 72 of the 74 papers accepted to ASPLOS 2025 to date. This includes papers accepted in the spring and summer cycles and those invited to submit a revision in the spring cycle that was ultimately accepted. Two of these 74 accepted papers are still undergoing artifact evaluation and will be published in a subsequent volume. The spring and summer review cycles saw a combined 586 submissions. These submissions were reviewed by a 208-person Program Committee augmented by 57 External Review Committee members. On occasion, we solicited a small number of external expert reviews. On the PC, 129 members self-reported they were in an academic role and 77 self-reported they were in an industrial role. On the ERC it was 43 and 13 respectively. The median PhD year of the combined committees was 2014. In addition to these committees, we engaged ten vice chairs, experienced and trusted reviewers who helped us monitor the review process for each paper.These committees reviewed all of the submissions that were not desk rejected (11 papers) or withdrawn (4 papers). In keeping with recent norms, the technical review happened in two phases. Each paper received three reviews in the first round, with, in most cases, two additional reviews in the second round for the 54% of submissions that advanced. To assign reviews, we used the Toronto Paper Matching System (TPMS) to provide a preliminary review assignment that matched reviewer expertise. We then manually inspected and adjusted these assignments as needed: for example, to correct errors in TPMS's topic modeling or adjust to late-discovered conflicts. In addition, each paper was assigned a non-conflicted chair and a non-conflicted vice chair to provide two extra sets of eyes to monitor and facilitate the process. Due to the size and distribution of the PC, which spanned 14 time zones, the PC did not meet synchronously. Instead, each paper was discussed by the reviewers via comments in the HotCRP system. Ultimately, the discussion for each paper reached one of three outcomes: rejection, conditional acceptance, or major revision. All conditionally accepted papers were shepherded. Major revision papers were invited to revise and resubmit their paper for a second round of review by a subset of the original reviewers. All authors of papers that advanced to the second round of review were given the opportunity to see and respond to their reviewer questions prior to the reviewer discussion.},
location = {Rotterdam, Netherlands}
}

@inproceedings{10.1145/3338906.3338928,
author = {Shahin, Ramy and Chechik, Marsha and Salay, Rick},
title = {Lifting Datalog-based analyses to software product lines},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338928},
doi = {10.1145/3338906.3338928},
abstract = {Applying program analyses to Software Product Lines (SPLs) has been a fundamental research problem at the intersection of Product Line Engineering and software analysis. Different attempts have been made to ”lift” particular product-level analyses to run on the entire product line. In this paper, we tackle the class of Datalog-based analyses (e.g., pointer and taint analyses), study the theoretical aspects of lifting Datalog inference, and implement a lifted inference algorithm inside the Souffl\'{e} Datalog engine. We evaluate our implementation on a set of benchmark product lines. We show significant savings in processing time and fact database size (billions of times faster on one of the benchmarks) compared to brute-force analysis of each product individually.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {39–49},
numpages = {11},
keywords = {Datalog, Doop, Lifting, Pointer Analysis, Program Analysis, Software Product Lines, Souffl'{e}},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.5555/1873738.1873746,
author = {Dethlefs, Nina and Cuay\'{a}huitl, Heriberto},
title = {Hierarchical reinforcement learning for adaptive text generation},
year = {2010},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We present a novel approach to natural language generation (NLG) that applies hierarchical reinforcement learning to text generation in the wayfinding domain. Our approach aims to optimise the integration of NLG tasks that are inherently different in nature, such as decisions of content selection, text structure, user modelling, referring expression generation (REG), and surface realisation. It also aims to capture existing interdependencies between these areas. We apply hierarchical reinforcement learning to learn a generation policy that captures these interdependencies, and that can be transferred to other NLG tasks. Our experimental results---in a simulated environment---show that the learnt wayfinding policy outperforms a baseline policy that takes reasonable actions but without optimization.},
booktitle = {Proceedings of the 6th International Natural Language Generation Conference},
pages = {37–45},
numpages = {9},
location = {Trim, Co. Meath, Ireland},
series = {INLG '10}
}

@article{10.1145/3697837,
author = {Liu, Chongyu and Peng, Dezhi and Liu, Yuliang and Jin, Lianwen},
title = {CTRNet++: Dual-Path Learning with Local-Global Context Modeling for Scene Text Removal},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3697837},
doi = {10.1145/3697837},
abstract = {Recent advances in scene text removal have attracted growing research interest due to its applications on privacy protection, document restoration, and text editing. While deep learning and generative adversarial network have shown significant progress, existing methods often struggle to generate consistent and plausible textures when erasing texts on complex backgrounds. To address this challenge, we propose a Contextual-guided Text Removal Network (CTRNet). CTRNet utilizes Low-level/High-level Contextual Guidance blocks (LCG, HCG) to explore both low-level structure and high-level discriminative context features from existing data to guide the text erasure and background restoration process. We further extend CTRNet to CTRNet++ by incorporate an auto-encoder architecture as a novel and effective HCG block, which serves as an additional image-inpainting branch, providing more accurate texture and context clues with the assistance of a large volume of natural images. Then we introduce a Context Embedding and Content Feature Modeling (CECFM) block that combines depth-wise CNN and Transformer layers to capture local features and establish long-term relationships among pixels globally. In addition, an efficient Progressive Feature Fusion Module (PFFM) is proposed to fully utilize multi-scale features from different branches. Experiments on benchmark datasets, SCUT-EnsText and SCUT-Syn, demonstrate that CTRNet++ significantly outperforms existing state-of-the-art methods and exhibits a stronger ability for complex background reconstruction. The code is available at .},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = dec,
articleno = {32},
numpages = {22},
keywords = {Scene Text Removal, Context Guidance, Dual-path Learning}
}

@inproceedings{10.1145/2993236.2993246,
author = {Kienzle, J\"{o}rg and Mussbacher, Gunter and Collet, Philippe and Alam, Omar},
title = {Delaying decisions in variable concern hierarchies},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993246},
doi = {10.1145/2993236.2993246},
abstract = {Concern-Oriented Reuse (CORE) proposes a new way of structuring model-driven software development, where models of the system are modularized by domains of abstraction within units of reuse called concerns. Within a CORE concern, models are further decomposed and modularized by features. This paper extends CORE with a technique that enables developers of high-level concerns to reuse lower-level concerns without unnecessarily committing to a specific feature selection. The developer can select the functionality that is minimally needed to continue development, and reexpose relevant alternative lower-level features of the reused concern in the reusing concern's interface. This effectively delays decision making about alternative functionality until the higher-level reuse context, where more detailed requirements are known and further decisions can be made. The paper describes the algorithms for composing the variation (i.e., feature and impact models), customization, and usage interfaces of a concern, as well as the concern's realization models and finally an entire concern hierarchy, as is necessary to support delayed decision making in CORE.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {93–103},
numpages = {11},
keywords = {Delaying of Decisions, Model Interfaces, Model Reuse, Model-Driven Engineering, Reuse Hierarchies},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@inproceedings{10.1145/3677996.3678289,
author = {Suckrow, Pierre-Louis Wolfgang L\'{e}on and Weber, Christoph Johannes and Rothe, Sylvia},
title = {Diffusion-Based Sound Synthesis in Music Production},
year = {2024},
isbn = {9798400710995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677996.3678289},
doi = {10.1145/3677996.3678289},
abstract = {In this paper, we explore the usability of generative artificial intelligence in music production through the development of a digital instrument that incorporates diffusion-based sound synthesis in its sound generation. Current text-to-audio models offer a novel method of defining sounds, which we aim to render utilizable in a music-production environment. Selected pretrained latent diffusion models, enable the synthesis of playable sounds through textual descriptions, which we incorporated into a digital instrument that integrates with standard music production tools. The resultant user interface not only allows generating but also modifying the sounds by editing model and instrument-specific parameters. We evaluated the applicability of current diffusion models with their parameters as well as the fitness of possible prompts for music production scenarios. Adapting published diffusion model pipelines for integration into the instrument, we facilitate experimentation and exploration of this innovative sound synthesis method. Our findings show that despite facing some limitations in the models' responsiveness to specific music production contexts and the instrument's functionality, the tool allows the development of novel and intriguing soundscapes. The instrument and code is published under https://github.com/suckrowPierre/WaveGenSynth},
booktitle = {Proceedings of the 12th ACM SIGPLAN International Workshop on Functional Art, Music, Modelling, and Design},
pages = {55–64},
numpages = {10},
keywords = {sound generation, text-to-sound, user interface, user study},
location = {Milan, Italy},
series = {FARM 2024}
}

@proceedings{10.1145/3555776,
title = {SAC '23: Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tallinn, Estonia}
}

@inproceedings{10.1145/3459637.3482467,
author = {Arcolezi, H\'{e}ber H. and Couchot, Jean-Fran\c{c}ois and Al Bouna, Bechara and Xiao, Xiaokui},
title = {Random Sampling Plus Fake Data: Multidimensional Frequency Estimates With Local Differential Privacy},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482467},
doi = {10.1145/3459637.3482467},
abstract = {With local differential privacy (LDP), users can privatize their data and thus guarantee privacy properties before transmitting it to the server (a.k.a. the aggregator). One primary objective of LDP is frequency (or histogram) estimation, in which the aggregator estimates the number of users for each possible value. In practice, when a study with rich content on a population is desired, the interest is in the multiple attributes of the population, that is to say, in multidimensional data (d ≥ 2). However, contrary to the problem of frequency estimation of a single attribute (the majority of the works), the multidimensional aspect imposes to pay particular attention to the privacy budget. This one can indeed grow extremely quickly due to the composition theorem. To the authors' knowledge, two solutions seem to stand out for this task: 1) splitting the privacy budget for each attribute, i.e., send each value with ε d ≥-LDP (Spl), and 2) random sampling a single attribute and spend all the privacy budget to send it with ε-LDP (Smp). AlthoughSmp adds additional sampling error, it has proven to provide higher data utility than the formerSpl solution. However, we argue that aggregators (who are also seen as attackers) are aware of the sampled attribute and its LDP value, which is protected by a "less strict" eε probability bound (rather than e^ε/d ). This way, we propose a solution named Random S ampling plus Fake Data (RS+FD), which allows creatinguncertainty over the sampled attribute by generating fake data for each non-sampled attribute; RS+FD further benefits from amplification by sampling. We theoretically and experimentally validate our proposed solution on both synthetic and real-world datasets to show that RS+FD achieves nearly the same or better utility than the state-of-the-artSmp solution.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {47–57},
numpages = {11},
keywords = {frequency estimation, local differential privacy, multidimensional data, sampling},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@proceedings{10.1145/3001867,
title = {FOSD 2016: Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.1145/1065010.1065048,
author = {Franchetti, Franz and Voronenko, Yevgen and P\"{u}schel, Markus},
title = {Formal loop merging for signal transforms},
year = {2005},
isbn = {1595930566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1065010.1065048},
doi = {10.1145/1065010.1065048},
abstract = {A critical optimization in the domain of linear signal transforms, such as the discrete Fourier transform (DFT), is loop merging, which increases data locality and reuse and thus performance. In particular, this includes the conversion of shuffle operations into array reindexings. To date, loop merging is well understood only for the DFT, and only for Cooley-Tukey FFT based algorithms, which excludes DFT sizes divisible by large primes. In this paper, we present a formal loop merging framework for general signal transforms and its implementation within the SPIRAL code generator. The framework consists of Ε-SPL, a mathematical language to express loops and index mappings; a rewriting system to merge loops in Ε-SPL and a compiler that translates Ε-SPL into code. We apply the framework to DFT sizes that cannot be handled using only the Cooley-Tukey FFT and compare our method to FFTW 3.0.1 and the vendor library Intel MKL 7.2.1. Compared to FFTW our generated code is a factor of 2--4 faster under equal implementation conditions (same algorithms, same unrolling threshold). For some sizes we show a speed-up of a factor of 9 using Bluestein's algorithm. Further, we give a detailed comparison against the Intel vendor library MKL; our generated code is between 2 times faster and 4.5 times slower.},
booktitle = {Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {315–326},
numpages = {12},
keywords = {DFT, automatic performance tuning, discrete Fourier transform, domain-specific language, linear signal transform, loop optimization},
location = {Chicago, IL, USA},
series = {PLDI '05}
}

@inproceedings{10.5555/2486788.2486851,
author = {Cordy, Maxime and Schobbens, Pierre-Yves and Heymans, Patrick and Legay, Axel},
title = {Beyond boolean product-line model checking: dealing with feature attributes and multi-features},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Model checking techniques for software product lines (SPL) are actively researched. A major limitation they currently have is the inability to deal efficiently with non-Boolean features and multi-features. An example of a non-Boolean feature is a numeric attribute such as maximum number of users which can take different numeric values across the range of SPL products. Multi-features are features that can appear several times in the same product, such as processing units which number is variable from one product to another and which can be configured independently. Both constructs are extensively used in practice but currently not supported by existing SPL model checking techniques. To overcome this limitation, we formally define a language that integrates these constructs with SPL behavioural specifications. We generalize SPL model checking algorithms correspondingly and evaluate their applicability. Our results show that the algorithms remain efficient despite the generalization.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {472–481},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1109/ASE51524.2021.9678761,
author = {Koc, Ugur and Mordahl, Austin and Wei, Shiyi and Foster, Jeffrey S. and Porter, Adam A.},
title = {SATune: a study-driven auto-tuning approach for configurable software verification tools},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678761},
doi = {10.1109/ASE51524.2021.9678761},
abstract = {Many program verification tools can be customized via run-time configuration options that trade off performance, precision, and soundness. However, in practice, users often run tools under their default configurations, because understanding these tradeoffs requires significant expertise. In this paper, we ask how well a single, default configuration can work in general, and we propose SATUNE, a novel tool for automatically configuring program verification tools for given target programs. To answer our question, we gathered a dataset that runs four well-known program verification tools against a range of C and Java benchmarks, with results labeled as correct, incorrect, or inconclusive (e.g., timeout). Examining the dataset, we find there is generally no one-size-fits-all best configuration. Moreover, a statistical analysis shows that many individual configuration options do not have simple tradeoffs: they can be better or worse depending on the program.Motivated by these results, we developed SATUNE, which constructs configurations using a meta-heuristic search. The search is guided by a surrogate fitness function trained on our dataset. We compare the performance of SATUNE to three baselines: a single configuration with the most correct results in our dataset; the most precise configuration followed by the most correct configuration (if needed); and the most precise configuration followed by random search (also if needed). We find that SATUNE outperforms these approaches by completing more correct tasks with high precision. In summary, our work shows that good configurations for verification tools are not simple to find, and SATUNE takes an important step towards automating the process of finding them.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {330–342},
numpages = {13},
keywords = {empirical software engineering, software analysis, testing, validation},
location = {Melbourne, Australia},
series = {ASE '21}
}

@inproceedings{10.1145/3664647.3681150,
author = {Zhang, Yue and Kordjamshidi, Parisa},
title = {Narrowing the Gap between Vision and Action in Navigation},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3681150},
doi = {10.1145/3664647.3681150},
abstract = {The existing methods for Vision and Language Navigation in the Continuous Environment (VLN-CE) commonly incorporate a waypoint predictor to discretize the environment. This simplifies the navigation actions into a view selection task and improves navigation performance significantly compared to direct training using low-level actions. However, the VLN-CE agents are still far from the real robots since there are gaps between their visual perception and executed actions. First, VLN-CE agents that discretize the visual environment are primarily trained with high-level view selection, which causes them to ignore crucial spatial reasoning within the low-level action movements. Second, in these models, the existing waypoint predictors neglect object semantics and their attributes related to passibility, which can be informative in indicating the feasibility of actions. To address these two issues, we introduce a low-level action decoder jointly trained with high-level action prediction, enabling the current VLN agent to learn and ground the selected visual view to the low-level controls. Moreover, we enhance the current waypoint predictor by utilizing visual representations containing rich semantic information and explicitly masking obstacles based on humans' prior knowledge about the feasibility of actions. Empirically, our agent can improve navigation performance metrics compared to the strong baselines on both high-level and low-level actions.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {856–865},
numpages = {10},
keywords = {embodied agent, vision and language, vision and language navigation in the continuous environment(vln-ce)},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@article{10.1007/s00165-021-00547-2,
author = {\v{C}e\v{s}ka, Milan and Hensel, Christian and Junges, Sebastian and Katoen, Joost-Pieter},
title = {Counterexample-guided inductive synthesis for probabilistic systems},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {4–5},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-021-00547-2},
doi = {10.1007/s00165-021-00547-2},
abstract = {This paper presents counterexample-guided inductive synthesis (CEGIS) to automatically synthesise probabilistic models. The starting point is a family of finite-stateMarkov chains with related but distinct topologies. Such families can succinctly be described by a sketch of a probabilistic program. Program sketches are programs containing holes. Every hole has a finite repertoire of possible program snippets by which it can be filled.We study several synthesis problems—feasibility, optimal synthesis, and complete partitioning—for a given quantitative specification φ. Feasibility amounts to determine a family member satisfying φ, optimal synthesis amounts to find a family member that maximises the probability to satisfy φ, and complete partitioning splits the family in satisfying and refuting members. Each of these problems can be considered under the additional constraint of minimising the total cost of instantiations, e.g., what are all possible instantiations for φ that are within a certain budget? The synthesis problems are tackled using a CEGIS approach. The crux is to aggressively prune the search space by using counterexamples provided by a probabilistic model checker. Counterexamples can be viewed as sub-Markov chains that rule out all family members that share this sub-chain. Our CEGIS approach leverages efficient probabilisticmodel checking,modern SMT solving, and programsnippets as counterexamples. Experiments on case studies froma diverse nature—controller synthesis, program sketching, and security—show that synthesis among up to a million candidate designs can be done using a few thousand verification queries.},
journal = {Form. Asp. Comput.},
month = aug,
pages = {637–667},
numpages = {31},
keywords = {Program Synthesis, Markov Chains, Probabilistic Model Checking, Counterexamples, CEGIS}
}

@inproceedings{10.1145/2601248.2601257,
author = {H\"{a}ser, Florian and Felderer, Michael and Breu, Ruth},
title = {Software paradigms, assessment types and non-functional requirements in model-based integration testing: a systematic literature review},
year = {2014},
isbn = {9781450324762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2601248.2601257},
doi = {10.1145/2601248.2601257},
abstract = {Context: In modern systems, like cyber-physical systems, where software and physical services are interacting, safety, security or performance play an important role. In order to guarantee the correct interoperability of such systems, with respect to functional and non-functional requirements, integration testing is an effective measure to achieve this. Model-based testing moreover not only enables early definition and validation, but also test automation. This makes it a good choice to overcome urgent challenges of integration testing. Objective: Many publications on model-based integration testing (MBIT) approaches can be found. Nevertheless, a study giving a systematic overview on the underlying software paradigms, measures for guiding the integration testing process as well as non-functional requirements they are suitable for, is missing. The aim of this paper is to find and synthesize the relevant primary studies to gain a comprehensive understanding of the current state of model-based integration testing. Method: For synthesizing the relevant studies, we conducted a systematic literature review (SLR) according to the guidelines of Kitchenham. Results: The systematic search and selection retrieved 83 relevant studies from which data has been extracted. Our review identified three assessment criteria for guiding the testing process, namely static metrics, dynamic metrics and stochastic &amp;random. In addition it shows that just a small fraction considers non-functional requirements. Most approaches are for component-oriented systems. Conclusion: Results from the SLR show that there are two major research gaps. First, there is an accumulated need for approaches in the MBIT field that support non-functional requirements, as they are gaining importance. Second, means for steering the integration testing process, especially together with automation, need to evolve.},
booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {29},
numpages = {10},
keywords = {assessment types, model-based integration testing, non-functional requirements, systematic literature review},
location = {London, England, United Kingdom},
series = {EASE '14}
}

@article{10.1109/TASLP.2023.3304491,
author = {Zhong, Jiaxin and Zhuang, Tao and Li, Mengtong and Kirby, Ray and Karimi, Mahmoud and Lu, Jing and Zhang, Dong},
title = {Sidelobe Suppression for a Steerable Parametric Source Using the Sparse Random Array Technique},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3304491},
doi = {10.1109/TASLP.2023.3304491},
abstract = {A steerable parametric source is designed to steer an audio beam without mechanically rotating the source. To achieve this without the generation of grating lobes requires an ultrasonic array with interelement spacing that is less than half the wavelength of the carrier ultrasound because of the spatial Nyquist criterion. However, ultrasonic wavelengths are typically smaller than the size of an ultrasonic transducer and this generates grating lobes in the radiation pattern, which is known as the spatial aliasing effect. This work proposes a method to suppress sidelobes including these grating lobes by optimizing the position and weight coefficients of the array elements by using a sparse random array technique. This is achieved by using the peak sidelobe level as the objective function and the simulated annealing algorithm for the optimization. Both simulation and experimental results demonstrate the sidelobe level can be effectively suppressed when the average interelement spacing is two wavelengths. It is also found that Westervelt directivity has a significant effect on the spatial aliasing, because it serves as a spatial filter on the product directivity of the ultrasound. Accordingly, the sidelobe suppression performance deteriorates at low audio frequencies and high ultrasound frequencies where Westervelt directivity tends to be broader. However, this deterioration in performance can be addressed by increasing the number of the array elements.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {3152–3161},
numpages = {10}
}

@inproceedings{10.1145/2212736.2212739,
author = {Takeuchi, Mikio and Makino, Yuki and Kawachiya, Kiyokuni and Horii, Hiroshi and Suzumura, Toyotaro and Suganuma, Toshio and Onodera, Tamiya},
title = {Compiling X10 to Java},
year = {2011},
isbn = {9781450307703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2212736.2212739},
doi = {10.1145/2212736.2212739},
abstract = {X10 is a new programming language for improving the software productivity in the multicore era by making parallel/distributed programming easier. X10 programs are compiled into C++ or Java source code, but X10 supports various features not supported directly in Java. To implement them efficiently in Java, new compilation techniques are needed.This paper discusses problems in translating X10-specific functions to Java and provides our solutions. By using appropriate implementations, sequential execution performance has been improved by about 5 times making it comparable to native Java. The parallel execution performance has also been improved and the gap from Java Fork/Join performance is about 3 times when run at a single place. Initial evaluation of distributed execution shows good scalability. Most of the results in this paper have already been incorporated in X10 release 2.1.2.Many of the compilation techniques described in this paper can be useful for implementing other programming languages targeted for Java or other managed environments.},
booktitle = {Proceedings of the 2011 ACM SIGPLAN X10 Workshop},
articleno = {3},
numpages = {10},
keywords = {Java, X10, code generation, evaluation, optimization},
location = {San Jose, California},
series = {X10 '11}
}

@inproceedings{10.1145/3243734.3243739,
author = {Ispoglou, Kyriakos K. and AlBassam, Bader and Jaeger, Trent and Payer, Mathias},
title = {Block Oriented Programming: Automating Data-Only Attacks},
year = {2018},
isbn = {9781450356930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243734.3243739},
doi = {10.1145/3243734.3243739},
abstract = {With the widespread deployment of Control-Flow Integrity (CFI), control-flow hijacking attacks, and consequently code reuse attacks, are significantly more difficult. CFI limits control flow to well-known locations, severely restricting arbitrary code execution. Assessing the remaining attack surface of an application under advanced control-flow hijack defenses such as CFI and shadow stacks remains an open problem. We introduce BOPC, a mechanism to automatically assess whether an attacker can execute arbitrary code on a binary hardened with CFI/shadow stack defenses. BOPC computes exploits for a target program from payload specifications written in a Turing-complete, high-level language called SPL that abstracts away architecture and program-specific details. SPL payloads are compiled into a program trace that executes the desired behavior on top of the target binary. The input for BOPC is an SPL payload, a starting point (e.g., from a fuzzer crash) and an arbitrary memory write primitive that allows application state corruption. To map SPL payloads to a program trace, BOPC introduces Block Oriented Programming (BOP), a new code reuse technique that utilizes entire basic blocks as gadgets along valid execution paths in the program, i.e., without violating CFI or shadow stack policies. We find that the problem of mapping payloads to program traces is NP-hard, so BOPC first reduces the search space by pruning infeasible paths and then uses heuristics to guide the search to probable paths. BOPC encodes the BOP payload as a set of memory writes. We execute 13 SPL payloads applied to 10 popular applications. BOPC successfully finds payloads and complex execution traces -- which would likely not have been found through manual analysis -- while following the target's Control-Flow Graph under an ideal CFI policy in 81% of the cases.},
booktitle = {Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1868–1882},
numpages = {15},
keywords = {binary analysis, block oriented programming, data only attacks, exploitation, program synthesis},
location = {Toronto, Canada},
series = {CCS '18}
}

@inproceedings{10.1145/337180.337455,
author = {Gannod, Gerald C. and Lutz, Robyn R.},
title = {An approach to architectural analysis of product lines},
year = {2000},
isbn = {1581132069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/337180.337455},
doi = {10.1145/337180.337455},
abstract = {This paper addresses the issue of how to perform architectural analysis on an existing product line architecture. The con tribution of the paper is to identify and demonstrate a repeatable product line architecture analysis process. The approach defines a “good” product line architecture in terms of those quality attributes required by the particular product line under development. It then analyzes the architecture against these criteria by both manual and tool-supported methods. The phased approach described in this paper provides a structured analysis of an existing product line architecture using (1) formal specification of the high-level architecture, (2) manual analysis of scenarios to exercise the architecture's support for required variabilities, and (3) model checking of critical behaviors at the architectural level that are required for all systems in the product line. Results of an application to a software product line of spaceborne telescopes are used to explain and evaluate the approach.},
booktitle = {Proceedings of the 22nd International Conference on Software Engineering},
pages = {548–557},
numpages = {10},
keywords = {interferometry software, product lines, software architecture analysis, software archtecture},
location = {Limerick, Ireland},
series = {ICSE '00}
}

@inproceedings{10.1145/3636534.3690663,
author = {Mollahosseini, Poorya and Afzal, Sayed Saad and Adib, Fadel and Ghasempour, Yasaman},
title = {SURF: Eavesdropping on Underwater Communications from the Air},
year = {2024},
isbn = {9798400704895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636534.3690663},
doi = {10.1145/3636534.3690663},
abstract = {This paper investigates how an airborne node can eavesdrop on the underwater acoustic communication between submerged nodes. Conventionally, such eavesdropping has been assumed impossible as acoustic signals do not cross the water-air boundary. Here, we demonstrate that underwater acoustic communications signals can be picked up and (under certain conditions) decoded using an airborne mmWave radar due to the minute vibrations induced by the communication signals on the water surface. We implemented and evaluated a proof-of-concept prototype of our method and tested it in controlled (pool) and uncontrolled environments (lake). Our results demonstrate that an airborne device can identify the modulation and bitrate of acoustic transmissions from an uncooperative underwater transmitter (victim), and even decode the transmitted symbols. Unlike conventional over-the-air communications, our results indicate that the secrecy of underwater links varies depending on the modulation type and provide insights into the underlying reasons behind these differences. We also highlight the theoretical limitations of such a threat model, and how these results may have a significant impact on the stealthiness of underwater communications, with particular concern to submarine warfare, underwater operations (e.g., oil &amp; gas, search &amp; rescue, mining), and conservation of endangered species. Finally, our investigation uncovers countermeasures that can be used to improve or restore the stealthiness of underwater acoustic communications against such threats.},
booktitle = {Proceedings of the 30th Annual International Conference on Mobile Computing and Networking},
pages = {815–829},
numpages = {15},
keywords = {wireless, cross-medium communications, security, subsea internet of things},
location = {Washington D.C., DC, USA},
series = {ACM MobiCom '24}
}

@inproceedings{10.1145/2866614.2866628,
author = {Th\"{u}m, Thomas and Winkelmann, Tim and Schr\"{o}ter, Reimar and Hentschel, Martin and Kr\"{u}ger, Stefan},
title = {Variability Hiding in Contracts for Dependent Software Product Lines},
year = {2016},
isbn = {9781450340199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2866614.2866628},
doi = {10.1145/2866614.2866628},
abstract = {Software product lines are used to efficiently develop and verify similar software products. While they focus on reuse of artifacts between products, a product line may also be reused itself in other product lines. A challenge with such dependent product lines is evolution; every change in a product line may influence all dependent product lines. With variability hiding, we aim to hide certain features and their artifacts in dependent product lines. In prior work, we focused on feature models and implementation artifacts. We build on this by discussing how variability hiding can be extended to specifications in terms of method contracts. We illustrate variability hiding in contracts by means of a running example and share our insights with preliminary experiments on the benefits for formal verification. In particular, we find that not every change in a certain product line requires a re-verification of other dependent product lines.},
booktitle = {Proceedings of the 10th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Multi product line, deductive verification, method contracts},
location = {Salvador, Brazil},
series = {VaMoS '16}
}

@inproceedings{10.1145/2162049.2162052,
author = {Brabrand, Claus and Ribeiro, M\'{a}rcio and Tol\^{e}do, T\'{a}rsis and Borba, Paulo},
title = {Intraprocedural dataflow analysis for software product lines},
year = {2012},
isbn = {9781450310925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2162049.2162052},
doi = {10.1145/2162049.2162052},
abstract = {Software product lines (SPLs) are commonly developed using annotative approaches such as conditional compilation that come with an inherent risk of constructing erroneous products. For this reason, it is essential to be able to analyze SPLs. However, as dataflow analysis techniques are not able to deal with SPLs, developers must generate and analyze all valid methods individually, which is expensive for non-trivial SPLs. In this paper, we demonstrate how to take any standard intraprocedural dataflow analysis and automatically turn it into a feature-sensitive dataflow analysis in three different ways. All are capable of analyzing all valid methods of an SPL without having to generate all of them explicitly. We have implemented all analyses as extensions of SOOT's intraprocedural dataflow analysis framework and experimentally evaluated their performance and memory characteristics on four qualitatively different SPLs. The results indicate that the feature-sensitive analyses are on average 5.6 times faster than the brute force approach on our SPLs, and that they have different time and space tradeoffs.},
booktitle = {Proceedings of the 11th Annual International Conference on Aspect-Oriented Software Development},
pages = {13–24},
numpages = {12},
keywords = {dataflow analysis, software product lines},
location = {Potsdam, Germany},
series = {AOSD '12}
}

@proceedings{10.5555/3643142,
title = {WSC '23: Proceedings of the Winter Simulation Conference},
year = {2023},
isbn = {9798350369663},
publisher = {IEEE Press},
location = {San Antonio, Texas, USA}
}

@inproceedings{10.1145/3678717.3691216,
author = {Yin, Du and Deng, Jinliang and Ao, Shuang and Li, Zechen and Xue, Hao and Prabowo, Arian and Jiang, Renhe and Song, Xuan and Salim, Flora},
title = {Enhancing Spatio-temporal Quantile Forecasting with Curriculum Learning: Lessons Learned},
year = {2024},
isbn = {9798400711077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678717.3691216},
doi = {10.1145/3678717.3691216},
abstract = {Training models on spatio-temporal (ST) data poses an open problem due to the complicated and diverse nature of the data itself, and it is challenging to ensure the model's performance directly trained on the original ST data. While limiting the variety of training data can make training easier, it can also lead to a lack of knowledge and information for the model, resulting in a decrease in performance. To address this challenge, we presented an innovative paradigm that incorporates three separate forms of curriculum learning specifically targeting from spatial, temporal, and quantile perspectives. Furthermore, our framework incorporates a stacking fusion module to combine diverse information from three types of curriculum learning, resulting in a strong and thorough learning process. We demonstrated the effectiveness of this framework with extensive empirical evaluations, highlighting its better performance in addressing complex ST challenges. We provided thorough ablation studies to investigate the effectiveness of our curriculum and to explain how it contributes to the improvement of learning efficiency on ST data.},
booktitle = {Proceedings of the 32nd ACM International Conference on Advances in Geographic Information Systems},
pages = {42–53},
numpages = {12},
keywords = {Curriculum learning, Deep learning, Quantile forecasting, Spatio-temporal},
location = {Atlanta, GA, USA},
series = {SIGSPATIAL '24}
}

@inproceedings{10.1145/3472456.3472521,
author = {Lehr, Jan-Patrick and Bischof, Christian and Dewald, Florian and Mantel, Heiko and Norouzi, Mohammad and Wolf, Felix},
title = {Tool-Supported Mini-App Extraction to Facilitate Program Analysis and Parallelization},
year = {2021},
isbn = {9781450390682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472456.3472521},
doi = {10.1145/3472456.3472521},
abstract = {The size and complexity of high-performance computing applications present a serious challenge to manual reasoning about program behavior. The vastness and diversity of code bases often break automatic analysis tools, which could otherwise be used. As a consequence, developers resort to mini-apps, i.e., trimmed-down proxies of the original programs that retain key performance characteristics. Unfortunately, their construction is difficult and time consuming and prevents their mass production. In this paper, we propose a systematic and tool-supported approach to extract mini-apps from large-scale applications that reduces the manual effort needed to create them. Our approach covers the stages kernel identification, data capture, code extraction and representativeness validation. We demonstrate it using an astrophysics simulation with ≈ 8.5 million lines of code and extract a mini-app with only ≈ 1, 100 lines of code. For the mini-app, we evaluate the reduction of code complexity and execution similarity, and show how it enables the tool-supported discovery of unexploited parallelization opportunities, reducing the simulation’s runtime significantly.},
booktitle = {Proceedings of the 50th International Conference on Parallel Processing},
articleno = {35},
numpages = {10},
location = {Lemont, IL, USA},
series = {ICPP '21}
}

@article{10.1145/3636512,
author = {Zhu, Weiyao and Wu, Ou and Su, Fengguang and Deng, Yingjun},
title = {Exploring the Learning Difficulty of Data: Theory and Measure},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3636512},
doi = {10.1145/3636512},
abstract = {‘‘Easy/hard sample” is a popular parlance in machine learning. Learning difficulty of samples refers to how easy/hard a sample is during a learning procedure. An increasing need of measuring learning difficulty demonstrates its importance in machine learning (e.g., difficulty-based weighting learning strategies). Previous literature has proposed a number of learning difficulty measures. However, no comprehensive investigation for learning difficulty is available to date, resulting in that nearly all existing measures are heuristically defined without a rigorous theoretical foundation. This study attempts to conduct a pilot theoretical study for learning difficulty of samples. First, influential factors for learning difficulty are summarized. Under various situations conducted by summarized influential factors, correlations between learning difficulty and two vital criteria of machine learning, namely, generalization error and model complexity, are revealed. Second, a theoretical definition of learning difficulty is proposed on the basis of these two criteria. A practical measure of learning difficulty is proposed under the direction of the theoretical definition by importing the bias-variance trade-off theory. Subsequently, the rationality of theoretical definition and the practical measure are demonstrated, respectively, by analysis of several classical weighting methods and abundant experiments realized under all situations conducted by summarized influential factors. The mentioned weighting methods can be reasonably explained under the proposed theoretical definition and concerned propositions. The comparison in these experiments indicates that the proposed measure significantly outperforms the other measures throughout the experiments.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {84},
numpages = {37},
keywords = {Learning difficulty, generalization error, bias-variance trade-off, model complexity}
}

@inproceedings{10.1145/2755644.2755647,
author = {Duplyakin, Dmitry and Haney, Matthew and Tufo, Henry},
title = {Architecting a Persistent and Reliable Configuration Management System},
year = {2015},
isbn = {9781450335706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2755644.2755647},
doi = {10.1145/2755644.2755647},
abstract = {Streamlined configuration management plays a significant role in modern, complex distributed systems. Via mechanisms that promote consistency, repeatability, and transparency, configuration management systems (CMSes) address complexity and aim to increase the efficiency of administrative procedures, including deployment and failure recovery scenarios. Considering the importance of minimizing disruptions in these systems, we design an architecture that increases persistency and reliability of infrastructure management. We present our architecture in the context of hybrid, cluster-cloud environments and describe our highly available implementation that builds upon the open source CMS called Chef and infrastructure-as-a-service cloud resources from Amazon Web Services. We demonstrate how we enabled a smooth transition from the pre-existing single-server configuration to the proposed highly available management system. We summarize our experience with managing a 20-node Linux cluster using this implementation. Our analysis of utilization and cost of necessary cloud resources indicates that the designed system is a low-cost alternative to acquiring additional physical hardware for hardening cluster management. We also highlight the prototype's security and manageability features that are suitable for larger, production-ready deployments.},
booktitle = {Proceedings of the 6th Workshop on Scientific Cloud Computing},
pages = {11–16},
numpages = {6},
keywords = {automated infrastructure management, cloud computing, computing cluster, configuration management system, high availability, hybrid systems},
location = {Portland, Oregon, USA},
series = {ScienceCloud '15}
}

@article{10.1109/TASLP.2024.3473315,
author = {Westhausen, Nils L. and Kayser, Hendrik and Jansen, Theresa and Meyer, Bernd T.},
title = {Real-Time Multichannel Deep Speech Enhancement in Hearing Aids: Comparing Monaural and Binaural Processing in Complex Acoustic Scenarios},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3473315},
doi = {10.1109/TASLP.2024.3473315},
abstract = {Deep learning has the potential to enhance speech signals and increase their intelligibility for users of hearing aids. Deep models suited for real-world application should feature a low computational complexity and low processing delay of only a few milliseconds. In this paper, we explore deep speech enhancement that matches these requirements and contrast monaural and binaural processing algorithms in two complex acoustic scenes. Both algorithms are evaluated with objective metrics and in experiments with hearing-impaired listeners performing a speech-in-noise test. Results are compared to two traditional enhancement strategies, i.e., adaptive differential microphone processing and binaural beamforming. While in diffuse noise, all algorithms perform similarly, the binaural deep learning approach performs best in the presence of spatial interferers. Through a post-analysis, this can be attributed to improvements at low SNRs and to precise spatial filtering.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {4596–4606},
numpages = {11}
}

@proceedings{10.1145/3698038,
title = {SoCC '24: Proceedings of the 2024 ACM Symposium on Cloud Computing},
year = {2024},
isbn = {9798400712869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Redmond, WA, USA}
}

@inproceedings{10.1145/2556624.2556627,
author = {B\"{u}rdek, Johannes and Lity, Sascha and Lochau, Malte and Berens, Markus and Goltz, Ursula and Sch\"{u}rr, Andy},
title = {Staged configuration of dynamic software product lines with complex binding time constraints},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556627},
doi = {10.1145/2556624.2556627},
abstract = {Dynamic software product lines (DSPL) constitute a promising approach for developing highly-configurable, runtime-adaptive systems in a feature-oriented way. A DSPL integrates both variability in time and space in a unified conceptual framework. For this, domain features are equipped with additional binding time information to distinguish between static configuration parameters and dynamically (re-) configurable features. Until now, little support exists to specify and validate staged (re-)configuration semantics for DSPLs in a concise way. In this paper, we propose conservative extensions to domain feature models comprising variable feature binding times together with different kinds of binding time constraints. Those extensions are motivated by a real-world industrial case study from the automation engineering domain. Our implementation performs a model transformation into plain feature models treatable by corresponding state-of-the-art analysis tools. We conducted an evaluation of our approach concerning the case study.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {16},
numpages = {8},
keywords = {dynamic software product lines, extended feature models, industrial case study, staged configuration},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@article{10.1145/3678594,
author = {Han, Feiyu and Yang, Panlong and Zuo, You and Shang, Fei and Xu, Fenglei and Li, Xiang-Yang},
title = {EarSpeech: Exploring In-Ear Occlusion Effect on Earphones for Data-efficient Airborne Speech Enhancement},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
url = {https://doi.org/10.1145/3678594},
doi = {10.1145/3678594},
abstract = {Earphones have become a popular voice input and interaction device. However, airborne speech is susceptible to ambient noise, making it necessary to improve the quality and intelligibility of speech on earphones in noisy conditions. As the dual-microphone structure (i.e., outer and in-ear microphones) has been widely adopted in earphones (especially ANC earphones), we design EarSpeech which exploits in-ear acoustic sensory as the complementary modality to enable airborne speech enhancement. The key idea of EarSpeech is that in-ear speech is less sensitive to ambient noise and exhibits a correlation with airborne speech. However, due to the occlusion effect, in-ear speech has limited bandwidth, making it challenging to directly correlate with full-band airborne speech. Therefore, we exploit the occlusion effect to carry out theoretical modeling and quantitative analysis of this cross-channel correlation and study how to leverage such cross-channel correlation for speech enhancement. Specifically, we design a series of methodologies including data augmentation, deep learning-based fusion, and noise mixture scheme, to improve the generalization, effectiveness, and robustness of EarSpeech, respectively. Lastly, we conduct real-world experiments to evaluate the performance of our system. Specifically, EarSpeech achieves an average improvement ratio of 27.23% and 13.92% in terms of PESQ and STOI, respectively, and significantly improves SI-SDR by 8.91 dB. Benefiting from data augmentation, EarSpeech can achieve comparable performance with a small-scale dataset that is 40 times less than the original dataset. In addition, we validate the generalization of different users, speech content, and language types, respectively, as well as robustness in the real world via comprehensive experiments. The audio demo of EarSpeech is available on https://github.com/EarSpeech/earspeech.github.io/.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {104},
numpages = {30},
keywords = {Earphone-based Sensing and Computing, In-ear Acoustic Sensing, Occlusion Effect, Speech Enhancement}
}

@article{10.1145/3639313,
author = {Baziotis, Stefanos and Kang, Daniel and Mendis, Charith},
title = {Dias: Dynamic Rewriting of Pandas Code},
year = {2024},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3639313},
doi = {10.1145/3639313},
abstract = {In recent years, dataframe libraries, such as pandas have exploded in popularity. Due to their flexibility, they are increasingly used in ad-hoc exploratory data analysis (EDA) workloads. These workloads are diverse, including custom functions which can span libraries or be written in pure Python. The majority of systems available to accelerate EDA workloads focus on bulk-parallel workloads, which contain vastly different computational patterns, typically within a single library. As a result, they can introduce excessive overheads for ad-hoc EDA workloads due to their expensive optimization techniques. Instead, we identify source-to-source, external program rewriting as a lightweight technique which can optimize across representations, and offer substantial speedups while also avoiding slowdowns. We implemented Dias, which rewrites notebook cells to be more efficient for ad-hoc EDA workloads. We develop techniques for efficient rewrites in Dias, including checking the preconditions under which rewrites are correct, dynamically, at fine-grained program points. We show that Dias can rewrite individual cells to be 57\texttimes{} faster compared to pandas and 1909\texttimes{} faster compared to optimized systems such as modin. Furthermore, Dias can accelerate whole notebooks by up to 3.6\texttimes{} compared to pandas and 27.1\texttimes{} compared to modin.},
journal = {Proc. ACM Manag. Data},
month = mar,
articleno = {58},
numpages = {27},
keywords = {cross-representation, dynamic, pandas, rewriting}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00012,
author = {Nuryyev, Batyr and Nadi, Sarah and Bhuiyan, Nazim Uddin and Banderali, Leonardo},
title = {Challenges of implementing software variability in eclipse OMR: an interview study},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00012},
doi = {10.1109/ICSE-SEIP52600.2021.00012},
abstract = {Software variability is the ability of a software system to be customized or configured for a particular context. In this paper, we discuss our experience investigating software variability implementation challenges in practice. Eclipse OMR, developed by IBM, is a set of highly configurable C++ components for building language runtimes; it supports multiple programming languages and target architectures. We conduct an interview study with 6 Eclipse OMR developers and identify 8 challenges incurred by the existing variability implementation, and 3 constraints that need to be taken into account for any reengineering effort. We discuss these challenges and investigate the literature and existing open-source systems for potential solutions. We contribute a solution for one of the challenges, namely adding variability to enumerations and arrays. We also share our experiences and lessons learned working with a large-scale highly configurable industry project. For example, we found that the "latest and greatest" research solutions may not always be favoured by developers due to small practical considerations such as build dependencies, or even C++ version constraints.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {31–40},
numpages = {10},
keywords = {eclipse OMR, language runtimes, software variability, variability implementation},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1145/3417990.3421263,
author = {Pett, Tobias and Eichhorn, Domenik and Schaefer, Ina},
title = {Risk-based compatibility analysis in automotive systems engineering},
year = {2020},
isbn = {9781450381352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417990.3421263},
doi = {10.1145/3417990.3421263},
abstract = {Software is the new leading factor for innovation in the automotive industry. With the increase of software in road vehicles new business models, such as after-sale updates (i.e., Function-on-Demand) and Over-the-Air-Updates come into focus of manufacturers. When updating a road vehicle in the field, it is required to ensure functional safety. An update shall not influence existing functionality and break its safety. Hence, it must be compatible with the existing software. The compatibility of an update is ensured by testing. However, testing all variants of a highly configurable system, such as a modern car's software, is infeasible, due to the combinatorial explosion. To address this problem, in this paper, we propose a risk-based change-impact analysis to identify system variants relevant for retesting after an update. We combine existing concepts from product sampling, risk-based testing, and configuration prioritization and apply them to automotive architectures. For validating our concept, we use the Body Comfort System case study from the automotive industry. Our evaluation reveals that the concept backed by tool support may reduce testing effort by identifying and prioritizing incompatible variants wrt to a system update.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
articleno = {34},
numpages = {10},
keywords = {automotive engineering, configurable systems, risk-based analysis},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@article{10.1145/3437479.3437485,
author = {Yoo, Shin and Aleti, Aldeida and Turhan, Burak and Minku, Leandro L. and Miranskyy, Andriy and Meri\c{c}li, \c{C}etin},
title = {The 8th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3437479.3437485},
doi = {10.1145/3437479.3437485},
abstract = {The International Workshop on Realizing Arti cial Intelligence Synergies in Software Engineering (RAISE) aims to present the state of the art in the crossover between Software Engineering and Arti cial Intelligence. This workshop explored not only the appli- cation of AI techniques to SE problems but also the application of SE techniques to AI problems. Software has become critical for realizing functions central to our society. For example, software is essential for nancial and transport systems, energy generation and distribution systems, and safety-critical medical applications. Software development costs trillions of dollars each year yet, still, many of our software engineering methods remain mostly man- ual. If we can improve software production by smarter AI-based methods, even by small margins, then this would improve a crit- ical component of the international infrastructure, while freeing up tens of billions of dollars for other tasks.},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {23–24},
numpages = {2}
}

@inbook{10.1145/3544585.3544596,
author = {Lengauer, Christian},
title = {A Personal View of Edsger W. Dijkstra and His Stance on Software Construction},
year = {2022},
isbn = {9781450397735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3544585.3544596},
booktitle = {Edsger Wybe Dijkstra: His Life,Work, and Legacy},
pages = {205–214},
numpages = {10}
}

@proceedings{10.1145/3643991,
title = {MSR '24: Proceedings of the 21st International Conference on Mining Software Repositories},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {MSR is a thriving research community that organizes a yearly conference with a solid reputation amongst software engineering researchers.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3658644,
title = {CCS '24: Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is with great enthusiasm that we, on behalf of the Organizing Committee, invite you to join us for the 31st ACM SIGSAC Conference on Computer and Communications Security (CCS), a premier security and privacy conference where researchers, practitioners, and educators come together to present, learn, and debate research, innovation, and trends in the field of Computer and Communications Security and Privacy.This year, we are proud to introduce our conference theme to be "Inclusion, Mentorship, Community." These three pillars reflect our collective commitment to fostering a vibrant, supportive, and forwardthinking environment within the CCS community. Particularly, we host our inaugural Doctoral Symposium, which offers PhD students a unique platform to receive timely, constructive feedback on their dissertation research from leading experts in our community. Additionally, our first-ever Diversity, Equity, and Inclusion (DEI) Workshop is designed to cultivate a culture that embraces diversity and champions equity in our field. Moreover, understanding the importance of guidance and support, we have organized panels focusing on Student Mentoring, Faculty Mentoring, and Public Service. These panels are designed to facilitate mentorship connections, share valuable experiences, and encourage service that extends the impact of our work beyond academia. These new initiatives are also opportunities to strengthen the bonds within our CCS community.Regarding the main conference, this year's main conference is our largest ever, featuring 328 paper presentations that showcase the latest research and developments in our field. We are also honored to have two distinguished keynote speakers: Dr. Dan Boneh and Dr. Gene Tsudik, who will share their invaluable insights and perspectives on pressing topics in security and privacy. Additionally, 18 specialized workshops will take place on the pre-conference and post-conference days, providing platforms for focused discussions and collaborations on numerous specialized topics.},
location = {Salt Lake City, UT, USA}
}

@proceedings{10.5555/3606013,
title = {ICSE '23: Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Victoria, Australia}
}

@article{10.1109/TASLP.2018.2798804,
author = {Donley, Jacob and Ritz, Christian and Kleijn, W. Bastiaan},
title = {Multizone Soundfield Reproduction With Privacy- and Quality-Based Speech Masking Filters},
year = {2018},
issue_date = {June 2018},
publisher = {IEEE Press},
volume = {26},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2798804},
doi = {10.1109/TASLP.2018.2798804},
abstract = {Reproducing zones of personal sound is a challenging signal processing problem that has garnered considerable research interest in recent years. We introduce in this work an extended method to multizone soundfield reproduction that overcomes issues with speech privacy and quality. Measures of speech intelligibility contrast SIC and speech quality are used as cost functions in an optimization of speech privacy and quality. Novel spatial and temporal frequency domain speech masker filter designs are proposed to accompany the optimization process. Spatial masking filters are designed using multizone soundfield algorithms that are dependent on the target speech multizone reproduction. Combinations of estimates of acoustic contrast and long term average speech spectra are proposed to provide equal masking influence on speech privacy and quality. Spatial aliasing specific to multizone soundfield reproduction geometry is further considered in analytically derived low-pass filters. Simulated and real-world experiments are conducted to verify the performance of the proposed method using semi-circular and linear loudspeaker arrays. Simulated implementations of the proposed method show that significant SIC and speech quality is achievable between zones. A range of perceptual evaluation of speech quality mean opinion scores that indicate good quality are obtained while at the same time providing confidential privacy as indicated by SIC. The simulations also show that the method is robust to variations in the speech, virtual source location, array geometry, and number of loudspeakers. Real-world experiments confirm the practicality of the proposed methods by showing that good quality and confidential privacy are achievable.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1037–1051},
numpages = {15}
}

@inproceedings{10.1145/3664647.3681526,
author = {Zhao, Haochen and Meng, Hui and Yang, Deqian and Xie, Xiaozheng and Wu, Xiaoze and Li, Qingfeng and Niu, Jianwei},
title = {GuidedNet: Semi-Supervised Multi-Organ Segmentation via Labeled Data Guide Unlabeled Data},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3681526},
doi = {10.1145/3664647.3681526},
abstract = {Semi-supervised multi-organ medical image segmentation aids physicians in improving disease diagnosis and treatment planning and reduces the time and effort required for organ annotation.Existing state-of-the-art methods train the labeled data with ground truths and train the unlabeled data with pseudo-labels. However, the two training flows are separate, which does not reflect the interrelationship between labeled and unlabeled data.To address this issue, we propose a semi-supervised multi-organ segmentation method called GuidedNet, which leverages the knowledge from labeled data to guide the training of unlabeled data. The primary goals of this study are to improve the quality of pseudo-labels for unlabeled data and to enhance the network's learning capability for both small and complex organs.A key concept is that voxel features from labeled and unlabeled data that are close to each other in the feature space are more likely to belong to the same class.On this basis, a 3D Consistent Gaussian Mixture Model (3D-CGMM) is designed to leverage the feature distributions from labeled data to rectify the generated pseudo-labels.Furthermore, we introduce a Knowledge Transfer Cross Pseudo Supervision (KT-CPS) strategy, which leverages the prior knowledge obtained from the labeled data to guide the training of the unlabeled data, thereby improving the segmentation accuracy for both small and complex organs. Extensive experiments on two public datasets, FLARE22 and AMOS, demonstrated that GuidedNet is capable of achieving state-of-the-art performance.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {886–895},
numpages = {10},
keywords = {3d medical image segmentation, abdominal organs, gaussian mixture model, semi-supervised learning},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@article{10.5555/3122009.3176858,
author = {Fenn, Shannon and Moscato, Pablo},
title = {Target curricula via selection of minimum feature sets: a case study in Boolean networks},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We consider the effect of introducing a curriculum of targets when training Boolean models on supervised Multi Label Classification (MLC) problems. In particular, we consider how to order targets in the absence of prior knowledge, and how such a curriculum may be enforced when using meta-heuristics to train discrete non-linear models.We show that hierarchical dependencies between targets can be exploited by enforcing an appropriate curriculum using hierarchical loss functions. On several multi-output circuit-inference problems with known target difficulties, Feedforward Boolean Networks (FBNs) trained with such a loss function achieve significantly lower out-of-sample error, up to 10% in some cases. This improvement increases as the loss places more emphasis on target order and is strongly correlated with an easy-to-hard curricula. We also demonstrate the same improvements on three real-world models and two Gene Regulatory Network (GRN) inference problems.We posit a simple a-priori method for identifying an appropriate target order and estimating the strength of target relationships in Boolean MLCs. These methods use intrinsic dimension as a proxy for target difficulty, which is estimated using optimal solutions to a combinatorial optimisation problem known as the Minimum-Feature-Set (minFS) problem. We also demonstrate that the same generalisation gains can be achieved without providing any knowledge of target difficulty.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {4070–4095},
numpages = {26},
keywords = {Boolean betworks, k-feature Set, multi-label classification, target curriculum}
}

@inproceedings{10.5555/3400397.3400543,
author = {Ghorpade, Tejas and Rangaraj, Narayan},
title = {Rolling horizon models for inter-depot empty container repositioning},
year = {2020},
isbn = {9781728132839},
publisher = {IEEE Press},
abstract = {Export customers requesting empty containers in the hinterland areas are serviced by maintaining sufficient inventory at each regional depot. The supply-demand imbalance at the regional level is stabilized by repositioning empty containers between inland depots. We propose an inter-depot empty container repositioning problem and a heuristic real-time decision algorithm to solve it. Initially, single-period travel time is considered and three models: Allocation Problem (AP), Value Approximation Model (SPL-VA), and Node Decomposition Heuristic (NDH-SP) are presented. The system is simulated over a certain time horizon by generating real-time supply and demand values, and the system's evolution is studied under each of the proposed models. The VA models perform better than the AP with modest computational effort. The NDH-SP is further generalized to accommodate multi-period travel times. By simulating this algorithm with a demand rejection policy, we observe that maximum demand satisfaction is obtained by allowing medium-sized demand queues at the depots.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1825–1836},
numpages = {12},
location = {National Harbor, Maryland},
series = {WSC '19}
}

@proceedings{10.1145/3603287,
title = {ACMSE '24: Proceedings of the 2024 ACM Southeast Conference},
year = {2024},
isbn = {9798400702372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome you to the 2024 ACM Southeast Conference (ACMSE 2024) sponsored by ACM and the College of Computing and Software Engineering (CCSE) at Kennesaw State University, Marietta, Georgia, USA. ACMSE 2024 continues the ACM Southeast Conference tradition of participation in all areas of computing disciplines. We hope this conference will be an excellent opportunity to share current and future hot research trends amongst researchers from around the world.},
location = {Marietta, GA, USA}
}

@inproceedings{10.1109/ICSE43902.2021.00028,
author = {Gao, Yanjie and Zhu, Yonghao and Zhang, Hongyu and Lin, Haoxiang and Yang, Mao},
title = {Resource-Guided Configuration Space Reduction for Deep Learning Models},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00028},
doi = {10.1109/ICSE43902.2021.00028},
abstract = {Deep learning models, like traditional software systems, provide a large number of configuration options. A deep learning model can be configured with different hyperparameters and neural architectures. Recently, AutoML (Automated Machine Learning) has been widely adopted to automate model training by systematically exploring diverse configurations. However, current AutoML approaches do not take into consideration the computational constraints imposed by various resources such as available memory, computing power of devices, or execution time. The training with non-conforming configurations could lead to many failed AutoML trial jobs or inappropriate models, which cause significant resource waste and severely slow down development productivity.In this paper, we propose DnnSAT, a resource-guided AutoML approach for deep learning models to help existing AutoML tools efficiently reduce the configuration space ahead of time. DnnSAT can speed up the search process and achieve equal or even better model learning performance because it excludes trial jobs not satisfying the constraints and saves resources for more trials. We formulate the resource-guided configuration space reduction as a constraint satisfaction problem. DnnSAT includes a unified analytic cost model to construct common constraints with respect to the model weight size, number of floating-point operations, model inference time, and GPU memory consumption. It then utilizes an SMT solver to obtain the satisfiable configurations of hyperparameters and neural architectures. Our evaluation results demonstrate the effectiveness of DnnSAT in accelerating state-of-the-art AutoML methods (Hyperparameter Optimization and Neural Architecture Search) with an average speedup from 1.19X to 3.95X on public benchmarks. We believe that DnnSAT can make AutoML more practical in a real-world environment with constrained resources.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {175–187},
numpages = {13},
keywords = {AutoML, configurable systems, constraint solving, deep learning},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3613904.3642526,
author = {Seixas Pereira, Let\'{\i}cia and Matos, Maria and Duarte, Carlos},
title = {Exploring Mobile Device Accessibility: Challenges, Insights, and Recommendations for Evaluation Methodologies},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642526},
doi = {10.1145/3613904.3642526},
abstract = {With the ubiquitous use of mobile applications, it is paramount that they are accessible, so they can empower all users, including those with different needs. Determining if an app is accessible implies conducting an accessibility evaluation. While accessibility evaluations have been thoroughly studied in the web domain, there are still many open questions when evaluating mobile applications. This paper investigates mobile accessibility evaluation methodologies. We conducted four studies, including an examination of accessibility reports from European Member-states, interviews with accessibility experts, manual evaluations, and usability tests involving users. Our investigations have uncovered significant limitations in current evaluation methods, suggesting that the absence of authoritative guidelines and standards, similar to what exists for the web, but tailored specifically to mobile devices, hampers the effectiveness of accessibility evaluation and monitoring activities. Based on our findings, we present a set of recommendations aimed at improving the evaluation methodologies for assessing mobile applications’ accessibility.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {964},
numpages = {17},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3380536.3380542,
author = {Ghane, Millad and Chandrasekaran, Sunita and Cheung, Margaret S.},
title = {Towards a portable hierarchical view of distributed shared memory systems: challenges and solutions},
year = {2020},
isbn = {9781450375221},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3380536.3380542},
doi = {10.1145/3380536.3380542},
abstract = {An ever-growing diversity in the architecture of modern super-computers has led to challenges in developing scientific software. Utilizing heterogeneous and disruptive architectures (e.g., off-chip and, in the near future, on-chip accelerators) has increased the software complexity and worsened its maintainability. To that end, we need a productive software ecosystem that improves the usability and portability of applications for such systems while allowing every parallelism opportunity to be exploited.In this paper, we outline several challenges that we encountered in the implementation of Gecko, a hierarchical model for distributed shared memory architectures, using a directive-based programming model, and discuss our solutions. Such challenges include: 1) inferred kernel execution with respect to the data placement, 2) workload distribution, 3) hierarchy maintenance, and 4) memory management.We performed the experimental evaluation of our implementation by using the Stream and Rodinia benchmarks. These benchmarks represent several major scientific software applications commonly used by the domain scientists. Our results reveal how the Stream benchmark reaches a sustainable bandwidth of 80 GB/s and 1.8 TB/s for single Intel Xeon Processor and four NVIDIA V100 GPUs, respectively. Additionally, the srad_v2 in the Rodinia benchmark reaches the 88% speedup efficiency while using four GPUs.},
booktitle = {Proceedings of the Eleventh International Workshop on Programming Models and Applications for Multicores and Manycores},
articleno = {5},
numpages = {10},
keywords = {abstraction, heterogeneous, hierarchy, portable, programming model, shared memory},
location = {San Diego, California},
series = {PMAM '20}
}

@article{10.1109/TASLP.2023.3282093,
author = {Drakopoulos, Fotios and Verhulst, Sarah},
title = {A Neural-Network Framework for the Design of Individualised Hearing-Loss Compensation},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3282093},
doi = {10.1109/TASLP.2023.3282093},
abstract = {Sound processing in the human auditory system is complex and highly non-linear, whereas hearing aids (HAs) still rely on simplified descriptions of auditory processing or hearing loss to restore hearing. Even though standard HA amplification strategies succeed in restoring audibility of faint sounds, they still fall short of providing targeted treatments for complex sensorineural deficits and adverse listening conditions. These shortcomings of current HA devices demonstrate the need for advanced hearing-loss compensation strategies that can effectively leverage the non-linear character of the auditory system. Here, we propose a differentiable deep-neural-network (DNN) framework that can be used to train DNN-based HA models based on biophysical auditory-processing differences between normal-hearing and hearing-impaired systems. We investigate different loss functions to accurately compensate for impairments that include outer-hair-cell (OHC) loss and cochlear synaptopathy (CS), and evaluate the benefits of our trained DNN-based HA models for speech processing in quiet and in noise. Our results show that auditory-processing enhancement was possible for all considered hearing-loss cases, with OHC loss proving easier to compensate than CS. Several objective metrics were considered to estimate the expected speech intelligibility after processing, and these simulations hold promise in yielding improved understanding of speech-in-noise for hearing-impaired listeners who use our DNN-based HA processing. Since our framework can be tuned to the hearing-loss profiles of individual listeners, we enter an era where truly individualised and DNN-based hearing-restoration strategies can be developed and be tested experimentally.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {2395–2409},
numpages = {15}
}

@inproceedings{10.1109/SEAMS.2017.6,
author = {Sousa, Gustavo and Rudametkin, Walter and Duchien, Laurence},
title = {Extending dynamic software product lines with temporal constraints},
year = {2017},
isbn = {9781538615508},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SEAMS.2017.6},
doi = {10.1109/SEAMS.2017.6},
abstract = {Due to the number of cloud providers, as well as the extensive collection of services, cloud computing provides very flexible environments, where resources and services can be provisioned and released on demand. However, reconfiguration and adaptation mechanisms in cloud environments are very heterogeneous and often exhibit complex constraints. For example, when reconfiguring a cloud system, a set of available services may be dependent on previous choices, or there may be alternative ways of adapting the system, with different impacts on performance, costs or reconfiguration time.Cloud computing systems exhibit high levels of variability, making dynamic software product lines (DSPLs) a promising approach for managing them. However, in DSPL approaches, verification is often limited to verifying conformance to a variability model, but this is insufficient to verify complex reconfiguration constraints that exist in cloud computing systems.In this paper, we propose the use of temporal constraints and reconfiguration operations to model a DSPL's reconfiguration lifecycle. We demonstrate how these concepts can be used to model the variability of cloud systems, and we use our approach to identify reconfigurations that meet given criteria.},
booktitle = {Proceedings of the 12th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {129–139},
numpages = {11},
keywords = {cloud computing, dynamic software product lines, feature models, variability},
location = {Buenos Aires, Argentina},
series = {SEAMS '17}
}

@inproceedings{10.1145/3698038.3698551,
author = {Shin, Jungeun and Arroyo, Diana and Tantawi, Asser and Wang, Chen and Youssef, Alaa and Nagi, Rakesh},
title = {Cloud-native Workflow Scheduling using a Hybrid Priority Rule, Dynamic Resource Allocation, and Dynamic Task Partition},
year = {2024},
isbn = {9798400712869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698038.3698551},
doi = {10.1145/3698038.3698551},
abstract = {As cloud-native workflow orchestration tools become increasingly important for complex data science workloads, there is a growing need for more efficient scheduling. Existing cloud schedulers rely on basic heuristics and user choice for task partitioning for parallel computing, leading to under-utilization of cluster resources and prolonged job completion times. To address this, we propose a novel workflow scheduling algorithm that leverages workflow characteristics to enhance resource utilization and reduce weighted job completion time. The algorithm combines three sub-algorithms, each reflecting a distinct aspect of the scheduling strategy: 1) Hybrid Maximum Children (MC) -Weighted Shortest Critical Path Time (WSCPT) rule alternates between two heuristics, MC and WSCPT, which prioritize jobs based on workflow structure and critical path, respectively. The choice between these heuristics is dynamically adjusted according to the cluster queue size. 2) Dynamic Resource Allocation (DRA), which dynamically adjusts the number of executors assigned to each workflow, and 3) Dynamic Task Partition (DTP), which autonomously determines the task parallelism level. We tested our algorithm with extensive experiments on various workflow types using Spark-imitated simulation. Our algorithm outperformed other schedulers, including learning-based models, by reducing 21-47% of the combined performance of average job completion time and makespan for unweighted workflows and reducing at least 50% of weighted job completion time for weighted workflows.},
booktitle = {Proceedings of the 2024 ACM Symposium on Cloud Computing},
pages = {830–846},
numpages = {17},
keywords = {Cloud native computing, Dynamic resource allocation, job scheduling, task partitioning, workflow scheduling},
location = {Redmond, WA, USA},
series = {SoCC '24}
}

@proceedings{10.1145/3712464,
title = {SPCT '24: Proceedings of the 2024 4th International Conference on Signal Processing and Communication Technology},
year = {2024},
isbn = {9798400710636},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@article{10.1145/3597457,
author = {Liu, Tiantian and Wang, Chao and Li, Zhengxiong and Huang, Ming-Chun and Xu, Wenyao and Lin, Feng},
title = {Wavoice: An mmWave-Assisted Noise-Resistant Speech Recognition System},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1550-4859},
url = {https://doi.org/10.1145/3597457},
doi = {10.1145/3597457},
abstract = {As automatic speech recognition evolves, deployment of the voice user interface (VUI) has boomingly expanded. Especially since the COVID-19 pandemic, the VUI has gained more attention in online communication owing to its non-contact property. However, the VUI struggles to be applied in public scenes due to the degradation of received audio signals caused by various ambient noises. In this article, we propose Wavoice, the first noise-resistant multi-modal speech recognition system that fuses two distinct voices sensing modalities (i.e., millimeter-wave signals and audio signals from a microphone) together. One key contribution is to model the inherent correlation between millimeter-wave and audio signals. Based on it, Wavoice facilitates the real-time noise-resistant voice activity detection and user targeting from multiple speakers. Additionally, we elaborate on two novel modules for multi-modal fusion embedded into the neural network, leading to accurate speech recognition. Extensive experiments prove the effectiveness of Wavoice under adverse conditions—that is, the character recognition error rate below 1% in a range of 7 m. In terms of robustness and accuracy, Wavoice considerably outperforms existing audio-only speech recognition methods with lower character error and word error rates.},
journal = {ACM Trans. Sen. Netw.},
month = may,
articleno = {86},
numpages = {29},
keywords = {Multi-modal systems, mmWave sensing, speech recognition, biometrics}
}

@inproceedings{10.1145/3674700.3674704,
author = {Chen, Yujiang and Xie, Mei},
title = {Improved Focus on Hard Samples for Lung Nodule Detection},
year = {2024},
isbn = {9798400718045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674700.3674704},
doi = {10.1145/3674700.3674704},
abstract = {Recently, lung nodule detection methods based on deep learning have shown excellent performance in the medical image processing field. Considering that only a few public lung datasets are available and lung nodules are more difficult to detect in CT (Computed Tomography) images than in natural images, the existing methods face many bottlenecks when detecting lung nodules, especially hard ones in CT images. In order to solve these problems, we plan to enhance the focus of our network. In this work, we present an improved detection network that pays more attention to hard samples and datasets to deal with lung nodules by introducing deformable convolution and self-paced learning. Experiments on the LUNA16 dataset demonstrate the effectiveness of our proposed components and show that our method has reached competitive performance.},
booktitle = {Proceedings of the 2024 6th International Conference on Control and Computer Vision},
pages = {21–26},
numpages = {6},
location = {Tianjin, China},
series = {ICCCV '24}
}

@inproceedings{10.1145/3510466.3511274,
author = {Meixner, Kristof and Feichtinger, Kevin and Rabiser, Rick and Biffl, Stefan},
title = {Efficient Production Process Variability Exploration},
year = {2022},
isbn = {9781450396042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510466.3511274},
doi = {10.1145/3510466.3511274},
abstract = {Cyber-Physical Production Systems (CPPSs) manufacture highly-customizable products from a product family following a sequence of production steps. For a CPPS, basic planners design feasible production process sequences by arranging atomic production steps based on implicit domain knowledge. However, the manual design of production sequences is inefficient and hard to reproduce due to the large configuration space. In this paper, we introduce the Iterative Process Sequence Exploration (IPSE) approach that (i) elicits domain knowledge in an industrial variability artifact, using the Product-Process-Resource Domain-Specific Language (PPR–DSL); (ii) reduces configuration space size regarding structural product variability and behavioral process variability; and (iii) facilitates efficiently exploring the configuration space in a process decision model. For production process sequence design, IPSE is a first approach to combine structural and behavioral variability models. We investigated the feasibility of the IPSE in a study on a typical manufacturing work line in automotive production. We compare the IPSE to a traditional process sequence planning approach. Our study indicates IPSE to be more efficient than the traditional manual approach.},
booktitle = {Proceedings of the 16th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {14},
numpages = {9},
keywords = {Configuration Reduction., Cyber-Physical Production System, Process Variability, Variability Modeling},
location = {Florence, Italy},
series = {VaMoS '22}
}

@article{10.1145/3676557,
author = {Gilberto, Lucas G. and Bermejo, Fernando Ra\'{u}l and Tommasini, Fabi\'{a}n C. and Garc\'{\i}a Bauza, Cristian},
title = {Virtual Reality Audio Game for Entertainment and Sound Localization Training},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1544-3558},
url = {https://doi.org/10.1145/3676557},
doi = {10.1145/3676557},
abstract = {Within the gaming and electronics industry, there is a continuous evolution of alternative applications. Nevertheless, accessibility to video games remains a persistent hurdle for individuals with disabilities, especially those with visual impairments due to the inherent visual-oriented design of games. Audio games (AGs) are electronic games that rely primarily on auditory cues instead of visual interfaces. This study focuses on the creation of a virtual reality AG for cell phones that integrates natural head and torso movements involved in spatial hearing. Its assessment encompasses user experience, interface usability, and sound localization performance. The study engaged eighteen sighted participants in a pre-post test with a control group. The experimental group underwent 7 training sessions with the AG. Via interviews, facets of the gaming experience were explored, while horizontal plane sound source localization was also tested before and after the training. The results enabled the characterization of sensations related to the use of the game and the interaction with the interfaces. Sound localization tests demonstrated distinct enhancements in performance among trained participants, varying with assessed stimuli. These promising results show advances for future virtual AGs, presenting prospects for auditory training. These innovations hold potential for skill development, entertainment, and the integration of visually impaired individuals.},
journal = {ACM Trans. Appl. Percept.},
month = nov,
articleno = {4},
numpages = {24},
keywords = {Audio games, spatial hearing training, natural interfaces}
}

@article{10.1109/TASLP.2020.2989582,
author = {Shi, Dongyuan and Gan, Woon-Seng and Lam, Bhan and Wen, Shulin},
title = {Feedforward Selective Fixed-Filter Active Noise Control: Algorithm and Implementation},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2989582},
doi = {10.1109/TASLP.2020.2989582},
abstract = {Conventional real-time active noise control (ANC) usually employs the adaptive filtered-x least mean square (FxLMS) algorithm to approach optimum coefficients for the control filter. However, lengthy training is usually required, and the perceived noise reduction is not immediately realized. Motivated by the practical implementation, we propose a selective fixed-filter active noise control (SFANC) algorithm, which selects a pretrained control filter to attenuate the detected primary noise rapidly. On top of improved robustness, the complexity analysis reveals that SFANC appears to be more efficient. The SFANC algorithm chooses the most suitable control filter based on the frequency-band-match approach implemented in a partitioned frequency-domain filter. Through simulations, SFANC is shown to exhibit a satisfactory response time and steady-state noise reduction performance, even for time-varying noise and real non-stationary disturbance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1479–1492},
numpages = {14}
}

@inproceedings{10.1145/3289602.3293916,
author = {Besta, Maciej and Fischer, Marc and Ben-Nun, Tal and De Fine Licht, Johannes and Hoefler, Torsten},
title = {Substream-Centric Maximum Matchings on FPGA},
year = {2019},
isbn = {9781450361378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289602.3293916},
doi = {10.1145/3289602.3293916},
abstract = {Developing high-performance and energy-efficient algorithms for maximum matchings is becoming increasingly important in social network analysis, computational sciences, scheduling, and others. In this work, we propose the first maximum matching algorithm designed for FPGAs; it is energy-efficient and has provable guarantees on accuracy, performance, and storage utilization. To achieve this, we forego popular graph processing paradigms, such as vertex-centric programming, that often entail large communication costs. Instead, we propose a substream-centric approach, in which the input stream of data is divided into substreams processed independently to enable more parallelism while lowering communication costs. We base our work on the theory of streaming graph algorithms and analyze 14 models and 28 algorithms. We use this analysis to provide theoretical underpinning that matches the physical constraints of FPGA platforms. Our algorithm delivers high performance (more than 4x speedup over tuned parallel CPU variants), low memory, high accuracy, and effective usage of FPGA resources. The substream-centric approach could easily be extended to other algorithms to offer low-power and high-performance graph processing on FPGAs.},
booktitle = {Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {152–161},
numpages = {10},
keywords = {fpga, graph streaming, matching, maximum matching, parallel graph algorithms},
location = {Seaside, CA, USA},
series = {FPGA '19}
}

@proceedings{10.1145/3639477,
title = {ICSE-SEIP '24: Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@inproceedings{10.1111/cgf.15172,
author = {Schreiner, P. and Netterstr\o{}m, R. and Yin, H. and Darkner, S. and Erleben, K.},
title = {ADAPT: AI-Driven Artefact Purging Technique for IMU Based Motion Capture},
year = {2024},
publisher = {Eurographics Association},
address = {Goslar, DEU},
url = {https://doi.org/10.1111/cgf.15172},
doi = {10.1111/cgf.15172},
abstract = {While IMU based motion capture offers a cost-effective alternative to premium camera-based systems, it often falls short in matching the latter's realism. Common distortions, such as self-penetrating body parts, foot skating, and floating, limit the usability of these systems, particularly for high-end users. To address this, we employed reinforcement learning to train an AI agent that mimics erroneous sample motion. Since our agent operates within a simulated environment, it inherently avoids generating these distortions since it must adhere to the laws of physics. Impressively, the agent manages to mimic the sample motions while preserving their distinctive characteristics. We assessed our method's efficacy across various types of input data, showcasing an ideal blend of artefact-laden IMU-based data with high-grade optical motion capture data. Furthermore, we compared the configuration of observation and action spaces with other implementations, pinpointing the most suitable configuration for our purposes. All our models underwent rigorous evaluation using a spectrum of quantitative metrics complemented by a qualitative review. These evaluations were performed using a benchmark dataset of IMU-based motion data from actors not included in the training data.},
booktitle = {Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation},
pages = {1–13},
numpages = {13},
location = {Montreal, Quebec, Canada},
series = {SCA '24}
}

@inproceedings{10.1145/3395035.3425325,
author = {Moustakas, Nikolaos and Rovithis, Emmanouel and Vogklis, Konstantinos and Floros, Andreas},
title = {Adaptive Audio Mixing for Enhancing Immersion in Augmented Reality Audio Games},
year = {2021},
isbn = {9781450380027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395035.3425325},
doi = {10.1145/3395035.3425325},
abstract = {In this work we present an adaptive audio mixing technique to be implemented in the design of Augmented Reality Audio (ARA) systems. The content of such systems is delivered entirely through the acoustic channel: the real acoustic environment is mixed with a virtual soundscape and returns to the listener as "pseudoacoustic" environment. We argue that the proposed adaptive mixing technique enhances user immersion in the augmented space in terms of the localization of sound objects. The need to optimise our ARA mixing engine emerged from our previous research, and more specifically from the analysis of the experimental results regarding the development of the Augmented Reality Audio Game (ARAG) "Audio Legends" that was tested on the field. The purpose of our new design was to aid sound localization, which is a crucial and demanding factor for delivering an immersive acoustic experience. We describe in depth the adaptive mixing along with the experimental test-bed. The results for the sound localization scenario indicate a substantial increase of 55 percent in accuracy compared to the legacy ARA mix model.},
booktitle = {Companion Publication of the 2020 International Conference on Multimodal Interaction},
pages = {220–227},
numpages = {8},
keywords = {adaptive audio mix, audio legends, augmented reality audio games, sonification, sound localization, virtual world},
location = {Virtual Event, Netherlands},
series = {ICMI '20 Companion}
}

@inproceedings{10.1145/3634713.3634722,
author = {Greiner, Sandra and Schmid, Klaus and Berger, Thorsten and Krieter, Sebastian and Meixner, Kristof},
title = {Generative AI And Software Variability - A Research Vision},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634722},
doi = {10.1145/3634713.3634722},
abstract = {Generative Artificial Intelligence (GAI) promises groundbreaking automation technology - a potential which may raise the management of variability-intensive software systems to a new level of automation. Several activities in maintaining variability-intensive software systems, such as extracting feature traces to updating features consistently, are repetitive and performed mainly manually or semi-automatically. Exploiting the potentials of GAI in maintaining variability-intensive software systems opens a fundamentally new research perspective, where GAI shall solve repetitive and hard-to-automate tasks. In this vision paper, we propose and discuss increasing levels of maintaining variability-intensive software systems automatically enabled through the support of GAI. We sketch actions necessary to reach the next levels of automation while discussing the current state-of-the-art.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {71–76},
numpages = {6},
keywords = {Variability-intensive software systems, generative AI, product lines},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@proceedings{10.1145/3626246,
title = {SIGMOD '24: Companion of the 2024 International Conference on Management of Data},
year = {2024},
isbn = {9798400704222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of the SIGMOD 2024 organizing committee, it is our distinct honor, as General Chairs, to welcome you to the 2024 ACM International Conference on Management of Data - SIGMOD 2024. We are thrilled to be hosting this prestigious event for the very first time in Latin America, and specifically in Santiago de Chile, a recognized leader in data technology within the region. This marks a significant milestone for the SIGMOD community, and we are honored to have you join us for a fully in-person experience in this vibrant and innovative city.},
location = {Santiago AA, Chile}
}

@inproceedings{10.1145/3196398.3196442,
author = {Nair, Vivek and Agrawal, Amritanshu and Chen, Jianfeng and Fu, Wei and Mathew, George and Menzies, Tim and Minku, Leandro and Wagner, Markus and Yu, Zhe},
title = {Data-driven search-based software engineering},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196442},
doi = {10.1145/3196398.3196442},
abstract = {This paper introduces Data-Driven Search-based Software Engineering (DSE), which combines insights from Mining Software Repositories (MSR) and Search-based Software Engineering (SBSE). While MSR formulates software engineering problems as data mining problems, SBSE reformulate Software Engineering (SE) problems as optimization problems and use meta-heuristic algorithms to solve them. Both MSR and SBSE share the common goal of providing insights to improve software engineering. The algorithms used in these two areas also have intrinsic relationships. We, therefore, argue that combining these two fields is useful for situations (a) which require learning from a large data source or (b) when optimizers need to know the lay of the land to find better solutions, faster.This paper aims to answer the following three questions: (1) What are the various topics addressed by DSE?, (2) What types of data are used by the researchers in this area?, and (3) What research approaches do researchers use? The paper briefly sets out to act as a practical guide to develop new DSE techniques and also to serve as a teaching resource.This paper also presents a resource (tiny.cc/data-se) for exploring DSE. The resource contains 89 artifacts which are related to DSE, divided into 13 groups such as requirements engineering, software product lines, software processes. All the materials in this repository have been used in recent software engineering papers; i.e., for all this material, there exist baseline results against which researchers can comparatively assess their new ideas.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {341–352},
numpages = {12},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1109/MICRO.2010.15,
author = {Watkins, Matthew A. and Albonesi, David H.},
title = {ReMAP: A Reconfigurable Heterogeneous Multicore Architecture},
year = {2010},
isbn = {9780769542997},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MICRO.2010.15},
doi = {10.1109/MICRO.2010.15},
abstract = {This paper presents ReMAP, a reconfigurable architecture geared towards accelerating and parallelizing applications within a heterogeneous CMP. In ReMAP, threads share a common reconfigurable fabric that can be configured for individual thread computation or fine-grained communication with integrated computation. The architecture supports both fine-grained point-to-point communication for pipeline parallelization and fine-grained barrier synchronization. The combination of communication and configurable computation within ReMAP provides the unique ability to perform customized computation while data is transferred between cores, and to execute custom global functions after barrier synchronization. ReMAP demonstrates significantly higher performance and energy efficiency compared to hard-wired communication-only mechanisms, and over what can ideally be achieved by allocating the fabric area to additional or more powerful cores.},
booktitle = {Proceedings of the 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {497–508},
numpages = {12},
series = {MICRO '43}
}

@inproceedings{10.1145/3019612.3019790,
author = {Ara\'{u}jo, Italo L. and Santos, Ismayle S. and Filho, Jo\~{a}o B. Ferreira and Andrade, Rossana M. C. and Neto, Pedro Santos},
title = {Generating test cases and procedures from use cases in dynamic software product lines},
year = {2017},
isbn = {9781450344869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019612.3019790},
doi = {10.1145/3019612.3019790},
abstract = {Software engineering is systematically evolving to address the production of families of systems instead of single products. This evolution comes at a price, it is now essential to deal with variability at both design and execution time. Developing test cases and procedures for a whole family of systems considering this dynamicity (variability at runtime) can be challenging. We propose a method to generate tests from use case specifications expressed in a controlled natural language, yet considering the variability and dynamicity in those specifications. We evaluate our method against use case specifications of a family of mobile and context-aware systems. Experimenting our method, we could measure that developing test cases and procedures becomes around 40+ faster when using our approach, opposed to manual development of tests under the same conditions.},
booktitle = {Proceedings of the Symposium on Applied Computing},
pages = {1296–1301},
numpages = {6},
keywords = {context-aware, dynamic software product lines, test generation},
location = {Marrakech, Morocco},
series = {SAC '17}
}

@proceedings{10.1145/3561212,
title = {AM '22: Proceedings of the 17th International Audio Mostly Conference},
year = {2022},
isbn = {9781450397018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {St. P\"{o}lten, Austria}
}

@proceedings{10.1145/3652583,
title = {ICMR '24: Proceedings of the 2024 International Conference on Multimedia Retrieval},
year = {2024},
isbn = {9798400706196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to present the 2024 edition of the ACM International Conference on Multimedia Retrieval, ACM ICMR 2024, that took place from 10-14 June 2024, in Phuket, Thailand.Effectively and efficiently retrieving information from multimedia collections (e.g., text, image, video, audio, sensor data, 3D) based on user needs is one of the most exciting areas in multimedia research. The Annual ACM International Conference on Multimedia Retrieval (ICMR) offers a great opportunity for exchanging leading-edge multimedia retrieval ideas among researchers, practitioners, and other potential users of multimedia retrieval systems. ACM ICMR was created in 2011 in a merger of ACM CIVR (International Conference on Image and Video Retrieval) and ACM MIR (International Conference on Multimedia Information Retrieval). ACM ICMR serves to illuminate the state of the art in multimedia retrieval. ACM ICMR 2024 in Phuket follows the successful previous editions of ICMR in Trento, Italy 2011; Hong Kong, China 2012; Dallas, USA 2013; Glasgow, UK 2014; Shanghai, China 2015; New York, USA 2016; Bucharest, Romania 2017; Yokohama, Japan 2018; Ottawa, Canada 2019; Dublin, Ireland 2020 (online); Taipei, Taiwan 2021 (online); Newark, USA 2022 (hybrid); and Thessaloniki, Greece 2023 (hybrid).},
location = {Phuket, Thailand}
}

@article{10.1109/TASLP.2023.3282097,
author = {Subakan, Cem and Ravanelli, Mirco and Cornell, Samuele and Grondin, Fran\c{c}ois and Bronzi, Mirko},
title = {Exploring Self-Attention Mechanisms for Speech Separation},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3282097},
doi = {10.1109/TASLP.2023.3282097},
abstract = {Transformers have enabled impressive improvements in deep learning. They often outperform recurrent and convolutional models in many tasks while taking advantage of parallel processing. Recently, we proposed the SepFormer, which obtains state-of-the-art performance in speech separation with the WSJ0-2/3 Mix datasets. This paper studies in-depth Transformers for speech separation. In particular, we extend our previous findings on the SepFormer by providing results on more challenging noisy and noisy-reverberant datasets, such as LibriMix, WHAM!, and WHAMR!. Moreover, we extend our model to perform speech enhancement and provide experimental evidence on denoising and dereverberation tasks. Finally, we investigate, for the first time in speech separation, the use of efficient self-attention mechanisms such as Linformers, Lonformers, and ReFormers. We found that they reduce memory requirements significantly. For example, we show that the Reformer-based attention outperforms the popular Conv-TasNet model on the WSJ0-2Mix dataset while being faster at inference and comparable in terms of memory consumption.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {2169–2180},
numpages = {12}
}

@inproceedings{10.1145/3641181.3641198,
author = {Rey, William P and Jain, Shreyansh M and Lambino, Juan Carlos S and Raymundo, Zachary Josh T},
title = {SenSabong: A sensor-based Application for the aid of training Cocks for CockFighting},
year = {2024},
isbn = {9798400709319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641181.3641198},
doi = {10.1145/3641181.3641198},
abstract = {This study presents the development and testing of a precise cockfighting training aid leveraging a modified impact sensor microphone. The research aimed to enhance cockfighting practices by providing trainers with a reliable tool to optimize rooster capabilities. The device, integrated with the SenSabong App, accurately recorded roosters' hits, distinguishing between fight-related and incidental impacts. Rigorous testing confirmed the device's reliability and effectiveness in improving rooster performance with recommended training procedures. The findings contribute to ethical and effective cockfighting training, representing a crucial step toward advancing the sport. Recommendations include integrating wireless connectivity for enhanced user experiences and incorporating gyroscope and accelerometer technologies for precise and real-time training feedback.},
booktitle = {Proceedings of the 2024 10th International Conference on Computing and Data Engineering},
pages = {133–140},
numpages = {8},
location = {Bangkok, Thailand},
series = {ICCDE '24}
}

@inproceedings{10.1145/3652583.3657607,
author = {Zheng, Yuhang and Wang, Zhen and Chen, Long},
title = {Improving Data Augmentation for Robust Visual Question Answering with Effective Curriculum Learning},
year = {2024},
isbn = {9798400706196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652583.3657607},
doi = {10.1145/3652583.3657607},
abstract = {Being widely used in learning unbiased visual question answering (VQA) models, Data Augmentation (DA) helps mitigate language biases by generating extra training samples beyond the original samples. While today's DA methods can generate robust samples, the augmented training set, significantly larger than the original dataset, often exhibits redundancy in terms of difficulty or content repetition, leading to inefficient model training and even compromising the model performance. To this end, we design an Effective Curriculum Learning strategy ECL to enhance DA-based VQA methods. Intuitively, ECL trains VQA models on relatively "easy'' samples first, and then gradually changes to "harder'' samples, and less-valuable samples are dynamically removed. Compared to training on the entire augmented dataset, ECL strategy can further enhance VQA models' performance with fewer training samples. Extensive ablations have demonstrated the effectiveness of ECL on various methods.},
booktitle = {Proceedings of the 2024 International Conference on Multimedia Retrieval},
pages = {1084–1088},
numpages = {5},
keywords = {curriculum learning, data augmentation, visual question answering},
location = {Phuket, Thailand},
series = {ICMR '24}
}

@article{10.1109/TASLP.2024.3372891,
author = {de Souza, Luciana M. X. and Costa, M\'{a}rcio H. and Borges, Renata Coelho},
title = {Envelope-Based Multichannel Noise Reduction for Cochlear Implant Applications},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3372891},
doi = {10.1109/TASLP.2024.3372891},
abstract = {Cochlear implants (CI) are implantable electronic prostheses that aim to restore communication in people with severe to profound hearing loss. This is achieved by transforming the acoustic signals into electrical stimuli and applying them directly to the cochlea through a set of electrodes. Despite its effectiveness under optimal environmental conditions, its performance is severely degraded in the presence of noise. This work proposes an envelope-based minimum-variance distortionless response (MVDR) approach for multichannel noise reduction in CI applications. The original constraint in the conventional time-domain MVDR beamformer is modified to ensure that the second-order statistical moments of the processed and original speech-only envelopes are equivalent. Assuming strict computational limitations, as those found in commercial multi-electrode systems, a low computational-cost particular case of the proposed method results in a power-based MVDR that has a semi-analytical solution. The resulting algorithm provides a fast calculation method for its optimal coefficients without losing significant performance compared to its general form. Results obtained from computational simulations and objective performance criteria indicate higher intelligibility levels achieved by the proposed methods compared to the conventional time-domain MVDR beamformer. This evidence is corroborated by psychoacoustic experiments with ten normal-hearing volunteers listening to vocoded speech, and one CI user employing a research interface.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {1873–1884},
numpages = {12}
}

@inproceedings{10.1145/3510466.3510474,
author = {Birkemeyer, Lukas and Pett, Tobias and Vogelsang, Andreas and Seidl, Christoph and Schaefer, Ina},
title = {Feature-Interaction Sampling for Scenario-based Testing of Advanced Driver Assistance Systems✱},
year = {2022},
isbn = {9781450396042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510466.3510474},
doi = {10.1145/3510466.3510474},
abstract = {Scenario-based testing is considered state-of-the-art to verify and validate Advanced Driver Assistance Systems. However, two essential unsolved challenges prevent the practical application of scenario-based testing according to the SOTIF-standard: (1)&nbsp;how to select a set of representative test scenarios, and (2)&nbsp;how to assess the effectiveness of a test scenario suite. In this paper, we leverage variability modelling techniques to select scenarios from a scenario space and assess the resulting scenario suites with a mutation score as metric. We capture the scenario space in a feature model and generate representative subsets with feature-interaction coverage sampling. The mutation score assesses the failure-finding effectiveness of these samples. We evaluate our concepts by sampling scenario suites for two independent Autonomous Emergency Braking function implementations and executing them on an industrial-strength simulator. Our results show that the feature model captures a scenario space that is relevant to identify all mutants. We show that sampling based on interaction coverage reduces the testing effort significantly while maintaining effectiveness in terms of mutation scores. Our results underline the potential of feature model sampling for testing in the automotive industry.},
booktitle = {Proceedings of the 16th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {5},
numpages = {10},
keywords = {ADAS, SOTIF, Sampling Strategies, Scenario-based testing},
location = {Florence, Italy},
series = {VaMoS '22}
}

@proceedings{10.1145/3581791,
title = {MobiSys '23: Proceedings of the 21st Annual International Conference on Mobile Systems, Applications and Services},
year = {2023},
isbn = {9798400701108},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of the entire organizing committee, it is with immense pleasure that we welcome you to the 21st ACM International Conference on Mobile Systems, Applications, and Services (ACM MobiSys 2023) hosted in Helsinki, Finland on June 18 - 22, 2023. ACM MobiSys is the leading conference in research on mobile systems, applications and services, and a flagship conference of ACM SIGMOBILE.},
location = {Helsinki, Finland}
}

@inproceedings{10.1145/3560905.3568518,
author = {Chen, Qianniu and Chen, Meng and Lu, Li and Yu, Jiadi and Chen, Yingying and Wang, Zhibo and Ba, Zhongjie and Lin, Feng and Ren, Kui},
title = {Push the Limit of Adversarial Example Attack on Speaker Recognition in Physical Domain},
year = {2023},
isbn = {9781450398862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560905.3568518},
doi = {10.1145/3560905.3568518},
abstract = {The integration of deep learning on Speaker Recognition (SR) advances its development and wide deployment, but also introduces the emerging threat of adversarial examples. However, only a few existing studies investigate its practical threat in physical domain, which either evaluate its feasibility only by directly replaying generated adversarial examples, or explore the partial channel interference for robustness improvement. In this paper, we propose a physical adversarial example attack, PhyTalker, which could generate and inject perturbations on voices in a live-streaming manner on attacking various SR models in different physical channels. Compared with the typical adversarial example for digital attacks, PhyTalker generates a subphoneme-level perturbation dictionary to decouple the perturbation optimization and injection. Moreover, we introduce the channel augmentation to compensate both device and environmental distortions, as well as model ensemble to improve the perturbation transferability. Finally, PhyTalker recognizes and localizes the latest recorded phoneme to determine the corresponding perturbations for real-time broadcasting. Extensive experiments are conducted with a large-scale corpus in real physical scenarios, and results show that PhyTalker achieves an overall Attack Success Rate (ASR) of 85.5% in attacking mainstream SR systems and Mel Cepstral Distortion (MCD) of 2.45dB in human audibility.},
booktitle = {Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems},
pages = {710–724},
numpages = {15},
keywords = {adversarial example attack, live-streaming, physical domain, speaker recognition},
location = {Boston, Massachusetts},
series = {SenSys '22}
}

@inproceedings{10.1145/3239372.3239398,
author = {Weckesser, Markus and Lochau, Malte and Ries, Michael and Sch\"{u}rr, Andy},
title = {Mathematical Programming for Anomaly Analysis of Clafer Models},
year = {2018},
isbn = {9781450349499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239372.3239398},
doi = {10.1145/3239372.3239398},
abstract = {Clafer combines UML-like class- and meta-modeling with feature-oriented variability-modeling and first-order logic constraints. The considerable expressiveness of Clafer mainly stems from its built-in variability constructs, multiplicity annotations and recursive model structures which yield a potentially unbounded number of valid model instances. As a result, automated reasoning about semantic properties like model consistency (i.e., existence of valid model instances) and anomalies (e.g., false cardinality bounds) is very challenging. Recent analysis techniques are inherently incomplete as they impose an a-priori finite search space with either manually or heuristically adjusted bounds. In this paper, we present a novel approach for automated search-space restriction for a considerably rich, yet decidable fragment of the Clafer language that guarantees sound and complete detection results for a wide range of semantic anomalies. Our approach employs principles from mathematical programming by encoding Clafer models as Mixed Integer Linear Programs (MILP). Our experimental evaluation shows remarkable improvements of runtime efficiency as well as effectiveness of anomaly detection as compared to existing techniques.},
booktitle = {Proceedings of the 21th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
pages = {34–44},
numpages = {11},
keywords = {Anomaly Detection, Consistency Checks, Software Product Lines},
location = {Copenhagen, Denmark},
series = {MODELS '18}
}

@proceedings{10.1145/3560905,
title = {SenSys '22: Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems},
year = {2022},
isbn = {9781450398862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to ACM SenSys 2022, the 20th ACM Conference on Embedded Networked Sensor Systems, the premier computer systems conference focused on networked sensing systems and applications.},
location = {Boston, Massachusetts}
}

@proceedings{10.1145/3658664,
title = {IH&amp;MMSec '24: Proceedings of the 2024 ACM Workshop on Information Hiding and Multimedia Security},
year = {2024},
isbn = {9798400706370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 12th ACM Information Hiding and Multimedia Security Workshop - IH&amp;MMSec'24 in Baiona, Galicia, Spain, organized by the Research Group in Signal Processing in Communications (GPSC) at the University of Vigo. GPSC is one of the pioneering research groups in Information Hiding and Multimedia Security with over 25 years of active engagement in this domain. During this time, our field has seen remarkable growth, evolution, and reinvention, yet it continues to preserve the same effervescence of its inception. This vibrancy is reflected in the diverse range of topics covered in this year's program.In response to our call for papers, we received in total 69 submissions. The top five countries with the highest number of submissions (first author) were Germany, China, France, Italy, and the United States. Each submission underwent rigorous evaluation, with a minimum of three independent reviews provided by members of the Program Committee, supplemented by external reviewers as needed. Based on these timely and high-quality reviews, the Technical Program Chairs selected the 33 most outstanding submissions. The acceptance rate of 47.8% (33/69) reflects our commitment to uphold IH&amp;MMSec as a premier scientific venue in the field of Information Hiding and Multimedia Security. The accepted papers cover the fields of forensics, steganography, steganalysis, watermarking, biometrics, anonymity, security and privacy.},
location = {Baiona, Spain}
}

@proceedings{10.5555/3694850,
title = {SCA '24: Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation},
year = {2024},
publisher = {Eurographics Association},
address = {Goslar, DEU},
location = {Montreal, Quebec, Canada}
}

@inproceedings{10.1145/3543873.3584630,
author = {Lin, Qingwei and Li, Tianci and Zhao, Pu and Liu, Yudong and Ma, Minghua and Zheng, Lingling and Chintalapati, Murali and Liu, Bo and Wang, Paul and Zhang, Hongyu and Dang, Yingnong and Rajmohan, Saravan and Zhang, Dongmei},
title = {EDITS: An Easy-to-difficult Training Strategy for Cloud Failure Prediction},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3584630},
doi = {10.1145/3543873.3584630},
abstract = {Cloud failures have been a major threat to the reliability of cloud services. Many failure prediction approaches have been proposed to predict cloud failures before they actually occur, so that proactive actions can be taken to ensure service reliability. In industrial practice, existing failure prediction approaches mainly focus on utilizing state-of-the-art time series models to enhance the performance of failure prediction but neglect the training strategy. However, as curriculum learning points out, models perform better when they are trained with data in an order of easy-to-difficult. In this paper, we propose EDITS, a novel training strategy for cloud failure prediction, which greatly improves the performance of the existing cloud failure prediction models. Our experimental results on industrial and public datasets show that EDITS can obviously enhance the performance of cloud failure prediction model. In addition, EDITS also outperforms other curriculum learning methods. More encouragingly, our proposed EDITS has been successfully applied to Microsoft 365 and Azure online service systems, and has obviously reduced financial losses caused by cloud failures.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {371–375},
numpages = {5},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@article{10.1145/3647643,
author = {Seixas Pereira, Let\'{\i}cia and Guerreiro, Jo\~{a}o and Rodrigues, Andr\'{e} and Guerreiro, Tiago and Duarte, Carlos},
title = {From Automation to User Empowerment: Investigating the Role of a Semi-automatic Tool in Social Media Accessibility},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1936-7228},
url = {https://doi.org/10.1145/3647643},
doi = {10.1145/3647643},
abstract = {This article focuses on evaluating SONAAR (Social Networks Accessible Authoring), a tool that combines automation and end-user empowerment to enhance the accessibility of social media content. SONAAR aims to increase user engagement in creating accessible content and expanding the availability of accessible media online. Additionally, SONAAR provides supplementary information to support the authoring of accessible media content. To assess SONAAR's effectiveness, we conducted three distinct studies. First, we analyzed user patterns and behaviors through log analysis. Next, we evaluated the clarity, helpfulness, and efficiency of the additional documentation and its potential to improve engagement in accessible practices. Finally, we explored user perceptions and challenges when interacting with SONAAR. The obtained findings indicate positive user feedback and provide valuable insights for improvement. These results underscore the importance of raising awareness and offering support for accessible practices, as well as the necessity for enhanced platform backing. Our study contributes to advancing accessible content authoring, promoting inclusivity and accessibility in online social media. We suggest future research directions to facilitate broader adoption of accessible practices and address user engagement challenges, ultimately enhancing the accessibility of social media content.},
journal = {ACM Trans. Access. Comput.},
month = sep,
articleno = {13},
numpages = {25},
keywords = {Accessibility, social media, visual content, user-generated content, artificial intelligence}
}

@inproceedings{10.1145/3574198.3574219,
author = {Du, Minghao and Zhang, Wenquan and Wang, Tao and Liu, Shuang and Ming, Dong},
title = {An Automatic Depression Recognition Method from Spontaneous Pronunciation Using Machine Learning},
year = {2023},
isbn = {9781450397223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3574198.3574219},
doi = {10.1145/3574198.3574219},
abstract = {The rapidly growing number of depressed people increases the burden of clinical diagnosis. Due to the abnormal speech signal of depressed patients, automatic audio-based depression recognition has the potential to become a complementary method for diagnosing. However, recognition performance varies largely with different speech acquisition tasks and classifiers, making results not comparable, and the performance requires further improvement before clinical application. This work extracted high-level statistical acoustic features (prosodic, voice-quality, and spectral features) of 23 depressed patients and 29 healthy subjects under spontaneous pronunciation tasks (interview and picture description) and mechanical pronunciation tasks (story reading and word reading), then applied principal component analysis (PCA) to reduce features dimensions, finally employed multilayer perceptron (MLP) to establish the classification model and compared with traditional classifiers (logistic regression, support vector machine, decision tree, and naive Bayes). The results showed that spontaneous pronunciation induced more significantly discriminative acoustic features and achieved better recognition performance accordingly. And the PCA retained 90% useful information with 50% features. Furthermore, MLP achieved the best performance with the accuracy 0.875 and average F1 score 0.855 under the picture description task. This study provides support for task design and classifier building for audio-based depression recognition, which could assist in mass screening for depression.},
booktitle = {Proceedings of the 2022 9th International Conference on Biomedical and Bioinformatics Engineering},
pages = {133–139},
numpages = {7},
keywords = {Audio, Depression recognition, Machine learning, Speech task},
location = {Kyoto, Japan},
series = {ICBBE '22}
}

@proceedings{10.1145/3678717,
title = {SIGSPATIAL '24: Proceedings of the 32nd ACM International Conference on Advances in Geographic Information Systems},
year = {2024},
isbn = {9798400711077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {These proceedings contain the papers from the 32nd ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (ACM SIGSPATIAL 2024), held as an in-person event in Atlanta, GA, USA on October 29- November 01, 2024. SIGSPATIAL academics, students, and industry practitioners could attend the technical talks in-person, meet in the hallway for further discussions, and boost their professional network over lunch.},
location = {Atlanta, GA, USA}
}

@proceedings{10.1145/3565287,
title = {MobiHoc '23: Proceedings of the Twenty-fourth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
year = {2023},
isbn = {9781450399265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ACM MobiHoc is a premier international annual conference with a highly selective single-track technical program dedicated to addressing the challenges emerging from networked systems that must operate in the face of dynamics.},
location = {Washington, DC, USA}
}

@article{10.1145/3596217,
author = {Cazzola, Walter and Favalli, Luca},
title = {Scrambled Features for Breakfast: Concepts of Agile Language Development},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/3596217},
doi = {10.1145/3596217},
abstract = {Describing a framework to support simpler development of languages best suited to express the problems and solutions of each particular domain.},
journal = {Commun. ACM},
month = oct,
pages = {50–60},
numpages = {11}
}

@inproceedings{10.1145/3698062.3698085,
author = {Chen, zitong and Xu, xiuzhong and Shi, jiayi and Zhou, wenjian and Feng, guoping},
title = {Active Control of Vehicle Interior Noise Based on Variable Step-size Delayed Adaptive Notch Algorithm},
year = {2024},
isbn = {9798400717086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698062.3698085},
doi = {10.1145/3698062.3698085},
abstract = {With the rapid advancement of technology and the automotive industry's growth, NVH (Noise, Vibration, Harshness) has become a critical measure of vehicle comfort. Traditional passive noise reduction methods effectively suppress medium and high-frequency noise but fail against low-frequency noise. Active noise control (ANC) technology addresses this by generating sound waves that destructively interfere with primary noise, canceling it out. Low-frequency noise in vehicles, mainly from engine harmonics, necessitates a multichannel ANC system for effective suppression. Adaptive algorithms, like the Multichannel Filter Reference Least Mean Square (McFxLMS), enhance noise elimination in dynamic environments. This paper proposes a new variable step adaptive notch algorithm to improve the efficiency of ANC systems. Using LABVIEW for simulation, the algorithm combines single-channel adaptive delay notch and variable step technology, verified through experiments. The study highlights the need for further research to expand single-channel ANC to multichannel systems and to incorporate real-time engine speed data for robust algorithm performance. The proposed method offers a lower computational complexity, addressing hardware limitations while ensuring effective noise reduction.},
booktitle = {Proceedings of the 2024 The 6th World Symposium on Software Engineering (WSSE)},
pages = {154–160},
numpages = {7},
keywords = {LABVIEW, active noise control, adaptive notch algorithm, delayed adaptive notch algorithm, secondary path identification, variable step size},
location = {
},
series = {WSSE '24}
}

@article{10.1109/TASLP.2023.3328285,
author = {Drgas, Szymon and Bramsl\o{}w, Lars and Politis, Archontis and Naithani, Gaurav and Virtanen, Tuomas},
title = {Dynamic Processing Neural Network Architecture for Hearing Loss Compensation},
year = {2023},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3328285},
doi = {10.1109/TASLP.2023.3328285},
abstract = {This paper proposes neural networks for compensating sensorineural hearing loss. The aim of the hearing loss compensation task is to transform a speech signal to increase speech intelligibility after further processing by a person with a hearing impairment, which is modeled by a hearing loss model. We propose an interpretable model called dynamic processing network, which has a structure similar to band-wise dynamic compressor. The network is differentiable, and therefore allows to learn its parameters to maximize speech intelligibility. More generic models based on convolutional layers were tested as well. The performance of the tested architectures was assessed using spectro-temporal objective index (STOI) with hearing-threshold noise and hearing aid speech intelligibility (HASPI) metrics. The dynamic processing network gave a significant improvement of STOI and HASPI in comparison to popular compressive gain prescription rule Camfit. A large enough convolutional network could outperform the interpretable model with the cost of larger computational load. Finally, a combination of the dynamic processing network with convolutional neural network gave the best results in terms of STOI and HASPI.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {203–214},
numpages = {12}
}

@inproceedings{10.5555/2663546.2663573,
author = {Fredericks, Erik M. and Ramirez, Andres J. and Cheng, Betty H. C.},
title = {Towards run-time testing of dynamic adaptive systems},
year = {2013},
isbn = {9781467344012},
publisher = {IEEE Press},
abstract = {It is challenging to design, develop, and validate a dynamically adaptive system (DAS) that satisfies requirements, particularly when requirements can change at run time. Testing at design time can help verify and validate that a DAS satisfies its specified requirements and constraints. While offline tests may demonstrate that a DAS is capable of satisfying its requirements before deployment, a DAS may encounter unanticipated system and environmental conditions that can prevent it from achieving its objectives. In working towards a requirements-aware DAS, this paper proposes run-time monitoring and adaptation of tests as another technique for evaluating whether a DAS satisfies, or is even capable of satisfying, its requirements given its current execution context. To this end, this paper motivates the need and identifies challenges for adaptively testing a DAS at run time, as well as suggests possible methods for leveraging offline testing techniques for verifying run-time behavior.},
booktitle = {Proceedings of the 8th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {169–174},
numpages = {6},
location = {San Francisco, California},
series = {SEAMS '13}
}

@proceedings{10.1145/3603166,
title = {UCC '23: Proceedings of the IEEE/ACM 16th International Conference on Utility and Cloud Computing},
year = {2023},
isbn = {9798400702341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The IEEE/ACM International Conference on Utility and Cloud Computing (UCC) is a premier annual conference series aiming to provide a platform for researchers from both academia and industry to present new discoveries in the broad area of Cloud and Edge utility computing and applications.},
location = {Taormina (Messina), Italy}
}

@inproceedings{10.1145/3581791.3596856,
author = {Chan, Justin and Glenn, Antonio and Itani, Malek and Mancl, Lisa R. and Gallagher, Emily and Bly, Randall and Patel, Shwetak and Gollakota, Shyamnath},
title = {Wireless earbuds for low-cost hearing screening},
year = {2023},
isbn = {9798400701108},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581791.3596856},
doi = {10.1145/3581791.3596856},
abstract = {We present the first wireless earbud hardware that can perform hearing screening by detecting otoacoustic emissions. The conventional wisdom has been that detecting otoacoustic emissions, which are the faint sounds generated by the cochlea, requires sensitive and expensive acoustic hardware. Thus, medical devices for hearing screening cost thousands of dollars and are inaccessible in low and middle income countries. We show that by designing wireless ear-buds using low-cost acoustic hardware and combining them with wireless sensing algorithms, we can reliably identify otoacoustic emissions and perform hearing screening. Our algorithms combine frequency modulated chirps with wideband pulses emitted from a low-cost speaker to reliably separate otoacoustic emissions from in-ear reflections and echoes. We conducted a clinical study with 50 ears across two healthcare sites. Our study shows that the low-cost earbuds detect hearing loss with 100% sensitivity and 89.7% specificity, which is comparable to the performance of a $8000 medical device. By developing low-cost and open-source wearable technology, our work may help address global health inequities in hearing screening by democratizing these medical devices.Open-source hardware and code can be found here: https://github.com/uw-x/OAEbuds},
booktitle = {Proceedings of the 21st Annual International Conference on Mobile Systems, Applications and Services},
pages = {84–95},
numpages = {12},
keywords = {wireless earbuds, hearing screening, mobile health, otoacoustic emissions, wearable technologies, acoustic sensing},
location = {Helsinki, Finland},
series = {MobiSys '23}
}

@inproceedings{10.1145/3468264.3468578,
author = {Oh, Jeho and Y\i{}ld\i{}ran, Necip Faz\i{}l and Braha, Julian and Gazzillo, Paul},
title = {Finding broken Linux configuration specifications by statically analyzing the Kconfig language},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468578},
doi = {10.1145/3468264.3468578},
abstract = {Highly-configurable software underpins much of our computing infrastructure. It enables extensive reuse, but opens the door to broken configuration specifications. The configuration specification language, Kconfig, is designed to prevent invalid configurations of the Linux kernel from being built. However, the astronomical size of the configuration space for Linux makes finding specification bugs difficult by hand or with random testing. In this paper, we introduce a software model checking framework for building Kconfig static analysis tools. We develop a formal semantics of the Kconfig language and implement the semantics in a symbolic evaluator called kclause that models Kconfig behavior as logical formulas. We then design and implement a bug finder, called kismet, that takes kclause models and leverages automated theorem proving to find unmet dependency bugs. kismet is evaluated for its precision, performance, and impact on kernel development for a recent version of Linux, which has over 140,000 lines of Kconfig across 28 architecture-specific specifications. Our evaluation finds 781 bugs (151 when considering sharing among Kconfig specifications) with 100% precision, spending between 37 and 90 minutes for each Kconfig specification, although it misses some bugs due to underapproximation. Compared to random testing, kismet finds substantially more true positive bugs in a fraction of the time.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {893–905},
numpages = {13},
keywords = {Kconfig, formal verification, software configuration, static analysis},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@proceedings{10.1145/3650212,
title = {ISSTA 2024: Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 33rd edition of the International Symposium on Software Testing and Analysis, ISSTA 2024, held on September 16--20, 2024 in Vienna, Austria. ISSTA 2024 is co-located with ECOOP and MPLR 2024. ISSTA brings together academics, industrial researchers, and practitioners from all over the world working on testing and analyzing software systems.},
location = {Vienna, Austria}
}

@article{10.14778/3229863.3229864,
author = {Li, Shen and Gerver, Paul and MacMillan, John and Debrunner, Daniel and Marshall, William and Wu, Kun-Lung},
title = {Challenges and experiences in building an efficient apache beam runner for IBM streams},
year = {2018},
issue_date = {August 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3229863.3229864},
doi = {10.14778/3229863.3229864},
abstract = {This paper describes the challenges and experiences in the development of IBM Streams runner for Apache Beam. Apache Beam is emerging as a common stream programming interface for multiple computing engines. Each participating engine implements a runner to translate Beam applications into engine-specific programs. Hence, applications written with the Beam SDK can be executed on different underlying stream computing engines, with negligible migration penalty. IBM Streams is a widely-used enterprise streaming platform. It has a rich set of connectors and toolkits for easy integration of streaming applications with other enterprise applications. It also supports a broad range of programming language interfaces, including Java, C++, Python, Stream Processing Language (SPL) and Apache Beam. This paper focuses on our solutions to efficiently support the Beam programming abstractions in IBM Streams runner. Beam organizes data into discrete event time windows. This design, on the one hand, supports out-of-order data arrivals, but on the other hand, forces runners to maintain more states, which leads to higher space and computation overhead. IBM Streams runner mitigates this problem by efficiently indexing inter-dependent states, garbage-collecting stale keys, and enforcing bundle sizes. We also share performance concerns in Beam that could potentially impact applications. Evaluations show that IBM Streams runner outperforms Flink runner and Spark runner in most scenarios when running the Beam NEXMark benchmarks. IBM Streams runner is available for download from IBM Cloud Streaming Analytics service console.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1742–1754},
numpages = {13}
}

@inproceedings{10.1145/3686490.3686523,
author = {Zhou, Zhen and Li, Zuoyong and Wang, Zhengyuan and Teng, Shenghua and Wang, Tao},
title = {Dual-view Label and Feature Supervision Network for Semi-supervised Medical Image Segmentation},
year = {2024},
isbn = {9798400717192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686490.3686523},
doi = {10.1145/3686490.3686523},
abstract = {Semi-supervised learning has garnered significant attention, particularly in medical image segmentation, owing to its capacity to leverage a large number of unlabeled data and a limited amount of labeled data to improve performance. However, most existing semi-supervised segmentation methods exhibit shortcomings in supervising unlabeled data, both in the label space due to potential noise in pseudo-labels, and the feature space due to indistinct class boundaries. To address the issues above, we propose Dual-view Label and Feature Supervision Network, termed DLFS-Net, to enhance label and feature supervision for unlabeled data. Our approach is based on a dual-view learning strategy that incorporates two modules: the Confidence Fusion Supervision Module (CFS) and the Dual-view Prototype Learning Module (DPL). Specifically, the CFS module generates weight matrices to integrate complementary information from two model outputs, producing more accurate pseudo-labels in the label space. The DPL module reduces intra-class variations in the feature space from two different views through prototype learning to generate clear class boundaries. Experiments on the LA and Pancreas-CT datasets demonstrate that our framework show solid gains (e.g.,1.76% Dice and 2.37% Jaccard improvement on Pancreas-CT dataset with 20% labeled data) compared with the state-of-the-art methods.},
booktitle = {Proceedings of the 2024 7th International Conference on Signal Processing and Machine Learning},
pages = {225–230},
numpages = {6},
keywords = {Confidence Fusion Supervision, Dual-view Prototype Learning, Medical Image Segmentation, Semi-supervised learning},
location = {Qingdao, China},
series = {SPML '24}
}

@article{10.1145/2579281.2579312,
author = {Ionita, Anca Daniela and Lewis, Grace A. and Litoiu, Marin},
title = {Report of the 2013 IEEE 7th international symposium on the maintenance and evolution of service-oriented and cloud-based systems (MESOCA 2013)},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/2579281.2579312},
doi = {10.1145/2579281.2579312},
abstract = {The 2013 IEEE 7th International Symposium on the Maintenance and Evolution of Service-Oriented and Cloud-Based Systems (MESOCA 2013) took place in Eindhoven, The Netherlands, on September 24, 2013, as a co-located event of the 29th IEEE International Conference on Software Maintenance (ICSM 2013). MESOCA 2013 covered a wide range of academic and industrial experiences, brought together through one keynote, two invited presentations and eleven paper presentations, which triggered lively discussions. They approached aspects related to the entire software maintenance process, from requirements to testing, with specific solutions for Service-Oriented Architecture and Cloud Computing environments. Technical and business perspectives were discussed, including issues about optimization techniques, pre-migration evaluation of legacy software, decision analysis, energy efficiency, multi-cloud architectures and adaptability. It thus confirmed MESOCA as an ongoing forum for researchers and practitioners to identify and address the increasing challenges related to the evolution of service-provisioning systems.},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {34–37},
numpages = {4},
keywords = {SOA, cloud computing, cloudbased systems, service-oriented systems, serviceoriented architecture, services, software evolution, software maintenance}
}

@inproceedings{10.1145/2641483.2641542,
author = {Alrashoud, Mubarak and Ahmed, Lubaid and Abhari, Abdolreza},
title = {Binary Linear Programming-based Release Planning for Multi-tenant Business SaaS},
year = {2014},
isbn = {9781450327121},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2641483.2641542},
doi = {10.1145/2641483.2641542},
abstract = {In multi-tenant Software as a Service (SaaS) business software, the degree of tenants' satisfaction is a significant indicator of the success of the SaaS system. Tenants' satisfaction can be achieved by continuously fulfilling their evolving needs. Usually, SaaS providers frequently deliver new releases of the application. Each release contains new or enhanced features. However, SaaS providers have limited resources, which makes it difficult to them to incorporate all of the tenants' requests in the next release. Therefore, some requirements shall be postponed to later releases. In order to achieve the highest possible level of tenets' satisfaction, SaaS providers shall include the most common requirements in the next release, which guarantee the satisfaction of highest possible number of tenants with less effort. Additionally, tenants' priorities and preferences about the requirements must be considered. Besides maximizing tenants' satisfaction, it is crucial to meet different types of constraints such as resource, technical, and contractual constraints. This paper identifies the factors that govern the release planning process for multi-tenant business software, which are contractual constraints, commonality of requirements, tenants' preferences and decision weights, risk, technical constraints. The first two factors are suggested by this paper, while the remaining factors are inherited from the traditional release planning process. Moreover, this paper proposes a framework that deals with the uniqueness of the release planning process in multi-tenant SaaS system. In this framework, Binary Linear Programming (BLP) is employed to optimize the selection process of the requirements that will be implemented in the next release. An experiments section is provided to illustrate the degree of satisfaction that can be achieved using the proposed framework.},
booktitle = {Proceedings of the 2014 International C* Conference on Computer Science &amp; Software Engineering},
articleno = {15},
numpages = {8},
keywords = {SaaS development, SaaS requirements engineering, Software release planning, applications of binary linear programming, requirements prioritization},
location = {Montreal, QC, Canada},
series = {C3S2E '14}
}

@inproceedings{10.1145/3696409.3700186,
author = {Zhou, Jieqiong and Zhang, Guoqing and Zheng, Yuhui and Zhang, Fuguo},
title = {Local Feature-Emphasizing Transformer for Cloth-Changing Person Re-identification},
year = {2024},
isbn = {9798400712739},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696409.3700186},
doi = {10.1145/3696409.3700186},
abstract = {Cloth-changing person re-identification (CC Re-ID) is a challenging problem for the prevalence of clothing texture information which occupies most of the pixels in the image, becoming invalid information or even misleading information. Recently, many methods introduce Vision Transformer into the field of Re-ID to establish global feature relationships between image patch sequences. However, Transformer tends to scatter critical local features, resulting in decreased performance within cloth-changing scenarios. In order to solve this problem, we propose a Local Feature-Emphasizing Transformer for CC Re-ID (LFET) that centers on the association of local features. Specifically, we propose a Patch Exchange Fusion Module (PEFM), which aims to improve the generalization ability of the reconstructed patch sequence by strengthening the correlation between image patches. Thus, we promote a closer interpretation of the spatial relationship within the image, and when environmental conditions change and various interference exist, the robustness of the model is enhanced. Furthermore, we design a dynamic feature weighting strategy (DyFW) to reduce noise and emphasize discriminative features by applying dynamically generated weighted tensors to sequences. We carry out experiments on various public datasets, demonstrating that our proposed method delivers strong performance in addressing CC Re-ID task.},
booktitle = {Proceedings of the 6th ACM International Conference on Multimedia in Asia},
articleno = {26},
numpages = {1},
keywords = {cloth-changing, person re-identification, Vision Transformer},
location = {
},
series = {MMAsia '24}
}

@inproceedings{10.1145/3689031.3717488,
author = {Zhang, Yifeng and Pan, Yanqi and Huang, Hao and Shan, Yuchen and Xia, Wen},
title = {Overcoming the Last Mile between Log-Structured File Systems and Persistent Memory via Scatter Logging},
year = {2025},
isbn = {9798400711961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689031.3717488},
doi = {10.1145/3689031.3717488},
abstract = {Log-structured file system (LFS) has been a popular option for persistent memory (PM) for its high write performance and lightweight crash consistency protocols. However, even with PM's byte-addressable I/O interface, existing PM LFSes still maintain contiguous space for log locality while using heavy garbage collection (GC) to synchronously reclaim space, causing up to 50% I/O performance degradation on PM. Thus, there exists a last-mile problem between the contiguous space management of LFS (that induces GC) and the non-contiguous byte-addressability of PM.To overcome this, we propose a novel scatter logging technique called SLOT. The core idea is to efficiently manage non-contiguous log entries on byte-addressable PM to prevent GC in LFS. Specifically, SLOT scatters log entries across PM and manages them in a per-entry granularity, thereby enabling the immediate reallocation of invalidated entries and eliminating GC overheads. SLOT further introduces an array of techniques to exploit PM write buffer efficiency to fully unleash PM I/O performance potential. We implement SlotFS to realize the efficiency of SLOT. Experimental results driven by synthetic and real-world workloads show that SlotFS significantly outperforms state-of-the-art PM file systems. Compared to two representative PM LFSes, NOVA and MadFS, SlotFS achieves 27%-47% and 59%-175% performance improvement under a series of real-world workloads.},
booktitle = {Proceedings of the Twentieth European Conference on Computer Systems},
pages = {1009–1025},
numpages = {17},
keywords = {Log-structured file systems, persistent memory},
location = {Rotterdam, Netherlands},
series = {EuroSys '25}
}

@inproceedings{10.1145/3637528.3671693,
author = {Wang, Guolong and Wu, Xun and Qin, Zheng and Shi, Liangliang},
title = {Routing Evidence for Unseen Actions in Video Moment Retrieval},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671693},
doi = {10.1145/3637528.3671693},
abstract = {Video moment retrieval (VMR) is a cutting-edge vision-language task locating a segment in a video according to the query. Though the methods have achieved significant performance, they assume that training and testing samples share the same action types, hindering real-world application. In this paper, we specifically consider a new problem: video moment retrieval by queries with unseen actions. We propose a plug-and-play structure, Routing Evidence (RE), with multiple evidence-learning heads and dynamically route one to locate a sentence with an unseen action. Each evidence-learning head estimates the uncertainty while regressing timestamps. We formulate the evidence distribution by a Normal-Inverse Gamma function and design a router to select the most appropriate distribution for a sample. Empirically, we study the efficacy of RE on three updated databases where training and testing samples contain different action types. We find that RE outperforms other state-of-the-art methods with a more robust predictor. Code and data will be available at https://github.com/dieuroi/Routing-Evidence.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3024–3035},
numpages = {12},
keywords = {dynamic routing, evidence learning, unseen action, video moment retrieval},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/2088876.2088879,
author = {Merle, Philippe and Rouvoy, Romain and Seinturier, Lionel},
title = {A reflective platform for highly adaptive multi-cloud systems},
year = {2011},
isbn = {9781450310703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2088876.2088879},
doi = {10.1145/2088876.2088879},
abstract = {Cloud platforms are increasingly used for hosting a broad diversity of services from traditional e-commerce applications to interactive web-based IDEs. However, we observe that the proliferation of offers by Cloud vendors raises several challenges. Developers will not only have to deploy applications for a specific Cloud, but will also have to consider migrating services from one cloud to another, and to manage applications spanning multiple Clouds. In this paper, we therefore report on a first experiment we conducted to build a multi-Cloud system on top of thirteen existing IaaS/PaaS. From this experiment, we advocate for two dimensions of adaptability---design and execution time---that applications for such systems require to exhibit. Finally, we propose a roadmap for future multi-Cloud systems.},
booktitle = {Adaptive and Reflective Middleware on Proceedings of the International Workshop},
pages = {14–21},
numpages = {8},
location = {Lisbon, Portugal},
series = {ARM '11}
}

@proceedings{10.1145/3586183,
title = {UIST '23: Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Francisco, CA, USA}
}

@article{10.1145/3543847,
author = {Idowu, Samuel and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Asset Management in Machine Learning: State-of-research and State-of-practice},
year = {2022},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3543847},
doi = {10.1145/3543847},
abstract = {Machine learning components are essential for today’s software systems, causing a need to adapt traditional software engineering practices when developing machine-learning-based systems. This need is pronounced due to many development-related challenges of machine learning components such as asset, experiment, and dependency management. Recently, many asset management tools addressing these challenges have become available. It is essential to understand the support such tools offer to facilitate research and practice on building new management tools with native supports for machine learning and software engineering assets. This article positions machine learning asset management as a discipline that provides improved methods and tools for performing operations on machine learning assets. We present a feature-based survey of 18 state-of-practice and 12 state-of-research tools supporting machine-learning asset management. We overview their features for managing the types of assets used in machine learning experiments. Most state-of-research tools focus on tracking, exploring, and retrieving assets to address development concerns such as reproducibility, while the state-of-practice tools also offer collaboration and workflow-execution-related operations. In addition, assets are primarily tracked intrusively from the source code through APIs and managed via web dashboards or command-line interfaces (CLIs). We identify asynchronous collaboration and asset reusability as directions for new tools and techniques.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {144},
numpages = {35},
keywords = {Machine learning, experiment management tools, SE4AI}
}

@inproceedings{10.5555/2662572.2662581,
author = {Sayyad, Abdel Salam and Ingram, Joseph and Menzies, Tim and Ammar, Hany},
title = {Optimum feature selection in software product lines: let your model and values guide your search},
year = {2013},
isbn = {9781467362849},
publisher = {IEEE Press},
abstract = {In Search-Based Software Engineering, well-known metaheuristic search algorithms are utilized to find solutions to common software engineering problems. The algorithms are usually taken "off the shelf" and applied with trust, i.e. software engineers are not concerned with the inner workings of algorithms, only with the results. While this may be sufficient is some domains, we argue against this approach, particularly where the complexity of the models and the variety of user preferences pose greater challenges to the metaheuristic search algorithms. We build on our previous investigation which uncovered the power of Indicator-Based Evolutionary Algorithm (IBEA) over traditionally-used algorithms (such as NSGA-II), and in this work we scrutinize the time behavior of user objectives subject to optimization. This analysis brings out the business perspective, previously veiled under Pareto-collective gauges such as Hypervolume and Spread. In addition, we show how slowing down the rates of crossover and mutation can help IBEA converge faster, as opposed to following the higher rates used in many other studies as "rules of thumb".},
booktitle = {Proceedings of the 1st International Workshop on Combining Modelling and Search-Based Software Engineering},
pages = {22–27},
numpages = {6},
keywords = {feature models, indicator-based evolutionary algorithm, multiobjective optimization, optimal feature selection, search-based software engineering, software product lines},
location = {San Francisco, California},
series = {CMSBSE '13}
}

@inproceedings{10.1145/3664647.3680661,
author = {Xu, Weiye and Wang, Min and Zhou, Wengang and Li, Houqiang},
title = {P-RAG: Progressive Retrieval Augmented Generation For Planning on Embodied Everyday Task},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3680661},
doi = {10.1145/3664647.3680661},
abstract = {Embodied Everyday Task is a popular task in the embodied AI community, requiring agents to make a sequence of actions based on natural language instructions and visual observations. Traditional learning-based approaches face two challenges. Firstly, natural language instructions often lack explicit task planning. Secondly, extensive training is required to equip models with knowledge of the task environment. Previous works based on Large Language Model (LLM) either suffer from poor performance due to the lack of task-specific knowledge or rely on ground truth as few-shot samples. To address the above limitations, we propose a novel approach called Progressive Retrieval Augmented Generation (P-RAG), which not only effectively leverages the powerful language processing capabilities of LLMs but also progressively accumulates task-specific knowledge without ground-truth. Compared to the conventional RAG methods, which retrieve relevant information from the database in a one-shot manner to assist generation, P-RAG introduces an iterative approach to progressively update the database. In each iteration, P-RAG retrieves the latest database and obtains historical information from the previous interaction as experiential references for the current interaction. Moreover, we also introduce a more granular retrieval scheme that not only retrieves similar tasks but also incorporates retrieval of similar situations to provide more valuable reference experiences. Extensive experiments reveal that P-RAG achieves competitive results without utilizing ground truth and can even further improve performance through self-iterations.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {6969–6978},
numpages = {10},
keywords = {embodied ai, large language model, progressive method, retrieval augmented generation},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@inproceedings{10.1145/2491956.2462163,
author = {Nowatzki, Tony and Sartin-Tarm, Michael and De Carli, Lorenzo and Sankaralingam, Karthikeyan and Estan, Cristian and Robatmili, Behnam},
title = {A general constraint-centric scheduling framework for spatial architectures},
year = {2013},
isbn = {9781450320146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491956.2462163},
doi = {10.1145/2491956.2462163},
abstract = {Specialized execution using spatial architectures provides energy efficient computation, but requires effective algorithms for spatially scheduling the computation. Generally, this has been solved with architecture-specific heuristics, an approach which suffers from poor compiler/architect productivity, lack of insight on optimality, and inhibits migration of techniques between architectures.Our goal is to develop a scheduling framework usable for all spatial architectures. To this end, we expresses spatial scheduling as a constraint satisfaction problem using Integer Linear Programming (ILP). We observe that architecture primitives and scheduler responsibilities can be related through five abstractions: placement of computation, routing of data, managing event timing, managing resource utilization, and forming the optimization objectives. We encode these responsibilities as 20 general ILP constraints, which are used to create schedulers for the disparate TRIPS, DySER, and PLUG architectures. Our results show that a general declarative approach using ILP is implementable, practical, and typically matches or outperforms specialized schedulers.},
booktitle = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {495–506},
numpages = {12},
keywords = {integer linear programming, spatial architecture scheduling, spatial architectures},
location = {Seattle, Washington, USA},
series = {PLDI '13}
}

@inproceedings{10.1109/SEAMS.2019.00018,
author = {Bennaceur, Amel and Ghezzi, Carlo and Tei, Kenji and Kehrer, Timo and Weyns, Danny and Calinescu, Radu and Dustdar, Schahram and Hu, Zhenjiang and Honiden, Shinichi and Ishikawa, Fuyuki and Jin, Zhi and Kramer, Jeffrey and Litoiu, Marin and Loreti, Michele and Moreno, Gabriel A. and M\"{u}ller, Hausi A. and Nenzi, Laura and Nuseibeh, Bashar and Pasquale, Liliana and Reisig, Wolfgang and Schmidt, Heinz and Tsigkanos, Christos and Zhao, Haiyan},
title = {Modelling and analysing resilient cyber-physical systems},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SEAMS.2019.00018},
doi = {10.1109/SEAMS.2019.00018},
abstract = {From smart buildings to medical devices to smart nations, software systems increasingly integrate computation, networking, and interaction with the physical environment. These systems are known as Cyber-Physical Systems (CPS). While these systems open new opportunities to deliver improved quality of life for people and reinvigorate computing, their engineering is a difficult problem given the level of heterogeneity and dynamism they exhibit. While progress has been made, we argue that complexity is now at a level such that existing approaches need a major re-think to define principles and associated techniques for CPS. In this paper, we identify research challenges when modelling, analysing and engineering CPS. We focus on three key topics: theoretical foundations of CPS, self-adaptation methods for CPS, and exemplars of CPS serving as a research vehicle shared by a larger community. For each topic, we present an overview and suggest future research directions, thereby focusing on selected challenges. This paper is one of the results of the Shonan Seminar 118 on Modelling and Analysing Resilient Cyber-Physical Systems, which took place in December 2018.},
booktitle = {Proceedings of the 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {70–76},
numpages = {7},
location = {Montreal, Quebec, Canada},
series = {SEAMS '19}
}

@article{10.1145/3549526,
author = {Perez-Cerrolaza, Jon and Abella, Jaume and Kosmidis, Leonidas and Calderon, Alejandro J. and Cazorla, Francisco and Flores, Jose Luis},
title = {GPU Devices for Safety-Critical Systems: A Survey},
year = {2022},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3549526},
doi = {10.1145/3549526},
abstract = {&nbsp;Graphics Processing Unit (GPU) devices and their associated software programming languages and frameworks can deliver the computing performance required to facilitate the development of next-generation high-performance safety-critical systems such as autonomous driving systems. However, the integration of complex, parallel, and computationally demanding software functions with different safety-criticality levels on GPU devices with shared hardware resources contributes to several safety certification challenges. This survey categorizes and provides an overview of research contributions that address GPU devices’ random hardware failures, systematic failures, and independence of execution.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {147},
numpages = {37},
keywords = {Diagnostic coverage, time independence, spatial independence}
}

@article{10.1145/3729241,
author = {Chang, Li-Wei and Li, Cheng-Te and Yang, Chun-Pai and Lin, Shou-de},
title = {Learning on Missing Tabular Data: Attention with Self-Supervision, Not Imputation, is All You Need},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3729241},
doi = {10.1145/3729241},
abstract = {Learning from data with missing values is a common challenge in real-world applications. Existing approaches for handling data incompleteness often involve imputation, which can introduce errors that propagate into downstream tasks or impose assumptions that limit the support for heterogeneous feature types. To address these issues, we propose Missing Feature Attention Network (MFAN), an end-to-end label prediction model that directly consumes incomplete data without requiring imputation. MFAN flexibly accommodates both continuous and categorical features through learnable embeddings, and leverages a transformer encoder with self-attention to capture the correlation among features as well as the correlation between features and missingness. This attention-based mechanism allows missing features to benefit from relationships learned among observed features, leading to enhanced hidden representations and robust prediction performance. Additionally, we introduce auxiliary self-supervised pre-training tasks that further guide the attention mechanism in modeling missingness. Experimental results on eight regression and seven classification datasets demonstrate MFAN’s superiority over state-of-the-art end-to-end methods and imputation-based approaches. Comprehensive ablation studies confirm the effectiveness of each MFAN component, underscoring the importance of explicitly modeling correlations among observed and missing features.},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
keywords = {missing data imputation, attention mechanisms, self attention, self-supervised pre-training, feature attention network}
}

@inproceedings{10.1145/3581783.3611988,
author = {Wang, Ao and Chen, Hui and Lin, Zijia and Ding, Zixuan and Liu, Pengzhang and Bao, Yongjun and Yan, Weipeng and Ding, Guiguang},
title = {Hierarchical Prompt Learning Using CLIP for Multi-label Classification with Single Positive Labels},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3611988},
doi = {10.1145/3581783.3611988},
abstract = {Collecting full annotations to construct multi-label datasets is difficult and labor-consuming. As an effective solution to relieve the annotation burden, single positive multi-label learning (SPML) draws increasing attention from both academia and industry. It only annotates each image with one positive label, leaving other labels unobserved. Therefore, existing methods strive to explore the cue of unobserved labels to compensate for the insufficiency of label supervision. Though achieving promising performance, they generally consider labels independently, leaving out the inherent hierarchical semantic relationship among labels which reveals that labels can be clustered into groups. In this paper, we propose a hierarchical prompt learning method with a novel Hierarchical Semantic Prompt Network (HSPNet) to harness such hierarchical semantic relationships using a large-scale pretrained vision and language model, i.e., CLIP, for SPML. We first introduce a Hierarchical Conditional Prompt (HCP) strategy to grasp the hierarchical label-group dependency. Then we equip a Hierarchical Graph Convolutional Network (HGCN) to capture the high-order inter-label and inter-group dependencies. Comprehensive experiments and analyses on several benchmark datasets show that our method significantly outperforms the state-of-the-art methods, well demonstrating its superiority and effectiveness. Our code will be available at https://github.com/jameslahm/HSPNet.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {5594–5604},
numpages = {11},
keywords = {hierarchical relationship, image recognition, multi-label classification, vision and language, weak supervision},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@proceedings{10.1145/3643916,
title = {ICPC '24: Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ICPC is the premier (CORE A) venue for research on program comprehension. Research on program comprehension encompasses both human activities for comprehending the software and technologies for supporting such comprehension.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3580305.3599495,
author = {Wu, Huimin and Shi, Wanli and Zhang, Chenkang and Gu, Bin},
title = {Self-Adaptive Perturbation Radii for Adversarial Training},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599495},
doi = {10.1145/3580305.3599495},
abstract = {Adversarial training has been shown to be the most popular and effective technique to protect models from imperceptible adversarial samples. Despite its success, it also accompanies the significant performance degeneration to clean data. To achieve a good performance on both clean and adversarial samples, the main effort is searching for an adaptive perturbation radius for each training sample. However, this method suffers from a conflict between exact searching and computational overhead. To address this conflict, in this paper, firstly we show the superiority of adaptive perturbation radii on the accuracy and robustness respectively. Then we propose our novel self-adaptive adjustment framework for perturbation radii without tedious searching. We also discuss this framework on both deep neural networks (DNNs) and kernel support vector machines (SVMs). Finally, extensive experimental results show that our framework can improve adversarial robustness without compromising the natural generalization. It is also competitive with existing searching strategies in terms of running time.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2570–2581},
numpages = {12},
keywords = {adversarial training, self-adaptive perturbation radii},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@proceedings{10.1145/3674658,
title = {ICBBT '24: Proceedings of the 2024 16th International Conference on Bioinformatics and Biomedical Technology},
year = {2024},
isbn = {9798400717666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3587819,
title = {MMSys '23: Proceedings of the 14th ACM Multimedia Systems Conference},
year = {2023},
isbn = {9798400701481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vancouver, BC, Canada}
}

@article{10.1145/3699752,
author = {Mahmud, Saif and Parikh, Vineet and Liang, Qikang and Li, Ke and Zhang, Ruidong and Ajit, Ashwin and Gunda, Vipin and Agarwal, Devansh and Guimbretiere, Francois and Zhang, Cheng},
title = {ActSonic: Recognizing Everyday Activities from Inaudible Acoustic Wave Around the Body},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
url = {https://doi.org/10.1145/3699752},
doi = {10.1145/3699752},
abstract = {We present ActSonic, an intelligent, low-power active acoustic sensing system integrated into eyeglasses that can recognize 27 different everyday activities (e.g., eating, drinking, toothbrushing) from inaudible acoustic waves around the body. It requires only a pair of miniature speakers and microphones mounted on each hinge of the eyeglasses to emit ultrasonic waves, creating an acoustic aura around the body. The acoustic signals are reflected based on the position and motion of various body parts, captured by the microphones, and analyzed by a customized self-supervised deep learning framework to infer the performed activities on a remote device such as a mobile phone or cloud server. ActSonic was evaluated in user studies with 19 participants across 19 households to track its efficacy in everyday activity recognition. Without requiring any training data from new users (leave-one-participant-out evaluation), ActSonic detected 27 activities, achieving an average F1-score of 86.6% in fully unconstrained scenarios and 93.4% in prompted settings at participants' homes.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = nov,
articleno = {183},
numpages = {32},
keywords = {Acoustic Sensing, Activity Recognition, Self-supervised Leaning}
}

@inproceedings{10.1145/3634737.3637643,
author = {R\"{o}ckl, Jonas and Bernsdorf, Nils and M\"{u}ller, Tilo},
title = {TeeFilter: High-Assurance Network Filtering Engine for High-End IoT and Edge Devices based on TEEs},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634737.3637643},
doi = {10.1145/3634737.3637643},
abstract = {Large botnets like Mirai, with 600,000 infected devices, prove that cyber criminals have recognized the potential of attacks against the fast-growing Internet of Things. Moreover, recent critical vulnerabilities like Ripple20 and Amnesia:33 show that taking over a remote system via the network is a real threat. Alarmingly, modern strains of malware rely on exploiting such vulnerabilities to spread, with an increasing tendency. Hence, effective techniques to mitigate the consequences of modern IoT malware are necessary.To that end, we propose TeeFilter, a novel network filtering engine that allows manufacturers and operators of IoT devices to restrict the network traffic of their devices. By selectively executing parts of the network stack in a Trusted Execution Environment, TeeFilter remains untampered even if the operating system is compromised. The operators can specify filtering rules in an LLVM-compatible programming language and compile them into eBPF code. Subsequently, TeeFilter can load and enforce the rules.We formally verify the majority of TeeFilter for correctness and memory safety to eradicate whole classes of vulnerabilities and prototype our system on real hardware to show that the network overhead is negligible. Therefore, we believe that our system is an impactful step to enhance the resiliency of future IoT infrastructure.},
booktitle = {Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
pages = {1568–1583},
numpages = {16},
keywords = {ARM TrustZone, traffic filtering, ethernet drivers, eBPF},
location = {Singapore, Singapore},
series = {ASIA CCS '24}
}

@inproceedings{10.1145/2465808.2465809,
author = {Blanc, Mathieu and Gros, Damien and Briffaut, J\'{e}r\'{e}my and Toinard, Christian},
title = {Mandatory access control with a multi-level reference monitor: PIGA-cluster},
year = {2013},
isbn = {9781450319843},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2465808.2465809},
doi = {10.1145/2465808.2465809},
abstract = {The protection of High Performance Computing architectures is still an open research problem. Generally, current solutions only feature confinement using sandboxing but none address the problematic of information flow control. This is why a better integration of mandatory access control mechanisms is needed in the HPC environment. In this paper, we propose a global architecture to protect a whole cluster. This architecture uses the specific cluster technologies in order not to reduce the operating system performances. The protection of the cluster relies on three levels of protection and the use of two kinds of reference monitors. SELinux is installed on the computing nodes and deals with direct information flows. PIGA, only installed on a specific node, performs advanced flow control and detects advanced threats. We present the various components of our architecture called PIGA-Cluster, then the results of several benchmarks on a computing node that show a low impact on the operating system performances. We also apply various security properties in order to protect the computing nodes against simple and advanced attacks. This paper takes advantage of previous works dealing with workstations or virtualisation technologies and extends the concepts for the HPC environment.},
booktitle = {Proceedings of the First Workshop on Changing Landscapes in HPC Security},
pages = {1–8},
numpages = {8},
keywords = {access control, information flow, low-latency network},
location = {New York, New York, USA},
series = {CLHS '13}
}

@inproceedings{10.1007/978-3-642-36757-1_3,
author = {Shamsaei, Azalia and Amyot, Daniel and Pourshahid, Alireza and Braun, Edna and Yu, Eric and Mussbacher, Gunter and Tawhid, Rasha and Cartwright, Nick},
title = {An approach to specify and analyze goal model families},
year = {2012},
isbn = {9783642367564},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-36757-1_3},
doi = {10.1007/978-3-642-36757-1_3},
abstract = {Goal-oriented languages have been used for years to model and reason about functional, non-functional, and legal requirements. It is however difficult to develop and maintain these models, especially when many models overlap with each other. This becomes an even bigger challenge when a single, generic model is used to capture a family of related goal models but different evaluations are required for each individual family member. In this work, we use ITU-T's Goal-oriented Requirement Language (GRL) and the jUCMNav tool to illustrate the problem and to formulate a solution that exploits the flexibility of standard GRL. In addition, we report on our recent experience on the modeling of aerodrome regulations. We demonstrate the usefulness of specifying families of goal models to address challenges associated with the maintenance of models used in the regulatory domain. We finally define and illustrate a new tool-supported algorithm used to evaluate individual goal models that are members of the larger family model.},
booktitle = {Proceedings of the 7th International Conference on System Analysis and Modeling: Theory and Practice},
pages = {34–52},
numpages = {19},
keywords = {URN, goal modeling, goal-oriented requirement language, key performance indicator, legal compliance, tools, variability},
location = {Innsbruck, Austria},
series = {SAM'12}
}

@inproceedings{10.1145/3597503.3639205,
author = {Zhang, Chenyangguang and Jia, Tong and Shen, Guopeng and Zhu, Pinyan and Li, Ying},
title = {MetaLog: Generalizable Cross-System Anomaly Detection from Logs with Meta-Learning},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639205},
doi = {10.1145/3597503.3639205},
abstract = {Log-based anomaly detection plays a crucial role in ensuring the stability of software. However, current approaches for log-based anomaly detection heavily depend on a vast amount of labeled historical data, which is often unavailable in many real-world systems. To mitigate this problem, we leverage the features of the abundant historical labeled logs of mature systems to help construct anomaly detection models of new systems with very few labels, that is, to generalize the model ability trained from labeled logs of mature systems to achieve anomaly detection on new systems with insufficient data labels. Specifically, we propose MetaLog, a generalizable cross-system anomaly detection approach. MetaLog first incorporates a globally consistent semantic embedding module to obtain log event semantic embedding vectors in a shared global space. Then it leverages the meta-learning paradigm to improve the model's generalization ability. We evaluate MetaLog's performance on four public log datasets (HDFS, BGL, OpenStack, and Thunderbird) from four different systems. Results show that MetaLog reaches over 80% F1-score when using only 1% labeled logs of the target system, showing similar performance with state-of-the-art supervised anomaly detection models trained with 100% labeled data. Besides, it outperforms state-of-art transfer-learning-based cross-system anomaly detection models by 20% in the same settings of 1% labeled training logs of the target system.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {154},
numpages = {12},
keywords = {meta-learning, anomaly detection, system logs},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@article{10.1145/3511021,
author = {Sharma, Prasen and Bisht, Ira and Sur, Arijit},
title = {Wavelength-based Attributed Deep Neural Network for Underwater Image Restoration},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3511021},
doi = {10.1145/3511021},
abstract = {Background: Underwater images, in general, suffer from low contrast and high color distortions due to the non-uniform attenuation of the light as it propagates through the water. In addition, the degree of attenuation varies with the wavelength, resulting in the asymmetric traversing of colors. Despite the prolific works for underwater image restoration (UIR) using deep learning, the above asymmetricity has not been addressed in the respective network engineering.Contributions: As the first novelty, this article shows that attributing the right receptive field size (context) based on the traversing range of the color channel may lead to a substantial performance gain for the task of UIR. Further, it is important to suppress the irrelevant multi-contextual features and increase the representational power of the model. Therefore, as a second novelty, we have incorporated an attentive skip mechanism to adaptively refine the learned multi-contextual features. The proposed framework, called Deep WaveNet, is optimized using the traditional pixel-wise and feature-based cost functions. An extensive set of experiments have been carried out to show the efficacy of the proposed scheme over existing best-published literature on benchmark datasets. More importantly, we have demonstrated a comprehensive validation of enhanced images across various high-level vision tasks, e.g., underwater image semantic segmentation and diver’s 2D pose estimation. A sample video to exhibit our real-world performance is available at . Also, we have open-sourced our framework at .},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jan,
articleno = {2},
numpages = {23},
keywords = {Image restoration, underwater vision, enhancement, super-resolution, deep learning}
}

@proceedings{10.1145/3698576,
title = {KISV '24: Proceedings of the 2nd Workshop on Kernel Isolation, Safety and Verification},
year = {2024},
isbn = {9798400713019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Austin, TX, USA}
}

@proceedings{10.1145/3599691,
title = {HotStorage '23: Proceedings of the 15th ACM Workshop on Hot Topics in Storage and File Systems},
year = {2023},
isbn = {9798400702242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Boston, MA, USA}
}

@article{10.1145/3507902,
author = {Du, Hang and Shi, Hailin and Zeng, Dan and Zhang, Xiao-Ping and Mei, Tao},
title = {The Elements of End-to-end Deep Face Recognition: A Survey of Recent Advances},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {10s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3507902},
doi = {10.1145/3507902},
abstract = {Face recognition (FR) is one of the most popular and long-standing topics in computer vision. With the recent development of deep learning techniques and large-scale datasets, deep face recognition has made remarkable progress and has been widely used in many real-world applications. Given a natural image or video frame as input, an end-to-end deep face recognition system outputs the face feature for recognition. To achieve this, a typical end-to-end system is built with three key elements: face detection, face alignment, and face representation. Face detection locates faces in the image or frame. Then, the face alignment is proceeded to calibrate the faces to the canonical view and crop them with a normalized pixel size. Finally, in the stage of face representation, the discriminative features are extracted from the aligned face for recognition. Nowadays, all of the three elements are fulfilled by the technique of deep convolutional neural network. In this survey article, we present a comprehensive review about the recent advance of each element of the end-to-end deep face recognition, since the thriving deep learning techniques have greatly improved their capability of them. To start with, we present an overview of the end-to-end deep face recognition. Then, we review the advance of each element, respectively, covering many aspects such as the to-date algorithm designs, evaluation metrics, datasets, performance comparison, existing challenges, and promising directions for future research. Also, we provide a detailed discussion about the effect of each element on its subsequent elements and the holistic system. Through this survey, we wish to bring contributions in two aspects: first, readers can conveniently identify the methods which are quite strong-baseline style in the subcategory for further exploration; second, one can also employ suitable methods for establishing a state-of-the-art end-to-end face recognition system from scratch.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {212},
numpages = {42},
keywords = {Deep learning, convolutional neural network, face recognition, face detection, face alignment, face representation}
}

@article{10.1109/TASLP.2024.3449037,
author = {Pawlak, Alan and Lee, Hyunkook and M\"{a}kivirta, Aki and Lund, Thomas},
title = {Spatial Analysis and Synthesis Methods: Subjective and Objective Evaluations Using Various Microphone Arrays in the Auralization of a Critical Listening Room},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3449037},
doi = {10.1109/TASLP.2024.3449037},
abstract = {Parametric sound field reproduction methods, such as the Spatial Decomposition Method (SDM) and Higher-Order Spatial Impulse Response Rendering (HO-SIRR), are widely used for the analysis and auralization of sound fields. This paper studies the performance of various sound field reproduction methods in the context of the auralization of a critical listening room, focusing on fixed head orientations. The influence on the perceived spatial and timbral fidelity of the following factors is considered: the rendering framework, direction of arrival (DOA) estimation method, microphone array structure, and use of a dedicated center reference microphone with SDM. Listening tests compare the synthesized sound fields to a reference binaural rendering condition, all for static head positions. Several acoustic parameters are measured to gain insights into objective differences between methods. All systems were distinguishable from the reference in perceptual tests. A high-quality pressure microphone improves the SDM framework's timbral fidelity, and spatial fidelity in certain scenarios. Additionally, SDM and HO-SIRR show similarities in spatial fidelity. Performance variation between SDM configurations is influenced by the DOA estimation method and microphone array construction. The binaural SDM (BSDM) presentations display temporal artifacts impacting sound quality.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {3986–4001},
numpages = {16}
}

@article{10.1145/3389397,
author = {Lu, Hong and Yue, Tao and Ali, Shaukat},
title = {Pattern-based Interactive Configuration Derivation for Cyber-physical System Product Lines},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2378-962X},
url = {https://doi.org/10.1145/3389397},
doi = {10.1145/3389397},
abstract = {Deriving a Cyber-Physical System (CPS) product from a product line requires configuring hundreds to thousands of configurable parameters of components and devices from multiple domains, e.g., computing, control, and communication. A fully automated configuration process for a CPS product line is seldom possible in practice, and a dynamic and interactive process is expected. Therefore, some configurable parameters are to be configured manually, and the rest can be configured either automatically or manually, depending on pre-defined constraints, the order of configuration steps, and previous configuration data in such a dynamic and interactive configuration process. In this article, we propose a pattern-based, interactive configuration derivation methodology (named as Pi-CD) to maximize opportunities of automatically deriving correct configurations of CPSs by benefiting from pre-defined constraints and configuration data of previous configuration steps. Pi-CD requires architectures of CPS product lines modeled with Unified Modeling Language extended with four types of variabilities, along with constraints specified in Object Constraint Language (OCL). Pi-CD is equipped with 324 configuration derivation patterns that we defined by systematically analyzing the OCL constructs and semantics. We evaluated Pi-CD by configuring 20 CPS products of varying complexity from two real-world CPS product lines. Results show that Pi-CD can achieve up to 72% automation degree with a negligible time cost. Moreover, its time performance remains stable with the increase in the number of configuration parameters as well as constraints.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = jun,
articleno = {44},
numpages = {24},
keywords = {Product line engineering, configuration derivation, object constraint language, product configuration}
}

@article{10.1109/TASLP.2024.3356980,
author = {\"{O}zer, Yigitcan and M\"{u}ller, Meinard},
title = {Source Separation of Piano Concertos Using Musically Motivated Augmentation Techniques},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3356980},
doi = {10.1109/TASLP.2024.3356980},
abstract = {In this work, we address the novel and rarely considered source separation task of decomposing piano concerto recordings into separate piano and orchestral tracks. Being a genre written for a pianist typically accompanied by an ensemble or orchestra, piano concertos often involve an intricate interplay of the piano and the entire orchestra, leading to high spectro–temporal correlations between the constituent instruments. Moreover, in the case of piano concertos, the lack of multi-track data for training constitutes another challenge in view of data-driven source separation approaches. As a basis for our work, we adapt existing deep learning (DL) techniques, mainly used for the separation of popular music recordings. In particular, we investigate spectrogram- and waveform-based approaches as well as hybrid models operating in both spectrogram and waveform domains. As a main contribution, we introduce a musically motivated data augmentation approach for training based on artificially generated samples. Furthermore, we systematically investigate the effects of various augmentation techniques for DL-based models. For our experiments, we use a recently published, open-source dataset of multi-track piano concerto recordings. Our main findings demonstrate that the best source separation performance is achieved by a hybrid model when combining all augmentation techniques.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1214–1225},
numpages = {12}
}

@article{10.1145/3294054,
author = {Choi, Young-Kyu and Cong, Jason and Fang, Zhenman and Hao, Yuchen and Reinman, Glenn and Wei, Peng},
title = {In-Depth Analysis on Microarchitectures of Modern Heterogeneous CPU-FPGA Platforms},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/3294054},
doi = {10.1145/3294054},
abstract = {Conventional homogeneous multicore processors are not able to provide the continued performance and energy improvement that we have expected from past endeavors. Heterogeneous architectures that feature specialized hardware accelerators are widely considered a promising paradigm for resolving this issue. Among different heterogeneous devices, FPGAs that can be reconfigured to accelerate a broad class of applications with orders-of-magnitude performance/watt gains, are attracting increased attention from both academia and industry. As a consequence, a variety of CPU-FPGA acceleration platforms with diversified microarchitectural features have been supplied by industry vendors. Such diversity, however, poses a serious challenge to application developers in selecting the appropriate platform for a specific application or application domain.This article aims to address this challenge by determining which microarchitectural characteristics affect performance, and in what ways. Specifically, we conduct a quantitative comparison and an in-depth analysis on five state-of-the-art CPU-FPGA acceleration platforms: (1) the Alpha Data board and (2) the Amazon F1 instance that represent the traditional PCIe-based platform with private device memory; (3) the IBM CAPI that represents the PCIe-based system with coherent shared memory; (4) the first generation of the Intel Xeon+FPGA Accelerator Platform that represents the QPI-based system with coherent shared memory; and (5) the second generation of the Intel Xeon+FPGA Accelerator Platform that represents a hybrid PCIe-based (non-coherent) and QPI-based (coherent) system with shared memory. Based on the analysis of their CPU-FPGA communication latency and bandwidth characteristics, we provide a series of insights for both application developers and platform designers. Furthermore, we conduct two case studies to demonstrate how these insights can be leveraged to optimize accelerator designs. The microbenchmarks used for evaluation have been released for public use.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = feb,
articleno = {4},
numpages = {20},
keywords = {AWS F1, CAPI, CPU-FPGA platform, Heterogeneous computing, Xeon+FPGA}
}

@article{10.1145/3482968,
author = {Tang, Yan and Cui, Weilong and Su, Jianwen},
title = {A Query Language for Workflow Logs},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3482968},
doi = {10.1145/3482968},
abstract = {A business process (workflow) is an assembly of tasks to accomplish a business goal. Real-world workflow models often demanded to change due to new laws and policies, changes in the environment, and so on. To understand the inner workings of a business process to facilitate changes, workflow logs have the potential to enable inspecting, monitoring, diagnosing, analyzing, and improving the design of a complex workflow. Querying workflow logs, however, is still mostly an ad hoc practice by workflow managers. In this article, we focus on the problem of querying workflow log concerning both control flow and dataflow properties. We develop a query language based on “incident patterns” to allow the user to directly query workflow logs instead of having to transform such queries into database operations. We provide the formal semantics and a query evaluation algorithm of our language. By deriving an accurate cost model, we develop an optimization mechanism to accelerate query evaluation. Our experiment results demonstrate the effectiveness of the optimization and achieves up to 50\texttimes{} speedup over an adaption of existing evaluation method.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {16},
numpages = {28},
keywords = {Business workflow, query languages, log}
}

@article{10.1145/3631120,
author = {Zhang, Guoming and Ji, Xiaoyu and Zhou, Xinyan and Qi, Donglian and Xu, Wenyuan},
title = {Ultrasound Communication Using the Nonlinearity Effect of Microphone Circuits in Smart Devices},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1550-4859},
url = {https://doi.org/10.1145/3631120},
doi = {10.1145/3631120},
abstract = {Acoustic communication has become a research focus without requiring extra hardware and facilitates numerous near-field applications such as mobile payment. To communicate, existing researchers use either an audible frequency band or an inaudible one. The former gains a high throughput but endures being audible, which can be annoying to users. The latter, although inaudible, falls short in throughput due to the available (near) ultrasonic bandwidth. In this article, we achieve both high speed and inaudibility for acoustic communication by utilizing the nonlinearity effect on microphones. We theoretically prove the maximum throughput of inaudible acoustic communication by modulating an audible signal onto an ultrasonic band. Then, we design and implement UltraComm, which utilizes a specially designed OFDM scheme. The scheme takes into account the characteristics of the nonlinear speaker-to-microphone channel, aiming to mitigate the effects of signal distortion. We evaluate UltraComm on different mobile devices and achieve throughput as high as 16.24 kbps.},
journal = {ACM Trans. Sen. Netw.},
month = feb,
articleno = {53},
numpages = {22},
keywords = {Ultrasound, microphone, inaudible acoustic communication, nonlinearity}
}

@inproceedings{10.1145/3626246.3653387,
author = {Zhang, Yupu and Cong, Guanglin and Qu, Jihan and Xu, Ran and Fu, Yuan and Li, Weiqi and Hu, Feiran and Liu, Jing and Zhang, Wenliang and Zheng, Kai},
title = {ESTELLE: An Efficient and Cost-effective Cloud Log Engine},
year = {2024},
isbn = {9798400704222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626246.3653387},
doi = {10.1145/3626246.3653387},
abstract = {With the advancement of cloud computing, more and more enterprises are adopting cloud services to build a variety of applications. Monitoring and observability are integral to the complex and fragile cloud-native architecture. As an extremely important data source for both, logs play an indispensable role in applications such as code debugging, root cause analysis, troubleshooting, and trend analysis. However, the inherent characteristic of cloud logs, with TB-level daily data production per user and continuous growth over time and with business, poses core challenges for log engines. Traditional log management systems are inadequate for handling the requirements of massive log data high-frequency writing and storage, along with low-frequency retrieval and analysis in cloud environments. Exploring a low-cost, high-performance cloud-native log engine solution is an extremely extraordinary challenging task. To tackle these challenges, we propose a cost-effective cloud-native log engine, called ESTELLE, equipped with a low-cost pluggable log index framework. This engine features a compute-storage separation and read-write separation architecture, enabling linear scalability. We designed a near-lock-free writing process for handling high-frequency writing demands of massive logs. Object storage is used to significantly reduce storage costs. We also tailored ESTELLE Log Bloom filter and approximate inverted index for this cloud-native engine, applying them flexibly to enhance query efficiency and optimize various queries. Extensive experiments on real open-source log datasets have demonstrated that the ESTELLE Log Engine achieves ultra-high single-core CPU write speeds and pretty low storage costs. Furthermore, when equipped with the complete index framework, it also maintains fairly low query latency across various log scenarios.},
booktitle = {Companion of the 2024 International Conference on Management of Data},
pages = {201–213},
numpages = {13},
keywords = {bloom filter, cloud-native, cost-effectiveness, index framework, log engine},
location = {Santiago AA, Chile},
series = {SIGMOD '24}
}

@inproceedings{10.1145/1028664.1028675,
author = {Spinczyk, Olaf and Beuche, Danilo},
title = {Modeling and building software product lines with eclipse},
year = {2004},
isbn = {1581138334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1028664.1028675},
doi = {10.1145/1028664.1028675},
booktitle = {Companion to the 19th Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages, and Applications},
pages = {18–19},
numpages = {2},
keywords = {embedded systems, model-driven software development, software product lines, variant management},
location = {Vancouver, BC, CANADA},
series = {OOPSLA '04}
}

@proceedings{10.1145/3593013,
title = {FAccT '23: Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chicago, IL, USA}
}

@proceedings{10.1145/3544549,
title = {CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hamburg, Germany}
}

@inproceedings{10.1145/3653946.3653965,
author = {Ugwu, Cynthia Ifeyinwa and Casarin, Sofia and Lanz, Oswald},
title = {Spatiotemporal Modeling Encounters 3D Medical Image Analysis: Slice-Shift UNet with Multi-View Fusion},
year = {2024},
isbn = {9798400716553},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3653946.3653965},
doi = {10.1145/3653946.3653965},
abstract = {As a fundamental part of computational healthcare, Computer Tomography (CT) and Magnetic Resonance Imaging (MRI) provide volumetric data, making the development of algorithms for 3D image analysis a necessity. Despite being computationally cheap, 2D Convolutional Neural Networks can only extract spatial information. In contrast, 3D CNNs can extract three-dimensional features, but they have higher computational costs and latency, which is a limitation for clinical practice that requires fast and efficient models. Inspired by the field of video action recognition we propose a new 2D-based model dubbed Slice SHift UNet (SSH-UNet) which encodes three-dimensional features at 2D CNN’s complexity. More precisely multi-view features are collaboratively learned by performing 2D convolutions along the three orthogonal planes of a volume and imposing a weights-sharing mechanism. The third dimension, which is neglected by the 2D convolution, is reincorporated by shifting a portion of the feature maps along the slices’ axis. The effectiveness of our approach is validated in Multi-Modality Abdominal Multi-Organ Segmentation (AMOS) and Multi-Atlas Labeling Beyond the Cranial Vault (BTCV) datasets, showing that SSH-UNet is more efficient while on par in performance with state-of-the-art architectures.},
booktitle = {Proceedings of the 2024 7th International Conference on Machine Vision and Applications},
pages = {126–133},
numpages = {8},
keywords = {healthcare, neural networks, segmentation, volumetric data},
location = {Singapore, Singapore},
series = {ICMVA '24}
}

@inproceedings{10.1145/2897010.2897011,
author = {Fischer, Stefan and Lopez-Herrejon, Roberto E. and Ramler, Rudolf and Egyed, Alexander},
title = {A preliminary empirical assessment of similarity for combinatorial interaction testing of software product lines},
year = {2016},
isbn = {9781450341660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897010.2897011},
doi = {10.1145/2897010.2897011},
abstract = {Extensive work on Search-Based Software Testing for Software Product Lines has been published in the last few years. Salient among them is the use of similarity as a surrogate metric for t-wise coverage whenever higher strengths are needed or whenever the size of the test suites is infeasible because of technological or budget limitations. Though promising, this metric has not been assessed with real fault data. In this paper, we address this limitation by using Drupal, a widely used open source web content management system, as an industry-strength case study for which both variability information and fault data have been recently made available. Our preliminary assessment corroborates some of the previous findings but also raises issues on some assumptions and claims made. We hope our work encourages further empirical evaluations of Combinatorial Interaction Testing approaches for Software Product Lines.},
booktitle = {Proceedings of the 9th International Workshop on Search-Based Software Testing},
pages = {15–18},
numpages = {4},
location = {Austin, Texas},
series = {SBST '16}
}

@proceedings{10.1145/3660515,
title = {EICS '24 Companion: Companion Proceedings of the 16th ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
year = {2024},
isbn = {9798400706516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Cagliari, Italy}
}

@inproceedings{10.1145/1054943.1054948,
author = {Briggs, Fay\'{e} and Chittor, Suresh and Cheng, Kai},
title = {Micro-architecture techniques in the intel® E8870 scalable memory controller},
year = {2004},
isbn = {159593040X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1054943.1054948},
doi = {10.1145/1054943.1054948},
abstract = {This paper describes several selected micro-architectural tradeoffs and optimizations for the scalable memory controller of the Intel E8870 chipset architecture. The Intel E8870 chipset architecture supports scalable coherent multiprocessor systems using 2 to 16 processors, and a point-to-point Scalability Port (SP) Protocol. The scalable memory controller micro-architecture applies a number of micro-architecture techniques to reduce the local &amp; remote idle and loaded latencies. The performance optimizations were achieved within the constraints of maintaining functional correctness, while reducing implementation complexity and cost. High bandwidth point-to-point interconnects and distributed memory are expected to be more common in future platforms to support powerful multi-core processors. The selected techniques discussed in this paper will be applicable to scalable memory controllers needed in those platforms. These techniques have been proven for production systems for the Itanium® II Processor platforms.},
booktitle = {Proceedings of the 3rd Workshop on Memory Performance Issues: In Conjunction with the 31st International Symposium on Computer Architecture},
pages = {30–36},
numpages = {7},
keywords = {distributed coherency, memory latency, scalability, transaction flows},
location = {Munich, Germany},
series = {WMPI '04}
}

@proceedings{10.1145/3544548,
title = {CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hamburg, Germany}
}

@inproceedings{10.1145/3664647.3680783,
author = {Jin, Haoyuan and Nie, Xuesong and Yan, Yunfeng and Chen, Xi and Zhu, Zhihang and Qi, Donglian},
title = {Object-Level Pseudo-3D Lifting for Distance-Aware Tracking},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3680783},
doi = {10.1145/3664647.3680783},
abstract = {Multi-object tracking (MOT) is a pivotal task for media interpretation, where reliable motion and appearance cues are essential for cross-frame identity preservation. However, limited by the inherent perspective properties of 2D space, the crowd density and frequent occlusions in real-world scenes expose the fragility of these cues. We observe the natural advantage of objects being well-separated in high-dimensional space and propose a novel 2D MOT framework, "Detecting-Lifting-Tracking'' (DLT). Initially, a pre-trained detector is employed to capture 2D object information. Secondly, we introduce a Mamba Distance Estimator to obtain the distances of objects to a monocular camera with temporal consistency, achieving object-level pseudo-3D lifting. Finally, we thoroughly explore distance-aware tracking via pseudo-3D information. Specifically, we introduce a Score-Distance Hierarchical Matching and Short-Long Terms Association to enhance accurate and robust association capability. Even without appearance cues, our DLT achieves state-of-the-art performance on MOT17, MOT20, and DanceTrack, demonstrating its potential to address occlusion challenges.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {8015–8023},
numpages = {9},
keywords = {distance-aware tracking, media interpretation, multi-object tracking, pseudo-3d lifting},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@inproceedings{10.1145/3239372.3239397,
author = {Ballar\'{\i}n, Manuel and Marc\'{e}n, Ana C. and Pelechano, Vicente and Cetina, Carlos},
title = {Measures to report the Location Problem of Model Fragment Location},
year = {2018},
isbn = {9781450349499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239372.3239397},
doi = {10.1145/3239372.3239397},
abstract = {Model Fragment Location (MFL) aims at identifying model elements that are relevant to a requirement, feature, or bug. Many MFL approaches have been introduced in the last few years to address the identification of the model elements that correspond to a specific functionality. However, there is a lack of detail when the measurements about the search space (models) and the measurements about the solution to be found (model fragment) are reported. Generally, the only reported measure is the model size. In this paper, we propose using five measurements (size, volume, density, multiplicity, and dispersion) to report the location problems. These measurements are the result of analyzing 1,308 MFLs in a family of industrial models over the last four years. Using two MFL approaches, we emphasize the importance of these measurements in order to compare results. Our work not only proposes improving the reporting of the location problem, but it also provides real measurements of location problems that are useful to other researchers in the design of synthetic location problems.},
booktitle = {Proceedings of the 21th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
pages = {189–199},
numpages = {11},
keywords = {Bug Location, Feature Location, Model Fragment Location, Traceability Link Recovery},
location = {Copenhagen, Denmark},
series = {MODELS '18}
}

@inproceedings{10.1145/3466933.3466961,
author = {Xex\'{e}o, Geraldo and Mangeli, Eduardo and Silva, Farmy and Ouriques, Leandro and Costa, Luis Felipe Coimbra and Monclar, Rafael Studart},
title = {Games as Information Systems},
year = {2021},
isbn = {9781450384919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466933.3466961},
doi = {10.1145/3466933.3466961},
abstract = {In this work, we formally build the construct of games as information systems. Starting from models and definitions that describe them, we present models that show game elements: components, players, environment, and their relations; as information systems elements. This paper also presents the analysis of five games and their elements based on that premise. As a ratification of our rationale, we present tools and visions shared by both fields. The presented premise and rationale could be easily linked to video games but, moreover, they are valid to all forms of games like tabletop games or sports.},
booktitle = {Proceedings of the XVII Brazilian Symposium on Information Systems},
articleno = {28},
numpages = {8},
keywords = {board game, card game, game design, games, information systems, video game},
location = {Uberl\^{a}ndia, Brazil},
series = {SBSI '21}
}

@inproceedings{10.1145/2749469.2750397,
author = {Akin, Berkin and Franchetti, Franz and Hoe, James C.},
title = {Data reorganization in memory using 3D-stacked DRAM},
year = {2015},
isbn = {9781450334020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2749469.2750397},
doi = {10.1145/2749469.2750397},
abstract = {In this paper we focus on common data reorganization operations such as shuffle, pack/unpack, swap, transpose, and layout transformations. Although these operations simply relocate the data in the memory, they are costly on conventional systems mainly due to inefficient access patterns, limited data reuse and roundtrip data traversal throughout the memory hierarchy. This paper presents a two pronged approach for efficient data reorganization, which combines (i) a proposed DRAM-aware reshape accelerator integrated within 3D-stacked DRAM, and (ii) a mathematical framework that is used to represent and optimize the reorganization operations.We evaluate our proposed system through two major use cases. First, we demonstrate the reshape accelerator in performing a physical address remapping via data layout transform to utilize the internal parallelism/locality of the 3D-stacked DRAM structure more efficiently for general purpose workloads. Then, we focus on offloading and accelerating commonly used data reorganization routines selected from the Intel Math Kernel Library package. We evaluate the energy and performance benefits of our approach by comparing it against existing optimized implementations on state-of-the-art GPUs and CPUs. For the various test cases, in-memory data reorganization provides orders of magnitude performance and energy efficiency improvements via low overhead hardware.},
booktitle = {Proceedings of the 42nd Annual International Symposium on Computer Architecture},
pages = {131–143},
numpages = {13},
location = {Portland, Oregon},
series = {ISCA '15}
}

@inproceedings{10.1145/3302506.3310407,
author = {Shih, Oliver and Rowe, Anthony},
title = {Can a phone hear the shape of a room?},
year = {2019},
isbn = {9781450362849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302506.3310407},
doi = {10.1145/3302506.3310407},
abstract = {Understanding the location of acoustically reflective surfaces in a room is a critical component in advanced sound processing. For example, intelligent speakers can use a room's acoustic geometry to improve playback quality, source separation accuracy, and speech recognition. In this paper, we present Synesthesia, a system for capturing the acoustic properties of a room using a single fixed speaker and a mobile phone that records audio at multiple locations. Using the arrival time of echoes, the system is able to reconstruct the position of reflective surfaces like walls and then estimate properties like surface absorption.Previous work has shown how the acoustic room impulse response (RIR) of an environment can be used to analyze echoes within a space to reconstruct room geometry. The best current RIR-based approaches rely on high-end equipment and capturing an acoustic signal broadcast into space from a known fixed constellation of microphones. They also require the precise calibration and measurement of microphone positions. In addition, most approaches pose constraints on room geometries and limit the order of RIR to achieve accurate and consistent results. In this paper, we introduce a new approach that performs RIR imaging using a mobile phone that tracks its location with visual inertial odometry (VIO) to record a dense set of samples albeit with noise in their locations. We present a new approach that is able to relax several key assumptions on RIR and show through both experimentation and simulation that even with 20cm of uncertainty in the microphone locations provided by VIO, we are still able to reconstruct the room geometry with accurate shape and dimensions. We demonstrate this capability by prototyping a tool for acoustic engineers, that allows a user to view a room's estimated geometry and absorption overlaid on the actual sensed space with augmented reality.},
booktitle = {Proceedings of the 18th International Conference on Information Processing in Sensor Networks},
pages = {277–288},
numpages = {12},
keywords = {active acoustic sensing, room reconstruction and mapping},
location = {Montreal, Quebec, Canada},
series = {IPSN '19}
}

@inproceedings{10.1145/3510466.3510483,
author = {Ananieva, Sofia and Greiner, Sandra and Krueger, Jacob and Linsbauer, Lukas and Gruener, Sten and Kehrer, Timo and Kuehn, Thomas and Seidl, Christoph and Reussner, Ralf},
title = {Unified Operations for Variability in Space and Time},
year = {2022},
isbn = {9781450396042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510466.3510483},
doi = {10.1145/3510466.3510483},
abstract = {Software and systems engineering is challenged by variability in space (concurrent variations at a single point in time) and time (sequential variations due to evolution). Managing both dimensions of variability independently is cumbersome and error-prone. A common foundation for operations on these dimensions is still missing, hampering the comparison and integration of existing techniques coping with variability in space and time as well as the design of new ones. In this paper, we address this problem by systematically identifying, categorizing, and unifying operations from contemporary tools and extending them to cope with both variability dimensions. Based on our gained insights, we identify gaps and trade-offs in current tools for managing variability in space and time, and discuss open challenges. The unified operations establish a common foundation that helps researchers and practitioners to gain a deeper understanding of existing techniques and tools for managing variability in space and/or time, analyze and compare them, and design new ones.},
booktitle = {Proceedings of the 16th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {7},
numpages = {10},
keywords = {product lines, variability, version control},
location = {Florence, Italy},
series = {VaMoS '22}
}

@proceedings{10.1145/3631726,
title = {WUWNet '23: Proceedings of the 17th International Conference on Underwater Networks &amp; Systems},
year = {2023},
isbn = {9798400716744},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Shenzhen, China}
}

@inproceedings{10.1145/3180155.3180163,
author = {Guo, Jianmei and Shi, Kai},
title = {To preserve or not to preserve invalid solutions in search-based software engineering: a case study in software product lines},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180163},
doi = {10.1145/3180155.3180163},
abstract = {Multi-objective evolutionary algorithms (MOEAs) have been successfully applied for software product lines (SPLs) to search for optimal or near-optimal solutions that balance multiple objectives. However, MOEAs usually produce invalid solutions that violate the constraints predefined. As invalid solutions are unbuildable in practice, we debate the preservation of invalid solutions during the search. We conduct experiments on seven real-world SPLs, including five largest SPLs hitherto reported and two SPLs with realistic values and constraints of quality attributes. We identify three potential limitations of preserving invalid solutions. Furthermore, based on the state-of-the-art, we design five algorithm variants that adopt different evolutionary operators. By performance evaluation, we provide empirical guidance on how to preserve valid solutions. Our empirical study demonstrates that whether or not to preserve invalid solutions deserves more attention in the community, and in some cases, we have to preserve valid solutions all along the way.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1027–1038},
numpages = {12},
keywords = {constraint solving, multi-objective evolutionary algorithms, search-based software engineering, software product lines, validity},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3641519.3657439,
author = {Zhang, Haonan and Guo, Jie and Zhang, Jiawei and Qin, Haoyu and Feng, Zesen and Yang, Ming and Guo, Yanwen},
title = {Deep Fourier-based Arbitrary-scale Super-resolution for Real-time Rendering},
year = {2024},
isbn = {9798400705250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641519.3657439},
doi = {10.1145/3641519.3657439},
abstract = {As a prevailing tool for effectively reducing rendering costs in many graphical applications, frame super-resolution has seen important progress in recent years. However, most of prior works designed for rendering contents face a common limitation: once a model is trained, it can only afford a single fixed scale. In this paper, we attempt to eliminate this limitation by supporting arbitrary-scale super-resolution for a trained neural model. The key is a Fourier-based implicit neural representation which maps arbitrary and naturally coordinates in the high-resolution spatial domain to valid pixel values. By observing that high-resolution G-buffers possess similar spectrum to high-resolution rendered frames, we design a High-Frequency Fourier Mapping (HFFM) module to recover fine details from low-resolution inputs, without introducing noticeable artifacts. A Low-Frequency Residual Learning (LFRL) strategy is adopted to preserve low-frequency structures and ensure low biasedness caused by network inference. Moreover, different rendering contents are well separated by our spatial-temporal masks derived from G-buffers and motion vectors. Several light-weight designs to the neural network guarantee the real-time performance on a wide range of scenes.},
booktitle = {ACM SIGGRAPH 2024 Conference Papers},
articleno = {65},
numpages = {11},
keywords = {Fourier, Implicit neural representation, Real-time rendering, Super-resolution},
location = {Denver, CO, USA},
series = {SIGGRAPH '24}
}

@inproceedings{10.1007/978-3-642-31087-4_78,
author = {Yazidi, Anis and Granmo, Ole-Christoffer and Oommen, B. John and Goodwin, Morten},
title = {A hierarchical learning scheme for solving the stochastic point location problem},
year = {2012},
isbn = {9783642310867},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-31087-4_78},
doi = {10.1007/978-3-642-31087-4_78},
abstract = {This paper deals with the Stochastic-Point Location (SPL) problem. It presents a solution which is novel in both philosophy and strategy to all the reported related learning algorithms. The SPL problem concerns the task of a Learning Mechanism attempting to locate a point on a line. The mechanism interacts with a random environment which essentially informs it, possibly erroneously, if the unknown parameter is on the left or the right of a given point which also is the current guess. The first pioneering work [6] on the SPL problem presented a solution which operates a one-dimensional controlled Random Walk (RW) in a discretized space to locate the unknown parameter. The primary drawback of the latter scheme is the fact that the steps made are always very conservative. If the step size is decreased the scheme yields a higher accuracy, but the convergence speed is correspondingly decreased.In this paper we introduce the Hierarchical Stochastic Searching on the Line (HSSL) solution. The HSSL solution is shown to provide orders of magnitude faster convergence when compared to the original SPL solution reported in [6]. The heart of the HSSL strategy involves performing a controlled RW on a discretized space, which unlike the traditional RWs, is not structured on the line per se, but rather on a binary tree described by intervals on the line. The overall learning scheme is shown to be optimal if the effectiveness of the environment, p, is greater than the golden ratio conjugate [4] --- which, in itself, is a very intriguing phenomenon. The solution has been both analytically analyzed and simulated, with extremely fascinating results. The strategy presented here can be utilized to determine the best parameter to be used in any optimization problem, and also in any application where the SPL can be applied [6].},
booktitle = {Proceedings of the 25th International Conference on Industrial Engineering and Other Applications of Applied Intelligent Systems: Advanced Research in Applied Artificial Intelligence},
pages = {774–783},
numpages = {10},
keywords = {controlled random walk, discretized learning, learning automata, stochastic-point problem},
location = {Dalian, China},
series = {IEA/AIE'12}
}

@inproceedings{10.1145/3448016.3457281,
author = {Zheng, Kaiping and Chen, Gang and Herschel, Melanie and Ngiam, Kee Yuan and Ooi, Beng Chin and Gao, Jinyang},
title = {PACE: Learning Effective Task Decomposition for Human-in-the-loop Healthcare Delivery},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457281},
doi = {10.1145/3448016.3457281},
abstract = {Human-in-the-loop data analysis involves both machine learning models and humans in analytic tasks. In healthcare applications, human-in-the-loop data analysis is crucial in that the model can handle "easy" tasks and hand over "hard" ones to medical experts for assistance and medical judgment, where easy tasks are the ones for which the model can provide high accuracy and hard tasks vice versa. In this process, how to decompose tasks in an effective manner is an important stage. To achieve task decomposition, classification with a reject option is a solution. However, existing studies either directly implement a reject option or dive into the theoretical details of the rejection mechanism. Different from such studies, we aim to optimize general classifiers with a reject option and hence, optimize task decomposition for healthcare applications.To this end, we first introduce task decomposition for healthcare applications, which is a crucial stage in human-in-the-loop healthcare delivery. We then devise a framework PACE to learn effective task decomposition concentrating on delivering high performance on the easy tasks. PACE is two-level: on the macro level, PACE employs the Self-Paced Learning method to select easy tasks for each training iteration; on the micro level, PACE adapts the weights of selected tasks through its weighted loss revision strategy. Experimental results in two real-world healthcare datasets show that PACE outperforms baselines in terms of their performance on the easy tasks which are expected to be solved by the learning model.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2156–2168},
numpages = {13},
keywords = {healthcare, human-in-the-loop, task decomposition},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@inproceedings{10.1145/1188455.1188575,
author = {Franchetti, Franz and Voronenko, Yevgen and P\"{u}schel, Markus},
title = {FFT program generation for shared memory: SMP and multicore},
year = {2006},
isbn = {0769527000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1188455.1188575},
doi = {10.1145/1188455.1188575},
abstract = {The chip maker's response to the approaching end of CPU frequency scaling are multicore systems, which offer the same programming paradigm as traditional shared memory platforms but have different performance characteristics. This situation considerably increases the burden on library developers and strengthens the case for automatic performance tuning frameworks like Spiral, a program generator and optimizer for linear transforms such as the discrete Fourier transform (DFT). We present a shared memory extension of Spiral. The extension within Spiral consists of a rewriting system that manipulates the structure of transform algorithms to achieve load balancing and avoids false sharing, and of a backend to generate multithreaded code. Application to the DFT produces a novel class of algorithms suitable for multicore systems as validated by experimental results: we demonstrate a parallelization speed-up already for sizes that fit into L1 cache and compare favorably to other DFT libraries across all small and midsize DFTs and considered platforms.},
booktitle = {Proceedings of the 2006 ACM/IEEE Conference on Supercomputing},
pages = {115–es},
keywords = {automatic parallelization, chip multiprocessor, fast fourier transform, multicore, shared memory},
location = {Tampa, Florida},
series = {SC '06}
}

@article{10.1145/3711897,
author = {Fjelde, Tor Erlend and Xu, Kai and Widmann, David and Tarek, Mohamed and Pfiffer, Cameron and Trapp, Martin and Axen, Seth D. and Sun, Xianda and Hauru, Markus and Yong, Penelope and Tebbutt, Will and Ghahramani, Zoubin and Ge, Hong},
title = {Turing.jl: a general-purpose probabilistic programming language},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711897},
doi = {10.1145/3711897},
abstract = {Probabilistic programming languages (PPLs) are becoming increasingly important in many scientific disciplines, such as economics, epidemiology, and biology, to extract meaning from sources of data while accounting for one's uncertainty. The key idea of probabilistic programming is to decouple inference and model specification, thus allowing the practitioner to approach their task at hand using Bayesian inference, without requiring extensive knowledge in programming or computational statistics. At the same time, the complexity of problem settings in which PPLs are employed is steadily increasing, both in terms of project size and model complexity, calling for more flexible and efficient systems.In this work, we describe Turing.jl, a general-purpose PPL, which is designed to be flexible, efficient, and easy to use. Turing.jl is built on top of the Julia programming language, which is known for its high performance and ease-of-use. We describe the design of Turing.jl, contextualizing it within different types of users and use cases, its key features, and how it can be used to solve a wide range of problems. We also provide a brief overview of the ecosystem around Turing.jl, including the different libraries and tools that can be used in conjunction with it. Finally, we provide a few examples of how Turing.jl can be used in practice.},
note = {Just Accepted},
journal = {ACM Trans. Probab. Mach. Learn.},
month = feb,
keywords = {Probabilistic Programming, Probabilistic Programming Languages, Probabilistic Inference, Bayesian Inference, Markov Chain Monte Carlo, Variational Inference, Sequential Monte Carlo, Uncertainty Quantification, Modeling Methodologies, Latent Variable Models, Maximum a Posteriori Modeling, Software Libraries and Repositories, Bayesian Computation, Variational Methods, Sequential Monte Carlo Methods}
}

@inproceedings{10.1145/2976767.2976789,
author = {Font, Jaime and Arcega, Lorena and Haugen, \O{}ystein and Cetina, Carlos},
title = {Feature location in models through a genetic algorithm driven by information retrieval techniques},
year = {2016},
isbn = {9781450343213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976767.2976789},
doi = {10.1145/2976767.2976789},
abstract = {In this work we propose a feature location approach that targets models as the feature realization artifacts. The approach combines Genetic Algorithms and Information Retrieval techniques. Given a model and a feature description, model fragments extracted from the model are evolved using genetic operations. Then, Formal Concept Analysis is used to cluster the model fragments based on their common attributes into feature realization candidates. Finally, Latent Semantic Analysis is used to rank the candidates based on the similarity with the feature description. As a result, the genetic algorithm evolves the population of model fragments to find the set of most suitable feature realizations. We have evaluated the approach with an industrial case study, locating features with precision and recall values around 90% (baseline obtains less than 40%). Finally, we provide recommendations on how to provide the input to the approach to improve the location of features over the models.},
booktitle = {Proceedings of the ACM/IEEE 19th International Conference on Model Driven Engineering Languages and Systems},
pages = {272–282},
numpages = {11},
location = {Saint-malo, France},
series = {MODELS '16}
}

@inproceedings{10.1145/3691620.3695268,
author = {Liao, Haoyu and Guo, Jianmei and Huang, Bo and Han, Yujie and Yang, Dingyu and Shi, Kai and Ding, Jonathan and Xu, Guoyao and Yang, Guodong and Zhang, Liping},
title = {DeployFix: Dynamic Repair of Software Deployment Failures via Constraint Solving},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695268},
doi = {10.1145/3691620.3695268},
abstract = {Software deployment misconfiguration often happens and has been one of the major causes of deployment failures that give rise to service interruptions. However, there is currently no existing approach to automatically repairing deployment failures. We propose DeployFix, which automatically repairs software deployment failures via constraint solving in the dynamic-changing deployment environments. DeployFix first defines DeployIR as a unified intermediate representation to achieve the translation of heterogeneous specifications from different schedulers with different syntaxes. By reducing the root-cause analysis of deployment failures to the conflict resolution in propositional logic, DeployFix uses off-the-shelf constraint solvers to achieve automatic localization and diagnosis of conflicting constraints, which are the root causes of deployment failures. DeployFix finally resolves the conflicting constraints and generates repaired deployment configurations in terms of practical requirements. We evaluate DeployFix in both simulation and production environments with tens of thousands of nodes at Alibaba, on which tens of thousands of applications are running guided by hundreds of thousands of deployment constraints. Experimental results demonstrate that DeployFix outperforms the state of the art and it correctly repairs the deployment failures in minutes, even in a large production data center.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {2053–2064},
numpages = {12},
keywords = {cloud computing, deployment failures, dynamic repair, constraint solving},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/253495.253518,
author = {Ito, Tetsuro and Yu, Clement T.},
title = {Optimization of a hierarchical file organization for spelling correction},
year = {1985},
isbn = {0897911598},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/253495.253518},
doi = {10.1145/253495.253518},
abstract = {A spelling program using a hierarchically organized file seems to be promising, since it can correct more than common typing mistakes. However, its speed of detecting spelling errors in the inputs is rather slow. Here some techniques of modifying the program to improve the speed are presented.},
booktitle = {Proceedings of the 8th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {131–137},
numpages = {7},
location = {Montreal, Quebec, Canada},
series = {SIGIR '85}
}

@inproceedings{10.5555/3466184.3466446,
author = {Rodriguez, Brodderick and Yilmaz, Levent},
title = {Learning rule-based explanatory models from exploratory multi-simulation for decision-support under uncertainty},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Exploratory modeling and simulation is an effective strategy when there are substantial contextual uncertainty and representational ambiguity in problem formulation. However, two significant challenges impede the use of an ensemble of models in exploratory simulation. The first challenge involves streamlining the maintenance and synthesis of multiple models from plausible features that are identified from and subject to the constraints of the research hypothesis. The second challenge is making sense of the data generated by multi-simulation over a model ensemble. To address both challenges, we introduce a computational framework that integrates feature-driven variability management with an anticipatory learning classifier system to generate explanatory rules from multi-simulation data.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2293–2304},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@inproceedings{10.1145/2642937.2642939,
author = {Segura, Sergio and S\'{a}nchez, Ana B. and Ruiz-Cort\'{e}s, Antonio},
title = {Automated variability analysis and testing of an E-commerce site.: an experience report},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2642939},
doi = {10.1145/2642937.2642939},
abstract = {In this paper, we report on our experience on the development of La Hilandera, an e-commerce site selling haberdashery products and craft supplies in Europe. The store has a huge input space where customers can place almost three millions of different orders which made testing an extremely difficult task. To address the challenge, we explored the applicability of some of the practices for variability management in software product lines. First, we used a feature model to represent the store input space which provided us with a variability view easy to understand, share and discuss with all the stakeholders. Second, we used techniques for the automated analysis of feature models for the detection and repair of inconsistent and missing configuration settings. Finally, we used test selection and prioritization techniques for the generation of a manageable and effective set of test cases. Our findings, summarized in a set of lessons learnt, suggest that variability techniques could successfully address many of the challenges found when developing e-commerce sites.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {139–150},
numpages = {12},
keywords = {automated testing, e-commerce, experience report, feature modelling, variability},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@inproceedings{10.1145/3560905.3568499,
author = {Sun, Yimiao and Wang, Weiguo and Mottola, Luca and Wang, Ruijin and He, Yuan},
title = {AIM: Acoustic Inertial Measurement for Indoor Drone Localization and Tracking},
year = {2023},
isbn = {9781450398862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560905.3568499},
doi = {10.1145/3560905.3568499},
abstract = {We present Acoustic Inertial Measurement (AIM), a one-of-a-kind technique for indoor drone localization and tracking. Indoor drone localization and tracking are arguably a crucial, yet unsolved challenge: in GPS-denied environments, existing approaches enjoy limited applicability, especially in Non-Line of Sight (NLoS), require extensive environment instrumentation, or demand considerable hardware/software changes on drones. In contrast, AIM exploits the acoustic characteristics of the drones to estimate their location and derive their motion, even in NLoS settings. We tame location estimation errors using a dedicated Kalman filter and the Interquartile Range rule (IQR). We implement AIM using an off-the-shelf microphone array and evaluate its performance with a commercial drone under varied settings. Results indicate that the mean localization error of AIM is 46% lower than commercial UWB-based systems in complex indoor scenarios, where state-of-the-art infrared systems would not even work because of NLoS settings. We further demonstrate that AIM can be extended to support indoor spaces with arbitrary ranges and layouts without loss of accuracy by deploying distributed microphone arrays.},
booktitle = {Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems},
pages = {476–488},
numpages = {13},
keywords = {acoustic signal, drone, indoor tracking},
location = {Boston, Massachusetts},
series = {SenSys '22}
}

@inproceedings{10.1109/ASE51524.2021.9678827,
author = {Wang, Bo and Lu, Sirui and Xiong, Yingfei and Liu, Feng},
title = {Faster mutation analysis with fewer processes and smaller overheads},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678827},
doi = {10.1109/ASE51524.2021.9678827},
abstract = {Mutation analysis is a powerful dynamic approach that has many applications, such as measuring the quality of test suites or automatically locating faults. However, the inherent low scalability hampers its practical use. To accelerate mutation analysis, researchers propose approaches to reduce redundant executions. A family of fork-based approaches tries to share identical executions among mutants. Fork-based approaches carry all mutants in one process and decide whether to fork new child processes when reaching a mutated statement. The mutants carried by the parent process are split into groups and distributed to different processes to finish the remaining executions. However, existing fork-based approaches have two limitations: (1) the limited analysis scope on a single statement to compare and cluster mutants prevents their systems from detecting more equivalent mutants, and (2) the interpretation of the mutants and the runtime equivalence analysis introduce significant overhead.In this paper, we present a novel fork-based mutation analysis approach WinMut, which (1) groups mutants in a scope of mutated statements and, (2) removes redundant computations inside interpreters. WinMut not only reduces the number of invoked processes but also has a lower cost for executing a single process. Our experiments show that our approach can further accelerate mutation analysis with an average speedup of 5.57x on top of the state-of-the-art fork-based approach, AccMut.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {381–393},
numpages = {13},
keywords = {dynamic analysis, fork-based mutation analysis, mutation analysis, mutation testing, software testing},
location = {Melbourne, Australia},
series = {ASE '21}
}

@article{10.1109/TASLP.2021.3053388,
author = {Kelly, Finnian and Hansen, John H.L.},
title = {Analysis and Calibration of Lombard Effect and Whisper for Speaker Recognition},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3053388},
doi = {10.1109/TASLP.2021.3053388},
abstract = {Variations in vocal effort can create challenges for speaker recognition systems that are optimized for use with neutral speech. The Lombard effect and whisper are two commonly-occurring forms of vocal effort variation that result in non-neutral speech, the first due to noise exposure and the second due to intentional adjustment on the part of the speaker. In this article, a comparative evaluation of speaker recognition performance in non-neutral conditions is presented using multiple Lombard effect and whisper corpora. The detrimental impact of these vocal effort variations on discrimination and calibration performance on global, per-corpus, and per-speaker levels is explored using conventional error metrics, along with visual representations of the model and score spaces. A non-neutral speech detector is subsequently introduced and used to inform score calibration in several ways. Two calibration approaches are proposed and shown to reduce error to the same level as an optimal calibration approach that relies on ground-truth vocal effort information. This article contributes a generalizable methodology towards detecting vocal effort variation and using this knowledge to inform and advance speaker recognition system behavior.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {927–942},
numpages = {16}
}

@article{10.1145/1101530.1101538,
author = {Brungart, Douglas S. and Simpson, Brian D.},
title = {Optimizing the spatial configuration of a seven-talker speech display},
year = {2005},
issue_date = {October 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {1544-3558},
url = {https://doi.org/10.1145/1101530.1101538},
doi = {10.1145/1101530.1101538},
abstract = {Although there is substantial evidence that performance in multitalker listening tasks can be improved by spatially separating the apparent locations of the competing talkers, very little effort has been made to determine the best locations and presentation levels for the talkers in a multichannel speech display. In this experiment, a call sign based color and number identification task was used to evaluate the effectiveness of three different spatial configurations and two different level normalization schemes in a seven-channel binaural speech display. When only two spatially adjacent channels of the seven-channel system were active, overall performance was substantially better with a geometrically spaced spatial configuration (with far-field talkers at −90°, −30°, −10°, 0°, +10°, +30°, and +90° azimuth) or a hybrid near-far configuration (with far-field talkers at −90°, −30°, 0°, +30°, and +90° azimuth and near-field talkers at ±90°) than with a more conventional linearly spaced configuration (with far-field talkers at −90°, −60°, −30°, 0°, +30°, +60°, and +90° azimuth). When all seven channels were active, performance was generally better with a “better-ear” normalization scheme that equalized the levels of the talkers in the more intense ear than with a default normalization scheme that equalized the levels of the talkers at the center of the head. The best overall performance in the seven-talker task occurred when the hybrid near-far spatial configuration was combined with the better-ear normalization scheme. This combination resulted in a 20% increase in the number of correct identifications relative to the baseline condition with linearly spaced talker locations and no level normalization. Although this is a relatively modest improvement, it should be noted that it could be achieved at little or no cost simply by reconfiguring the HRTFs used in a multitalker speech display.},
journal = {ACM Trans. Appl. Percept.},
month = oct,
pages = {430–436},
numpages = {7},
keywords = {Cocktail party effect, informational masking}
}

@article{10.1145/3610534,
author = {Peng, Yunjie and Wu, Jinlin and Xu, Boqiang and Cao, Chunshui and Liu, Xu and Sun, Zhenan and He, Zhiqiang},
title = {Deep Learning Based Occluded Person Re-Identification: A Survey},
year = {2023},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1551-6857},
url = {https://doi.org/10.1145/3610534},
doi = {10.1145/3610534},
abstract = {Occluded person re-identification (Re-ID) focuses on addressing the occlusion problem when retrieving the person of interest across non-overlapping cameras. With the increasing demand for intelligent video surveillance and the application of person Re-ID technology, the real-world occlusion problem draws considerable interest from researchers. Although a large number of occluded person Re-ID methods have been proposed, there are few surveys that focus on occlusion. To fill this gap and help boost future research, this article provides a systematic survey of occluded person Re-ID. In this work, we review recent deep learning based occluded person Re-ID research. First, we summarize the main issues caused by occlusion as four groups: position misalignment, scale misalignment, noisy information, and missing information. Second, we categorize existing methods into six solution groups: matching, image transformation, multi-scale features, attention mechanism, auxiliary information, and contextual recovery. We also discuss the characteristics of each approach, as well as the issues they address. Furthermore, we present the performance comparison of recent occluded person Re-ID methods on four public datasets: Partial-ReID, Partial-iLIDS, Occluded-ReID, and Occluded-DukeMTMC. We conclude the study with thoughts on promising future research directions.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = oct,
articleno = {73},
numpages = {27},
keywords = {Occluded person re-identification, partial person re-identification, literature survey, deep learning}
}

@proceedings{10.1145/3632971,
title = {JCRAI '23: Proceedings of the 2023 International Joint Conference on Robotics and Artificial Intelligence},
year = {2023},
isbn = {9798400707704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Shanghai, China}
}

@article{10.1145/3571854,
author = {Zampetti, Fiorella and Tamburri, Damian and Panichella, Sebastiano and Panichella, Annibale and Canfora, Gerardo and Di Penta, Massimiliano},
title = {Continuous Integration and Delivery Practices for Cyber-Physical Systems: An Interview-Based Study},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3571854},
doi = {10.1145/3571854},
abstract = {Continuous Integration and Delivery (CI/CD) practices have shown several benefits for software development and operations, such as faster release cycles and early discovery of defects. For Cyber-Physical System (CPS) development, CI/CD can help achieving required goals, such as high dependability, yet it may be challenging to apply. This article empirically investigates challenges, barriers, and their mitigation occurring when applying CI/CD practices to develop CPSs in 10 organizations working in eight different domains. The study has been conducted through semi-structured interviews, by applying an open card sorting procedure together with a member-checking survey within the same organizations, and by validating the results through a further survey involving 55 professional developers. The study reveals several peculiarities in the application of CI/CD to CPSs. These include the need for (i) combining continuous and periodic builds while balancing the use of Hardware-in-the-Loop and simulators, (ii) coping with difficulties in software deployment (iii) accounting for simulators and Hardware-in-the-Loop differing in their behavior, and (vi) combining hardware/software expertise in the development team. Our findings open the road toward recommenders aimed at supporting the setting and evolution of CI/CD pipelines, as well as university curricula requiring interdisciplinarity, such as knowledge about hardware, software, and their interplay.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {73},
numpages = {44},
keywords = {Continuous Integration and Delivery, Cyber-Physical Systems, empirical software engineering}
}

@proceedings{10.1145/3551349,
title = {ASE '22: Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
year = {2022},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rochester, MI, USA}
}

@proceedings{10.1145/3567512,
title = {SLE 2022: Proceedings of the 15th ACM SIGPLAN International Conference on Software Language Engineering},
year = {2022},
isbn = {9781450399197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 15th ACM SIGPLAN International Conference on Software Language Engineering (SLE), co-located with the ACM SIGPLAN conference on Systems, Programming, Languages, and Applications (SPLASH) in Auckland, a vibrant port city in northern New Zealand, from December 5th to December 10th 2022. Like its predecessors, the this edition of the SLE conference, SLE 2022, is devoted to the principles of software languages: their design, their implementation, and their evolution. As such, SLE brings together researchers united by their common interest in the creation, capture, and tooling of software languages.},
location = {Auckland, New Zealand}
}

@article{10.5555/2591248.2591263,
author = {Androutsopoulos, Ion and Lampouras, Gerasimos and Galanis, Dimitrios},
title = {Generating natural language descriptions from OWL ontologies: the natural OWL system},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {We present Naturalowl, a natural language generation system that produces texts describing individuals or classes of owl ontologies. Unlike simpler owl verbalizers, which typically express a single axiom at a time in controlled, often not entirely fluent natural language primarily for the benefit of domain experts, we aim to generate fluent and coherent multi-sentence texts for end-users. With a system like Naturalowl, one can publish information in owl on the Web, along with automatically produced corresponding texts in multiple languages, making the information accessible not only to computer programs and domain experts, but also end-users. We discuss the processing stages of Naturalowl, the optional domain-dependent linguistic resources that the system can use at each stage, and why they are useful. We also present trials showing that when the domain-dependent linguistic resources are available, Naturalowl produces significantly better texts compared to a simpler verbalizer, and that the resources can be created with relatively light effort.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {671–715},
numpages = {45}
}

@inproceedings{10.5555/2387636.2387713,
author = {Espl\`{a}-Gomis, Miquel and S\'{a}nchez-Mart\'{\i}nez, Felipe and Forcada, Mikel L.},
title = {UAlacant: using online machine translation for cross-lingual textual entailment},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This paper describes a new method for cross-lingual textual entailment (CLTE) detection based on machine translation (MT). We use sub-segment translations from different MT systems available online as a source of cross-lingual knowledge. In this work we describe and evaluate different features derived from these sub-segment translations, which are used by a support vector machine classifier to detect CLTEs. We presented this system to the SemEval 2012 task 8 obtaining an accuracy up to 59.8% on the English-Spanish test set, the second best performing approach in the contest.},
booktitle = {Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation},
pages = {472–476},
numpages = {5},
location = {Montr\'{e}al, Canada},
series = {SemEval '12}
}

@article{10.1145/2658993,
author = {Nowatzki, Tony and Sartin-Tarm, Michael and De Carli, Lorenzo and Sankaralingam, Karthikeyan and Estan, Cristian and Robatmili, Behnam},
title = {A Scheduling Framework for Spatial Architectures Across Multiple Constraint-Solving Theories},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {0164-0925},
url = {https://doi.org/10.1145/2658993},
doi = {10.1145/2658993},
abstract = {Spatial architectures provide energy-efficient computation but require effective scheduling algorithms. Existing heuristic-based approaches offer low compiler/architect productivity, little optimality insight, and low architectural portability.We seek to develop a spatial-scheduling framework by utilizing constraint-solving theories and find that architecture primitives and scheduler responsibilities can be related through five abstractions: computation placement, data routing, event timing, resource utilization, and the optimization objective. We encode these responsibilities as 20 mathematical constraints, using SMT and ILP, and create schedulers for the TRIPS, DySER, and PLUG architectures. Our results show that a general declarative approach using constraint solving is implementable, is practical, and can outperform specialized schedulers.},
journal = {ACM Trans. Program. Lang. Syst.},
month = nov,
articleno = {2},
numpages = {30},
keywords = {Satisfiability Modulo Theories, Spatial architectures, integer linear programming, spatial architecture scheduling}
}

@article{10.5555/3455716.3455773,
author = {Ma, Fan and Meng, Deyu and Dong, Xuanyi and Yang, Yi},
title = {Self-paced multi-view co-training},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Co-training is a well-known semi-supervised learning approach which trains classifiers on two or more different views and exchanges pseudo labels of unlabeled instances in an iterative way. During the co-training process, pseudo labels of unlabeled instances are very likely to be false especially in the initial training, while the standard co-training algorithm adopts a "draw without replacement" strategy and does not remove these wrongly labeled instances from training stages. Besides, most of the traditional co-training approaches are implemented for two-view cases, and their extensions in multi-view scenarios are not intuitive. These issues not only degenerate their performance as well as available application range but also hamper their fundamental theory. Moreover, there is no optimization model to explain the objective a co-training process manages to optimize. To address these issues, in this study we design a unified self-paced multi-view co-training (SPamCo) framework which draws unlabeled instances with replacement. Two specified co-regularization terms are formulated to develop different strategies for selecting pseudo-labeled instances during training. Both forms share the same optimization strategy which is consistent with the iteration process in co-training and can be naturally extended to multi-view scenarios. A distributed optimization strategy is also introduced to train the classifier of each view in parallel to further improve the efficiency of the algorithm. Furthermore, the SPamCo algorithm is proved to be PAC learnable, supporting its theoretical soundness. Experiments conducted on synthetic, text categorization, person re-identification, image recognition and object detection data sets substantiate the superiority of the proposed method.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {57},
numpages = {38},
keywords = {co-training, self-paced learning, multi-view learning, semi-supervised learning, ε-expansion theory, probably approximately correct learnable}
}

@proceedings{10.1145/3634737,
title = {ASIA CCS '24: Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to ACM AsiaCCS 2024, the 19th ACM Asia Conference on Computer and Communications Security. AsiaCCS 2024 takes place in Singapore from 1 July to 5 July.},
location = {Singapore, Singapore}
}

@article{10.1109/TASLP.2022.3190727,
author = {Opinto, Alessandro and Martal\`{o}, Marco and Costalunga, Alessandro and Strozzi, Nicol\`{o} and Tripodi, Carlo and Raheli, Riccardo},
title = {Experimental Analysis and Design Guidelines for Microphone Virtualization in Automotive Scenarios},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3190727},
doi = {10.1109/TASLP.2022.3190727},
abstract = {In this article, a performance analysis on the estimation of the so-called observation filter for the Virtual Microphone Technique (VMT) in a realistic automotive environment is presented. A performance comparison between adaptive and fixed observation filter estimation methods, namely Least Mean Square (LMS) and Minimum Mean Square Error (MMSE), respectively, was carried on. Two different experimental setups were implemented on a popular B-segment car. Eight microphones were placed at the monitoring and virtual positions in order to sense environmental acoustic noise propagating within the cabin of the car running at variable speed on a smooth asphalt. Our experimental results show that a large spectral coherence between monitoring and virtual microphone signals indicates a potentially effective and relatively wide-band virtual microphone signal reconstruction. The fixed observation filter estimation method achieves better performance than the adaptive one, guaranteeing remarkable broadband estimation accuracy. Moreover, for each considered setup, design guidelines are proposed to obtain a good trade-off between estimation accuracy and material costs.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {2337–2346},
numpages = {10}
}

@article{10.1109/TNET.2019.2925658,
author = {Liaskos, Christos and Tsioliaridou, Ageliki and Nie, Shuai and Pitsillides, Andreas and Ioannidis, Sotiris and Akyildiz, Ian F.},
title = {On the Network-Layer Modeling and Configuration of Programmable Wireless Environments},
year = {2019},
issue_date = {August 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2019.2925658},
doi = {10.1109/TNET.2019.2925658},
abstract = {Programmable wireless environments enable the software-defined propagation of waves within them, yielding exceptional performance. Several building-block technologies have been implemented and evaluated at the physical layer in the past. The present work contributes a network-layer solution to configure such environments for multiple users and objectives, and for any underlying physical-layer technology. Supported objectives include any combination of Quality of Service and power transfer optimization, eavesdropping, and Doppler effect mitigation, in multi-cast or uni-cast settings. In addition, a graph-based model of programmable environments is proposed, which incorporates core physical observations and efficiently separates physical and networking concerns. The evaluation takes place in a specially developed simulation tool, and in a variety of environments, validating the model and reaching insights into the user capacity of programmable environments.},
journal = {IEEE/ACM Trans. Netw.},
month = aug,
pages = {1696–1713},
numpages = {18}
}

@article{10.1145/3214263,
author = {Arora, Nivedita and Zhang, Steven L. and Shahmiri, Fereshteh and Osorio, Diego and Wang, Yi-Cheng and Gupta, Mohit and Wang, Zhengjun and Starner, Thad and Wang, Zhong Lin and Abowd, Gregory D.},
title = {SATURN: A Thin and Flexible Self-powered Microphone Leveraging Triboelectric Nanogenerator},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1145/3214263},
doi = {10.1145/3214263},
abstract = {We demonstrate the design, fabrication, evaluation, and use of a self-powered microphone that is thin, flexible, and easily manufactured. Our technology is referred to as a Self-powered Audio Triboelectric Ultra-thin Rollable Nanogenerator (SATURN) microphone. This acoustic sensor takes advantage of the triboelectric nanogenerator (TENG) to transform vibrations into an electric signal without applying an external power source. The sound quality of the SATURN mic, in terms of acoustic sensitivity, frequency response, and directivity, is affected by a set of design parameters that we explore based on both theoretical simulation and empirical evaluation. The major advantage of this audio material sensor is that it can be manufactured simply and deployed easily to convert every-day objects and physical surfaces into microphones which can sense audio. We explore the space of potential applications for such a material as part of a self-sustainable interactive system.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jul,
articleno = {60},
numpages = {28},
keywords = {TENG (Triboelectric Nanogenerator), Triboelectic effect, applications, flexible electronics, passive microphone}
}

@article{10.5555/1181901.1181950,
author = {Hunt, John M. and McGregor, John D.},
title = {Software product lines: a pedagogical application},
year = {2006},
issue_date = {December 2006},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {22},
number = {2},
issn = {1937-4771},
abstract = {This paper provides an overview of Software Product Lines and discusses issues involved in using Software Product Lines in courses. An SPL designs and produces common assets for a group of related products as a family; rather then building the products one at a time. SPL has been successful in delivering 80% to 100% reuse. SPL is moving out of an early adaptor phase and into mainstream reuse, which should increase industry demand for developers familiar with SPL. Understanding SPL is best done with a complete example. We discuss a complete set of publicly available SPL related assets that we developed, and discuss their use in the classroom.},
journal = {J. Comput. Sci. Coll.},
month = dec,
pages = {295–302},
numpages = {8}
}

@article{10.1109/TCBB.2022.3225300,
author = {Dang, Qi and Liang, Yong and Ouyang, Dong and Miao, Rui and Ling, Caijin and Liu, Xiaoying and Xie, Shengli},
title = {Improved Computational Drug-Repositioning by Self-Paced Non-Negative Matrix Tri-Factorization},
year = {2022},
issue_date = {May-June 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2022.3225300},
doi = {10.1109/TCBB.2022.3225300},
abstract = {Drug repositioning (DR) is a strategy to find new targets for existing drugs, which plays an important role in reducing the costs, time, and risk of traditional drug development. Recently, the matrix factorization approach has been widely used in the field of DR prediction. Nevertheless, there are still two challenges: 1) Learning ability deficiencies, the model cannot accurately predict more potential associations. 2) Easy to fall into a bad local optimal solution, the model tends to get a suboptimal result. In this study, we propose a self-paced non-negative matrix tri-factorization (SPLNMTF) model, which integrates three types of different biological data from patients, genes, and drugs into a heterogeneous network through non-negative matrix tri-factorization, thereby learning more information to improve the learning ability of the model. In the meantime, the SPLNMTF model sequentially includes samples into training from easy (high-quality) to complex (low-quality) in the soft weighting way, which effectively alleviates falling into a bad local optimal solution to improve the prediction performance of the model. The experimental results on two real datasets of ovarian cancer and acute myeloid leukemia (AML) show that SPLNMTF outperforms the other eight state-of-the-art models and gets better prediction performance in drug repositioning. The data and source code are available at: &lt;uri&gt;https://github.com/qi0906/SPLNMTF&lt;/uri&gt;.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = nov,
pages = {1953–1962},
numpages = {10}
}

@inproceedings{10.1145/3037697.3037754,
author = {Churchill, Berkeley and Sharma, Rahul and Bastien, JF and Aiken, Alex},
title = {Sound Loop Superoptimization for Google Native Client},
year = {2017},
isbn = {9781450344654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3037697.3037754},
doi = {10.1145/3037697.3037754},
abstract = {Software fault isolation (SFI) is an important technique for the construction of secure operating systems, web browsers, and other extensible software. We demonstrate that superoptimization can dramatically improve the performance of Google Native Client, a SFI system that ships inside the Google Chrome Browser. Key to our results are new techniques for superoptimization of loops: we propose a new architecture for superoptimization tools that incorporates both a fully sound verification technique to ensure correctness and a bounded verification technique to guide the search to optimized code. In our evaluation we optimize 13 libc string functions, formally verify the correctness of the optimizations and report a median and average speedup of 25% over the libraries shipped by Google.},
booktitle = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {313–326},
numpages = {14},
keywords = {assembly, bounded verification, data-driven verification, equivalence checking, native client, superoptimization, verification, x86-64},
location = {Xi'an, China},
series = {ASPLOS '17}
}

@proceedings{10.1145/3549555,
title = {CBMI '22: Proceedings of the 19th International Conference on Content-based Multimedia Indexing},
year = {2022},
isbn = {9781450397209},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Graz, Austria}
}

@inproceedings{10.1145/3474085.3481541,
author = {Huang, Lianghua and Liu, Yu and Zhou, Xiangzeng and You, Ansheng and Li, Ming and Wang, Bin and Zhang, Yingya and Pan, Pan and Yinghui, Xu},
title = {Once and for All: Self-supervised Multi-modal Co-training on One-billion Videos at Alibaba},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3481541},
doi = {10.1145/3474085.3481541},
abstract = {Videos grow to be one of the largest mediums on the Internet. E-commerce platforms like Alibaba need to process millions of video data across multimedia (e.g., visual, audio, image, and text) and on a variety of tasks (e.g., retrieval, tagging, and summary) every day. In this work, we aim to develop a once and for all pretraining technique for diverse modalities and downstream tasks. To achieve this, we make the following contributions: (1) We propose a self-supervised multi-modal co-training framework. It takes cross-modal pseudo-label consistency as the supervision and can jointly learn representations of multiple modalities. (2) We introduce several novel techniques (e.g., sliding-window subset sampling, coarse-to-fine clustering, fast spatial-temporal convolution and parallel data transmission and processing) to optimize the training process, making billion-scale stable training feasible. (3) We construct a large-scale multi-modal dataset consisting of 1.4 billion videos (~0.5 PB) and train our framework on it. The training takes only 4.6 days on an in-house 256 GPUs cluster, and it simultaneously produces pretrained video, audio, image, motion, and text networks. (4) Finetuning from our pretrained models, we obtain significant performance gains and faster convergence on diverse multimedia tasks at Alibaba. Furthermore, we also validate the learned representation on public datasets. Despite the domain gap between our commodity-centric pretraining and the action-centric evaluation data, we show superior results against state-of-the-arts.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1148–1156},
numpages = {9},
keywords = {co-training, multi-modal, once and for all, self-supervised learning},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3377811.3380372,
author = {Lienhardt, Michael and Damiani, Ferruccio and Johnsen, Einar Broch and Mauro, Jacopo},
title = {Lazy product discovery in huge configuration spaces},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380372},
doi = {10.1145/3377811.3380372},
abstract = {Highly-configurable software systems can have thousands of interdependent configuration options across different subsystems. In the resulting configuration space, discovering a valid product configuration for some selected options can be complex and error prone. The configuration space can be organized using a feature model, fragmented into smaller interdependent feature models reflecting the configuration options of each subsystem.We propose a method for lazy product discovery in large fragmented feature models with interdependent features. We formalize the method and prove its soundness and completeness. The evaluation explores an industrial-size configuration space. The results show that lazy product discovery has significant performance benefits compared to standard product discovery, which in contrast to our method requires all fragments to be composed to analyze the feature model. Furthermore, the method succeeds when more efficient, heuristics-based engines fail to find a valid configuration.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1509–1521},
numpages = {13},
keywords = {Linux distribution, composition, configurable software, feature models, software product lines, variability modeling},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3643832.3661882,
author = {He, Juan and Xiong, Jie and Hu, Weihang and Feng, Chao and Yao, Enjie and Wang, Xiaojing and Liu, Chen and Chen, Xiaojiang},
title = {CW-AcousLen: A Configurable Wideband Acoustic Metasurface},
year = {2024},
isbn = {9798400705816},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643832.3661882},
doi = {10.1145/3643832.3661882},
abstract = {Acoustic metasurface was recently proposed to enhance the performance of acoustic communication and sensing. While promising, there are two issues hindering the adoption of acoustic metasurface for real-life usage. The first issue is that configurable metasurface is still expensive and unscalable. The second issue is that it is difficult for an acoustic metasurface to work in a large frequency range. In this paper, we present a wideband and configurable acoustic metasurface for the first time. We show that with a large number of metasurface elements, a cheap and simple two-state element design can achieve performance very close to that achieved by expensive continuous-state elements. We also fine-tune the geometric parameter of the element structure to support similar phase changes across a large frequency range, laying the foundation to enable wideband acoustic metasurface. Extensive experiments show that our system can achieve an average signal strength improvement of 7.5 dB and 10.5 dB in LoS and NLoS scenarios respectively with the help of a metasurface with a size of 17.6 \texttimes{} 17.6 cm. Two representative sensing applications (i.e., respiration sensing and gesture recognition) and one communication case study are employed to show the effectiveness of the metasurface.},
booktitle = {Proceedings of the 22nd Annual International Conference on Mobile Systems, Applications and Services},
pages = {29–41},
numpages = {13},
keywords = {acoustic metasurface, configurable environment, wideband beam-forming, acoustic sensing},
location = {Minato-ku, Tokyo, Japan},
series = {MOBISYS '24}
}

@inproceedings{10.1145/3613904.3642437,
author = {Yang, Yongjie and Chen, Tao and Huang, Yujing and Guo, Xiuzhen and Shangguan, Longfei},
title = {MAF: Exploring Mobile Acoustic Field for Hand-to-Face Gesture Interactions},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642437},
doi = {10.1145/3613904.3642437},
abstract = {We present MAF, a novel acoustic sensing approach that leverages the commodity hardware in bone conduction earphones for hand-to-face gesture interactions. Briefly, by shining audio signals with bone conduction earphones, we observe that these signals not only propagate along the surface of the human face but also dissipate into the air, creating an acoustic field that envelops the individual’s head. We conduct benchmark studies to understand how various hand-to-face gestures and human factors influence this acoustic field. Building on the insights gained from these initial studies, we then propose a deep neural network combined with signal preprocessing techniques. This combination empowers MAF to effectively detect, segment, and subsequently recognize a variety of hand-to-face gestures, whether in close contact with the face or above it. Our comprehensive evaluation based on 22 participants demonstrates that MAF achieves an average gesture recognition accuracy of 92% across ten different gestures tailored to users’ preferences.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {638},
numpages = {20},
keywords = {Acoustic Sensing, Gesture Detection, Wearable Computing},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.1145/3131608,
author = {Oulasvirta, Antti and Feit, Anna and L\"{a}hteenlahti, Perttu and Karrenbauer, Andreas},
title = {Computational Support for Functionality Selection in Interaction Design},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {5},
issn = {1073-0516},
url = {https://doi.org/10.1145/3131608},
doi = {10.1145/3131608},
abstract = {Designing interactive technology entails several objectives, one of which is identifying and selecting appropriate functionality. Given candidate functionalities such as “print,” “bookmark,” and “share,” a designer has to choose which functionalities to include and which to leave out. Such choices critically affect the acceptability, productivity, usability, and experience of the design. However, designers may overlook reasonable designs because there is an exponential number of functionality sets and multiple factors to consider. This article is the first to formally define this problem and propose an algorithmic method to support designers to explore alternative functionality sets in early stage design. Based on interviews of professional designers, we mathematically define the task of identifying functionality sets that strike the best balance among four objectives: usefulness, satisfaction, ease of use, and profitability. We develop an integer linear programming solution that can efficiently solve very large instances (set size over 1,300) on a regular computer. Further, we build on techniques of robust optimization to search for diverse and surprising functionality designs. Empirical results from a controlled study and field deployment are encouraging. Most designers rated computationally created sets to be of the comparable or superior quality than their own. Designers reported gaining better understanding of available functionalities and the design space.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = oct,
articleno = {34},
numpages = {30},
keywords = {Functionality selection, computer-supported design, creativity, design tools, integer linear programming, interaction design, optimization methods, user-centered design}
}

@inproceedings{10.1145/2593743.2593746,
author = {Sierszecki, Krzysztof and Steffens, Michaela and Fogdal, Thomas and Savolainen, Juha and Mikkonen, Tommi},
title = {Towards green power electronics: software controllers and domain knowledge},
year = {2014},
isbn = {9781450328449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593743.2593746},
doi = {10.1145/2593743.2593746},
abstract = {One of the key challenges of green software is that various aspects have an impact to the overall energy consumption over the lifetime of a system operated by software. In particular, in the field of industrial applications, where embedded devices cooperate with many IT systems to make the industrial processes more efficient, to reduce waste or raw materials, and to save the environment, the concept of green software becomes unclear. In this paper, we address the green aspects of software in different phases – software construction, software execution, and software control in both inside an individual component and as a part of a complete industrial application. Furthermore, we demonstrate that the insight into system knowledge, not aspects related to software per se, is the key to create truly green software. Consequently, when considering truly software green, the focus is to be placed on the system level savings for embedded systems at the highest possible level where domain knowledge can be taken into account, not on software development or execution.},
booktitle = {Proceedings of the 3rd International Workshop on Green and Sustainable Software},
pages = {17–22},
numpages = {6},
keywords = {Green software, embedded control systems, green systems, software development, software product lines, variable speed drives},
location = {Hyderabad, India},
series = {GREENS 2014}
}

@inproceedings{10.1145/2658761.2658767,
author = {Ruprecht, Andreas and Heinloth, Bernhard and Lohmann, Daniel},
title = {Automatic feature selection in large-scale system-software product lines},
year = {2014},
isbn = {9781450331616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658761.2658767},
doi = {10.1145/2658761.2658767},
abstract = {System software can typically be configured at compile time via a comfortable feature-based interface to tailor its functionality towards a specific use case. However, with the growing number of features, this tailoring process becomes increasingly difficult: As a prominent example, the Linux kernel in v3.14 provides nearly 14 000 configuration options to choose from. Even developers of embedded systems refrain from trying to build a minimized distinctive kernel configuration for their device – and thereby waste memory and money for unneeded functionality. In this paper, we present an approach for the automatic use-case specific tailoring of system software for special-purpose embedded systems. We evaluate the effectiveness of our approach on the example of Linux by generating tailored kernels for well-known applications of the Rasperry Pi and a Google Nexus 4 smartphone. Compared to the original configurations, our approach leads to memory savings of 15–70 percent and requires only very little manual intervention.},
booktitle = {Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences},
pages = {39–48},
numpages = {10},
keywords = {Feature Selection, Linux, Software Product Lines, Software Tailoring},
location = {V\"{a}ster\r{a}s, Sweden},
series = {GPCE 2014}
}

@inproceedings{10.1145/3581783.3612394,
author = {Jiang, Xun and Zhou, Zailei and Xu, Xing and Yang, Yang and Wang, Guoqing and Shen, Heng Tao},
title = {Faster Video Moment Retrieval with Point-Level Supervision},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612394},
doi = {10.1145/3581783.3612394},
abstract = {Video Moment Retrieval (VMR) aims at retrieving the most relevant events from an untrimmed video with natural language queries. Existing VMR methods suffer from two defects: (1) massive expensive temporal annotations are required to obtain satisfying performance; (2) complicated cross-modal interaction modules are deployed, which lead to high computational cost and low efficiency for the retrieval process. To address these issues, we propose a novel method termed Cheaper and Faster Moment Retrieval (CFMR), which balances the retrieval accuracy, efficiency, and annotation cost for VMR. Specifically, our proposed CFMR method learns from point-level supervision where each annotation is a single frame randomly located within the target moment. Such a labeling strategy achieves 6 times cheaper than the conventional annotations of event boundaries. Furthermore, we also design a concept-based multimodal alignment mechanism to bypass the usage of cross-modal interaction modules during the inference process, remarkably improving retrieval efficiency. The experimental results on three widely used VMR benchmarks demonstrate our proposed CFMR method achieves superior comprehensive performance to current state-of-the-art methods. Moreover, it significantly accelerates the retrieval speed with more than 100 times FLOPs compared to existing approaches with point-level supervision. Our open-source implementation is available at https://github.com/CFM-MSG/Code_CFMR.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {1334–1342},
numpages = {9},
keywords = {model efficiency, multimedia embedding, point-level supervised, video moment retrieval, video understanding},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3062341.3062369,
author = {Mamouras, Konstantinos and Raghothaman, Mukund and Alur, Rajeev and Ives, Zachary G. and Khanna, Sanjeev},
title = {StreamQRE: modular specification and efficient evaluation of quantitative queries over streaming data},
year = {2017},
isbn = {9781450349888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3062341.3062369},
doi = {10.1145/3062341.3062369},
abstract = {Real-time decision making in emerging IoT applications typically relies on computing quantitative summaries of large data streams in an efficient and incremental manner. To simplify the task of programming the desired logic, we propose StreamQRE, which provides natural and high-level constructs for processing streaming data. Our language has a novel integration of linguistic constructs from two distinct programming paradigms: streaming extensions of relational query languages and quantitative extensions of regular expressions. The former allows the programmer to employ relational constructs to partition the input data by keys and to integrate data streams from different sources, while the latter can be used to exploit the logical hierarchy in the input stream for modular specifications.  We first present the core language with a small set of combinators, formal semantics, and a decidable type system. We then show how to express a number of common patterns with illustrative examples. Our compilation algorithm translates the high-level query into a streaming algorithm with precise complexity bounds on per-item processing time and total memory footprint. We also show how to integrate approximation algorithms into our framework. We report on an implementation in Java, and evaluate it with respect to existing high-performance engines for processing streaming data. Our experimental evaluation shows that (1) StreamQRE allows more natural and succinct specification of queries compared to existing frameworks, (2) the throughput of our implementation is higher than comparable systems (for example, two-to-four times greater than RxJava), and (3) the approximation algorithms supported by our implementation can lead to substantial memory savings.},
booktitle = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {693–708},
numpages = {16},
keywords = {Quantitative Regular Expressions, data stream processing},
location = {Barcelona, Spain},
series = {PLDI 2017}
}

@inproceedings{10.1145/2047862.2047866,
author = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Pukall, Mario and Apel, Sven},
title = {Tailoring dynamic software product lines},
year = {2011},
isbn = {9781450306898},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047862.2047866},
doi = {10.1145/2047862.2047866},
abstract = {Software product lines (SPLs) and adaptive systems aim at variability to cope with changing requirements. Variability can be described in terms of features, which are central for development and configuration of SPLs. In traditional SPLs, features are bound statically before runtime. By contrast, adaptive systems support feature binding at runtime and are sometimes called dynamic SPLs (DSPLs). DSPLs are usually built from coarse-grained components, which reduces the number of possible application scenarios. To overcome this limitation, we closely integrate static binding of traditional SPLs and runtime adaptation of DSPLs. We achieve this integration by statically generating a tailor-made DSPL from a highly customizable SPL. The generated DSPL provides only the runtime variability required by a particular application scenario and the execution environment. The DSPL supports self-configuration based on coarse-grained modules. We provide a feature-based adaptation mechanism that reduces the effort of computing an optimal configuration at runtime. In a case study, we demonstrate the practicability of our approach and show that a seamless integration of static binding and runtime adaptation reduces the complexity of the adaptation process.},
booktitle = {Proceedings of the 10th ACM International Conference on Generative Programming and Component Engineering},
pages = {3–12},
numpages = {10},
keywords = {dynamic binding, feature-oriented programming, software product lines},
location = {Portland, Oregon, USA},
series = {GPCE '11}
}

@inproceedings{10.1145/1368088.1368131,
author = {K\"{a}stner, Christian and Apel, Sven and Kuhlemann, Martin},
title = {Granularity in software product lines},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1368088.1368131},
doi = {10.1145/1368088.1368131},
abstract = {Building software product lines (SPLs) with features is a challenging task. Many SPL implementations support features with coarse granularity - e.g., the ability to add and wrap entire methods. However, fine-grained extensions, like adding a statement in the middle of a method, either require intricate workarounds or obfuscate the base code with annotations. Though many SPLs can and have been implemented with the coarse granularity of existing approaches, fine-grained extensions are essential when extracting features from legacy applications. Furthermore, also some existing SPLs could benefit from fine-grained extensions to reduce code replication or improve readability. In this paper, we analyze the effects of feature granularity in SPLs and present a tool, called Colored IDE (CIDE), that allows features to implement coarse-grained and fine-grained extensions in a concise way. In two case studies, we show how CIDE simplifies SPL development compared to traditional approaches.},
booktitle = {Proceedings of the 30th International Conference on Software Engineering},
pages = {311–320},
numpages = {10},
keywords = {feature refactoring, ide, software product lines, virtual separation of concerns},
location = {Leipzig, Germany},
series = {ICSE '08}
}

@proceedings{10.1145/3674805,
title = {ESEM '24: Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
year = {2024},
isbn = {9798400710476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Barcelona, Spain}
}

@inproceedings{10.1145/3586183.3606775,
author = {Liu, Tiantian and Lin, Feng and Wang, Chao and Xu, Chenhan and Zhang, Xiaoyu and Li, Zhengxiong and Xu, Wenyao and Huang, Ming-Chun and Ren, Kui},
title = {WavoID: Robust and Secure Multi-modal User Identification via mmWave-voice Mechanism},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606775},
doi = {10.1145/3586183.3606775},
abstract = {With the increasing deployment of voice-controlled devices in homes and enterprises, there is an urgent demand for voice identification to prevent unauthorized access to sensitive information and property loss. However, due to the broadcast nature of sound wave, a voice-only system is vulnerable to adverse conditions and malicious attacks. We observe that the cooperation of millimeter waves (mmWave) and voice signals can significantly improve the effectiveness and security of user identification. Based on the properties, we propose a multi-modal user identification system (named WavoID) by fusing the uniqueness of mmWave-sensed vocal vibration and mic-recorded voice of users. To estimate fine-grained waveforms, WavoID splits signals and adaptively combines useful decomposed signals according to correlative contents in both mmWave and voice. An elaborated anti-spoofing module in WavoID comprising biometric bimodal information defend against attacks. WavoID produces and fuses the response maps of mmWave and voice to improve the representation power of fused features, benefiting accurate identification, even facing adverse circumstances. We evaluate WavoID using commercial sensors on extensive experiments. WavoID has significant performance on user identification with over 98% accuracy on 100 user datasets.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {64},
numpages = {15},
keywords = {User authentication, mmWave sensing, multi-modal fusion, voice identification},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@proceedings{10.1145/3607890,
title = {EMSOFT '23: Proceedings of the International Conference on Embedded Software},
year = {2023},
isbn = {9798400702914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {EMSOFT is the flagship conference of ACM SIGBED, the Special Interest Group on Embedded Systems, and a premier academic forum that brings together researchers and developers from academia, industry, and government to advance the science and practice of embedded software and systems. EMSOFT is held as part of ESWEEK, the annual highlight event of the embedded systems community. ESWEEK also features the International Conference on Compilers, Architectures, and Synthesis for Embedded Systems (CASES) and the International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS).},
location = {Hamburg, Germany}
}

@inproceedings{10.5555/2830865.2830876,
author = {Skelin, Mladen and Geilen, Marc and Catthoor, Francky and Hendseth, Sverre},
title = {Parametrized dataflow scenarios},
year = {2015},
isbn = {9781467380799},
publisher = {IEEE Press},
abstract = {The FSM-based scenario-aware dataflow (FSM-SADF) model of computation has been introduced to facilitate the analysis of dynamic streaming applications. FSM-SADF interprets application's execution as an execution of a sequence of static modes of operation called scenarios. Each scenario is modeled using a synchronous dataflow (SDF) graph (SDFG), while a finite-state machine (FSM) is used to encode scenario occurrence patterns. However, FSM-SADF can precisely capture only those dynamic applications whose behaviors can be abstracted into a reasonably sized set of scenarios (coarse-grained dynamism). Nevertheless, in many cases, the application may exhibit thousands or even millions of behaviours (fine-grained dynamism). In this work, we generalize the concept of FSM-SADF to one that is able to model dynamic applications exhibiting fine-grained dynamism. We achieve this by applying parametrization to the FSM-SADF's base model, i.e. SDF, and defining scenarios over parametrized SDFGs. We refer to the extension as parametrized FSM-SADF (PFSM-SADF). Thereafter, we present a novel and a fully parametric analysis technique that allows us to derive tight worst-case performance (throughput and latency) guarantees for PFSM-SADF specifications. We evaluate our approach on a realistic case-study from the multimedia domain.},
booktitle = {Proceedings of the 12th International Conference on Embedded Software},
pages = {95–104},
numpages = {10},
keywords = {max-plus algebra, parametrized dataflow, scenario-aware dataflow, synchronous dataflow, worst-case performance},
location = {Amsterdam, The Netherlands},
series = {EMSOFT '15}
}

@inproceedings{10.1145/3652620.3688199,
author = {F\"{o}ldi\'{a}k, M\'{a}t\'{e}},
title = {Probabilistic Graph Queries for Design Space Exploration Under Uncertainty},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688199},
doi = {10.1145/3652620.3688199},
abstract = {Critical cyber-physical systems have an increasingly significant role in the world, and ensuring their safety is a high priority objective. State of the art approaches and engineering tools can support the development process from very early stages, with high-level system modeling, analysis capabilities, and exploration of alternatives. However, these approaches are limited when it comes evaluation of complex extra-functional characteristics over designs with uncertainties, typical to early system designs. In my thesis project, I intend to introduce probabilistic graph queries for high level, scalable probabilistic analysis, for analysing system models with design uncertainty and applicable in design space exploration. The approach will be evaluated on external case studies, focusing on key performance metrics related to applicability in the target context, such as runtime, precision and formal guarantees.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {142–148},
numpages = {7},
keywords = {cyber-physical systems, probabilistic analysis, graph queries, design space exploration, design uncertainty, lifting, safety},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3377813.3381350,
author = {Ramanathan, Murali Krishna and Clapp, Lazaro and Barik, Rajkishore and Sridharan, Manu},
title = {Piranha: reducing feature flag debt at uber},
year = {2020},
isbn = {9781450371230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377813.3381350},
doi = {10.1145/3377813.3381350},
abstract = {Feature flags are commonly used in mobile app development and can introduce technical debt related to deleting their usage from the codebase. This can adversely affect the overall reliability of the apps and increase their maintenance complexity. Reducing this debt without imposing additional overheads on the developers necessitates the design of novel tools and automated workflows.In this paper, we describe the design and implementation of Piranha, an automated code refactoring tool which is used to automatically generate differential revisions (a.k.a diffs) to delete code corresponding to stale feature flags. Piranha takes as input the name of the flag, expected treatment behavior, and the name of the flag's author. It analyzes the ASTs of the program to generate appropriate refactorings which are packaged into a diff. The diff is assigned to the author of the flag for further processing, who can land it after performing any additional refactorings.We have implemented Piranha to delete code in Objective-C, Java, and Swift programs, and deployed it to handle stale flags in multiple Uber apps. We present our experiences with the deployment of Piranha from Dec 2017 to May 2019, including the following highlights: (a) generated code cleanup diffs for 1381 flags (17% of total flags), (b) 65% of the diffs landed without any changes, (c) over 85% of the generated diffs compile and pass tests successfully, (d) around 80% of the diffs affect more than one file, (e) developers process more than 88% of the generated diffs, (f) 75% of the generated diffs are processed within a week, and (g) Piranha diffs have been interacted with by ~200 developers across Uber.Piranha is available as open source at https://github.com/uber/piranha.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice},
pages = {221–230},
numpages = {10},
location = {Seoul, South Korea},
series = {ICSE-SEIP '20}
}

@article{10.1109/TASLP.2024.3459430,
author = {Meng, Weixin and Li, Xiaoyu and Li, Andong and Luo, Xiaoxue and Yan, Shefeng and Li, Xiaodong and Zheng, Chengshi},
title = {Deep Kronecker Product Beamforming for Large-Scale Microphone Arrays},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3459430},
doi = {10.1109/TASLP.2024.3459430},
abstract = {Although deep learning based beamformers have achieved promising performance using small microphone arrays, they suffer from performance degradation in very challenging environments, such as extremely low Signal-to-Noise Ratio (SNR) environments, e.g., SNR &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$le$&lt;/tex-math&gt;&lt;/inline-formula&gt;−10 dB. A large-scale microphone array with dozens or hundreds of microphones can improve the performance of beamformers in these challenging scenarios because of its high spatial resolution. While a dramatic increase in the number of microphones leads to feature redundancy, causing difficulties in feature extraction and network training. As an attempt to improve the performance of deep beamformers for speech extraction in very challenging scenarios, this paper proposes a novel all neural Kronecker product beamforming denoted by ANKP-BF for large-scale microphone arrays by taking the following two aspects into account. Firstly, a larger microphone array can provide higher performance of spatial filtering when compared with a small microphone array, and deep neural networks are introduced for their powerful non-linear modeling capability in the speech extraction task. Secondly, the feature redundancy problem is solved by introducing the Kronecker product rule to decompose the original one high-dimension weight vector into the Kronecker product of two much lower-dimensional weight vectors. The proposed ANKP-BF is designed to operate in an end-to-end manner. Extensive experiments are conducted on simulated large-scale microphone-array signals using the DNS-Challenge corpus and WSJ0-SI84 corpus, and the real recordings in a semi-anechoic room and outdoor scenes are also used to evaluate and compare the performance of different methods. Quantitative results demonstrate that the proposed method outperforms existing advanced baselines in terms of multiple objective metrics, especially in very low SNR environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {4537–4553},
numpages = {17}
}

@article{10.1145/3653714,
author = {Hu, Xiaobo and Lin, Youfang and Fan, Hehe and Wang, Shuo and Wu, Zhihao and Lv, Kai},
title = {Building Category Graphs Representation with Spatial and Temporal Attention for Visual Navigation},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {7},
issn = {1551-6857},
url = {https://doi.org/10.1145/3653714},
doi = {10.1145/3653714},
abstract = {Given an object of interest, visual navigation aims to reach the object’s location based on a sequence of partial observations. To this end, an agent needs to (1) acquire specific knowledge about the relations of object categories in the world during training and (2) locate the target object based on the pre-learned object category relations and its trajectory in the current unseen environment. In this article, we propose a Category Relation Graph (CRG) to learn the knowledge of object category layout relations and a Temporal-Spatial-Region attention (TSR) architecture to perceive the long-term spatial-temporal dependencies of objects, aiding navigation. We establish CRG to learn prior knowledge of object layout and deduce the positions of specific objects. Subsequently, we propose the TSR architecture to capture relationships among objects in temporal, spatial, and regions within observation trajectories. Specifically, we implement a Temporal attention module (T) to model the temporal structure of the observation sequence, implicitly encoding historical moving or trajectory information. Then, a Spatial attention module (S) uncovers the spatial context of the current observation objects based on CRG and past observations. Last, a Region attention module (R) shifts the attention to the target-relevant region. Leveraging the visual representation extracted by our method, the agent accurately perceives the environment and easily learns a superior navigation policy. Experiments on AI2-THOR demonstrate that our CRG-TSR method significantly outperforms existing methods in both effectiveness and efficiency. The supplementary material includes the code and will be publicly available.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = may,
articleno = {217},
numpages = {22},
keywords = {Object visual navigation, relation graph, attention network, reinforcement learning}
}

@article{10.1145/3472809,
author = {Zhai, Deming and Shi, Ruifeng and Jiang, Junjun and Liu, Xianming},
title = {Rectified Meta-learning from Noisy Labels for Robust Image-based Plant Disease Classification},
year = {2022},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3472809},
doi = {10.1145/3472809},
abstract = {Plant diseases serve as one of main threats to food security and crop production. It is thus valuable to exploit recent advances of artificial intelligence to assist plant disease diagnosis. One popular approach is to transform this problem as a leaf image classification task, which can be then addressed by the powerful convolutional neural networks (CNNs). However, the performance of CNN-based classification approach depends on a large amount of high-quality manually labeled training data, which inevitably introduce noise on labels in practice, leading to model overfitting and performance degradation. To overcome this problem, we propose a novel framework that incorporates rectified meta-learning module into common CNN paradigm to train a noise-robust deep network without using extra supervision information. The proposed method enjoys the following merits: (i) A rectified meta-learning is designed to pay more attention to unbiased samples, leading to accelerated convergence and improved classification accuracy. (ii) Our method is free on assumption of label noise distribution, which works well on various kinds of noise. (iii) Our method serves as a plug-and-play module, which can be embedded into any deep models optimized by gradient descent-based method. Extensive experiments are conducted to demonstrate the superior performance of our algorithm over the state-of-the-arts.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jan,
articleno = {30},
numpages = {17},
keywords = {Meta learning, noisy labels, plant disease classification}
}

@inproceedings{10.1145/3664647.3681248,
author = {Chen, Bolei and Kang, Jiaxu and Zhong, Ping and Liang, Yixiong and Sheng, Yu and Wang, Jianxin},
title = {Embodied Contrastive Learning with Geometric Consistency and Behavioral Awareness for Object Navigation},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3681248},
doi = {10.1145/3664647.3681248},
abstract = {Object Navigation (ObjcetNav), which enables an agent to seek any instance of an object category specified by a semantic label, has shown great advances. However, current agents are built upon occlusion-prone visual observations or compressed 2D semantic maps, which hinder their embodied perception of 3D scene geometry and easily lead to ambiguous object localization and blind exploration. To address these limitations, we present an Embodied Contrastive Learning (ECL) method with Geometric Consistency (GC) and Behavioral Awareness (BA), which motivates agents to actively encode 3D scene layouts and semantic cues. Driven by our embodied exploration strategy, BA is modeled by predicting navigational actions based on multi-frame visual images, as behaviors that cause differences between adjacent visual sensations are crucial for learning correlations among continuous visions. The GC is modeled as the alignment of behavior-aware visual stimulus with 3D semantic shapes by employing unsupervised contrastive learning. The aligned behavior-aware visual features and geometric invariance priors are injected into a modular ObjectNav framework to enhance object recognition and exploration capabilities. As expected, our ECL method performs well on object detection and instance segmentation tasks. Our ObjectNav strategy outperforms state-of-the-art methods on MP3D and Gibson datasets, showing the potential of our ECL in embodied navigation.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {4776–4785},
numpages = {10},
keywords = {behavioral awareness, contrastive representation learning, embodied ai, geometric consistency, object navigation},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@inproceedings{10.1145/3458864.3467880,
author = {Garg, Nakul and Bai, Yang and Roy, Nirupam},
title = {Owlet: enabling spatial information in ubiquitous acoustic devices},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3467880},
doi = {10.1145/3458864.3467880},
abstract = {This paper presents a low-power and miniaturized design for acoustic direction-of-arrival (DoA) estimation and source localization, called Owlet. The required aperture, power consumption, and hardware complexity of the traditional array-based spatial sensing techniques make them unsuitable for small and power-constrained IoT devices. Aiming to overcome these fundamental limitations, Owlet explores acoustic microstructures for extracting spatial information. It uses a carefully designed 3D-printed metamaterial structure that covers the microphone. The structure embeds a direction-specific signature in the recorded sounds. Owlet system learns the directional signatures through a one-time in-lab calibration. The system uses an additional microphone as a reference channel and develops techniques that eliminate environmental variation, making the design robust to noises and multipaths in arbitrary locations of operations. Owlet prototype shows 3.6° median error in DoA estimation and 10cm median error in source localization while using a 1.5cm \texttimes{} 1.3cm acoustic structure for sensing. The prototype consumes less than 100th of the energy required by a traditional microphone array to achieve similar DoA estimation accuracy. Owlet opens up possibilities of low-power sensing through 3D-printed passive structures.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {255–268},
numpages = {14},
keywords = {IoT, acoustic metamaterial, low-power sensing, spatial sensing},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}

@inproceedings{10.1145/3643916.3644406,
author = {Wang, Che and Li, Yue and Gao, Jianbo and Wang, Ke and Zhang, Jiashuo and Guan, Zhi and Chen, Zhong},
title = {SolaSim: Clone Detection for Solana Smart Contracts via Program Representation},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644406},
doi = {10.1145/3643916.3644406},
abstract = {The open-source nature of smart contracts provides the facility for developers to clone contracts and introduces the risk of vulnerability proliferation as well. Despite intensive research on smart contract clone detection in recent years, existing techniques are still unsatisfactory in detecting Solana smart contracts. To fill this gap, in this paper, we designed a clone detection tool SolaSim for Solana smart contracts and conducted an empirical study to understand the code reuse in the Solana ecosystem. Specifically, SolaSim is based on the semantic metadata extractor and the similarity checker. For each contract, the semantic metadata extractor generates an instruction-level weighted Attributed Control Flow Graph (ACFG) and its semantic metadata (i.e., a combination of high-level semantic and structure information) based on Rust Mid-level Intermediate Representation. The similarity checker adopts a combinatorial optimization algorithm to compute the statistical similarity of a pair of contracts. The evaluation results demonstrated the effectiveness of SolaSim in identifying clones with 94.3% accuracy and it can identify up to Type-3 clone level. Notably, we found there are over 50% clone ratios in the Solana smart contracts ecosystem, in which most of them are cloned from famous open-sourced projects.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {258–269},
numpages = {12},
keywords = {smart contract, solana, code reuse, clone detection},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.5555/3586210.3586473,
author = {Zhao, Tianlang and Jin, Xiao and Lee, Loo Hay},
title = {Optimal Computing Budget Allocation for Multi-Objective Ranking and Selection under Bernoulli Distribution},
year = {2023},
publisher = {IEEE Press},
abstract = {This paper studies a multi-objective ranking and selection (MORS) issue with observations following Bernoulli distribution. The Pareto-optimal set is aimed to be selected with each design and performance measure pair being evaluated separately. Our contribution is twofold. (i) We provide a frequentist work under Bernoulli assumption in MORS where a robust asymptotic optimal sampling strategy is derived based on large deviation principle (LDP). (ii) From the optimal sampling strategy, we propose a sequential selection procedure, named MOCBA-B. Numerical results based on averaged probability of correct selection (PCS) show that MOCBA-B is significantly superior to equal allocation (EA) and is comparable to the theoretically optimal allocation strategy.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3122–3133},
numpages = {12},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.1145/2541940.2541962,
author = {Zahedi, Seyed Majid and Lee, Benjamin C.},
title = {REF: resource elasticity fairness with sharing incentives for multiprocessors},
year = {2014},
isbn = {9781450323055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541940.2541962},
doi = {10.1145/2541940.2541962},
abstract = {With the democratization of cloud and datacenter computing, users increasingly share large hardware platforms. In this setting, architects encounter two challenges: sharing fairly and sharing multiple resources. Drawing on economic game-theory, we rethink fairness in computer architecture. A fair allocation must provide sharing incentives (SI), envy-freeness (EF), and Pareto efficiency (PE).We show that Cobb-Douglas utility functions are well suited to modeling user preferences for cache capacity and memory bandwidth. And we present an allocation mechanism that uses Cobb-Douglas preferences to determine each user's fair share of the hardware. This mechanism provably guarantees SI, EF, and PE, as well as strategy-proofness in the large (SPL). And it does so with modest performance penalties, less than 10% throughput loss, relative to an unfair mechanism.},
booktitle = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {145–160},
numpages = {16},
keywords = {multiprocessor architectures, game theory, fair sharing, economic mechanisms},
location = {Salt Lake City, Utah, USA},
series = {ASPLOS '14}
}

@article{10.1145/3580889,
author = {Wang, Lihao and Wang, Wei and Dai, Haipeng and Liu, Shizhe},
title = {MagSound: Magnetic Field Assisted Wireless Earphone Tracking},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
url = {https://doi.org/10.1145/3580889},
doi = {10.1145/3580889},
abstract = {Wireless earphones are pervasive acoustic sensing platforms that can be used for many applications such as motion tracking and handwriting input. However, wireless earphones suffer clock offset between the connected smart devices, which would accumulate error rapidly over time. Moreover, compared with smartphone and voice assistants, the acoustic signal transmitted by wireless earphone is much weaker due to the poor frequency response. In this paper, we propose MagSound, which uses the built-in magnets to improve the tracking and acoustic sensing performance of Commercial-Off-The-Shelf (COTS) earphones. Leveraging magnetic field strength, MagSound can predict the position of wireless earphones free from clock offset, which can be used to re-calibrate the acoustic tracking. Further, the fusion of the two modalities mitigates the accumulated clock offset and multipath effect. Besides, to increase the robustness to noise, MagSound employs finely designed Orthogonal Frequency-Division Multiplexing (OFDM) ranging signals. We implement a prototype of MagSound on COTS and perform experiments for tracking and handwriting input. Results demonstrate that MagSound maintains millimeter-level error in 2D tracking, and improves the handwriting recognition accuracy by 49.81%. We believe that MagSound can contribute to practical applications of wireless earphones-based sensing.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = mar,
articleno = {33},
numpages = {32},
keywords = {Acoustic sensing, Distributed ranging, Earable computing}
}

@inproceedings{10.1145/3580305.3599491,
author = {Zhu, Zhangchi and Wang, Lu and Zhao, Pu and Du, Chao and Zhang, Wei and Dong, Hang and Qiao, Bo and Lin, Qingwei and Rajmohan, Saravan and Zhang, Dongmei},
title = {Robust Positive-Unlabeled Learning via Noise Negative Sample Self-correction},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599491},
doi = {10.1145/3580305.3599491},
abstract = {Learning from positive and unlabeled data is known as positive-unlabeled (PU) learning in literature and has attracted much attention in recent years. One common approach in PU learning is to sample a set of pseudo-negatives from the unlabeled data using ad-hoc thresholds so that conventional supervised methods can be applied with both positive and negative samples. Owing to the label uncertainty among the unlabeled data, errors of misclassifying unlabeled positive samples as negative samples inevitably appear and may even accumulate during the training processes. Those errors often lead to performance degradation and model instability. To mitigate the impact of label uncertainty and improve the robustness of learning with positive and unlabeled data, we propose a new robust PU learning method with a training strategy motivated by the nature of human learning: easy cases should be learned first. Similar intuition has been utilized in curriculum learning to only use easier cases in the early stage of training before introducing more complex cases. Specifically, we utilize a novel ''hardness'' measure to distinguish unlabeled samples with a high chance of being negative from unlabeled samples with large label noise. An iterative training strategy is then implemented to fine-tune the selection of negative samples during the training process in an iterative manner to include more ''easy'' samples in the early stage of training. Extensive experimental validations over a wide range of learning tasks show that this approach can effectively improve the accuracy and stability of learning with positive and unlabeled data. Our code is available at https://github.com/woriazzc/Robust-PU.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3663–3673},
numpages = {11},
keywords = {positive-unlabeled learning, curriculum learning},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.5555/2486788.2486853,
author = {Sayyad, Abdel Salam and Menzies, Tim and Ammar, Hany},
title = {On the value of user preferences in search-based software engineering: a case study in software product lines},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Software design is a process of trading off competing objectives. If the user objective space is rich, then we should use optimizers that can fully exploit that richness. For example, this study configures software product lines (expressed as feature maps) using various search-based software engineering methods. As we increase the number of optimization objectives, we find that methods in widespread use (e.g. NSGA-II, SPEA2) perform much worse than IBEA (Indicator-Based Evolutionary Algorithm). IBEA works best since it makes most use of user preference knowledge. Hence it does better on the standard measures (hypervolume and spread) but it also generates far more products with 0% violations of domain constraints. Our conclusion is that we need to change our methods for search-based software engineering, particularly when studying complex decision spaces.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {492–501},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/2875913.2875938,
author = {Liu, Jiwei and Mao, Xinjun},
title = {Towards Realisation of Evolvable Runtime Variability in Internet-Based Service Systems via Dynamical Software Update},
year = {2015},
isbn = {9781450336413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2875913.2875938},
doi = {10.1145/2875913.2875938},
abstract = {Today's Internet-based service systems tend to run in open environments and try to satisfy varying requirements. In the context, the changes of requirements and environments can emerge at any point of their life cycle, and their runtime variability is supposed to be evolvable to deal with the changes. In other words, the number, type or attribute of variability elements (variation points and their associated variants) in the systems is regarded to be changeable, especially at runtime. How to realise evolvable runtime variability in software systems is a challenge in the community of software engineering. Software architecture is supposed to support updating software without interrupting service if runtime variability is to be changed. Besides, the relevant mechanisms have to be provided to achieve dynamic update. Thus, we propose a dynamically reconfigurable reference architecture to construct the systems with evolvable software variability. The resultant system established using our approach consists of variability units and their containers. Variability units embody the variability elements that can be changed through local relay, i.e., starting a new version of variability unit to take place of older ones. Containers are mainly used to carry out the dynamic updating for variability units and realise the functionality invocation among different variability units. They can also be added, removed or replaced without shutting down the entire system. We analyse the requirements and scenarios of evolvable runtime variability in the case of Personal Data Resource Network, and further show the effectiveness and applicability of our approach by constructing the system using our class library and solving the issues proposed in the case.},
booktitle = {Proceedings of the 7th Asia-Pacific Symposium on Internetware},
pages = {229–238},
numpages = {10},
keywords = {Software Architecture, Internet-based Service Systems, Evolvable Runtime Variability, Dynamic Software Update},
location = {Wuhan, China},
series = {Internetware '15}
}

@inproceedings{10.1145/3426182.3426187,
author = {Kloibhofer, Sebastian and Pointhuber, Thomas and Heisinger, Maximilian and M\"{o}ssenb\"{o}ck, Hanspeter and Stadler, Lukas and Leopoldseder, David},
title = {SymJEx: symbolic execution on the GraalVM},
year = {2020},
isbn = {9781450388535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426182.3426187},
doi = {10.1145/3426182.3426187},
abstract = {Developing software systems is inherently subject to errors that can later cause failures in production. While testing can help to identify critical issues, it is limited to concrete inputs and states. Exhaustive testing is infeasible in practice; hence we can never prove the absence of faults. Symbolic execution, i.e., the process of symbolically reasoning about the program state during execution, can inspect the behavior of a system under all possible concrete inputs at run time. It automatically generates logical constraints that match the program semantics and uses theorem provers to verify the existence of error states within the application. This paper presents a novel symbolic execution engine called SymJEx, implemented on top of the multi-language Java Virtual Machine GraalVM. SymJEx uses the Graal compiler's intermediate representation to derive and evaluate path conditions, allowing GraalVM users to leverage the engine to improve software quality. In this work, we show how SymJEx finds non-trivial faults in existing software systems and compare our approach with established symbolic execution engines.},
booktitle = {Proceedings of the 17th International Conference on Managed Programming Languages and Runtimes},
pages = {63–72},
numpages = {10},
keywords = {Symbolic execution, Java, GraalVM, Compiler optimizations},
location = {Virtual, UK},
series = {MPLR '20}
}

@article{10.1145/3480139,
author = {Chemistruck, Mike and Allen, Andrew and Snyder, John and Raghuvanshi, Nikunj},
title = {Efficient acoustic perception for virtual AI agents},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
url = {https://doi.org/10.1145/3480139},
doi = {10.1145/3480139},
abstract = {We model acoustic perception in AI agents efficiently within complex scenes with many sound events. The key idea is to employ perceptual parameters that capture how each sound event propagates through the scene to the agent's location. This naturally conforms virtual perception to human. We propose a simplified auditory masking model that limits localization capability in the presence of distracting sounds. We show that anisotropic reflections as well as the initial sound serve as useful localization cues. Our system is simple, fast, and modular and obtains natural results in our tests, letting agents navigate through passageways and portals by sound alone, and anticipate or track occluded but audible targets. Source code is provided.},
journal = {Proc. ACM Comput. Graph. Interact. Tech.},
month = sep,
articleno = {43},
numpages = {13},
keywords = {virtual agents, sound propagation, perception, masking, localization, game AI, acoustics, NPC AI}
}

@inproceedings{10.1145/3652620.3688341,
author = {Kegel, Karl and Domanowski, Andreas and Feichtinger, Kevin and Pascual, Romain and A\ss{}mann, Uwe},
title = {A Delta-Oracle for Fast Model Merge Conflict Estimation using Sketch-Based Critical Pair Analysis},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688341},
doi = {10.1145/3652620.3688341},
abstract = {Conflicting changes are a major challenge in branch-based development and modeling. State-of-the-art research proposes continuous analysis via attempted three-way merges to find potential merge conflicts early on. These approaches are computation-heavy due to the necessity of comparing all variant combinations, ideally for each change. This work proposes a conflict approximation algorithm (oracle) for quick feedback. The oracle approximates conflicts using critical pair analysis on tracked delta sequences, providing a quick feedback loop. The oracle is paired with a classical slow-but-precise full model comparison algorithm, which is run occasionally to validate the oracle's results. This work contributes the Sketch-based Critical Pair Analysis (SCPA) approach for fast merge conflict estimation. SCPA's runtime depends only on the number of changes and not the model size. We evaluate SCPA against EMFCompare in different simulated model evolution scenarios. We found that for the investigated model sizes, SCPA is faster by a magnitude while the number of found conflicts strongly correlates with EMFCompare.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {1046–1055},
numpages = {10},
keywords = {merge conflict estimation, critical pair analysis, oracle algorithm},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.5555/3643142.3643341,
author = {Meisner, Kai and Stein, Heiderose and Leopold, Nadiia and Uhlig, Tobias and Rose, Oliver},
title = {A Modular Simulation Model for Mass Casualty Incidents},
year = {2024},
isbn = {9798350369663},
publisher = {IEEE Press},
abstract = {During military conflicts, the number of casualties is likely to exceed medical capabilities. For best treatment results, the patients must be distributed according to their needs to the available resources such as medical facilities and means of transportation. Computer simulations are used to verify and optimize current medical planning. However, recent models lack the capability of testing a wide range of decision rules. In this paper, we address this issue and propose a modular simulation concept whose components can be adapted and exchanged independently. Using modular submodels to control the simulated objects, we enable the implementation of a wide range of object behavior. A prototype implementation of the proposed concept is presented, showing the effects of applying different dispatching rules in an evacuation scenario.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2403–2414},
numpages = {12},
location = {San Antonio, Texas, USA},
series = {WSC '23}
}

@proceedings{10.1145/3686424,
title = {EDCS '24: Proceedings of the 2024 Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Education Digitalization and Computer Science},
year = {2024},
isbn = {9798400710360},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Shenzhen, China}
}

@inproceedings{10.1145/3495243.3560531,
author = {Shi, Cong and Zhang, Tianfang and Li, Zhuohang and Phan, Huy and Zhao, Tianming and Wang, Yan and Liu, Jian and Yuan, Bo and Chen, Yingying},
title = {Audio-domain position-independent backdoor attack via unnoticeable triggers},
year = {2022},
isbn = {9781450391818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495243.3560531},
doi = {10.1145/3495243.3560531},
abstract = {Deep learning models have become key enablers of voice user interfaces. With the growing trend of adopting outsourced training of these models, backdoor attacks, stealthy yet effective training-phase attacks, have gained increasing attention. They inject hidden trigger patterns through training set poisoning and overwrite the model's predictions in the inference phase. Research in backdoor attacks has been focusing on image classification tasks, while there have been few studies in the audio domain. In this work, we explore the severity of audio-domain backdoor attacks and demonstrate their feasibility under practical scenarios of voice user interfaces, where an adversary injects (plays) an unnoticeable audio trigger into live speech to launch the attack. To realize such attacks, we consider jointly optimizing the audio trigger and the target model in the training phase, deriving a position-independent, unnoticeable, and robust audio trigger. We design new data poisoning techniques and penalty-based algorithms that inject the trigger into randomly generated temporal positions in the audio input during training, rendering the trigger resilient to any temporal position variations. We further design an environmental sound mimicking technique to make the trigger resemble unnoticeable situational sounds and simulate played over-the-air distortions to improve the trigger's robustness during the joint optimization process. Extensive experiments on two important applications (i.e., speech command recognition and speaker recognition) demonstrate that our attack can achieve an average success rate of over 99% under both digital and physical attack settings.},
booktitle = {Proceedings of the 28th Annual International Conference on Mobile Computing And Networking},
pages = {583–595},
numpages = {13},
keywords = {position-independent attacks, over-the-air physical attacks, audio-domain backdoor attacks},
location = {Sydney, NSW, Australia},
series = {MobiCom '22}
}

@article{10.1145/1409060.1409096,
author = {Patney, Anjul and Owens, John D.},
title = {Real-time Reyes-style adaptive surface subdivision},
year = {2008},
issue_date = {December 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {5},
issn = {0730-0301},
url = {https://doi.org/10.1145/1409060.1409096},
doi = {10.1145/1409060.1409096},
abstract = {We present a GPU based implementation of Reyes-style adaptive surface subdivision, known in Reyes terminology as the Bound/Split and Dice stages. The performance of this task is important for the Reyes pipeline to map efficiently to graphics hardware, but its recursive nature and irregular and unbounded memory requirements present a challenge to an efficient implementation. Our solution begins by characterizing Reyes subdivision as a work queue with irregular computation, targeted to a massively parallel GPU. We propose efficient solutions to these general problems by casting our solution in terms of the fundamental primitives of prefix-sum and reduction, often encountered in parallel and GPGPU environments.Our results indicate that real-time Reyes subdivision can indeed be obtained on today's GPUs. We are able to subdivide a complex model to subpixel accuracy within 15 ms. Our measured performance is several times better than that of Pixar's RenderMan. Our implementation scales well with the input size and depth of subdivision. We also address concerns of memory size and bandwidth, and analyze the feasibility of conventional ideas on screen-space buckets.},
journal = {ACM Trans. Graph.},
month = dec,
articleno = {143},
numpages = {8},
keywords = {graphics hardware, adaptive surface subdivision, Reyes, GPGPU}
}

@inproceedings{10.1145/3581783.3611859,
author = {Liu, Yingchi and Liu, Zhu and Ma, Long and Liu, Jinyuan and Fan, Xin and Luo, Zhongxuan and Liu, Risheng},
title = {Bilevel Generative Learning for Low-Light Vision},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3611859},
doi = {10.1145/3581783.3611859},
abstract = {Recently, there has been a growing interest in constructing deep learning schemes for Low-Light Vision (LLV). Existing techniques primarily focus on designing task-specific and data-dependent vision models on the standard RGB domain, which inherently contain latent data associations. In this study, we propose a generic low-light vision solution by introducing a generative block to convert data from the RAW to the RGB domain. This novel approach connects diverse vision problems by explicitly depicting data generation, which is the first in the field. To precisely characterize the latent correspondence between the generative procedure and the vision task, we establish a bilevel model with the parameters of the generative block defined as the upper level and the parameters of the vision task defined as the lower level. We further develop two types of learning strategies targeting different goals, namely low cost and high accuracy, to acquire a new bilevel generative learning paradigm. The generative blocks embrace a strong generalization ability in other low-light vision tasks through the bilevel optimization on enhancement tasks. Extensive experimental evaluations on three representative low-light vision tasks, namely enhancement, detection, and segmentation, fully demonstrate the superiority of our proposed approach. The code will be available at https://github.com/Yingchi1998/BGL.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {7758–7766},
numpages = {9},
keywords = {bilevel generative learning, low-light vision, raw generative block},
location = {Ottawa ON, Canada},
series = {MM '23}
}

