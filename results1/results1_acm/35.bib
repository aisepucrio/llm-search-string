@inproceedings{10.1145/3503229.3547055,
author = {Friesel, Birte and M\"{u}ller, Michael and Ferraz, Matheus and Spinczyk, Olaf},
title = {On the relation of variability modeling languages and non-functional properties},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547055},
doi = {10.1145/3503229.3547055},
abstract = {Non-functional properties (NFPs) such as code size (RAM, ROM), performance, and energy consumption are at least as important as functional properties in many software development domains. When configuring a software product line - especially in the area of resource-constrained embedded systems - developers must be aware of the NFPs of the configured product instance. Several NFP-aware variability modeling languages have been proposed to address this in the past. However, it is not clear whether a variability modeling language is the best place for handling NFP-related concerns, or whether separate NFP prediction models should be preferred. We shine light onto this question by discussing limitations of state-of-the-art NFP-aware variability modeling languages, and find that both in terms of the development process and model accuracy a separate NFP model is favorable. Our quantitative analysis is based on six different software product lines, including the widely used busybox multi-call binary and the x264 video encoder. We use classification and regression trees (CART) and our recently proposed Regression Model Trees [8] as separate NFP models. These tree-based models can cover the effects of arbitrary feature interactions and thus easily outperform variability models with static, feature-wise NFP annotations. For example, when estimating the throughput of an embedded AI product line, static annotations come with a mean generalization error of 114.5% while the error of CART is only 9.4 %.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {140–144},
numpages = {5},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3336294.3336304,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Software Product Line Engineering: A Practical Experience},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336304},
doi = {10.1145/3336294.3336304},
abstract = {The lack of mature tool support is one of the main reasons that make the industry to be reluctant to adopt Software Product Line (SPL) approaches. A number of systematic literature reviews exist that identify the main characteristics offered by existing tools and the SPL phases in which they can be applied. However, these reviews do not really help to understand if those tools are offering what is really needed to apply SPLs to complex projects. These studies are mainly based on information extracted from the tool documentation or published papers. In this paper, we follow a different approach, in which we firstly identify those characteristics that are currently essential for the development of an SPL, and secondly analyze whether the tools provide or not support for those characteristics. We focus on those tools that satisfy certain selection criteria (e.g., they can be downloaded and are ready to be used). The paper presents a state of practice with the availability and usability of the existing tools for SPL, and defines different roadmaps that allow carrying out a complete SPL process with the existing tool support.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {164–176},
numpages = {13},
keywords = {tooling roadmap, tool support, state of practice, spl in practice},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3522664.3528602,
author = {Friesel, Birte and Spinczyk, Olaf},
title = {Black-box models for non-functional properties of AI software systems},
year = {2022},
isbn = {9781450392754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3522664.3528602},
doi = {10.1145/3522664.3528602},
abstract = {Non-functional properties (NFPs) such as latency, memory requirements, or hardware cost are an important characteristic of AI software systems, especially in the domain of resource-constrained embedded devices. Embedded AI products require sufficient resources for satisfactory latency and accuracy, but should also be cost-efficient and therefore not use more powerful hardware than strictly necessary. Traditionally, modeling and optimization efforts focus on the AI architecture, utilizing methods such as neural architecture search (NAS). However, before developers can start optimizing, they need to know which architectures are suitable candidates for their use case. To this end, architectures must be viewed in context: model post-processing (e.g. quantization), hardware platform, and run-time configuration such as batching all have significant effects on NFPs and therefore on AI architecture performance. Moreover, scalar parameters such as batch size cannot be benchmarked exhaustively. We argue that it is worthwhile to address this issue by means of black-box models before deciding on AI architectures for optimization and hardware/software platforms for inference. To support our claim, we present an AI product line with variable hardware and software components, perform benchmarks, and present notable results. Additionally, we evaluate both compactness and generalization capabilities of regression tree-based modeling approaches from the machine learning and product line engineering communities. We find that linear model trees perform best: they can capture NFPs of known AI configurations with a mean error of up to 13 %, and can predict unseen configurations with a mean error of 10 to 26 %. We find linear model trees to be more compact and interpretable than other tree-based approaches.},
booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
pages = {170–180},
numpages = {11},
keywords = {AI, performance prediction, product lines, regression trees},
location = {Pittsburgh, Pennsylvania},
series = {CAIN '22}
}

@article{10.1145/3442389,
author = {Castro, Thiago and Teixeira, Leopoldo and Alves, Vander and Apel, Sven and Cordy, Maxime and Gheyi, Rohit},
title = {A Formal Framework of Software Product Line Analyses},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3442389},
doi = {10.1145/3442389},
abstract = {A number of product-line analysis approaches lift analyses such as type checking, model checking, and theorem proving from the level of single programs to the level of product lines. These approaches share concepts and mechanisms that suggest an unexplored potential for reuse of key analysis steps and properties, implementation, and verification efforts. Despite the availability of taxonomies synthesizing such approaches, there still remains the underlying problem of not being able to describe product-line analyses and their properties precisely and uniformly. We propose a formal framework that models product-line analyses in a compositional manner, providing an overall understanding of the space of family-based, feature-based, and product-based analysis strategies. It defines precisely how the different types of product-line analyses compose and inter-relate. To ensure soundness, we formalize the framework, providing mechanized specification and proofs of key concepts and properties of the individual analyses. The formalization provides unambiguous definitions of domain terminology and assumptions as well as solid evidence of key properties based on rigorous formal proofs. To qualitatively assess the generality of the framework, we discuss to what extent it describes five representative product-line analyses targeting the following properties: safety, performance, dataflow facts, security, and functional program properties.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {34},
numpages = {37},
keywords = {product-line analysis, Software product lines}
}

@inproceedings{10.1145/3233027.3233028,
author = {Rabiser, Rick and Schmid, Klaus and Becker, Martin and Botterweck, Goetz and Galster, Matthias and Groher, Iris and Weyns, Danny},
title = {A study and comparison of industrial vs. academic software product line research published at SPLC},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233028},
doi = {10.1145/3233027.3233028},
abstract = {The study presented in this paper aims to provide evidence for the hypothesis that software product line research has been changing and that the works in industry and academia have diverged over time. We analysed a subset (140) of all (593) papers published at the Software Product Line Conference (SPLC) until 2017. The subset was randomly selected to cover all years as well as types of papers. We assessed the research type of the papers (academic or industry), the kind of evaluation (application example, empirical, etc.), and the application domain. Also, we assessed which product line life-cycle phases, development practices, and topics the papers address. We present an analysis of the topics covered by academic vs. industry research and discuss the evolution of these topics and their relation over the years. We also discuss implications for researchers and practitioners. We conclude that even though several topics have received more attention than others, academic and industry research on software product lines are actually rather in line with each other.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {14–24},
numpages = {11},
keywords = {software product lines, industry, academia, SPLC},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3646548.3676546,
author = {G\"{u}thing, Lukas and Pett, Tobias and Schaefer, Ina},
title = {Out-of-the-Box Prediction of Non-Functional Variant Properties Using Automated Machine Learning},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676546},
doi = {10.1145/3646548.3676546},
abstract = {A configurable system is characterized by the configuration options present or absent in its variants. Selecting and deselecting those configuration options directly influences the functional properties of the system. Apart from functional properties, there are system characteristics that influence the performance (e.g., power demand), safety (e.g., fault probabilities), and security (e.g., susceptibility to attacks) of the system, called Non-Functional Properties (NFPs). Knowledge of NFPs is crucial for evaluating a system’s feasibility, usability, and resource demands. Although variability influences these characteristics, NFPs do not compose linearly for every selected feature. Feature interactions can increase the overall NFP values through (potentially exponential) amplification or decrease them through mitigation effects. In this paper, we propose an automated machine learning (AutoML) approach to predict NFP values for new configurations based on previously measured configuration values. Using AutoML, we leverage the advantages of machine learning for predicting NFPs without having to parameterize and fine-tune machine learning models. This approach and the resulting pipeline aim to reduce the complexity of performance prediction for configurable systems. We test the feasibility of our pipeline in a first evaluation on 4 real-world subject systems and discuss cases where AutoML may improve the prediction of NFPs.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {82–87},
numpages = {6},
keywords = {AutoML, Cyber-physical systems, Machine learning, Software product lines},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/2491627.2493906,
author = {Bashari, Mahdi and Bagheri, Ebrahim},
title = {Engineering self-adaptive systems and dynamic software product line},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2493906},
doi = {10.1145/2491627.2493906},
abstract = {Self-adaptive systems are a class of software applications, which are able to dynamically transform their internal structure and hence their behavior in response to internal or external stimuli. The transformation may provide the basis for new functionalities or improve or maintain non-functional properties in order to match the application better to its operational requirements and standards. Software Product Line Engineering has rich methods and techniques in variability modeling and management which is one of the main issues in developing self-adaptive systems. Dynamic software product lines (DSPL) have been proposed to exploit the knowledge acquired in SPLE to develop self-adaptive software systems.In this tutorial, we portray the problem of developing self-adaptive systems. Then we investigate how the idea of dynamic software product line could help to deal with the challenges that we face in developing efficient self-adaptive software. We also offer insight into the different approaches that use dynamic software product line engineering for developing self-adaptive systems focusing on practical approaches by showing how the approaches are applied to real case studies and also methods for evaluating these approaches. This tutorial also discuss how DSPL could be used some relevant areas to self-adaptive systems and challenges which still exist in the area.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {285},
numpages = {1},
keywords = {self-adaptive systems, dynamic software product line},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2934466.2934481,
author = {Sion, Laurens and Van Landuyt, Dimitri and Joosen, Wouter and de Jong, Gjalt},
title = {Systematic quality trade-off support in the software product-line configuration process},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934481},
doi = {10.1145/2934466.2934481},
abstract = {Software product line engineering is a compelling methodology that accomplishes systematic reuse in families of systems by relying on two key principles: (i) the decomposition of complex systems into composable and reusable building blocks (often logical units called features), and (ii) on-demand construction of products and product variants by composing these building blocks.However, unless the stakeholder responsible for product configuration has detailed knowledge of the technical ins and outs of the software product line (e.g., the architectural impact of a specific feature, or potential feature interactions), he is in many cases flying in the dark. Although many initial approaches and techniques have been proposed that take into account quality considerations and involve trade-off decisions during product configuration, no systematic support exists.In this paper, we present a reference architecture for product configuration tooling, providing support for (i) up-front generation of variants, and (ii) quality analysis of these variants. This allows pro-actively assessing and predicting architectural quality properties for each product variant and in turn, product configuration tools can take into account architectural considerations. In addition, we provide an in-depth discussion of techniques and tactics for dealing with the problem of variant explosion, and as such to maintain practical feasibility of such approaches.We validated and implemented our reference architecture in the context of a real-world industrial application, a product-line for the firmware of an automotive sensor. Our prototype, based on FeatureIDE, is open for extension and readily available.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {164–173},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3546932.3547015,
author = {Marchezan, Luciano and Assun\c{c}\~{a}o, Wesley K. G. and Michelon, Gabriela and Herac, Edvin and Egyed, Alexander},
title = {Code smell analysis in cloned Java variants: the apo-games case study},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547015},
doi = {10.1145/3546932.3547015},
abstract = {Families of software products are usually created using opportunistic reuse (clone-and-own) in which products are cloned and adapted to meet new requirements, user preferences, or non-functional properties. Opportunistic reuse brings short-term benefits, e.g., reduced time-to-market, whereas creating long-term drawbacks, e.g., the need of changing multiple variants for any maintenance and evolution activity. This situation is even worse when the individual products have poor design or implementation choices, the so-called code smells. Due to their harmfulness to software quality, code smells should be detected and removed as early as possible. In a family of software products, the same code smell must be identified and removed in all variants where it is are present. Identifying instances of similar code smells affecting different variants has not been investigated in the literature yet. This is the case of the Apo-Games family, which has the challenge of identifying the flaws in the design and implementation of cloned games. To address this challenge, we applied our inconsistency and repair approach to detect and suggest solutions for six types of code smells in 19 products of the Apo-games family. Our results show that a considerable number of smells were identified, most of them for the long parameter list and data class types. The number of the same smells identified in multiple variants ranged between 2.9 and 20.2 on average, showing that clone-and-own may lead to the replication of code smells in multiple products. Lastly, our approach was able to generate between 4.9 and 28.98 repair alternatives per smell on average.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {250–254},
numpages = {5},
keywords = {software product line, inconsistency repair, consistency checking, code smells},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/2701319.2701326,
author = {Soares, Larissa Rocha and do Carmo Machado, Ivan and de Almeida, Eduardo Santana},
title = {Non-Functional Properties in Software Product Lines: A Reuse Approach},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701326},
doi = {10.1145/2701319.2701326},
abstract = {Software Product Line Engineering (SPLE) emerges for software organizations interested in customized products at reasonable costs. Based on the selection of features, stakeholders can derive programs satisfying a range of functional properties and non-functional ones. The explicit definition of Non-Functional Properties (NFP) during software configuration has been considered a challenging task. Dealing with them is not well established yet, neither in theory nor in practice. In this sense, we present a framework to specify NFP for SPLE and we also propose a reuse approach that promotes the reuse of NFP values during the product configuration. We discuss the results of a case study aimed to evaluate the applicability of the proposed work.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {67–74},
numpages = {8},
keywords = {Software Product Line, Quality Attributes, Empirical Software Engineering},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@inproceedings{10.1145/2648511.2648513,
author = {Harman, M. and Jia, Y. and Krinke, J. and Langdon, W. B. and Petke, J. and Zhang, Y.},
title = {Search based software engineering for software product line engineering: a survey and directions for future work},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648513},
doi = {10.1145/2648511.2648513},
abstract = {This paper presents a survey of work on Search Based Software Engineering (SBSE) for Software Product Lines (SPLs). We have attempted to be comprehensive, in the sense that we have sought to include all papers that apply computational search techniques to problems in software product line engineering. Having surveyed the recent explosion in SBSE for SPL research activity, we highlight some directions for future work. We focus on suggestions for the development of recent advances in genetic improvement, showing how these might be exploited by SPL researchers and practitioners: Genetic improvement may grow new products with new functional and non-functional features and graft these into SPLs. It may also merge and parameterise multiple branches to cope with SPL branchmania.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {5–18},
numpages = {14},
keywords = {program synthesis, genetic programming, SPL, SBSE},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3646548.3676545,
author = {Azonhoumon, Vidjinnangni Alphonse Ignace},
title = {Design of a Meta-Factory for Product Lines Model-Driven Software},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676545},
doi = {10.1145/3646548.3676545},
abstract = {Software Product Lines (SPLs) offer a robust method for reusing code in software development. Despite various approaches to managing variability in SPLs, existing tools fall short of fully meeting industry needs, such as capturing non-functional requirements, modelling variability mechanisms, combining multiple tactics, maintaining traceability across features, tactics and assets, and ultimately, deriving products. The Bespoke project addresses these gaps by introducing a novel model-driven approach that transitions from Domain-Specific Modelling Language (DSML) to Feature Model (FM), allowing the integration of non-functional properties, and guiding products derivation with comprehensive tactics management. This solution is later validated through evaluation on a real-world SPL project focused on ageing. By leveraging existing tools and focusing on product derivation, this project aims to significantly improve code reuse and product derivation in SPLs, offering practical solutions for industry adoption. This article presents research questions related to the implementation of this methodology.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {6–10},
numpages = {5},
keywords = {DSML, Product derivation, Quality attributes, SPL, Tactics},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, M\'{a}rcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Software Product Lines, Recommender Systems, Non-Functional Properties, Feature Model, Configuration},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/2491627.2499880,
author = {Clarke, Dave and Schaefer, Ina and ter Beek, Maurice H. and Apel, Sven and Atlee, Joanne M.},
title = {Formal methods and analysis in software product line engineering: 4th edition of FMSPLE workshop series},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2499880},
doi = {10.1145/2491627.2499880},
abstract = {FMSPLE 2013 is the fourth edition of the FMSPLE workshop series aimed at connecting researchers and practitioners interested in raising the efficiency and the effectiveness of software product line engineering through the application of innovative analysis approaches and formal methods.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {266–267},
numpages = {2},
keywords = {verification, variability, testing, software product lines, semantics, formal methods, evolution},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3167132.3167353,
author = {Pereira, Juliana Alves and Martinez, Jabier and Gurudu, Hari Kumar and Krieter, Sebastian and Saake, Gunter},
title = {Visual guidance for product line configuration using recommendations and non-functional properties},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167353},
doi = {10.1145/3167132.3167353},
abstract = {Software Product Lines (SPLs) are a mature approach for the derivation of a family of products using systematic reuse. Different combinations of predefined features enable tailoring the product to fit the needs of each customer. These needs are related to functional properties of the system (optional features) as well as non-functional properties (e.g., performance or cost of the final product). In industrial scenarios, the configuration process of a final product is complex and the tool support is usually limited to check functional properties interdependencies. In addition, the importance of nonfunctional properties as relevant drivers during configuration has been overlooked. Thus, there is a lack of holistic paradigms integrating recommendation systems and visualizations that can help the decision makers. In this paper, we propose and evaluate an interrelated set of visualizations for the configuration process filling these gaps. We integrate them as part of the FeatureIDE tool and we evaluate its effectiveness, scalability, and performance.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {2058–2065},
numpages = {8},
keywords = {visualization, software product lines, recommendation systems, feature model, configuration},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3646548.3676548,
author = {Romero-Organvidez, David and Neira, Pablo and Galindo, Jos\'{e} A. and Benavides, David},
title = {Kconfig metamodel: a first approach},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676548},
doi = {10.1145/3646548.3676548},
abstract = {Kconfig is the de facto configuration language for describing and configuring the variability of the Linux kernel. Nonetheless, it has been used since the early stages of kernel development. Moreover, Kconfig is also used as a niche configuration languages, such as microkernel compilation for air navigation systems, proprietary routers or embedded systems. In the last decade, the software product line (SPL) community worked intensively on observing Linux Kernel and Kconfig. However, the official documentation is difficult to understand, and the examples are long and challenging to synthesize for non-Kconfig experts, such as SPL engineers, and researchers. In this paper, we propose a Kconfig metamodel based on the documentation and the feedback of a kernel developer expert. Thanks to this metamodel, the design of transformations from Kconfig to other variability models such as UVL (Universal Variability Language) can be facilitated. To our knowledge, this is the first proposal for a metamodel of the Kconfig language. This opens the door to further research, such as Kconfig analysis and transformations, and leverage interoperability among the Kconfig toolchain and SPL tools.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {55–60},
numpages = {6},
keywords = {Kconfig, linux kernel, metamodel, variability},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/2362536.2362576,
author = {ter Beek, Maurice H. and Becker, Martin and Classen, Andreas and Roos-Frantz, Fabricia and Schaefer, Ina and Wong, Peter Y. H.},
title = {Formal methods and analysis in software product line engineering: 3rd edition of FMSPLE workshop series},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362576},
doi = {10.1145/2362536.2362576},
abstract = {FMSPLE 2012 is the third edition of the FMSPLE workshop series, traditionally affiliated with SPLC, which aims to connect researchers and practitioners interested in raising the efficiency and the effectiveness of SPLE through the application of innovative analysis approaches and formal methods.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {286–287},
numpages = {2},
keywords = {verification, variability, testing, software product lines, semantics, formal methods, evolution},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3646548.3676552,
author = {Malenfant, Jacques and Ziadi, Tewfik},
title = {Can Conditional Preferences and *CP-net Concepts Enhance Feature Models?},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676552},
doi = {10.1145/3646548.3676552},
abstract = {Variability in software systems is a key concept in mastering complexity. Most software systems exhibit functionalities that can either be implemented in several different ways or may be options that can be chosen late in their design, depending on the deployment conditions. Expressing these forms of variability attracted a lot of attention since several years, leading to a predominant expression media: Feature models. Feature models can express the mandatory/optional status of a feature, as well as alternative realizations of a feature, exclusive of each others or not. Though extensions of feature models have been proposed to express more properties, they are still limited in their ability to capture complex configuration problems, especially those involving extra functional properties. Lines of research have proposed extended feature models with non-functional attributes and constraint-solving approach to reason about them. Yet, these approaches appear too restrictive and inflexible to cater for really complex SPL. In this paper, we rather propose to extend feature models with conditional preferences concepts from the multi-criteria decision-making field, and more specifically on relationships introduced in the graphical modeling approach of the * CP-net family of models. We show how these new relationships enable the expression of complex configuration constraints, while ensuring that the feature models remain intuitive and user-friendly across various feature model analysis-based activities.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {66–74},
numpages = {9},
keywords = {conditional preferences, feature models, variability management},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3461001.3461660,
author = {Michelon, Gabriela Karoline and Obermann, David and Assun\c{c}\~{a}o, Wesley K. G. and Linsbauer, Lukas and Gr\"{u}nbacher, Paul and Egyed, Alexander},
title = {Managing systems evolving in space and time: four challenges for maintenance, evolution and composition of variants},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3461660},
doi = {10.1145/3461001.3461660},
abstract = {Software companies need to provide a large set of features satisfying functional and non-functional requirements of diverse customers, thereby leading to variability in space. Feature location techniques have been proposed to support software maintenance and evolution in space. However, so far only one feature location technique also analyses the evolution in time of system variants, which is required for feature enhancements and bug fixing. Specifically, existing tools for managing a set of systems over time do not offer proper support for keeping track of feature revisions, updating existing variants, and creating new product configurations based on feature revisions. This paper presents four challenges concerning such capabilities for feature (revision) location and composition of new product configurations based on feature/s (revisions). We also provide a benchmark containing a ground truth and support for computing metrics. We hope that this will motivate researchers to provide and evaluate tool-supported approaches aiming at managing systems evolving in space and time. Further, we do not limit the evaluation of techniques to only this benchmark: we introduce and provide instructions on how to use a benchmark extractor for generating ground truth data for other systems. We expect that the feature (revision) location techniques maximize information retrieval in terms of precision, recall, and F-score, while keeping execution time and memory consumption low.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {75–80},
numpages = {6},
keywords = {software product line, repository mining, feature revision, feature location, benchmark extractor},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3646548.3672586,
author = {Fernandez-Amoros, David and Heradio, Ruben and Horcas Aguilera, Jose Miguel and Galindo, Jos\'{e} A. and Benavides, David and Fuentes, Lidia},
title = {Pragmatic Random Sampling of the Linux Kernel: Enhancing the Randomness and Correctness of the conf Tool},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672586},
doi = {10.1145/3646548.3672586},
abstract = {The configuration space of some systems is so large that it cannot be computed. This is the case with the Linux Kernel, which provides almost 19,000 configurable options described across more than 1,600 files in the Kconfig language. As a result, many analyses of the Kernel rely on sampling its configuration space (e.g., debugging compilation errors, predicting configuration performance, finding the configuration that optimizes specific performance metrics, etc.). The Kernel can be sampled pragmatically, with its built-in tool conf, or idealistically, translating the Kconfig files into logic formulas. The pros of the idealistic approach are that it provides statistical guarantees for the sampled configurations, but the cons are that it sets out many challenging problems that have not been solved yet, such as scalability issues. This paper introduces a new version of conf called randconfig+, which incorporates a series of improvements that increase the randomness and correctness of pragmatic sampling and also help validate the Boolean translation required for the idealistic approach. randconfig+ has been tested on 20,000 configurations generated for 10 different Kernel versions from 2003 to the present day. The experimental results show that randconfig+ is compatible with all tested Kernel versions, guarantees the correctness of the generated configurations, and increases conf’s randomness for numeric and string options.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {24–35},
numpages = {12},
keywords = {Kconfig, SAT, configurable systems, randconfig, random sampling, software product lines, variability modeling},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3646548.3676539,
author = {M\"{u}ller, Robert and Wei\ss{}, Mathis and Lochau, Malte},
title = {Mapping Cardinality-based Feature Models to Weighted Automata over Featured Multiset Semirings},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676539},
doi = {10.1145/3646548.3676539},
abstract = {Cardinality-based feature models permit to select multiple copies of the same feature, thus generalizing the notion of product configurations from subsets of Boolean features to multisets of feature instances. This increased expressiveness shapes a-priori infinite and non-convex configuration spaces, which renders established solution-space mappings based on Boolean presence conditions insufficient for cardinality-based feature models. To address this issue, we propose weighted automata over featured multiset semirings as a novel behavioral variability modeling formalism for cardinality-based feature models. The formalism uses multisets over features as a predefined semantic domain for transition weights. It permits to use any algebraic structure forming a proper semiring on multisets to aggregate the weights traversed along paths to map accepted words to multiset configurations. In particular, tropical semirings constitute a promising sub-class with a reasonable trade-off between expressiveness and computational tractability of canonical analysis problems. The formalism is strictly more expressive than featured transition systems, as it enables upper-bound multiplicity constraints depending on the length of words. We provide a tool implementation of the behavioral variability model and present preliminary experimental results showing applicability and computational feasibility of the proposed approach.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {1–11},
numpages = {11},
keywords = {Behavioral Variability Modeling, Cardinality-Based Feature Models, Weighted Automata},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3336294.3336309,
author = {Temple, Paul and Acher, Mathieu and Perrouin, Gilles and Biggio, Battista and Jezequel, Jean-Marc and Roli, Fabio},
title = {Towards Quality Assurance of Software Product Lines with Adversarial Configurations},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336309},
doi = {10.1145/3336294.3336309},
abstract = {Software product line (SPL) engineers put a lot of effort to ensure that, through the setting of a large number of possible configuration options, products are acceptable and well-tailored to customers' needs. Unfortunately, options and their mutual interactions create a huge configuration space which is intractable to exhaustively explore. Instead of testing all products, machine learning is increasingly employed to approximate the set of acceptable products out of a small training sample of configurations. Machine learning (ML) techniques can refine a software product line through learned constraints and a priori prevent non-acceptable products to be derived. In this paper, we use adversarial ML techniques to generate adversarial configurations fooling ML classifiers and pinpoint incorrect classifications of products (videos) derived from an industrial video generator. Our attacks yield (up to) a 100% misclassification rate and a drop in accuracy of 5%. We discuss the implications these results have on SPL quality assurance.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {277–288},
numpages = {12},
keywords = {software variability, software testing, software product line, quality assurance, machine learning},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2420942.2420948,
author = {Gonz\'{a}lez-Huerta, Javier and Insfran, Emilio and Abrah\~{a}o, Silvia and McGregor, John D.},
title = {Non-functional requirements in model-driven software product line engineering},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420948},
doi = {10.1145/2420942.2420948},
abstract = {Developing variant-rich software systems through the application of the software product line approach requires the management of a wide set of requirements. However, in most cases, the focus of those requirements is limited to the functional requirements. The non-functional requirements are often informally defined and their management does not provide traceability mechanisms for their validation. In this paper, we present a multimodel approach that allows the explicit representation of non-functional requirements for software product lines both at domain engineering, and application engineering levels. The multimodel allows the representation of different viewpoints of a software product line, including the non-functional requirements and the relationships that these non-functional requirements might have with features and functionalities. The feasibility of this approach is illustrated through a specific example from the automotive domain.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {6},
numpages = {6},
keywords = {software product lines, non-functional requirements, model driven engineering},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@inproceedings{10.5555/2337223.2337302,
author = {Cordy, Maxime and Classen, Andreas and Perrouin, Gilles and Schobbens, Pierre-Yves and Heymans, Patrick and Legay, Axel},
title = {Simulation-based abstractions for software product-line model checking},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Software Product Line (SPL) engineering is a software engineering paradigm that exploits the commonality between similar software products to reduce life cycle costs and time-to-market. Many SPLs are critical and would benefit from efficient verification through model checking. Model checking SPLs is more difficult than for single systems, since the number of different products is potentially huge. In previous work, we introduced Featured Transition Systems (FTS), a formal, compact representation of SPL behaviour, and provided efficient algorithms to verify FTS. Yet, we still face the state explosion problem, like any model checking-based verification. Model abstraction is the most relevant answer to state explosion. In this paper, we define a novel simulation relation for FTS and provide an algorithm to compute it. We extend well-known simulation preservation properties to FTS and thus lay the theoretical foundations for abstraction-based model checking of SPLs. We evaluate our approach by comparing the cost of FTS-based simulation and abstraction with respect to product-by-product methods. Our results show that FTS are a solid foundation for simulation-based model checking of SPL.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {672–682},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1145/3546932.3546997,
author = {Acher, Mathieu and Martin, Hugo and Lesoil, Luc and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc and Khelladi, Djamel Eddine and Barais, Olivier and Pereira, Juliana Alves},
title = {Feature subset selection for learning huge configuration spaces: the case of linux kernel size},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3546997},
doi = {10.1145/3546932.3546997},
abstract = {Linux kernels are used in a wide variety of appliances, many of them having strong requirements on the kernel size due to constraints such as limited memory or instant boot. With more than nine thousands of configuration options to choose from, developers and users of Linux actually spend significant effort to document, understand, and eventually tune (combinations of) options for meeting a kernel size. In this paper, we describe a large-scale endeavour automating this task and predicting a given Linux kernel binary size out of unmeasured configurations. We first experiment that state-of-the-art solutions specifically made for configurable systems such as performance-influence models cannot cope with that number of options, suggesting that software product line techniques may need to be adapted to such huge configuration spaces. We then show that tree-based feature selection can learn a model achieving low prediction errors over a reduced set of options. The resulting model, trained on 95 854 kernel configurations, is fast to compute, simple to interpret and even outperforms the accuracy of learning without feature selection.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {85–96},
numpages = {12},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3382025.3414942,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Kr\"{u}ger, Jacob and Mendon\c{c}a, Willian D. F.},
title = {Variability management meets microservices: six challenges of re-engineering microservice-based webshops},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414942},
doi = {10.1145/3382025.3414942},
abstract = {A microservice implements a small unit of functionality that it provides through a network using lightweight protocols. So, microservices can be combined to fulfill tasks and implement features of a larger software system---resembling a variability mechanism in the context of a software product line (SPL). Microservices and SPLs have similar goals, namely facilitating reuse and customizing, but they are usually employed in different contexts. Any developer who has access to the network can provide a microservice for any task, while SPLs are usually intended to implement features of a specific domain. Due to their different concepts, using microservices to implement an SPL or adopting SPL practices (e.g., variability management) for microservices is a challenging cross-area research problem. However, both techniques can complement each other, and thus tackling this problem promises benefits for organizations that employ either technique. In this paper, we reason on the importance of advancing in this direction, and sketch six concrete challenges to initiate research, namely (1) feature identification, (2) variability modeling, (3) variable microservice architectures, (4) interchangeability, (5) deep customization, and (6) re-engineering an SPL. We intend these challenges to serve as a starting point for future research in this cross-area research direction---avoiding that the concepts of one area are reinvented in the other.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {22},
numpages = {6},
keywords = {variability management, software product line, re-engineering, microservices, cloud computing},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2245276.2231956,
author = {Horikoshi, Hisayuki and Nakagawa, Hiroyuki and Tahara, Yasuyuki and Ohsuga, Akihiko},
title = {Dynamic reconfiguration in self-adaptive systems considering non-functional properties},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2231956},
doi = {10.1145/2245276.2231956},
abstract = {Self-adaptive systems have recently been receiving much attention because of their ability to cope with the changes of environment, failures, and unanticipated events. These systems need an adaptation mechanism, which automatically computes the possible configurations, and decides the most appropriate configuration to fit the environment. In particular, the satisfaction of non-functional requirements must be considered when selecting the best reconfiguration. However, there are trade-off problems among non-functional requirements. Moreover, the adaptation mechanisms are typically developed separately from the components to be implemented, and it complicates the construction of such systems. We propose (1) a feature-oriented analysis technique, which can identify adaptation points, and calculate the contribution to non-functional goals of the configuration; (2) a component specification model, which extends an architectural description language for self-adaptation; (3) a reconfiguration framework aimed to reduce the complexity of the reconfiguration and generate the best configuration at run-time. We evaluate the feasibility of our framework by four different scenarios, and show that our framework reduces the complexity of the reconfiguration, and solves the trade-off problem among non-functional requirements.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1144–1150},
numpages = {7},
keywords = {software architecture, self-adaptive systems, feature-oriented analysis, dynamic reconfiguration, architecture description language},
location = {Trento, Italy},
series = {SAC '12}
}

@inproceedings{10.1145/3461002.3473944,
author = {Ballesteros, Joaqu\'{\i}n and Fuentes, Lidia},
title = {Transfer learning for multiobjective optimization algorithms supporting dynamic software product lines},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473944},
doi = {10.1145/3461002.3473944},
abstract = {Dynamic Software Product Lines (DSPLs) are a well-accepted approach for self-adapting Cyber-Physical Systems (CPSs) at run-time. The DSPL approaches make decisions supported by performance models, which capture system features' contribution to one or more optimization goals. Combining performance models with Multi-Objectives Evolutionary Algorithms (MOEAs) as decision-making mechanisms is common in DSPLs. However, MOEAs algorithms start solving the optimization problem from a randomly selected population, not finding good configurations fast enough after a context change, requiring too many resources so scarce in CPSs. Also, the DSPL engineer must deal with the hardware and software particularities of the target platform in each CPS deployment. And although each system instantiation has to solve a similar optimization problem of the DSPL, it does not take advantage of experiences gained in similar CPS. Transfer learning aims at improving the efficiency of systems by sharing the previously acquired knowledge and applying it to similar systems. In this work, we analyze the benefits of transfer learning in the context of DSPL and MOEAs testing on 8 feature models with synthetic performance models. Results are good enough, showing that transfer learning solutions dominate up to 71% of the non-transfer learning ones for similar DSPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {51–59},
numpages = {9},
keywords = {transfer learning, self-adaptation, multiobjective optimization algorithms, dynamic software product lines, cyber-physical systems},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3503229.3547026,
author = {Friesel, Birte and Elmenhorst, Kathrin and Kaiser, Lennart and M\"{u}ller, Michael and Spinczyk, Olaf},
title = {kconfig-webconf: retrofitting performance models onto kconfig-based software product lines},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547026},
doi = {10.1145/3503229.3547026},
abstract = {Despite decades of research and clear advantages, performance-aware configuration of real-world software product lines is still an exception rather than the norm. One reason for this may be tooling: configuration software with support for non-functional property models is generally not compatible with the configuration and build process of existing product lines. Specifically, the Kconfig language is popular in open source software projects, but neither language nor configuration frontends support performance models. To address this, we present kconfig-webconf: a performance-aware, Kconfig-compatible software product line configuration frontend. It is part of a toolchain that can automatically generate performance models with a minimal amount of changes to a software product line's build process. With such a performance model, kconfig-webconf can serve as a performance-aware drop-in replacement for existing Kconfig frontends. We evaluate its usage in five examples, including the busybox multi-call binary and the resKIL agricultural AI product line.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {58–61},
numpages = {4},
keywords = {kconfig, performance prediction, product lines, regression trees},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/1842752.1842809,
author = {Helleboogh, Alexander and Avgeriou, Paris and Bouck\'{e}, Nelis and Heymans, Patrick},
title = {Workshop on Variability in Software Product Line Architectures (VARI-ARCH 2010)},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842809},
doi = {10.1145/1842752.1842809},
abstract = {The objective of this workshop is to bring together researchers from the software product line community and software architecture community to identify critical challenges and progress the state-of-the-art on variability in software product line architectures.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {309–311},
numpages = {3},
keywords = {viewpoint, view, variability, software architecture, product lines, product line architecture, model, concern, assets},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@inproceedings{10.1145/3546932.3546989,
author = {Bertolotti, Francesco and Cazzola, Walter and Favalli, Luca},
title = {Features, believe it or not! a design pattern for first-class citizen features on stock JVM},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3546989},
doi = {10.1145/3546932.3546989},
abstract = {Modern software systems must fulfill the needs of an ever-growing customer base. Due to the innate diversity of human needs, software should be highly customizable and reconfigurable. Researchers and practitioners gained interest in software product lines (SPL), mimicking aspects of product lines in industrial production for the engineering of highly-variable systems. There are two main approaches towards the engineering of SPLs. The first uses macros---such as the #ifdef macro in C. The second---called feature-oriented programming (FOP)---uses variability-aware preprocessors called composers to generate a program variant from a set of features and a configuration. Both approaches have disadvantages. Most notably, these approaches are usually not supported by the base language; for instance Java is one of the most commonly used FOP languages among researchers, but it does not support macros rather it relies on the C preprocessor or a custom one to translate macros into actual Java code. As a result, developers must struggle to keep up with the evolution of the base language, hindering the general applicability of SPL engineering. Moreover, to effectively evolve a software configuration and its features, their location must be known. The problem of recording and maintaining traceability information is considered expensive and error-prone and it is once again handled externally through dedicated modeling languages and tools. Instead, to properly convey the FOP paradigm, software features should be treated as first-class citizens using concepts that are proper to the host language, so that the variability can be expressed and analyzed with the same tools used to develop any other software in the same language. In this paper, we present a simple and flexible design pattern for JVM-based languages---dubbed devise pattern---that can be used to express feature dependencies and behaviors with a light-weight syntax both at domain analysis and at domain implementation level. To showcase the qualities and feasibility of our approach, we present several variability-aware implementations of a MNIST-encoder---including one using the devise pattern---and compare strengths and weaknesses of each approach.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {32–42},
numpages = {11},
keywords = {variability modeling, software product lines, design patterns},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3236405.3236426,
author = {Belarbi, Maouaheb},
title = {A methodological framework to enable the generation of code from DSML in SPL},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236426},
doi = {10.1145/3236405.3236426},
abstract = {Software Product Line has acquired a significant momentum at the end of the 1990ies since it allows the production of variable software systems corresponding to the same domain portfolio. The effectiveness of the derivation process depends on how well variability is defined and implemented which is a crucial topic area that was addressed among two essential trends: On the one hand, starting from Domain Specific Modelling Language to express domain requirements and automate the code generation with Model-Driven Engineering techniques and on the second hand, exploiting the soar of variability mechanisms.In this context, the current research presents a method that unifies the two aforementioned approaches to cover the overall strategies by defining a framework that allows a better code generation in terms of documentation, maintainability, rapidity,etc. The starting point is the usage of the Domain Specific Modelling Language to represent the stakeholders requirements. Then, the resulting meta-model will be converted into one our several Feature Diagrams on which variability mechanisms can be applied to generate all the family products.A preliminary experiment has been undertaken to design the methodology of the proposed software factory in a meta-model. The validation task was evaluated with an academic use case called HandiWeb developed to facilitate handicap persons access to the internet. The first results allow us to put the hand on the key challenges that must be resolved by the proposed methodology.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {64–71},
numpages = {8},
keywords = {variability, software factory, methodology, SPL, DSML},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3336294.3336302,
author = {Str\"{u}ber, Daniel and Mukelabai, Mukelabai and Kr\"{u}ger, Jacob and Fischer, Stefan and Linsbauer, Lukas and Martinez, Jabier and Berger, Thorsten},
title = {Facing the Truth: Benchmarking the Techniques for the Evolution of Variant-Rich Systems},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336302},
doi = {10.1145/3336294.3336302},
abstract = {The evolution of variant-rich systems is a challenging task. To support developers, the research community has proposed a range of different techniques over the last decades. However, many techniques have not been adopted in practice so far. To advance such techniques and to support their adoption, it is crucial to evaluate them against realistic baselines, ideally in the form of generally accessible benchmarks. To this end, we need to improve our empirical understanding of typical evolution scenarios for variant-rich systems and their relevance for benchmarking. In this paper, we establish eleven evolution scenarios in which benchmarks would be beneficial. Our scenarios cover typical lifecycles of variant-rich system, ranging from clone &amp; own to adopting and evolving a configurable product-line platform. For each scenario, we formulate benchmarking requirements and assess its clarity and relevance via a survey with experts in variant-rich systems and software evolution. We also surveyed the existing benchmarking landscape, identifying synergies and gaps. We observed that most scenarios, despite being perceived as important by experts, are only partially or not at all supported by existing benchmarks-a call to arms for building community benchmarks upon our requirements. We hope that our work raises awareness for benchmarking as a means to advance techniques for evolving variant-rich systems, and that it will lead to a benchmarking initiative in our community.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {177–188},
numpages = {12},
keywords = {software variability, software evolution, product lines, benchmark},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3106195.3106212,
author = {Marimuthu, C. and Chandrasekaran, K.},
title = {Systematic Studies in Software Product Lines: A Tertiary Study},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106212},
doi = {10.1145/3106195.3106212},
abstract = {Software product lines are widely used in the software industries to increase the re-usability and to decrease maintenance cost. On the other hand, systematic reviews are widely used in the software engineering research community to provide the overview of the research field and practitioners guidelines. Researchers have conducted many systematic studies on the different aspects of SPLs. To the best of our knowledge, till now there is no tertiary study conducted on systematic studies of SPL related research topics. In this paper, we aim at conducting a systematic mapping study of existing systematic studies to report the overview of the findings for researchers and practitioners. We performed snowballing and automated search to find out the relevant systematic studies. As a result, we analyzed 60 relevant studies to answer 5 research questions. The main focus of this tertiary study is to highlight the research topics, type of published reviews, active researchers and publication forums. Additionally, we highlight some of the limitations of the systematic studies. The important finding of this study is that the research field is well matured as the systematic studies covered a wide range of research topics. Another important finding is that many studies provided information for practitioners as well as researchers which is a notable improvement in the systematic reviews. However, many studies failed to assess the quality of the primary studies which is the major limitation of the existing systematic studies.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {143–152},
numpages = {10},
keywords = {tertiary study, systematic review, software product line},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3233027.3236395,
author = {Pereira, Juliana Alves and Maciel, Lucas and Noronha, Thiago F. and Figueiredo, Eduardo},
title = {Heuristic and exact algorithms for product configuration in software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236395},
doi = {10.1145/3233027.3236395},
abstract = {The Software Product Line (SPL) configuration field is an active area of research and has attracted both practitioners and researchers attention in the last years. A key part of an SPL configuration is a feature model that represents features and their dependencies (i.e., SPL configuration rules). This model can be extended by adding Non-Functional Properties (NFPs) as feature attributes resulting in Extended Feature Models (EFMs). Configuring products from an EFM requires considering the configuration rules of the model and satisfying the product functional and non-functional requirements. Although the configuration of a product arising from EFMs may reduce the space of valid configurations, selecting the most appropriate set of features is still an overwhelming task due to many factors including technical limitations and diversity of contexts. Consequently, configuring large and complex SPLs by using configurators is often beyond the users' capabilities of identifying valid combinations of features that match their (non-functional) requirements. To overcome this limitation, several approaches have modeled the product configuration task as a combinatorial optimization problem and proposed constraint programming algorithms to automatically derive a configuration. Although these approaches do not require any user intervention to guarantee the optimality of the generated configuration, due to the NP-hard computational complexity of finding an optimal variant, exact approaches have inefficient exponential time. Thus, to improve scalability and performance issues, we introduced the adoption of a greedy heuristic algorithm and a biased random-key genetic algorithm (BRKGA). Our experiment results show that our proposed heuristics found optimal solutions for all instances where those are known. For the instances where optimal solutions are not known, the greedy heuristic outperformed the best solution obtained by a one-hour run of the exact algorithm by up to 67.89%. Although the BRKGA heuristic slightly outperformed the greedy heuristic, it has shown larger running times (especially on the largest instances). Therefore, to ensure a good user experience and enable a very fast configuration task, we extended a state-of-the-art configurator with the proposed greedy heuristic approach.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {247},
numpages = {1},
keywords = {software product lines, software product line configuration, search-based software engineering, configuration optimization},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3382025.3414943,
author = {Th\"{u}m, Thomas},
title = {A BDD for Linux? the knowledge compilation challenge for variability},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414943},
doi = {10.1145/3382025.3414943},
abstract = {What is the number of valid configurations for Linux? How to generate uniform random samples for Linux? Can we create a binary decision diagram for Linux? It seems that the product-line community tries hard to answer such questions for Linux and other configurable systems. However, attempts are often not published due to the publication bias (i.e., unsuccessful attempts are not published). As a consequence, researchers keep trying by potentially spending redundant effort. The goal of this challenge is to guide research on these computationally complex problems and to foster the exchange between researchers and practitioners.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {16},
numpages = {6},
keywords = {software product line, software configuration, satisfiability solving, product configuration, knownledge compilation, feature models, decision models, configurable system, binary decision diagrams, artificial intelligence},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3307630.3342705,
author = {Krieter, Sebastian},
title = {Enabling Efficient Automated Configuration Generation and Management},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342705},
doi = {10.1145/3307630.3342705},
abstract = {Creating and managing valid configurations is one of the main tasks in software product line engineering. Due to the often complex constraints from a feature model, some kind of automated configuration generation is required to facilitate the configuration process for users and developers. For instance, decision propagation can be applied to support users in configuring a product from a software product line (SPL) with less manual effort and error potential, leading to a semi-automatic configuration process. Furthermore, fully-automatic configuration processes, such as random sampling or t-wise interaction sampling can be employed to test or to optimize an SPL. However, current techniques for automated configuration generation still do not scale well to SPLs with large and complex feature models. Within our thesis, we identify current challenges regarding the efficiency and effectiveness of the semi- and fully-automatic configuration process and aim to address these challenges by introducing novel techniques and improving current ones. Our preliminary results show already show promising progress for both, the semi- and fully-automatic configuration process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {215–221},
numpages = {7},
keywords = {uniform random sampling, t-wise sampling, software product lines, decision propagation, configurable system},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/1383559.1383571,
author = {Tawhid, Rasha and Petriu, Dorina C.},
title = {Towards automatic derivation of a product performance model from a UML software product line model},
year = {2008},
isbn = {9781595938732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1383559.1383571},
doi = {10.1145/1383559.1383571},
abstract = {Software Product Line (SPL) engineering is a software development approach that takes advantage of the commonality and variability between products from a family, and supports the generation of specific products by reusing a set of core family assets. This paper proposes a UML model transformation approach for software product lines to derive a performance model for a specific product. The input to the proposed technique, the "source model", is a UML model of a SPL with performance annotations, which uses two separate profiles: a "product line" profile from literature for specifying the commonality and variability between products, and the MARTE profile recently standardized by OMG for performance annotations. The source model is generic and therefore its performance annotations must be parameterized. The proposed derivation of a performance model for a concrete product requires two steps: a) the transformation of a SPL model to a UML model with performance annotations for a given product, and b) the transformation of the outcome of the first step into a performance model. This paper focuses on the first step, whereas the second step will use the PUMA transformation approach of annotated UML models to performance models, developed in previous work. The output of the first step, named "target model", is a UML model with MARTE annotations, where the variability expressed in the SPL model has been analyzed and bound to a specific product, and the generic performance annotations have been bound to concrete values for the product. The proposed technique is illustrated with an e-commerce case study.},
booktitle = {Proceedings of the 7th International Workshop on Software and Performance},
pages = {91–102},
numpages = {12},
keywords = {uml, software product line, software performance engineering, model transformation, marte},
location = {Princeton, NJ, USA},
series = {WOSP '08}
}

@inproceedings{10.1145/3461002.3473073,
author = {Pett, Tobias and Krieter, Sebastian and Th\"{u}m, Thomas and Lochau, Malte and Schaefer, Ina},
title = {AutoSMP: an evaluation platform for sampling algorithms},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473073},
doi = {10.1145/3461002.3473073},
abstract = {Testing configurable systems is a challenging task due to the combinatorial explosion problem. Sampling is a promising approach to reduce the testing effort for product-based systems by finding a small but still representative subset (i.e., a sample) of all configurations for testing. The quality of a generated sample wrt. evaluation criteria such as run time of sample generation, feature coverage, sample size, and sampling stability depends on the subject systems and the sampling algorithm. Choosing the right sampling algorithm for practical applications is challenging because each sampling algorithm fulfills the evaluation criteria to a different degree. Researchers keep developing new sampling algorithms with improved performance or unique properties to satisfy application-specific requirements. Comparing sampling algorithms is therefore a necessary task for researchers. However, this task needs a lot of effort because of missing accessibility of existing algorithm implementations and benchmarks. Our platform AutoSMP eases practitioners and researchers lifes by automatically executing sampling algorithms on predefined benchmarks and evaluating the sampling results wrt. specific user requirements. In this paper, we introduce the open-source application of AutoSMP and a set of predefined benchmarks as well as a set of T-wise sampling algorithms as examples.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {41–44},
numpages = {4},
keywords = {sampling evalutaion, sampling, product lines},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3109729.3109751,
author = {Krieter, Sebastian and Pinnecke, Marcus and Kr\"{u}ger, Jacob and Sprey, Joshua and Sontag, Christopher and Th\"{u}m, Thomas and Leich, Thomas and Saake, Gunter},
title = {FeatureIDE: Empowering Third-Party Developers},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109751},
doi = {10.1145/3109729.3109751},
abstract = {FeatureIDE is a popular open-source tool for modeling, implementing, configuring, and analyzing software product lines. However, FeatureIDE's initial design was lacking mechanisms that facilitate extension and reuse of core implementations. In current releases, we improve these traits by providing a modular concept for core data structures and functionalities. As a result, we are facilitating the usage of external implementations for feature models and file formats within FeatureIDE. Additionally, we provide a Java library containing FeatureIDE's core functionalities, including feature modeling and configuration. This allows developers to use these functionalities in their own tools without relying on external dependencies, such as the Eclipse framework.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {42–45},
numpages = {4},
keywords = {feature-oriented software development, feature modeling, configuration, Software product line},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2491627.2491631,
author = {Myll\"{a}rniemi, Varvana and Savolainen, Juha and M\"{a}nnist\"{o}, Tomi},
title = {Performance variability in software product lines: a case study in the telecommunication domain},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491631},
doi = {10.1145/2491627.2491631},
abstract = {In the research on software product lines, product variants typically differ by their functionality, and quality attributes are more or less similar across products. To accumulate empirical evidence, this paper presents a descriptive case study of performance variability in a software product line of mobile network base stations. The goal is to study the motivation to vary performance, and the strategy for realizing performance variability in the product line architecture. The results highlight that the evolution of customer needs motivates performance variability; performance variability can be realized either with software or hardware variability strategy, with the latter often being prevailing; and the software strategy can be kept focused by downgrading performance.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {32–41},
numpages = {10},
keywords = {variability, software product line, case study, architecture},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and J\'{e}z\'{e}quel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2362536.2362548,
author = {Soltani, Samaneh and Asadi, Mohsen and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Bagheri, Ebrahim},
title = {Automated planning for feature model configuration based on functional and non-functional requirements},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362548},
doi = {10.1145/2362536.2362548},
abstract = {Feature modeling is one of the main techniques used in Software Product Line Engineering to manage the variability within the products of a family. Concrete products of the family can be generated through a configuration process. The configuration process selects and/or removes features from the feature model according to the stakeholders' requirements. Selecting the right set of features for one product from amongst all of the available features in the feature model is a complex task because: 1) the multiplicity of stakeholders' functional requirements; 2) the positive or negative impact of features on non-functional properties; and 3) the stakeholders' preferences w.r.t. the desirable non-functional properties of the final product. Many configurations techniques have already been proposed to facilitate automated product derivation. However, most of the current proposals are not designed to consider stakeholders' preferences and constraints especially with regard to non-functional properties. We address the software product line configuration problem and propose a framework, which employs an artificial intelligence planning technique to automatically select suitable features that satisfy both the stakeholders' functional and non-functional preferences and constraints. We also provide tooling support to facilitate the use of our framework. Our experiments show that despite the complexity involved with the simultaneous consideration of both functional and non-functional properties our configuration technique is scalable.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {56–65},
numpages = {10},
keywords = {software product line engineering, planning techniques, feature model, configuration, artificial intelligence},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3382025.3414962,
author = {Chrszon, Philipp and Baier, Christel and Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha},
title = {From features to roles},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414962},
doi = {10.1145/3382025.3414962},
abstract = {The detection of interactions is a challenging task present in almost all stages of software development. In feature-oriented system design, this task is mainly investigated for interactions of features within a single system, detected by their emergent behaviors. We propose a formalism to describe interactions in hierarchies of feature-oriented systems (hierarchical interactions) and the actual situations where features interact (active interplays). Based on the observation that such interactions are also crucial in role-based systems, we introduce a compositional modeling framework based on concepts and notions of roles, comprising role-based automata (RBAs). To describe RBAs, we present a modeling language that is close to the input language of the probabilistic model checker Prism. To exemplify the use of RBAs, we implemented a tool that translates RBA models into Prism and thus enables the formal analysis of functional and non-functional properties including system dynamics, contextual changes, and interactions. We carry out two case studies as a proof of concept of such analyses: First, a peer-to-peer protocol case study illustrates how undesired hierarchical interactions can be discovered automatically. Second, a case study on a self-adaptive production cell demonstrates how undesired interactions influence quality-of-service measures such as reliability and throughput.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {19},
numpages = {11},
keywords = {verification, roles, formal methods, feature-oriented systems},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2934466.2946045,
author = {Noir, J\'{e}rome Le and Madel\'{e}nat, S\'{e}bastien and Gailliard, Gr\'{e}gory and Labreuche, Christophe and Acher, Mathieu and Barais, Olivier and Constant, Olivier},
title = {A decision-making process for exploring architectural variants in systems engineering},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946045},
doi = {10.1145/2934466.2946045},
abstract = {In systems engineering, practitioners shall explore numerous architectural alternatives until choosing the most adequate variant. The decision-making process is most of the time a manual, time-consuming, and error-prone activity. The exploration and justification of architectural solutions is ad-hoc and mainly consists in a series of tries and errors on the modeling assets. In this paper, we report on an industrial case study in which we apply variability modeling techniques to automate the assessment and comparison of several candidate architectures (variants). We first describe how we can use a model-based approach such as the Common Variability Language (CVL) to specify the architectural variability. We show that the selection of an architectural variant is a multi-criteria decision problem in which there are numerous interactions (veto, favor, complementary) between criteria.We present a tooled process for exploring architectural variants integrating both CVL and the MYRIAD method for assessing and comparing variants based on an explicit preference model coming from the elicitation of stakeholders' concerns. This solution allows understanding differences among variants and their satisfactions with respect to criteria. Beyond variant selection automation improvement, this experiment results highlight that the approach improves rationality in the assessment and provides decision arguments when selecting the preferred variants.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {277–286},
numpages = {10},
keywords = {systems engineering, multi-criteria decision analysis, model-driven engineering, design exploration, decision-making, architecture},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3307630.3342411,
author = {Meixner, Kristof and Rabiser, Rick and Biffl, Stefan},
title = {Towards Modeling Variability of Products, Processes and Resources in Cyber-Physical Production Systems Engineering},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342411},
doi = {10.1145/3307630.3342411},
abstract = {Planning and developing Cyber-Physical Production Systems (CPPS) are multi-disciplinary engineering activities that rely on effective and efficient knowledge exchange for better collaboration between engineers of different disciplines. The Product-Process-Resource (PPR) approach allows modeling products produced by industrial processes using specific production resources. In practice, a CPPS manufactures a portfolio of product type variants, i.e., a product line. Therefore, engineers need to create and maintain several PPR models to cover PPR variants and their evolving versions. In this paper, we detail a representative use case, identify challenges for using Variability Modeling (VM) methods to describe and manage PPR variants, and present a first solution approach based on cooperation with domain experts at an industry partner, a system integrator of automation for high-performance CPPS. We conclude that integrating basic variability concepts into PPR models is a promising first step and describe our further research plans to support PPR VM in CPPS.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {49–56},
numpages = {8},
keywords = {variability modelling, product-process-resource, cyber-physical production system},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3106195.3106205,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green Configurations of Functional Quality Attributes},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106205},
doi = {10.1145/3106195.3106205},
abstract = {Functional quality attributes (FQAs) are those quality attributes that, to be satisfied, require the incorporation of additional functionality into the application architecture. By adding an FQA (e.g., security) we can improve the quality of the final product, but there is also an increase in energy consumption. This paper proposes a solution to help the software architect to generate configurations of FQAs whilst keeping the energy consumed by the application as low as possible. For this, a usage model is defined for each FQA, taking into account the variables that affect the energy consumption, and that the values of these variables change according to the part of the application where the FQA is required. We extend a Software Product Line that models a family of FQAs to incorporate the variability of the usage model and the existing frameworks that implement FQAs. We generate the most eco-efficient configuration of FQAs by selecting the framework with the most suitable characteristics according to the requirements of the application.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {79–83},
numpages = {5},
keywords = {Variability, SPL, Quality Attributes, FQA, Energy Consumption},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3461001.3471149,
author = {Lesoil, Luc and Acher, Mathieu and T\'{e}rnava, Xhevahire and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {The interplay of compile-time and run-time options for performance prediction},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471149},
doi = {10.1145/3461001.3471149},
abstract = {Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {100–111},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461001.3471146,
author = {Horcas, Jose-Miguel and Galindo, Jos\'{e} A. and Heradio, Ruben and Fernandez-Amoros, David and Benavides, David},
title = {Monte Carlo tree search for feature model analyses: a general framework for decision-making},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471146},
doi = {10.1145/3461001.3471146},
abstract = {The colossal solution spaces of most configurable systems make intractable their exhaustive exploration. Accordingly, relevant analyses remain open research problems. There exist analyses alternatives such as SAT solving or constraint programming. However, none of them have explored simulation-based methods. Monte Carlo-based decision making is a simulation-based method for dealing with colossal solution spaces using randomness. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo methods, which have proven to succeed in vast search spaces (e.g., game theory). Our general framework is described formally, and its flexibility to cope with a diversity of analysis problems is discussed (e.g., finding defective configurations, feature model reverse engineering or getting optimal performance configurations). Additionally, we present a Python implementation of the framework that shows the feasibility of our proposal. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state of the art a step forward.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {190–201},
numpages = {12},
keywords = {variability modeling, software product lines, monte carlo tree search, feature models, configurable systems},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3382026.3431246,
author = {Kenner, Andy},
title = {Model-Based Evaluation of Vulnerabilities in Software Systems},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431246},
doi = {10.1145/3382026.3431246},
abstract = {Vulnerabilities in software systems result from faults, which occur at different stages in a software's life cycle, for example, in the design (i.e., undesired feature-interactions), the development (i.e., buffer overflows), or the operation (i.e., configuration errors). Various databases provide detailed information about vulnerabilities in software systems or the way to exploit it, but face severe limitations. The information is scattered across these databases, fluctuates in quality and granularity, and provides only an insight into a single vulnerability per entry. Even for a single software system it is challenging for any security-related stakeholder to determine the threat level, which consists of all vulnerabilities of the software system and its environment (i.e., operating system). Manual vulnerability management is feasible only to a limited extend if we want to identify all configurations that are affected by vulnerabilities, or determine a system's threat level and the resulting risk we have to deal with. For variant-rich systems, we also have to deal with variability, allowing different stakeholders to understand the threats to their particular setup. To deal with this variability, we propose vulnerability feature models, which offer a homogeneous view on all vulnerabilities of a software system. These models and the resulting analyses offer advantages in many disciplines of the vulnerability management process. In this paper, we report the research plan for our project, in which we focus on the model-based evaluation of vulnerabilities. This includes research objectives that take into account the design of vulnerability feature models, their application in the process of vulnerability management, and the impact of evolution, discovery, and verification of vulnerabilities.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {112–119},
numpages = {8},
keywords = {Vulnerability Analysis and Management, Vulnerability, Variability Model, Feature Model, Exploit},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3307630.3342398,
author = {Beek, Maurice H. ter and Schmid, Klaus and Eichelberger, Holger},
title = {Textual Variability Modeling Languages: An Overview and Considerations},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342398},
doi = {10.1145/3307630.3342398},
abstract = {During the three decades since the invention of the first variability modeling approach [28], there have been multiple attempts to introduce advanced variability modeling capabilities. More recently, we have seen increased attention on textual variability modeling languages. In this paper, we summarize the main capabilities of state of the art textual variability modeling languages, based on [23], including updates regarding more recent work. Based on this integrated characterization, we provide a discussion of additional concerns, opportunities and challenges that are relevant for designing future (textual) variability modeling languages. The paper also summarizes relevant contributions by the authors as input to further discussions on future (textual) variability modeling languages.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {151–157},
numpages = {7},
keywords = {variability modeling, textual specification languages, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2499777.2500710,
author = {Gabillon, Yoann and Biri, Nicolas and Otjacques, Beno\^{\i}t},
title = {Methodology to integrate multi-context UI variations into a feature model},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500710},
doi = {10.1145/2499777.2500710},
abstract = {Software product line (SPL) paradigm aims to explore commonalities and variabilities in a set of applications for developing an efficient derivation of products. One of the most common ways to model variability in this paradigm is to use a Feature Model. However, variability in SPL is often limited to functional features. The User Interface (UI) variations are modeled as entire UIs and thus these variations are not reusable and inspectable. Research in the Human Computer Interaction (HCI) field has proven the importance of variability for non functional, purely UI centric features. The HCI community has proposed several levels of abstraction for multi-context UI design. Indeed, new variations can be introduced at each abstraction level. UI designers are used to them and they usually introduce variability at each step of the UI definition without using SPL. To build usable softwares that take into account UI, we propose to merge functional concerns and UI concerns, providing a methodology to integrate variability of both aspects into a single Feature Model.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {74–81},
numpages = {8},
keywords = {variability, user interface, usability, software product line, multi-context, feature model, abstraction levels},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3382025.3414965,
author = {Young, Jeffrey M. and Walkingshaw, Eric and Th\"{u}m, Thomas},
title = {Variational satisfiability solving},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414965},
doi = {10.1145/3382025.3414965},
abstract = {Incremental satisfiability (SAT) solving is an extension of classic SAT solving that allows users to efficiently solve a set of related SAT problems by identifying and exploiting shared terms. However, using incremental solvers effectively is hard since performance is sensitive to a problem's structure and the order sub-terms are fed to the solver, and the burden to track results is placed on the end user. For analyses that generate sets of related SAT problems, such as those in software product lines, incremental SAT solvers are either not used at all, used but not explicitly stated so in the literature, or used but suffer from the aforementioned usability problems. This paper translates the ordering problem to an encoding problem and automates the use of incremental SAT solving. We introduce variational SAT solving, which differs from incremental SAT solving by accepting all related problems as a single variational input and returning all results as a single variational output. Our central idea is to make explicit the operations of incremental SAT solving, thereby encoding differences between related SAT problems as local points of variation. Our approach automates the interaction with the incremental solver and enables methods to automatically optimize sharing of the input. To evaluate our methods we construct a prototype variational SAT solver and perform an empirical analysis on two real-world datasets that applied incremental solvers to software evolution scenarios. We show, assuming a variational input, that the prototype solver scales better for these problems than naive incremental solving while also removing the need to track individual results.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {18},
numpages = {12},
keywords = {variation, software product lines, satisfiability solving, choice calculus},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3382025.3414963,
author = {Creff, Stephen and Noir, J\'{e}r\^{o}me Le and Lenormand, Eric and Madel\'{e}nat, S\'{e}bastien},
title = {Towards facilities for modeling and synthesis of architectures for resource allocation problem in systems engineering},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414963},
doi = {10.1145/3382025.3414963},
abstract = {Exploring architectural design space is often beyond human capacity and makes architectural design a difficult task. Model-based systems engineering must include assistance to the system designer in identifying candidate architectures to subsequently analyze tradeoffs. Unfortunately, existing languages and approaches do not incorporate this concern, generally favoring solution analysis over exploring a set of candidate architectures.In this paper, we explore the advantages of designing and configuring the variability problem to solve one of the problems of exploring (synthesizing) candidate architectures in systems engineering: the resource allocation problem. More specifically, this work reports on the use of the Clafer modeling language and its gateway to the CSP Choco Solver, on an industrial case study of heterogeneous hardware resource allocation (GPP-GPGPU-FPGA).Based on experiments on the modeling in Clafer, and the impact of its translation into the constraint programming paradigm (performance studies), discussions highlight some issues concerning facilities for modeling and synthesis of architectures and recommendations are proposed towards the use of this variability approach.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {32},
numpages = {11},
keywords = {variability modeling, empirical study, constraint solving, architecture synthesis, allocation problem},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3382025.3414945,
author = {G\"{o}ttmann, Hendrik and Luthmann, Lars and Lochau, Malte and Sch\"{u}rr, Andy},
title = {Real-time-aware reconfiguration decisions for dynamic software product lines},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414945},
doi = {10.1145/3382025.3414945},
abstract = {Dynamic Software Product Lines (DSPL) have recently shown promising potentials as integrated engineering methodology for (self-)adaptive software systems. Based on the software-configuration principles of software product lines, DSPL additionally foster reconfiguration capabilities to continuously adapt software products to ever-changing environmental contexts. However, in most recent works concerned with finding near-optimal reconfiguration decisions, real-time aspects of reconfiguration processes are usually out of scope. In this paper, we present a model-based methodology for specifying and automatically analyzing real-time constraints of reconfiguration decisions in a feature-oriented and compositional way. Those real-time aware DSPL specifications are internally translated into timed automata, a well-founded formalism for real-time behaviors. This representation allows for formally reasoning about consistency and worst-case/best-case execution-time behaviors of sequences of reconfiguration decisions. The technique is implemented in a prototype tool and experimentally evaluated with respect to a set of case studies1.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {13},
numpages = {11},
keywords = {timed automata, reconfiguration decisions, dynamic software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2499777.2500725,
author = {Varshosaz, Mahsa and Khosravi, Ramtin},
title = {Discrete time Markov chain families: modeling and verification of probabilistic software product lines},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500725},
doi = {10.1145/2499777.2500725},
abstract = {Software product line engineering (SPLE) enables systematic reuse in development of a family of related software systems by explicitly defining commonalities and variabilities among the individual products in the family. Nowadays, SPLE is used in a variety of complex domains such as avionics and automotive. As such domains include safety critical systems which exhibit probabilistic behavior, there is a major need for modeling and verification approaches dealing with probabilistic aspects of systems in the presence of variabilities. In this paper, we introduce a mathematical model, Discrete Time Markov Chain Family (DTMCF), which compactly represents the probabilistic behavior of all the products in the product line. We also provide a probabilistic model checking method to verify DTMCFs against Probabilistic Computation Tree Logic (PCTL) properties. This way, instead of verifying each product individually, the whole family is model checked at once, resulting in the set of products satisfying the desired property. This reduces the required cost for model checking by eliminating redundant processing caused by the commonalities among the products.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {34–41},
numpages = {8},
keywords = {variable discrete time Markov chains, software product line, probabilistic model checking},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3307630.3342404,
author = {Th\"{u}m, Thomas and Seidl, Christoph and Schaefer, Ina},
title = {On Language Levels for Feature Modeling Notations},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342404},
doi = {10.1145/3307630.3342404},
abstract = {Configuration is a key enabling technology for the engineering of systems and software as wells as physical goods. A selection of configuration options (aka. features) is often enough to automatically generate a product tailored to the needs of a customer. It is common that not all combinations of features are possible in a given domain. Feature modeling is the de-facto standard for specifying features and their valid combinations. However, a pivotal hurdle for practitioners, researchers, and teachers in applying feature modeling is that there are hundreds of tools and languages available. While there have been first attempts to define a standard feature modeling language, they still struggle with finding an appropriate level of expressiveness. If the expressiveness is too high, the language will not be adopted, as it is too much effort to support all language constructs. If the expressiveness is too low, the language will not be adopted, as many interesting domains cannot be modeled in such a language. Towards a standard feature modeling notation, we propose the use of language levels with different expressiveness each and discuss criteria to be used to define such language levels. We aim to raise the awareness on the expressiveness and eventually contribute to a standard feature modeling notation.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {158–161},
numpages = {4},
keywords = {variability modeling, product lines, language design, feature model, expressiveness, automated analysis},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3307630.3342403,
author = {Berger, Thorsten and Collet, Philippe},
title = {Usage Scenarios for a Common Feature Modeling Language},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342403},
doi = {10.1145/3307630.3342403},
abstract = {Feature models are recognized as a de facto standard for variability modeling. Presented almost three decades ago, dozens of different variations and extensions to the original feature-modeling notation have been proposed, together with hundreds of variability management techniques building upon feature models. Unfortunately, despite several attempts to establish a unified language, there is still no emerging consensus on a feature-modeling language that is both intuitive and simple, but also expressive enough to cover a range of important usage scenarios. There is not even a documented and commonly agreed set of such scenarios.Following an initiative among product-line engineering researchers in September 2018, we present 14 usage scenarios together with examples and requirements detailing each scenario. The scenario descriptions are the result of a systematic process, where members of the initiative authored original descriptions, which received feedback via a survey, and which we then refined and extended based on the survey results, reviewers' comments, and our own expertise. We also report the relevance of supporting each usage scenario for the language, as perceived by the initiative's members, prioritizing each scenario. We present a roadmap to build and implement a first version of the envisaged common language.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {174–181},
numpages = {8},
keywords = {unified language, software product lines, feature models},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2019136.2019168,
author = {Nakagawa, Elisa Yumi and Antonino, Pablo Oliveira and Becker, Martin},
title = {Exploring the use of reference architectures in the development of product line artifacts},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019168},
doi = {10.1145/2019136.2019168},
abstract = {Software Product Line (SPL) has arisen as an approach for developing a family of software-intensive systems at lower costs, within shorter time, and with higher quality. In particular, SPL is supported by a product line architecture (sometimes also referred to as reference architecture) that captures the architectures of a product family. In another context, a special type of architecture that contains knowledge about a specific domain has been increasingly investigated, resulting in the Reference Architecture research area. In spite of the positive impact of this type of architecture on reuse and productivity, the use of existing domain-specific reference architectures as basis of SPL has not been widely explored. The main contribution of this paper is to present how and when elements contained in existing reference architectures could contribute to the building of SPL artifacts during development of an SPL. We have observed that, in fact, reference architectures could make an important contribution to improving reuse and productivity, which are also important concerns in SPL.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {28},
numpages = {8},
keywords = {software product line, reference architecture, SPL design method},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2491627.2491630,
author = {Linsbauer, Lukas and Lopez-Herrejon, E. Roberto and Egyed, Alexander},
title = {Recovering traceability between features and code in product variants},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491630},
doi = {10.1145/2491627.2491630},
abstract = {Many companies offer a palette of similar software products though they do not necessarily have a Software Product Line (SPL). Rather, they start building and selling individual products which they then adapt, customize and extend for different customers. As the number of product variants increases, these companies then face the severe problem of having to maintain them all. Software Product Lines can be helpful here - not so much as a platform for creating new products but as a means of maintaining the existing ones with their shared features. Here, an important first step is to determine where features are implemented in the source code and in what product variants. To this end, this paper presents a novel technique for deriving the traceability between features and code in product variants by matching code overlaps and feature overlaps. This is a difficult problem because a feature's implementation not only covers its basic functionality (which does not change across product variants) but may include code that deals with feature interaction issues and thus changes depending on the combination of features present in a product variant. We empirically evaluated the approach on three non-trivial case studies of different sizes and domains and found that our approach correctly identifies feature to code traces except for code that traces to multiple disjunctive features, a rare case involving less than 1% of the code.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {131–140},
numpages = {10},
keywords = {traceability, product variants, features},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3106195.3106215,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Self-healing in Service Mashups Through Feature Adaptation},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106215},
doi = {10.1145/3106195.3106215},
abstract = {The composition of the functionality of multiple services into a single unique service mashup has received wide interest in the recent years. Given the distributed nature of these mashups where the constituent services can be located on different servers, it is possible that a change in the functionality or availability of a constituent service result in the failure of the service mashup. In this paper, we propose a novel method based on the Software Product Line Engineering (SPLE) paradigm which is able to find an alternate valid service mashup which has maximum possible number of original service mashup features in order to mitigate a service failure when complete recovery is not possible. This method also has an advantage that it can recover or mitigate the failure automatically without requiring the user to specify any adaptation rule or strategy. We show the practicality of our proposed approach through extensive experiments.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {94–103},
numpages = {10},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3233027.3233039,
author = {Pereira, Juliana Alves and Schulze, Sandro and Figueiredo, Eduardo and Saake, Gunter},
title = {N-dimensional tensor factorization for self-configuration of software product lines at runtime},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233039},
doi = {10.1145/3233027.3233039},
abstract = {Dynamic software product lines demand self-adaptation of their behavior to deal with runtime contextual changes in their environment and offer a personalized product to the user. However, taking user preferences and context into account impedes the manual configuration process, and thus, an efficient and automated procedure is required. To automate the configuration process, context-aware recommendation techniques have been acknowledged as an effective mean to provide suggestions to a user based on their recognized context. In this work, we propose a collaborative filtering method based on tensor factorization that allows an integration of contextual data by modeling an N-dimensional tensor User-Feature-Context instead of the traditional two-dimensional User-Feature matrix. In the proposed approach, different types of non-functional properties are considered as additional contextual dimensions. Moreover, we show how to self-configure software product lines by applying our N-dimensional tensor factorization recommendation approach. We evaluate our approach by means of an empirical study using two datasets of configurations derived for medium-sized product lines. Our results reveal significant improvements in the predictive accuracy of the configuration over a state-of-the-art non-contextual matrix factorization approach. Moreover, it can scale up to a 7-dimensional tensor containing hundred of configurations in a couple of milliseconds.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {87–97},
numpages = {11},
keywords = {software product lines, self-configuration, runtime decision-making, recommender systems},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3307630.3342414,
author = {Th\"{u}m, Thomas and Teixeira, Leopoldo and Schmid, Klaus and Walkingshaw, Eric and Mukelabai, Mukelabai and Varshosaz, Mahsa and Botterweck, Goetz and Schaefer, Ina and Kehrer, Timo},
title = {Towards Efficient Analysis of Variation in Time and Space},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342414},
doi = {10.1145/3307630.3342414},
abstract = {Variation is central to today's software development. There are two fundamental dimensions to variation: Variation in time refers to the fact that software exists in numerous revisions that typically replace each other (i.e., a newer version supersedes an older one). Variation in space refers to differences among variants that are designed to coexist in parallel. There are numerous analyses to cope with variation in space (i.e., product-line analyses) and others that cope with variation in time (i.e., regression analyses). The goal of this work is to discuss to which extent product-line analyses can be applied to revisions and, conversely, where regression analyses can be applied to variants. In addition, we discuss challenges related to the combination of product-line and regression analyses. The overall goal is to increase the efficiency of analyses by exploiting the inherent commonality between variants and revisions.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {57–64},
numpages = {8},
keywords = {variability-aware analysis, variability management, software variation, software product lines, software evolution, software configuration management, regression analysis, product-line analysis},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2499777.2500711,
author = {Ciolfi Felice, Marianela and Filho, Joao Bosco Ferreira and Acher, Mathieu and Blouin, Arnaud and Barais, Olivier},
title = {Interactive visualisation of products in online configurators: a case study for variability modelling technologies},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500711},
doi = {10.1145/2499777.2500711},
abstract = {Numerous companies develop interactive environments to assist users in customising sales products through the selection of configuration options. A visual representation of these products is an important factor in terms of user experience. However, an analysis of 100+ existing configurators highlights that not all provide visual representations of configured products. One of the current challenges is the trade-off developers face between either the memory consuming use of pregenerated images of all the combinations of options, or rendering products on the fly, which is non trivial to implement efficiently. We believe that a new approach to associate product configurations to visual representations is needed to compose and render them dynamically. In this paper we present a formal statement of the problem and a model-driven perspective for addressing it as well as our ongoing work and further challenges.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {82–85},
numpages = {4},
keywords = {variability modelling, user interface, software product line, configurator},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@article{10.1145/3628158,
author = {Xiang, Yi and Huang, Han and Li, Sizhe and Li, Miqing and Luo, Chuan and Yang, Xiaowei},
title = {Automated Test Suite Generation for Software Product Lines Based on Quality-Diversity Optimization},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3628158},
doi = {10.1145/3628158},
abstract = {A Software Product Line (SPL) is a set of software products that are built from a variability model. Real-world SPLs typically involve a vast number of valid products, making it impossible to individually test each of them. This arises the need for automated test suite generation, which was previously modeled as either a single-objective or a multi-objective optimization problem considering only objective functions. This article provides a completely different mathematical model by exploiting the benefits of Quality-Diversity (QD) optimization that is composed of not only an objective function (e.g., t-wise coverage or test suite diversity) but also a user-defined behavior space (e.g., the space with test suite size as its dimension). We argue that the new model is more suitable and generic than the two alternatives because it provides at a time a large set of diverse (measured in the behavior space) and high-performing solutions that can ease the decision-making process. We apply MAP-Elites, one of the most popular QD algorithms, to solve the model. The results of the evaluation, on both realistic and artificial SPLs, are promising, with MAP-Elites significantly and substantially outperforming both single- and multi-objective approaches, and also several state-of-the-art SPL testing tools. In summary, this article provides a new and promising perspective on the test suite generation for SPLs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {46},
numpages = {52},
keywords = {Quality-Diversity (QD) optimization, automated test suite generation, Software Product Line}
}

@inproceedings{10.1145/3233027.3233035,
author = {Varshosaz, Mahsa and Al-Hajjaji, Mustafa and Th\"{u}m, Thomas and Runge, Tobias and Mousavi, Mohammad Reza and Schaefer, Ina},
title = {A classification of product sampling for software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233035},
doi = {10.1145/3233027.3233035},
abstract = {The analysis of software product lines is challenging due to the potentially large number of products, which grow exponentially in terms of the number of features. Product sampling is a technique used to avoid exhaustive testing, which is often infeasible. In this paper, we propose a classification for product sampling techniques and classify the existing literature accordingly. We distinguish the important characteristics of such approaches based on the information used for sampling, the kind of algorithm, and the achieved coverage criteria. Furthermore, we give an overview on existing tools and evaluations of product sampling techniques. We share our insights on the state-of-the-art of product sampling and discuss potential future work.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {1–13},
numpages = {13},
keywords = {testing, software product lines, sampling algorithms, feature interaction, domain models},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2499777.2500719,
author = {Schr\"{o}ter, Reimar and Siegmund, Norbert and Th\"{u}m, Thomas},
title = {Towards modular analysis of multi product lines},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500719},
doi = {10.1145/2499777.2500719},
abstract = {Software product-line engineering enables efficient development of tailor-made software by means of reusable artifacts. As practitioners increasingly develop software systems as product lines, there is a growing potential to reuse product lines in other product lines, which we refer to as multi product line. We identify challenges when developing multi product lines and propose interfaces for different levels of abstraction ranging from variability modeling to functional and non-functional properties. We argue that these interfaces ease the reuse of product lines and identify research questions that need to be solved toward modular analysis of multi product lines.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {96–99},
numpages = {4},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/2647908.2655972,
author = {Meinicke, Jens and Th\"{u}m, Thomas and Schr\"{o}ter, Reimar and Benduhn, Fabian and Saake, Gunter},
title = {An overview on analysis tools for software product lines},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655972},
doi = {10.1145/2647908.2655972},
abstract = {A software product line is a set of different software products that share commonalities. For a selection of features, specialized products of one domain can be generated automatically from domain artifacts. However, analyses of software product lines need to handle a large number of products that can be exponential in the number of features. In the last decade, many approaches have been proposed to analyze software product lines efficiently. For some of these approaches tool support is available. Based on a recent survey on analysis for software product lines, we provide a first overview on such tools. While our discussion is limited to analysis tools, we provide an accompanying website covering further tools for product-line development. We compare tools according to their analysis and implementation strategy to identify underrepresented areas. In addition, we want to ease the reuse of existing tools for researchers and students, and to simplify research transfer to practice.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {94–101},
numpages = {8},
keywords = {type checking, tool support, theorem proving, testing, static analysis, software product lines, sampling, non-functional properties, model checking, code metrics},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2491627.2491647,
author = {Murashkin, Alexandr and Antkiewicz, Micha\l{} and Rayside, Derek and Czarnecki, Krzysztof},
title = {Visualization and exploration of optimal variants in product line engineering},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491647},
doi = {10.1145/2491627.2491647},
abstract = {The decision-making process in Product Line Engineering (PLE) is often concerned with variant qualities such as cost, battery life, or security. Pareto-optimal variants, with respect to a set of objectives such as minimizing a variant's cost while maximizing battery life and security, are variants in which no single quality can be improved without sacrificing other qualities. We propose a novel method and a tool for visualization and exploration of a multi-dimensional space of optimal variants (i.e., a Pareto front). The visualization method is an integrated, interactive, and synchronized set of complementary views onto a Pareto front specifically designed to support PLE scenarios, including: understanding differences among variants and their positioning with respect to quality dimensions; solving trade-offs; selecting the most desirable variants; and understanding the impact of changes during product line evolution on a variant's qualities. We present an initial experimental evaluation showing that the visualization method is a good basis for supporting these PLE scenarios.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {111–115},
numpages = {5},
keywords = {visualization, product line engineering, pareto front, optimal variant, feature modeling, exploration, clafer, ClaferMoo visualizer, ClaferMoo},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2019136.2019158,
author = {Guana, Victor and Correal, Dario},
title = {Variability quality evaluation on component-based software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019158},
doi = {10.1145/2019136.2019158},
abstract = {Quality assurance and evaluation in Model Driven Software Product Lines (MD-SPLs) are pivotal points for the growing and solidification of the generative software factories. They are framed as one of the future fact methodologies for the construction of software systems. Although several approximations address the problem of generative environments, software product line scope expression, and core asset definition, not many of them try to solve, as a fundamental step, the automation of the quality attribute evaluation in the MD-SPL development cycle. This paper presents a model-driven engineering method and a tool for the quality evaluation of product line configurations through a cross architectural view analysis.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {19},
numpages = {8},
keywords = {sensitivity point, quality attribute, model-driven software product line, model composition, domain specific modeling},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2791060.2791099,
author = {Filho, Jo\~{a}o Bosco Ferreira and Allier, Simon and Barais, Olivier and Acher, Mathieu and Baudry, Benoit},
title = {Assessing product line derivation operators applied to Java source code: an empirical study},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791099},
doi = {10.1145/2791060.2791099},
abstract = {Product Derivation is a key activity in Software Product Line Engineering. During this process, derivation operators modify or create core assets (e.g., model elements, source code instructions, components) by adding, removing or substituting them according to a given configuration. The result is a derived product that generally needs to conform to a programming or modeling language. Some operators lead to invalid products when applied to certain assets, some others do not; knowing this in advance can help to better use them, however this is challenging, specially if we consider assets expressed in extensive and complex languages such as Java. In this paper, we empirically answer the following question: which product line operators, applied to which program elements, can synthesize variants of programs that are incorrect, correct or perhaps even conforming to test suites? We implement source code transformations, based on the derivation operators of the Common Variability Language. We automatically synthesize more than 370,000 program variants from a set of 8 real large Java projects (up to 85,000 lines of code), obtaining an extensive panorama of the sanity of the operations.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {36–45},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2791060.2791100,
author = {ter Beek, Maurice H. and Fantechi, Alessandro and Gnesi, Stefania},
title = {Applying the product lines paradigm to the quantitative analysis of collective adaptive systems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791100},
doi = {10.1145/2791060.2791100},
abstract = {Engineering a Collective Adaptive System (CAS) requires the support of a framework for quantitative modeling and analysis of the system. In order to jointly address variability and quantitative analysis, we apply the Product Lines paradigm, considered at the level of system engineering, to a case study of the European project QUANTICOL, by first defining a reference feature model and then adding feature attributes and global quantitative constraints, in the form of a Clafer attributed feature model. ClaferMOOVisualizer is subsequently used for quantitative analyses and multi-objective optimization of the resulting attributed feature model.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {321–326},
numpages = {6},
keywords = {variability analysis, quantitative modeling, quantitative analysis, multi-objective optimization, collective adaptive systems, ClaferMOO},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2934466.2946046,
author = {Arrieta, Aitor and Wang, Shuai and Sagardui, Goiuria and Etxeberria, Leire},
title = {Search-based test case selection of cyber-physical system product lines for simulation-based validation},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946046},
doi = {10.1145/2934466.2946046},
abstract = {Cyber-Physical Systems (CPSs) are often tested at different test levels following "X-in-the-Loop" configurations: Model-, Software- and Hardware-in-the-loop (MiL, SiL and HiL). While MiL and SiL test levels aim at testing functional requirements at the system level, the HiL test level tests functional as well as non-functional requirements by performing a real-time simulation. As testing CPS product line configurations is costly due to the fact that there are many variants to test, test cases are long, the physical layer has to be simulated and co-simulation is often necessary. It is therefore extremely important to select the appropriate test cases that cover the objectives of each level in an allowable amount of time. We propose an efficient test case selection approach adapted to the "X-in-the-Loop" test levels. Search algorithms are employed to reduce the amount of time required to test configurations of CPS product lines while achieving the test objectives of each level. We empirically evaluate three commonly-used search algorithms, i.e., Genetic Algorithm (GA), Alternating Variable Method (AVM) and Greedy (Random Search (RS) is used as a baseline) by employing two case studies with the aim of integrating the best algorithm into our approach. Results suggest that as compared with RS, our approach can reduce the costs of testing CPS product line configurations by approximately 80% while improving the overall test quality.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {297–306},
numpages = {10},
keywords = {test case selection, search-based software engineering, cyber-physical system product lines},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3233027.3233046,
author = {Beek, Maurice H. ter and Fantechi, Alessandro and Gnesi, Stefania},
title = {Product line models of large cyber-physical systems: the case of ERTMS/ETCS},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233046},
doi = {10.1145/3233027.3233046},
abstract = {A product line perspective may help to understand the possible variants in interactions between the subsystems of a large, cyber-physical system. This observation is exemplified in this paper by proposing a feature model of the family of ERTMS/ETCS train control systems and their foreseen extensions. This model not only shows the different components that have to be installed when deploying the system at the different levels established by the ERTMS/ETCS standards, but it also helps to identify and discuss specific issues, such as the borders between onboard and wayside equipment, different manufacturers of the subsystems, interoperability among systems developed at different levels, backward compatibility of trains equipped with higher level equipment running on lines equipped with lower level equipment, and evolution towards future trends of railway signalling. The feature model forms the basis for formal modelling of the behaviour of the critical components of the system and for evaluating the overall cost, effectiveness and sustainability, for example by adding cost and performance attributes to the feature model.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {208–214},
numpages = {7},
keywords = {variability, product lines, feature models, cyber-physical systems, ERTMS/ETCS train control systems},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2791060.2791082,
author = {Hotz, Lothar and Wang, Yibo and Riebisch, Matthias and G\"{o}tz, Olaf and Lackhove, Josef},
title = {Evaluation across multiple views for variable automation systems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791082},
doi = {10.1145/2791060.2791082},
abstract = {Automation systems in industry are often software-intensive systems consisting of software and hardware components. During their development several engineers of different disciplines are involved, such as mechanical, electrical and software engineering. Each engineer focuses on specific system aspects to be developed. To enable an efficient development, product lines especially with feature models for variability modeling are promising technologies. In order to reduce the complexity of both feature models and development process, views on feature models can be applied. The use of views for filtering purposes constitutes an established method. However, views also enable further options missing in current approaches, such as evaluations regarding requirements, including non-functional ones. This paper presents an approach for evaluation across multiple views to enable collaborative development for developers who focus on different system aspects. We validate our approach by applying it in an industrial project for the planning of flying saws.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {311–315},
numpages = {5},
keywords = {product lines, multi-criteria evaluation, feature model, consistency check, configuration, automation systems},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2362536.2362567,
author = {Savolainen, Juha and Mannion, Mike and Kuusela, Juha},
title = {Developing platforms for multiple software product lines},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362567},
doi = {10.1145/2362536.2362567},
abstract = {Many approaches to software product line engineering have been founded on the development of a single product line platform. However as customer requirements change and new products are added to the product line, software producers recognize that the platform cannot be "stretched" indefinitely and a significant problem is striking a balance between development efficiency by increasing platform commonality and customer dissatisfaction from products with additional undesirable features and properties.One alternative is to develop multiple product lines (MPLs). However the challenge remains about what to include in a multiple product line platform. Drawing upon industrial experience of working with 4 companies, this paper explores the characteristics of the contexts in which MPLs are a viable alternative development strategy and then proposes a framework of approaches to platform development.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {220–228},
numpages = {9},
keywords = {software reuse, multiple product lines, industrial experience},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2019136.2019177,
author = {Abbas, Nadeem and Andersson, Jesper and Weyns, Danny},
title = {Knowledge evolution in autonomic software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019177},
doi = {10.1145/2019136.2019177},
abstract = {We describe ongoing work in knowledge evolution management for autonomic software product lines. We explore how an autonomic product line may benefit from new knowledge originating from different source activities and artifacts at run time. The motivation for sharing run-time knowledge is that products may self-optimize at run time and thus improve quality faster compared to traditional software product line evolution. We propose two mechanisms that support knowledge evolution in product lines: online learning and knowledge sharing. We describe two basic scenarios for runtime knowledge evolution that involves these mechanisms. We evaluate online learning and knowledge sharing in a small product line setting that shows promising results.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {36},
numpages = {8},
keywords = {software product-lines, software design, self-adaptation, product-line management, online learning, knowledge sharing},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2791060.2791066,
author = {Dhungana, Deepak and Falkner, Andreas and Haselb\"{o}ck, Alois and Schreiner, Herwig},
title = {Smart factory product lines: a configuration perspective on smart production ecosystems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791066},
doi = {10.1145/2791060.2791066},
abstract = {Smart production aims to increase the flexibility of the production processes and be more efficient in the use of resources. Two important pillars of this initiative are "smart products" and "smart factories". From the perspective of product line engineering, these can be seen as two product lines (product line of factories and product line of goods) that need to be integrated for a common systems engineering approach. In this paper, we look at this problem from the perspective of configuration technologies, outline the research challenges in this area and illustrate our vision using an industrial example. The factory product line goes hand-in-hand with the product line of the products to be manufactured. Future research in product line engineering needs to consider an ecosystem of a multitude of stakeholders - e.g., factory component vendors, product designers, factory owners/operators and end-consumers.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {201–210},
numpages = {10},
keywords = {smart production, smart product, smart factory, product line of factories, product and production configuration},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3571788.3571793,
author = {Schulthei\ss{}, Alexander and Bittner, Paul Maximilian and Greiner, Sandra and Kehrer, Timo},
title = {Benchmark Generation with VEVOS: A Coverage Analysis of Evolution Scenarios in Variant-Rich Systems},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571788.3571793},
doi = {10.1145/3571788.3571793},
abstract = {Clone-and-own development is a simple and flexible approach to realize multi-variant software systems in practice but typically provokes costly challenges in maintaining a continuously evolving set of variants. Therefore, managed clone-and-own development is key to efficiently mitigate these problems. While supporting techniques have been proposed in the literature, hardly any of them have been evaluated in a realistic setting due to a substantial lack of publicly available clone-and-own projects which could be used as experimental subjects. Recently, we presented the benchmark generation framework VEVOS for simulating clone-and-own development. However, it is yet unclear to which extent VEVOS can cover key scenarios for evaluating evolving variant-rich systems. This paper examines to what extent benchmarks created by VEVOS satisfy evaluation requirements for evolution scenarios demanded by the community. In addition, we report on our own experiences when employing VEVOS within six studies and elaborate on necessary extensions we implemented into VEVOS.},
booktitle = {Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {13–22},
numpages = {10},
keywords = {software product lines, empirical evaluation, Clone-and-own},
location = {Odense, Denmark},
series = {VaMoS '23}
}

@inproceedings{10.1145/2499777.2500715,
author = {Ishida, Yuzo},
title = {Scalable variability management for enterprise applications with data model driven development},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500715},
doi = {10.1145/2499777.2500715},
abstract = {Unlike embedded systems, some of enterprise systems are evolved over the decades. The predictability of requirements is a key to success in building reusable assets however it is very hard to predict future business context changes, which are driving factors of requirements. Thus, both functional and context variability must be managed in order to satisfy ever-changing requirements. Scalability does matter for enterprise systems in two aspects. One aspect comes from data volume. Once data become big, it is difficult to maintain performance requirements without de-normalizing database schema. Since database de-normalization is driven by non-functional properties, a model driven approach is not feasible if the model cannot express such properties. Another aspect comes from the unpredictability of future functional requirements. A functional decomposition of enterprise systems usually introduces ever-increasing complexity among systems' interactions due to cross-cutting requirements across functional systems. This paper reflects our empirical studies in data intensive large enterprise systems such as retail and telecommunication industries with industry independent application framework to separate functional and non-functional concerns. Our variability management technique is based on database schema modeling, which can be evolved incrementally in scaling an enterprise system with both data and functional aspects.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {90–93},
numpages = {4},
keywords = {type theory, relational algebra, quality attributes, higher-order simple predicate logic, core assets},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/2362536.2362560,
author = {Lettner, Daniela and Vierhauser, Michael and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {Supporting end users with business calculations in product configuration},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362560},
doi = {10.1145/2362536.2362560},
abstract = {Business calculations like break-even, return on investment, or cost are essential in many domains to support decision making while configuring products. For instance, customers and sales people need to estimate and compare the business value of different product variants. Some product line approaches provide initial support, e.g., by defining quality attributes in relation to features. However, an approach that allows domain engineers to easily define business calculations together with variability models is still lacking. In product configuration, calculation results need to be instantly presented to end users after making configuration choices. Further, due to the often high number of calculations, the presentation of calculation results to end users can be challenging. These challenges cannot be addressed by integrating off-the-shelf applications performing the calculations with product line tools. We thus present an approach based on dedicated calculation models that are related to variability models. Our approach seamlessly integrates business calculations with product configuration and provides support for formatting calculations and calculation results. We use the DOPLER tool suite to deploy calculations together with variability models to end users in product configuration. We evaluate the expressiveness and practical relevance of the approach by investigating the development of business calculations for 15 product lines from the domain of industrial automation.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {171–180},
numpages = {10},
keywords = {variability models, product configuration, business calculations},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2019136.2019178,
author = {Brataas, Gunnar and Jiang, Shanshan and Reichle, Roland and Geihs, Kurt},
title = {Performance property prediction supporting variability for adaptive mobile systems},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019178},
doi = {10.1145/2019136.2019178},
abstract = {A performance property prediction (PPP) method for component-based self-adaptive applications is presented. Such performance properties are required by an adaptation middleware for reasoning about adaptation activities. Our PPP method is based on the Structure and Performance (SP) framework, a conceptually simple, yet powerful performance modelling framework based on matrices. The main contribution of this paper are the integration of SP-based PPP into a comprehensive model- and variability-based adaptation framework for context-aware mobile applications. A meta model for the SP method is described. The framework is demonstrated using a practical example.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {37},
numpages = {8},
keywords = {mobile systems, autonomic computing},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1145/3361146,
author = {Hierons, Robert M. and Li, Miqing and Liu, Xiaohui and Parejo, Jose Antonio and Segura, Sergio and Yao, Xin},
title = {Many-Objective Test Suite Generation for Software Product Lines},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3361146},
doi = {10.1145/3361146},
abstract = {A Software Product Line (SPL) is a set of products built from a number of features, the set of valid products being defined by a feature model. Typically, it does not make sense to test all products defined by an SPL and one instead chooses a set of products to test (test selection) and, ideally, derives a good order in which to test them (test prioritisation). Since one cannot know in advance which products will reveal faults, test selection and prioritisation are normally based on objective functions that are known to relate to likely effectiveness or cost. This article introduces a new technique, the grid-based evolution strategy (GrES), which considers several objective functions that assess a selection or prioritisation and aims to optimise on all of these. The problem is thus a many-objective optimisation problem. We use a new approach, in which all of the objective functions are considered but one (pairwise coverage) is seen as the most important. We also derive a novel evolution strategy based on domain knowledge. The results of the evaluation, on randomly generated and realistic feature models, were promising, with GrES outperforming previously proposed techniques and a range of many-objective optimisation algorithms.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {2},
numpages = {46},
keywords = {test selection, test prioritisation, multi-objective optimisation, Software product line}
}

@article{10.1145/2853073.2853082,
author = {Soujanya, K. L.S. and AnandaRao, A.},
title = {A Generic Framework for Configuration Management of SPL and Controlling Evolution of Complex Software Products},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853082},
doi = {10.1145/2853073.2853082},
abstract = {Efficient configuration management system is crucial for the success of any software product line (SPL). Due to ever changing needs of customers, SPL undergoes constant changes that are to be tracked in real time. In the context of customer-driven development, anticipation and change management are to be given paramount importance. It demands implementation of software variability that drives home changed, extended and customized configurations besides economy at scale. Moreover, the emergence of distributed technologies, the unprecedented growth of component based, serviceoriented systems throw ever increasing challenges to software product line configuration management. Derivation of a new product is a dynamic process in software product line that should consider functionality and quality attributes. Very few approaches are found on configuration management (CM) of SPL though CM is enough matured for traditional products. They are tailor made and inadequate to provide a general solution. Stated differently, a comprehensive approach for SPL configuration management and product derivation is still to be desired. In this paper, we proposed a framework that guides in doing so besides helping in SPL definitions in generic way. Our framework facilitates SPL configuration management and product derivation based on critical path analysis, weight computation and feedback. We proposed two algorithms namely Quality Driven Product Derivation (QDPD) and Composition Analysis algorithm for generating satisfied compositions and to find best possible composition respectively. The usage of weights and critical path analysis improves quality of product derivation. The framework is extensible and flexible thus it can be leveraged with variability-aware design patterns and ontology. We built a prototype that demonstrates the proof of concept. We tested our approach with Dr. School product line. The results reveal that the framework supports configuration management of SPL and derivation of high quality product in the product line. We evaluated results with ground truth to establish significance of our implementation},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–10},
numpages = {10},
keywords = {weighted approach, product derivation, critical path analysis, configuration management, Software product line}
}

@inproceedings{10.5555/1753235.1753255,
author = {Sun, Hongyu and Lutz, Robyn R. and Basu, Samik},
title = {Product-line-based requirements customization for web service compositions},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Customizing web services according to users' individual functional and non-functional requirements has become increasingly difficult as the number of users increases. This paper introduces a new way to customize and verify composite web services by incorporating a software product-line engineering approach into web-service composition. The approach uses a partitioning similar to that between domain engineering and application engineering in the product-line context. It specifies the options that the user can select and constructs the resulting web-service compositions. By first creating a web-service composition search space that satisfies the common requirements and then querying the search space as the user selects values for the parameters of variation, we provide a more efficient way to customize web services. A decision model, illustrated with examples from an emergency-response application, is created to interact with the customers and ensure the consistency of their specifications. The capability to reuse the composition search space may also help improve the quality and reliability of the composite services and reduce the cost of re-verifying the same compositions.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {141–150},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.5555/1753235.1753263,
author = {Than Tun, Thein and Boucher, Quentin and Classen, Andreas and Hubaux, Arnaud and Heymans, Patrick},
title = {Relating requirements and feature configurations: a systematic approach},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {A feature model captures various possible configurations of products within a product family. When configuring a product, several features are selected and composed. Selecting features at the program level has a general limitation of not being able to relate the resulting configuration to its requirements. As a result, it is difficult to decide whether a given configuration of features is optimal. An optimal configuration satisfies all stakeholder requirements and quantitative constraints, while ensuring that there is no extraneous feature in it. In relating requirements and feature configurations, we use the description of the problem world context in which the software is designed to operate as the intermediate description between them. The advantage of our approach is that feature selection can be done at the requirements level, and an optimal program level configuration can be generated from the requirements selected. Our approach is illustrated with a real-life problem of configuring a satellite communication software. The use of an existing tool to support our approach is also discussed.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {201–210},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2499777.2499779,
author = {Antkiewicz, Micha\l{} and B\k{a}k, Kacper and Murashkin, Alexandr and Olaechea, Rafael and Liang, Jia Hui (Jimmy) and Czarnecki, Krzysztof},
title = {Clafer tools for product line engineering},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499779},
doi = {10.1145/2499777.2499779},
abstract = {Clafer is a lightweight yet expressive language for structural modeling: feature modeling and configuration, class and object modeling, and metamodeling. Clafer Tools is an integrated set of tools based on Clafer. In this paper, we describe some product-line variability modeling scenarios of Clafer Tools from the viewpoints of product-line owner, product-line engineer, and product engineer.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {130–135},
numpages = {6},
keywords = {clafer configurator, ClaferWiki, ClaferMOO visualizer, ClaferMOO, ClaferIG, Clafer},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@article{10.1145/3088440,
author = {Acher, Mathieu and Lopez-Herrejon, Roberto E. and Rabiser, Rick},
title = {Teaching Software Product Lines: A Snapshot of Current Practices and Challenges},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
url = {https://doi.org/10.1145/3088440},
doi = {10.1145/3088440},
abstract = {Software Product Line (SPL) engineering has emerged to provide the means to efficiently model, produce, and maintain multiple similar software variants, exploiting their common properties, and managing their variabilities (differences). With over two decades of existence, the community of SPL researchers and practitioners is thriving, as can be attested by the extensive research output and the numerous successful industrial projects. Education has a key role to support the next generation of practitioners to build highly complex, variability-intensive systems. Yet, it is unclear how the concepts of variability and SPLs are taught, what are the possible missing gaps and difficulties faced, what are the benefits, and what is the material available. Also, it remains unclear whether scholars teach what is actually needed by industry. In this article, we report on three initiatives we have conducted with scholars, educators, industry practitioners, and students to further understand the connection between SPLs and education, that is, an online survey on teaching SPLs we performed with 35 scholars, another survey on learning SPLs we conducted with 25 students, as well as two workshops held at the International Software Product Line Conference in 2014 and 2015 with both researchers and industry practitioners participating. We build upon the two surveys and the workshops to derive recommendations for educators to continue improving the state of practice of teaching SPLs, aimed at both individual educators as well as the wider community.},
journal = {ACM Trans. Comput. Educ.},
month = oct,
articleno = {2},
numpages = {31},
keywords = {variability modeling, software product line teaching, software engineering teaching, Software product lines}
}

@article{10.1145/3313789,
author = {Reuling, Dennis and Kelter, Udo and B\"{u}rdek, Johannes and Lochau, Malte},
title = {Automated N-way Program Merging for Facilitating Family-based Analyses of Variant-rich Software},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3313789},
doi = {10.1145/3313789},
abstract = {Nowadays software tends to come in many different, yet similar variants, often derived from a common code base via clone-and-own. Family-based-analysis strategies have recently shown very promising potential for improving efficiency in applying quality-assurance techniques to such variant-rich programs, as compared to variant-by-variant approaches. Unfortunately, these strategies require a single program representation superimposing all program variants in a syntactically well-formed, semantically sound, and variant-preserving manner, which is usually not available and manually hard to obtain in practice. In this article, we present a novel methodology, called SiMPOSE, for automatically generating superimpositions of existing program variants to facilitate family-based analyses of variant-rich software. To this end, we propose a novel N-way model-merging methodology to integrate the control-flow automaton (CFA) representations of N given variants of a C program into one unified CFA representation. CFA constitute a unified program abstraction used by many recent software-analysis tools for automated quality assurance. To cope with the inherent complexity of N-way model-merging, our approach (1) utilizes principles of similarity-propagation to reduce the number of potential N-way matches, and (2) enables us to decompose a set of N variants into arbitrary subsets and to incrementally derive an N-way superimposition from partial superimpositions. We apply our tool implementation of SiMPOSE to a selection of realistic C programs, frequently considered for experimental evaluation of program-analysis techniques. In particular, we investigate applicability and efficiency/effectiveness trade-offs of our approach by applying SiMPOSE in the context of family-based unit-test generation as well as model-checking as sample program-analysis techniques. Our experimental results reveal very impressive efficiency improvements by an average factor of up to 2.6 for test-generation and up to 2.4 for model-checking under stable effectiveness, as compared to variant-by-variant approaches, thus amortizing the additional effort required for merging. In addition, our results show that merging all N variants at once produces, in almost all cases, clearly more precise results than incremental step-wise 2-way merging. Finally, our comparison with major existing N-way merging techniques shows that SiMPOSE constitutes, in most cases, the best efficiency/effectiveness trade-off.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {13},
numpages = {59},
keywords = {variability encoding, quality assurance, model matching, control flow automata, Program merging}
}

@inproceedings{10.1145/3180155.3180159,
author = {Krieter, Sebastian and Th\"{u}m, Thomas and Schulze, Sandro and Schr\"{o}ter, Reimar and Saake, Gunter},
title = {Propagating configuration decisions with modal implication graphs},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180159},
doi = {10.1145/3180155.3180159},
abstract = {Highly-configurable systems encompass thousands of interdependent configuration options, which require a non-trivial configuration process. Decision propagation enables a backtracking-free configuration process by computing values implied by user decisions. However, employing decision propagation for large-scale systems is a time-consuming task and, thus, can be a bottleneck in interactive configuration processes and analyses alike. We propose modal implication graphs to improve the performance of decision propagation by precomputing intermediate values used in the process. Our evaluation results show a significant improvement over state-of-the-art algorithms for 120 real-world systems.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {898–909},
numpages = {12},
keywords = {configuration, decision propagation, software product line},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3194078.3194082,
author = {Pukhkaiev, Dmytro and G\"{o}tz, Sebastian},
title = {BRISE: energy-efficient benchmark reduction},
year = {2018},
isbn = {9781450357326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194078.3194082},
doi = {10.1145/3194078.3194082},
abstract = {A considerable portion of research activities in computer science heavily relies on the process of benchmarking, e.g., to evaluate a hypothesis in an empirical study. The goal is to reveal how a set of independent variables (factors) influences one or more dependent variables. With a vast number of factors or a high amount of factors' values (levels), this process becomes time- and energy-consuming. Current approaches to lower the benchmarking effort suffer from two deficiencies: (1) they focus on reducing the number of factors and, hence, are inapplicable to experiments with only two factors, but a vast number of levels and (2) being adopted from, e.g., combinatorial optimization they are designed for a different search space structure and, thus, can be very wasteful. This paper provides an approach for benchmark reduction, based on adaptive instance selection and multiple linear regression. We evaluate our approach using four empirical studies, which investigate the effect made by dynamic voltage and frequency scaling in combination with dynamic concurrency throttling on the energy consumption of a computing system (parallel compression, sorting, and encryption algorithms as well as database query processing). Our findings show the effectiveness of the approach. We can save 78% of benchmarking effort, while the result's quality decreases only by 3 pp, due to using only a near-optimal configuration.},
booktitle = {Proceedings of the 6th International Workshop on Green and Sustainable Software},
pages = {23–30},
numpages = {8},
keywords = {non-functional properties, fractional factorial design, benchmarking, adaptive instance selection, active learning},
location = {Gothenburg, Sweden},
series = {GREENS '18}
}

@inproceedings{10.1145/2556624.2556628,
author = {Lengauer, Philipp and Bitto, Verena and Angerer, Florian and Gr\"{u}nbacher, Paul and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {Where has all my memory gone? determining memory characteristics of product variants using virtual-machine-level monitoring},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556628},
doi = {10.1145/2556624.2556628},
abstract = {Non-functional properties such as memory footprint have recently gained importance in software product line research. However, determining the memory characteristics of individual features and product variants is extremely challenging. We present an approach that supports the monitoring of memory characteristics of individual features at the level of Java virtual machines. Our approach provides extensions to Java virtual machines to track memory allocations and deal-locations of individual features based on a feature-to-code mapping. The approach enables continuous monitoring at the level of features to detect anomalies such as memory leaks, excessive memory consumption, or abnormal garbage collection times in product variants. We provide an evaluation of our approach based on different product variants of the DesktopSearcher product line. Our experiment with different program inputs demonstrates the feasibility of our technique.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {13},
numpages = {8},
keywords = {monitoring, memory footprint, feature-oriented software development, Java},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@inproceedings{10.1145/3555776.3578613,
author = {T\"{e}rnava, Xhevahire and Acher, Mathieu and Combemale, Benoit},
title = {Specialization of Run-time Configuration Space at Compile-time: An Exploratory Study},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3578613},
doi = {10.1145/3555776.3578613},
abstract = {Numerous software systems are highly configurable through runtime options (e.g., command-line parameters). Users can tune some of the options to meet various functional and non-functional requirements such as footprint, security, or execution time. However, some options are never set for a given system instance, and their values remain the same whatever the use cases of the system. Herein, we design a controlled experiment in which the system's run-time configuration space can be specialized at compile-time and combinations of options can be removed on demand. We perform an in-depth study of the well-known x264 video encoder and quantify the effects of its specialization to its non-functional properties, namely on binary size, attack surface, and performance while ensuring its validity. Our exploratory study suggests that the configurable specialization of a system has statistically significant benefits on most of its analysed non-functional properties, which benefits depend on the number of the debloated options. While our empirical results and insights show the importance of removing code related to unused run-time options to improve software systems, an open challenge is to further automate the specialization process.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1459–1468},
numpages = {10},
keywords = {unused variability, debloating, performance, program specialization},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1109/ICSE.2019.00090,
author = {Lillack, Max and St\u{a}nciulescu, \c{S}tefan and Hedman, Wilhelm and Berger, Thorsten and W\k{a}sowski, Andrzej},
title = {Intention-based integration of software variants},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00090},
doi = {10.1109/ICSE.2019.00090},
abstract = {Cloning is a simple way to create new variants of a system. While cheap at first, it increases maintenance cost in the long term. Eventually, the cloned variants need to be integrated into a configurable platform. Such an integration is challenging: it involves merging the usual code improvements between the variants, and also integrating the variable code (features) into the platform. Thus, variant integration differs from traditional software merging, which does not produce or organize configurable code, but creates a single system that cannot be configured into variants. In practice, variant integration requires fine-grained code edits, performed in an exploratory manner, in multiple iterations. Unfortunately, little tool support exists for integrating cloned variants.In this work, we show that fine-grained code edits needed for integration can be alleviated by a small set of integration intentions---domain-specific actions declared over code snippets controlling the integration. Developers can interactively explore the integration space by declaring (or revoking) intentions on code elements. We contribute the intentions (e.g., 'keep functionality' or 'keep as a configurable feature') and the IDE tool INCLINE, which implements the intentions and five editable views that visualize the integration process and allow declaring intentions producing a configurable integrated platform. In a series of experiments, we evaluated the completeness of the proposed intentions, the correctness and performance of INCLINE, and the benefits of using intentions for variant integration. The experiments show that INCLINE can handle complex integration tasks, that views help to navigate the code, and that it consistently reduces mistakes made by developers during variant integration.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {831–842},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3571788.3571801,
author = {Acher, Mathieu and Lesoil, Luc and Randrianaina, Georges Aaron and T\"{e}rnava, Xhevahire and Zendra, Olivier},
title = {A Call for Removing Variability},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571788.3571801},
doi = {10.1145/3571788.3571801},
abstract = {Software variability is largely accepted and explored in software engineering and seems to have become a norm and a must, if only in the context of product lines. Yet, the removal of superfluous or unneeded software artefacts and functionalities is an inevitable trend. It is frequently investigated in relation to software bloat. This paper is essentially a call to the community on software variability to devise methods and tools that will facilitate the removal of unneeded variability from software systems. The advantages are expected to be numerous in terms of functional and non-functional properties, such as maintainability (lower complexity), security (smaller attack surface), reliability, and performance (smaller binaries).},
booktitle = {Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {82–84},
numpages = {3},
keywords = {software variability, software bloat, removing variability},
location = {Odense, Denmark},
series = {VaMoS '23}
}

@inproceedings{10.1145/2814251.2814263,
author = {Ochoa, Lina and Gonz\'{a}lez-Rojas, Oscar and Th\"{u}m, Thomas},
title = {Using decision rules for solving conflicts in extended feature models},
year = {2015},
isbn = {9781450336864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814251.2814263},
doi = {10.1145/2814251.2814263},
abstract = {Software Product Line Engineering has introduced feature modeling as a domain analysis technique used to represent the variability of software products and decision-making scenarios. We present a model-based transformation approach to solve conflicts among configurations performed by different stakeholders on feature models. We propose the usage of a domain-specific language named CoCo to specify attributes as non-functional properties of features, and to describe business-related decision rules in terms of costs, time, and human resources. These specifications along with the stakeholders' configurations and the feature model are transformed into a constraint programming problem, on which decision rules are executed to find a non-conflicting set of solution configurations that are aligned to business objectives. We evaluate CoCo's compositionality and model complexity simplification while using a set of motivating decision scenarios.},
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Software Language Engineering},
pages = {149–160},
numpages = {12},
keywords = {model transformation chain, extended feature model, domain-specific language, constraint satisfaction problem, conflicting configurations, Domain engineering},
location = {Pittsburgh, PA, USA},
series = {SLE 2015}
}

@inproceedings{10.1145/3510003.3510200,
author = {Dubslaff, Clemens and Weis, Kallistos and Baier, Christel and Apel, Sven},
title = {Causality in configurable software systems},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510200},
doi = {10.1145/3510003.3510200},
abstract = {Detecting and understanding reasons for defects and inadvertent behavior in software is challenging due to their increasing complexity. In configurable software systems, the combinatorics that arises from the multitude of features a user might select from adds a further layer of complexity. We introduce the notion of feature causality, which is based on counterfactual reasoning and inspired by the seminal definition of actual causality by Halpern and Pearl. Feature causality operates at the level of system configurations and is capable of identifying features and their interactions that are the reason for emerging functional and non-functional properties. We present various methods to explicate these reasons, in particular well-established notions of responsibility and blame that we extend to the feature-oriented setting. Establishing a close connection of feature causality to prime implicants, we provide algorithms to effectively compute feature causes and causal explications. By means of an evaluation on a wide range of configurable software systems, including community benchmarks and real-world systems, we demonstrate the feasibility of our approach: We illustrate how our notion of causality facilitates to identify root causes, estimate the effects of features, and detect feature interactions.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {325–337},
numpages = {13},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/2420942.2420944,
author = {Olaechea, Rafael and Stewart, Steven and Czarnecki, Krzysztof and Rayside, Derek},
title = {Modelling and multi-objective optimization of quality attributes in variability-rich software},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420944},
doi = {10.1145/2420942.2420944},
abstract = {Variability-rich software, such as software product lines, offers optional and alternative features to accommodate varying needs of users. Designers of variability-rich software face the challenge of reasoning about the impact of selecting such features on the quality attributes of the resulting software variant. Attributed feature models have been proposed to model such features and their impact on quality attributes, but existing variability modelling languages and tools have limited or no support for such models and the complex multi-objective optimization problem that arises. This paper presents ClaferMoo, a language and tool that addresses these shortcomings. ClaferMoo uses type inheritance to modularize the attribution of features in feature models and allows specifying multiple optimization goals. We evaluate an implementation of the language on a set of attributed feature models from the literature, showing that the optimization infrastructure can handle small-scale feature models with about a dozen features within seconds.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {2},
numpages = {6},
keywords = {software product lines, multi-objective optimization},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@inproceedings{10.1145/1982185.1982522,
author = {Mohabbati, Bardia and Hatala, Marek and Ga\v{s}evi\'{c}, Dragan and Asadi, Mohsen and Bo\v{s}kovi\'{c}, Marko},
title = {Development and configuration of service-oriented systems families},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982522},
doi = {10.1145/1982185.1982522},
abstract = {Software Product Lines (SPLs) are families of software systems which share a common sets of feature and are developed through common set of core assets in order to promotes software reusability, mass customization, reducing cost, time-to-market and improving the quality of the product. SPLs are sets (i.e., families) of software applications developed as a whole for a specific business domain. Particular applications are derived from software families by selecting the desired features through configuration process. Traditionally, SPLs are implemented with systematically developed components, shared by members of the SPLs and reused every time a new application is derived. In this paper, we propose an approach to the development and configuration of Service-Oriented SPLs in which services are used as reusable assets and building blocks of implementation. Our proposed approach also suggests prioritization of family features according to stakeholder's non-functional requirements (NFRs) and preferences. Priorities of NFRs are used to filter the most important features of the family, which is performed by Stratified Analytic Hierarchical Process (S-AHP). The priorities also are used further for the selection of appropriate services implementation for business processes realizing features. We apply Mixed Integer Linear Programming to find the optimal service selection within the constraints boundaries specified by stakeholders.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {1606–1613},
numpages = {8},
keywords = {software product line, service-oriented architecture, service selection, optimization, feature-oriented development},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@article{10.1145/3647640,
author = {Jord\~{a}o, Rodolfo and Becker, Matthias and Sander, Ingo},
title = {IDeSyDe: Systematic Design Space Exploration via Design Space Identification},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3647640},
doi = {10.1145/3647640},
abstract = {Design space exploration (DSE) is a key activity in embedded design processes, where a mapping between applications and platforms that meets the process design requirements must be found. Finding such mappings is very challenging due to the complexity of modern embedded platforms and applications. DSE tools aid in this challenge by potentially covering sections of the design space that could be unintuitive to designers, leading to more optimised designs. Despite this potential benefit, DSE tools remain relatively niche in the embedded industry. A significant obstacle hindering their wider adoption is integrating such tools into embedded design processes.We present two contributions that address this integration issue. First, we present the design space identification (DSI) approach for systematically constructing DSE solutions that are modular and tuneable. Modularity means that DSE solutions can be reused to construct other DSE solutions, while tuneability means that the most specific DSE solution is chosen for the target DSE problem. Moreover, DSI enables transparent cooperation between exploration algorithms. Second, we present IDeSyDe, an extensible DSE framework for DSE solutions based on DSI. IDeSyDe allows extensions to be developed in different programming languages in a manner compliant with the DSI approach.We showcase the relevance of these contributions through five different case studies. The case study evaluations showed that non-exploration DSI procedures create overheads, which are marginal compared to the exploration algorithms. Empirically, most evaluations average 2% of the total DSE request. More importantly, the case studies have shown that IDeSyDe indeed provides a modular and incremental framework for constructing DSE solutions. In particular, the last case study required minimal extensions over the previous case studies so that support for a new application type was added to IDeSyDe.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = sep,
articleno = {87},
numpages = {45},
keywords = {Design space exploration, design space identification, embedded system design}
}

@inproceedings{10.1145/1629716.1629738,
author = {Alf\'{e}rez, Mauricio and Moreira, Ana and Kulesza, Uir\'{a} and Ara\'{u}jo, Jo\~{a}o and Mateus, Ricardo and Amaral, Vasco},
title = {Detecting feature interactions in SPL requirements analysis models},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629738},
doi = {10.1145/1629716.1629738},
abstract = {The consequences of unwanted feature interactions in a Software Product Line (SPL) can range from minor problems to critical software failures. However, detecting feature interactions in reasonably complex model-based SPLs is a non-trivial task. This is due to the often large number of interdependent models that describe the SPL features and the lack of support for analyzing the relationships inside those models. We believe that the early detection of the points, where two or more features interact --- based on the models that describe the behavior of the features ---, is a starting point for the detection of conflicts and inconsistencies between features, and therefore, take an early corrective action.This vision paper foresees a process to find an initial set of points where it is likely to find potential feature interactions in model-based SPL requirements, by detecting: (i) dependency patterns between features using use case models; and (ii) overlapping between use case scenarios modeled using activity models.We focus on requirements models, which are special, since they do not contain many details about the structural components and the interactions between the higher-level abstraction modules of the system. Therefore, use cases and activity models are the means that help us to analyze the functionality of a complex system looking at it from a high level end-user view to anticipate the places where there are potential feature interactions. We illustrate the approach with a home automation SPL and then discuss about its applicability.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {117–123},
numpages = {7},
keywords = {feature interactions, software product lines requirements},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@inproceedings{10.1145/2110147.2110160,
author = {Schroeter, Julia and Cech, Sebastian and G\"{o}tz, Sebastian and Wilke, Claas and A\ss{}mann, Uwe},
title = {Towards modeling a variable architecture for multi-tenant SaaS-applications},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110160},
doi = {10.1145/2110147.2110160},
abstract = {A widespread business model in cloud computing is to offer software as a service (SaaS) over the Internet. Such applications are often multi-tenant aware, which means that multiple tenants share hardware and software resources of the same application instance. However, SaaS stakeholders have different or even contradictious requirements and interests: For a user, the application's quality and non-functional properties have to be maximized (e.g., choosing the fastest available algorithm for a computation at runtime). In contrast, a resource or application provider is interested in minimizing the operating costs while maximizing his profit. Finally, tenants are interested in offering a customized functionality to their users. To identify an optimal compromise for all these objectives, multiple levels of variability have to be supported by reference architectures for multi-tenant SaaS applications. In this paper, we identify requirements for such a runtime architecture addressing the individual interests of all involved stakeholders. Furthermore, we show how our existing architecture for dynamically adaptive applications can be extended for the development and operation of multi-tenant applications.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {111–120},
numpages = {10},
keywords = {variability modeling, software-as-a-service, self-optimization, multi-tenancy, auto-tuning},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@inproceedings{10.1145/1385486.1385488,
author = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Schirmeier, Horst and Sincero, Julio and Apel, Sven and Leich, Thomas and Spinczyk, Olaf and Saake, Gunter},
title = {FAME-DBMS: tailor-made data management solutions for embedded systems},
year = {2008},
isbn = {9781595939647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1385486.1385488},
doi = {10.1145/1385486.1385488},
abstract = {Data management functionality is not only needed in large-scale server systems, but also in embedded systems. Resource restrictions and heterogeneity of hardware, however, complicate the development of data management solutions for those systems. In current practice, this typically leads to the redevelopment of data management because existing solutions cannot be reused and adapted appropriately. In this paper, we present our ongoing work on FAME-DBMS, a research project that explores techniques to implement highly customizable data management solutions, and illustrate how such systems can be created with a software product line approach. With this approach a concrete instance of a DBMS is derived by composing features of the DBMS product line that are needed for a specific application scenario. This product derivation process is getting complex if a large number of features is available. Furthermore, in embedded systems also non-functional properties, e.g., memory consumption, have to be considered when creating a DBMS instance. To simplify the derivation process we present approaches for its automation.},
booktitle = {Proceedings of the 2008 EDBT Workshop on Software Engineering for Tailor-Made Data Management},
pages = {1–6},
numpages = {6},
location = {Nantes, France},
series = {SETMDM '08}
}

@inproceedings{10.1145/2851613.2851959,
author = {Noorian, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Quality-centric feature model configuration using goal models},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851959},
doi = {10.1145/2851613.2851959},
abstract = {In software product line engineering, a feature model represents the possible configuration space and can be customized based on the stakeholders' needs. Considering the complexity of feature models in addition to the diversity of the stake-holders' expectations, the configuration process is viewed as a complex optimization problem. In this paper, we propose a holistic approach for the configuration process that seeks to satisfy the stakeholders' requirements as well as the feature models' structural and integrity constraints. Here, we model stakeholders' functional and non-functional needs and their preferences using requirement engineering goal models. We formalize the structure of the feature model, the stake-holders' objectives, and their preferences in the form of an integer linear program to automatically perform feature selection.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1296–1299},
numpages = {4},
keywords = {configuration process, feature model, goal model},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/3106237.3106252,
author = {Kn\"{u}ppel, Alexander and Th\"{u}m, Thomas and Mennicke, Stephan and Meinicke, Jens and Schaefer, Ina},
title = {Is there a mismatch between real-world feature models and product-line research?},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106252},
doi = {10.1145/3106237.3106252},
abstract = {Feature modeling has emerged as the de-facto standard to compactly capture the variability of a software product line. Multiple feature modeling languages have been proposed that evolved over the last decades to manage industrial-size product lines. However, less expressive languages, solely permitting require and exclude constraints, are permanently and carelessly used in product-line research. We address the problem whether those less expressive languages are sufficient for industrial product lines. We developed an algorithm to eliminate complex cross-tree constraints in a feature model, enabling the combination of tools and algorithms working with different feature model dialects in a plug-and-play manner. However, the scope of our algorithm is limited. Our evaluation on large feature models, including the Linux kernel, gives evidence that require and exclude constraints are not sufficient to express real-world feature models. Hence, we promote that research on feature models needs to consider arbitrary propositional formulas as cross-tree constraints prospectively.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {291–302},
numpages = {12},
keywords = {require constraints, model transformation, feature modeling, expressiveness, exclude constraints, cross-tree constraints, Software product lines},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/3023956.3023968,
author = {Mjeda, Anila and Wasala, Asanka and Botterweck, Goetz},
title = {Decision spaces in product lines, decision analysis, and design exploration: an interdisciplinary exploratory study},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023968},
doi = {10.1145/3023956.3023968},
abstract = {Context. From recent works on product properties resulting from configurations and the optimisation of these properties, one comes quickly to more complex challenges such as multi-objective optimisation, conflicting objectives, multiple stakeholders, and conflict resolution. The intuition is that Software Product Line Engineering (SPLE) can draw from other disciplines that deal with decision spaces and complex decision scenarios.Objectives. We aim to (1) explore links to such disciplines, (2) systematise and compare concepts, and (3) identify opportunities, where SPLE approaches can be enriched.Method. We undertake an exploratory study: Starting from common SPLE activities and artefacts, we identify aspects where we expect to find corresponding counterparts in other disciplines. We focus on Multiple Criteria Decision Analysis (MCDA), Multi-Objective Optimisation (MOO), and Design Space Exploration (DSE), and perform a comparison of the key concepts.Results. The resulting comparison relates SPLE activities and artefacts to concepts from MCDA, MOO, and DSE and identifies areas where SPLE approaches can be enriched. We also provide examples of existing work at the intersections of SPLE with the other fields. These findings are aimed to foster the conversation on research opportunities where SPLE can draw techniques from other disciplines dealing with complex decision scenarios.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {68–75},
numpages = {8},
keywords = {multi-objective optimisation, multi-criteria decision analysis, design-space exploration, decision modelling},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/2884781.2884821,
author = {Devroey, Xavier and Perrouin, Gilles and Papadakis, Mike and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Featured model-based mutation analysis},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884821},
doi = {10.1145/2884781.2884821},
abstract = {Model-based mutation analysis is a powerful but expensive testing technique. We tackle its high computation cost by proposing an optimization technique that drastically speeds up the mutant execution process. Central to this approach is the Featured Mutant Model, a modelling framework for mutation analysis inspired by the software product line paradigm. It uses behavioural variability models, viz., Featured Transition Systems, which enable the optimized generation, configuration and execution of mutants. We provide results, based on models with thousands of transitions, suggesting that our technique is fast and scalable. We found that it outperforms previous approaches by several orders of magnitude and that it makes higher-order mutation practically applicable.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {655–666},
numpages = {12},
keywords = {variability, mutation analysis, featured transition systems},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.5555/2667025.2667027,
author = {Siegmund, Norbert and Mory, Maik and Feigenspan, Janet and Saake, Gunter and Nykolaychuk, Mykhaylo and Schumann, Marco},
title = {Interoperability of non-functional requirements in complex systems},
year = {2012},
isbn = {9781467318532},
publisher = {IEEE Press},
abstract = {Heterogeneity of embedded systems leads to the development of variable software, such as software product lines. From such a family of programs, stakeholders select the specific variant that satisfies their functional requirements. However, different functionality exposes different non-functional properties of these variants. Especially in the embedded-system domain, non-functional requirements are vital, because resources are scarce. Hence, when selecting an appropriate variant, we have to fulfill also non-functional requirements. Since more systems are interconnected, the challenge is to find a variant that additionally satisfies global nonfunctional (or quality) requirements. In this paper, we advert the problem of achieving interoperability of non-functional requirements among multiple interacting systems using a real-world scenario. Furthermore, we show an approach to find optimal variants for multiple systems that reduces computation effort by means of a stepwise configuration process.},
booktitle = {Proceedings of the Second International Workshop on Software Engineering for Embedded Systems},
pages = {2–8},
numpages = {7},
location = {Zurich, Switzerland},
series = {SEES '12}
}

@article{10.1145/2501654.2501664,
author = {Benoit, Anne and \c{C}ataly\"{u}rek, \"{U}mit V. and Robert, Yves and Saule, Erik},
title = {A survey of pipelined workflow scheduling: Models and algorithms},
year = {2013},
issue_date = {August 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/2501654.2501664},
doi = {10.1145/2501654.2501664},
abstract = {A large class of applications need to execute the same workflow on different datasets of identical size. Efficient execution of such applications necessitates intelligent distribution of the application components and tasks on a parallel machine, and the execution can be orchestrated by utilizing task, data, pipelined, and/or replicated parallelism. The scheduling problem that encompasses all of these techniques is called pipelined workflow scheduling, and it has been widely studied in the last decade. Multiple models and algorithms have flourished to tackle various programming paradigms, constraints, machine behaviors, or optimization goals. This article surveys the field by summing up and structuring known results and approaches.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {50},
numpages = {36},
keywords = {throughput, scheduling, pipeline, parallel systems, models, latency, filter-stream programming, distributed systems, algorithms, Workflow programming}
}

@inproceedings{10.5555/2821357.2821367,
author = {Baresi, Luciano and Quinton, Cl\'{e}ment},
title = {Dynamically evolving the structural variability of dynamic software product lines},
year = {2015},
publisher = {IEEE Press},
abstract = {A Dynamic Software Product Line (dspl) is a widely used approach to handle variability at runtime, e.g., by activating or deactivating features to adapt the running configuration. With the emergence of highly configurable and evolvable systems, dspls have to cope with the evolution of their structural variability, i.e., the Feature Model (fm) used to derive the configuration. So far, little is known about the evolution of the fm while a configuration derived from this fm is running. In particular, such a dynamic evolution changes the dspl configuration space, which is thus unsynchronized with the running configuration and its adaptation capabilities. In this position paper, we propose and describe an initial architecture to manage the dynamic evolution of dspls and their synchronization. In particular, we explain how this architecture supports the evolution of dspls based on fms extended with cardinality and attributes, which, to the best of our knowledge, has never been addressed yet.},
booktitle = {Proceedings of the 10th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {57–63},
numpages = {7},
location = {Florence, Italy},
series = {SEAMS '15}
}

@inproceedings{10.1145/2701319.2701335,
author = {Gamez, Nadia and El Haddad, Joyce and Fuentes, Lidia},
title = {Managing the Variability in the Transactional Services Selection},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701335},
doi = {10.1145/2701319.2701335},
abstract = {Web service composition is the capability to recursively construct a value added service by means of picking up existing services. An important step in the composition process is the selection step, which includes choosing services located in repositories. The selection approaches of Web services need to consider their specifics which raises important challenges as the management of the inherent service variability in functionality and implementation and ensuring correct execution termination between others. To realize reliable service compositions, transactional properties of services must be considered during the selection step. We argue that the transactional properties should be considered at the operation level of each service to be composed. However, modelling transactional services composition at the operation level drastically increment the complexity of service selection. In order to overcome this difficulty, in this paper we report on our research in progress on transactional service selection, which follows a Software Product Line approach considering the set of services that provide the same functionality as part of a service family. We model the variable operations of the service families using Feature Models. In this way, the selection process consists of selecting each service from a service family such that the aggregated transactional property satisfies the user preference.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {88–95},
numpages = {8},
keywords = {Transactional Services, Feature Modeling, Discovery and Selection},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@inproceedings{10.5555/2093889.2093921,
author = {Bagheri, Ebrahim and Asadi, Mohsen and Ensan, Faezeh and Ga\v{s}evi\'{c}, Dragan and Mohabbati, Bardia},
title = {Bringing semantics to feature models with SAFMDL},
year = {2011},
publisher = {IBM Corp.},
address = {USA},
abstract = {Software product line engineering is a paradigm that advocates the reusability of software engineering assets and the rapid development of new applications for a target domain. These objectives are achieved by capturing the commonalities and variabilities between the applications of a target domain and through the development of comprehensive and variability-covering domain models. The domain models developed within the software product line development process need to cover all of the possible features and aspects of the target domain. In other words, the domain models often described using feature models should be elaborate representations of the feature space of that domain. In order to operationalize feature-based representations of a software application, appropriate implementation mechanisms need to be employed. In this paper, we propose a Semantic Web-oriented language, called Semantic Annotations for Feature Modeling Description Language (SAFMDL) that provides the means to semantically describe feature models. We will show that using SAFMDL along with Semantic Web Query techniques, we are able to bridge the gap between software product lines and SOA technology. Our proposed work allows software practitioners to use Semantic Web technology to quickly and rapidly develop new software products based on SOA technology from software product lines.},
booktitle = {Proceedings of the 2011 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {287–300},
numpages = {14},
location = {Toronto, Ontario, Canada},
series = {CASCON '11}
}

@inproceedings{10.1145/1629716.1629735,
author = {Asadi, Mohsen and Mohabbati, Bardia and Kaviani, Nima and Ga\v{s}evi\'{c}, Dragan and Bo\v{s}kovi\'{c}, Marko and Hatala, Marek},
title = {Model-driven development of families of Service-Oriented Architectures},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629735},
doi = {10.1145/1629716.1629735},
abstract = {The paradigms of Service Oriented Architecture (SOA) and Software Product Line Engineering (SPLE) facilitate the development of families of software-intensive products. Software Product Line practices can be leveraged to support the development of service-oriented applications to promote the reusability of assets throughout the iterative and incremental development of software product families. Such an approach enables various service oriented business processes and software products of the same family to be systematically created and integrated. In this paper, we advocate integration of software product line engineering with model driven engineering to enable a model driven specification of software services, capable of creating software products from a family of software services. Using the proposed method, we aim to provide a consistent view of a composed software system from a higher business administration perspective to lower levels of service implementation and deployment. We demonstrate how Model Driven Engineering (MDE) can help with injecting the set of required commonalities and variabilities of a software product from a high level business process design to the lower levels of service use.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {95–102},
numpages = {8},
keywords = {business process management, semantic web, service-oriented architectures, software product lines},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@inproceedings{10.1145/2892664.2892686,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Towards the dynamic reconfiguration of quality attributes},
year = {2016},
isbn = {9781450340335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2892664.2892686},
doi = {10.1145/2892664.2892686},
abstract = {There are some Quality Attributes (QAs) whose variability is addressed through functional variability in the software architecture. Separately modelling the variability of these QAs from the variability of the base functionality of the application has many advantages (e.g., a better reusability), and facilitates the reconfiguration of the QA variants at runtime. Many factors may vary the QA functionality: variations in the user preferences and usage needs; variations in the non-functional QAs; variations in resources, hardware, or even in the functionality of the base application, that directly affect the product's QAs. In this paper, we aim to elicit the relationships and dependencies between the functionalities required to satisfy the QAs and all those factors that can provoke a reconfiguration of the software architecture at runtime. We follow an approach in which the variability of the QAs is modelled separately from the base application functionality, and propose a dynamic approach to reconfigure the software architecture based on those reconfiguration criteria.},
booktitle = {Companion Proceedings of the 15th International Conference on Modularity},
pages = {131–136},
numpages = {6},
keywords = {variability, software architecture, reconfiguration, SPL, Quality attributes},
location = {M\'{a}laga, Spain},
series = {MODULARITY Companion 2016}
}

@inproceedings{10.1109/ICSE.2019.00112,
author = {Kaltenecker, Christian and Grebhahn, Alexander and Siegmund, Norbert and Guo, Jianmei and Apel, Sven},
title = {Distance-based sampling of software configuration spaces},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00112},
doi = {10.1109/ICSE.2019.00112},
abstract = {Configurable software systems provide a multitude of configuration options to adjust and optimize their functional and non-functional properties. For instance, to find the fastest configuration for a given setting, a brute-force strategy measures the performance of all configurations, which is typically intractable. Addressing this challenge, state-of-the-art strategies rely on machine learning, analyzing only a few configurations (i.e., a sample set) to predict the performance of other configurations. However, to obtain accurate performance predictions, a representative sample set of configurations is required. Addressing this task, different sampling strategies have been proposed, which come with different advantages (e.g., covering the configuration space systematically) and disadvantages (e.g., the need to enumerate all configurations). In our experiments, we found that most sampling strategies do not achieve a good coverage of the configuration space with respect to covering relevant performance values. That is, they miss important configurations with distinct performance behavior. Based on this observation, we devise a new sampling strategy, called distance-based sampling, that is based on a distance metric and a probability distribution to spread the configurations of the sample set according to a given probability distribution across the configuration space. This way, we cover different kinds of interactions among configuration options in the sample set. To demonstrate the merits of distance-based sampling, we compare it to state-of-the-art sampling strategies, such as t-wise sampling, on 10 real-world configurable software systems. Our results show that distance-based sampling leads to more accurate performance models for medium to large sample sets.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1084–1094},
numpages = {11},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/1944892.1944894,
author = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Th\"{u}m, Thomas and Saake, Gunter},
title = {Multi-dimensional variability modeling},
year = {2011},
isbn = {9781450305709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944892.1944894},
doi = {10.1145/1944892.1944894},
abstract = {The variability of a software product line (SPL)is often described with a feature model. To avoid highly complex models, stakeholders usually try to separate different variability dimensions, such as domain variability and implementation variability. This results in distinct variability models, which are easier to handle than one large model. On the other hand, it is sometimes required to analyze the variability dimensions of an SPL in combination using a single model only. To combine separate modeling and integrated analysis of variability, we present Velvet, a language for multi-dimensional variability modeling. Velvet allows stakeholders to model each variability dimension of an SPL separately and to compose the separated dimensions on demand. This improves reuse of feature models and supports independent modeling variability dimensions. Furthermore, Velvet integrates feature modeling and configuration in a single language. The combination of both concepts creates further reuse opportunities and allows stakeholders to independently configure variability dimensions.},
booktitle = {Proceedings of the 5th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {11–20},
numpages = {10},
keywords = {variability modeling, separation of concerns, feature models},
location = {Namur, Belgium},
series = {VaMoS '11}
}

@inproceedings{10.1145/2993236.2993246,
author = {Kienzle, J\"{o}rg and Mussbacher, Gunter and Collet, Philippe and Alam, Omar},
title = {Delaying decisions in variable concern hierarchies},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993246},
doi = {10.1145/2993236.2993246},
abstract = {Concern-Oriented Reuse (CORE) proposes a new way of structuring model-driven software development, where models of the system are modularized by domains of abstraction within units of reuse called concerns. Within a CORE concern, models are further decomposed and modularized by features. This paper extends CORE with a technique that enables developers of high-level concerns to reuse lower-level concerns without unnecessarily committing to a specific feature selection. The developer can select the functionality that is minimally needed to continue development, and reexpose relevant alternative lower-level features of the reused concern in the reusing concern's interface. This effectively delays decision making about alternative functionality until the higher-level reuse context, where more detailed requirements are known and further decisions can be made. The paper describes the algorithms for composing the variation (i.e., feature and impact models), customization, and usage interfaces of a concern, as well as the concern's realization models and finally an entire concern hierarchy, as is necessary to support delayed decision making in CORE.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {93–103},
numpages = {11},
keywords = {Reuse Hierarchies, Model-Driven Engineering, Model Reuse, Model Interfaces, Delaying of Decisions},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@inproceedings{10.1145/3669940.3707286,
author = {Cai, Xuran and Goharshady, Amir Kafshdar and Hitarth, S. and Lam, Chun Kit},
title = {Faster Chaitin-like Register Allocation via Grammatical Decompositions of Control-Flow Graphs},
year = {2025},
isbn = {9798400706981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3669940.3707286},
doi = {10.1145/3669940.3707286},
abstract = {It is well-known that control-flow graphs (CFGs) of structured programs are sparse. This sparsity has been previously formalized in terms of graph parameters such as treewidth and pathwidth and used to design faster parameterized algorithms for numerous compiler optimization, model checking and program analysis tasks.In this work, we observe that the known graph sparsity parameters fail to exactly capture the kind of sparsity exhibited by CFGs. For example, while all structured CFGs have a treewidth of at most 7, not every graph with a treewidth of 7 or less is realizable as a CFG. As a result, current parameterized algorithms are solving the underlying graph problems over a more general family of graphs than the CFGs.To address this problem, we design a new but natural concept of graph decomposition based on a grammar that precisely captures the set of graphs that can be realized as CFGs of programs. We show that our notion of decomposition enables the same type of dynamic programming algorithms that are often used in treewidth/pathwidth-based methods. As two concrete applications, using our grammatical decomposition of CFGs, we provide asymptotically more efficient algorithms for two variants of the classical problem of register allocation as defined by Chaitin, i.e. assigning program variables to a limited number of registers such that variables with intersecting lifetimes are not assigned to the same register. Note that Chaitin's formulation of register allocation does not allow live-range splitting. Our algorithms are asymptotically faster not only in comparison with the non-parameterized solutions for these problems, but also compared to the state-of-the-art treewidth/pathwidth-based approaches in the literature. For minimum-cost register allocation over a fixed number of registers, we provide an algorithm with a runtime of O(|G| ⋅ |𝕈| 5 ⋅ r) where |G| is the size of the program, 𝕈 is the set of program variables and r is the number of registers. In contrast, the previous treewidth-based algorithm had a runtime of O(|G| ⋅ |𝕈| 16 ⋅ r). For the decision problem of spill-free register allocation, our algorithm's runtime is O(|G| ⋅ r5 ⋅ r + 5) whereas the previous works had a runtime of O(|G| ⋅ r16 ⋅ r).Finally, we provide extensive experimental results on spill-free register allocation, showcasing the scalability of our approach in comparison to previous state-of-the-art methods. Most notably, our approach can handle real-world instances with up to 20 registers, whereas previous works could only scale to 8. This is a significant improvement since most ubiquitous architectures, such as the x86 family, have 16 registers. For such architectures, our approach is the first-ever exact algorithm that scales up to solve the real-world instances of spill-free register allocation.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1},
pages = {463–477},
numpages = {15},
keywords = {control-flow graphs, graph decompositions, register allocation, sparsity},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/1837154.1837157,
author = {Siegmund, Norbert and Feigenspan, Janet and Soffner, Michael and Fruth, Jana and K\"{o}ppen, Veit},
title = {Challenges of secure and reliable data management in heterogeneous environments},
year = {2010},
isbn = {9781605589923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1837154.1837157},
doi = {10.1145/1837154.1837157},
abstract = {Ubiquitous computing is getting more important since requirements for complex systems grow fast. In these systems, embedded devices have to fulfill different tasks. They have to monitor the environment, store data, communicate with other devices, and react to user input. In addition to this complexity, quality issues such as security and reliability have to be considered, as well, due to their increasing use in life critical application scenarios. Finally, different devices with different application goals are used, which results in interoperability problems. In this paper, we highlight challenges for interoperability, data management, and security, which arise with complex systems. Furthermore, we present approaches to overcome different problems and how an integrated solution can be realized using software product line techniques.},
booktitle = {Proceedings of the First International Workshop on Digital Engineering},
pages = {17–24},
numpages = {8},
keywords = {software product lines, security, digital engineering, data management},
location = {Magdeburg, Germany},
series = {IWDE '10}
}

@inproceedings{10.1145/2577080.2577095,
author = {Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha and Baier, Christel},
title = {Probabilistic model checking for energy analysis in software product lines},
year = {2014},
isbn = {9781450327725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2577080.2577095},
doi = {10.1145/2577080.2577095},
abstract = {In a software product line (SPL), a collection of software products is defined by their commonalities in terms of features rather than explicitly specifying all products one-by-one. Several verification techniques were adapted to establish temporal properties of SPLs. Symbolic and family-based model checking have been proven to be successful for tackling the combinatorial blow-up arising when reasoning about several feature combinations. However, most formal verification approaches for SPLs presented in the literature focus on the static SPLs, where the features of a product are fixed and cannot be changed during runtime. This is in contrast to dynamic SPLs, allowing to adapt feature combinations of a product dynamically after deployment.The main contribution of the paper is a compositional modeling framework for dynamic SPLs, which supports probabilistic and nondeterministic choices and allows for quantitative analysis. We specify the feature changes during runtime within an automata-based coordination component, enabling to reason over strategies how to trigger dynamic feature changes for optimizing various quantitative objectives, e.g., energy or monetary costs and reliability. For our framework there is a natural and conceptually simple translation into the input language of the prominent probabilistic model checker PRISM. This facilitates the application of PRISM's powerful symbolic engine to the operational behavior of dynamic SPLs and their family-based analysis against various quantitative queries. We demonstrate feasibility of our approach by a case study issuing an energy-aware bonding network device.},
booktitle = {Proceedings of the 13th International Conference on Modularity},
pages = {169–180},
numpages = {12},
keywords = {software product lines, probabilistic model checking, energy analysis, dynamic features},
location = {Lugano, Switzerland},
series = {MODULARITY '14}
}

@inproceedings{10.1145/1562860.1562864,
author = {Siegmund, Norbert and Pukall, Mario and Soffner, Michael and K\"{o}ppen, Veit and Saake, Gunter},
title = {Using software product lines for runtime interoperability},
year = {2009},
isbn = {9781605585482},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1562860.1562864},
doi = {10.1145/1562860.1562864},
abstract = {Today, often small, heterogeneous systems have to cooperate in order to fulfill a certain task. Interoperability between these systems is needed for their collaboration. However, achieving this interoperability raises several problems. For example, embedded systems might induce a higher probability for a system failure due to constrained power supply. Nevertheless, interoperability must be guaranteed even in scenarios where embedded systems are used. To overcome this problem, we use services to abstract the functionality from the system which realizes it. We outline how services can be generated using software product line techniques to bridge the heterogeneity of cooperating systems. Additionally, we address runtime changes of already deployed services to overcome system failures. In this paper, we show the runtime adaption process of these changes which includes the following two points. First, we outline why feature-oriented programming is appropriate in such scenarios. Second, we describe the runtime adaption process of services with feature-oriented programming.},
booktitle = {Proceedings of the Workshop on AOP and Meta-Data for Software Evolution},
articleno = {4},
numpages = {7},
keywords = {software product lines, runtime adaption, interoperability},
location = {Genova, Italy},
series = {RAM-SE '09}
}

@inproceedings{10.1109/PESOS.2009.5068815,
author = {Mietzner, Ralph and Metzger, Andreas and Leymann, Frank and Pohl, Klaus},
title = {Variability modeling to support customization and deployment of multi-tenant-aware Software as a Service applications},
year = {2009},
isbn = {9781424437160},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/PESOS.2009.5068815},
doi = {10.1109/PESOS.2009.5068815},
abstract = {More and more companies are offering their software by following the Software as a Service (SaaS) model. The promise of the SaaS model is to exploit economies of scale on the provider side by hosting multiple customers (or tenants) on the same hardware and software infrastructure. However, to attract a significant number of tenants, SaaS applications have to be customizable to fulfill the varying functional and quality requirements of individual tenants. In this paper, we describe how variability modeling techniques from software product line engineering can support SaaS providers in managing the variability of SaaS applications and their requirements. Specifically, we propose using explicit variability models to systematically derive customization and deployment information for individual SaaS tenants. We also demonstrate how variability models could be used to systematically consider information about already deployed SaaS applications for efficiently deploying SaaS applications for new tenants. We illustrate our approach by a running example for a meeting planning application.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Principles of Engineering Service Oriented Systems},
pages = {18–25},
numpages = {8},
series = {PESOS '09}
}

@inproceedings{10.1145/1868433.1868445,
author = {Trujillo, Salvador and Perez, Antonio and Gonzalez, David and Hamid, Brahim},
title = {Towards the integration of advanced engineering paradigms into RCES: raising the issues for the safety-critical model-driven product-line case},
year = {2010},
isbn = {9781450303682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868433.1868445},
doi = {10.1145/1868433.1868445},
abstract = {The conception and design of Resource Constrained Embedded Systems is an inherently complex endeavor. In particular, non-functional requirements from security, dependability and variability are exacerbating this complexity. Recent times have seen a paradigm shift in terms of design through the combination of multiple software engineering paradigms together, namely, Model Driven Engineering and Software Product Line Engineering. Such paradigm shift is changing the way systems are developed nowadays, reducing development time significantly. Embedded systems are a case in point where a range of products for assorted domains such as energy, transportation, automotive, and so on are conceived as a family. However, most of the work so far has been focused on functional parts. The purpose of this talk is to foster some discussion during the workshop on the issues that need to be faced for these techniques to be applicable for Resource Constrained Embedded Systems for which security and dependability are primary requirements.},
booktitle = {Proceedings of the International Workshop on Security and Dependability for Resource Constrained Embedded Systems},
articleno = {9},
numpages = {4},
keywords = {software product lines, resource constrained embedded systems, model-driven development, dependability},
location = {Vienna, Austria},
series = {S&amp;D4RCES '10}
}

@proceedings{10.1145/3571788,
title = {VaMoS '23: Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Odense, Denmark}
}

@inproceedings{10.1109/WI-IAT.2014.170,
author = {Louati, Amine and Haddad, Joyce El and Pinson, Suzanne},
title = {A Multilevel Agent-Based Approach for Trustworthy Service Selection in Social Networks},
year = {2014},
isbn = {9781479941438},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2014.170},
doi = {10.1109/WI-IAT.2014.170},
abstract = {The growing number of services available within social applications (viz. Social networks) raises a new and challenging search issue: selecting desired services from social networks. Traditional discovery and selection approaches, which are registry-based (e.g., UDDI, ebXML), have manifested their limitations as they often fall behind users' expectations. This is because registries fail to (i) take into consideration non functional properties such as QoS and trust and (ii) capitalize on the information resulting from the previous experiences between agents. To address these shortcomings, we use software agents as they support interactions and offer well-developed capabilities to formally express and interpret semantic information useful to evaluate trust. Trust in a service is a multi-aspect concept that includes a social-based aspect such as judging whether the provider is worthwhile pursuing before using his services (viz. Trust in sociability), expert-based aspect such as estimating whether the service behaves well and as expected (viz. Trust in expertise) and, recommender-based aspect such as assessing whether an agent is reliable and we can rely on its recommendations (viz. Trust in recommendation).},
booktitle = {Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 03},
pages = {214–221},
numpages = {8},
keywords = {Trust, Social Networks, Service Selection, Referral Systems, Multi-Agent Systems},
series = {WI-IAT '14}
}

@inproceedings{10.1145/1529282.1529388,
author = {Bure\v{s}, Tom\'{a}\v{s} and Hn\v{e}tynka, Petr and Malohlava, Michal},
title = {Using a product line for creating component systems},
year = {2009},
isbn = {9781605581668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1529282.1529388},
doi = {10.1145/1529282.1529388},
abstract = {Component systems have become a wide-spread technology and found their place in several application domains. Each component system has its specifics and particularities that reflect its focus and the application domain it is intended for. Although important, the diversity of component systems leads to a number of problems including having different tools for each systems, unnecessary duplication of functionality and problems with integration when several domains are to be targeted. Based on categorization of component application domains, we propose a "meta-component system", which provides a software product line for creating custom component systems. We focus especially on the deployment and execution environment, which is where most diversities are found. We demonstrate the usage of the "meta-component system" and propose how it is to be realized by two core concepts of SOFA 2, namely connector generator and microcomponents.},
booktitle = {Proceedings of the 2009 ACM Symposium on Applied Computing},
pages = {501–508},
numpages = {8},
keywords = {runtime environment, product line engineering, generative programming, component systems},
location = {Honolulu, Hawaii},
series = {SAC '09}
}

@inproceedings{10.1145/3634713.3634715,
author = {B\"{o}hm, Sabrina and Krieter, Sebastian and He\ss{}, Tobias and Th\"{u}m, Thomas and Lochau, Malte},
title = {Incremental Identification of T-Wise Feature Interactions},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634715},
doi = {10.1145/3634713.3634715},
abstract = {Developers of configurable software use the concept of selecting and deselecting features to create different variants of a software product. In this context, one of the most challenging aspects is to identify unwanted interactions between those features. Due to the combinatorial explosion of the number of potentially interacting features, it is currently an open question how to systematically identify a particular feature interaction that causes a specific fault in a set of software products. In this paper, we propose an incremental approach to identify such t-wise feature interactions based on testing additional configurations in a black-box setting. We present the algorithm Inciident, which generates and selects new configurations based on a divide-and-conquer strategy to efficiently identify the feature interaction with a preferably minimal number of configurations. We evaluate our approach by considering simulated and real interactions of different sizes for 48 real-world feature models. Our results show that on average, Inciident requires 80&nbsp;% less configurations to identify an interaction than using randomly selected configurations.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {27–36},
numpages = {10},
keywords = {Configurable Systems, Feature Interaction, Feature-Model Analysis, Software Product Lines},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@proceedings{10.1145/3634713,
title = {VaMoS '24: Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bern, Switzerland}
}

@inproceedings{10.1145/3377024.3377026,
author = {Kenner, Andy and Dassow, Stephan and Lausberger, Christian and Kr\"{u}ger, Jacob and Leich, Thomas},
title = {Using variability modeling to support security evaluations: virtualizing the right attack scenarios},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377026},
doi = {10.1145/3377024.3377026},
abstract = {A software system's security is constantly threatened by vulnerabilities that result from faults in the system's design (e.g., unintended feature interactions) and which can be exploited with attacks. While various databases summarize information on vulnerabilities and other security issues for many software systems, these databases face severe limitations. For example, the information's quality is unclear, often only semi-structured, and barely connected to other information. Consequently, it can be challenging for any security-related stakeholder to extract and understand what information is relevant, considering that most systems exist in different variants and versions. To tackle this problem, we propose to design vulnerability feature models that represent the vulnerabilities of a system and enable developers to virtualize corresponding attack scenarios. In this paper, we report a first case study on Mozilla Firefox for which we extracted vulnerabilities and used them to virtualize vulnerable instances in Docker. To this end, we focused on extracting information from available databases and on evaluating the usability of the results. Our findings indicate several problems with the extraction that complicate modeling, understanding, and testing of vulnerabilities. Nonetheless, the databases provide a valuable foundation for our technique, which we aim to extend with automatic synthesis and analyses of feature models, as well as virtualization for attack scenarios in future work.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {10},
numpages = {9},
keywords = {vulnerability, variability model, software architecture, feature model, exploit, docker-container, attack scenarios},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@article{10.1145/3715002,
author = {Blasco, Daniel and Iglesias, Antonio and Echeverr\'{\i}a, Jorge and P\'{e}rez, Francisca and Cetina, Carlos},
title = {Introducing Phylogenetics in Search-based Software Engineering: Phylogenetics-aware SBSE},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715002},
doi = {10.1145/3715002},
abstract = {Phylogenetics studies the relationships, in terms of biological history and kinship, of a set of taxa (e.g., species). We argue that in Search-based Software Engineering (SBSE), the individuals of an evolutionary computation-driven population could be considered as taxa for which the leverage of Phylogenetic Inference might be beneficial. In this work, we present our Phylogenetics-aware SBSE approach. Our approach introduces a novel Phylogenetic Operation to promote results which are sufficiently aligned (in terms of lineage) with a certain reference given by the domain expert. Our approach is evaluated in two heterogeneous industrial case studies: Procedural Content Generation from Game Software Engineering, and Feature Location from Software Maintenance. The results are analyzed using quality-of-the-solution and acceptance-by-developers measurements. We performed a statistical analysis to determine whether the impact on the results is significant compared to baselines that do not leverage Phylogenetics. The results show that our approach significantly outperforms two baselines in both case studies. Furthermore, two focus groups confirmed the acceptance of our approach and stressed that solution acceptance may make the difference in industrial environments. Our work has the potential to motivate a new breed of research work on Phylogenetics awareness to produce better results in Software Engineering.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Phylogenetics, Search-based Software Engineering, Evolutionary Computation, Game Software Engineering, Procedural Content Generation, Software Maintenance, Feature Location, Model-Driven Engineering}
}

@inproceedings{10.1145/1842752.1842812,
author = {Abbas, Nadeem and Andersson, Jesper and L\"{o}we, Welf},
title = {Autonomic Software Product Lines (ASPL)},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842812},
doi = {10.1145/1842752.1842812},
abstract = {We describe ongoing work on a variability mechanism for Autonomic Software Product Lines (ASPL). The autonomic software product lines have self-management characteristics that make product line instances more resilient to context changes and some aspects of product line evolution. Instances sense the context, selects and bind the best component variants to variation-points at run-time. The variability mechanism we describe is composed of a profile guided dispatch based on off-line and on-line training processes. Together they form a simple, yet powerful variability mechanism that continuously learns, which variants to bind given the current context and system goals.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {324–331},
numpages = {8},
keywords = {variation-points, variants, variability, on-line, off-line training, goals, context, autonomic elements, MAPE-K},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@article{10.1145/3591335.3591343,
author = {Bordis, Tabea and Runge, Tobias and Kittelmann, Alexander and Schaefer, Ina},
title = {Correctness-by-Construction: An Overview of the CorC Ecosystem},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {2},
issn = {1094-3641},
url = {https://doi.org/10.1145/3591335.3591343},
doi = {10.1145/3591335.3591343},
abstract = {Correctness-by-Construction (CbC) is an incremental software development technique in the field of formal methods to create functionally correct programs guided by a specification. In contrast to post-hoc verification, where the specification and verification take part after implementing a program, with CbC the specification is defined first, and then the program is successively created using a small set of refinement rules that define side conditions preserving the correctness of the program. This specification-first, refinement-based approach as pursued by CbC has the advantage that errors are likely to be detected earlier in the design process and can be tracked more easily. Even though the idea of CbC emerged over 40 years ago, CbC is not widespread and is mostly used to create small algorithms.We believe in the idea of CbC and envision a scaled CbC approach that contributes to solving problems of modern software verification. In this short paper, we give an overview of our research regarding CbC in four different lines of research. For all of them, we provide tool support for building the CorC ecosystem that even further enables CbC-based development for different fields of application and size of software systems. Furthermore, we give an outlook on future work that extends on our concepts for CbC.},
journal = {Ada Lett.},
month = apr,
pages = {75–78},
numpages = {4},
keywords = {software product lines, program verification, information flow control, correctness-by-construction, architecture}
}

@inproceedings{10.1109/MiSE.2019.00018,
author = {Sch\"{o}ttle, Matthias and Kienzle, J\"{o}rg},
title = {On the difficulties of raising the level of abstraction and facilitating reuse in software modelling: the case for signature extension},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MiSE.2019.00018},
doi = {10.1109/MiSE.2019.00018},
abstract = {Reuse is central to improving the software development process, increasing software quality and decreasing time-to-market. Hence it is of paramount importance that modelling languages provide features that enable the specification and modularization of reusable artefacts, as well as their subsequent reuse. In this paper we outline several difficulties caused by the finality of method signatures that make it hard to specify and use reusable artefacts encapsulating several variants. The difficulties are illustrated with a running example. To evaluate whether these difficulties can be observed at the programming level, we report on an empirical study conducted on the Java Platform API as well as present workarounds used in various programming languages to deal with the rigid nature of signatures. Finally, we outline signature extension as an approach to overcome these problems at the modelling level.},
booktitle = {Proceedings of the 11th International Workshop on Modelling in Software Engineerings},
pages = {71–77},
numpages = {7},
location = {Montreal, Quebec, Canada},
series = {MiSE '19}
}

@inproceedings{10.1145/3510455.3512792,
author = {Randrianaina, Georges Aaron and Khelladi, Djamel Eddine and Zendra, Olivier and Acher, Mathieu},
title = {Towards incremental build of software configurations},
year = {2022},
isbn = {9781450392242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510455.3512792},
doi = {10.1145/3510455.3512792},
abstract = {Building software is a crucial task to compile, test, and deploy software systems while continuously ensuring quality. As software is more and more configurable, building multiple configurations is a pressing need, yet, costly and challenging to instrument. The common practice is to independently build (a.k.a., clean build) a software for a subset of configurations. While incremental build has been considered for software evolution and relatively small modifications of the source code, it has surprisingly not been considered for software configurations. In this vision paper, we formulate the hypothesis that incremental build can reduce the cost of exploring the configuration space of software systems. We detail how we apply incremental build for two real-world application scenarios and conduct a preliminary evaluation on two case studies, namely x264 and Linux Kernel. For x264, we found that one can incrementally build configurations in an order such that overall build time is reduced. Nevertheless, we could not find any optimal order with the Linux Kernel, due to a high distance between random configurations. Therefore, we show it is possible to control the process of generating configurations: we could reuse commonality and gain up to 66% of build time compared to only clean builds.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {101–105},
numpages = {5},
keywords = {incremental build, highly configurable system, build system},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-NIER '22}
}

@article{10.1145/3034827,
author = {Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz},
title = {CASE Tool Support for Variability Management in Software Product Lines},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3034827},
doi = {10.1145/3034827},
abstract = {Software product lines (SPL) aim at reducing time-to-market and increasing software quality through extensive, planned reuse of artifacts. An essential activity in SPL is variability management, i.e., defining and managing commonality and variability among member products. Due to the large scale and complexity of today's software-intensive systems, variability management has become increasingly complex to conduct. Accordingly, tool support for variability management has been gathering increasing momentum over the last few years and can be considered a key success factor for developing and maintaining SPLs. While several studies have already been conducted on variability management, none of these analyzed the available tool support in detail. In this work, we report on a survey in which we analyzed 37 existing variability management tools identified using a systematic literature review to understand the tools’ characteristics, maturity, and the challenges in the field. We conclude that while most studies on variability management tools provide a good motivation and description of the research context and challenges, they often lack empirical data to support their claims and findings. It was also found that quality attributes important for the practical use of tools such as usability, integration, scalability, and performance were out of scope for most studies.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {14},
numpages = {45},
keywords = {software variability, computer-aided software engineering, Software engineering}
}

@inproceedings{10.1145/3023956.3023963,
author = {Halin, Axel and Nuttinck, Alexandre and Acher, Mathieu and Devroey, Xavier and Perrouin, Gilles and Heymans, Patrick},
title = {Yo variability! JHipster: a playground for web-apps analyses},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023963},
doi = {10.1145/3023956.3023963},
abstract = {Though variability is everywhere, there has always been a shortage of publicly available cases for assessing variability-aware tools and techniques as well as supports for teaching variability-related concepts. Historical software product lines contains industrial secrets their owners do not want to disclose to a wide audience. The open source community contributed to large-scale cases such as Eclipse, Linux kernels, or web-based plugin systems (Drupal, WordPress). To assess accuracy of sampling and prediction approaches (bugs, performance), a case where all products can be enumerated is desirable. As configuration issues do not lie within only one place but are scattered across technologies and assets, a case exposing such diversity is an additional asset. To this end, we present in this paper our efforts in building an explicit product line on top of JHipster, an industrial open-source Web-app configurator that is both manageable in terms of configurations (≈ 163,000) and diverse in terms of technologies used. We present our efforts in building a variability-aware chain on top of JHipster's configurator and lessons learned using it as a teaching case at the University of Rennes. We also sketch the diversity of analyses that can be performed with our infrastructure as well as early issues found using it. Our long term goal is both to support students and researchers studying variability analysis and JHipster developers in the maintenance and evolution of their tools.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {44–51},
numpages = {8},
keywords = {web-apps, variability-related analyses, case study},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/3338906.3338974,
author = {Ne\v{s}i\'{c}, Damir and Kr\"{u}ger, Jacob and St\u{a}nciulescu, undefinedtefan and Berger, Thorsten},
title = {Principles of feature modeling},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338974},
doi = {10.1145/3338906.3338974},
abstract = {Feature models are arguably one of the most intuitive and successful notations for modeling the features of a variant-rich software system. Feature models help developers to keep an overall understanding of the system, and also support scoping, planning, development, variant derivation, configuration, and maintenance activities that sustain the system's long-term success. Unfortunately, feature models are difficult to build and evolve. Features need to be identified, grouped, organized in a hierarchy, and mapped to software assets. Also, dependencies between features need to be declared. While feature models have been the subject of three decades of research, resulting in many feature-modeling notations together with automated analysis and configuration techniques, a generic set of principles for engineering feature models is still missing. It is not even clear whether feature models could be engineered using recurrent principles. Our work shows that such principles in fact exist. We analyzed feature-modeling practices elicited from ten interviews conducted with industrial practitioners and from 31 relevant papers. We synthesized a set of 34 principles covering eight different phases of feature modeling, from planning over model construction, to model maintenance and evolution. Grounded in empirical evidence, these principles provide practical, context-specific advice on how to perform feature modeling, describe what information sources to consider, and highlight common characteristics of feature models. We believe that our principles can support researchers and practitioners enhancing feature-modeling tooling, synthesis, and analyses techniques, as well as scope future research.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {62–73},
numpages = {12},
keywords = {software product lines, modeling principles, Feature models},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3302333.3302350,
author = {Garc\'{\i}a, Sergio and Str\"{u}ber, Daniel and Brugali, Davide and Di Fava, Alessandro and Schillinger, Philipp and Pelliccione, Patrizio and Berger, Thorsten},
title = {Variability Modeling of Service Robots: Experiences and Challenges},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302350},
doi = {10.1145/3302333.3302350},
abstract = {Sensing, planning, controlling, and reasoning, are human-like capabilities that can be artificially replicated in an autonomous robot. Such a robot implements data structures and algorithms devised on a large spectrum of theories, from probability theory, mechanics, and control theory to ethology, economy, and cognitive sciences. Software plays a key role in the development of robotic systems, as it is the medium to embody intelligence in the machine. During the last years, however, software development is increasingly becoming the bottleneck of robotic systems engineering due to three factors: (a) the software development is mostly based on community efforts and it is not coordinated by key stakeholders; (b) robotic technologies are characterized by a high variability that makes reuse of software a challenging practice; and (c) robotics developers are usually not specifically trained in software engineering. In this paper, we illustrate our experiences from EU, academic, and industrial projects in identifying, modeling, and managing variability in the domain of service robots. We hope to raise awareness for the specific variability challenges in robotics software engineering and to inspire other researchers to advance this field.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {8},
numpages = {6},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/3510003.3510190,
author = {Randrianaina, Georges Aaron and T\"{e}rnava, Xhevahire and Khelladi, Djamel Eddine and Acher, Mathieu},
title = {On the benefits and limits of incremental build of software configurations: an exploratory study},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510190},
doi = {10.1145/3510003.3510190},
abstract = {Software projects use build systems to automate the compilation, testing, and continuous deployment of their software products. As software becomes increasingly configurable, the build of multiple configurations is a pressing need, but expensive and challenging to implement. The current state of practice is to build independently (a.k.a., clean build) a software for a subset of configurations. While incremental build has been studied for software evolution and relatively small changes of the source code, it has surprisingly not been considered for software configurations. In this exploratory study, we examine the benefits and limits of building software configurations incrementally, rather than always building them cleanly. By using five real-life configurable systems as subjects, we explore whether incremental build works, outperforms a sequence of clean builds, is correct w.r.t. clean build, and can be used to find an optimal ordering for building configurations. Our results show that incremental build is feasible in 100% of the times in four subjects and in 78% of the times in one subject. In average, 88.5% of the configurations could be built faster with incremental build while also finding several alternatives faster incremental builds. However, only 60% of faster incremental builds are correct. Still, when considering those correct incremental builds with clean builds, we could always find an optimal order that is faster than just a collection of clean builds with a gain up to 11.76%.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1584–1596},
numpages = {13},
keywords = {build systems, configurable software systems, configuration build},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3442391.3442409,
author = {G\"{o}ttmann, Hendrik and Bacher, Isabelle and Gottwald, Nicolas and Lochau, Malte},
title = {Static Analysis Techniques for Efficient Consistency Checking of Real-Time-Aware DSPL Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442409},
doi = {10.1145/3442391.3442409},
abstract = {Dynamic Software Product Lines (DSPL) have recently gained momentum as integrated engineering methodology for (self-)adaptive software. DSPL enhance statically configurable software by enabling run-time reconfiguration to facilitate continuous adaptations to changing environmental contexts. In a previous work, we presented a model-based methodology for specifying and automatically analyzing real-time constraints of reconfiguration decisions in a feature-oriented and compositional way. Internally, we translate real-time-aware DSPL specifications into timed automata serving as input for off-the-shelf model checkers like Uppaal for automatically checking semantic consistency properties. However, due to the very high computational complexity of model checking timed automata, those consistency checks suffer from scalability problems thus obstructing practical applications of the proposed approach. In this paper, we tackle this issue by investigating various kinds of static-analysis techniques that (1) aim to avoid expensive model checker calls by statically detecting certain classes of inconsistencies beforehand and otherwise (2) perform model reduction by detecting and merging equivalence states prior to model checker calls. The results of our experimental evaluation show very promising performance improvements achievable by those techniques, especially by the model-reduction approach.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {17},
numpages = {9},
keywords = {Timed Automata, Reconfiguration Decisions, Dynamic Software Product Lines},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.1145/3643991.3644913,
author = {Randrianaina, Georges Aaron and Khelladi, Djamel Eddine and Zendra, Olivier and Acher, Mathieu},
title = {Options Matter: Documenting and Fixing Non-Reproducible Builds in Highly-Configurable Systems},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644913},
doi = {10.1145/3643991.3644913},
abstract = {A critical aspect of software development, build reproducibility, ensures the dependability, security, and maintainability of software systems. Although several factors, including the build environment, have been investigated in the context of non-reproducible builds, to the best of our knowledge the precise influence of configuration options in configurable systems has not been thoroughly investigated. This paper aims at filling this gap.This paper thus proposes an approach to automatically identify configuration options causing non-reproducibility of builds. It begins by building a set of builds in order to detect non-reproducible ones through binary comparison. We then develop automated techniques that combine statistical learning with symbolic reasoning to analyze over 20,000 configuration options. Our methods are designed to both detect options causing non-reproducibility, and remedy non-reproducible configurations, two tasks that are challenging and costly to perform manually.We evaluate our approach on three case studies, namely Toybox, Busybox, and Linux, analyzing more than 2,000 configurations for each of them. Toybox and Busybox come exempt from non-reproducibility. In contrast, 47% of Linux configurations lead to non-reproducible builds. The approach we propose in this paper is capable of identifying 10 configuration options that caused this non-reproducibility. When confronted to the Linux documentation, none of these are documented as non-reproducible. Thus, our identified non-reproducible configuration options are novel knowledge and constitute a direct, actionable information improvement for the Linux community. Finally, we demonstrate that our methodology effectively identifies a set of undesirable option values, enabling the enhancement and expansion of the Linux kernel documentation while automatically rectifying 96% of encountered non-reproducible builds.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {654–664},
numpages = {11},
keywords = {reproducible build, build system, highly-configurable system},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@article{10.1145/3528100,
author = {Cheng, Jiezhu and Gao, Cuiyun and Zheng, Zibin},
title = {HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3528100},
doi = {10.1145/3528100},
abstract = {Modern software systems are usually highly configurable, providing users with customized functionality through various configuration options. Understanding how system performance varies with different option combinations is important to determine optimal configurations that meet specific requirements. Due to the complex interactions among multiple options and the high cost of performance measurement under a huge configuration space, it is challenging to study how different configurations influence the system performance. To address these challenges, we propose HINNPerf, a novel hierarchical interaction neural network for performance prediction of configurable systems. HINNPerf employs the embedding method and hierarchic network blocks to model the complicated interplay between configuration options, which improves the prediction accuracy of the method. In addition, we devise a hierarchical regularization strategy to enhance the model robustness. Empirical results on 10 real-world configurable systems show that our method statistically significantly outperforms state-of-the-art approaches by achieving average 22.67% improvement in prediction accuracy. In addition, combined with the Integrated Gradients method, the designed hierarchical architecture provides some insights about the interaction complexity and the significance of configuration options, which might help users and developers better understand how the configurable system works and efficiently identify significant options affecting the performance.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {46},
numpages = {30},
keywords = {machine learning, deep neural network, highly configurable systems, Software performance prediction}
}

@article{10.1145/2853073.2853095,
author = {Alebrahim, Azadeh and Fa\ss{}bender, Stephan and Filipczyk, Martin and Goedicke, Michael and Heisel, Maritta and Zdun, Uwe},
title = {Variability for Qualities in Software Architecture},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853095},
doi = {10.1145/2853073.2853095},
abstract = {Variability is a key factor of most systems. While there are many works covering variability in functionality, there is a research gap regarding variability in software qualities. There is an obvious imbalance between the importance of variability in the context of quality attributes, and the intensity of research in this area. To improve this situation, the First International Workshop on VAri- ability for QUalIties in SofTware Architecture (VAQUITA) was held jointly with ECSA 2015 in Cavtat/Dubrovnik, Croatia as a one-day workshop. The goal of VAQUITA was to investigate and stimulate the discourse about the matter of variability, qualities, and software architectures. The workshop featured three research paper presentations, one keynote talk, and two working group discussions. In this workshop report, we summarize the keynote talk and the presented papers. Additionally, we present the results of the working group discussions},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {32–35},
numpages = {4},
keywords = {variability, quality attributes, Software architecture}
}

@inproceedings{10.1145/3528233.3530702,
author = {Rao, Anyi and Xu, Linning and Lin, Dahua},
title = {Shoot360: Normal View Video Creation from City Panorama Footage},
year = {2022},
isbn = {9781450393379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528233.3530702},
doi = {10.1145/3528233.3530702},
abstract = {We present Shoot360, a system that efficiently generates multi-shot normal view videos with desired content presentation and various cinematic styles, given a collection of 360 video recordings on different environments. The core of our system is a three-step decision process: 1) It firstly semantically analyzes the contents of interest from each panorama environment based on shot units, and produces a guidance that specifies the semantic focus and movement type of its output shot according to the user specification on content presentation and cinematic styles. 2) Based on the obtained guidance, it generates video candidates for each shot with shot-level control parameters for view projections following the filming rules. 3) The system further aggregates the projected normal view shots with the imposed local and global constraints, which incorporates the external knowledge learned from exemplar videos and professional filming rules. Extensive experiments verify the effectiveness of our system design, and we conclude with promising extensions for applying it to more generalized scenarios.},
booktitle = {ACM SIGGRAPH 2022 Conference Proceedings},
articleno = {13},
numpages = {9},
keywords = {video editing, panorama videos, neural networks},
location = {Vancouver, BC, Canada},
series = {SIGGRAPH '22}
}

@inproceedings{10.1145/2593735.2593739,
author = {Kugele, Stefan and Pucea, Gheorghe},
title = {Model-based optimization of automotive E/E-architectures},
year = {2014},
isbn = {9781450328470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593735.2593739},
doi = {10.1145/2593735.2593739},
abstract = {In this paper we present a generic framework to enable constraint-based automotive E/E-architecture optimization using a domain-specific language. The quality of today's automotive E/E-architectures is highly influenced by the mapping of software to executing hardware components: the so-called deployment problem. First, we introduce a holistic architectural model facilitating a seamless model-based development from requirements management to deployment, which is the focus of this work. Second, we introduce our domain-specific constraint and optimization language AAOL (Automotive Architecture Optimization Language) capable to express a wide range of deployment-relevant problems. Third, we present a generic, i.e., solver-independent framework currently supporting multi-objective evolutionary algorithms (MOEA). We investigate the feasibility of the approach by dint of a case study taken from the literature.},
booktitle = {Proceedings of the 6th International Workshop on Constraints in Software Testing, Verification, and Analysis},
pages = {18–29},
numpages = {12},
keywords = {domain-specific languages, constraint satisfaction problem, automotive E/E-architecture, Model-based optimization},
location = {Hyderabad, India},
series = {CSTVA 2014}
}

@inproceedings{10.1145/2602576.2602585,
author = {Etxeberria, Leire and Trubiani, Catia and Cortellessa, Vittorio and Sagardui, Goiuria},
title = {Performance-based selection of software and hardware features under parameter uncertainty},
year = {2014},
isbn = {9781450325769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602576.2602585},
doi = {10.1145/2602576.2602585},
abstract = {Configurable software systems allow stakeholders to derive variants by selecting software and/or hardware features. Performance analysis of feature-based systems has been of large interest in the last few years, however a major research challenge is still to conduct such analysis before achieving full knowledge of the system, namely under a certain degree of uncertainty. In this paper we present an approach to analyze the correlation between selection of features embedding uncertain parameters and system performance. In particular, we provide best and worst case performance bounds on the basis of selected features and, in cases of wide gaps among these bounds, we carry on a sensitivity analysis process aimed at taming the uncertainty of parameters. The application of our approach to a case study in the e-health domain demonstrates how to support stakeholders in the identification of system variants that meet performance requirements.},
booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {23–32},
numpages = {10},
keywords = {uncertainty, software architectures, performance analysis, feature selection},
location = {Marcq-en-Bareul, France},
series = {QoSA '14}
}

@inproceedings{10.1145/3023956.3023959,
author = {Ochoa, Lina and Pereira, Juliana Alves and Gonz\'{a}lez-Rojas, Oscar and Castro, Harold and Saake, Gunter},
title = {A survey on scalability and performance concerns in extended product lines configuration},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023959},
doi = {10.1145/3023956.3023959},
abstract = {Product lines have been employed as a mass customisation method that reduces production costs and time-to-market. Multiple product variants are represented in a product line, however the selection of a particular configuration depends on stakeholders' functional and non-functional requirements. Methods like constraint programming and evolutionary algorithms have been used to support the configuration process. They consider a set of product requirements like resource constraints, stakeholders' preferences, and optimization objectives. Nevertheless, scalability and performance concerns start to be an issue when facing large-scale product lines and runtime environments. Thus, this paper presents a survey that analyses strengths and drawbacks of 21 approaches that support product line configuration. This survey aims to: i) evidence which product requirements are currently supported by studied methods; ii) how scalability and performance is considered in existing approaches; and iii) point out some challenges to be addressed in future research.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {5–12},
numpages = {8},
keywords = {survey, scalability, product requirements, product line, performance, literature review, configuration},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/2897045.2897047,
author = {da Mota Silveira Neto, Paulo Anselmo and de Santana, Taijara Loiola and de Almeida, Eduardo Santana and Cavalcanti, Yguarata Cerqueira},
title = {RiSE events: a testbed for software product lines experimentation},
year = {2016},
isbn = {9781450341769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897045.2897047},
doi = {10.1145/2897045.2897047},
abstract = {Software Product Lines (SPL) demand mature software engineering, planning and reuse, adequate practices of management and development, and also the ability to deal with organizational issues and architectural complexity. Thus, it is important the development of new techniques, tools and methods to deal with SPL complexity required by the variability management. To address this issue, an SPL has been proposed, where the existing variability was implemented by applying conditional compilation. Moreover, no framework was used to develop it, allowing any researcher to use the SPL without losing time learning some framework. In this work, we implemented an SPL test bed containing 34 functional features has 26.457 lines of code, 1493 methods and 496 classes.},
booktitle = {Proceedings of the 1st International Workshop on Variability and Complexity in Software Design},
pages = {12–13},
numpages = {2},
keywords = {variability, test bed, software product lines, security and availability tacticts},
location = {Austin, Texas},
series = {VACE '16}
}

@inproceedings{10.1145/3368089.3409675,
author = {Siegmund, Norbert and Ruckel, Nicolai and Siegmund, Janet},
title = {Dimensions of software configuration: on the configuration context in modern software development},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409675},
doi = {10.1145/3368089.3409675},
abstract = {With the rise of containerization, cloud development, and continuous integration and delivery, configuration has become an essential aspect not only to tailor software to user requirements, but also to configure a software system’s environment and infrastructure. This heterogeneity of activities, domains, and processes blurs the term configuration, as it is not clear anymore what tasks, artifacts, or stakeholders are involved and intertwined. However, each re- search study and each paper involving configuration places their contributions and findings in a certain context without making the context explicit. This makes it difficult to compare findings, translate them to practice, and to generalize the results. Thus, we set out to evaluate whether these different views on configuration are really distinct or can be summarized under a common umbrella. By interviewing practitioners from different domains and in different roles about the aspects of configuration and by analyzing two qualitative studies in similar areas, we derive a model of configuration that provides terminology and context for research studies, identifies new research opportunities, and allows practitioners to spot possible challenges in their current tasks. Although our interviewees have a clear view about configuration, it substantially differs due to their personal experience and role. This indicates that the term configuration might be overloaded. However, when taking a closer look, we see the interconnections and dependencies among all views, arriving at the conclusion that we need to start considering the entire spectrum of dimensions of configuration.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {338–349},
numpages = {12},
keywords = {variability, developer study, configuration management and life cycle, Dimensions of software configuration},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/2739482.2768422,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Assun\c{c}\~{a}o, Wesley K.G. and Fischer, Stefan and Vergilio, Silvia R. and Egyed, Alexander},
title = {Genetic Improvement for Software Product Lines: An Overview and a Roadmap},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2768422},
doi = {10.1145/2739482.2768422},
abstract = {Software Product Lines (SPLs) are families of related software systems that provide different combinations of features. Extensive research and application attest to the significant economical and technological benefits of employing SPL practices. However, there are still several challenges that remain open. Salient among them is reverse engineering SPLs from existing variants of software systems and their subsequent evolution. In this paper, we aim at sketching connections between research on these open SPL challenges and ongoing work on Genetic Improvement. Our hope is that by drawing such connections we can spark the interest of both research communities on the exciting synergies at the intersection of these subject areas.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {823–830},
numpages = {8},
keywords = {variability, software product lines, genetic programming, genetic improvement, evolutionary algorithms},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@inproceedings{10.1145/2532443.2532448,
author = {Zhao, Tianqi and Zhao, Haiyan and Zhang, Wei},
title = {A preliminary study on requirements modeling methods for self-adaptive software systems},
year = {2013},
isbn = {9781450323697},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2532443.2532448},
doi = {10.1145/2532443.2532448},
abstract = {Internetware denotes a kind of complex distributed software system, which executes in an open, uncertain and dynamic environment, and adapts itself to changes in the environment. An important problem related to the development of Internetware applications is how to define their requirements. Traditional requirements modeling methods work well with software applications deployed in predictable environment, but cannot deal with Internetware applications, which have to identify and adapt themselves to the unpredictable situations of their environment. The self-adaptation characteristic of Internetware applications introduces challenges to the effective modeling of the requirements of Internetware applications. In this paper, we carry out a preliminary study on requirements modeling methods for self-adaptive software systems. In particular, we focus on how existing requirements modeling methods address the challenges caused by self-adaptation and what are the advantages and disadvantages of their solutions. By doing this study, we aim to identify the essential capabilities or properties that a requirements modeling method should possess so as to support the requirements modeling of self-adaptive software systems like Internetware.},
booktitle = {Proceedings of the 5th Asia-Pacific Symposium on Internetware},
articleno = {3},
numpages = {10},
keywords = {self-adaptive system, requirement verification, requirement representation, requirement modeling method, requirement evolution},
location = {Changsha, China},
series = {Internetware '13}
}

@inproceedings{10.1109/MEMCOD.2015.7340482,
title = {On the deployment problem of embedded systems},
year = {2015},
isbn = {9781509002375},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MEMCOD.2015.7340482},
doi = {10.1109/MEMCOD.2015.7340482},
abstract = {The quality of today's embedded systems e. g. in vehicles, airplanes, or automation plants is highly influenced by their architecture. In this context, we study the so-called deployment problem. The question is where (i. e., on which execution unit) to deploy which software application or which sensor/actuator shall be connected to which device in an automation plant. First, we introduce a domain-specific constraint and optimization language fitting the needs of our partners. Second, we investigate different approaches to tackle the deployment problem even for industrial size systems. Therefore, we present different solving strategies using (i) multi-objective evolutionary algorithms, (ii) SMT-based, and (iii) ILP-based solving approaches. Furthermore, a combination of the first two is used. We investigate the proposed methods and demonstrate their feasibility using two realistic systems: a civil flight control system (FCS), and a seawater desalination plant.},
booktitle = {Proceedings of the 2015 ACM/IEEE International Conference on Formal Methods and Models for Codesign},
pages = {158–167},
numpages = {10},
series = {MEMOCODE '15}
}

@inproceedings{10.1145/2866614.2866628,
author = {Th\"{u}m, Thomas and Winkelmann, Tim and Schr\"{o}ter, Reimar and Hentschel, Martin and Kr\"{u}ger, Stefan},
title = {Variability Hiding in Contracts for Dependent Software Product Lines},
year = {2016},
isbn = {9781450340199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2866614.2866628},
doi = {10.1145/2866614.2866628},
abstract = {Software product lines are used to efficiently develop and verify similar software products. While they focus on reuse of artifacts between products, a product line may also be reused itself in other product lines. A challenge with such dependent product lines is evolution; every change in a product line may influence all dependent product lines. With variability hiding, we aim to hide certain features and their artifacts in dependent product lines. In prior work, we focused on feature models and implementation artifacts. We build on this by discussing how variability hiding can be extended to specifications in terms of method contracts. We illustrate variability hiding in contracts by means of a running example and share our insights with preliminary experiments on the benefits for formal verification. In particular, we find that not every change in a certain product line requires a re-verification of other dependent product lines.},
booktitle = {Proceedings of the 10th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {method contracts, deductive verification, Multi product line},
location = {Salvador, Brazil},
series = {VaMoS '16}
}

@inproceedings{10.1145/2576768.2598305,
author = {Lopez-Herrejon, Roberto Erick and Javier Ferrer, Javier and Chicano, Francisco and Haslinger, Evelyn Nicole and Egyed, Alexander and Alba, Enrique},
title = {A parallel evolutionary algorithm for prioritized pairwise testing of software product lines},
year = {2014},
isbn = {9781450326629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2576768.2598305},
doi = {10.1145/2576768.2598305},
abstract = {Software Product Lines (SPLs) are families of related software systems, which provide different feature combinations. Different SPL testing approaches have been proposed. However, despite the extensive and successful use of evolutionary computation techniques for software testing, their application to SPL testing remains largely unexplored. In this paper we present the Parallel Prioritized product line Genetic Solver (PPGS), a parallel genetic algorithm for the generation of prioritized pairwise testing suites for SPLs. We perform an extensive and comprehensive analysis of PPGS with 235 feature models from a wide range of number of features and products, using 3 different priority assignment schemes and 5 product prioritization selection strategies. We also compare PPGS with the greedy algorithm prioritized-ICPL. Our study reveals that overall PPGS obtains smaller covering arrays with an acceptable performance difference with prioritized-ICPL.},
booktitle = {Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {1255–1262},
numpages = {8},
keywords = {software product lines, pairwise testing, feature models, combinatorial interaction testing},
location = {Vancouver, BC, Canada},
series = {GECCO '14}
}

@inproceedings{10.1145/3194760.3194761,
author = {Kessel, Marcus and Atkinson, Colin},
title = {Integrating reuse into the rapid, continuous software engineering cycle through test-driven search},
year = {2018},
isbn = {9781450357456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194760.3194761},
doi = {10.1145/3194760.3194761},
abstract = {Today's advanced agile practices such as Continuous Integration and Test-Driven Development support a wide range of software development activities to facilitate the rapid delivery of high-quality software. However, the reuse of pre-existing, third-party software components is not one of them. Software reuse is still primarily perceived as a time-consuming, unsystematic and ultimately, "discontinuous" activity even though it aims to deliver the same basic benefits as continuous software engineering - namely, a reduction in the time and effort taken to deliver quality software. However, the increasingly central role of testing in continuous software engineering offers a way of addressing this problem by exploiting the new generation of test-driven search engines that can harvest components based on tests. This search technology not only exploits artifacts that have already been created as part of the continuous testing process to harvest components, it returns results that have a high likelihood of being fit for purpose and thus of being worth reusing. In this paper, we propose to augment continuous software engineering with the rapid, continuous reuse of software code units by integrating the test-driven mining of software artifact repositories into the continuous integration process. More specifically, we propose to use tests written as part of the Test-First Development approach to perform test-driven searches for matching functionality while developers are working on their normal development activities. We discuss the idea of rapid, continuous code reuse based on recent advances in our test-driven search platform and elaborate on scenarios for its application in the future.},
booktitle = {Proceedings of the 4th International Workshop on Rapid Continuous Software Engineering},
pages = {8–11},
numpages = {4},
keywords = {test-driven search, test-driven reuse, test-driven development, rapid continuous integration, rapid continuous code reuse},
location = {Gothenburg, Sweden},
series = {RCoSE '18}
}

@inproceedings{10.1145/2420942.2420943,
author = {Bo\v{s}kovi\'{c}, Marko and Mussbacher, Gunter and Ga\v{s}evi\'{c}, Dragan and Bagheri, Ebrahim},
title = {The Fourth International Workshop on Non-functional System Properties in Domain Specific Modeling Languages (NFPinDSML2012)},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420943},
doi = {10.1145/2420942.2420943},
abstract = {The International Workshop on Non-functional System Properties in Domain Specific Modeling Languages (NFPinDSML) series traditionally takes place as part of the Satellite Events of the ACM/IEEE International Conference on Model Driven Engineering Languages and Systems (MODELS). Traditionally, NFPinDSML gathers researchers and practitioners interested in the estimation and evaluation of system quality and their integration in Domain Specific Modeling Languages and Model Driven Engineering in general. This paper is the summary of the fourth NFPinDSML workshop which was affiliated with MODELS 2012.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {1},
numpages = {2},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@article{10.1145/3487921,
author = {Hezavehi, Sara M. and Weyns, Danny and Avgeriou, Paris and Calinescu, Radu and Mirandola, Raffaela and Perez-Palacin, Diego},
title = {Uncertainty in Self-adaptive Systems: A Research Community Perspective},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/3487921},
doi = {10.1145/3487921},
abstract = {One of the primary drivers for self-adaptation is ensuring that systems achieve their goals regardless of the uncertainties they face during operation. Nevertheless, the concept of uncertainty in self-adaptive systems is still insufficiently understood. Several taxonomies of uncertainty have been proposed, and a substantial body of work exists on methods to tame uncertainty. Yet, these taxonomies and methods do not fully convey the research community’s perception on what constitutes uncertainty in self-adaptive systems and on the key characteristics of the approaches needed to tackle uncertainty. To understand this perception and learn from it, we conducted a survey comprising two complementary stages in which we collected the views of 54 and 51 participants, respectively. In the first stage, we focused on current research and development, exploring how the concept of uncertainty is understood in the community and how uncertainty is currently handled in the engineering of self-adaptive systems. In the second stage, we focused on directions for future research to identify potential approaches to dealing with unanticipated changes and other open challenges in handling uncertainty in self-adaptive systems. The key findings of the first stage are: (a) an overview of uncertainty sources considered in self-adaptive systems, (b) an overview of existing methods used to tackle uncertainty in concrete applications, (c) insights into the impact of uncertainty on non-functional requirements, (d) insights into different opinions in the perception of uncertainty within the community and the need for standardised uncertainty-handling processes to facilitate uncertainty management in self-adaptive systems. The key findings of the second stage are: (a) the insight that over 70% of the participants believe that self-adaptive systems can be engineered to cope with unanticipated change, (b) a set of potential approaches for dealing with unanticipated change, (c) a set of open challenges in mitigating uncertainty in self-adaptive systems, in particular in those with safety-critical requirements. From these findings, we outline an initial reference process to manage uncertainty in self-adaptive systems. We anticipate that the insights on uncertainty obtained from the community and our proposed reference process will inspire valuable future research on self-adaptive systems.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = dec,
articleno = {10},
numpages = {36},
keywords = {survey, uncertainty challenges, unanticipated change, uncertainty methods, uncertainty models, uncertainty, Self-adaptation}
}

@inproceedings{10.1145/3555776.3578611,
author = {Limaylla-Lunarejo, Maria-Isabel and Condori-Fernandez, Nelly and Luaces, Miguel R.},
title = {Towards a FAIR Dataset for non-functional requirements},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3578611},
doi = {10.1145/3555776.3578611},
abstract = {In the last years, the application of supervised Machine Learning (ML) algorithms in Requirements Engineering (RE) has allowed increasing the performance (e.g. accuracy, precision) and scalability of automatic requirements classification. However, the lack of publicly labeled datasets is still one concern when conducting ML experiments. Few publicly labeled datasets for non-functional requirements classification are available, and even less in the Spanish language. Moreover, most of the available datasets present some limitations, such as imbalanced classes (e.g. PROMISE NFR). This study aims to generate a FAIR dataset of non-functional requirements in the Spanish language for facilitating reuse in ML classification experiments. 109 non-functional requirements were collected from final degree projects from the University of A Coru\~{n}a. We conducted a pilot quasi-experiment for non-functional requirements labeling in the categories and subcategories of the ISO/IEC 25010 quality model. The labeling process was accomplished by 7 annotators. The inter-annotator agreement using a Fleiss' Kappa test obtained a substantial agreement in the category level (0.78) and a moderate agreement (0.48) when the classification is per subcategory.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1414–1421},
numpages = {8},
keywords = {FAIR principles, spanish dataset, non-functional requirements, data labeling},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/2047862.2047866,
author = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Pukall, Mario and Apel, Sven},
title = {Tailoring dynamic software product lines},
year = {2011},
isbn = {9781450306898},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047862.2047866},
doi = {10.1145/2047862.2047866},
abstract = {Software product lines (SPLs) and adaptive systems aim at variability to cope with changing requirements. Variability can be described in terms of features, which are central for development and configuration of SPLs. In traditional SPLs, features are bound statically before runtime. By contrast, adaptive systems support feature binding at runtime and are sometimes called dynamic SPLs (DSPLs). DSPLs are usually built from coarse-grained components, which reduces the number of possible application scenarios. To overcome this limitation, we closely integrate static binding of traditional SPLs and runtime adaptation of DSPLs. We achieve this integration by statically generating a tailor-made DSPL from a highly customizable SPL. The generated DSPL provides only the runtime variability required by a particular application scenario and the execution environment. The DSPL supports self-configuration based on coarse-grained modules. We provide a feature-based adaptation mechanism that reduces the effort of computing an optimal configuration at runtime. In a case study, we demonstrate the practicability of our approach and show that a seamless integration of static binding and runtime adaptation reduces the complexity of the adaptation process.},
booktitle = {Proceedings of the 10th ACM International Conference on Generative Programming and Component Engineering},
pages = {3–12},
numpages = {10},
keywords = {software product lines, feature-oriented programming, dynamic binding},
location = {Portland, Oregon, USA},
series = {GPCE '11}
}

@inproceedings{10.1145/2593770.2593781,
author = {Ramaswamy, Arunkumar and Monsuez, Bruno and Tapus, Adriana},
title = {Model-driven software development approaches in robotics research},
year = {2014},
isbn = {9781450328494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593770.2593781},
doi = {10.1145/2593770.2593781},
abstract = {Recently, there is an encouraging trend in adopting model-driven engineering approaches for software development in robotics research. In this paper, currently available model-driven techniques in robotics are analyzed with respect to the domain-specific requirements. A conceptual overview of our software development approach called 'Self Adaptive Framework for Robotic Systems (SafeRobots)' is explained and we also try to position our approach within this model ecosystem.},
booktitle = {Proceedings of the 6th International Workshop on Modeling in Software Engineering},
pages = {43–48},
numpages = {6},
keywords = {Robotics, Model-driven software development},
location = {Hyderabad, India},
series = {MiSE 2014}
}

@inproceedings{10.1145/2601248.2601257,
author = {H\"{a}ser, Florian and Felderer, Michael and Breu, Ruth},
title = {Software paradigms, assessment types and non-functional requirements in model-based integration testing: a systematic literature review},
year = {2014},
isbn = {9781450324762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2601248.2601257},
doi = {10.1145/2601248.2601257},
abstract = {Context: In modern systems, like cyber-physical systems, where software and physical services are interacting, safety, security or performance play an important role. In order to guarantee the correct interoperability of such systems, with respect to functional and non-functional requirements, integration testing is an effective measure to achieve this. Model-based testing moreover not only enables early definition and validation, but also test automation. This makes it a good choice to overcome urgent challenges of integration testing. Objective: Many publications on model-based integration testing (MBIT) approaches can be found. Nevertheless, a study giving a systematic overview on the underlying software paradigms, measures for guiding the integration testing process as well as non-functional requirements they are suitable for, is missing. The aim of this paper is to find and synthesize the relevant primary studies to gain a comprehensive understanding of the current state of model-based integration testing. Method: For synthesizing the relevant studies, we conducted a systematic literature review (SLR) according to the guidelines of Kitchenham. Results: The systematic search and selection retrieved 83 relevant studies from which data has been extracted. Our review identified three assessment criteria for guiding the testing process, namely static metrics, dynamic metrics and stochastic &amp;random. In addition it shows that just a small fraction considers non-functional requirements. Most approaches are for component-oriented systems. Conclusion: Results from the SLR show that there are two major research gaps. First, there is an accumulated need for approaches in the MBIT field that support non-functional requirements, as they are gaining importance. Second, means for steering the integration testing process, especially together with automation, need to evolve.},
booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {29},
numpages = {10},
keywords = {systematic literature review, non-functional requirements, model-based integration testing, assessment types},
location = {London, England, United Kingdom},
series = {EASE '14}
}

@inproceedings{10.1145/3387940.3391474,
author = {Brings, Jennifer and Daun, Marian},
title = {Towards automated safety analysis for architectures of dynamically forming networks of cyber-physical systems},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391474},
doi = {10.1145/3387940.3391474},
abstract = {Dynamically forming networks of cyber-physical systems are becoming increasingly widespread in manufacturing, transportation, automotive, avionics and more domains. The emergence of future internet technology and the ambition for ever closer integration of different systems leads to highly collaborative cyber-physical systems. Such cyber-physical systems form networks to provide additional functions, behavior, and benefits the individual systems cannot provide on their own. As safety is a major concern of systems from these domains, there is a need to provide adequate support for safety analyses of these collaborative cyber-physical systems. This support must explicitly consider the dynamically formed networks of cyber-physical systems. This is a challenging task as the configurations of these cyber-physical system networks (i.e. the architecture of the super system the individual system joins) can differ enormously depending on the actual systems joining a cyber-physical system network. Furthermore, the configuration of the network heavily impacts the adaptations performed by the individual systems and thereby impacting the architecture not only of the system network but of all individual systems involved. As existing safety analysis techniques, however, are not meant for supporting such an array of potential system network configurations the individual system will have to be able to cope with at runtime, we propose automated support for safety analysis for these systems that considers the configuration of the system network. Initial evaluation results from the application to industrial case examples show that the proposed support can aid in the detection of safety defects.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {258–265},
numpages = {8},
keywords = {system architecture, safety analysis, cyber-physical system},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@article{10.1145/3582571,
author = {Xu, Qinghua and Ali, Shaukat and Yue, Tao},
title = {Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3582571},
doi = {10.1145/3582571},
abstract = {Anomaly detection is critical to ensure the security of cyber-physical systems (CPS). However, due to the increasing complexity of attacks and CPS themselves, anomaly detection in CPS is becoming more and more challenging. In our previous work, we proposed a digital twin-based anomaly detection method, called ATTAIN, which takes advantage of both historical and real-time data of CPS. However, such data vary significantly in terms of difficulty. Therefore, similar to human learning processes, deep learning models (e.g., ATTAIN) can benefit from an easy-to-difficult curriculum. To this end, in this paper, we present a novel approach, named digitaL twin-based Anomaly deTecTion wIth Curriculum lEarning (LATTICE), which extends ATTAIN by introducing curriculum learning to optimize its learning paradigm. LATTICE attributes each sample with a difficulty score, before being fed into a training scheduler. The training scheduler samples batches of training data based on these difficulty scores such that learning from easy to difficult data can be performed. To evaluate LATTICE, we use five publicly available datasets collected from five real-world CPS testbeds. We compare LATTICE with ATTAIN and two other state-of-the-art anomaly detectors. Evaluation results show that LATTICE outperforms the three baselines and ATTAIN by 0.906%-2.367% in terms of the F1 score. LATTICE also, on average, reduces the training time of ATTAIN by 4.2% on the five datasets and is on par with the baselines in terms of detection delay time.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {113},
numpages = {32},
keywords = {anomaly detection, deep learning, curriculum learning, digital twin, Cyber-physical system}
}

@inproceedings{10.1145/2610384.2610411,
author = {Galindo, Jos\'{e} A. and Alf\'{e}rez, Mauricio and Acher, Mathieu and Baudry, Benoit and Benavides, David},
title = {A variability-based testing approach for synthesizing video sequences},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610411},
doi = {10.1145/2610384.2610411},
abstract = {A key problem when developing video processing software is the difficulty to test different input combinations. In this paper, we present VANE, a variability-based testing approach to derive video sequence variants. The ideas of VANE are i) to encode in a variability model what can vary within a video sequence; ii) to exploit the variability model to generate testable configurations; iii) to synthesize variants of video sequences corresponding to configurations. VANE computes T-wise covering sets while optimizing a function over attributes. Also, we present a preliminary validation of the scalability and practicality of VANE in the context of an industrial project involving the test of video processing algorithms.},
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {293–303},
numpages = {11},
keywords = {Video analysis, Variability, Combinatorial testing},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@article{10.1145/3672555,
author = {Mahdavi-Hezaveh, Rezvan and Fatima, Sameeha and Williams, Laurie},
title = {Paving a Path for a Combined Family of Feature Toggle and Configuration Option Research},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3672555},
doi = {10.1145/3672555},
abstract = {Feature toggles and configuration options are techniques to include or exclude functionality in software. The research contributions to these two techniques have most often been focused on either one of them. However, focusing on the similarities of these two techniques and the use of a common terminology may enable a combined family of research on software configuration (a term we use to encompass both techniques) and prevent duplication of effort. The goal of this study is to aid researchers in conducting a family of research on software configuration by extending an existing model of software configuration that provides a common terminology for feature toggles and configuration options in research studies. We started with Siegmund et al.’s Model of Software Configuration (MSC), which was developed based on configuration option-related resources. We extend the MSC by qualitative analysis of feature toggle-related resources. From our analysis, we proposed MSCv2 and evaluated it through its application on publications and an industrial system. Our results indicate researchers studying the same system may provide different definitions of software configuration in publications, similar research questions may be answered repeatedly because of a lack of a clear definition of software configuration, and having an MSC may enable generalized research on this family of research.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {172},
numpages = {27},
keywords = {Feature toggle, configuration option, software configuration, software engineering}
}

@inproceedings{10.1145/1868688.1868690,
author = {Siegmund, Norbert and Rosenm\"{u}ller, Marko and Apel, Sven},
title = {Automating energy optimization with features},
year = {2010},
isbn = {9781450302081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868688.1868690},
doi = {10.1145/1868688.1868690},
abstract = {Mobile devices such as cell phones and notebooks rely on battery power supply. For these systems, optimizing the power consumption is important to increase the system's lifetime. However, this is hard to achieve because energy-saving functions often depend on the hardware, and operating systems. The diversity of hardware components and operating systems makes the implementation time consuming and difficult. We propose an approach to automate energy optimization of programs by implementing energy-saving functionality as modular, separate implementation units (e.g., feature modules or aspects). These units are bundled as energy features into an energy-optimization feature library. Based on aspect-oriented and feature-oriented programming, we discuss different techniques to compose the source code of a client program and the implementation units of the energy features.},
booktitle = {Proceedings of the 2nd International Workshop on Feature-Oriented Software Development},
pages = {2–9},
numpages = {8},
keywords = {software product lines, feature-oriented programming, energy consumption},
location = {Eindhoven, The Netherlands},
series = {FOSD '10}
}

@inproceedings{10.1145/2642937.2642990,
author = {Abal, Iago and Brabrand, Claus and Wasowski, Andrzej},
title = {42 variability bugs in the linux kernel: a qualitative analysis},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2642990},
doi = {10.1145/2642937.2642990},
abstract = {Feature-sensitive verification pursues effective analysis of the exponentially many variants of a program family. However, researchers lack examples of concrete bugs induced by variability, occurring in real large-scale systems. Such a collection of bugs is a requirement for goal-oriented research, serving to evaluate tool implementations of feature-sensitive analyses by testing them on real bugs. We present a qualitative study of 42 variability bugs collected from bug-fixing commits to the Linux kernel repository. We analyze each of the bugs, and record the results in a database. In addition, we provide self-contained simplified C99 versions of the bugs, facilitating understanding and tool evaluation. Our study provides insights into the nature and occurrence of variability bugs in a large C software system, and shows in what ways variability affects and increases the complexity of software bugs.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {421–432},
numpages = {12},
keywords = {software variability, linux, feature interactions, bugs},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@inproceedings{10.1145/3324884.3416620,
author = {Dorn, Johannes and Apel, Sven and Siegmund, Norbert},
title = {Mastering uncertainty in performance estimations of configurable software systems},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416620},
doi = {10.1145/3324884.3416620},
abstract = {Understanding the influence of configuration options on performance is key for finding optimal system configurations, system understanding, and performance debugging. In prior research, a number of performance-influence modeling approaches have been proposed, which model a configuration option's influence and a configuration's performance as a scalar value. However, these point estimates falsely imply a certainty regarding an option's influence that neglects several sources of uncertainty within the assessment process, such as (1) measurement bias, (2) model representation and learning process, and (3) incomplete data. This leads to the situation that different approaches and even different learning runs assign different scalar performance values to options and interactions among them. The true influence is uncertain, though. There is no way to quantify this uncertainty with state-of-the-art performance modeling approaches. We propose a novel approach, P4, based on probabilistic programming that explicitly models uncertainty for option influences and consequently provides a confidence interval for each prediction of a configuration's performance alongside a scalar. This way, we can explain, for the first time, why predictions may cause errors and which option's influences may be unreliable. An evaluation on 12 real-world subject systems shows that P4's accuracy is in line with the state of the art while providing reliable confidence intervals, in addition to scalar predictions.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {684–696},
numpages = {13},
keywords = {P4, configurable software systems, performance-influence modeling, probabilistic programming},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/2695664.2695875,
author = {Almeida, Andr\'{e} and Bencomo, Nelly and Batista, Thais and Cavalcante, Everton and Dantas, Francisco},
title = {Dynamic decision-making based on NFR for managing software variability and configuration selection},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695875},
doi = {10.1145/2695664.2695875},
abstract = {Due to dynamic variability, identifying the specific conditions under which non-functional requirements (NFRs) are satisfied may be only possible at runtime. Therefore, it is necessary to consider the dynamic treatment of relevant information during the requirements specifications. The associated data can be gathered by monitoring the execution of the application and its underlying environment to support reasoning about how the current application configuration is fulfilling the established requirements. This paper presents a dynamic decision-making infrastructure to support both NFRs representation and monitoring, and to reason about the degree of satisfaction of NFRs during runtime. The infrastructure is composed of: (i) an extended feature model aligned with a domain-specific language for representing NFRs to be monitored at runtime; (ii) a monitoring infrastructure to continuously assess NFRs at runtime; and (iii) a flexible decision-making process to select the best available configuration based on the satisfaction degree of the NRFs. The evaluation of the approach has shown that it is able to choose application configurations that well fit user NFRs based on runtime information. The evaluation also revealed that the proposed infrastructure provided consistent indicators regarding the best application configurations that fit user NFRs. Finally, a benefit of our approach is that it allows us to quantify the level of satisfaction with respect to NFRs specification.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1376–1382},
numpages = {7},
keywords = {variability, non-functional requirements, monitoring, SPLs},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.5555/2555523.2555556,
author = {Bagheri, Ebrahim and Ensan, Faezeh},
title = {Light-weight software product lines for small and medium-sized enterprises (SMEs)},
year = {2013},
publisher = {IBM Corp.},
address = {USA},
abstract = {Product line engineering practices promote the idea of systematic reuse of core assets and have been reported to decrease time-to-market and development costs for new products. However, our recent efforts to transfer our product line engineering knowledge to several of our small and medium-size enterprise industrial partner showed that there are challenges that need to be addressed before core product line engineering ideas can be deployed in SME context. These challenges include upfront investment costs, business traceability, levels of abstraction of functional features and semantic distinction between functional and non-functional software aspects. In order to address these challenges within the context of SMEs, we adopt and extend the behavior-driven development methodology in a way to not only offer agility in practice but also to equip software developers with the means to capture and manage software variability within the behavior-driven development process. We introduce the details of the extended methodology and discuss its advantages and disadvantages in detail.},
booktitle = {Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {311–324},
numpages = {14},
location = {Ontario, Canada},
series = {CASCON '13}
}

@inproceedings{10.1145/2024436.2024438,
author = {Calinescu, Radu},
title = {When the requirements for adaptation and high integrity meet},
year = {2011},
isbn = {9781450308533},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2024436.2024438},
doi = {10.1145/2024436.2024438},
abstract = {Two classes of software that are notoriously difficult to develop on their own are rapidly merging into one. This will affect every key service that we rely upon in modern society, yet a successful merge is unlikely to be achievable using software development techniques specific to either class.This paper explains the growing demand for software capable of both self-adaptation and high integrity, and advocates the use of a collection of "@runtime" techniques for its development, operation and management. We summarise early research into the development of such techniques, and discuss the remaining work required to overcome the great challenge of self-adaptive high-integrity software.},
booktitle = {Proceedings of the 8th Workshop on Assurances for Self-Adaptive Systems},
pages = {1–4},
numpages = {4},
keywords = {self-adaptive software, model checking, high-integrity software},
location = {Szeged, Hungary},
series = {ASAS '11}
}

@inproceedings{10.1145/2642937.2642939,
author = {Segura, Sergio and S\'{a}nchez, Ana B. and Ruiz-Cort\'{e}s, Antonio},
title = {Automated variability analysis and testing of an E-commerce site.: an experience report},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2642939},
doi = {10.1145/2642937.2642939},
abstract = {In this paper, we report on our experience on the development of La Hilandera, an e-commerce site selling haberdashery products and craft supplies in Europe. The store has a huge input space where customers can place almost three millions of different orders which made testing an extremely difficult task. To address the challenge, we explored the applicability of some of the practices for variability management in software product lines. First, we used a feature model to represent the store input space which provided us with a variability view easy to understand, share and discuss with all the stakeholders. Second, we used techniques for the automated analysis of feature models for the detection and repair of inconsistent and missing configuration settings. Finally, we used test selection and prioritization techniques for the generation of a manageable and effective set of test cases. Our findings, summarized in a set of lessons learnt, suggest that variability techniques could successfully address many of the challenges found when developing e-commerce sites.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {139–150},
numpages = {12},
keywords = {automated testing, e-commerce, experience report, feature modelling, variability},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@inproceedings{10.5555/2008503.2008518,
author = {Bo\v{s}kovi\'{c}, Marko and Mussbacher, Gunter and Bagheri, Ebrahim and Amyot, Daniel and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek},
title = {Aspect-oriented feature models},
year = {2010},
isbn = {9783642212093},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software Product Lines (SPLs) have emerged as a prominent approach for software reuse. SPLs are sets of software systems called families that are usually developed as a whole and share many common features. Feature models are most typically used as a means for capturing commonality and managing variability of the family. A particular product from the family is configured by selecting the desired features of that product. Typically, feature models are considered monolithic entities that do not support modularization well. As industrial feature models tend to be large, their modularization has become an important research topic lately. However, existing modularization approaches do not support modularization of crosscutting concerns. In this paper, we introduce Aspect-oriented Feature Models (AoFM) and argue that using aspect-oriented techniques improves the manageability and reduces the maintainability effort of feature models. Particularly, we advocate an asymmetric approach that allows for the modularization of basic and crosscutting concerns in feature models.},
booktitle = {Proceedings of the 2010 International Conference on Models in Software Engineering},
pages = {110–124},
numpages = {15},
keywords = {aspect-oriented modeling, feature models, software product lines},
location = {Oslo, Norway},
series = {MODELS'10}
}

@inproceedings{10.1145/1944892.1944913,
author = {Nguyen, Tuan and Colman, Alan and Talib, Muhammad Adeel and Han, Jun},
title = {Managing service variability: state of the art and open issues},
year = {2011},
isbn = {9781450305709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944892.1944913},
doi = {10.1145/1944892.1944913},
abstract = {In addition to inherited characteristics from software variability, service variability exposes two distinct characteristics that impose certain challenges in variability management. These characteristics are: i) Different types of variability and their inter-relationships; and ii) Dynamic and recursive variability communication among different stakeholders. This paper elaborates these distinct characteristics in detail with a case study. The challenges brought about by these distinct characteristics in managing variability also are highlighted. We present a review of related work in service variability management and briefly propose our ongoing approach to addressing these challenges.},
booktitle = {Proceedings of the 5th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {165–173},
numpages = {9},
keywords = {web services, variability management, variability communication, service variability, service oriented computing},
location = {Namur, Belgium},
series = {VaMoS '11}
}

@inproceedings{10.1109/ICSE.2019.00113,
author = {Ha, Huong and Zhang, Hongyu},
title = {DeepPerf: performance prediction for configurable software with deep sparse neural network},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00113},
doi = {10.1109/ICSE.2019.00113},
abstract = {Many software systems provide users with a set of configuration options and different configurations may lead to different runtime performance of the system. As the combination of configurations could be exponential, it is difficult to exhaustively deploy and measure system performance under all possible configurations. Recently, several learning methods have been proposed to build a performance prediction model based on performance data collected from a small sample of configurations, and then use the model to predict system performance under a new configuration. In this paper, we propose a novel approach to model highly configurable software system using a deep feedforward neural network (FNN) combined with a sparsity regularization technique, e.g. the L1 regularization. Besides, we also design a practical search strategy for automatically tuning the network hyperparameters efficiently. Our method, called DeepPerf, can predict performance values of highly configurable software systems with binary and/or numeric configuration options at much higher prediction accuracy with less training data than the state-of-the art approaches. Experimental results on eleven public real-world datasets confirm the effectiveness of our approach.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1095–1106},
numpages = {12},
keywords = {deep sparse feedforward neural network, highly configurable systems, software performance prediction, sparsity regularization},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/2025113.2025177,
author = {Mori, Marco},
title = {A software lifecycle process for context-aware adaptive systems},
year = {2011},
isbn = {9781450304436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2025113.2025177},
doi = {10.1145/2025113.2025177},
abstract = {It is increasingly important for computing systems to evolve their behavior at run-time because of resources uncertainty, system failures and emerging user needs. Our approach supports software engineers to analyze and develop context-aware adaptive applications. The software lifecycle process we propose supports static and dynamic decision making mechanisms, run-time consistent evolution and it is amenable to be automated.},
booktitle = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
pages = {412–415},
numpages = {4},
keywords = {feature engineering, software lifecycle process, consistent evolution, context-aware adaptive systems},
location = {Szeged, Hungary},
series = {ESEC/FSE '11}
}

@article{10.1145/3229048,
author = {Zheng, Yongjie and Cu, Cuong and Taylor, Richard N.},
title = {Maintaining Architecture-Implementation Conformance to Support Architecture Centrality: From Single System to Product Line Development},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3229048},
doi = {10.1145/3229048},
abstract = {Architecture-centric development addresses the increasing complexity and variability of software systems by focusing on architectural models, which are generally easier to understand and manipulate than source code. It requires a mechanism that can maintain architecture-implementation conformance during architectural development and evolution. The challenge is twofold. There is an abstraction gap between software architecture and implementation, and both may evolve. Existing approaches are deficient in support for both change mapping and product line architecture. This article presents a novel approach named 1.x-way mapping and its extension, 1.x-line mapping to support architecture-implementation mapping in single system development and in product line development, respectively. They specifically address mapping architecture changes to code, maintaining variability conformance between product line architecture and code, and tracing architectural implementation. We built software tools named xMapper and xLineMapper to realize the two approaches, and conducted case studies with two existing open-source systems to evaluate the approaches. The result shows that our approaches are applicable to the implementation of a real software system and are capable of maintaining architecture-implementation conformance during system evolution.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {8},
numpages = {52},
keywords = {Architecture-implementation mapping, architectural evolution, architecture-centric development, architecture-centric feature traceability, variability conformance}
}

@inproceedings{10.1145/2088876.2088879,
author = {Merle, Philippe and Rouvoy, Romain and Seinturier, Lionel},
title = {A reflective platform for highly adaptive multi-cloud systems},
year = {2011},
isbn = {9781450310703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2088876.2088879},
doi = {10.1145/2088876.2088879},
abstract = {Cloud platforms are increasingly used for hosting a broad diversity of services from traditional e-commerce applications to interactive web-based IDEs. However, we observe that the proliferation of offers by Cloud vendors raises several challenges. Developers will not only have to deploy applications for a specific Cloud, but will also have to consider migrating services from one cloud to another, and to manage applications spanning multiple Clouds. In this paper, we therefore report on a first experiment we conducted to build a multi-Cloud system on top of thirteen existing IaaS/PaaS. From this experiment, we advocate for two dimensions of adaptability---design and execution time---that applications for such systems require to exhibit. Finally, we propose a roadmap for future multi-Cloud systems.},
booktitle = {Adaptive and Reflective Middleware on Proceedings of the International Workshop},
pages = {14–21},
numpages = {8},
location = {Lisbon, Portugal},
series = {ARM '11}
}

@inproceedings{10.1145/2897010.2897011,
author = {Fischer, Stefan and Lopez-Herrejon, Roberto E. and Ramler, Rudolf and Egyed, Alexander},
title = {A preliminary empirical assessment of similarity for combinatorial interaction testing of software product lines},
year = {2016},
isbn = {9781450341660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897010.2897011},
doi = {10.1145/2897010.2897011},
abstract = {Extensive work on Search-Based Software Testing for Software Product Lines has been published in the last few years. Salient among them is the use of similarity as a surrogate metric for t-wise coverage whenever higher strengths are needed or whenever the size of the test suites is infeasible because of technological or budget limitations. Though promising, this metric has not been assessed with real fault data. In this paper, we address this limitation by using Drupal, a widely used open source web content management system, as an industry-strength case study for which both variability information and fault data have been recently made available. Our preliminary assessment corroborates some of the previous findings but also raises issues on some assumptions and claims made. We hope our work encourages further empirical evaluations of Combinatorial Interaction Testing approaches for Software Product Lines.},
booktitle = {Proceedings of the 9th International Workshop on Search-Based Software Testing},
pages = {15–18},
numpages = {4},
location = {Austin, Texas},
series = {SBST '16}
}

@inproceedings{10.1145/2658761.2658767,
author = {Ruprecht, Andreas and Heinloth, Bernhard and Lohmann, Daniel},
title = {Automatic feature selection in large-scale system-software product lines},
year = {2014},
isbn = {9781450331616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658761.2658767},
doi = {10.1145/2658761.2658767},
abstract = {System software can typically be configured at compile time via a comfortable feature-based interface to tailor its functionality towards a specific use case. However, with the growing number of features, this tailoring process becomes increasingly difficult: As a prominent example, the Linux kernel in v3.14 provides nearly 14 000 configuration options to choose from. Even developers of embedded systems refrain from trying to build a minimized distinctive kernel configuration for their device – and thereby waste memory and money for unneeded functionality. In this paper, we present an approach for the automatic use-case specific tailoring of system software for special-purpose embedded systems. We evaluate the effectiveness of our approach on the example of Linux by generating tailored kernels for well-known applications of the Rasperry Pi and a Google Nexus 4 smartphone. Compared to the original configurations, our approach leads to memory savings of 15–70 percent and requires only very little manual intervention.},
booktitle = {Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences},
pages = {39–48},
numpages = {10},
keywords = {Feature Selection, Linux, Software Product Lines, Software Tailoring},
location = {V\"{a}ster\r{a}s, Sweden},
series = {GPCE 2014}
}

@inproceedings{10.5555/2337223.2337416,
author = {Perrouin, Gilles and Morin, Brice and Chauvel, Franck and Fleurey, Franck and Klein, Jacques and Le Traon, Yves and Barais, Olivier and J\'{e}z\'{e}quel, Jean-Marc},
title = {Towards flexible evolution of dynamically adaptive systems},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Modern software systems need to be continuously available under varying conditions. Their ability to dynamically adapt to their execution context is thus increasingly seen as a key to their success. Recently, many approaches were proposed to design and support the execution of Dynamically Adaptive Systems (DAS). However, the ability of a DAS to evolve is limited to the addition, update or removal of adaptation rules or reconfiguration scripts. These artifacts are very specific to the control loop managing such a DAS and runtime evolution of the DAS requirements may affect other parts of the DAS. In this paper, we argue to evolve all parts of the loop. We suggest leveraging recent advances in model-driven techniques to offer an approach that supports the evolution of both systems and their adaptation capabilities. The basic idea is to consider the control loop itself as an adaptive system.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {1353–1356},
numpages = {4},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@proceedings{10.1145/2993236,
title = {GPCE 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.1109/MISE.2009.5069896,
author = {Acher, Mathieu and Lahire, Philippe and Moisan, Sabine and Rigault, Jean-Paul},
title = {Tackling high variability in video surveillance systems through a model transformation approach},
year = {2009},
isbn = {9781424437221},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MISE.2009.5069896},
doi = {10.1109/MISE.2009.5069896},
abstract = {This work explores how model-driven engineering techniques can support the configuration of systems in domains presenting multiple variability factors. Video surveillance is a good candidate for which we have an extensive experience. Ultimately, we wish to automatically generate a software component assembly from an application specification, using model to model transformations. The challenge is to cope with variability both at the specification and at the implementation levels. Our approach advocates a clear separation of concerns. More precisely, we propose two feature models, one for task specification and the other for software components. The first model can be transformed into one or several valid component configurations through step-wise specialization. This paper outlines our approach, focusing on the two feature models and their relations. We particularly insist on variability and constraint modeling in order to achieve the mapping from domain variability to software variability through model transformations.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Modeling in Software Engineering},
pages = {44–49},
numpages = {6},
series = {MISE '09}
}

@inproceedings{10.1145/1134650.1134678,
author = {Pandey, Raju and Wu, Jeffrey},
title = {BOTS: a constraint-based component system for synthesizing scalable software systems},
year = {2006},
isbn = {159593362X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1134650.1134678},
doi = {10.1145/1134650.1134678},
abstract = {Embedded application developers create applications for a wide range of devices with different resource constraints. Developers want to maximize the use of the limited resources available on the device while still not exceeding the capabilities of the device. To do this, the developer must be able to scale his software for different platforms. In this paper, we present a software engineering methodology that automatically scales software to different platforms. We intend to have the application developer write high level functional specifications of his software and have tools that automatically scale the underlying runtime. These tools will use the functional and non-functional constraints of both the hardware and client application to produce an appropriate runtime. Our initial results show that the proposed approach can scale operating systems and virtual machines that satisfy the constraints of varying hardware/application combinations.},
booktitle = {Proceedings of the 2006 ACM SIGPLAN/SIGBED Conference on Language, Compilers, and Tool Support for Embedded Systems},
pages = {189–198},
numpages = {10},
keywords = {components, constraints, embedded systems, generative programming, runtime systems, wireless sensor networks},
location = {Ottawa, Ontario, Canada},
series = {LCTES '06}
}

@article{10.1145/3303849,
author = {R\"{o}ger, Henriette and Mayer, Ruben},
title = {A Comprehensive Survey on Parallelization and Elasticity in Stream Processing},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3303849},
doi = {10.1145/3303849},
abstract = {Stream Processing (SP) has evolved as the leading paradigm to process and gain value from the high volume of streaming data produced, e.g., in the domain of the Internet of Things. An SP system is a middleware that deploys a network of operators between data sources, such as sensors, and the consuming applications. SP systems typically face intense and highly dynamic data streams. Parallelization and elasticity enable SP systems to process these streams with continuous high quality of service. The current research landscape provides a broad spectrum of methods for parallelization and elasticity in SP. Each method makes specific assumptions and focuses on particular aspects. However, the literature lacks a comprehensive overview and categorization of the state of the art in SP parallelization and elasticity, which is necessary to consolidate the state of the research and to plan future research directions on this basis. Therefore, in this survey, we study the literature and develop a classification of current methods for both parallelization and elasticity in SP systems.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {36},
numpages = {37},
keywords = {Stream processing, complex event processing, data stream management system, elasticity, parallelization}
}

@inproceedings{10.5555/2663546.2663573,
author = {Fredericks, Erik M. and Ramirez, Andres J. and Cheng, Betty H. C.},
title = {Towards run-time testing of dynamic adaptive systems},
year = {2013},
isbn = {9781467344012},
publisher = {IEEE Press},
abstract = {It is challenging to design, develop, and validate a dynamically adaptive system (DAS) that satisfies requirements, particularly when requirements can change at run time. Testing at design time can help verify and validate that a DAS satisfies its specified requirements and constraints. While offline tests may demonstrate that a DAS is capable of satisfying its requirements before deployment, a DAS may encounter unanticipated system and environmental conditions that can prevent it from achieving its objectives. In working towards a requirements-aware DAS, this paper proposes run-time monitoring and adaptation of tests as another technique for evaluating whether a DAS satisfies, or is even capable of satisfying, its requirements given its current execution context. To this end, this paper motivates the need and identifies challenges for adaptively testing a DAS at run time, as well as suggests possible methods for leveraging offline testing techniques for verifying run-time behavior.},
booktitle = {Proceedings of the 8th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {169–174},
numpages = {6},
location = {San Francisco, California},
series = {SEAMS '13}
}

@inproceedings{10.1145/2430502.2430522,
author = {von Rhein, Alexander and Apel, Sven and K\"{a}stner, Christian and Th\"{u}m, Thomas and Schaefer, Ina},
title = {The PLA model: on the combination of product-line analyses},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430522},
doi = {10.1145/2430502.2430522},
abstract = {Product-line analysis has received considerable attention in the last decade. As it is often infeasible to analyze each product of a product line individually, researchers have developed analyses, called variability-aware analyses, that consider and exploit variability manifested in a code base. Variability-aware analyses are often significantly more efficient than traditional analyses, but each of them has certain weaknesses regarding applicability or scalability. We present the Product-Line-Analysis model, a formal model for the classification and comparison of existing analyses, including traditional and variability-aware analyses, and lay a foundation for formulating and exploring further, combined analyses. As a proof of concept, we discuss different examples of analyses in the light of our model, and demonstrate its benefits for systematic comparison and exploration of product-line analyses.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {14},
numpages = {8},
keywords = {PLA model, product-line analysis, software product lines},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@inproceedings{10.1145/2721956.2721977,
author = {Kajtazovic, Nermin and Preschern, Christopher and H\"{o}ller, Andrea and Kreiner, Christian},
title = {Towards pattern-based reuse in safety-critical systems},
year = {2014},
isbn = {9781450334167},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2721956.2721977},
doi = {10.1145/2721956.2721977},
abstract = {Challenges such as time-to-market, reduced costs for change and maintenance have radically influenced development of today's safety-critical systems. Many domains have already adopted their system's engineering to support modular and component-based architectures. With the component-based design paradigm, the system engineering is utilized allowing to distribute development among different development teams, however, with the price that there is no full trust in independently developed parts, which makes their reuse challenging. Until now, many approaches that address reuse, on conceptual or detailed level, have been proposed. A very important aspect addressed here is to document the information flow between system parts in detail, i.e. from higher abstraction levels down to the implementation details, in order to put more trust into independently developed parts of the system.In this paper, we describe a compact pattern system with the aim to establish a link between high level concepts for reuse and detailed description of the behavior of system parts. The main goal is to document these details up to the higher levels of abstraction in more systematic way.},
booktitle = {Proceedings of the 19th European Conference on Pattern Languages of Programs},
articleno = {33},
numpages = {15},
location = {Irsee, Germany},
series = {EuroPLoP '14}
}

@inproceedings{10.1145/2884781.2884861,
author = {Tan, Tian Huat and Chen, Manman and Sun, Jun and Liu, Yang and Andr\'{e}, \'{E}tienne and Xue, Yinxing and Dong, Jin Song},
title = {Optimizing selection of competing services with probabilistic hierarchical refinement},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884861},
doi = {10.1145/2884781.2884861},
abstract = {Recently, many large enterprises (e.g., Netflix, Amazon) have decomposed their monolithic application into services, and composed them to fulfill their business functionalities. Many hosting services on the cloud, with different Quality of Service (QoS) (e.g., availability, cost), can be used to host the services. This is an example of competing services. QoS is crucial for the satisfaction of users. It is important to choose a set of services that maximize the overall QoS, and satisfy all QoS requirements for the service composition. This problem, known as optimal service selection, is NP-hard. Therefore, an effective method for reducing the search space and guiding the search process is highly desirable. To this end, we introduce a novel technique, called Probabilistic Hierarchical Refinement (ProHR). ProHR effectively reduces the search space by removing competing services that cannot be part of the selection. ProHR provides two methods, probabilistic ranking and hierarchical refinement, that enable smart exploration of the reduced search space. Unlike existing approaches that perform poorly when QoS requirements become stricter, ProHR maintains high performance and accuracy, independent of the strictness of the QoS requirements. ProHR has been evaluated on a publicly available dataset, and has shown significant improvement over existing approaches.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {85–95},
numpages = {11},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/2088876.2088882,
author = {Huynh, Ngoc-Tho and Phung-Khac, An and Segarra, Maria-Teresa},
title = {Towards reliable distributed reconfiguration},
year = {2011},
isbn = {9781450310703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2088876.2088882},
doi = {10.1145/2088876.2088882},
abstract = {In component-based software engineering, reconfiguration often refers to the activity of changing a running software system at the component level. Reconfiguration is widely used for evolving and adapting software systems that can not be shut down for update. However, in distributed systems, supporting reconfiguration is a challenging task since a reconfiguration consists of distributed reconfiguration actions that need to be coordinated. Particularly, this task becomes much more challenging in the context of unstable networks where nodes may disconnect frequently, even during reconfiguration. To address this challenge, we propose a platform supporting distributed reconfiguration that embodies a solution for managing system states at reconfiguration time. We define (1) different system states regarding reconfiguration and (2) ways that the system will act accordingly. When a disconnection is detected during a reconfiguration, the system may correct reconfiguration plans to continue the reconfiguration if possible, or recover if the reconfiguration fails.},
booktitle = {Adaptive and Reflective Middleware on Proceedings of the International Workshop},
pages = {36–41},
numpages = {6},
keywords = {component-based software engineering, distributed reconfiguration, reliability},
location = {Lisbon, Portugal},
series = {ARM '11}
}

@inproceedings{10.1145/1596495.1596500,
author = {Peper, Christian and Schneider, Daniel},
title = {On runtime service quality models in adaptive ad-hoc systems},
year = {2009},
isbn = {9781605586816},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1596495.1596500},
doi = {10.1145/1596495.1596500},
abstract = {Ad-hoc computer systems can automatically realize higher services when at least two distributed and communicating (embedded) devices come together. For this purpose, they must able to manage appearance and loss of devices and resources, and they have to adapt to changes in requirements and environment. Based on a component-oriented approach for adaptive ad-hoc systems, this paper suggests a high-level service quality reference model to advocate further research on the quality matching problem between service provider and client components.},
booktitle = {Proceedings of the 2009 ESEC/FSE Workshop on Software Integration and Evolution @ Runtime},
pages = {11–18},
numpages = {8},
keywords = {ad-hoc systems, adaptivity, ambient intelligence, component-orientation, distributed systems, quality-of-service, ubiquitous computing},
location = {Amsterdam, The Netherlands},
series = {SINTER '09}
}

@inproceedings{10.1145/1944892.1944897,
author = {Gilson, Fabian and Englebert, Vincent},
title = {Towards handling architecture design, variability and evolution with model transformations},
year = {2011},
isbn = {9781450305709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944892.1944897},
doi = {10.1145/1944892.1944897},
abstract = {Software systems have to face evolving requirements from information system stakeholders, infrastructure modifications, and evolving rationales about the implementation. This increases the rate of migration and redeployment of systems. Recent approaches intend to abstract architectural element specifications from the implementing technology and manage software design through model transformations. Based on an Architecture Description Language integrating infrastructure modelling facilities and a requirement modelling language, the present work manages architecturally significant requirements and infrastructure evolutions by model transformations. Our approach offers support for evolution and variability management tasks as it makes explicit the rationales concerning requirements, infrastructure and implementation alternatives that guide both the software architecture and the infrastructure definition.},
booktitle = {Proceedings of the 5th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {39–48},
numpages = {10},
keywords = {architecturally significant requirement, architecture description language, architecture variability, infrastructure constraint, model transformation},
location = {Namur, Belgium},
series = {VaMoS '11}
}

@inproceedings{10.5555/2337223.2337243,
author = {Siegmund, Norbert and Kolesnikov, Sergiy S. and K\"{a}stner, Christian and Apel, Sven and Batory, Don and Rosenm\"{u}ller, Marko and Saake, Gunter},
title = {Predicting performance via automated feature-interaction detection},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Customizable programs and program families provide user-selectable features to allow users to tailor a program to an application scenario. Knowing in advance which feature selection yields the best performance is difficult because a direct measurement of all possible feature combinations is infeasible. Our work aims at predicting program performance based on selected features. However, when features interact, accurate predictions are challenging. An interaction occurs when a particular feature combination has an unexpected influence on performance. We present a method that automatically detects performance-relevant feature interactions to improve prediction accuracy. To this end, we propose three heuristics to reduce the number of measurements required to detect interactions. Our evaluation consists of six real-world case studies from varying domains (e.g., databases, encoding libraries, and web servers) using different configuration techniques (e.g., configuration files and preprocessor flags). Results show an average prediction accuracy of 95%.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {167–177},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1145/3510003.3510094,
author = {He, Haochen and Jia, Zhouyang and Li, Shanshan and Yu, Yue and Zhou, Chenglong and Liao, Qing and Wang, Ji and Liao, Xiangke},
title = {Multi-intention-aware configuration selection for performance tuning},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510094},
doi = {10.1145/3510003.3510094},
abstract = {Automatic configuration tuning helps users who intend to improve software performance. However, the auto-tuners are limited by the huge configuration search space. More importantly, they focus only on performance improvement while being unaware of other important user intentions (e.g., reliability, security). To reduce the search space, researchers mainly focus on pre-selecting performance-related parameters which requires a heavy stage of dynamically running under different configurations to build performance models. Given that other important user intentions are not paid attention to, we focus on guiding users in pre-selecting performance-related parameters in general while warning about side-effects on non-performance intentions. We find that the configuration document often, if it does not always, contains rich information about the parameters' relationship with diverse user intentions, but documents might also be long and domain-specific.In this paper, we first conduct a comprehensive study on 13 representative software containing 7,349 configuration parameters, and derive six types of ways in which configuration parameters may affect non-performance intentions. Guided by this study, we design SafeTune, a multi-intention-aware method that preselects important performance-related parameters and warns about their side-effects on non-performance intentions. Evaluation on target software shows that SafeTune correctly identifies 22--26 performance-related parameters that are missed by state-of-the-art tools but have significant performance impact (up to 14.7x). Furthermore, we illustrate eight representative cases to show that SafeTune can effectively prevent real-world and critical side-effects on other user intentions.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1431–1442},
numpages = {12},
keywords = {non-performance property, performance tuning, user intention},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1109/ASE.2015.45,
author = {Sarkar, Atri and Guo, Jianmei and Siegmund, Norbert and Apel, Sven and Czarnecki, Krzysztof},
title = {Cost-efficient sampling for performance prediction of configurable systems},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.45},
doi = {10.1109/ASE.2015.45},
abstract = {A key challenge of the development and maintenance of configurable systems is to predict the performance of individual system variants based on the features selected. It is usually infeasible to measure the performance of all possible variants, due to feature combinatorics. Previous approaches predict performance based on small samples of measured variants, but it is still open how to dynamically determine an ideal sample that balances prediction accuracy and measurement effort. In this paper, we adapt two widely-used sampling strategies for performance prediction to the domain of configurable systems and evaluate them in terms of sampling cost, which considers prediction accuracy and measurement effort simultaneously. To generate an initial sample, we introduce a new heuristic based on feature frequencies and compare it to a traditional method based on t-way feature coverage. We conduct experiments on six real-world systems and provide guidelines for stakeholders to predict performance by sampling.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {342–352},
numpages = {11},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@proceedings{10.1145/3564719,
title = {GPCE 2022: Proceedings of the 21st ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2022},
isbn = {9781450399203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 21st ACM SIGPLAN International Conference on Generative Programming: Concept &amp; Experiences (GPCE 2022) held on December 6th and 7th, 2022 in Auckland, New Zealand. GPCE is the premiere venue for researchers and practitioners interested in techniques that use program generation, domain-specific languages, and component deployment to increase programmer productivity, improve software quality, and shorten the time-to-market of software products. In addition to exploring cutting-edge techniques of generative software, GPCE seeks to foster cross-fertilization between software engineering and programming language.},
location = {Auckland, New Zealand}
}

@inproceedings{10.1145/2000259.2000263,
author = {Koziolek, Heiko},
title = {Sustainability evaluation of software architectures: a systematic review},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000263},
doi = {10.1145/2000259.2000263},
abstract = {Long-living software systems are sustainable if they can be cost-efficiently maintained and evolved over their entire life-cycle. The quality of software architectures determines sustainability to a large extent. Scenario-based software architecture evaluation methods can support sustainability analysis, but they are still reluctantly used in practice. They are also not integrated with architecture-level metrics when evaluating implemented systems, which limits their capabilities. Existing literature reviews for architecture evaluation focus on scenario-based methods, but do not provide a critical reflection of the applicability of such methods for sustainability evaluation. Our goal is to measure the sustainability of a software architecture both during early design using scenarios and during evolution using scenarios and metrics, which is highly relevant in practice. We thus provide a systematic literature review assessing scenario-based methods for sustainability support and categorize more than 40 architecture-level metrics according to several design principles. Our review identifies a need for further empirical research, for the integration of existing methods, and for the more efficient use of formal architectural models.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {3–12},
numpages = {10},
keywords = {architectural metric, evolution scenario, software architecture, survey, sustainability},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@inproceedings{10.1109/WI-IAT.2009.363,
author = {Li, Mu and Huai, JinPeng and Guo, HuiPeng},
title = {An Adaptive Web Services Selection Method Based on the QoS Prediction Mechanism},
year = {2009},
isbn = {9780769538013},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2009.363},
doi = {10.1109/WI-IAT.2009.363},
abstract = {In recent years, many QoS-based web service selection methods have been proposed. However, as QoS changes dynamically, the atomic services of a composite web service could be replaced with other ones that have better quality. The performance of a composite web service will be decreased if this replacement happens frequently in runtime. Predicting the change of QoS accurately in select phase can effectively reduce this web services “thrash”. In this paper, we propose a web service selection algorithm GFS (Goodness-Fit Selection algorithm) based on QoS prediction mechanism in dynamic environments. We use structural equation to model the QoS measurement of web services. By taking the advantage of the prediction mechanism of structural equation model, we can quantitatively predict the change of quality of service dynamically. Optimal web service is selected based on the predicted results. Simulation results show that in dynamic environments, GFS provides higher selection accuracy than previous selection methods.},
booktitle = {Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology - Volume 01},
pages = {395–402},
numpages = {8},
keywords = {QoS, Structural Equation Modeling, prediction, web service selection},
series = {WI-IAT '09}
}

@inproceedings{10.1145/2480362.2480567,
author = {Durelli, Rafael S. and Santib\'{a}\~{n}ez, Daniel S. M. and Anquetil, Nicolas and Delamaro, M\'{a}rcio E. and de Camargo, Valter Vieira},
title = {A systematic review on mining techniques for crosscutting concerns},
year = {2013},
isbn = {9781450316569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2480362.2480567},
doi = {10.1145/2480362.2480567},
abstract = {&lt;u&gt;Background:&lt;/u&gt; The several maintenance tasks a system is submitted during its life usually cause its architecture deviates from the original conceivable design, ending up with scattered and tangled concerns across the software. The research area named concern mining attempts to identify such scattered and tangled concerns to support maintenance and reverse-engineering. &lt;u&gt;Objectives:&lt;/u&gt; The aim of this paper is threefold: (i) identifying techniques employed in this research area, (ii) extending a taxonomy available on the literature and (iii) recommending an initial combination of some techniques. &lt;u&gt;Results:&lt;/u&gt; We selected 62 papers by their mining technique. Among these papers, we identified 18 mining techniques for crosscutting concern. Based on these techniques, we have extended a taxonomy available in the literature, which can be used to position each new technique, and to compare it with the existing ones along relevant dimensions. As consequence, we present some combinations of these techniques taking into account high values of precision and recall that could improve the identification of both Persistence and Observer concerns. The combination that we recommend may serve as a roadmap to potential users of mining techniques for crosscutting concerns.},
booktitle = {Proceedings of the 28th Annual ACM Symposium on Applied Computing},
pages = {1080–1087},
numpages = {8},
keywords = {aspect mining, concern mining, cross-cutting concerns, systematic review},
location = {Coimbra, Portugal},
series = {SAC '13}
}

@article{10.1145/1366546.1366547,
author = {G\'{e}rard, S\'{e}bastien and Feiler, Peter and Rolland, Jean-Francois and Filali, Mamoun and Reiser, Mark-Oliver and Delanote, Didier and Berbers, Yolande and Pautet, Laurent and Perseil, Isabelle},
title = {UML&amp;AADL '2007 grand challenges},
year = {2007},
issue_date = {October 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/1366546.1366547},
doi = {10.1145/1366546.1366547},
abstract = {On today's sharply competitive industrial market, engineers must focus on their core competencies to produce ever more innovative products, while also reducing development times and costs. This has further heightened the complexity of the development process. At the same time, industrial systems, and specifically real-time embedded systems, have become increasingly software-intensive. New software development approaches and methods must therefore be found to free engineers from the even more complex technical constraints of development and to enable them to concentrate on their core business specialties. One emerging solution is to foster model-based development by defining modeling artifacts well-suited to their domain concerns instead of asking them to write code. However, model-driven approaches will be solutions to the previous issues only if models evolves from a contemplative role to a productive role within the development processes. In this context, model transformation is a key design paradigm that will foster this revolution. This paper is the result of discussions and exchanges that took place within the second edition of the workshop "UML&amp;AADL" (http://www.artist-embedded.org/artist/Topics.html) that-was hold in 2007 in Auckland, New Zealand, in conjunction with the ICECCS07 conference. The purpose of this workshop was to gather people of both communities from UML (including its domain specific extensions, with a focus on MARTE) and AADL (including its annexes) in order to foster sharing of results and experiments. More specially this year, the focus was on how both standards do subscribe to the model driven engineering paradigm, or to be more precise, how MDE may ease and foster the usage of both sets of standards for developing real-time embedded systems. This paper will show that, even if the work is not yet finished, the current results seems to be already very promising.},
journal = {SIGBED Rev.},
month = oct,
articleno = {1},
numpages = {1},
keywords = {AADL, ADL, MARTE, MDA, MDD, MDE, TLA+, UML, embedded, real-time, xUML}
}

@article{10.1145/3571854,
author = {Zampetti, Fiorella and Tamburri, Damian and Panichella, Sebastiano and Panichella, Annibale and Canfora, Gerardo and Di Penta, Massimiliano},
title = {Continuous Integration and Delivery Practices for Cyber-Physical Systems: An Interview-Based Study},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3571854},
doi = {10.1145/3571854},
abstract = {Continuous Integration and Delivery (CI/CD) practices have shown several benefits for software development and operations, such as faster release cycles and early discovery of defects. For Cyber-Physical System (CPS) development, CI/CD can help achieving required goals, such as high dependability, yet it may be challenging to apply. This article empirically investigates challenges, barriers, and their mitigation occurring when applying CI/CD practices to develop CPSs in 10 organizations working in eight different domains. The study has been conducted through semi-structured interviews, by applying an open card sorting procedure together with a member-checking survey within the same organizations, and by validating the results through a further survey involving 55 professional developers. The study reveals several peculiarities in the application of CI/CD to CPSs. These include the need for (i) combining continuous and periodic builds while balancing the use of Hardware-in-the-Loop and simulators, (ii) coping with difficulties in software deployment (iii) accounting for simulators and Hardware-in-the-Loop differing in their behavior, and (vi) combining hardware/software expertise in the development team. Our findings open the road toward recommenders aimed at supporting the setting and evolution of CI/CD pipelines, as well as university curricula requiring interdisciplinarity, such as knowledge about hardware, software, and their interplay.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {73},
numpages = {44},
keywords = {Continuous Integration and Delivery, Cyber-Physical Systems, empirical software engineering}
}

@inproceedings{10.1145/2660190.2660191,
author = {Kolesnikov, Sergiy and Roth, Judith and Apel, Sven},
title = {On the relation between internal and external feature interactions in feature-oriented product lines: a case study},
year = {2014},
isbn = {9781450329804},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660190.2660191},
doi = {10.1145/2660190.2660191},
abstract = {The feature-interaction problem has been explored for many years. Still, we lack sufficient knowledge about the interplay of different kinds of interactions in software product lines. Exploring the relations between different kinds of feature interactions will allow us to learn more about the nature of interactions and their causes. This knowledge can then be applied for improving existing approaches for detecting, managing, and resolving feature interactions. We present a framework for studying relations between different kinds of interactions. Furthermore, we report and discuss the results of a preliminary study in which we examined correlations between internal feature interactions (quantified by a set of software measures) and external feature interactions (represented by product-line-specific type errors). We performed the evaluation on a set of 15 feature-oriented, Java-based product lines. We observed moderate correlations between the interactions under discussion. This gives us confidence that we can apply our approach to studying other types of external feature interactions (e.g., performance interactions).},
booktitle = {Proceedings of the 6th International Workshop on Feature-Oriented Software Development},
pages = {1–8},
numpages = {8},
keywords = {feature interactions, feature-oriented software development, software measures},
location = {V\"{a}ster\r{a}s, Sweden},
series = {FOSD '14}
}

@inproceedings{10.1145/1081706.1081757,
author = {Estublier, Jacky and Vega, German},
title = {Reuse and variability in large software applications},
year = {2005},
isbn = {1595930140},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081706.1081757},
doi = {10.1145/1081706.1081757},
abstract = {Reuse has always been a major goal in software engineering, since it promises large gains in productivity, quality and time to market reduction. Practical experience has shown that substantial reuse has only successfully happened in two cases: libraries, where many generic and small components can be found; and product lines, where domains-specific components can be assembled in different ways to produce variations of a given product.In this paper we examine how product lines have successfully achieved reuse of coarse-grained components, and the underlying factors limiting this approach to narrowly scoped domains. We then build on this insight to present an approach, called software federation, which proposes a mechanism to overcome the identified limitations, and therefore makes reuse of coarse-grained components possible over a larger range of applications. Our approach extends and generalizes the product line approach, extending the concepts and mechanisms available to manage variability. The system is in use in different companies, validating the claims made in this paper.},
booktitle = {Proceedings of the 10th European Software Engineering Conference Held Jointly with 13th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {316–325},
numpages = {10},
keywords = {AOP, COTS, EAI, MDA, interoperability, model driven software engineering, process driven application, product families, product line, reuse, variability, workflow},
location = {Lisbon, Portugal},
series = {ESEC/FSE-13}
}

@proceedings{10.1145/3669940,
title = {ASPLOS '25: Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1},
year = {2025},
isbn = {9798400706981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are delighted to introduce the first volume of the ASPLOS proceedings for 2025. The conference is in its third year of an experiment with a three-deadline structure: authors can submit to any of three separate review cycles handled by a single year-long program committee. This volume includes papers from the first two review cycles, which had submission deadlines in the spring and summer of 2024. We combined the two cycles because submission volumes in the spring cycle were disproportionately small.This volume contains 72 of the 74 papers accepted to ASPLOS 2025 to date. This includes papers accepted in the spring and summer cycles and those invited to submit a revision in the spring cycle that was ultimately accepted. Two of these 74 accepted papers are still undergoing artifact evaluation and will be published in a subsequent volume. The spring and summer review cycles saw a combined 586 submissions. These submissions were reviewed by a 208-person Program Committee augmented by 57 External Review Committee members. On occasion, we solicited a small number of external expert reviews. On the PC, 129 members self-reported they were in an academic role and 77 self-reported they were in an industrial role. On the ERC it was 43 and 13 respectively. The median PhD year of the combined committees was 2014. In addition to these committees, we engaged ten vice chairs, experienced and trusted reviewers who helped us monitor the review process for each paper.These committees reviewed all of the submissions that were not desk rejected (11 papers) or withdrawn (4 papers). In keeping with recent norms, the technical review happened in two phases. Each paper received three reviews in the first round, with, in most cases, two additional reviews in the second round for the 54% of submissions that advanced. To assign reviews, we used the Toronto Paper Matching System (TPMS) to provide a preliminary review assignment that matched reviewer expertise. We then manually inspected and adjusted these assignments as needed: for example, to correct errors in TPMS's topic modeling or adjust to late-discovered conflicts. In addition, each paper was assigned a non-conflicted chair and a non-conflicted vice chair to provide two extra sets of eyes to monitor and facilitate the process. Due to the size and distribution of the PC, which spanned 14 time zones, the PC did not meet synchronously. Instead, each paper was discussed by the reviewers via comments in the HotCRP system. Ultimately, the discussion for each paper reached one of three outcomes: rejection, conditional acceptance, or major revision. All conditionally accepted papers were shepherded. Major revision papers were invited to revise and resubmit their paper for a second round of review by a subset of the original reviewers. All authors of papers that advanced to the second round of review were given the opportunity to see and respond to their reviewer questions prior to the reviewer discussion.},
location = {Rotterdam, Netherlands}
}

@inproceedings{10.1145/3319008.3319015,
author = {Fu, Changlan and Zhang, He and Huang, Xin and Zhou, Xin and Li, Zhi},
title = {A Review of Meta-ethnographies in Software Engineering},
year = {2019},
isbn = {9781450371452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319008.3319015},
doi = {10.1145/3319008.3319015},
abstract = {Context: Data synthesis is one of the most significant tasks in Systematic Literature Review (SLR). Software Engineering (SE) researchers have adopted a variety of methods of synthesizing data that originated in other disciplines. One of the qualitative data synthesis methods is meta-ethnography, which is being used in SE SLRs. Objective: We aim at studying the adoption of meta-ethnography in SE SLRs in order to understand how this method has been used in SE. Method: We conducted a tertiary study of the use of meta-ethnography by reviewing sixteen SLRs. We carried out an empirical inquiry by integrating SLR and confirmatory email survey. Results: There is a general lack of knowledge, or even awareness, of different aspects of meta-ethnography and/or how to apply it. Conclusion: There is a need of investment in gaining in-depth knowledge and skills of correctly applying meta-ethnography in order to increase the quality and reliability of the findings generated from SE SLRs. Our study reveals that meta-ethnography is a suitable method to SE research. We discuss challenges and propose recommendations of adopting meta-ethnography in SE. Our effort also offers a preliminary checklist of the systematic considerations for doing meta-ethnography in SE and improving the quality of meta-ethnographic research in SE.},
booktitle = {Proceedings of the 23rd International Conference on Evaluation and Assessment in Software Engineering},
pages = {68–77},
numpages = {10},
keywords = {meta-ethnography, qualitative research synthesis, systematic (literature) review},
location = {Copenhagen, Denmark},
series = {EASE '19}
}

@inproceedings{10.1145/2509136.2509522,
author = {Bhattacharya, Suparna and Gopinath, Kanchi and Nanda, Mangala Gowri},
title = {Combining concern input with program analysis for bloat detection},
year = {2013},
isbn = {9781450323741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509136.2509522},
doi = {10.1145/2509136.2509522},
abstract = {Framework based software tends to get bloated by accumulating optional features (or concerns) just-in-case they are needed. The good news is that such feature bloat need not always cause runtime execution bloat. The bad news is that often enough, only a few statements from an optional concern may cause execution bloat that may result in as much as 50% runtime overhead.We present a novel technique to analyze the connection between optional concerns and the potential sources of execution bloat induced by them. Our analysis automatically answers questions such as (1) whether a given set of optional concerns could lead to execution bloat and (2) which particular statements are the likely sources of bloat when those concerns are not required. The technique combines coarse grain concern input from an external source with a fine-grained static analysis. Our experimental evaluation highlights the effectiveness of such concern augmented program analysis in execution bloat assessment of ten programs.},
booktitle = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages &amp; Applications},
pages = {745–764},
numpages = {20},
keywords = {feature oriented programming, program concerns, software bloat},
location = {Indianapolis, Indiana, USA},
series = {OOPSLA '13}
}

@proceedings{10.1145/3629264,
title = {ICCDA '23: Proceedings of the 2023 7th International Conference on Computing and Data Analysis},
year = {2023},
isbn = {9798400700576},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guiyang, China}
}

@proceedings{10.1145/3639478,
title = {ICSE-Companion '24: Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ICSE is the leading and, by far, the largest conference in Software Engineering, attracting researchers, practitioners, and students worldwide. ICSE2024 is co-located with 11 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3653644,
title = {FAIML '24: Proceedings of the 2024 3rd International Conference on Frontiers of Artificial Intelligence and Machine Learning},
year = {2024},
isbn = {9798400709777},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yichang, China}
}

@inproceedings{10.1145/974044.974052,
author = {Grassi, Vincenzo and Mirandola, Raffaela},
title = {Towards automatic compositional performance analysis of component-based systems},
year = {2004},
isbn = {1581136730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/974044.974052},
doi = {10.1145/974044.974052},
abstract = {To make predictive analysis an effective tool for component-based software development (CBSD), it should be, as much as possible: compositional, to allow the re-use of known information about the properties of existing components, and automatic, to keep the pace with the timeliness and cost-effectiveness promises of CBSD. Towards this end, focusing on the predictive analysis of performance properties, we define a simple language, based on an abstract component model, to describe a component assembly, outlining which information should be included in it to support compositional performance analysis. Moreover, we outline a mapping of the constructs of the proposed language to elements of the RT-UML Profile, to give them a precisely defined "performance semantics", and to get a starting point for the exploitation of proposed UML-based methodologies and algorithms for performance analysis.},
booktitle = {Proceedings of the 4th International Workshop on Software and Performance},
pages = {59–63},
numpages = {5},
keywords = {component specification, performance, predictive analysis, software component},
location = {Redwood Shores, California},
series = {WOSP '04}
}

@inproceedings{10.1145/2095536.2095554,
author = {Siala, Fatma and Lajmi, Soufiene and Ghedira, Khaled},
title = {Multi-agent selection of multiple composite web services based on CBR method and driven by QoS},
year = {2011},
isbn = {9781450307840},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2095536.2095554},
doi = {10.1145/2095536.2095554},
abstract = {Many companies aim to use Web services to integrate heterogeneous or remote applications in SOA (Service Oriented Architecture) contexts. Indeed, one of the main assets of service-orientation is a composition to develop higher level services, so-called composite services, by re-using existing services. Since many available Web services provide overlapping or identical functionality, with different Quality of Service (QoS), a choice needs to be made to determine which services are to participate in a given composite service. However, for a composition, we can have different combinations and execution paths. Particularly, a composite service can generate different schemes that give various QoS scores.This paper presents a framework which deals with the selection of composite Web services on the base of Multi-Agents negotiation. The objective of these agents is to find out the best Composite QoS (CQoS) based on Web services availability. This scalable framework supports different combinations and execution paths using CBR technique. The proposed Multi-Agents framework is compared to an existing approach in terms of execution time. Experiments have demonstrated that our framework provide reliable results in comparison with the existing approach.},
booktitle = {Proceedings of the 13th International Conference on Information Integration and Web-Based Applications and Services},
pages = {90–97},
numpages = {8},
keywords = {CBR technique, QoS, composition, contract-net protocol, execution paths, multi-agent system, web service},
location = {Ho Chi Minh City, Vietnam},
series = {iiWAS '11}
}

@inproceedings{10.1145/1173706.1173740,
author = {Leavens, Gary T. and Abrial, Jean-Raymond and Batory, Don and Butler, Michael and Coglio, Alessandro and Fisler, Kathi and Hehner, Eric and Jones, Cliff and Miller, Dale and Peyton-Jones, Simon and Sitaraman, Murali and Smith, Douglas R. and Stump, Aaron},
title = {Roadmap for enhanced languages and methods to aid verification},
year = {2006},
isbn = {1595932372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1173706.1173740},
doi = {10.1145/1173706.1173740},
abstract = {This roadmap describes ways that researchers in four areas---specification languages, program generation, correctness by construction, and programming languages---might help further the goal of verified software. It also describes what advances the "verified software" grand challenge might anticipate or demand from work in these areas. That is, the roadmap is intended to help foster collaboration between the grand challenge and these research areas.A common goal for research in these areas is to establish language designs and tool architectures that would allow multiple annotations and tools to be used on a single program. In the long term, researchers could try to unify these annotations and integrate such tools.},
booktitle = {Proceedings of the 5th International Conference on Generative Programming and Component Engineering},
pages = {221–236},
numpages = {16},
keywords = {annotations, correctness by construction, program generation, programming languages, specification languages, tools, verification, verified software grand challenge},
location = {Portland, Oregon, USA},
series = {GPCE '06}
}

@inproceedings{10.1145/2786805.2786845,
author = {Siegmund, Norbert and Grebhahn, Alexander and Apel, Sven and K\"{a}stner, Christian},
title = {Performance-influence models for highly configurable systems},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786845},
doi = {10.1145/2786805.2786845},
abstract = {Almost every complex software system today is configurable. While configurability has many benefits, it challenges performance prediction, optimization, and debugging. Often, the influences of individual configuration options on performance are unknown. Worse, configuration options may interact, giving rise to a configuration space of possibly exponential size. Addressing this challenge, we propose an approach that derives a performance-influence model for a given configurable system, describing all relevant influences of configuration options and their interactions. Our approach combines machine-learning and sampling heuristics in a novel way. It improves over standard techniques in that it (1) represents influences of options and their interactions explicitly (which eases debugging), (2) smoothly integrates binary and numeric configuration options for the first time, (3) incorporates domain knowledge, if available (which eases learning and increases accuracy), (4) considers complex constraints among options, and (5) systematically reduces the solution space to a tractable size. A series of experiments demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {284–294},
numpages = {11},
keywords = {Performance-influence models, machine learning, sampling},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/2668930.2688051,
author = {Hork\'{y}, Vojt\v{e}ch and Libi\v{c}, Peter and Marek, Luk\'{a}\v{s} and Steinhauser, Antonin and T\r{u}ma, Petr},
title = {Utilizing Performance Unit Tests To Increase Performance Awareness},
year = {2015},
isbn = {9781450332484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668930.2688051},
doi = {10.1145/2668930.2688051},
abstract = {Many decisions taken during software development impact the resulting application performance. The key decisions whose potential impact is large are usually carefully weighed. In contrast, the same care is not used for many decisions whose individual impact is likely to be small -- simply because the costs would outweigh the benefits. Developer opinion is the common deciding factor for these cases, and our goal is to provide the developer with information that would help form such opinion, thus preventing performance loss due to the accumulated effect of many poor decisions.Our method turns performance unit tests into recipes for generating performance documentation. When the developer selects an interface and workload of interest, relevant performance documentation is generated interactively. This increases performance awareness -- with performance information available alongside standard interface documentation, developers should find it easier to take informed decisions even in situations where expensive performance evaluation is not practical. We demonstrate the method on multiple examples, which show how equipping code with performance unit tests works.},
booktitle = {Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
pages = {289–300},
numpages = {12},
keywords = {java, javadoc, performance awareness, performance documentation, performance testing},
location = {Austin, Texas, USA},
series = {ICPE '15}
}

@proceedings{10.1145/3540250,
title = {ESEC/FSE 2022: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of all members of the organizing committee, we are delighted to welcome everyone to the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2022. The event continues the long, distinguished ESEC/FSE tradition of presenting the most innovative research, and facilitating interactions between scientists and engineers who are passionate about advancing the theory and practice of software engineering.},
location = {Singapore, Singapore}
}

@proceedings{10.1145/3561212,
title = {AM '22: Proceedings of the 17th International Audio Mostly Conference},
year = {2022},
isbn = {9781450397018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {St. P\"{o}lten, Austria}
}

@proceedings{10.1145/3689031,
title = {EuroSys '25: Proceedings of the Twentieth European Conference on Computer Systems},
year = {2025},
isbn = {9798400711961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are delighted to welcome you to EuroSys 2025, the 20th edition of the European Conference on Computer Systems! We are excited to host EuroSys 2025 in the modern and dynamic city of Rotterdam, Netherlands. This year's EuroSys is very special as it is co-located (for the first time) with the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 2025). We hope you will enjoy an excellent technical program, engaging discussions, and networking opportunities in this vibrant city known for its innovative architecture, bustling port, and rich cultural scene.},
location = {Rotterdam, Netherlands}
}

@proceedings{10.1145/3603166,
title = {UCC '23: Proceedings of the IEEE/ACM 16th International Conference on Utility and Cloud Computing},
year = {2023},
isbn = {9798400702341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The IEEE/ACM International Conference on Utility and Cloud Computing (UCC) is a premier annual conference series aiming to provide a platform for researchers from both academia and industry to present new discoveries in the broad area of Cloud and Edge utility computing and applications.},
location = {Taormina (Messina), Italy}
}

@proceedings{10.1145/3567512,
title = {SLE 2022: Proceedings of the 15th ACM SIGPLAN International Conference on Software Language Engineering},
year = {2022},
isbn = {9781450399197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 15th ACM SIGPLAN International Conference on Software Language Engineering (SLE), co-located with the ACM SIGPLAN conference on Systems, Programming, Languages, and Applications (SPLASH) in Auckland, a vibrant port city in northern New Zealand, from December 5th to December 10th 2022. Like its predecessors, the this edition of the SLE conference, SLE 2022, is devoted to the principles of software languages: their design, their implementation, and their evolution. As such, SLE brings together researchers united by their common interest in the creation, capture, and tooling of software languages.},
location = {Auckland, New Zealand}
}

@proceedings{10.1145/3555776,
title = {SAC '23: Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tallinn, Estonia}
}

@article{10.1145/270849.270854,
author = {Edwards, Stephen H. and Weide, Bruce W.},
title = {WISR8: 8th annual workshop on software reuse: summary and working group reports},
year = {1997},
issue_date = {Sept. 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/270849.270854},
doi = {10.1145/270849.270854},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {17–32},
numpages = {16}
}

@proceedings{10.1145/3652583,
title = {ICMR '24: Proceedings of the 2024 International Conference on Multimedia Retrieval},
year = {2024},
isbn = {9798400706196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to present the 2024 edition of the ACM International Conference on Multimedia Retrieval, ACM ICMR 2024, that took place from 10-14 June 2024, in Phuket, Thailand.Effectively and efficiently retrieving information from multimedia collections (e.g., text, image, video, audio, sensor data, 3D) based on user needs is one of the most exciting areas in multimedia research. The Annual ACM International Conference on Multimedia Retrieval (ICMR) offers a great opportunity for exchanging leading-edge multimedia retrieval ideas among researchers, practitioners, and other potential users of multimedia retrieval systems. ACM ICMR was created in 2011 in a merger of ACM CIVR (International Conference on Image and Video Retrieval) and ACM MIR (International Conference on Multimedia Information Retrieval). ACM ICMR serves to illuminate the state of the art in multimedia retrieval. ACM ICMR 2024 in Phuket follows the successful previous editions of ICMR in Trento, Italy 2011; Hong Kong, China 2012; Dallas, USA 2013; Glasgow, UK 2014; Shanghai, China 2015; New York, USA 2016; Bucharest, Romania 2017; Yokohama, Japan 2018; Ottawa, Canada 2019; Dublin, Ireland 2020 (online); Taipei, Taiwan 2021 (online); Newark, USA 2022 (hybrid); and Thessaloniki, Greece 2023 (hybrid).},
location = {Phuket, Thailand}
}

@article{10.1145/979743.979745,
author = {ACM SIGSOFT Software Engineering Notes staff},
title = {Back matter (abstracts and calendar)},
year = {2004},
issue_date = {March 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/979743.979745},
doi = {10.1145/979743.979745},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {27–62},
numpages = {36}
}

@proceedings{10.1145/3664647,
title = {MM '24: Proceedings of the 32nd ACM International Conference on Multimedia},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are delighted to welcome you to Melbourne, Australia for ACM Multimedia 2024, the 32nd ACM International Conference on Multimedia. ACM Multimedia is the premier international conference series in the area of multimedia within the field of computer science. Since 1993, ACM Multimedia has been bringing together worldwide researchers and practitioners from academia and industry to present their innovative research and to discuss recent advancements in multimedia.For the first time since the end of the COVID-19 pandemic, this year's conference returns to the Asia-Pacific region and resumes as a full-fledged, inperson event. With no travel restrictions or significant visa challenges, we are excited to once again experience the warmth of face-to-face gatherings, where we can reconnect with colleagues and friends.The enthusiasm and support from the community have been incredible. ACM Multimedia 2024 received over 4,300 main conference submissions, accepting more than 1,100 papers (please refer to the TPC Chairs' message for details). In addition, 10 Grand Challenges were selected from 22 submissions, 18 workshops from 30 submissions, and 8 tutorials from 13 proposals. We've prepared an exciting five-day program: workshops, grand challenges, and tutorials will be held on the 1st and 5th days, with the main conference occupying the middle three days. All accepted papers will be accessible online prior to the conference, and we are working to ensure proceedings are available through the ACM Digital Library around the conference period.This year's conference features three distinguished academic keynote speeches, several prestigious SIGMM award talks, a panel discussion on Generative AI in Multimedia, a refreshed Brave New Idea (BNI) session, and our inaugural industry program.The opening keynote will be delivered by Prof. Pascale Fung from HKUST, a Fellow of AAAI, ACL, and IEEE. Her talk will explore the pressing topic of Agents in the Large Language Model (LLM) Era. Prof. Judy Kay from the University of Sydney, a renowned expert in HCI, user modeling, and ubiquitous computing, will give the second keynote on how to empower individuals to harness and control their multimodal data. The final academic keynote will be presented by Prof. Jiebo Luo from the University of Rochester, a Fellow of ACM, AAAI, IEEE, SPIE, and IAPR, as well as a member of Academia Europaea and the US National Academy of Inventors. He will discuss leveraging LLMs as social multimedia analysis engines.This year, we continue using OpenReview to ensure an open and transparent review process. Thanks to the exceptional efforts of the technical program committee, every paper received at least three reviews before the review announcement. The BNI track has also revamped its review process to align with the main conference, promoting visionary papers. Additionally, we are excited to introduce the industry program to ACM Multimedia for the first time, featuring industry keynote speeches, expert talks, and demonstrations (please refer to the industry chairs' message for further details).We are also committed to making the conference inclusive and accessible. To support students with financial constraints, we have awarded travel grants to at least 25 students from the ACM Multimedia 2024 budget, with an additional 20+ students receiving SIGMM travel grants. Over 20 local students have also been recruited as volunteers, benefiting from complimentary registration. Furthermore, we have arranged childcare facilities to accommodate attendees with young children. A welcome reception will take place on the 2nd day of the conference, followed by a gala dinner on the 3rd day, featuring exciting cultural performances.We hope you find this year's program engaging and thought-provoking and that it offers valuable opportunities to exchange ideas with fellow researchers and practitioners from around the globe. We also encourage you to take time to explore the beautiful city of Melbourne and its surrounding regions.},
location = {Melbourne VIC, Australia}
}

@proceedings{10.1145/3570361,
title = {ACM MobiCom '23: Proceedings of the 29th Annual International Conference on Mobile Computing and Networking},
year = {2023},
isbn = {9781450399906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Madrid, Spain}
}

@proceedings{10.1145/3613424,
title = {MICRO '23: Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Toronto, ON, Canada}
}

@proceedings{10.1145/3626246,
title = {SIGMOD '24: Companion of the 2024 International Conference on Management of Data},
year = {2024},
isbn = {9798400704222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of the SIGMOD 2024 organizing committee, it is our distinct honor, as General Chairs, to welcome you to the 2024 ACM International Conference on Management of Data - SIGMOD 2024. We are thrilled to be hosting this prestigious event for the very first time in Latin America, and specifically in Santiago de Chile, a recognized leader in data technology within the region. This marks a significant milestone for the SIGMOD community, and we are honored to have you join us for a fully in-person experience in this vibrant and innovative city.},
location = {Santiago AA, Chile}
}

@inproceedings{10.1145/3626246.3653378,
author = {Pavlenko, Anna and Cahoon, Joyce and Zhu, Yiwen and Kroth, Brian and Nelson, Michael and Carter, Andrew and Liao, David and Wright, Travis and Camacho-Rodr\'{\i}guez, Jes\'{u}s and Saur, Karla},
title = {Vertically Autoscaling Monolithic Applications with CaaSPER: Scalable Container-as-a-Service Performance Enhanced Resizing Algorithm for the Cloud},
year = {2024},
isbn = {9798400704222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626246.3653378},
doi = {10.1145/3626246.3653378},
abstract = {Kubernetes has emerged as a prominent open-source platform for managing cloud applications, including stateful databases. These monolithic applications rely on vertical scaling, adjusting CPU cores based on load fluctuations. However, our analysis of Kubernetes-based Database-as-a-Service (DBaaS) offerings at Microsoft revealed that many customers consistently over-provision resources for peak workloads, neglecting cost-saving opportunities through resource scale-down. We found that there is a gap in the ability of existing vertical autoscaling tools to minimize resource slack and respond promptly to throttling, leading to increased costs and impacting crucial metrics such as throughput and availability.To address this challenge, we propose CaaSPER, a vertical autoscaling algorithm that blends reactive and proactive strategies. By dynamically adjusting CPU resources, CaaSPER minimizes resource slack, maintains optimal CPU utilization, and reduces throttling. Importantly, customers have the flexibility to prioritize either cost savings or high performance based on their preferences. Extensive testing demonstrates that CaaSPER effectively reduces throttling and keeps CPU utilization within target levels. CaaSPER is designed to be application-agnostic and platform-agnostic, with potential for extension to other applications requiring vertical autoscaling.},
booktitle = {Companion of the 2024 International Conference on Management of Data},
pages = {241–254},
numpages = {14},
keywords = {containers, kubernetes, resource optimization, vertical auto-scaling},
location = {Santiago AA, Chile},
series = {SIGMOD '24}
}

@proceedings{10.1145/3643991,
title = {MSR '24: Proceedings of the 21st International Conference on Mining Software Repositories},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {MSR is a thriving research community that organizes a yearly conference with a solid reputation amongst software engineering researchers.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3600006,
title = {SOSP '23: Proceedings of the 29th Symposium on Operating Systems Principles},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the Proceedings of the 29th ACM Symposium on Operating Systems Principles (SOSP 2023). This year's program includes 43 papers that reflect today's broad range of topics that comprise modern computer systems research. The program committee carefully reviewed submitted papers and worked closely with the authors of selected papers to produce the collection of high-quality, readable papers presented here. We hope that you enjoy the program!},
location = {Koblenz, Germany}
}

@proceedings{10.1145/3597503,
title = {ICSE '24: Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/336512.336546,
author = {Lamsweerde, Axel van},
title = {Formal specification: a roadmap},
year = {2000},
isbn = {1581132530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/336512.336546},
doi = {10.1145/336512.336546},
booktitle = {Proceedings of the Conference on The Future of Software Engineering},
pages = {147–159},
numpages = {13},
location = {Limerick, Ireland},
series = {ICSE '00}
}

@proceedings{10.1145/3611643,
title = {ESEC/FSE 2023: Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to ESEC/FSE 2023, the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ESEC/FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. ESEC/FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {San Francisco, CA, USA}
}

@proceedings{10.5555/3606010,
title = {ICSE '23: Proceedings of the 45th International Conference on Software Engineering},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Victoria, Australia}
}

@proceedings{10.1145/3652628,
title = {ICAICE '23: Proceedings of the 4th International Conference on Artificial Intelligence and Computer Engineering},
year = {2023},
isbn = {9798400708831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Dalian, China}
}

@proceedings{10.1145/3650212,
title = {ISSTA 2024: Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 33rd edition of the International Symposium on Software Testing and Analysis, ISSTA 2024, held on September 16--20, 2024 in Vienna, Austria. ISSTA 2024 is co-located with ECOOP and MPLR 2024. ISSTA brings together academics, industrial researchers, and practitioners from all over the world working on testing and analyzing software systems.},
location = {Vienna, Austria}
}

@proceedings{10.5555/3643142,
title = {WSC '23: Proceedings of the Winter Simulation Conference},
year = {2023},
isbn = {9798350369663},
publisher = {IEEE Press},
location = {San Antonio, Texas, USA}
}

@proceedings{10.1145/2951913,
title = {ICFP 2016: Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming},
year = {2016},
isbn = {9781450342193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nara, Japan}
}

@proceedings{10.1145/3551349,
title = {ASE '22: Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
year = {2022},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rochester, MI, USA}
}

@proceedings{10.1145/3626252,
title = {SIGCSE 2024: Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 55th annual SIGCSE Technical Symposium on Computer Science Education (SIGCSE TS 2024)! This year, we have returned to Portland, Oregon. We hope that, like us, you are looking forward to a highly productive and engaging symposium that provides ample opportunity to renew old relationships, build new connections, and learn about the latest advances in our field. While we are sure that there will be a few surprises along the way, we hope and expect that we won't experience anything nearly as disruptive as the opening days of the pandemic, which occurred when we last tried to gather here in 2020.Our theme for this year's symposium is "Blazing New Trails in CS Education." This broad theme captures the exceptional work being performed by this community to enhance our teaching, improve our assessments, attract diverse students, and all of the other laudable projects, initiatives, and undertakings that affect positive change. The breadth of the program is substantial - there truly should be something for everyone. In fact, your biggest challenge may be deciding which session to attend in each time slot because there is so much going on! We know that many of you want to attend as many sessions as possible while you are here in Portland, but we encourage you to also find a little bit of time for yourself so that you leave Portland refreshed, renewed and encouraged, rather than exhausted or burnt out.},
location = {Portland, OR, USA}
}

@proceedings{10.1145/3544549,
title = {CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hamburg, Germany}
}

@proceedings{10.1145/3544548,
title = {CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hamburg, Germany}
}

@article{10.1145/279437.279460,
author = {Thompson, Craig},
title = {Workshop on compositional software architectures: workshop report},
year = {1998},
issue_date = {May 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/279437.279460},
doi = {10.1145/279437.279460},
journal = {SIGSOFT Softw. Eng. Notes},
month = may,
pages = {44–63},
numpages = {20}
}

@proceedings{10.1145/3308558,
title = {WWW '19: The World Wide Web Conference},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to The Web Conference 2019. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, CA, USA}
}

@proceedings{10.1145/3508352,
title = {ICCAD '22: Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design},
year = {2022},
isbn = {9781450392174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Jointly sponsored by ACM and IEEE, ICCAD is the premier forum to explore new challenges, present leading-edge innovative solutions, and identify emerging technologies in the Electronic Design Automation (EDA) research areas. ICCAD covers the full range of Computer-Aided Design (CAD) topics - from device and circuit-level up through system-level, as well as post-CMOS design.},
location = {San Diego, California}
}

@proceedings{10.1145/2950290,
title = {FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@proceedings{10.1145/3672758,
title = {CAICE '24: Proceedings of the 3rd International Conference on Computer, Artificial Intelligence and Control Engineering},
year = {2024},
isbn = {9798400716942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xi' an, China}
}

@proceedings{10.1145/3711129,
title = {EITCE '24: Proceedings of the 2024 8th International Conference on Electronic Information Technology and Computer Engineering},
year = {2024},
isbn = {9798400710094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

