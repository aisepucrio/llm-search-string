@inproceedings{10.1145/3663529.3663831,
author = {Stradowski, Szymon and Madeyski, Lech},
title = {Costs and Benefits of Machine Learning Software Defect Prediction: Industrial Case Study},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663831},
doi = {10.1145/3663529.3663831},
abstract = {Context: Our research is set in the industrial context of Nokia 5G and the introduction of Machine Learning Software Defect Prediction (ML SDP) to the existing quality assurance process within the company. Objective: We aim to support or undermine the profitability of the proposed ML SDP solution designed to complement the system-level black-box testing at Nokia, as cost-effectiveness is the main success criterion for further feasibility studies leading to a potential commercial introduction. Method: To evaluate the expected cost-effectiveness, we utilize one of the available cost models for software defect prediction formulated by previous studies on the subject. Second, we calculate the standard Return on Investment (ROI) and Benefit-Cost Ratio (BCR) financial ratios to demonstrate the profitability of the developed approach based on real-world, business-driven examples. Third, we build an MS Excel-based tool to automate the evaluation of similar scenarios that other researchers and practitioners can use. Results: We considered different periods of operation and varying efficiency of predictions, depending on which of the two proposed scenarios were selected (lightweight or advanced). Performed ROI and BCR calculations have shown that the implemented ML SDP can have a positive monetary impact and be cost-effective in both scenarios. Conclusions: The cost of adopting new technology is rarely analyzed and discussed in the existing scientific literature, while it is vital for many software companies worldwide. Accordingly, we bridge emerging technology (machine learning software defect prediction) with a software engineering domain (5G system-level testing) and business considerations (cost efficiency) in an industrial environment of one of the leaders in 5G wireless technology.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {92–103},
numpages = {12},
keywords = {case study, cost-benefit analysis, industry, machine learning, software defect prediction, software testing},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/3590837.3590918,
author = {Gautam, Shikha and Khunteta, Ajay and Ghosh, Debolina},
title = {A Review on Software Defect Prediction Using Machine Learning},
year = {2023},
isbn = {9781450399937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3590837.3590918},
doi = {10.1145/3590837.3590918},
abstract = {Software plays an important role in many of the systems and devices that make up our modern societies. In order to provide their customers with software of a higher quality in a shorter amount of time, numerous software companies are developing software systems of varying sizes for various purposes. It is too challenging to produce high-quality software in a shorter amount of time due to the constraints of software development and the growing size of software data. Therefore, prior to delivering the software product, defect prediction can significantly contribute to a project's success in terms of; cost and quality to evaluate the quality of their software. The goal of the literature review is to investigate about the current trends of software defect prediction approaches. Conclusion of the literature review introduce that many machine learning algorithms are implemented named with Random forest, Logistic regression, Na\"{\i}ve Bayes and Artificial neutral Network etc. with different software metrics like CK metrics, Source code metric etc. The performance measurement of the model done by various methods like accuracy, precision etc.},
booktitle = {Proceedings of the 4th International Conference on Information Management &amp; Machine Intelligence},
articleno = {81},
numpages = {10},
keywords = {Datasets, Machine Learning, Software Defect Prediction, Software Metrics, Statement Level},
location = {Jaipur, India},
series = {ICIMMI '22}
}

@inproceedings{10.1109/ASE56229.2023.00026,
author = {Stradowski, Szymon and Madeyski, Lech},
title = {Bridging the Gap between Academia and Industry in Machine Learning Software Defect Prediction: Thirteen Considerations},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00026},
doi = {10.1109/ASE56229.2023.00026},
abstract = {This experience paper describes thirteen considerations for implementing machine learning software defect prediction (ML SDP) in vivo. Specifically, we provide the following report on the ground of the most important observations and lessons learned gathered during a large-scale research effort and introduction of ML SDP to the system-level testing quality assurance process of one of the leading telecommunication vendors in the world --- Nokia. We adhere to a holistic and logical progression based on the principles of the business analysis body of knowledge: from identifying the need and setting requirements, through designing and implementing the solution, to profitability analysis, stakeholder management, and handover. Conversely, for many years, industry adoption has not kept up the pace of academic achievements in the field, despite promising potential to improve quality and decrease the cost of software products for many companies worldwide. Therefore, discussed considerations hopefully help researchers and practitioners bridge the gaps between academia and industry.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1098–1110},
numpages = {13},
keywords = {machine learning, software defect prediction, nokia 5G, industry introduction, experience paper},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@article{10.1145/3640331,
author = {Chowdhury, Shaiful and Uddin, Gias and Hemmati, Hadi and Holmes, Reid},
title = {Method-level Bug Prediction: Problems and Promises},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3640331},
doi = {10.1145/3640331},
abstract = {Fixing software bugs can be colossally expensive, especially if they are discovered in the later phases of the software development life cycle. As such, bug prediction has been a classic problem for the research community. As of now, the Google Scholar site generates ∼113,000 hits if searched with the “bug prediction” phrase. Despite this staggering effort by the research community, bug prediction research is criticized for not being decisively adopted in practice. A significant problem of the existing research is the granularity level (i.e., class/file level) at which bug prediction is historically studied. Practitioners find it difficult and time-consuming to locate bugs at the class/file level granularity. Consequently, method-level bug prediction has become popular in the past decade. We ask, are these method-level bug prediction models ready for industry use? Unfortunately, the answer is no. The reported high accuracies of these models dwindle significantly if we evaluate them in different realistic time-sensitive contexts. It may seem hopeless at first, but, encouragingly, we show that future method-level bug prediction can be improved significantly. In general, we show how to reliably evaluate future method-level bug prediction models and how to improve them by focusing on four different improvement avenues: building noise-free bug data, addressing concept drift, selecting similar training projects, and developing a mixture of models. Our findings are based on three publicly available method-level bug datasets and a newly built bug dataset of 774,051 Java methods originating from 49 open-source software projects.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {98},
numpages = {31},
keywords = {Method-level bug prediction, code metrics, maintenance, McCabe, code complexity}
}

@inproceedings{10.1145/3643991.3644920,
author = {Zhao, Guoliang and Georgiou, Stefanos and Hassan, Safwat and Zou, Ying and Truong, Derek and Corbin, Toby},
title = {Enhancing Performance Bug Prediction Using Performance Code Metrics},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644920},
doi = {10.1145/3643991.3644920},
abstract = {Performance bugs are non-functional defects that can significantly reduce the performance of an application (e.g., software hanging or freezing) and lead to poor user experience. Prior studies found that each type of performance bugs follows a unique code-based performance anti-pattern and proposed different approaches to detect such anti-patterns by analyzing the source code of a program. However, each approach can only recognize one performance anti-pattern. Different approaches need to be applied separately to identify different performance anti-patterns. To predict a large variety of performance bug types using a unified approach, we propose an approach that predicts performance bugs by leveraging various historical data (e.g., source code and code change history). We collect performance bugs from 80 popular Java projects. Next, we propose performance code metrics to capture the code characteristics of performance bugs. We build performance bug predictors using machine learning models, such as Random Forest, eXtreme Gradient Boosting, and Linear Regressions. We observe that: (1) Random Forest and eXtreme Gradient Boosting are the best algorithms for predicting performance bugs at a file level with a median of 0.84 AUC, 0.21 PR-AUC, and 0.38 MCC; (2) The proposed performance code metrics have the most significant impact on the performance of our models compared to code and process metrics. In particular, the median AUC, PR-AUC, and MCC of the studied machine learning models drop by 7.7%, 25.4%, and 20.2% without using the proposed performance code metrics; and (3) Our approach can predict additional performance bugs that are not covered by the anti-patterns proposed in the prior studies.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {50–62},
numpages = {13},
keywords = {performance bugs, performance anti-patterns, performance code metrics, performance bug prediction},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3475716.3475790,
author = {Wang, Song and Wang, Junjie and Nam, Jaechang and Nagappan, Nachiappan},
title = {Continuous Software Bug Prediction},
year = {2021},
isbn = {9781450386654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475716.3475790},
doi = {10.1145/3475716.3475790},
abstract = {Background: Many software bug prediction models have been proposed and evaluated on a set of well-known benchmark datasets. We conducted pilot studies on the widely used benchmark datasets and observed common issues among them. Specifically, most of existing benchmark datasets consist of randomly selected historical versions of software projects, which poses non-trivial threats to the validity of existing bug prediction studies since the real-world software projects often evolve continuously. Yet how to conduct software bug prediction in the real-world continuous software development scenarios is not well studied.Aims: In this paper, to bridge the gap between current software bug prediction practice and real-world continuous software development, we propose new approaches to conduct bug prediction in real-world continuous software development regarding model building, updating, and evaluation.Method: For model building, we propose ConBuild, which leverages distributional characteristics of bug prediction data to guide the training version selection. For model updating, we propose ConUpdate, which leverages the evolution of distributional characteristics of bug prediction data between versions to guide the reuse or update of bug prediction models in continuous software development. For model evaluation, we propose ConEA, which leverages the evolution of buggy probability of files between versions to conduct effort-aware evaluation.Results: Experiments on 120 continuously release versions that span across six large-scale open-source software systems show the practical value of our approaches.Conclusions: This paper provides new insights and guidelines for conducting software bug prediction in the context of continuous software development.},
booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {14},
numpages = {12},
keywords = {Empirical software engineering, continuous software development, software defect prediction, software quality},
location = {Bari, Italy},
series = {ESEM '21}
}

@proceedings{10.1145/3617572,
title = {SDD 2023: Proceedings of the 1st International Workshop on Software Defect Datasets},
year = {2023},
isbn = {9798400703775},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the First International Workshop on Software Defect Datasets (SDD), co-located with ESEC/FSE 2023 and to take place in San Francisco, CA on December 8th, 2023.},
location = {San Francisco, CA, USA}
}

@article{10.1145/3664607,
author = {Jiang, Siyu and He, Zhenhang and Chen, Yuwen and Zhang, Mingrong and Ma, Le},
title = {Mobile Application Online Cross-Project Just-in-Time Software Defect Prediction Framework},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3664607},
doi = {10.1145/3664607},
abstract = {As mobile applications evolve rapidly, their fast iterative update nature leads to an increase in software defects. Just-In-Time Software Defect Prediction (JIT-SDP) offers immediate feedback on code changes. For new applications without historical data, researchers have proposed Cross-Project JIT-SDP (CP JIT-SDP). Existing CP JIT-SDP approaches are designed for offline scenarios where target data is available in advance. However, target data in real-world applications usually arrives online in a streaming manner, making online CP JIT-SDP face cross-project distribution differences and target project data concept drift challenges in online scenarios. These challenges often co-exist during application development, and their interactions cause model performance to degrade. To address these issues, we propose an online CP JIT-SDP framework called COTL. Specifically, COTL consists of two stages: offline and online. In the offline stage, the cross-domain structure preserving projection algorithm is used to reduce the cross-project distribution differences. In the online stage, target data arrives sequentially over time. By reducing the differences in marginal and conditional distributions between offline and online data for target project, concept drift is mitigated and classifier weights are updated online. Experimental results on 15 mobile application benchmark datasets show that COTL outperforms 13 benchmark methods on four performance metrics.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {157},
numpages = {31},
keywords = {Online transfer learning, mobile applications bug prediction, cross-project just-in-time software defect prediction, concept drift}
}

@inproceedings{10.1145/3689236.3695374,
author = {Li, Xizhi and Li, Tingting and Jiang, Fan and Qiu, Jifu and Zheng, Chen and Gu, Zhiqi},
title = {Research on Software Defect Detection Based on Random Forest Algorithm},
year = {2024},
isbn = {9798400718137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689236.3695374},
doi = {10.1145/3689236.3695374},
abstract = {With the increase of software scale and complexity, software defect detection has become a key link in ensuring software quality and system security. Traditional software defect detection methods often rely on static analysis and manual review, resulting in low efficiency and accuracy. This study aims to explore a software defect detection method based on the random forest algorithm. The open NASA MDP dataset is selected, and the random forest algorithm is used to detect and extract feature information from software code. The OOB error method is used to calculate the importance of each feature and sort it in descending order to identify potential defects. The accuracy of this experiment reached 89%, verifying the effectiveness of the random forest model in software defect detection. This study provides an efficient and scalable technical approach for defect detection in future large-scale software systems.},
booktitle = {Proceedings of the 2024 9th International Conference on Cyber Security and Information Engineering},
pages = {215–220},
numpages = {6},
keywords = {Data preprocessing, Defect detection, Parameter, Random forest, Software code},
location = {
},
series = {ICCSIE '24}
}

@inproceedings{10.1145/3661167.3661195,
author = {Guo, Yuchen and Shepperd, Martin and Li, Ning},
title = {Improving classifier-based effort-aware software defect prediction by reducing ranking errors},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661195},
doi = {10.1145/3661167.3661195},
abstract = {Context: Software defect prediction utilizes historical data to direct software quality assurance resources to potentially problematic components. Effort-aware (EA) defect prediction prioritizes more bug-like components by taking cost-effectiveness into account. In other words, it is a ranking problem, however, existing ranking strategies based on classification, give limited consideration to ranking errors. Objective: Improve the performance of classifier-based EA ranking methods by focusing on ranking errors. Method: We propose a ranking score calculation strategy called EA-Z which sets a lower bound to avoid near-zero ranking errors. We investigate four primary EA ranking strategies with 16 classification learners, and conduct the experiments for EA-Z and the other four existing strategies. Results: Experimental results from 72 data sets show EA-Z is the best ranking score calculation strategy in terms of Recall@20% and Popt when considering all 16 learners. For particular learners, imbalanced ensemble learner UBag-svm and UBst-rf achieve top performance with EA-Z. Conclusion: Our study indicates the effectiveness of reducing ranking errors for classifier-based effort-aware defect prediction. We recommend using EA-Z with imbalanced ensemble learning.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {160–169},
numpages = {10},
keywords = {Effort-aware, Ranking error, Ranking strategy, Software defect prediction},
location = {Salerno, Italy},
series = {EASE '24}
}

@article{10.1145/3567550,
author = {Zhao, Yunhua and Damevski, Kostadin and Chen, Hui},
title = {A Systematic Survey of Just-in-Time Software Defect Prediction},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {10},
issn = {0360-0300},
url = {https://doi.org/10.1145/3567550},
doi = {10.1145/3567550},
abstract = {Recent years have experienced sustained focus in research on software defect prediction that aims to predict the likelihood of software defects. Moreover, with the increased interest in continuous deployment, a variant of software defect prediction called Just-in-Time Software Defect Prediction (JIT-SDP) focuses on predicting whether each incremental software change is defective. JIT-SDP is unique in that it consists of two interconnected data streams, one consisting of the arrivals of software changes stemming from design and implementation, and the other the (defective or clean) labels of software changes resulting from quality assurance processes.We present a systematic survey of 67 JIT-SDP studies with the objective to help researchers advance the state of the art in JIT-SDP and to help practitioners become familiar with recent progress. We summarize best practices in each phase of the JIT-SDP workflow, carry out a meta-analysis of prior studies, and suggest future research directions. Our meta-analysis of JIT-SDP studies indicates, among other findings, that the predictive performance correlates with change defect ratio, suggesting that JIT-SDP is most performant in projects that experience relatively high defect ratios. Future research directions for JIT-SDP include situating each technique into its application domain, reliability-aware JIT-SDP, and user-centered JIT-SDP.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {201},
numpages = {35},
keywords = {Software defect prediction, release software defect prediction, just-in-time software defect prediction, change-level software defect prediction, machine learning, searching-based algorithms, software change metrics, change defect density}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00056,
author = {Aleithan, Reem},
title = {Explainable just-in-time bug prediction: are we there yet?},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00056},
doi = {10.1109/ICSE-Companion52605.2021.00056},
abstract = {Explaining the prediction results of software bug prediction models is a challenging task, which can provide useful information for developers to understand and fix the predicted bugs. Recently, Jirayus et al. [4] proposed to use two model-agnostic techniques (i.e., LIME and iBreakDown) to explain the prediction results of bug prediction models. Although their experiments on file-level bug prediction show promising results, the performance of these techniques on explaining the results of just-in-time (i.e., change-level) bug prediction is unknown. This paper conducts the first empirical study to explore the explainability of these model-agnostic techniques on just-in-time bug prediction models. Specifically, this study takes a three-step approach, 1) replicating previously widely used just-in-time bug prediction models, 2) applying Local Interpretability Model-agnostic Explanation Technique (LIME) and iBreakDown on the prediction results, and 3) manually evaluating the explanations for buggy instances (i.e. positive predictions) against the root cause of the bugs. The results of our experiment show that LIME and iBreakDown fail to explain defect prediction explanations for just-in-time bug prediction models, unlike file-level [4]. This paper urges for new approaches for explaining the results of just-in-time bug prediction models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {129–131},
numpages = {3},
keywords = {bug prediction, prediction explanation},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@article{10.1145/3716857,
author = {Mangal, Akshat and Rathore, Santosh Singh},
title = {ATE-FS: An Average Treatment Effect-based Feature Selection Technique for Software Fault Prediction},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3716857},
doi = {10.1145/3716857},
abstract = {In software development, software fault prediction (SFP) models aim to identify code sections with a high likelihood of faults before the testing process. SFP models achieve this by analyzing data about the structural properties of the software’s previous versions. Consequently, the accuracy and interpretation of SFP models depend heavily on the chosen software metrics and how well they correlate with patterns of fault occurrence. Previous research has explored improving SFP model performance through feature selection (metric selection). Yet inconsistencies in conclusions arose due to the presence of inconsistent and correlated software metrics. Relying solely on correlations between metrics and faults makes it difficult for developers to take actionable steps, as the causal relationships remain unclear. To address this challenge, this work investigates the use of Causal Inference (CI) methods to understand the causal relationships between software project characteristics, development practices, and the fault-proneness of code sections. We propose a CI-based technique called Average Treatment Effect for Feature Selection (ATE-FS). This technique leverages the causal inference concept to quantify the cause-and-effect relationships between software metrics and fault-proneness. ATE-FS utilizes Average Treatment Effect (ATE) features to identify code metrics that are most suitable for building SFP models. These ATE features capture the causal impact of a metric on fault-proneness. Through an experimental analysis involving twenty-seven SFP datasets, we validate the performance of ATE-FS. We further compare its performance with other state-of-the-art feature selection techniques. The results demonstrate that ATE-FS achieves a significant performance for fault prediction. Additionally, ATE-FS improved consistency in feature selection across diverse SFP datasets.},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
keywords = {Software Fault Prediction, Causal Inference, Average Treatment Effect, Empirical Analysis}
}

@inproceedings{10.1145/3643991.3645065,
author = {Patil, Sangameshwar and Ravindran, B.},
title = {Zero Shot Learning based Alternatives for Class Imbalanced Learning Problem in Enterprise Software Defect Analysis},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645065},
doi = {10.1145/3643991.3645065},
abstract = {Software defect reports are an important type of text data for enterprises as they provide actionable information for improving software quality. Identifying the software defect type automatically can greatly enhance and expedite defect management. Class imbalance is a real-life problem in enterprise software defect classification task and adversely affects the automation effort. We show that zero shot learning based technique can be a good alternative to the well-known supervised learning and SMOTE techniques.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {140–141},
numpages = {2},
keywords = {class imbalance, software defect analysis, zero shot learning},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3594315.3594371,
author = {Tang, Fanggeng and He, Pan},
title = {Software Defect Prediction using Multi-scale Structural Information},
year = {2023},
isbn = {9781450399029},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594315.3594371},
doi = {10.1145/3594315.3594371},
abstract = {In recent years, most researches have used the sequence of nodes in the abstract syntax tree (AST) of code to extract features for software defect prediction (SDP). While the AST is a kind of graph data, it may ignore some part of the structural information to use the original graph data as a sequence for input. Thus, Graph neural network (GNN) has been used to extract structural information in SDP. However, existing researches ignore that GNN learning is inherently local. It is difficult to interact between remote nodes and to capture long-term dependencies in source code. We apply a combination model of GNN Transformer to predict the software defects. Using an AST directly as the input, GNN extracts local features and structural information between the node and its neighbors. We then encode the relative and absolute positions of the nodes in the AST. The position encodings are passed into the Transformer along with the feature information extracted by GNN to extract the global features, which are the long-term dependencies between nodes. Finally, the extracted fused features are used in the SDP. Experiments on the PROMISE dataset have shown that our method achieves higher F-measure and better identification of defective features in source code than the state-of-the-art SDP method.},
booktitle = {Proceedings of the 2023 9th International Conference on Computing and Artificial Intelligence},
pages = {548–556},
numpages = {9},
keywords = {Graph neural network, Software defect prediction, Transformer, deep learning},
location = {Tianjin, China},
series = {ICCAI '23}
}

@inproceedings{10.1145/3647750.3647755,
author = {Wang, Yushuo and Mo, Ran and Zhang, Yao},
title = {Machine Learning-based Models for Predicting Defective Packages},
year = {2024},
isbn = {9798400716546},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3647750.3647755},
doi = {10.1145/3647750.3647755},
abstract = {Software defects are often expensive to fix, especially when they are identified late in development. Packages encapsulate logical functionality and are often developed by particular teams. Package-level defect prediction provides insights into defective designs or implementations in a system early. However, there is little work studying how to build prediction models at the package level. In this paper, we develop prediction models by using seven machine-learning algorithms and code metrics. After evaluating our approach on 20 open-source projects, we have presented that we can build effective models for predicting defective packages by using an appropriate set of metrics. However, there is no single set of metrics that can be generalized across all projects. Our study demonstrates the potential for machine-learning models to enable effective package-level defect prediction. This can guide testing and quality assurance to efficiently locate and fix defects.},
booktitle = {Proceedings of the 2024 8th International Conference on Machine Learning and Soft Computing},
pages = {25–31},
numpages = {7},
keywords = {Code Metrics, Defective Packages Prediction, Machine Learning},
location = {Singapore, Singapore},
series = {ICMLSC '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00104,
author = {Stradowski, Szymon and Madeyski, Lech},
title = {Can We Knapsack Software Defect Prediction? Nokia 5G Case},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00104},
doi = {10.1109/ICSE-Companion58688.2023.00104},
abstract = {As software products become larger and more complex, the test infrastructure needed for quality assurance grows similarly, causing a constant increase in operational and maintenance costs. Although rising in popularity, most Artificial Intelligence (AI) and Machine Learning (ML) Software Defect Prediction (SDP) solutions address singular test phases. In contrast, the need to address the whole Software Development Life Cycle (SDLC) is rarely explored. Therefore in this paper, we define the problem of extending the SDP concept to the entire SDLC, as this may be one of the significant next steps for the field. Furthermore, we explore the similarity between the defined challenge and the widely known Multidimensional Knapsack Problem (MKP). We use Nokia's 5G wireless technology test process to illustrate the proposed concept. Resulting comparison validates the applicability of MKP to optimize the overall test cycle, which can be similarly relevant to any large-scale industrial software development process.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {365–369},
numpages = {5},
keywords = {artificial intelligence, software defect prediction, software testing, continuous integration, software development life cycle, Nokia 5G},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3651781.3651796,
author = {Akour, Mohammed and Alenezi, Mamdouh and Alqasem, Osama},
title = {Enhancing Software Fault Detection with Deep Reinforcement Learning: A Q-Learning Approach},
year = {2024},
isbn = {9798400708329},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3651781.3651796},
doi = {10.1145/3651781.3651796},
abstract = {With the increasing complexity of software systems, traditional software fault detection methods are becoming less effective. This paper proposes a novel approach that leverages Deep Reinforcement Learning (DRL) to improve software fault detection. DRL, a subset of machine learning, has shown promising results in various domains and has the potential to revolutionize software engineering practices. By formulating software fault detection as a reinforcement learning task, we develop a DRL-based model using Q-learning that can learn complex fault patterns and make accurate predictions. Our approach also incorporates feature extraction using Random Forest and Na\"{\i}ve Bayes. We evaluate our method using real-world software datasets, demonstrating its potential to enhance fault detection accuracy and contribute to more reliable and efficient software development processes.},
booktitle = {Proceedings of the 2024 13th International Conference on Software and Computer Applications},
pages = {97–101},
numpages = {5},
keywords = {Deep Reinforcement Learning, machine learning, reinforcement learning, reliability., resilience, software fault detection},
location = {Bali Island, Indonesia},
series = {ICSCA '24}
}

@inproceedings{10.1145/3416508.3417114,
author = {Aljamaan, Hamoud and Alazba, Amal},
title = {Software defect prediction using tree-based ensembles},
year = {2020},
isbn = {9781450381277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416508.3417114},
doi = {10.1145/3416508.3417114},
abstract = {Software defect prediction is an active research area in software engineering. Accurate prediction of software defects assists software engineers in guiding software quality assurance activities. In machine learning, ensemble learning has been proven to improve the prediction performance over individual machine learning models. Recently, many Tree-based ensembles have been proposed in the literature, and their prediction capabilities were not investigated in defect prediction. In this paper, we will empirically investigate the prediction performance of seven Tree-based ensembles in defect prediction. Two ensembles are classified as bagging ensembles: Random Forest and Extra Trees, while the other five ensembles are boosting ensembles: Ada boost, Gradient Boosting, Hist Gradient Boosting, XGBoost and CatBoost. The study utilized 11 publicly available MDP NASA software defect datasets. Empirical results indicate the superiority of Tree-based bagging ensembles: Random Forest and Extra Trees ensembles over other Tree-based boosting ensembles. However, none of the investigated Tree-based ensembles was significantly lower than individual decision trees in prediction performance. Finally, Adaboost ensemble was the worst performing ensemble among all Tree-based ensembles.},
booktitle = {Proceedings of the 16th ACM International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {1–10},
numpages = {10},
keywords = {Bagging, Boosting, Classification, Ensemble Learning, Machine Learning, Prediction, Software Defect},
location = {Virtual, USA},
series = {PROMISE 2020}
}

@inproceedings{10.1145/3639478.3643113,
author = {Du, Xiaoting and Li, Chenglong and Ma, Xiangyue and Zheng, Zheng},
title = {How Does Pre-trained Language Model Perform on Deep Learning Framework Bug Prediction?},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643113},
doi = {10.1145/3639478.3643113},
abstract = {Understanding and predicting bugs is crucial for developers seeking to enhance testing efficiency and mitigate issues in software releases. Bug reports, though semi-structured texts, contain a wealth of semantic information, rendering their comprehension a critical aspect of bug prediction. In light of the recent success of pre-trained language models (PLMs) in the domain of natural language processing, numerous studies have leveraged these models to grasp various forms of textual information. However, the capability of PLMs to understand bug reports remains uncertain. To tackle this challenge, we introduce KnowBug, a framework with a bug report knowledge-enhanced PLM. In this framework, utilizing bug reports obtained from open-source deep learning frameworks as input, prompts are designed and the PLM is fine-tuned for evaluating KnowBug's ability to comprehend bug reports and predict bug types.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {346–347},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@article{10.1145/3589342,
author = {Gangwar, Arvind Kumar and Kumar, Sandeep},
title = {Concept Drift in Software Defect Prediction: A Method for Detecting and Handling the Drift},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3589342},
doi = {10.1145/3589342},
abstract = {Software Defect Prediction (SDP) is crucial towards software quality assurance in software engineering. SDP analyzes the software metrics data for timely prediction of defect prone software modules. Prediction process is automated by constructing defect prediction classification models using machine learning techniques. These models are trained using metrics data from historical projects of similar types. Based on the learned experience, models are used to predict defect prone modules in currently tested software. These models perform well if the concept is stationary in a dynamic software development environment. But their performance degrades unexpectedly in the presence of change in concept (Concept Drift). Therefore, concept drift (CD) detection is an important activity for improving the overall accuracy of the prediction model. Previous studies on SDP have shown that CD may occur in software defect data and the used defect prediction model may require to be updated to deal with CD. This phenomenon of handling the CD is known as CD adaptation. It is observed that still efforts need to be done in this direction in the SDP domain. In this article, we have proposed a pair of paired learners (PoPL) approach for handling CD in SDP. We combined the drift detection capabilities of two independent paired learners and used the paired learner (PL) with the best performance in recent time for next prediction. We experimented on various publicly available software defect datasets garnered from public data repositories. Experimentation results showed that our proposed approach performed better than the existing similar works and the base PL model based on various performance measures.},
journal = {ACM Trans. Internet Technol.},
month = may,
articleno = {31},
numpages = {28},
keywords = {Concept drift, paired learning, software defect prediction, software quality assurance}
}

@inproceedings{10.1145/3543895.3543924,
author = {Alshehri, Yasser Ali and Alnazzawi, Noha and Hijazi, Haneen and Alharbi, Rawan},
title = {Stratifying large software files to improve prediction performance in software defect prediction},
year = {2023},
isbn = {9781450397605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543895.3543924},
doi = {10.1145/3543895.3543924},
abstract = {Size is one of the significant factors associated with bugs, and it has been used to predict software faults. We believe that stratifying software files based on size can play an essential role in improving prediction performance. This study explored the effect of size by stratifying our sample based on each unit’s size and distributing software units in multiple stratified groups based on an equal distribution approach. We stratified the Eclipse Europa project files, and we reported the performance of each stratified group and compared them. We used two popular classifiers, decision tree J48, and random forest, to implement this experiment. These classifiers presented similar results on the same group of files. The results indicated that predicting faults with large files is better than predicting those in small files. In addition, the results showed higher median values of all performance measures and less variation in each measure.},
booktitle = {Proceedings of the 9th International Conference on Applied Computing &amp; Information Technology},
pages = {1–5},
numpages = {5},
keywords = {data mining, decision tree J48, machine learning, random forest, software fault proneness, software quality},
location = {Virtual Event, USA},
series = {ACIT '22}
}

@inproceedings{10.1145/3611643.3616307,
author = {Song, Liyan and Minku, Leandro Lei and Teng, Cong and Yao, Xin},
title = {A Practical Human Labeling Method for Online Just-in-Time Software Defect Prediction},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616307},
doi = {10.1145/3611643.3616307},
abstract = {Just-in-Time Software Defect Prediction (JIT-SDP) can be seen as an online learning problem where additional software changes produced over time may be labeled and used to create training examples. These training examples form a data stream that can be used to update JIT-SDP models in an attempt to avoid models becoming obsolete and poorly performing. However, labeling procedures adopted in existing online JIT-SDP studies implicitly assume that practitioners would not inspect software changes upon a defect-inducing prediction, delaying the production of training examples. This is inconsistent with a real-world scenario where practitioners would adopt JIT-SDP models and inspect certain software changes predicted as defect-inducing to check whether they really induce defects. Such inspection means that some software changes would be labeled much earlier than assumed in existing work, potentially leading to different JIT-SDP models and performance results. This paper aims at formulating a more practical human labeling procedure that takes into account the adoption of JIT-SDP models during the software development process. It then analyses whether and to what extent it would impact the predictive performance of JIT-SDP models. We also propose a new method to target the labeling of software changes with the aim of saving human inspection effort. Experiments based on 14 GitHub projects revealed that adopting a more realistic labeling procedure led to significantly higher predictive performance than when delaying the labeling process, meaning that existing work may have been underestimating the performance of JIT-SDP. In addition, our proposed method to target the labeling process was able to reduce human effort while maintaining predictive performance by recommending practitioners to inspect software changes that are more likely to induce defects. We encourage the adoption of more realistic human labeling methods in research studies to obtain an evaluation of JIT-SDP predictive performance that is closer to reality.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {605–617},
numpages = {13},
keywords = {Just-in-time software defect prediction, human inspection, human labeling, online learning, verification latency, waiting time},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3387940.3391463,
author = {Omri, Safa and Sinz, Carsten},
title = {Deep Learning for Software Defect Prediction: A Survey},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391463},
doi = {10.1145/3387940.3391463},
abstract = {Software fault prediction is an important and beneficial practice for improving software quality and reliability. The ability to predict which components in a large software system are most likely to contain the largest numbers of faults in the next release helps to better manage projects, including early estimation of possible release delays, and affordably guide corrective actions to improve the quality of the software. However, developing robust fault prediction models is a challenging task and many techniques have been proposed in the literature. Traditional software fault prediction studies mainly focus on manually designing features (e.g. complexity metrics), which are input into machine learning classifiers to identify defective code. However, these features often fail to capture the semantic and structural information of programs. Such information is needed for building accurate fault prediction models. In this survey, we discuss various approaches in fault prediction, also explaining how in recent studies deep learning algorithms for fault prediction help to bridge the gap between programs' semantics and fault prediction features and make accurate predictions.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {209–214},
numpages = {6},
keywords = {deep learning, machine learning, software defect prediction, software quality assurance, software testing},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3474198.3478215,
author = {Du, Xiaozhi and Yue, Hehe and Dong, Honglei},
title = {Software Defect Prediction Method based on Hybrid Sampling},
year = {2022},
isbn = {9781450390149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474198.3478215},
doi = {10.1145/3474198.3478215},
abstract = {Software defect prediction is an essential technology to provide guidance and assistance for software testers and developers. However, the problem of imbalanced data sets limits the effect and application of the software defect prediction. To address this issue, this paper proposes a software defect prediction method based on hybrid sampling, which combines the strategies of over-sampling with under-sampling. For minority class, over-sampling uses k-means to cluster samples, then adopts SMOTE to generate artificial data based on safe areas of the clustering outcome. For majority class, under-sampling uses logistic regression classifier to get the misclassification probability of each sample and its instance hardness value. Then the samples, whose instance hardness values are lower than the threshold, are removed from the datasets. The experimental results show that our method is superior to the previous methods. Compared with SMOTE-kNN, SMOTE-Tomek, SMOTE and DBSMOTE, the accuracy of our method is improved by 17.60%, 6.99%, 8.66% and 26.18% on average respectively.},
booktitle = {International Conference on Frontiers of Electronics, Information and Computation Technologies},
articleno = {93},
numpages = {9},
keywords = {Data imbalance, Hybrid sampling, Software defect prediction},
location = {Changsha, China},
series = {ICFEICT 2021}
}

@inproceedings{10.1145/3686852.3686881,
author = {Alhazeem, Ensaf and Alsobeh, Anas and Al-Ahmad, Bilal},
title = {Enhancing Software Engineering Education through AI: An Empirical Study of Tree-Based Machine Learning for Defect Prediction},
year = {2024},
isbn = {9798400711060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686852.3686881},
doi = {10.1145/3686852.3686881},
abstract = {In the rapidly evolving field of information technology education,integrating artificial intelligence (AI) and machine learning (ML) techniques presents opportunities and challenges. This empirical study investigates the application of tree-based ML techniques, specifically Random Forest (RF) and Extreme Gradient Boosting (XGBoost), for software defect prediction in the context of IT education. We analyze nine publicly available NASA software defect datasets to compare the performance of these algorithms across multiple metrics, including accuracy, precision, recall, and ROC area. Our findings demonstrate that XGBoost consistently outperforms Random Forest, achieving near-perfect accuracy across most datasets. The paper explores how these advanced techniques can be responsibly integrated into software engineering (SE) education to enhance student learning while addressing concerns about potential over-reliance on AI tools. We discuss the implications of our results for IT education, emphasizing the need to balance the use of sophisticated AI technologies with the development of fundamental software assurance skills. Furthermore, we examine the role of AI in augmenting SE education, particularly in areas such as software assurance explanations, feature identification, and data augmentation.},
booktitle = {Proceedings of the 25th Annual Conference on Information Technology Education},
pages = {153–156},
numpages = {4},
keywords = {AI in Education, Machine Learning (ML), Random Forest, Software Defect Prediction, Software Engineering, XGBoost},
location = {El Paso, TX, USA},
series = {SIGITE '24}
}

@inproceedings{10.1145/3530019.3531330,
author = {sadaf, saadia and Iqbal, Danish and Buhnova, Barbora},
title = {AI-Based Software Defect Prediction for Trustworthy Android Apps},
year = {2022},
isbn = {9781450396134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3530019.3531330},
doi = {10.1145/3530019.3531330},
abstract = {The present time in the industry is a time where Android Applications are in a wide range with its widespread of the users also. With the increased use of Android applications, the defects in the Android context have also been increasing. The malware of defective software can be any pernicious program with malignant effects. Many techniques based on static, dynamic, and hybrid approaches have been proposed with the combination of Machine learning (ML) or Artificial Intelligence (AI) techniques. In this regard. Scientifically, it is complicated to examine the malignant effects. A single approach cannot predict defects alone, so multiple approaches must be used simultaneously. However, the proposed techniques do not describe the types of defects they address. The paper aims to propose a framework that classifies the defects. The Artificial Intelligence (AI) techniques are described, and the different defects are mapped to them. The mapping of defects to AI techniques is based on the types of defects found in the Android Context. The accuracy of the techniques and the working criteria has been set as the mapping metrics. This will significantly improve the quality and testing of the product. However, the appropriate technique for a particular type of defect could be easily selected. This will reduce the cost and time efforts put into predicting defects.},
booktitle = {Proceedings of the 26th International Conference on Evaluation and Assessment in Software Engineering},
pages = {393–398},
numpages = {6},
keywords = {Artificial Intelligence, Defect Prediction Technique, Machine Learning, Software Defect prevention technique},
location = {Gothenburg, Sweden},
series = {EASE '22}
}

@inproceedings{10.1145/3650212.3685554,
author = {Decrop, Alix},
title = {Leveraging Natural Language Processing and Data Mining to Augment and Validate APIs},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3685554},
doi = {10.1145/3650212.3685554},
abstract = {APIs are increasingly prominent for modern web applications, allowing millions of users around the world to access data. Reducing the risk of API defects - and consequently failures - is key, notably for security, availability, and maintainability purposes. Documenting an API is crucial, allowing the user to better understand it. Moreover, API testing techniques often require formal documentation as input. However, documenting is a time-consuming and error-prone task, often overlooked by developers. Natural Language Processing (NLP) could assist API development, as recent Large Language Models (LLMs) demonstrated exceptional abilities to automate tasks based on their colossal training data. Data mining could also be utilized, synthesizing API information scattered across the web. Hence, I present my PhD project aimed at exploring the usage of NLP-related technologies and data mining to augment and validate APIs. The research questions of this PhD project are: (1) What types of APIs can benefit from NLP and data mining assistance? (2) What API problems can be solved with such methods? (3) How effective are the methods (i.e. LLMs) in assisting APIs? (4) How efficient are the methods in assisting APIs (i.e. time and costs)?},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1906–1908},
numpages = {3},
keywords = {API, Automation, Data Mining, LLM, NLP, Software Testing},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3368926.3369711,
author = {Ha, Duy-An and Chen, Ting-Hsuan and Yuan, Shyan-Ming},
title = {Unsupervised methods for Software Defect Prediction},
year = {2019},
isbn = {9781450372459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368926.3369711},
doi = {10.1145/3368926.3369711},
abstract = {Software Defect Prediction (SDP) aims to assess software quality by using machine learning techniques. Recently, by proposing the connectivity-based unsupervised learning method, Zhang et al. have been proven that unsupervised classification has great potential to apply to this problem. Inspiring by this idea, in our work we try to replicate the results of Zhang et al.'s experiment and attempt to improve the performance by examining different techniques at each step of the approach using unsupervised learning methods to solve the SDP problem. Specifically, we try to follow the steps of the experiment described in their work strictly and examine three other clustering methods with four other ways for feature selection besides using all. To the best of our knowledge, these methods are first applied in SDP to evaluate their predictive power. For replicating the results, generally results in our experiments are not as good as the previous work. It may be due to we do not know which features are used in their experiment exactly. Fluid clustering and spectral clustering give better results than Newman clustering and CNM clustering in our experiments. Additionally, the experiments also show that using Kernel Principal Component Analysis (KPCA) or Non-Negative Matrix Factorization (NMF) for feature selection step gives better performance than using all features in the case of unlabeled data. Lastly, to make replicating our work easy, a lightweight framework is created and released on Github.},
booktitle = {Proceedings of the 10th International Symposium on Information and Communication Technology},
pages = {49–55},
numpages = {7},
keywords = {Community Structure Detection, Machine Learning, Software Defect Prediction, Software Engineering, Unsupervised Learning},
location = {Hanoi, Ha Long Bay, Viet Nam},
series = {SoICT '19}
}

@inproceedings{10.1145/3568562.3568587,
author = {Ho, Anh and Nhat Hai, Nguyen and Thi-Mai-Anh, Bui},
title = {Combining Deep Learning and Kernel PCA for Software Defect Prediction},
year = {2022},
isbn = {9781450397254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568562.3568587},
doi = {10.1145/3568562.3568587},
abstract = {Software defect prediction aims to automatically determine the most likely location of defective program elements (i.e., statement, method, class, module etc.). Previous studies for software defect prediction mainly focus on exploring designing features such as source code complexity, object oriented design metrics etc. to classify program elements into two categories: (i) defective and (ii) non-defective. Although these approaches have obtained promising results, there exists two significant challenges in this research field: (i) removing irrelevant and redundant information from designing structures ; (ii) reducing the impact of skewed data distribution on learning models. In this paper, we aim to address these two issues by firstly applying kernel PCA to extract essential information from designing features and secondly proposing a deep neural network model which investigates the non-linear relationship among features. In order to mitigate the class imbalance, we apply a weighted loss function combined with a bootstrapping method to handle batch training mechanism of our model. We conducted some experiments to assess the performance of our proposed approach over NASA (with 10 projects) and PROMISE (with 34 projects) datasets. In order to leverage the efficiency of kernel PCA technique in software defect prediction, we compared it to some traditional feature selection approaches over a high-dimensional dataset ECLIPSE. The empirical results showed that our proposed method has outperformed these other state-of-the-art models by effectively predicting defective source files.},
booktitle = {Proceedings of the 11th International Symposium on Information and Communication Technology},
pages = {360–367},
numpages = {8},
keywords = {deep neural network, feature reduction, kernel PCA},
location = {Hanoi, Vietnam},
series = {SoICT '22}
}

@inproceedings{10.1145/3628797.3628963,
author = {Thi-Mai-Anh, Bui and Nhat-Hai, Nguyen},
title = {On the Value of Code Embedding and Imbalanced Learning Approaches for Software Defect Prediction},
year = {2023},
isbn = {9798400708916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628797.3628963},
doi = {10.1145/3628797.3628963},
abstract = {Automated software defect prediction aims to identify and estimate the likelihood of defects in software source code elements, seeking to enhance software quality while reducing testing costs. Previous research on software defect prediction primarily concentrated on investigating design-related features such as source code complexity and object-oriented design metrics for the purpose of classifying program elements into two categories: (i) defective and (ii) non-detective. Nevertheless, the majority of these studies have relied solely on hand-crafted software metrics, neglecting the valuable asset of source code instruction, which can play a pivotal role in detecting bugs. This study leverages the use of source code embedding techniques to extract essential information from program elements through a convolutional neural network. The likelihood of a source file element (e.g., class or method) being defective is established through the utilization of a fully connected network that incorporates both source code features and design-related attributes. Additionally, we explore specific imbalanced learning strategies to address the skewed defect data distribution issue. To assess the effectiveness of our proposed approach, we conducted experiments on the publicly available dataset, namely PROMISE. The empirical results consistently showcase the superior performance of our method, as it effectively predicts defective source files, outperforming other state-of-the-art models.},
booktitle = {Proceedings of the 12th International Symposium on Information and Communication Technology},
pages = {510–516},
numpages = {7},
keywords = {code embedding, convolutional neural network, cost sensitive learning, sampling data},
location = {Ho Chi Minh, Vietnam},
series = {SOICT '23}
}

@inproceedings{10.1145/3410352.3410747,
author = {Almaghairbe, Rafig and Roper, Marc and Almabruk, Tahani},
title = {Machine Learning Techniques for Automated Software Fault Detection via Dynamic Execution Data: Empirical Evaluation Study},
year = {2020},
isbn = {9781450377362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410352.3410747},
doi = {10.1145/3410352.3410747},
abstract = {The biggest obstacle of automated software testing is the construction of test oracles. Today, it is possible to generate enormous amount of test cases for an arbitrary system that reach a remarkably high level of coverage, but the effectiveness of test cases is limited by the availability of test oracles that can distinguish failing executions. Previous work by the authors has explored the use of unsupervised and semi-supervised learning techniques to develop test oracles so that the correctness of software outputs and behaviours on new test cases can be predicated [1], [2], [10], and experimental results demonstrate the promise of this approach. In this paper, we present an evaluation study for test oracles based on machine-learning approaches via dynamic execution data (firstly, input/output pairs and secondly, amalgamations of input/output pairs and execution traces) by comparing their effectiveness with existing techniques from the specification mining domain (the data invariant detector Daikon [5]). The two approaches are evaluated on a range of mid-sized systems and compared in terms of their fault detection ability and false positive rate. The empirical study also discuss the major limitations and the most important properties related to the application of machine learning techniques as test oracles in practice. The study also gives a road map for further research direction in order to tackle some of discussed limitations such as accuracy and scalability. The results show that in most cases semi-supervised learning techniques performed far better as an automated test classifier than Daikon (especially in the case that input/output pairs were augmented with their execution traces). However, there is one system for which our strategy struggles and Daikon performed far better. Furthermore, unsupervised learning techniques performed on a par when compared with Daikon in several cases particularly when input/output pairs were used together with execution traces.},
booktitle = {Proceedings of the 6th International Conference on Engineering &amp; MIS 2020},
articleno = {15},
numpages = {12},
keywords = {Automated Testing Oracles, Empirical Study, Machine Learning Techniques, Specification Mining},
location = {Almaty, Kazakhstan},
series = {ICEMIS'20}
}

@inproceedings{10.1145/3368089.3417062,
author = {Suh, Alexander},
title = {Adapting bug prediction models to predict reverted commits at Wayfair},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417062},
doi = {10.1145/3368089.3417062},
abstract = {Researchers have proposed many algorithms to predict software bugs. Given a software entity (e.g., a file or method), these algorithms predict whether the entity is bug-prone. However, since these algorithms cannot identify specific bugs, this does not tend to be particularly useful in practice. In this work, we adapt this prior work to the related problem of predicting whether a commit is likely to be reverted. Given the batch nature of continuous integration deployment at scale, this allows developers to find time-sensitive bugs in production more quickly. The models in this paper are based on features extracted from the revision history of a codebase that are typically used in bug prediction. Our experiments, performed on the three main repositories for the Wayfair website, show that our models can rank reverted commits above 80% of non-reverted commits on average. Moreover, when given to Wayfair developers, our models reduce the amount of time needed to find certain kinds of bugs by 55%. Wayfair continues to use our findings and models today to help find bugs during software deployments.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1251–1262},
numpages = {12},
keywords = {reverted commits, software defect prediction, software deployment},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3544902.3546255,
author = {Moussa, Rebecca and Guizzo, Giovani and Sarro, Federica},
title = {MEG: Multi-objective Ensemble Generation for Software Defect Prediction},
year = {2022},
isbn = {9781450394277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544902.3546255},
doi = {10.1145/3544902.3546255},
abstract = {Background: Defect Prediction research aims at assisting software engineers in the early identification of software defect during the development process. A variety of automated approaches, ranging from traditional classification models to more sophisticated learning approaches, have been explored to this end. Among these, recent studies have proposed the use of ensemble prediction models (i.e., aggregation of multiple base classifiers) to build more robust defect prediction models. Aims: In this paper, we introduce a novel approach based on multi-objective evolutionary search to automatically generate defect prediction ensembles. Our proposal is not only novel with respect to the more general area of evolutionary generation of ensembles, but it also advances the state-of-the-art in the use of ensemble in defect prediction. Method: We assess the effectiveness of our approach, dubbed as Multi-objectiveEnsembleGeneration (MEG), by empirically benchmarking it with respect to the most related proposals we found in the literature on defect prediction ensembles and on multi-objective evolutionary ensembles (which, to the best of our knowledge, had never been previously applied to tackle defect prediction). Result: Our results show that MEG is able to generate ensembles which produce similar or more accurate predictions than those achieved by all the other approaches considered in 73% of the cases (with favourable large effect sizes in 80% of them). Conclusions: MEG is not only able to generate ensembles that yield more accurate defect predictions with respect to the benchmarks considered, but it also does it automatically, thus relieving the engineers from the burden of manual design and experimentation.},
booktitle = {Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {159–170},
numpages = {12},
keywords = {Defect Prediction, Empirical Study, Hyper-Heuristic, Multi-Objective Optimisation, Search-Based Software Engineering},
location = {Helsinki, Finland},
series = {ESEM '22}
}

@article{10.1145/3672451,
author = {Wan, Chengcheng and Liu, Shicheng and Xie, Sophie and Liu, Yuhan and Hoffmann, Henry and Maire, Michael and Lu, Shan},
title = {Keeper: Automated Testing and Fixing of Machine Learning Software},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3672451},
doi = {10.1145/3672451},
abstract = {The increasing number of software applications incorporating machine learning (ML) solutions has led to the need for testing techniques. However, testing ML software requires tremendous human effort to design realistic and relevant test inputs and to judge software output correctness according to human common sense. Even when misbehavior is exposed, it is often unclear whether the defect is inside ML API or the surrounding code and how to fix the implementation. This article tackles these challenges by proposing Keeper, an automated testing and fixing tool for ML software. The core idea of Keeper is designing pseudo-inverse functions that semantically reverse the corresponding ML task in an empirical way and proxy common human judgment of real-world data. It incorporates these functions into a symbolic execution engine to generate tests. Keeper also detects code smells that degrade software performance. Once misbehavior is exposed, Keeper attempts to change how ML APIs are used to alleviate the misbehavior.Our evaluation on a variety of applications shows that Keeper greatly improves branch coverage, while identifying 74 previously unknown failures and 19 code smells from 56 out of 104 applications. Our user studies show that 78% of end-users and 95% of developers agree with Keeper’s detection and fixing results.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {167},
numpages = {33},
keywords = {Software testing, machine learning, machine learning API}
}

@inproceedings{10.1145/3583133.3595850,
author = {Moussa, Rebecca and Guizzo, Giovani and Sarro, Federica},
title = {MEG: Multi-objective Ensemble Generation for Software Defect Prediction (HOP GECCO'23)},
year = {2023},
isbn = {9798400701207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583133.3595850},
doi = {10.1145/3583133.3595850},
abstract = {This Hot-off-the-Press abstract aims at disseminating our recent work titled "MEG: Multi-objective Ensemble Generation for Software Defect Prediction" published in the proceedings of the 16th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM) [4]. We believe this work is of interest for the GECCO community as it proposes a novel way to automatically generate ensemble machine learning models leveraging the power of evolutionary computation: MEG introduces the concept of whole-ensemble generation as opposed to the well known Pareto-ensemble generation. While we evaluate the effectiveness of MEG for Software Defect Prediction in our work, MEG can be applied to any classification or regression problem and we invite both researchers and practitioners to further explore its effectiveness for other application domains. To this end, we have made MEG's source code publicly available.},
booktitle = {Proceedings of the Companion Conference on Genetic and Evolutionary Computation},
pages = {37–38},
numpages = {2},
keywords = {multi-objective ensamble, search-based software engineering},
location = {Lisbon, Portugal},
series = {GECCO '23 Companion}
}

@inproceedings{10.1145/3573834.3574472,
author = {Wang, Wennan and Zhao, Hanxu and Li, Yu and Su, Junyu and Lu, Jiadong and Wang, Baoping},
title = {Research on cross-project software defect prediction based on feature transfer method},
year = {2023},
isbn = {9781450397933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573834.3574472},
doi = {10.1145/3573834.3574472},
abstract = {In this paper, the research and experimental analysis of cross-project application software defect prediction is carried out, and the TCA model is used to improve the application function of its prediction. The models pointed out in this paper usually include: normalization processing model and mathematical linear kernel mathematical statistics The difference between the functional SVM classifier and the extended migration component analysis TCA+ model is that the model pointed out in this paper not only satisfies the prediction of software defects within the project suitable for TCA, but also meets the prediction of software defects in the cross-project of TCA+, so the most appropriate normalization can be selected. Optimized processing options to improve cross-project software defect prediction capabilities.},
booktitle = {Proceedings of the 4th International Conference on Advanced Information Science and System},
articleno = {7},
numpages = {5},
keywords = {AEEEM dataset, Improved TCA+, ReLink dataset},
location = {Sanya, China},
series = {AISS '22}
}

@inproceedings{10.1145/3691620.3695532,
author = {Shree, Sunny and Khadka, Krishna and Lei, Yu and Kacker, Raghu N. and Kuhn, D. Richard},
title = {Constructing Surrogate Models in Machine Learning Using Combinatorial Testing and Active Learning},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695532},
doi = {10.1145/3691620.3695532},
abstract = {Machine learning (ML)-based models are often black box, making it challenging to understand and interpret their decision-making processes. Surrogate models are constructed to approximate the behavior of a target model and are an essential tool for analyzing black-box models. The construction of a surrogate model typically includes querying the target model with carefully selected data points and using the responses from the target model to infer information about its structure and parameters.In this paper, we propose an approach to surrogate model construction using combinatorial testing and active learning, aiming to efficiently capture the essential interactions between features that drive the target model's predictions. Our approach first leverages t-way testing to generate data points that capture all the t-way feature interactions. We then use an iterative process to isolate the essential feature interactions, i.e., those that can determine a model prediction. In the iterative process, we remove nonessential feature interactions, generate additional data points to contain the remaining interactions, and employ active learning techniques to select a subset of the data points to update the surrogate model. This process is continued until we construct a surrogate model that closely mirrors the target model's behavior. We evaluate our approach on 4 public datasets and 12 ML models and compare the results with the state-of-the-art (SOTA) approaches. Our experimental results show that our approach can perform in most cases better than the SOTA approaches in terms of accuracy and efficiency.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1645–1654},
numpages = {10},
keywords = {machine learning, surrogate model, proxy model, model extraction attack, combinatorial testing, feature interactions, test case generation},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3640310.3674092,
author = {Naveed, Hira and Grundy, John and Arora, Chetan and Khalajzadeh, Hourieh and Haggag, Omar},
title = {Towards Runtime Monitoring for Responsible Machine Learning using Model-driven Engineering},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674092},
doi = {10.1145/3640310.3674092},
abstract = {Machine learning (ML) components are used heavily in many current software systems, but developing them responsibly in practice remains challenging. 'Responsible ML' refers to developing, deploying and maintaining ML-based systems that adhere to human-centric requirements, such as fairness, privacy, transparency, safety, accessibility, and human values. Meeting these requirements is essential for maintaining public trust and ensuring the success of ML-based systems. However, as changes are likely in production environments and requirements often evolve, design-time quality assurance practices are insufficient to ensure such systems' responsible behavior. Runtime monitoring approaches for ML-based systems can potentially offer valuable solutions to address this problem. Many currently available ML monitoring solutions overlook human-centric requirements due to a lack of awareness and tool support, the complexity of monitoring human-centric requirements, and the effort required to develop and manage monitors for changing requirements. We believe that many of these challenges can be addressed by model-driven engineering. In this new ideas paper, we present an initial meta-model, model-driven approach, and proof of concept prototype for runtime monitoring of human-centric requirements violations, thereby ensuring responsible ML behavior. We discuss our prototype, current limitations and propose some directions for future work.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {195–202},
numpages = {8},
keywords = {Human-centric requirements, Machine learning components, Model-driven engineering, Responsible ML, Runtime monitoring},
location = {Linz, Austria},
series = {MODELS '24}
}

@inproceedings{10.1145/3611643.3617845,
author = {Risse, Niklas},
title = {Detecting Overfitting of Machine Learning Techniques for Automatic Vulnerability Detection},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3617845},
doi = {10.1145/3611643.3617845},
abstract = {Recent results of machine learning for automatic vulnerability detection have been very promising indeed: Given only the source code of a function f, models trained by machine learning techniques can decide if f contains a security flaw with up to 70% accuracy. But how do we know that these results are general and not specific to the datasets? To study this question, researchers proposed to amplify the testing set by injecting semantic preserving changes and found that the model’s accuracy significantly drops. In other words, the model uses some unrelated features during classification. In order to increase the robustness of the model, researchers proposed to train on amplified training data, and indeed model accuracy increased to previous levels. In this paper, we replicate and continue this investigation, and provide an actionable model benchmarking methodology to help researchers better evaluate advances in machine learning for vulnerability detection. Specifically, we propose a cross validation algorithm, where a semantic preserving transformation is applied during the amplification of either the training set or the testing set. Using 11 transformations and 3 ML techniques, we find that the improved robustness only applies to the specific transformations used during training data amplification. In other words, the robustified models still rely on unrelated features for predicting the vulnerabilities in the testing data.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {2189–2191},
numpages = {3},
keywords = {automatic vulnerability detection, large language models, machine learning, semantic preserving transformations},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3661167.3661268,
author = {Pontillo, Valeria},
title = {Insights Into Test Code Quality Prediction: Managing Machine Learning Techniques},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661268},
doi = {10.1145/3661167.3661268},
abstract = {Test cases represent the first line of defence against the introduction of software faults, especially when testing for regressions. They must be constantly maintained and updated as part of software components to keep them useful. With the help of testing frameworks, developers create test methods and run them periodically on their code. The entire team relies on the results from these tests to decide whether to merge a pull request or deploy the system. Unfortunately, tests are not immune to bugs or technical debts: indeed, they often suffer from issues that can preclude their effectiveness. Typical problems in test cases are called flaky tests and test smells. Over the last decades, the software engineering research community has been proposing a number of static and dynamic approaches to assist developers with the (semi-)automatic detection and removal of these problems. Despite this, most of these approaches rely on expensive dynamic steps and depend on tunable thresholds. These limitations have been partially targeted through machine learning solutions that could predict test quality issues using various features, like source code vocabulary or a mixture of static and dynamic metrics. In this tutorial, I will discuss our experience building prediction models to detect quality issues in test code. The tutorial will discuss the design choices to make in the context of test code quality prediction and the implications these choices have for the reliability of the resulting models.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {2},
numpages = {1},
keywords = {Empirical Studies., Machine Learning, Test Code Quality Prediction},
location = {Salerno, Italy},
series = {EASE '24}
}

@inproceedings{10.1145/3584871.3584885,
author = {Malhotra, Ruchika and Chawla, Sonali and Sharma, Anjali},
title = {An Artificial Neural Network Model based on Binary Particle Swarm Optimization for enhancing the efficiency of Software Defect Prediction},
year = {2023},
isbn = {9781450398237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584871.3584885},
doi = {10.1145/3584871.3584885},
abstract = {With the rise in the growth of the software industry, it is essential to identify software defects in earlier stages to save costs and improve the efficiency of the software development lifecycle process. We have devised a hybrid software defect prediction (SDP) model that integrates Binary Particle Swarm Optimization (Binary PSO), Synthetic Minority Oversampling Technique (SMOTE), and Artificial Neural Network (ANN). BPSO is applied as a wrapper feature selection process utilizing AUC as a fitness function, SMOTE handles the dataset imbalance, and ANN is used as a classification algorithm for predicting software defects. We analyze the proposed BPSO-SMOTE-ANN model's predictive capability using the AUC and G-mean performance metrics. The proposed hybrid model is found helpful in predicting software defects. The statistical results suggest the enhanced performance of the proposed hybrid model concerning AUC and G-mean values. Also, the hybrid model was found to be competitive with other machine learning(ML) algorithms in determining software defects.},
booktitle = {Proceedings of the 2023 6th International Conference on Software Engineering and Information Management},
pages = {92–100},
numpages = {9},
keywords = {Artificial Neural Networks, Particle Swarm Optimization, SMOTE, Search-based Techniques, Software Defect Prediction},
location = {Palmerston North, New Zealand},
series = {ICSIM '23}
}

@inproceedings{10.1145/3352411.3352412,
author = {Li, Ran and Zhou, Lijuan and Zhang, Shudong and Liu, Hui and Huang, Xiangyang and Sun, Zhong},
title = {Software Defect Prediction Based on Ensemble Learning},
year = {2019},
isbn = {9781450371414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352411.3352412},
doi = {10.1145/3352411.3352412},
abstract = {Software defect prediction is one of the important ways to guarantee the quality of software systems. Combining various algorithms in machine learning to predict software defects has become a hot topic in the current study. The paper uses the datasets of MDP as the experimental research objects and takes ensemble learning as research focus to construct software defect prediction model. With experimenting five different types of ensemble algorithms and analyzing the features and procedures, this paper discusses the best ensemble algorithm which is Random Forest through experimental comparison. Then we utilize the SMOTE over-sampling and Resample methods to improve the quality of datasets to build a complete new software defect prediction model. Therefore, the results show that the model can improve defect classification performance effectively.},
booktitle = {Proceedings of the 2019 2nd International Conference on Data Science and Information Technology},
pages = {1–6},
numpages = {6},
keywords = {Ensemble algorithm, Over-sampling, Software defect prediction, Under-sampling},
location = {Seoul, Republic of Korea},
series = {DSIT 2019}
}

@inproceedings{10.1109/SCW63240.2024.00127,
author = {Xu, Wubiao and Huang, Xin and Meng, Shiman and Zhang, Weiping and Guo, Luanzheng and Sato, Kento},
title = {An Efficient Checkpointing System for Large Machine Learning Model Training},
year = {2025},
isbn = {9798350355543},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SCW63240.2024.00127},
doi = {10.1109/SCW63240.2024.00127},
abstract = {Checkpointing is one of the fundamental techniques to resume training while system fails. It has been generally used in various domains, such as high-performance computing (HPC) and machine learning (ML). However, as machine learning models increase in size and complexity rapidly, the cost of checkpointing in ML training became a bottleneck in storage and performance (time). For example, the latest GPT-4 model has massive parameters at the scale of 1.76 trillion. It is highly time and storage consuming to frequently writes the model to checkpoints with more than 1 trillion floating point values to storage. This work aims to understand and attempt to mitigate this problem. First, we characterize the checkpointing interface in a collection of representative large machine learning/language models with respect to storage consumption and performance overhead. Second, we propose the two optimizations: i) A periodic cleaning strategy that periodically cleans up outdated checkpoints to reduce the storage burden; ii) A data staging optimization that coordinates checkpoints between local and shared file systems for performance improvement. The experimental results with GPT-2 variants show that, overall the proposed optimizations significantly reduce the storage consumption to a constant while improves performance by average 2.1\texttimes{} for checkpointing in GPT-2 training.},
booktitle = {Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {896–900},
numpages = {5},
location = {Atlanta, GA, USA},
series = {SC-W '24}
}

@inproceedings{10.1145/3183440.3183449,
author = {Eken, Beyza},
title = {Assessing personalized software defect predictors},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3183449},
doi = {10.1145/3183440.3183449},
abstract = {Software defect prediction models guide developers and testers to identify defect prone software modules in fewer time and effort, compared to manual inspections of the source code. The state-of-the-art predictors on publicly available software engineering data could catch around 70% of the defects. While early studies mostly utilize static code properties of the software, recent studies incorporate the people factor into the prediction models, such as the number of developers that touched a code unit, the experience of the developer, and interaction and cognitive behaviors of developers. Those information could give a stronger clue about the defect-prone parts because they could explain defect injection patterns in software development. Personalization has been emerging in many other systems such as social platforms, web search engines such that people get customized recommendations based on their actions, profiles and interest. Following this point of view, customization in defect prediction with respect to each developer would increase predictions' accuracy and usefulness than traditional, general models. In this thesis, we focus on building a personalized defect prediction framework that gives instant feedback to the developer at change level, based on historical defect and change data. Our preliminary analysis of the personalized prediction models of 121 developers in six open source projects indicate that, a personalized approach is not always the best model when compared to general models built for six projects. Other factors such as project characteristics, developer's historical data, the context and frequency of contributions, and/or development methodologies might affect which model to consider in practice. Eventually, this topic is open to improvement with further empirical studies on each of these factors.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {488–491},
numpages = {4},
keywords = {bug prediction, customization, personalized defect prediction},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3644032.3644467,
author = {Khan, Md Asif and Azim, Akramul and Liscano, Ramiro and Smith, Kevin and Chang, Yee-Kang and Tauseef, Qasim and Seferi, Gkerta},
title = {Machine Learning-based Test Case Prioritization using Hyperparameter Optimization},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644467},
doi = {10.1145/3644032.3644467},
abstract = {Continuous integration pipelines execute extensive automated test suites to validate new software builds. In this fast-paced development environment, delivering timely testing results to developers is critical to ensuring software quality. Test case prioritization (TCP) emerges as a pivotal solution, enabling the prioritization of fault-prone test cases for immediate attention. Recent advancements in machine learning have showcased promising results in TCP, offering the potential to revolutionize how we optimize testing workflows. Hyperparameter tuning plays a crucial role in enhancing the performance of ML models. However, there needs to be more work investigating the effects of hyperparameter tuning on TCP. Therefore, we explore how optimized hyperparameters influence the performance of various ML classifiers, focusing on the Average Percentage of Faults Detected (APFD) metric. Through empirical analysis of ten real-world, large-scale, diverse datasets, we conduct a grid search-based tuning with 885 hyperparameter combinations for four machine learning models. Our results provide model-specific insights and demonstrate an average 15% improvement in model performance with hyperparameter tuning compared to default settings. We further explain how hyperparameter tuning improves precision (max = 1), recall (max = 0.9633), F1-score (max = 0.9662), and influences APFD value (max = 0.9835), indicating a direct connection between tuning and prioritization performance. Hence, this study underscores the importance of hyperparameter tuning in optimizing failure prediction models and their direct impact on prioritization performance.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {125–135},
numpages = {11},
keywords = {hyperparameter optimization, test case prioritization, machine learning, continuous integration},
location = {Lisbon, Portugal},
series = {AST '24}
}

@inproceedings{10.1145/3652620.3688201,
author = {Meijer, Willem},
title = {Contract-based Validation of Conceptual Design Bugs for Engineering Complex Machine Learning Software},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688201},
doi = {10.1145/3652620.3688201},
abstract = {Context. Modern software systems increasingly commonly contain one or multiple machine learning (ML) components. Current development practices are generally on a trial-and-error basis, posing a significant risk of introducing bugs. One type of bug is the "conceptual design bug," referring to a misunderstanding between the properties of input data and prerequisites imposed by ML algorithms (e.g., using unscaled data in a scale-sensitive algorithm). These bugs are challenging to test at design time, causing problems at runtime through crashes, noticeably poor model performance, or not at all, threatening the system's robustness and transparency. Objective. In this work, I propose the line of research I intend to pursue during my PhD, addressing conceptual design bugs in complex ML software from a prevention-oriented perspective. I intend to build open-source tooling for ML engineers that can be used to detect conceptual design bugs, enabling them to make quality assurances about their system design's robustness. Approach. We need to understand conceptual bugs beyond the status quo, identifying their types, prevalence, impacts, and structural elements in the code. We operationalize this knowledge into a tool that detects them at design time, allowing ML engineers to resolve them before running their code and wasting resources. We anticipate this tool will leverage contract-based validation applied to partial ML software models. Evaluation. We plan to evaluate the built tool two-fold using professional (industrial) ML software. First, we will study its effectiveness regarding bug detection at design time, identifying whether it fulfills its functional objective. Second, we will study its usability, identifying whether ML engineers benefit when tools like this are introduced into their ML engineering workflow.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {155–161},
numpages = {7},
keywords = {machine learning, software bugs, software design, knowledge mining, software contracts, empirical software engineering},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@article{10.1145/3572905,
author = {Kotti, Zoe and Galanopoulou, Rafaila and Spinellis, Diomidis},
title = {Machine Learning for Software Engineering: A Tertiary Study},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3572905},
doi = {10.1145/3572905},
abstract = {Machine learning (ML) techniques increase the effectiveness of software engineering (SE) lifecycle activities. We systematically collected, quality-assessed, summarized, and categorized 83 reviews in ML for SE published between 2009 and 2022, covering 6,117 primary studies. The SE areas most tackled with ML are software quality and testing, while human-centered areas appear more challenging for ML. We propose a number of ML for SE research challenges and actions, including conducting further empirical validation and industrial studies on ML, reconsidering deficient SE methods, documenting and automating data collection and pipeline processes, reexamining how industrial practitioners distribute their proprietary data, and implementing incremental ML approaches.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {256},
numpages = {39},
keywords = {Tertiary study, machine learning, software engineering, systematic literature review}
}

@inproceedings{10.1145/3639478.3643069,
author = {Haldar, Susmita and Capretz, Luiz Fernando},
title = {Interpretable Software Maintenance and Support Effort Prediction Using Machine Learning},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643069},
doi = {10.1145/3639478.3643069},
abstract = {Software maintenance and support efforts consume a significant amount of the software project budget to operate the software system in its expected quality. Manually estimating the total hours required for this phase can be very time-consuming, and often differs from the actual cost that is incurred. The automation of these estimation processes can be implemented with the aid of machine learning algorithms. The maintenance and support effort prediction models need to be explainable so that project managers can understand which features contributed to the model outcome. This study contributes to the development of the maintenance and support effort prediction model using various tree-based regression machine-learning techniques from cross-company project information. The developed models were explained using the state-of-the-art model agnostic technique SHapley Additive Explanations (SHAP) to understand the significance of features from the developed model. This study concluded that staff size, application size, and number of defects are major contributors to the maintenance and support effort prediction models.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {288–289},
numpages = {2},
keywords = {maintenance and support effort prediction, explainable machine learning models, model agnostic interpretation},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00070,
author = {Bhutamapuram, Umamaheswara Sharma},
title = {Some Investigations of Machine Learning Models for Software Defects},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00070},
doi = {10.1109/ICSE-Companion58688.2023.00070},
abstract = {Software defect prediction (SDP) and software defect severity prediction (SDSP) models alleviate the burden on the testers by providing the automatic assessment of a newly-developed program in a short amount of time. The research on defect prediction or defect severity prediction is primarily focused on proposing classification frameworks or addressing challenges in developing prediction models; however, the primary yet significant gap in the literature is interpreting the predictions in terms of project objectives. Furthermore, the literature indicates that these models have poor predictive performance. In this thesis, we investigate the use of a diversity-based ensemble learning mechanism for the cross-project defect prediction (CPDP) task and self-training semi-supervised learning for the software defect severity prediction, respectively, for obtaining better prediction performances. We also propose a few project-specific performance measures to interpret the predictions in terms of project objectives (such as a reduction in expenditure, time, and failure chances). Through the empirical analysis, we observe that (1) the diversity-based ensemble learning mechanism improves the prediction performance in terms of both the traditional and proposed measures, and (2) the self-training semi-supervised learning model has a positive impact on predicting the severity of a defective module.Once a potential prediction model is developed, any software organisation may utilise its services. How can an organisation showcase their trust in the developed prediction model? To this end, we investigate the feasibility of SDP models in real-world testing environments by providing proofs using the probabilistic bounds. The proofs summarised show that even if the prediction model has a lower failure probability, the probability of obtaining fewer failures in SDP-tested software than in similar but manually tested software is still exponentially small. This result enables the researchers in SDP to avoid proposing prediction models.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {259–263},
numpages = {5},
keywords = {software defect prediction, cross-project defect prediction, software defect severity prediction, software reliability, performance measures, feasibility study},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3178212.3178221,
author = {Rizwan, Syed and Tiantian, Wang and Xiaohong, Su and Salahuddin},
title = {Empirical Study on Software Bug Prediction},
year = {2017},
isbn = {9781450354882},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178212.3178221},
doi = {10.1145/3178212.3178221},
abstract = {Software defect prediction is a vital research direction in software engineering field. Software defect prediction predicts whether software errors are present in the software by using machine learning analysis on software metrics. It can help software developers to improve the quality of the software. Software defect prediction is usually a binary classification problem, which relies on software metrics and the use of classifiers. There have been many research efforts to improve accuracy in software defect prediction using a variety of classifiers and data preprocessing techniques. However, the "classic classifier validity" and "data preprocessing techniques can enhance the functionality of software defect prediction" has not yet been answered explicitly. Therefore, it is necessary to conduct an empirical analysis to compare these studies. In software defect prediction, the category of interest is a defective module, and the number of defective modules is much less than that of a non-defective module in data. This leads to a category of imbalance problem that reduces the accuracy of the prediction. Therefore, the problem of imbalance is a key problem that needs to be solved in software defect prediction. In this paper, we proposed an experimental model and used the NASA MDP data set to analyze the software defect prediction. Five research questions were defined and analyzed experimentally. In addition to experimental analysis, this paper focuses on the improvement of SMOTE. SMOTE ASMO algorithm has been proposed to overcome the shortcomings of SMOTE.},
booktitle = {Proceedings of the 2017 International Conference on Software and E-Business},
pages = {55–59},
numpages = {5},
keywords = {Classification, Data preprocessing, Defect prediction, SMOTE},
location = {Hong Kong, Hong Kong},
series = {ICSEB '17}
}

@inproceedings{10.1145/3127005.3127017,
author = {Osman, Haidar and Ghafari, Mohammad and Nierstrasz, Oscar and Lungu, Mircea},
title = {An Extensive Analysis of Efficient Bug Prediction Configurations},
year = {2017},
isbn = {9781450353052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127005.3127017},
doi = {10.1145/3127005.3127017},
abstract = {Background: Bug prediction helps developers steer maintenance activities towards the buggy parts of a software. There are many design aspects to a bug predictor, each of which has several options, i.e., software metrics, machine learning model, and response variable.Aims: These design decisions should be judiciously made because an improper choice in any of them might lead to wrong, misleading, or even useless results. We argue that bug prediction configurations are intertwined and thus need to be evaluated in their entirety, in contrast to the common practice in the field where each aspect is investigated in isolation.Method: We use a cost-aware evaluation scheme to evaluate 60 different bug prediction configuration combinations on five open source Java projects.Results: We find out that the best choices for building a cost-effective bug predictor are change metrics mixed with source code metrics as independent variables, Random Forest as the machine learning model, and the number of bugs as the response variable. Combining these configuration options results in the most efficient bug predictor across all subject systems.Conclusions: We demonstrate a strong evidence for the interplay among bug prediction configurations and provide concrete guidelines for researchers and practitioners on how to build and evaluate efficient bug predictors.},
booktitle = {Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {107–116},
numpages = {10},
keywords = {Bug Prediction, Effort-Aware Evaluation},
location = {Toronto, Canada},
series = {PROMISE}
}

@inproceedings{10.1145/3643659.3643927,
author = {Dobslaw, Felix and Feldt, Robert},
title = {Automated Boundary Identification for Machine Learning Classifiers},
year = {2024},
isbn = {9798400705625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643659.3643927},
doi = {10.1145/3643659.3643927},
abstract = {AI and Machine Learning (ML) models are increasingly used as (critical) components in software systems, even safety-critical ones. This puts new demands on the degree to which we need to test them and requires new and expanded testing methods. Recent boundary-value identification methods have been developed and shown to automatically find boundary candidates for traditional, non-ML software: pairs of nearby inputs that result in (highly) differing outputs. These can be shown to developers and testers, who can judge if the boundary is where it is supposed to be.Here, we explore how this method can identify decision boundaries of ML classification models. The resulting ML Boundary Spanning Algorithm (ML-BSA) is a search-based method extending previous work in two main ways. We empirically evaluate ML-BSA on seven ML datasets and show that it better spans and thus better identifies the entire classification boundary(ies). The diversity objective helps spread out the boundary pairs more broadly and evenly. This, we argue, can help testers and developers better judge where a classification boundary actually is, compare to expectations, and then focus further testing, validation, and even further training and model refinement on parts of the boundary where behaviour is not ideal.},
booktitle = {Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing},
pages = {1–8},
numpages = {8},
location = {Lisbon, Portugal},
series = {SBFT '24}
}

@inproceedings{10.1145/3379247.3379278,
author = {Ahmed, Md. Razu and Ali, Md. Asraf and Ahmed, Nasim and Zamal, Md. Fahad Bin and Shamrat, F.M. Javed Mehedi},
title = {The Impact of Software Fault Prediction in Real-World Application: An Automated Approach for Software Engineering},
year = {2020},
isbn = {9781450376730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379247.3379278},
doi = {10.1145/3379247.3379278},
abstract = {Software fault prediction and proneness has long been considered as a critical issue for the tech industry and software professionals. In the traditional techniques, it requires previous experience of faults or a faulty module while detecting the software faults inside an application. An automated software fault recovery models enable the software to significantly predict and recover software faults using machine learning techniques. Such ability of the feature makes the software to run more effectively and reduce the faults, time and cost. In this paper, we proposed a software defect predictive development models using machine learning techniques that can enable the software to continue its projected task. Moreover, we used different prominent evaluation benchmark to evaluate the model's performance such as ten-fold cross-validation techniques, precision, recall, specificity, f 1 measure, and accuracy. This study reports a significant classification performance of 98-100% using SVM on three defect datasets in terms of f1 measure. However, software practitioners and researchers can attain independent understanding from this study while selecting automated task for their intended application.},
booktitle = {Proceedings of 2020 6th International Conference on Computing and Data Engineering},
pages = {247–251},
numpages = {5},
keywords = {Defect prediction, Machine learning, Software engineering, Software fault},
location = {Sanya, China},
series = {ICCDE '20}
}

@inproceedings{10.1109/ICSE43902.2021.00050,
author = {Shrikanth, N. C. and Majumder, Suvodeep and Menzies, Tim},
title = {Early Life Cycle Software Defect Prediction: Why? How?},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00050},
doi = {10.1109/ICSE43902.2021.00050},
abstract = {Many researchers assume that, for software analytics, "more data is better." We write to show that, at least for learning defect predictors, this may not be true.To demonstrate this, we analyzed hundreds of popular GitHub projects. These projects ran for 84 months and contained 3,728 commits (median values). Across these projects, most of the defects occur very early in their life cycle. Hence, defect predictors learned from the first 150 commits and four months perform just as well as anything else. This means that, at least for the projects studied here, after the first few months, we need not continually update our defect prediction models.We hope these results inspire other researchers to adopt a "simplicity-first" approach to their work. Some domains require a complex and data-hungry analysis. But before assuming complexity, it is prudent to check the raw data looking for "short cuts" that can simplify the analysis.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {448–459},
numpages = {12},
keywords = {analytics, defect prediction, early, sampling},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1145/3092566,
author = {Ghaffarian, Seyed Mohammad and Shahriari, Hamid Reza},
title = {Software Vulnerability Analysis and Discovery Using Machine-Learning and Data-Mining Techniques: A Survey},
year = {2017},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3092566},
doi = {10.1145/3092566},
abstract = {Software security vulnerabilities are one of the critical issues in the realm of computer security. Due to their potential high severity impacts, many different approaches have been proposed in the past decades to mitigate the damages of software vulnerabilities. Machine-learning and data-mining techniques are also among the many approaches to address this issue. In this article, we provide an extensive review of the many different works in the field of software vulnerability analysis and discovery that utilize machine-learning and data-mining techniques. We review different categories of works in this domain, discuss both advantages and shortcomings, and point out challenges and some uncharted territories in the field.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {56},
numpages = {36},
keywords = {Software vulnerability analysis, data-mining, machine-learning, review, software security, software vulnerability discovery, survey}
}

@inproceedings{10.1145/3524304.3524310,
author = {Abaei, Golnoush and Tah, Wen Zhong and Toh, Jason Zhern Wee and Hor, Ethan Sheng Jian},
title = {Improving software fault prediction in imbalanced datasets using the under-sampling approach},
year = {2022},
isbn = {9781450385770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524304.3524310},
doi = {10.1145/3524304.3524310},
abstract = {To make most software defect-free, a considerable amount of budget needs to be allocated to the software testing phase. As each day goes by, this budget slowly rises, as most software grows in size and complexity, which causes an issue for specific companies that cannot allocate sufficient resources towards testing. To tackle this, many researchers use machine learning methods to create software fault prediction models that can help detect defect-prone modules so that resources can be allocated more efficiently during testing. Although this is a feasible plan, the effectiveness of these machine learning models also depends on a few factors, such as the issue of data imbalance. There are many known techniques in class imbalance research that can potentially improve the performance of prediction models through processing the dataset before providing it as input. However, not all methods are compatible with one another. Before building a prediction model, the dataset undergoes the preprocessing step, the under-sampling, and the feature selection process. This study uses an under-sampling process by employing the Instance Hardness Threshold (IHT), which reduces the number of data present in the majority class. The performance of the proposed approach is evaluated based on eight machine learning algorithms by applying it to eight moderate and highly imbalanced NASA datasets. The results of our proposed approach show improvement in AUC and F1-Score by 33% and 26%, respectively, compared to other research work in some datasets.},
booktitle = {Proceedings of the 2022 11th International Conference on Software and Computer Applications},
pages = {41–47},
numpages = {7},
keywords = {Imbalanced Dataset, Software Fault Prediction, Testing, Under-sampling},
location = {Melaka, Malaysia},
series = {ICSCA '22}
}

@proceedings{10.1145/3696687,
title = {MLPRAE '24: Proceedings of the International Conference on Machine Learning, Pattern Recognition and Automation Engineering},
year = {2024},
isbn = {9798400709876},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3172871.3172872,
author = {Kumar, Lov and Sureka, Ashish},
title = {Feature Selection Techniques to Counter Class Imbalance Problem for Aging Related Bug Prediction: Aging Related Bug Prediction},
year = {2018},
isbn = {9781450363983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172871.3172872},
doi = {10.1145/3172871.3172872},
abstract = {Aging-Related Bugs (ARBs) occur in long running systems due to error conditions caused because of accumulation of problems such as memory leakage or unreleased files and locks. Aging-Related Bugs are hard to discover during software testing and also challenging to replicate. Automatic identification and prediction of aging related fault-prone files and classes in an object oriented system can help the software quality assurance team to optimize their testing efforts. In this paper, we present a study on the application of static source code metrics and machine learning techniques to predict aging related bugs. We conduct a series of experiments on publicly available dataset from two large open-source software systems: Linux and MySQL. Class imbalance and high dimensionality are the two main technical challenges in building effective predictors for aging related bugs.We investigate the application of five different feature selection techniques (OneR, Information Gain, Gain Ratio, RELEIF and Symmetric Uncertainty) for dimensionality reduction and five different strategies (Random Under-sampling, Random Oversampling, SMOTE, SMOTEBoost and RUSBoost) to counter the effect of class imbalance in our proposed machine learning based solution approach. Experimental results reveal that the random under-sampling approach performs best followed by RUSBoost in-terms of the mean AUC metric. Statistical significance test demonstrates that there is a significant difference between the performance of the various feature selection techniques. Experimental results shows that Gain Ratio and RELEIF performs best in comparison to other strategies to address the class imbalance problem. We infer from the statistical significance test that there is no difference between the performances of the five different learning algorithms.},
booktitle = {Proceedings of the 11th Innovations in Software Engineering Conference},
articleno = {2},
numpages = {11},
keywords = {Aging Related Bugs, Empirical Software Engineering, Feature Selection Techniques, Imbalance Learning, Machine Learning, Predictive Modeling, Software Maintenance, Source Code Metrics},
location = {Hyderabad, India},
series = {ISEC '18}
}

@inproceedings{10.1145/3624032.3624039,
author = {Nascimento, Lidia Perside Gomes and Prud\^{e}ncio, Ricardo Bastos Cavalcante and Mota, Alexandre Cabral and Filho, Audir de Araujo Paiva and Cruz, Pedro Henrique Alves and Oliveira, Daniel Cardoso Coelho Alves de and Moreira, Pedro Roncoli Sarmet},
title = {Machine Learning Techniques for Escaped Defect Analysis in Software Testing},
year = {2023},
isbn = {9798400716294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624032.3624039},
doi = {10.1145/3624032.3624039},
abstract = {Software testing is crucial to ensure the quality of a software under development. Once a potential bug is identified, a Bug Report (BR) is opened with information to describe and reproduce the found issue. Usually in big companies, hundreds of BRs are opened weekly by different testing teams, which have to be inspected and fixed adequately. This paper is focused on the use of Machine Learning (ML) techniques to automate the Escaped Defect Analysis (EDA), which is an important (but expensive) task to improve the effectiveness of the testing teams. In our work, Escaped Defects (EDs) are bugs or issues that should have been opened by a specific team, but which was accidentally found by another team. The occurrence of EDs is risky, as it is usually related to failures in the testing activities. EDA is usually performed manually by software engineers, who read each BR’s textual content to judge whether it is an ED or not. This is challenging and time-consuming. In our solution, the BR’s content is preprocessed by textual operations and then a feature representation is adopted by a ML classifier to return the probability of EDA labels. Experiments were performed in a dataset of 3767 BRs provided by the Motorola Mobility Com\'{e}rcio de Produtos Eletr\^{o}nicos Ltda. Different ML algorithms were adopted to build classifiers, obtaining high AUC values (usually higher than 0.8), in a cross-validation experiment. This result indicates a good trade-off between the number of EDs correctly identified and the number of BRs that have to be actually inspected in the EDA process. This paper presents a ML based approach to classify escaped defects described in bug reports. EDs are bugs missed by the QA team in charge and happened to be uncovered by a different team. To automate the identification of EDs (a costly and error-prone task), a dataset of a partner company is leveraged, text processing operators are adopted for feature engineering and 6 classical ML algorithms are applied. The results show satisfactory accuracy and AUC and the experiments indicate a good trade-off between the number of EDs correctly identified and the number of BRs that have to be inspected in the EDA.},
booktitle = {Proceedings of the 8th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {47–53},
numpages = {7},
keywords = {Bug Reports, Escaped Defect Analysis, Machine Learning},
location = {Campo Grande, MS, Brazil},
series = {SAST '23}
}

@inproceedings{10.1145/3474124.3474127,
author = {Rajnish, Kumar and Bhattacharjee, Vandana and Chandrabanshi, Vishnu},
title = {Applying Cognitive and Neural Network Approach over Control Flow Graph for Software Defect Prediction},
year = {2021},
isbn = {9781450389204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474124.3474127},
doi = {10.1145/3474124.3474127},
booktitle = {Proceedings of the 2021 Thirteenth International Conference on Contemporary Computing},
pages = {13–17},
numpages = {5},
keywords = {CFGs, Cognitive Complexity, Cognitive Measures, Graph Convolutional Network, Neural Network, Software Defect Prediction},
location = {Noida, India},
series = {IC3-2021}
}

@article{10.1145/3699711,
author = {Shiri Harzevili, Nima and Boaye Belle, Alvine and Wang, Junjie and Wang, Song and Jiang, Zhen Ming (Jack) and Nagappan, Nachiappan},
title = {A Systematic Literature Review on Automated Software Vulnerability Detection Using Machine Learning},
year = {2024},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3699711},
doi = {10.1145/3699711},
abstract = {In recent years, numerous Machine Learning (ML) models, including Deep Learning (DL) and classic ML models, have been developed to detect software vulnerabilities. However, there is a notable lack of comprehensive and systematic surveys that summarize, classify, and analyze the applications of these ML models in software vulnerability detection. This absence may lead to critical research areas being overlooked or under-represented, resulting in a skewed understanding of the current state of the art in software vulnerability detection. To close this gap, we propose a comprehensive and systematic literature review that characterizes the different properties of ML-based software vulnerability detection systems using six major Research Questions (RQs).Using a custom web scraper, our systematic approach involves extracting a set of studies from four widely used online digital libraries: ACM Digital Library, IEEE Xplore, ScienceDirect, and Google Scholar. We manually analyzed the extracted studies to filter out irrelevant work unrelated to software vulnerability detection, followed by creating taxonomies and addressing RQs. Our analysis indicates a significant upward trend in applying ML techniques for software vulnerability detection over the past few years, with many studies published in recent years. Prominent conference venues include the International Conference on Software Engineering (ICSE), the International Symposium on Software Reliability Engineering (ISSRE), the Mining Software Repositories (MSR) conference, and the ACM International Conference on the Foundations of Software Engineering (FSE), whereas Information and Software Technology (IST), Computers &amp; Security (C&amp;S), and Journal of Systems and Software (JSS) are the leading journal venues.Our results reveal that 39.1% of the subject studies use hybrid sources, whereas 37.6% of the subject studies utilize benchmark data for software vulnerability detection. Code-based data are the most commonly used data type among subject studies, with source code being the predominant subtype. Graph-based and token-based input representations are the most popular techniques, accounting for 57.2% and 24.6% of the subject studies, respectively. Among the input embedding techniques, graph embedding and token vector embedding are the most frequently used techniques, accounting for 32.6% and 29.7% of the subject studies. Additionally, 88.4% of the subject studies use DL models, with recurrent neural networks and graph neural networks being the most popular subcategories, whereas only 7.2% use classic ML models. Among the vulnerability types covered by the subject studies, CWE-119, CWE-20, and CWE-190 are the most frequent ones. In terms of tools used for software vulnerability detection, Keras with TensorFlow backend and PyTorch libraries are the most frequently used model-building tools, accounting for 42 studies for each. In addition, Joern is the most popular tool used for code representation, accounting for 24 studies.Finally, we summarize the challenges and future directions in the context of software vulnerability detection, providing valuable insights for researchers and practitioners in the field.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {55},
numpages = {36},
keywords = {Source code, software security, software vulnerability detection, software bug detection, machine learning, deep learning}
}

@inproceedings{10.5555/3340730.3340750,
author = {Catolino, Gemma and Di Nucci, Dario and Ferrucci, Filomena},
title = {Cross-project just-in-time bug prediction for mobile apps: an empirical assessment},
year = {2019},
publisher = {IEEE Press},
abstract = {Bug Prediction is an activity aimed at identifying defect-prone source code entities that allows developers to focus testing efforts on specific areas of software systems. Recently, the research community proposed Just-in-Time (JIT) Bug Prediction with the goal of detecting bugs at commit-level. While this topic has been extensively investigated in the context of traditional systems, to the best of our knowledge, only a few preliminary studies assessed the performance of the technique in a mobile environment, by applying the metrics proposed by Kamei et al. in a within-project scenario. The results of these studies highlighted that there is still room for improvement. In this paper, we faced this problem to understand (i) which Kamei et al.'s metrics are useful in the mobile context, (ii) if different classifiers impact the performance of cross-project JIT bug prediction models and (iii) whether the application of ensemble techniques improves the capabilities of the models. To carry out the experiment, we first applied a feature selection technique, i.e., InfoGain, to filter relevant features and avoid models multicollinearity. Then, we assessed and compared the performance of four different well-known classifiers and four ensemble techniques. Our empirical study involved 14 apps and 42,543 commits extracted from the Commit Guru platform. The results show that Naive Bayes achieves the best performance with respect to the other classifiers and in some cases outperforms some well-known ensemble techniques.},
booktitle = {Proceedings of the 6th International Conference on Mobile Software Engineering and Systems},
pages = {99–110},
numpages = {12},
keywords = {JIT bug prediction, empirical study, metrics},
location = {Montreal, Quebec, Canada},
series = {MOBILESoft '19}
}

@article{10.1145/3622806,
author = {Wan, Chengcheng and Liu, Yuhan and Du, Kuntai and Hoffmann, Henry and Jiang, Junchen and Maire, Michael and Lu, Shan},
title = {Run-Time Prevention of Software Integration Failures of Machine Learning APIs},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622806},
doi = {10.1145/3622806},
abstract = {Due to the under-specified interfaces, developers face challenges in correctly integrating machine learning (ML) APIs in software. Even when the ML API and the software are well designed on their own, the resulting application misbehaves when the API output is incompatible with the software. It is desirable to have an adapter that converts ML API output at runtime to better fit the software need and prevent integration failures.  
In this paper, we conduct an empirical study to understand ML API integration problems in real-world applications. Guided by this study, we present SmartGear, a tool that automatically detects and converts mismatching or incorrect ML API output at run time, serving as a middle layer between ML API and software. Our evaluation on a variety of open-source applications shows that SmartGear detects 70% incompatible API outputs and prevents 67% potential integration failures, outperforming alternative solutions.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {231},
numpages = {28},
keywords = {machine learning API, run-time patching, software integration failure}
}

@inproceedings{10.1145/3520084.3520091,
author = {Wei, Wei and Jiang, Feng and Yu, Xu and Du, Junwei},
title = {An Under-sampling Algorithm Based on Weighted Complexity and Its Application in Software Defect Prediction},
year = {2022},
isbn = {9781450395519},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520084.3520091},
doi = {10.1145/3520084.3520091},
abstract = {The under-sampling technique is an important method to solve the class imbalance issue in software defect prediction. However, the existing under-sampling methods generally ignore the problem that there are great differences in the complexities of different samples. In fact, the complexities of samples can play an important role in defect prediction, since there is a close relation between the complexities of samples and whether they have defects. Therefore, when we use the under-sampling technique to handle the class imbalance issue in software defect prediction, it is necessary to consider the complexities of samples. In this paper, we propose the notion of weighted complexity. When calculating the weighted complexity of each sample, the weights of different condition attributes are considered. Based on the weighted complexity, we propose a new under-sampling algorithm, called WCP-UnderSampler, and apply it to software defect prediction. In WCP-UnderSampler, we first employ the granularity decision entropy in rough sets to calculate the significance and the weight of each condition attribute; Second, the weighted complexity of each sample is obtained by calculating the weighted sum of the values of the sample on all attributes; Third, the majority class samples are sorted in descending order according to their weighted complexities, and the majority class samples with higher complexities are selected until a balanced data set is obtained. Experiments on defect prediction data sets show that we can obtain better software defect prediction results by using WCP-UnderSampler to handle the imbalanced data.},
booktitle = {Proceedings of the 2022 5th International Conference on Software Engineering and Information Management},
pages = {38–44},
numpages = {7},
keywords = {Granularity decision entropy, Rough set, Software defect prediction, Unbalanced data, Under sampling, Weighted complexity},
location = {Yokohama, Japan},
series = {ICSIM '22}
}

@inproceedings{10.1145/3374549.3374553,
author = {Zong, Liang},
title = {Classification Based Software Defect Prediction Model for Finance Software System - An Industry Study},
year = {2020},
isbn = {9781450376495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374549.3374553},
doi = {10.1145/3374549.3374553},
abstract = {Automated software defect prediction is an important and fundamental activity in the domain of software development. Successful software defect prediction can save testing effort thus reduce the time and cost for software development. However, software systems for finance company are inherently large and complex with numerous interfaces with other systems. Thus, identifying and selecting a good model and a set of features is important but challenging problem. In our paper, we first define the problem we want to solve. Then we propose a prediction model based on binary classification and a set of novel features, which is more specific for finance software systems. We collected 15 months real production data and labelled it as our dataset. The experiment shows our model and features can give a better prediction accuracy for finance systems. In addition, we demonstrate how our prediction model helps improve our production quality further. Unlike other research papers, our proposal focuses to solve problem in real finance industry.},
booktitle = {Proceedings of the 2019 3rd International Conference on Software and E-Business},
pages = {60–65},
numpages = {6},
keywords = {Faulty change, Finance system, Machine learning, Software defect prediction},
location = {Tokyo, Japan},
series = {ICSEB '19}
}

@inproceedings{10.1145/3549034.3570200,
author = {Brun, Yuriy},
title = {The promise and perils of using machine learning when engineering software (keynote paper)},
year = {2022},
isbn = {9781450394567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549034.3570200},
doi = {10.1145/3549034.3570200},
abstract = {Machine learning has radically changed what computing can accomplish, 
including the limits of what software engineering can do. I will discuss 
recent software engineering advances machine learning has enabled, from 
automatically repairing software bugs to data-driven software systems that 
automatically learn to make decisions. Unfortunately, with the promises of 
these new technologies come serious perils. For example, automatically 
generated program patches can break as much functionality as they repair. And 
self-learning, data-driven software can make decisions that result in 
unintended consequences, including unsafe, racist, or sexist behavior. But to 
build solutions to these shortcomings we may need to look no further than 
machine learning itself. I will introduce multiple ways machine learning can 
help verify software properties, leading to higher-quality systems.},
booktitle = {Proceedings of the 6th International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {1–4},
numpages = {4},
keywords = {Machine learning and software engineering},
location = {Singapore, Singapore},
series = {MaLTeSQuE 2022}
}

@inproceedings{10.1109/ASE.2019.00071,
author = {Gong, Lina and Jiang, Shujuan and Wang, Rongcun and Jiang, Li},
title = {Empirical evaluation of the impact of class overlap on software defect prediction},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00071},
doi = {10.1109/ASE.2019.00071},
abstract = {Software defect prediction (SDP) utilizes the learning models to detect the defective modules in project, and their performance depends on the quality of training data. The previous researches mainly focus on the quality problems of class imbalance and feature redundancy. However, training data often contains some instances that belong to different class but have similar values on features, and this leads to class overlap to affect the quality of training data. Our goal is to investigate the impact of class overlap on software defect prediction. At the same time, we propose an improved K-Means clustering cleaning approach (IKMCCA) to solve both the class overlap and class imbalance problems. Specifically, we check whether K-Means clustering cleaning approach (KMCCA) or neighborhood cleaning learning (NCL) or IKMCCA is feasible to improve defect detection performance for two cases (i) within-project defect prediction (WPDP) (ii) cross-project defect prediction (CPDP). To have an objective estimate of class overlap, we carry out our investigations on 28 open source projects, and compare the performance of state-of-the-art learning models for the above-mentioned cases by using IKMCCA or KMCCA or NCL VS. without cleaning data. The experimental results make clear that learning models obtain significantly better performance in terms of balance, Recall and AUC for both WPDP and CPDP when the overlapping instances are removed. Moreover, it is better to consider both class overlap and class imbalance.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {698–709},
numpages = {12},
keywords = {K-Means clustering, class overlap, machine learning, software defect prediction},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/3627106.3627188,
author = {Ganz, Tom and Imgrund, Erik and H\"{a}rterich, Martin and Rieck, Konrad},
title = {PAVUDI: Patch-based Vulnerability Discovery using Machine Learning},
year = {2023},
isbn = {9798400708862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627106.3627188},
doi = {10.1145/3627106.3627188},
abstract = {Machine learning has been increasingly adopted for automatic security vulnerability discovery in research and industry. The ability to automatically identify and prioritize bugs in patches is crucial to organizations seeking to defend against potential threats. Previous works, however only consider bug discovery on statement, function or file level. How one would apply them to patches in realistic scenarios remains unclear. This paper presents a novel deep learning-based approach leveraging an interprocedural patch graph representation and graph neural networks to analyze software patches for identifying and locating potential security vulnerabilities. We modify current state-of-the-art learning-based static analyzers to be applicable to patches and show that our patch-based vulnerability discovery method, a context and flow-sensitive learning-based model, has a more than increased detection performance, is twice as robust against concept drift after model deployment and is particularly better suited for analyzing large patches. In comparison, other methods already lose their efficiency when a patch touches more than five methods.},
booktitle = {Proceedings of the 39th Annual Computer Security Applications Conference},
pages = {704–717},
numpages = {14},
keywords = {Patches, Program Representations, Vulnerability Discovery},
location = {Austin, TX, USA},
series = {ACSAC '23}
}

@inproceedings{10.1145/3194104.3194112,
author = {Di Nucci, Dario and Palomba, Fabio and De Lucia, Andrea},
title = {Evaluating the adaptive selection of classifiers for cross-project bug prediction},
year = {2018},
isbn = {9781450357234},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194104.3194112},
doi = {10.1145/3194104.3194112},
abstract = {Bug prediction models are used to locate source code elements more likely to be defective. One of the key factors influencing their performances is related to the selection of a machine learning method (a.k.a., classifier) to use when discriminating buggy and non-buggy classes. Given the high complementarity of stand-alone classifiers, a recent trend is the definition of ensemble techniques, which try to effectively combine the predictions of different stand-alone machine learners. In a recent work we proposed ASCI, a technique that dynamically selects the right classifier to use based on the characteristics of the class on which the prediction has to be done. We tested it in a within-project scenario, showing its higher accuracy with respect to the Validation and Voting strategy. In this paper, we continue on the line of research, by (i) evaluating ASCI in a global and local cross-project setting and (ii) comparing its performances with those achieved by a stand-alone and an ensemble baselines, namely Naive Bayes and Validation and Voting, respectively. A key finding of our study shows that ASCI is able to perform better than the other techniques in the context of cross-project bug prediction. Moreover, despite local learning is not able to improve the performances of the corresponding models in most cases, it is able to improve the robustness of the models relying on ASCI.},
booktitle = {Proceedings of the 6th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
pages = {48–54},
numpages = {7},
keywords = {bug prediction, cross-project, ensemble classifiers},
location = {Gothenburg, Sweden},
series = {RAISE '18}
}

@article{10.1145/3468744.3468751,
author = {Pinzger, Martin and Giger, Emanuel and Gall, Harald C.},
title = {Comparing fine-grained source code changes and code churn for bug prediction - A retrospective},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3468744.3468751},
doi = {10.1145/3468744.3468751},
abstract = {More than two decades ago, researchers started to mine the data stored in software repositories to help software developers in making informed decisions for developing and testing software systems. Bug prediction was one of the most promising and popular research directions that uses the data stored in software repositories to predict the bug-proneness or number of bugs in source files. On that topic and as part of Emanuel's PhD studies, we submitted a paper with the title Comparing fine-grained source code changes and code churn for bug prediction [8] to the 8th Working Conference on Mining Software Engineering, held 2011 in beautiful Honolulu, Hawaii. Ten years later, it got selected as one of the finalists to receive the MSR 2021 Most Influential Paper Award. In the following, we provide a retrospective on our work, describing the road to publishing this paper, its impact in the field of bug prediction, and the road ahead.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jul,
pages = {21–23},
numpages = {3}
}

@inproceedings{10.1145/3663529.3663785,
author = {Wang, Yiran and L\'{o}pez, Jos\'{e} Antonio Hern\'{a}ndez and Nilsson, Ulf and Varr\'{o}, D\'{a}niel},
title = {Using Run-Time Information to Enhance Static Analysis of Machine Learning Code in Notebooks},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663785},
doi = {10.1145/3663529.3663785},
abstract = {A prevalent method for developing machine learning (ML) prototypes involves the use of notebooks. Notebooks are sequences of cells containing both code and natural language documentation. When executed during development, these code cells provide valuable run-time information. Nevertheless, current static analyzers for notebooks do not leverage this run-time information to detect ML bugs. Consequently, our primary proposition in this paper is that harvesting this run-time information in notebooks can significantly improve the effectiveness of static analysis in detecting ML bugs. To substantiate our claim, we focus on bugs related to tensor shapes and conduct experiments using two static analyzers: 1) PYTHIA, a traditional rule-based static analyzer, and 2) GPT-4, a large language model that can also be used as a static analyzer. The results demonstrate that using run-time information in static analyzers enhances their bug detection performance and it also helped reveal a hidden bug in a public dataset.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {497–501},
numpages = {5},
keywords = {large language models, machine learning bugs, notebook, run-time information, static analysis},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.5555/3566055.3566058,
author = {Sendyk, Nicholas and Davies, Curtis and Priscu, Titus and Sutherland, Miles and Madi, Atallah and Dick, Kevin and Khalil, Hoda and Abu Alkheir, Ala and Wainer, Gabriel},
title = {A Task-Agnostic Machine Learning Framework for Dynamic Knowledge Graphs},
year = {2022},
publisher = {IBM Corp.},
address = {USA},
abstract = {Many applications require well-structured and current information to enable downstream tasks. Knowledge graphs are a type of knowl-edge representation that effectively organize current information capturing elements and the relationships between them such that they can be queried and/or reasoned over in more advanced applica-tions. A particular challenge is ensuring that an application-specific knowledge graph is both comprehensive and contains the most current representation, achieved through dynamic updating. Some available software frameworks for managing information as part of a data science pipeline are effective in collecting, labelling, and analysing textual data using natural language processing. Despite the utility of these frameworks, they can nonetheless be daunting for use by industry professionals and/or researchers who may not be familiar with the specifics of each tool. In this work, we present a generalized task-agnostic supervised machine learning frame-work that serves as a streamlined methodology for the creation and dynamic updating of knowledge graphs. A user needs only to define task-specific parameters allowing the tool to scrape data from the internet, generating a candidate corpus. The user may then provide sample annotations from the corpus to train task-specific natural language processing models to extract the relevant knowl-edge graph elements and the relationships connecting them. We demonstrate the utility of this framework for a case study seeking to build knowledge graph representations of merger and acquisition events between companies from scraped online articles reporting these instances. Our task-specific machine learning models achieve upwards of 99.2% F1 score evaluation metric on candidate web page classification and 81.5% F1 score on sentence-level extraction of entity relationships, demonstrating the promise of this framework. Our framework is freely available at: github.com/Checktr/tadkg.},
booktitle = {Proceedings of the 32nd Annual International Conference on Computer Science and Software Engineering},
pages = {22–31},
numpages = {10},
keywords = {fault injection attack, software vulnerability detection, machine learning, software fault injection, software fault model},
location = {Toronto, Canada},
series = {CASCON '22}
}

@inproceedings{10.1145/3691620.3695258,
author = {Vitui, Arthur and Chen, Tse-Hsun},
title = {MLOLET - Machine Learning Optimized Load and Endurance Testing: An industrial experience report},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695258},
doi = {10.1145/3691620.3695258},
abstract = {Load testing is essential for ensuring the performance and stability of modern large-scale systems, which must handle vast numbers of concurrent requests. Traditional load tests, often requiring extensive execution times, are costly and impractical within the short release cycles typical of contemporary software development. In this paper, we present our experience deploying MLOLET, a machine learning optimized load testing framework, at Ericsson. MLOLET addresses key challenges in load testing by determining early stop points for tests and forecasting throughput and response time trends in production environments. By training a time-series model on key performance indicators (KPIs) collected from load tests, MLOLET enables early detection of abnormal system behavior and provides accurate performance forecasting. This capability allows load test engineers to make informed decisions on resource allocation, enhancing both testing efficiency and system reliability. We document the design of MLOLET, its application in industrial settings, and the feedback received from its implementation, highlighting its impact on improving load testing processes and operational performance.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1956–1966},
numpages = {11},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@proceedings{10.1145/3647750,
title = {ICMLSC '24: Proceedings of the 2024 8th International Conference on Machine Learning and Soft Computing},
year = {2024},
isbn = {9798400716546},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3540250.3569451,
author = {Pezz\`{e}, Mauro},
title = {Machine learning and natural language processing for automating software testing (tutorial)},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3569451},
doi = {10.1145/3540250.3569451},
abstract = {In this tutorial, we see how natural language processing and machine learning can help us address the open challenges of software testing. We overview the open challenges of testing autonomous and self-adaptive software systems, discuss the leading-edge technologies that can address the core issues, and see the latest progresses and future prospective of natural language processing and machine learning to cope with core problems.  

Automating test case and oracle generation are still largely open issues. Autonomous and self-adaptive systems, like self-driving cars, smart cities, and smart buildings, raise new issues that further toughen the already challenging scenarios. In the tutorial we understand the growing importance of field testing to address failures that emerge in production, the role of dynamic analysis and deep learning in revealing failure-prone scenarios, the need of symbolic fuzzing to explore unexpected scenarios, and the potentiality of reinforcement learning and natural language processing to generate test cases and oracles. We see in details state-of-the-art approaches that exploit natural language processing to automatically generate executable test oracles, as well as semantic matching, deep and reinforcement learning to automatically generate test cases and reveal failure-prone scenarios in production.  

The tutorial is designed for both researchers, whose research roadmap focuses on software testing and applications of natural language processing and machine learning to software engineering, and practitioners, who see important professional opportunities from autonomous and self-adaptive systems. It is particularly well suited to PhD students and postdoctoral researchers who aim to address new challenges with novel technologies. The tutorial is self-contained, and is designed for a software engineering audience, who many not have a specific background in natural language processing and machine learning.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1821},
numpages = {1},
keywords = {Machine Learning, Natural Language Processing, Software Testing},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3511430.3511437,
author = {Saxena, Amol and Bhatnagar, Roheet and Kumar Srivastava, Devesh},
title = {Effective Lightweight Software Fault Localization based on Test Suite Optimization},
year = {2022},
isbn = {9781450396189},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511430.3511437},
doi = {10.1145/3511430.3511437},
abstract = {Automated software fault localization techniques aid developers in program debugging by identifying the probable locations of faults in a program with minimum human intervention. As software is growing in complexity and scale today, increasing the efficiency of fault localization techniques is very essential in order to reduce the overall software development cost. The effectiveness of the test suites used in the fault localization process has a significant impact on the efficiency of the process. Previous studies, on the other hand, have placed less focus on the adequacy of test suites for the fault localization process. We apply optimized test suites in this paper to improve the performance of software fault localization in a single-fault scenario. For our experiments, we use spectrum-based fault localization (SBFL) techniques. Because of its minimal computing overhead and scalability, spectrum-based fault localization is a popular, efficient, and yet lightweight fault localization technique. To optimize the test suite, we employ a heuristic that asserts that if a faulty statement is executed by a passing test case, that test case will have a negative impact on fault localization performance. In contrast, if a passing test case does not execute the faulty statement, the faulty statement's suspiciousness increases, which has a positive impact on fault localization performance. The test suite optimization approach used in this paper significantly improves fault localization performance, as demonstrated by our experiments. The results show that the proposed method efficiently reduces the number of statements examined by about 84.94 percent on average.},
booktitle = {Proceedings of the 15th Innovations in Software Engineering Conference},
articleno = {9},
numpages = {10},
location = {Gandhinagar, India},
series = {ISEC '22}
}

@article{10.1145/3712198,
author = {Annunziata, Giusy and Lambiase, Stefano and Tamburri, Damian A. and van den Heuvel, Willem-Jan and Palomba, Fabio and Catolino, Gemma and Ferrucci, Filomena and De Lucia, Andrea},
title = {Uncovering Community Smells in Machine Learning-Enabled Systems: Causes, Effects, and Mitigation Strategies},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3712198},
doi = {10.1145/3712198},
abstract = {Successful software development hinges on effective communication and collaboration, which are significantly influenced by human and social dynamics. Poor management of these elements can lead to the emergence of ‘community smells’, i.e., negative patterns in socio-technical interactions that gradually accumulate as ‘social debt’. This issue is particularly pertinent in machine learning-enabled systems, where diverse actors such as data engineers and software engineers interact at various levels. The unique collaboration context of these systems presents an ideal setting to investigate community smells and their impact on development communities. This paper addresses a gap in the literature by identifying the types, causes, effects, and potential mitigation strategies of community smells in machine learning-enabled systems. Using Partial Least Squares Structural Equation Modeling (PLS-SEM), we developed hypotheses based on existing literature and interviews, and conducted a questionnaire-based study to collect data. Our analysis resulted in the construction and validation of five models that represent the causes, effects, and strategies for five specific community smells. These models can help practitioners identify and address community smells within their organizations, while also providing valuable insights for future research on the socio-technical aspects of machine learning-enabled system communities.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Socio-Techinical Aspects, ML-Enabled Teams, Partial Least Squares Structural Equation Modeling, PLS-SEM}
}

@inproceedings{10.1145/3183440.3195100,
author = {Su, Emily and Joshi, Sameer},
title = {Leveraging product relationships to generate candidate bugs for duplicate bug prediction},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3195100},
doi = {10.1145/3183440.3195100},
abstract = {Adaptive Bug Search (ABS) is a service developed by Oracle that uses machine learning to find potential duplicate bugs for a given input bug. ABS leverages the product and component relationships of existing duplicate bug pairs to limit the set of candidate bugs in which it searches for potential duplicates. In this paper, we discuss various approaches for selecting and refining the set of candidate bugs.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {210–211},
numpages = {2},
keywords = {duplicate bug prediction, human factors, machine learning},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3382025.3414976,
author = {Pereira, Juliana Alves and Martin, Hugo and Temple, Paul and Acher, Mathieu},
title = {Machine learning and configurable systems: a gentle introduction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414976},
doi = {10.1145/3382025.3414976},
abstract = {The goal of this tutorial is to give a gentle introduction to how machine learning can be used to support software product line configuration. This is our second practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance and bug prediction) on real-world systems (Linux, VaryLaTeX, x264). The material is designed for academics and practitioners with basic knowledge in software product lines and machine learning.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {40},
numpages = {1},
keywords = {configurable systems, machine learning, software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.5555/3566055.3566057,
author = {Gangolli, Aakash and Mahmoud, Qusay H. and Azim, Akramul},
title = {A Machine Learning Approach to Predict System-Level Threats from Hardware-Based Fault Injection Attacks on IoT Software},
year = {2022},
publisher = {IBM Corp.},
address = {USA},
abstract = {As the prevalence of Internet of Things (IoT) systems and applications increases, securing IoT software has become important. Fault injection attacks target a wide range of IoT software, including Advanced Encryption Standard (AES) encryption software, authentication access software, and application software. Potential sources of fault injection into IoT software include system hardware, network components, and application software. Hardware-based attackers introduce intentional faults into the system hardware, such as clock and voltage glitches, resulting in unintended behavior of the IoT software. Low-level software fault injection is a simulation-based technique for identifying software vulnerabilities to such hardware faults. This enables the software to be resistant to such attacks by incorporating the necessary countermeasures into its source code. Low-level software fault injection is commonly used to determine whether the injected fault caused an exception or program crash by manually monitoring the software fault injection output. However, software fault injection can also identify system-level threats such as data corruption and control flow changes caused by hardware-based faults, which otherwise go unidentified. This paper presents an approach for automatically predicting such system-level threats and vulnerabilities by utilizing machine learning on the software fault injection output. In addition, the paper examines the relationship between the range of application input data used in the software fault injection process and the predicted threats. A case study is conducted on an IoT software to demonstrate and evaluate the effectiveness of the proposed approach.},
booktitle = {Proceedings of the 32nd Annual International Conference on Computer Science and Software Engineering},
pages = {4–11},
numpages = {8},
keywords = {fault injection attack, software vulnerability detection, machine learning, software fault injection, software fault model},
location = {Toronto, Canada},
series = {CASCON '22}
}

@proceedings{10.1145/3549034,
title = {MaLTeSQuE 2022: Proceedings of the 6th International Workshop on Machine Learning Techniques for Software Quality Evaluation},
year = {2022},
isbn = {9781450394567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 6th edition of the workshop on Machine Learning Techniques for Software Quality Evaluation (MaLTeSQuE 2022), held in Singapore, on November 18th, 2022, co-located with the 30th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2022). MaLTeSQuE received a total of six submissions from all over the world, from which five papers were included in the program. The program also features two keynotes, by Yuriy Brun and Mike Papadakis, on the promises, dangers, and best practices of working at the intersection of machine learning and software engineering.},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3238147.3240469,
author = {Qu, Yu and Liu, Ting and Chi, Jianlei and Jin, Yangxu and Cui, Di and He, Ancheng and Zheng, Qinghua},
title = {node2defect: using network embedding to improve software defect prediction},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240469},
doi = {10.1145/3238147.3240469},
abstract = {Network measures have been proved to be useful in predicting software defects. Leveraging the dependency relationships between software modules, network measures can capture various structural features of software systems. However, existing studies have relied on user-defined network measures (e.g., degree statistics or centrality metrics), which are inflexible and require high computation cost, to describe the structural features. In this paper, we propose a new method called node2defect which uses a newly proposed network embedding technique, node2vec, to automatically learn to encode dependency network structure into low-dimensional vector spaces to improve software defect prediction. Specifically, we firstly construct a program's Class Dependency Network. Then node2vec is used to automatically learn structural features of the network. After that, we combine the learned features with traditional software engineering features, for accurate defect prediction. We evaluate our method on 15 open source programs. The experimental results show that in average, node2defect improves the state-of-the-art approach by 9.15% in terms of F-measure.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {844–849},
numpages = {6},
keywords = {Software defect, defect prediction, network embedding, software metrics},
location = {Montpellier, France},
series = {ASE '18}
}

@inproceedings{10.1109/MSR.2017.20,
author = {Patil, Sangameshwar},
title = {Concept-based classification of software defect reports},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.20},
doi = {10.1109/MSR.2017.20},
abstract = {Automatic identification of the defect type from the textual description of a software defect can significantly speedup as well as improve the software defect management life-cycle. This has been recognized in the research community and multiple solutions based on supervised learning approach have been proposed in the recent literature. However, these approaches need significant amount of labeled training data for use in real-life projects.In this paper, we propose to use Explicit Semantic Analysis (ESA) to carry out concept-based classification of software defect reports. We compute the "semantic similarity" between the defect type labels and the defect report in a concept space spanned by Wikipedia articles and then, assign the defect type which has the highest similarity with the defect report. This approach helps us to circumvent the problem of dependence on labeled training data. Experimental results show that using concept-based classification is a promising approach for software defect classification to avoid the expensive process of creating labeled training data and yet get accuracy comparable to the traditional supervised learning approaches. To the best of our knowledge, this is the first use of Wikipedia and ESA for software defect classification problem.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {182–186},
numpages = {5},
keywords = {explicit semantic analysis, mining software respositories, software defect classification, text data mining},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@proceedings{10.1145/3639479,
title = {MLNLP '23: Proceedings of the 2023 6th International Conference on Machine Learning and Natural Language Processing},
year = {2023},
isbn = {9798400709241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sanya, China}
}

@inproceedings{10.1145/3609437.3609449,
author = {Liu, Lei and Wang, Sinan and Liu, Yepang and Deng, Jinliang and Liu, Sicen},
title = {Drift: Fine-Grained Prediction of the Co-Evolution of Production and Test Code via Machine Learning},
year = {2023},
isbn = {9798400708947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609437.3609449},
doi = {10.1145/3609437.3609449},
abstract = {As production code evolves, test code can quickly become outdated. When test code is outdated, it may fail to capture errors in the programs under test and can lead to serious software bugs that result in significant losses for both developers and users. To ensure high software quality, it is crucial to promptly update the test code after making changes to the production code. This practice ensures that the test code and production code evolve together, reducing the likelihood of errors and ensuring the software remains reliable. However, maintaining test code can be challenging and time-consuming. To automate the identification of outdated test code, recent research has proposed Sitar, a machine learning-based method. Despite Sitar’s usefulness, it has major limitations, including its coarse prediction granularity (at class level), reliance on naming conventions to discover test code, and dependence on manually summarized features to construct machine learning models. In this paper, we address the limitations of Sitar and propose a new machine learning-based approach Drift. Drift&nbsp;predicts outdated test cases at the method level. It leverages method-calling relationships to accurately infer the links between production and test code, and automatically learns features via code analysis. We evaluate Drift&nbsp;using 40 open-source Java projects in both within-project and cross-project scenarios, and find that Drift&nbsp;can achieve satisfactory prediction performances in both scenarios. We also compare Drift&nbsp;with existing methods for outdated test code prediction and find that Drift&nbsp;can significantly outperform them. For example, compared with Sitar, the accuracy of Drift&nbsp;is increased by about 8.5%, the F1-score is increased by about 8.3%, and more importantly, the number of test cases that developers need to check is reduced by about 75%. Therefore, our method, Drift, can predict outdated test cases more accurately at a fine-grained level, and thus better facilitate the co-evolution of production and test code.},
booktitle = {Proceedings of the 14th Asia-Pacific Symposium on Internetware},
pages = {227–237},
numpages = {11},
keywords = {Machine Learning, Outdated Test Code, Software Evolution},
location = {Hangzhou, China},
series = {Internetware '23}
}

@inproceedings{10.1145/3538969.3543809,
author = {Sotgiu, Angelo and Pintor, Maura and Biggio, Battista},
title = {Explainability-based Debugging of Machine Learning for Vulnerability Discovery},
year = {2022},
isbn = {9781450396707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538969.3543809},
doi = {10.1145/3538969.3543809},
abstract = {Machine learning has been successfully used for increasingly complex and critical tasks, achieving high performance and efficiency that would not be possible for human operators. Unfortunately, recent studies have shown that, despite its power, this technology tends to learn spurious correlations from data, making it weak and susceptible to manipulation. Explainability techniques are often used to identify the most relevant features contributing to the decision. However, this is often done by taking examples one by one and trying to show the problem locally. To mitigate this issue, we propose in this paper a systematic method to leverage explainability techniques and build on their results to highlight problems in the model design and training. With an empirical analysis on the Devign dataset, we validate the proposed methodology with a CodeBERT model trained for vulnerability discovery, showing that, despite its impressive performances, spurious correlations consistently steer its decision.},
booktitle = {Proceedings of the 17th International Conference on Availability, Reliability and Security},
articleno = {113},
numpages = {8},
keywords = {code vulnerability detection, datasets, machine learning, neural networks},
location = {Vienna, Austria},
series = {ARES '22}
}

@article{10.1145/3511805,
author = {Ram\'{\i}rez, Aurora and Feldt, Robert and Romero, Jos\'{e} Ra\'{u}l},
title = {A Taxonomy of Information Attributes for Test Case Prioritisation: Applicability, Machine Learning},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3511805},
doi = {10.1145/3511805},
abstract = {Most software companies have extensive test suites and re-run parts of them continuously to ensure that recent changes have no adverse effects. Since test suites are costly to execute, industry needs methods for test case prioritisation (TCP). Recently, TCP methods use machine learning (ML) to exploit the information known about the system under test and its test cases. However, the value added by ML-based TCP methods should be critically assessed with respect to the cost of collecting the information. This article analyses two decades of TCP research and presents a taxonomy of 91 information attributes that have been used. The attributes are classified with respect to their information sources and the characteristics of their extraction process. Based on this taxonomy, TCP methods validated with industrial data and those applying ML are analysed in terms of information availability, attribute combination and definition of data features suitable for ML. Relying on a high number of information attributes, assuming easy access to system under test code and simplified testing environments are identified as factors that might hamper industrial applicability of ML-based TCP. The TePIA taxonomy provides a reference framework to unify terminology and evaluate alternatives considering the cost-benefit of the information attributes.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {21},
numpages = {42},
keywords = {Regression testing, taxonomy, machine learning, test case prioritisation, industry}
}

@inproceedings{10.1109/ICSE48619.2023.00135,
author = {Gesi, Jiri and Shen, Xinyun and Geng, Yunfan and Chen, Qihong and Ahmed, Iftekhar},
title = {Leveraging Feature Bias for Scalable Misprediction Explanation of Machine Learning Models},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00135},
doi = {10.1109/ICSE48619.2023.00135},
abstract = {Interpreting and debugging machine learning models is necessary to ensure the robustness of the machine learning models. Explaining mispredictions can help significantly in doing so. While recent works on misprediction explanation have proven promising in generating interpretable explanations for mispredictions, the state-of-the-art techniques "blindly" deduce misprediction explanation rules from all data features, which may not be scalable depending on the number of features. To alleviate this problem, we propose an efficient misprediction explanation technique named Bias Guided Misprediction Diagnoser (BGMD), which leverages two prior knowledge about data: a) data often exhibit highly-skewed feature distributions and b) trained models in many cases perform poorly on subdataset with under-represented features. Next, we propose a technique named MAPS (Mispredicted Area UPweight Sampling). MAPS increases the weights of subdataset during model retraining that belong to the group that is prone to be mispredicted because of containing under-represented features. Thus, MAPS make retrained model pay more attention to the under-represented features. Our empirical study shows that our proposed BGMD outperformed the state-of-the-art misprediction diagnoser and reduces diagnosis time by 92%. Furthermore, MAPS outperformed two state-of-the-art techniques on fixing the machine learning model's performance on mispredicted data without compromising performance on all data. All the research artifacts (i.e., tools, scripts, and data) of this study are available in the accompanying website [1].},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1559–1570},
numpages = {12},
keywords = {machine learning, data imbalance, rule induction, misprediction explanation},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3428757.3429152,
author = {Philipp, Robert and Mladenow, Andreas and Strauss, Christine and V\"{o}lz, Alexander},
title = {Machine Learning as a Service: Challenges in Research and Applications},
year = {2021},
isbn = {9781450389228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428757.3429152},
doi = {10.1145/3428757.3429152},
abstract = {This study aims to evaluate the current state of research with regards to Machine Learning as a Service (MLaaS) and to identify challenges and research fields of this novel topic. First, a literature review on a basket of eight leading journals was performed. We motivate this study by identifying a lack of studies in the field of MLaaS. The structured literature review was further extended to established scientific databases relevant in this field. We found 30 contributions on MLaaS. As a result of the analysis we grouped them into four key concepts: Platform, Applications; Performance Enhancements and Challenges. Three of the derived concepts are discussed in detail to identify future research areas and to reveal challenges in research as well as in applications.},
booktitle = {Proceedings of the 22nd International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {396–406},
numpages = {11},
keywords = {MLaaS, Machine Learning Services, Machine Learning, Machine Learning Platform, Machine Learning as a Service},
location = {Chiang Mai, Thailand},
series = {iiWAS '20}
}

@inproceedings{10.1145/3510003.3510068,
author = {Wan, Chengcheng and Liu, Shicheng and Xie, Sophie and Liu, Yifan and Hoffmann, Henry and Maire, Michael and Lu, Shan},
title = {Automated testing of software that uses machine learning APIs},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510068},
doi = {10.1145/3510003.3510068},
abstract = {An increasing number of software applications incorporate machine learning (ML) solutions for cognitive tasks that statistically mimic human behaviors. To test such software, tremendous human effort is needed to design image/text/audio inputs that are relevant to the software, and to judge whether the software is processing these inputs as most human beings do. Even when misbehavior is exposed, it is often unclear whether the culprit is inside the cognitive ML API or the code using the API.This paper presents Keeper, a new testing tool for software that uses cognitive ML APIs. Keeper designs a pseudo-inverse function for each ML API that reverses the corresponding cognitive task in an empirical way (e.g., an image search engine pseudo-reverses the image-classification API), and incorporates these pseudo-inverse functions into a symbolic execution engine to automatically generate relevant image/text/audio inputs and judge output correctness. Once misbehavior is exposed, Keeper attempts to change how ML APIs are used in software to alleviate the misbehavior. Our evaluation on a variety of open-source applications shows that Keeper greatly improves the branch coverage, while identifying many previously unknown bugs.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {212–224},
numpages = {13},
keywords = {machine learning, machine learning API, software testing},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3342999.3343010,
author = {Cui, Mengtian and Sun, Yue and Lu, Yang and Jiang, Yue},
title = {Study on the Influence of the Number of Features on the Performance of Software Defect Prediction Model},
year = {2019},
isbn = {9781450371605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342999.3343010},
doi = {10.1145/3342999.3343010},
abstract = {The software defect prediction model based on machine learning technology is the key to improve the reliability of software. The influence of the number of features on the performance of different software defect prediction models was proposed in this paper. First, a new data sets was built, which is increasing by the number of features based on the NASA public data sets. Then, the eight predictive models are experimented based on these data sets. Next, the influence of the number of features on the performance of different prediction models was analyzed based on the experimental results. Next, the AUC values obtained from the experiment were used to evaluate the performance of different prediction models, and the coefficient of variation C·V values was used to evaluate the performance stability of different prediction models while the number of features changed. In the end, the experiments show that the performance of the predictive model C4.5 is highly susceptible to changes in the number of features, while the performance of the predictive model SMO is relatively stable.},
booktitle = {Proceedings of the 2019 3rd International Conference on Deep Learning Technologies},
pages = {32–37},
numpages = {6},
keywords = {feature selection, machine learning, number of features, software defect prediction},
location = {Xiamen, China},
series = {ICDLT '19}
}

@inproceedings{10.1145/3377811.3380389,
author = {Chen, Jinyin and Hu, Keke and Yu, Yue and Chen, Zhuangzhi and Xuan, Qi and Liu, Yi and Filkov, Vladimir},
title = {Software visualization and deep transfer learning for effective software defect prediction},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380389},
doi = {10.1145/3377811.3380389},
abstract = {Software defect prediction aims to automatically locate defective code modules to better focus testing resources and human effort. Typically, software defect prediction pipelines are comprised of two parts: the first extracts program features, like abstract syntax trees, by using external tools, and the second applies machine learning-based classification models to those features in order to predict defective modules. Since such approaches depend on specific feature extraction tools, machine learning classifiers have to be custom-tailored to effectively build most accurate models.To bridge the gap between deep learning and defect prediction, we propose an end-to-end framework which can directly get prediction results for programs without utilizing feature-extraction tools. To that end, we first visualize programs as images, apply the self-attention mechanism to extract image features, use transfer learning to reduce the difference in sample distributions between projects, and finally feed the image files into a pre-trained, deep learning model for defect prediction. Experiments with 10 open source projects from the PROMISE dataset show that our method can improve cross-project and within-project defect prediction. Our code and data pointers are available at https://zenodo.org/record/3373409#.XV0Oy5Mza35.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {578–589},
numpages = {12},
keywords = {cross-project defect prediction, deep transfer learning, self-attention, software visualization, within-project defect prediction},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3583788.3583800,
author = {Duvvuri, Venkata and Lee, Gahyoung and Hsu, Yuwei and Makwana, Asha and Morgan, Chris},
title = {Post Processing Selection of Automatic Item Generation in Testing to Ensure Human-Like Quality with Machine Learning},
year = {2023},
isbn = {9781450398633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583788.3583800},
doi = {10.1145/3583788.3583800},
abstract = {Automatic Item Generation (AIG) is increasingly used to process large amounts of information and scale the demand for computerized testing. Recent work in Artificial Intelligence for AIG (aka Natural Question Generation-NQG), states that even newer AIG techniques are short in syntactic, semantic, and contextual relevance when evaluated qualitatively on small datasets. We confirm this deficiency quantitatively over large datasets. Additionally, we find that human evaluation by Subject Matter Experts (SMEs) conservatively rejects at least ∼9% portion of AI test questions in our experiment over large diverse dataset topics. Here we present an analytical study of these differences, and this motivates our two-phased post-processing AI daisy chain machine learning (ML) architecture for selection and editing of AI generated questions using current techniques. Finally, we identify and propose the first selection step in the daisy chain using ML with 97+% accuracy, and provide analytical guidance for development of the second editing step with a measured lower bound on a BLEU score improvement of 2.4+%.},
booktitle = {Proceedings of the 2023 7th International Conference on Machine Learning and Soft Computing},
pages = {82–88},
numpages = {7},
keywords = {Analytics, Automation item generation, CNN, LSTM, NLP},
location = {Chongqing, China},
series = {ICMLSC '23}
}

@proceedings{10.1145/3616901,
title = {FAIML '23: Proceedings of the 2023 International Conference on Frontiers of Artificial Intelligence and Machine Learning},
year = {2023},
isbn = {9798400707544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@inproceedings{10.1145/3338906.3341462,
author = {Caulo, Maria},
title = {A taxonomy of metrics for software fault prediction},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3341462},
doi = {10.1145/3338906.3341462},
abstract = {In the field of Software Fault Prediction (SFP), researchers exploit software metrics to build predictive models using machine learning and/or statistical techniques. SFP has existed for several decades and the number of metrics used has increased dramatically. Thus, the need for a taxonomy of metrics for SFP arises firstly to standardize the lexicon used in this field so that the communication among researchers is simplified and then to organize and systematically classify the used metrics. In this doctoral symposium paper, I present my ongoing work which aims not only to build such a taxonomy as comprehensive as possible, but also to provide a global understanding of the metrics for SFP in terms of detailed information: acronym(s), extended name, univocal description, granularity of the fault prediction (e.g., method and class), category, and research papers in which they were used.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1144–1147},
numpages = {4},
keywords = {software fault prediction, software metrics, taxonomy},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3028842.3028859,
author = {Gao, Yan and Yang, Chunhui},
title = {Software defect prediction based on manifold learning in subspace selection},
year = {2016},
isbn = {9781450347990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3028842.3028859},
doi = {10.1145/3028842.3028859},
abstract = {Software defects will lead to software running error and system crashes. In order to detect software defect as early as possible at early stage of software development, a series of machine learning approaches have been studied and applied to predict defects in software modules. Unfortunately, the imbalanceof software defect datasets brings great challenge to software defect prediction model training. In this paper, a new manifold learning based subspace learning algorithm, Discriminative Locality Alignment(DLA), is introduced into software defects prediction. Experimental results demonstrate that DLA is consistently superior to LDA (Linear Discriminant Analysis) and PCA (Principal Component Analysis) in terms of discriminate information extraction and prediction performance. In addition, DLA reveals some attractive intrinsic properties for numeric calculation, e.g. it can overcome the matrix singular problem and small sample size problem in software defect prediction.},
booktitle = {Proceedings of the 1st International Conference on Intelligent Information Processing},
articleno = {17},
numpages = {6},
keywords = {discriminative locality alignment, manifold learning, software defect prediction, support vector machine},
location = {Wuhan, China},
series = {ICIIP '16}
}

@inproceedings{10.1145/3540250.3558943,
author = {Chatterjee, Ayan and Ahmed, Bestoun S. and Hallin, Erik and Engman, Anton},
title = {Testing of machine learning models with limited samples: an industrial vacuum pumping application},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558943},
doi = {10.1145/3540250.3558943},
abstract = {There is often a scarcity of training data for machine learning (ML) classification and regression models in industrial production, especially for time-consuming or sparsely run manufacturing processes. Traditionally, a majority of the limited ground-truth data is used for training, while a handful of samples are left for testing. In that case, the number of test samples is inadequate to properly evaluate the robustness of the ML models under test (i.e., the system under test) for classification and regression. Furthermore, the output of these ML models may be inaccurate or even fail if the input data differ from the expected. This is the case for ML models used in the Electroslag Remelting (ESR) process in the refined steel industry to predict the pressure in a vacuum chamber. A vacuum pumping event that occurs once a workday generates a few hundred samples in a year of pumping for training and testing. In the absence of adequate training and test samples, this paper first presents a method to generate a fresh set of augmented samples based on vacuum pumping principles. Based on the generated augmented samples, three test scenarios and one test oracle are presented to assess the robustness of an ML model used for production on an industrial scale. Experiments are conducted with real industrial production data obtained from Uddeholms AB steel company. The evaluations indicate that Ensemble and Neural Network are the most robust when trained on augmented data using the proposed testing strategy. The evaluation also demonstrates the proposed method's effectiveness in checking and improving ML algorithms' robustness in such situations. The work improves software testing's state-of-the-art robustness testing in similar settings. Finally, the paper presents an MLOps implementation of the proposed approach for real-time ML model prediction and action on the edge node and automated continuous delivery of ML software from the cloud.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1280–1290},
numpages = {11},
keywords = {data augmentation, data decomposition, machine learning, mlops, software testing, vacuum pumping},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3551349.3559510,
author = {Wei, Chenhao and Xiao, Lu and Yu, Tingting and Chen, Xinyu and Wang, Xiao and Wong, Sunny and Clune, Abigail},
title = {Automatically Tagging the “AAA” Pattern in Unit Test Cases Using Machine Learning Models},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3559510},
doi = {10.1145/3551349.3559510},
abstract = {The AAA pattern, i.e. the Arrangement, Action, and Assertion, is a common and nature layout to create a test case. Following this pattern in test cases may benefit comprehension, debugging, and maintenance. The AAA structure of real-life test cases may not be explicit due to its high complexity. Manually labeling AAA statements in test cases is tedious. Thus, an automated approach for labeling AAA statements in existing test cases could benefit new developers and projects that practice collective code ownership and test driven development. This study contributes an automatic approach based on machine learning models. The “secret sauce” of this approach is a set of three learning features that are based on the semantic, syntax, and context information in test cases, derived from the manual tagging process. Thus, our approach mimics how developers may manually tag the AAA pattern of a test case. We assess the precision, recall, and F-1 score of our approach based on 449 test cases, containing about 16,612 statements, across 4 Apache open source projects. For achieving the best performance in our approach, we explore the usage of six machine learning models; the contribution of the SMOTE data balancing technique; the comparison of the three learning features; and the comparison of five different methods for calculating the semantic feature. The results show our approach is able to identify Arrangement, Action, and Assertion statements with a precision upwards of 92%, and recall up to 74%. Our experiments also provide empirical insights regarding how to best leverage machine learning for software engineering tasks.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {194},
numpages = {3},
keywords = {AAA pattern, Feature engineering, Machine Learning, Natural language processing, Software testing, Unit testing},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/2723742.2723754,
author = {Muthukumaran, K. and Rallapalli, Akhila and Murthy, N. L. Bhanu},
title = {Impact of Feature Selection Techniques on Bug Prediction Models},
year = {2015},
isbn = {9781450334327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723742.2723754},
doi = {10.1145/2723742.2723754},
abstract = {Several change metrics and source code metrics have been introduced and proved to be effective features in building bug prediction models. Researchers performed comparative studies of bug prediction models built using the individual metrics as well as combination of these metrics. In this paper, we investigate whether the prediction accuracy of bug prediction models is improved by applying feature selection techniques. We explore if there is one algorithm amongst ten popular feature selection algorithms that consistently fares better than others across sixteen bench marked open source projects. We also study whether the metrics in best feature subset are consistent across projects.},
booktitle = {Proceedings of the 8th India Software Engineering Conference},
pages = {120–129},
numpages = {10},
keywords = {Bug prediction, Feature selection, Software Quality},
location = {Bangalore, India},
series = {ISEC '15}
}

@inproceedings{10.1145/3383219.3383281,
author = {Khan, Bilal and Iqbal, Danish and Badshah, Sher},
title = {Cross-Project Software Fault Prediction Using Data Leveraging Technique to Improve Software Quality},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383281},
doi = {10.1145/3383219.3383281},
abstract = {Software fault prediction is a process to detect bugs in software projects. Fault prediction in software engineering has attracted much attention from the last decade. The early prognostication of faults in software minimize the cost and effort of errors that come at later stages. Different machine learning techniques have been utilized for fault prediction, that is proven to be utilizable. Despite, the significance of fault prediction most of the companies do not consider fault prediction in practice and do not build useful models due to lack of data or lack of enough data to strengthen the power of fault predictors. However, models trained and tested on less amount of data are difficult to generalize, because they do not consider project size, project differences, and features selection. To overcome these issues, we proposed an instance-based transfer learning through data leveraging using logistic linear regression as a base proposed statistical methodology. In our study, we considered three software projects within the same domain. Finally, we performed a comparative analysis of three different experiments for building models (targeted project). The experimental results of the proposed approach show promising improvements in (SFP).},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {434–438},
numpages = {5},
keywords = {Cross-project, Instance-based learning, Machine learning, Software Quality, Software fault prediction, data leveraging},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1145/3377816.3381734,
author = {Byun, Taejoon and Rayadurgam, Sanjai},
title = {Manifold for machine learning assurance},
year = {2020},
isbn = {9781450371261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377816.3381734},
doi = {10.1145/3377816.3381734},
abstract = {The increasing use of machine-learning (ML) enabled systems in critical tasks fuels the quest for novel verification and validation techniques yet grounded in accepted system assurance principles. In traditional system development, model-based techniques have been widely adopted, where the central premise is that abstract models of the required system provide a sound basis for judging its implementation. We posit an analogous approach for ML systems using an ML technique that extracts from the high-dimensional training data implicitly describing the required system, a low-dimensional underlying structure---a manifold. It is then harnessed for a range of quality assurance tasks such as test adequacy measurement, test input generation, and runtime monitoring of the target ML system. The approach is built on variational autoencoder, an unsupervised method for learning a pair of mutually near-inverse functions between a given high-dimensional dataset and a low-dimensional representation. Preliminary experiments establish that the proposed manifold-based approach, for test adequacy drives diversity in test data, for test generation yields fault-revealing yet realistic test cases, and for run-time monitoring provides an independent means to assess trustability of the target system's output.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {97–100},
numpages = {4},
keywords = {machine learning testing, neural networks, variational autoencoder},
location = {Seoul, South Korea},
series = {ICSE-NIER '20}
}

@inproceedings{10.1145/3239576.3239622,
author = {Yang, Zhao and Qian, Hongbing},
title = {Automated Parameter Tuning of Artificial Neural Networks for Software Defect Prediction},
year = {2018},
isbn = {9781450364607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239576.3239622},
doi = {10.1145/3239576.3239622},
abstract = {Defect prediction can help predict defect-prone software modules and improve the efficiency and accuracy of defect location and repair, which plays an extremely important role in software quality assurance. Artificial Neural Networks (ANNs), a family of powerful machine learning regression or classification models, have been widely applied for defect prediction. However, the performance of these models will be degraded if they use suboptimal default parameter settings (e.g., the number of units in the hidden layer). This paper utilizes an automated parameter tuning technique-Caret to optimize parameter settings. In our study, 30 datasets are downloaded from the Tera-PROMISE Repository. According to the characteristics of the datasets, we select key features (metrics) as predictors to train defect prediction models. The experiment applies feed-forward, single hidden layer artificial neural network as classifier to build different defect prediction models respectively with optimized parameter settings and with default parameter settings. Confusion matrix and ROC curve are used for evaluating the quality of the models above. The results show that the models trained with optimized parameter settings outperform the models trained with default parameter settings. Hence, we suggest that researchers should pay attention to tuning parameter settings by Caret for ANNs instead of using suboptimal default settings if they select ANNs for training models in the future defect prediction studies.},
booktitle = {Proceedings of the 2nd International Conference on Advances in Image Processing},
pages = {203–209},
numpages = {7},
keywords = {Artificial Neural Networks, Automated Parameter Tuning, Metrics, Software defect prediction},
location = {Chengdu, China},
series = {ICAIP '18}
}

@inproceedings{10.1109/ICSE43902.2021.00138,
author = {Wang, Song and Shrestha, Nishtha and Subburaman, Abarna Kucheri and Wang, Junjie and Wei, Moshi and Nagappan, Nachiappan},
title = {Automatic Unit Test Generation for Machine Learning Libraries: How Far Are We?},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00138},
doi = {10.1109/ICSE43902.2021.00138},
abstract = {Automatic unit test generation that explores the input space and produces effective test cases for given programs have been studied for decades. Many unit test generation tools that can help generate unit test cases with high structural coverage over a program have been examined. However, the fact that existing test generation tools are mainly evaluated on general software programs calls into question about its practical effectiveness and usefulness for machine learning libraries, which are statistically-orientated and have fundamentally different nature and construction from general software projects.In this paper, we set out to investigate the effectiveness of existing unit test generation techniques on machine learning libraries. To investigate this issue, we conducted an empirical study on five widely-used machine learning libraries with two popular unit test case generation tools, i.e., EVOSUITE and Randoop. We find that (1) most of the machine learning libraries do not maintain a high-quality unit test suite regarding commonly applied quality metrics such as code coverage (on average is 34.1%) and mutation score (on average is 21.3%), (2) unit test case generation tools, i.e., EVOSUITE and Randoop, lead to clear improvements in code coverage and mutation score, however, the improvement is limited, and (3) there exist common patterns in the uncovered code across the five machine learning libraries that can be used to improve unit test case generation tasks.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1548–1560},
numpages = {13},
keywords = {Empirical software engineering, test case generation, testing machine learning libraries},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1109/ASE51524.2021.9678592,
author = {Lai, Tuan Dung},
title = {Towards the generation of machine learning defect reports},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678592},
doi = {10.1109/ASE51524.2021.9678592},
abstract = {Effective locating and fixing defects requires detailed defect reports. Unlike traditional software systems, machine learning applications are subject defects caused from changes in the input data streams (concept drift) and assumptions encoded into models. Without appropriate training, developers face difficulties understanding and interpreting faults in machine learning (ML). However, little research is done on how to prepare developers to detect and investigate machine learning system defects. Software engineers often do not have sufficient knowledge to fix the issues themselves without the help of data scientists or domain experts. To investigate this issue, we analyse issue templates and check how developers report machine learning related issues in open-source applied AI projects. The overall goal is to develop a tool for automatically repairing ML defects or generating defect reports if a fix cannot be made. Previous research has identified classes of faults specific to machine learning systems, such as performance degradation arising from concept drift where the machine learning model is no longer aligned with the real-world environment. However, the current issue templates that developers use do not seem to capture the information needed. This research seeks to systematically develop a two-way human-machine information exchange protocol to support domain experts, software engineers, and data scientists to collaboratively detect, report, and respond to these new classes of faults.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1038–1042},
numpages = {5},
location = {Melbourne, Australia},
series = {ASE '21}
}

@inproceedings{10.1145/3377811.3380403,
author = {Tabassum, Sadia and Minku, Leandro L. and Feng, Danyi and Cabral, George G. and Song, Liyan},
title = {An investigation of cross-project learning in online just-in-time software defect prediction},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380403},
doi = {10.1145/3377811.3380403},
abstract = {Just-In-Time Software Defect Prediction (JIT-SDP) is concerned with predicting whether software changes are defect-inducing or clean based on machine learning classifiers. Building such classifiers requires a sufficient amount of training data that is not available at the beginning of a software project. Cross-Project (CP) JIT-SDP can overcome this issue by using data from other projects to build the classifier, achieving similar (not better) predictive performance to classifiers trained on Within-Project (WP) data. However, such approaches have never been investigated in realistic online learning scenarios, where WP software changes arrive continuously over time and can be used to update the classifiers. It is unknown to what extent CP data can be helpful in such situation. In particular, it is unknown whether CP data are only useful during the very initial phase of the project when there is little WP data, or whether they could be helpful for extended periods of time. This work thus provides the first investigation of when and to what extent CP data are useful for JIT-SDP in a realistic online learning scenario. For that, we develop three different CP JIT-SDP approaches that can operate in online mode and be updated with both incoming CP and WP training examples over time. We also collect 2048 commits from three software repositories being developed by a software company over the course of 9 to 10 months, and use 19,8468 commits from 10 active open source GitHub projects being developed over the course of 6 to 14 years. The study shows that training classifiers with incoming CP+WP data can lead to improvements in G-mean of up to 53.90% compared to classifiers using only WP data at the initial stage of the projects. For the open source projects, which have been running for longer periods of time, using CP data to supplement WP data also helped the classifiers to reduce or prevent large drops in predictive performance that may occur over time, leading to up to around 40% better G-Mean during such periods. Such use of CP data was shown to be beneficial even after a large number of WP data were received, leading to overall G-means up to 18.5% better than those of WP classifiers.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {554–565},
numpages = {12},
keywords = {class imbalance, concept drift, cross-project learning, online learning, software defect prediction, transfer learning, verification latency},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3368089.3417043,
author = {Ahmed, Md Sohel and Ishikawa, Fuyuki and Sugiyama, Mahito},
title = {Testing machine learning code using polyhedral region},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417043},
doi = {10.1145/3368089.3417043},
abstract = {To date, although machine learning has been successful in various practical applications, generic methods of testing machine learning code have not been established yet. Here we present a new approach to test machine learning code using the possible input region obtained as a polyhedron. If an ML system generates different output for multiple input in the polyhedron, it is ensured that there exists a bug in the code. This property is known as one of theoretical fundamentals in statistical inference, for example, sparse regression models such as the lasso, and a wide range of machine learning algorithms satisfy this polyhedral condition, to which our testing procedure can be applied. We empirically show that the existence of bugs in lasso code can be effectively detected by our method in the mutation testing framework.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1533–1536},
numpages = {4},
keywords = {Lasso, Machine learning code, Mutation Analysis, Polyhedral region, Testing},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.5555/3507788.3507798,
author = {Khan, Md Asif and Azim, Akramul and Liscano, Ramiro and Smith, Kevin and Chang, Yee-Kang and Garcon, Sylvain and Tauseef, Qasim},
title = {Failure prediction using machine learning in IBM WebSphere liberty continuous integration environment},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {The growing complexity and dependencies of software have increased the importance of testing to ensure that frequent changes do not adversely affect existing functionality. Moreover, continuous integration comes with unique challenges associated with maintaining a stable build environment. Several studies have shown that the testing environment becomes more efficient with proper test case prioritization techniques. However, an application's dynamic behavior makes it challenging to derive test case prioritization techniques for achieving optimal results. With the advance of machine learning, the context of an application execution can be analyzed to select and prioritize test suites more efficiently.Test suite prioritization techniques aim to reorder test suites' execution to deliver high quality, maintainable software at lower costs to meet specific objectives such as revealing failures earlier. The state-of-the-art techniques on test prioritization in a continuous integration environment focus on relatively small, single-language, unit-tested projects. This paper compares and analyzes Machine learning-based test suite prioritization technique on two large-scale dataset collected from a continuous integration environment Google and IBM respectively. We optimize hyperparameters and report on experiments' findings by using different machine learning algorithms for test suite prioritization. Our optimized algorithms prioritize test suites with 93% accuracy on average and require 20% fewer test suites to detect 80% of the failures than the test suites prioritized randomly.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {63–72},
numpages = {10},
keywords = {CI, continuous integration, machine learning, test prioritization},
location = {Toronto, Canada},
series = {CASCON '21}
}

@inproceedings{10.1145/2372251.2372285,
author = {Giger, Emanuel and D'Ambros, Marco and Pinzger, Martin and Gall, Harald C.},
title = {Method-level bug prediction},
year = {2012},
isbn = {9781450310567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2372251.2372285},
doi = {10.1145/2372251.2372285},
abstract = {Researchers proposed a wide range of approaches to build effective bug prediction models that take into account multiple aspects of the software development process. Such models achieved good prediction performance, guiding developers towards those parts of their system where a large share of bugs can be expected. However, most of those approaches predict bugs on file-level. This often leaves developers with a considerable amount of effort to examine all methods of a file until a bug is located. This particular problem is reinforced by the fact that large files are typically predicted as the most bug-prone. In this paper, we present bug prediction models at the level of individual methods rather than at file-level. This increases the granularity of the prediction and thus reduces manual inspection efforts for developers. The models are based on change metrics and source code metrics that are typically used in bug prediction. Our experiments---performed on 21 Java open-source (sub-)systems---show that our prediction models reach a precision and recall of 84% and 88%, respectively. Furthermore, the results indicate that change metrics significantly outperform source code metrics.},
booktitle = {Proceedings of the ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {171–180},
numpages = {10},
keywords = {code metrics, fine-grained source code changes, method-level bug prediction},
location = {Lund, Sweden},
series = {ESEM '12}
}

@inproceedings{10.1145/3180374.3181331,
author = {Li, Yuting and Su, Jianmin and Yang, Xiaoxing},
title = {Multi-Objective vs. Single-Objective Approaches for Software Defect Prediction},
year = {2018},
isbn = {9781450354318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180374.3181331},
doi = {10.1145/3180374.3181331},
abstract = {Software defect prediction employs attributes of software modules to identify defect-prone modules and thus improves software reliability by allocating testing resources more efficiently. Realizing that single-objective methods might be insufficient for solving defect prediction problems, some researchers have proposed multi-objective learning approaches, and proved better performance of multi-objective than single-objective methods. However, existing compared single-objective methods optimize a completely different goal from goals of multi-objective approaches, which might lead to bias. In this paper, we compare a multi-objective approach that optimizes two objectives and a single-objective approach that directly optimizes a trade-off of the two objectives, in order to further investigate the comparison of multi-objective and single-objective approaches. The conclusion will help to appropriately choose multi-objective or single-objective learning approaches for defect prediction.},
booktitle = {Proceedings of the 2018 2nd International Conference on Management Engineering, Software Engineering and Service Sciences},
pages = {122–127},
numpages = {6},
keywords = {Multi-objective learning, cost, effectiveness, single-objective learning, software defect prediction},
location = {Wuhan, China},
series = {ICMSS 2018}
}

@inproceedings{10.1145/3028842.3028858,
author = {Gao, Yan and Yang, Chunhui and Liang, Lixin},
title = {Pseudo-samples generation in Gaussian mixture distribution for software defect prediction},
year = {2016},
isbn = {9781450347990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3028842.3028858},
doi = {10.1145/3028842.3028858},
abstract = {In this paper, we present GCRF method based on pseudo-samples generation and conditional random field (CRF) for software defect prediction in Gaussian Mixture Distribution. In the proposed method, firstly, we leverage Gaussian Mixture Distribution (GMM) to generate pseudo-samples, which can increase the samples of minority class for balancing the train dataset. Secondly, we propose to apply CRF model in the balanced train dataset because the CRF model can handle complex features in nonlinear high dimensional subspace. Moreover, in order to avoid explicit modeling of the observed data, the proposed method can incorporate the classification of software defect data with different statistics characteristics into a unified probabilistic framework. Interestingly, the experiments show that the GCRF method achieves much better prediction performance than the other approach as shown in the software defect data classification task.},
booktitle = {Proceedings of the 1st International Conference on Intelligent Information Processing},
articleno = {16},
numpages = {6},
keywords = {conditional random field, gaussian mixture distribution, imbalance distribution, software defect prediction},
location = {Wuhan, China},
series = {ICIIP '16}
}

@inproceedings{10.1145/3460319.3464844,
author = {Dutta, Saikat and Selvam, Jeeva and Jain, Aryaman and Misailovic, Sasa},
title = {TERA: optimizing stochastic regression tests in machine learning projects},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464844},
doi = {10.1145/3460319.3464844},
abstract = {The stochastic nature of many Machine Learning (ML) algorithms makes testing of ML tools and libraries challenging. ML algorithms allow a developer to control their accuracy and run-time through a set of hyper-parameters, which are typically manually selected in tests. This choice is often too conservative and leads to slow test executions, thereby increasing the cost of regression testing.  We propose TERA, the first automated technique for reducing the cost of regression testing in Machine Learning tools and libraries(jointly referred to as projects) without making the tests more flaky. TERA solves the problem of exploring the trade-off space between execution time of the test and its flakiness as an instance of Stochastic Optimization over the space of algorithm hyper-parameters. TERA presents how to leverage statistical convergence-testing techniques to estimate the level of flakiness of the test for a specific choice of hyper-parameters during optimization.  We evaluate TERA on a corpus of 160 tests selected from 15 popular machine learning projects. Overall, TERA obtains a geo-mean speedup of 2.23x over the original tests, for the minimum passing probability threshold of 99%. We also show that the new tests did not reduce fault detection ability through a mutation study and a study on a set of 12 historical build failures in studied projects.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {413–426},
numpages = {14},
keywords = {Bayesian Optimization, Machine Learning, Software Testing, Test Optimization},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@article{10.1145/3442181,
author = {Sabir, Bushra and Ullah, Faheem and Babar, M. Ali and Gaire, Raj},
title = {Machine Learning for Detecting Data Exfiltration: A Review},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3442181},
doi = {10.1145/3442181},
abstract = {Context: Research at the intersection of cybersecurity, Machine Learning (ML), and Software Engineering (SE) has recently taken significant steps in proposing countermeasures for detecting sophisticated data exfiltration attacks. It is important to systematically review and synthesize the ML-based data exfiltration countermeasures for building a body of knowledge on this important topic. Objective: This article aims at systematically reviewing ML-based data exfiltration countermeasures to identify and classify ML approaches, feature engineering techniques, evaluation datasets, and performance metrics used for these countermeasures. This review also aims at identifying gaps in research on ML-based data exfiltration countermeasures. Method: We used Systematic Literature Review (SLR) method to select and review 92 papers. Results: The review has enabled us to: (a) classify the ML approaches used in the countermeasures into data-driven, and behavior-driven approaches; (b) categorize features into six types: behavioral, content-based, statistical, syntactical, spatial, and temporal; (c) classify the evaluation datasets into simulated, synthesized, and real datasets; and (d) identify 11 performance measures used by these studies. Conclusion: We conclude that: (i) The integration of data-driven and behavior-driven approaches should be explored; (ii) There is a need of developing high quality and large size evaluation datasets; (iii) Incremental ML model training should be incorporated in countermeasures; (iv) Resilience to adversarial learning should be considered and explored during the development of countermeasures to avoid poisoning attacks; and (v) The use of automated feature engineering should be encouraged for efficiently detecting data exfiltration attacks.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {50},
numpages = {47},
keywords = {Data exfiltration, advanced persistent threat, data breach, data leakage, machine learning}
}

@inproceedings{10.1145/3530019.3531333,
author = {Hussain, Shahid and Ibrahim, Naseem},
title = {Empirical Investigation of role of Meta-learning approaches for the Improvement of Software Development Process via Software Fault Prediction},
year = {2022},
isbn = {9781450396134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3530019.3531333},
doi = {10.1145/3530019.3531333},
abstract = {Context: Software Engineering (SE) community has empirically investigated software defect prediction as a proxy to benchmark it as a process improvement activity to assure software quality. In the domain of software fault prediction, the performance of classification algorithms is highly provoked with the residual effects attributed to feature irrelevance and data redundancy issues. Problem: The meta-learning-based ensemble methods are usually carried out to mitigate these noise effects and boost the software fault prediction performance. However, there is a need to benchmark the performance of meta-learning ensemble methods (as fault predictor) to assure software quality control and aid developers in their decision making. Method: We conduct an empirical and comparative study to evaluate and benchmark the improvement in the fault prediction performance via meta-learning ensemble methods as compared to their component base-level fault predictors. In this study, we perform a series of experiments with four well-known meta-level ensemble methods Vote, StackingC (i.e., Stacking), MultiScheme, and Grading. We also use five high-performance fault predictors Logistic (i.e., Logistic Regression), J48 (i.e., Decision Tree), IBK (i.e. k-nearest neighbor), NaiveBayes, and Decision Table (DT). Subsequently, we performed these experiments on public defect datasets with k-fold (k=10) cross-validation. We used F-measure and ROC-AUC (Receiver Operating Characteristic-Area Under Curve) performance measures and applied the four non-parametric tests to benchmark the fault prediction performance results of meta-learning ensemble methods. Results and Conclusion: we conclude that meta-learning ensemble methods, especially Vote could outperform the base-level fault predictors to tackle the feature irrelevance and redundancy issues in the domain of software fault prediction. Having said that, their performance is highly related to the number of base-level classifiers and the set of software fault prediction metrics.},
booktitle = {Proceedings of the 26th International Conference on Evaluation and Assessment in Software Engineering},
pages = {413–420},
numpages = {8},
keywords = {Classification, Ensemble method, Fault Prediction, Metrics, Performance},
location = {Gothenburg, Sweden},
series = {EASE '22}
}

@inproceedings{10.1145/3493700.3493704,
author = {Saha, Diptikalyan and Aggarwal, Aniya and Hans, Sandeep},
title = {Data Synthesis for Testing Black-Box Machine Learning Models},
year = {2022},
isbn = {9781450385824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493700.3493704},
doi = {10.1145/3493700.3493704},
abstract = {The increasing usage of machine learning models raises the question of the reliability of these models. The current practice of testing with limited data is often insufficient. In this paper, we provide a framework for automated test data synthesis to test black-box ML/DL models. We address an important challenge of generating realistic user-controllable data with model agnostic coverage criteria to test a varied set of properties, essentially to increase trust in machine learning models. We experimentally demonstrate the effectiveness of our technique.},
booktitle = {Proceedings of the 5th Joint International Conference on Data Science &amp; Management of Data (9th ACM IKDD CODS and 27th COMAD)},
pages = {110–114},
numpages = {5},
location = {Bangalore, India},
series = {CODS-COMAD '22}
}

@inproceedings{10.1145/3341105.3374008,
author = {Zakurdaeva, Alla and Weiss, Michael and Muegge, Steven},
title = {Detecting architectural integrity violation patterns using machine learning},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3374008},
doi = {10.1145/3341105.3374008},
abstract = {Recent1 years have seen a surge of research into new ways of analyzing software quality. Specifically, a set of studies has been devoted to the impact the architectural relations among files have on system maintainability and file bug-proneness. The literature has proposed a set of rules for determining recurring architectural design flaws that occur in most complex systems, are associated with bugs, and thus incur high maintenance costs. In the present paper we advocate for using machine learning as the means of refining the approach and revealing new patterns of architectural integrity violations. Having trained a machine learning model on the combination of structural and historical information acquired from the Tiki open source project, we have been able to replicate three of the six known types of architectural violations and discover one new type, the Reverse Unstable Interface pattern. The implication of our study is that machine learning can provide valuable insights into the problem and discover novel patterns which would help software analysts to pinpoint specific architectural problems that may be the root causes of elevated bug- and change-proneness.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1480–1487},
numpages = {8},
keywords = {architectural flaws, bug-proneness, hotspot patterns, machine learning, software architecture},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1109/ASE.2009.76,
author = {Shivaji, Shivkumar and Jr., E. James Whitehead and Akella, Ram and Kim, Sunghun},
title = {Reducing Features to Improve Bug Prediction},
year = {2009},
isbn = {9780769538914},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2009.76},
doi = {10.1109/ASE.2009.76},
abstract = {Recently, machine learning classifiers have emerged as a way to predict the existence of a bug in a change made to a source code file. The classifier is first trained on software history data, and then used to predict bugs. Two drawbacks of existing classifier-based bug prediction are potentially insufficient accuracy for practical use, and use of a large number of features. These large numbers of features adversely impact scalability and accuracy of the approach. This paper proposes a feature selection technique applicable to classification-based bug prediction. This technique is applied to predict bugs in software changes, and performance of Naive Bayes and Support Vector Machine (SVM) classifiers is characterized.},
booktitle = {Proceedings of the 24th IEEE/ACM International Conference on Automated Software Engineering},
pages = {600–604},
numpages = {5},
keywords = {Bug prediction, Feature Selection, Machine Learning, Reliability},
series = {ASE '09}
}

@inproceedings{10.1145/3439961.3439979,
author = {Santos, Geanderson and Figueiredo, Eduardo and Veloso, Adriano and Viggiato, Markos and Ziviani, Nivio},
title = {Predicting Software Defects with Explainable Machine Learning},
year = {2021},
isbn = {9781450389235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439961.3439979},
doi = {10.1145/3439961.3439979},
abstract = {Most software systems must evolve to cope with stakeholders’ requirements and fix existing defects. Hence, software defect prediction represents an area of interest in both academia and the software industry. As a result, predicting software defects can help the development team to maintain substantial levels of software quality. For this reason, machine learning models have increased in popularity for software defect prediction and have demonstrated effectiveness in many scenarios. In this paper, we evaluate a machine learning approach for selecting features to predict software module defects. We use a tree boosting algorithm that receives as input a training set comprising records of software features encoding characteristics of each module and outputs whether the corresponding module is defective prone. For nine projects within the widely known NASA data program, we build prediction models from a set of easy-to-compute module features. We then sample this sizable model space by randomly selecting software features to compose each model. This significant number of models allows us to structure our work along model understandability and predictive accuracy. We argue that explaining model predictions is meaningful to provide information to developers on features related to each module defective-prone. We show that (i) features that contribute most to finding the best models may vary depending on the project, and (ii) effective models are highly understandable based on a survey with 40 developers.},
booktitle = {Proceedings of the XIX Brazilian Symposium on Software Quality},
articleno = {18},
numpages = {10},
keywords = {NASA datasets, SHAP values, explainable models, software defects},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {SBQS '20}
}

@inproceedings{10.1145/3540250.3549093,
author = {Chen, Zhenpeng and Zhang, Jie M. and Sarro, Federica and Harman, Mark},
title = {MAAT: a novel ensemble approach to addressing fairness and performance bugs for machine learning software},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549093},
doi = {10.1145/3540250.3549093},
abstract = {Machine Learning (ML) software can lead to unfair and unethical decisions, making software fairness bugs an increasingly significant concern for software engineers. However, addressing fairness bugs often comes at the cost of introducing more ML performance (e.g., accuracy) bugs. In this paper, we propose MAAT, a novel ensemble approach to improving fairness-performance trade-off for ML software. Conventional ensemble methods combine different models with identical learning objectives. MAAT, instead, combines models optimized for different objectives: fairness and ML performance. We conduct an extensive evaluation of MAAT with 5 state-of-the-art methods, 9 software decision tasks, and 15 fairness-performance measurements. The results show that MAAT significantly outperforms the state-of-the-art. In particular, MAAT beats the trade-off baseline constructed by a recent benchmarking tool in 92.2% of the overall cases evaluated, 12.2 percentage points more than the best technique currently available. Moreover, the superiority of MAAT over the state-of-the-art holds on all the tasks and measurements that we study. We have made publicly available the code and data of this work to allow for future replication and extension.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1122–1134},
numpages = {13},
keywords = {Software fairness, bias mitigation, ensemble learning, fairness-performance trade-off, machine learning software},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3485447.3512244,
author = {Park, Saerom and Kim, Seongmin and Lim, Yeon-sup},
title = {Fairness Audit of Machine Learning Models with Confidential Computing},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512244},
doi = {10.1145/3485447.3512244},
abstract = {Algorithmic discrimination is one of the significant concerns in applying machine learning models to a real-world system. Many researchers have focused on developing fair machine learning algorithms without discrimination based on legally protected attributes. However, the existing research has barely explored various security issues that can occur while evaluating model fairness and verifying fair models. In this study, we propose a fairness audit framework that assesses the fairness of ML algorithms while addressing potential security issues such as data privacy, model secrecy, and trustworthiness. To this end, our proposed framework utilizes confidential computing and builds a chain of trust through enclave attestation primitives combined with public scrutiny and state-of-the-art software-based security techniques, enabling fair ML models to be securely certified and clients to verify a certified one. Our micro-benchmarks on various ML models and real-world datasets show the feasibility of the fairness certification implemented with Intel SGX in practice. In addition, we analyze the impact of data poisoning, which is an additional threat during data collection for fairness auditing. Based on the analysis, we illustrate the theoretical curves of fairness gap and minimal group size and the empirical results of fairness certification on poisoned datasets.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {3488–3499},
numpages = {12},
keywords = {Algorithmic audit, Confidential computing, Fairness, Security and privacy},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3358331.3358376,
author = {Easttom, Chuck},
title = {A Methodological Approach to Weaponizing Machine Learning},
year = {2019},
isbn = {9781450372022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358331.3358376},
doi = {10.1145/3358331.3358376},
abstract = {The current literature is replete with studies involving the use of machine learning algorithms for defensive security implementations. For example, machine learning has been utilized to enhance antivirus software and intrusion detection systems. The use of machine learning in defensive cybersecurity operations is well documented. However, there is a substantial gap in the literature on the offensive use of machine learning. Particularly, use of machine learning algorithms to enhance cyber warfare operations. Cyber components to modern conflicts, whether those conflicts are cyber or kinetic warfare, are a fact of the modern international political landscape. It is a natural progression to explore applications of machine learning to cyber warfare, particularly weaponized malware.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Advanced Manufacturing},
articleno = {45},
numpages = {5},
keywords = {Weaponized malware, cyber warfare, machine learning, weaponized malware},
location = {Dublin, Ireland},
series = {AIAM 2019}
}

@inproceedings{10.1145/2896387.2900324,
author = {Rahman, Md. Habibur and Sharmin, Sadia and Sarwar, Sheikh Muhammad and Shoyaib, Mohammad},
title = {Software Defect Prediction Using Feature Space Transformation},
year = {2016},
isbn = {9781450340632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896387.2900324},
doi = {10.1145/2896387.2900324},
abstract = {In software quality estimation research, software defect prediction is a key topic. A defect prediction model is generally constructed using a variety of software attributes and each attribute may have positive, negative or neutral effect on a specific model. Selection of an optimal set of attributes for model development remains a vital yet unexplored issue. In this paper, we have introduced a new feature space transformation process with a normalization technique to improve the defect prediction accuracy. We proposed a feature space transformation technique and classify the instances using Support Vector Machine (SVM) with its histogram intersection kernel. The proposed method is evaluated using the data sets from NASA metric data repository and its application demonstrates acceptable accuracy.},
booktitle = {Proceedings of the International Conference on Internet of Things and Cloud Computing},
articleno = {72},
numpages = {6},
keywords = {Attribute selection, Feature space transformation, Software defect prediction},
location = {Cambridge, United Kingdom},
series = {ICC '16}
}

@inproceedings{10.1109/ICSE.2019.00076,
author = {Cabral, George G. and Minku, Leandro L. and Shihab, Emad and Mujahid, Suhaib},
title = {Class imbalance evolution and verification latency in just-in-time software defect prediction},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00076},
doi = {10.1109/ICSE.2019.00076},
abstract = {Just-in-Time Software Defect Prediction (JIT-SDP) is an SDP approach that makes defect predictions at the software change level. Most existing JIT-SDP work assumes that the characteristics of the problem remain the same over time. However, JIT-SDP may suffer from class imbalance evolution. Specifically, the imbalance status of the problem (i.e., how much underrepresented the defect-inducing changes are) may be intensified or reduced over time. If occurring, this could render existing JIT-SDP approaches unsuitable, including those that rebuild classifiers over time using only recent data. This work thus provides the first investigation of whether class imbalance evolution poses a threat to JIT-SDP. This investigation is performed in a realistic scenario by taking into account verification latency - the often overlooked fact that labeled training examples arrive with a delay. Based on 10 GitHub projects, we show that JIT-SDP suffers from class imbalance evolution, significantly hindering the predictive performance of existing JIT-SDP approaches. Compared to state-of-the-art class imbalance evolution learning approaches, the predictive performance of JIT-SDP approaches was up to 97.2% lower in terms of g-mean. Hence, it is essential to tackle class imbalance evolution in JIT-SDP. We then propose a novel class imbalance evolution approach for the specific context of JIT-SDP. While maintaining top ranked g-means, this approach managed to produce up to 63.59% more balanced recalls on the defect-inducing and clean classes than state-of-the-art class imbalance evolution approaches. We thus recommend it to avoid overemphasizing one class over the other in JIT-SDP.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {666–676},
numpages = {11},
keywords = {class imbalance, concept drift, ensembles, online learning, software defect prediction, verification latency},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3368089.3418538,
author = {\v{C}egi\v{n}, J\'{a}n},
title = {Machine learning based test data generation for safety-critical software},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3418538},
doi = {10.1145/3368089.3418538},
abstract = {Unit testing focused on Modified Condition/Decision Coverage (MC/DC) criterion is essential in development safety-critical systems. However, design of test data that meets the MC/DC criterion currently needs detailed manual analysis of branching conditions in units under test by test engineers. Multiple state-of-art approaches exist with proven usage even in industrial projects. However, these approaches have multiple shortcomings, one of them being the Path explosion problem which has not been fully solved yet. Machine learning methods as meta-heuristic approximations can model behaviour of programs that are hard to test using traditional approaches, where the Path explosion problem does occur and thus could solve the limitations of the current state-of-art approaches. I believe, motivated by an ongoing collaboration with an industrial partner, that the machine learning methods could be combined with existing approaches to produce an approach suitable for testing of safety-critical projects.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1678–1681},
numpages = {4},
keywords = {MC/DC criterion, machine learning, test data generation, unit testing},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/2591062.2591151,
author = {Jing, Xiao-Yuan and Zhang, Zhi-Wu and Ying, Shi and Wang, Feng and Zhu, Yang-Ping},
title = {Software defect prediction based on collaborative representation classification},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591151},
doi = {10.1145/2591062.2591151},
abstract = {In recent years, machine learning techniques have been successfully applied into software defect prediction. Although they can yield reasonably good prediction results, there still exists much room for improvement on the aspect of prediction accuracy. Sparse representation is one of the most advanced machine learning techniques. It performs well with respect to signal compression and classification, but suffers from its time-consuming sparse coding. Compared with sparse representation, collaborative representation classification (CRC) can yield significantly lower computational complexity and competitive classification performance in pattern recognition domains. To achieve better defect prediction results, we introduce the CRC technique in this paper and propose a CRC based software defect prediction (CSDP) approach. We first design a CRC based learner to build a prediction model, whose computational burden is low. Then, we design a CRC based predictor to classify whether the query software modules are defective or defective-free. Experimental results on the widely used NASA datasets demonstrate the effectiveness and efficiency of the proposed approach.},
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {632–633},
numpages = {2},
keywords = {Collaborative representation classification, Machine learning, Prediction model, Software defect prediction},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@inproceedings{10.1145/3195546.3206423,
author = {Hamou-Lhadj, Wahab and Nayrolles, Mathieu},
title = {A project on software defect prevention at commit-time: a success story of university-industry research collaboration},
year = {2018},
isbn = {9781450357449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195546.3206423},
doi = {10.1145/3195546.3206423},
abstract = {In this talk, we describe a research collaboration project between Concordia University and Ubisoft. The project consists of investigating techniques for defect prevention at commit-time for increased software quality. The outcome of this project is a tool called CLEVER (Combining Levels of Bug Prevention and Resolution techniques) that uses machine learning to automatically detect coding defects as programmers write code. The main novelty of CLEVER is that it relies on code matching techniques to detect coding mistakes based on a database of historical code defects found in multiple related projects. The tool also proposes fixes based on known patterns.},
booktitle = {Proceedings of the 5th International Workshop on Software Engineering Research and Industrial Practice},
pages = {24–25},
numpages = {2},
keywords = {bug prevention at commit-time, machine learning, software maintenance and evolution, university-industry research project},
location = {Gothenburg, Sweden},
series = {SER&amp;IP '18}
}

@inproceedings{10.1145/2568225.2568320,
author = {Jing, Xiao-Yuan and Ying, Shi and Zhang, Zhi-Wu and Wu, Shan-Shan and Liu, Jin},
title = {Dictionary learning based software defect prediction},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568320},
doi = {10.1145/2568225.2568320},
abstract = {In order to improve the quality of a software system, software defect prediction aims to automatically identify defective software modules for efficient software test. To predict software defect, those classification methods with static code attributes have attracted a great deal of attention. In recent years, machine learning techniques have been applied to defect prediction. Due to the fact that there exists the similarity among different software modules, one software module can be approximately represented by a small proportion of other modules. And the representation coefficients over the pre-defined dictionary, which consists of historical software module data, are generally sparse. In this paper, we propose to use the dictionary learning technique to predict software defect. By using the characteristics of the metrics mined from the open source software, we learn multiple dictionaries (including defective module and defective-free module sub-dictionaries and the total dictionary) and sparse representation coefficients. Moreover, we take the misclassification cost issue into account because the misclassification of defective modules generally incurs much higher risk cost than that of defective-free ones. We thus propose a cost-sensitive discriminative dictionary learning (CDDL) approach for software defect classification and prediction. The widely used datasets from NASA projects are employed as test data to evaluate the performance of all compared methods. Experimental results show that CDDL outperforms several representative state-of-the-art defect prediction methods.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {414–423},
numpages = {10},
keywords = {Software defect prediction, cost-sensitive discriminative dictionary learning (CDDL), dictionary learning, sparse representation},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1109/ICSE-NIER52604.2021.00022,
author = {Panichella, Annibale and Liem, Cynthia C. S.},
title = {What are we really testing in mutation testing for machine learning? a critical reflection},
year = {2021},
isbn = {9780738133249},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER52604.2021.00022},
doi = {10.1109/ICSE-NIER52604.2021.00022},
abstract = {Mutation testing is a well-established technique for assessing a test suite's quality by injecting artificial faults into production code. In recent years, mutation testing has been extended to machine learning (ML) systems, and deep learning (DL) in particular; researchers have proposed approaches, tools, and statistically sound heuristics to determine whether mutants in DL systems are killed or not. However, as we will argue in this work, questions can be raised to what extent currently used mutation testing techniques in DL are actually in line with the classical interpretation of mutation testing. We observe that ML model development resembles a test-driven development (TDD) process, in which a training algorithm ('programmer') generates a model (program) that fits the data points (test data) to labels (implicit assertions), up to a certain threshold. However, considering proposed mutation testing techniques for ML systems under this TDD metaphor, in current approaches, the distinction between production and test code is blurry, and the realism of mutation operators can be challenged. We also consider the fundamental hypotheses underlying classical mutation testing: the competent programmer hypothesis and coupling effect hypothesis. As we will illustrate, these hypotheses do not trivially translate to ML system development, and more conscious and explicit scoping and concept mapping will be needed to truly draw parallels. Based on our observations, we propose several action points for better alignment of mutation testing techniques for ML with paradigms and vocabularies of classical mutation testing.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {66–70},
numpages = {5},
keywords = {machine learning, mutation operators, mutation testing, software testing},
location = {Virtual Event, Spain},
series = {ICSE-NIER '21}
}

@inproceedings{10.1145/3395363.3404364,
author = {Guo, Zichen and Liu, Jiawei and He, Tieke and Li, Zhuoyang and Zhangzhu, Peitian},
title = {TauJud: test augmentation of machine learning in judicial documents},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3404364},
doi = {10.1145/3395363.3404364},
abstract = {The booming of big data makes the adoption of machine learning ubiquitous in the legal field. As we all know, a large amount of test data can better reflect the performance of the model, so the test data must be naturally expanded. In order to solve the high cost problem of labeling data in natural language processing, people in the industry have improved the performance of text classification tasks through simple data amplification techniques. However, the data amplification requirements in the judgment documents are interpretable and logical, as observed from CAIL2018 test data with over 200,000 judicial documents. Therefore, we have designed a test augmentation tool called TauJud specifically for generating more effective test data with uniform distribution over time and location for model evaluation and save time in marking data. The demo can be found at https://github.com/governormars/TauJud.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {549–552},
numpages = {4},
keywords = {Judicial Documents, Machine Learning, Test Augmentation},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/2915970.2916007,
author = {Petri\'{c}, Jean and Bowes, David and Hall, Tracy and Christianson, Bruce and Baddoo, Nathan},
title = {The jinx on the NASA software defect data sets},
year = {2016},
isbn = {9781450336918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2915970.2916007},
doi = {10.1145/2915970.2916007},
abstract = {Background: The NASA datasets have previously been used extensively in studies of software defects. In 2013 Shepperd et al. presented an essential set of rules for removing erroneous data from the NASA datasets making this data more reliable to use.Objective: We have now found additional rules necessary for removing problematic data which were not identified by Shepperd et al.Results: In this paper, we demonstrate the level of erroneous data still present even after cleaning using Shepperd et al.'s rules and apply our new rules to remove this erroneous data.Conclusion: Even after systematic data cleaning of the NASA MDP datasets, we found new erroneous data. Data quality should always be explicitly considered by researchers before use.},
booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {13},
numpages = {5},
keywords = {data quality, machine learning, software defect prediction},
location = {Limerick, Ireland},
series = {EASE '16}
}

@proceedings{10.1145/3472674,
title = {MaLTESQuE 2021: Proceedings of the 5th International Workshop on Machine Learning Techniques for Software Quality Evolution},
year = {2021},
isbn = {9781450386258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the fifth edition of the workshop on Machine Learning Techniques for Software Quality Evaluation (MaLTeSQuE 2021) to be held virtually on August 23, 2021, co-located with the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2021).},
location = {Athens, Greece}
}

@inproceedings{10.1145/3425174.3425226,
author = {Santos, Sebasti\~{a}o H. N. and da Silveira, Beatriz Nogueira Carvalho and Andrade, Stev\~{a}o A. and Delamaro, M\'{a}rcio and Souza, Simone R. S.},
title = {An Experimental Study on Applying Metamorphic Testing in Machine Learning Applications},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425226},
doi = {10.1145/3425174.3425226},
abstract = {Machine learning techniques have been successfully employed in various areas and, in particular, for the development of healthcare applications, aiming to support in more effective and faster diagnostics (such as cancer diagnosis). However, machine learning models may present uncertainties and errors. Errors in the training process, classification, and evaluation can generate incorrect results and, consequently, to wrong clinical decisions, reducing the professionals' confidence in the use of such techniques. Similar to other application domains, the quality should be guaranteed to produce more reliable models capable of assisting health professionals in their daily activities. Metamorphic testing can be an interesting option to validate machine learning applications. Using this testing approach is possible to define relationships that define changes to be made in the application's input data to identify faults. This paper presents an experimental study to evaluate the effectiveness of metamorphic testing to validate machine learning applications. A Machine learning application to verify breast cancer diagnostic was developed, using an available dataset composed of 569 samples whose data were taken from breast cancer images, and used as the software under test, in which the metamorphic testing was applied. The results indicate that metamorphic testing can be an alternative to support the validation of machine learning applications.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {98–106},
numpages = {9},
keywords = {Experimental Study, Machine Learning, Metamorphic Test},
location = {Natal, Brazil},
series = {SAST '20}
}

@inproceedings{10.1109/ICSE43902.2021.00100,
author = {Velez, Miguel and Jamshidi, Pooyan and Siegmund, Norbert and Apel, Sven and K\"{a}stner, Christian},
title = {White-Box Analysis over Machine Learning: Modeling Performance of Configurable Systems},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00100},
doi = {10.1109/ICSE43902.2021.00100},
abstract = {Performance-influence models can help stakeholders understand how and where configuration options and their interactions influence the performance of a system. With this understanding, stakeholders can debug performance behavior and make deliberate configuration decisions. Current black-box techniques to build such models combine various sampling and learning strategies, resulting in tradeoffs between measurement effort, accuracy, and interpretability. We present Comprex, a white-box approach to build performance-influence models for configurable systems, combining insights of local measurements, dynamic taint analysis to track options in the implementation, compositionality, and compression of the configuration space, without relying on machine learning to extrapolate incomplete samples. Our evaluation on 4 widely-used, open-source projects demonstrates that Comprex builds similarly accurate performance-influence models to the most accurate and expensive black-box approach, but at a reduced cost and with additional benefits from interpretable and local models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1072–1084},
numpages = {13},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3239576.3239607,
author = {Du, Yuntao and Zhang, Lu and Shi, Jiahao and Tang, Jingjuan and Yin, Ying},
title = {Feature-Grouping-Based Two Steps Feature Selection Algorithm in Software Defect Prediction},
year = {2018},
isbn = {9781450364607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239576.3239607},
doi = {10.1145/3239576.3239607},
abstract = {In order to improve the effect of software defect prediction, many algorithms including feature selection, have been proposed. Based on Wrapper and Filter hybrid framework, a feature-grouping-based feature selection algorithm is proposed in this paper. The algorithm is composed of two steps. In the first step, in order to remove the redundant features, we group the features according to the redundancy between the features. The symmetry uncertainty is used as the constant indicator of the correlation and the FCBF-based grouping algorithm is used to group the features. In the second step, a subset of the features are selected from each group to form the final subset of features. Many classical methods select the representative feature from each group. We consider that when the number of intra-group features is large, the representative features are not enough to reflect the information in this group. Therefore, we require that at least one feature be selected within each group, in this step, the PSO algorithm is used for Searching Randomly from each group. We tested on the open source NASA and PROMISE data sets. Using three kinds of classifier. Compared to the other methods tested in this article, our method resulted in 90% improvement in the predictive performance of 30 sets of results on 10 data sets. Compared with the algorithms without feature selection, the AUC values of this method in the Logistic regression, Naive Bayesian, and K-neighbor classifiers are improved by 5.94% and 4.69% And 8.05%. The FCBF algorithm can also be regarded as a kind of first performing feature grouping. Compared with the FCBF algorithm, the AUC values of this method are improved by 4.78%, 6.41% and 4.4% on the basis of Logistic regression, Naive Bayes and K-neighbor. We can also see that for the FCBF-based grouping algorithm, it could be better to choose a characteristic cloud from each group than to choose a representative one.},
booktitle = {Proceedings of the 2nd International Conference on Advances in Image Processing},
pages = {173–178},
numpages = {6},
keywords = {FCBF-based grouping algorithm, Feature grouping, Intra-group feature selection, PSO, Software defect prediction},
location = {Chengdu, China},
series = {ICAIP '18}
}

@inproceedings{10.1145/3468264.3468615,
author = {Dutta, Saikat and Shi, August and Misailovic, Sasa},
title = {FLEX: fixing flaky tests in machine learning projects by updating assertion bounds},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468615},
doi = {10.1145/3468264.3468615},
abstract = {Many machine learning (ML) algorithms are inherently random – multiple executions using the same inputs may produce slightly different results each time. Randomness impacts how developers write tests that check for end-to-end quality of their implementations of these ML algorithms. In particular, selecting the proper thresholds for comparing obtained quality metrics with the reference results is a non-intuitive task, which may lead to flaky test executions.  We present FLEX, the first tool for automatically fixing flaky tests due to algorithmic randomness in ML algorithms. FLEX fixes tests that use approximate assertions to compare actual and expected values that represent the quality of the outputs of ML algorithms. We present a technique for systematically identifying the acceptable bound between the actual and expected output quality that also minimizes flakiness. Our technique is based on the Peak Over Threshold method from statistical Extreme Value Theory, which estimates the tail distribution of the output values observed from several runs. Based on the tail distribution, FLEX updates the bound used in the test, or selects the number of test re-runs, based on a desired confidence level.  We evaluate FLEX on a corpus of 35 tests collected from the latest versions of 21 ML projects. Overall, FLEX identifies and proposes a fix for 28 tests. We sent 19 pull requests, each fixing one test, to the developers. So far, 9 have been accepted by the developers.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {603–614},
numpages = {12},
keywords = {Extreme Value Theory, Flaky tests, Machine Learning},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1109/MSR.2019.00052,
author = {Bangash, Abdul Ali and Sahar, Hareem and Chowdhury, Shaiful and Wong, Alexander William and Hindle, Abram and Ali, Karim},
title = {What do developers know about machine learning: a study of ML discussions on StackOverflow},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00052},
doi = {10.1109/MSR.2019.00052},
abstract = {Machine learning, a branch of Artificial Intelligence, is now popular in software engineering community and is successfully used for problems like bug prediction, and software development effort estimation. Developers' understanding of machine learning, however, is not clear, and we require investigation to understand what educators should focus on, and how different online programming discussion communities can be more helpful. We conduct a study on Stack Overflow (SO) machine learning related posts using the SOTorrent dataset. We found that some machine learning topics are significantly more discussed than others, and others need more attention. We also found that topic generation with Latent Dirichlet Allocation (LDA) can suggest more appropriate tags that can make a machine learning post more visible and thus can help in receiving immediate feedback from sites like SO.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {260–264},
numpages = {5},
keywords = {machine learning, stackoverflow, topic modeling},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3379597.3387461,
author = {Chen, Yang and Santosa, Andrew E. and Yi, Ang Ming and Sharma, Abhishek and Sharma, Asankhaya and Lo, David},
title = {A Machine Learning Approach for Vulnerability Curation},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387461},
doi = {10.1145/3379597.3387461},
abstract = {Software composition analysis depends on database of open-source library vulerabilities, curated by security researchers using various sources, such as bug tracking systems, commits, and mailing lists. We report the design and implementation of a machine learning system to help the curation by by automatically predicting the vulnerability-relatedness of each data item. It supports a complete pipeline from data collection, model training and prediction, to the validation of new models before deployment. It is executed iteratively to generate better models as new input data become available. We use self-training to significantly and automatically increase the size of the training dataset, opportunistically maximizing the improvement in the models' quality at each iteration. We devised new deployment stability metric to evaluate the quality of the new models before deployment into production, which helped to discover an error. We experimentally evaluate the improvement in the performance of the models in one iteration, with 27.59% maximum PR AUC improvements. Ours is the first of such study across a variety of data sources. We discover that the addition of the features of the corresponding commits to the features of issues/pull requests improve the precision for the recall values that matter. We demonstrate the effectiveness of self-training alone, with 10.50% PR AUC improvement, and we discover that there is no uniform ordering of word2vec parameters sensitivity across data sources.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {32–42},
numpages = {11},
keywords = {application security, classifiers ensemble, machine learning, open-source software, self-training},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3510858.3511421,
author = {Wang, Yan},
title = {Automatic management analysis of computer software engineering project based on data mining},
year = {2022},
isbn = {9781450390422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510858.3511421},
doi = {10.1145/3510858.3511421},
abstract = {In recent years, with the continuous development of data technology, especially with the continuous improvement and research and development of software systems, software development has become a very complex system engineering. Because the backward software production method can't meet the increasing demand of computer software, a series of problems have appeared in the development and maintenance of software engineering. Software projects are different from other projects, and the general project management method can't be applied to all software project management. This paper puts forward the application of data mining in software engineering. Data mining is an interdisciplinary subject involving many fields, and it has many mature technologies. The quantitative analysis method of demand evolution influence based on data mining clusters the new demand and the existing demand of software projects, and puts forward corresponding strategies to solve them, thus improving the automation management level of computer software engineering.},
booktitle = {2021 International Conference on Aviation Safety and Information Technology},
pages = {910–914},
numpages = {5},
location = {Changsha, China},
series = {ICASIT 2021}
}

@inproceedings{10.1145/3340482.3342742,
author = {Borg, Markus and Svensson, Oscar and Berg, Kristian and Hansson, Daniel},
title = {SZZ unleashed: an open implementation of the SZZ algorithm - featuring example usage in a study of just-in-time bug prediction for the Jenkins project},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340482.3342742},
doi = {10.1145/3340482.3342742},
abstract = {Machine learning applications in software engineering often rely on detailed information about bugs. While issue trackers often contain information about when bugs were fixed, details about when they were introduced to the system are often absent. As a remedy, researchers often rely on the SZZ algorithm as a heuristic approach to identify bug-introducing software changes. Unfortunately, as reported in a recent systematic literature review, few researchers have made their SZZ implementations publicly available. Consequently, there is a risk that research effort is wasted as new projects based on SZZ output need to initially reimplement the approach. Furthermore, there is a risk that newly developed (closed source) SZZ implementations have not been properly tested, thus conducting research based on their output might introduce threats to validity. We present SZZ Unleashed, an open implementation of the SZZ algorithm for git repositories. This paper describes our implementation along with a usage example for the Jenkins project, and conclude with an illustrative study on just-in-time bug prediction. We hope to continue evolving SZZ Unleashed on GitHub, and warmly invite the community to contribute.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {7–12},
numpages = {6},
keywords = {SZZ, defect prediction, issue tracking, mining software repositories},
location = {Tallinn, Estonia},
series = {MaLTeSQuE 2019}
}

@inproceedings{10.1145/3395363.3397366,
author = {Dutta, Saikat and Shi, August and Choudhary, Rutvik and Zhang, Zhekun and Jain, Aryaman and Misailovic, Sasa},
title = {Detecting flaky tests in probabilistic and machine learning applications},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397366},
doi = {10.1145/3395363.3397366},
abstract = {Probabilistic programming systems and machine learning frameworks like Pyro, PyMC3, TensorFlow, and PyTorch provide scalable and efficient primitives for inference and training. However, such operations are non-deterministic. Hence, it is challenging for developers to write tests for applications that depend on such frameworks, often resulting in flaky tests – tests which fail non-deterministically when run on the same version of code.  In this paper, we conduct the first extensive study of flaky tests in this domain. In particular, we study the projects that depend on four frameworks: Pyro, PyMC3, TensorFlow-Probability, and PyTorch. We identify 75 bug reports/commits that deal with flaky tests, and we categorize the common causes and fixes for them. This study provides developers with useful insights on dealing with flaky tests in this domain.  Motivated by our study, we develop a technique, FLASH, to systematically detect flaky tests due to assertions passing and failing in different runs on the same code. These assertions fail due to differences in the sequence of random numbers in different runs of the same test. FLASH exposes such failures, and our evaluation on 20 projects results in 11 previously-unknown flaky tests that we reported to developers.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {211–224},
numpages = {14},
keywords = {Flaky tests, Machine Learning, Non-Determinism, Probabilistic Programming, Randomness},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@article{10.1145/2841425,
author = {Natella, Roberto and Cotroneo, Domenico and Madeira, Henrique S.},
title = {Assessing Dependability with Software Fault Injection: A Survey},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/2841425},
doi = {10.1145/2841425},
abstract = {With the rise of software complexity, software-related accidents represent a significant threat for computer-based systems. Software Fault Injection is a method to anticipate worst-case scenarios caused by faulty software through the deliberate injection of software faults. This survey provides a comprehensive overview of the state of the art on Software Fault Injection to support researchers and practitioners in the selection of the approach that best fits their dependability assessment goals, and it discusses how these approaches have evolved to achieve fault representativeness, efficiency, and usability. The survey includes a description of relevant applications of Software Fault Injection in the context of fault-tolerant systems.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {44},
numpages = {55},
keywords = {Software faults, dependability assessment, software fault tolerance}
}

@inproceedings{10.1145/2851613.2851788,
author = {das D\^{o}res, Silvia N. and Alves, Luciano and Ruiz, Duncan D. and Barros, Rodrigo C.},
title = {A meta-learning framework for algorithm recommendation in software fault prediction},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851788},
doi = {10.1145/2851613.2851788},
abstract = {Software fault prediction is a significant part of software quality assurance and it is commonly used to detect faulty software modules based on software measurement data. Several machine learning based approaches have been proposed for generating predictive models from collected data, although none has become standard given the specificities of each software project. Hence, we believe that recommending the best algorithm for each project is much more important and useful than developing a single algorithm for being used in any project. For achieving that goal, we propose in this paper a novel framework for recommending machine learning algorithms that is capable of automatically identifying the most suitable algorithm according to the software project that is being considered. Our solution, namely SFP-MLF, makes use of the meta-learning paradigm in order to learn the best learner for a particular project. Results show that the SFP-MLF framework provides both the best single algorithm recommendation and also the best ranking recommendation for the software fault prediction problem.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1486–1491},
numpages = {6},
keywords = {algorithm recommendation, machine learning, meta-learning, software fault prediction, software quality},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00034,
author = {Lwakatare, Lucy Ellen and R\r{a}nge, Ellinor and Crnkovic, Ivica and Bosch, Jan},
title = {On the experiences of adopting automated data validation in an industrial machine learning project},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00034},
doi = {10.1109/ICSE-SEIP52600.2021.00034},
abstract = {Background: Data errors are a common challenge in machine learning (ML) projects and generally cause significant performance degradation in ML-enabled software systems. To ensure early detection of erroneous data and avoid training ML models using bad data, research and industrial practice suggest incorporating a data validation process and tool in ML system development process.Aim: The study investigates the adoption of a data validation process and tool in industrial ML projects. The data validation process demands significant engineering resources for tool development and maintenance. Thus, it is important to identify the best practices for their adoption especially by development teams that are in the early phases of deploying ML-enabled software systems.Method: Action research was conducted at a large-software intensive organization in telecommunications, specifically within the analytics R&amp;D organization for an ML use case of classifying faults from returned hardware telecommunication devices.Results: Based on the evaluation results and learning from our action research, we identified three best practices, three benefits, and two barriers to adopting the data validation process and tool in ML projects. We also propose a data validation framework (DVF) for systematizing the adoption of a data validation process.Conclusions: The results show that adopting a data validation process and tool in ML projects is an effective approach of testing ML-enabled software systems. It requires having an overview of the level of data (feature, dataset, cross-dataset, data stream) at which certain data quality tests can be applied.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {248–257},
numpages = {10},
keywords = {data errors, data quality, data validation, machine learning, software engineering},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00049,
author = {Zhu, Junjie and Long, Teng and Memon, Atif},
title = {Automatically authoring regression tests for machine-learning based systems},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00049},
doi = {10.1109/ICSE-SEIP52600.2021.00049},
abstract = {Two key design characteristics of machine learning (ML) systems---their ever-improving nature, and learning-based emergent functional behavior---create a moving target, posing new challenges for authoring/maintaining functional regression tests. We identify four specific challenges and address them by developing a new general methodology to automatically author and maintain tests. In particular, we use the volume of production data to periodically refresh our large corpus of test inputs and expected outputs; we use perturbation of the data to obtain coverage-adequate tests; and we use clustering to help identify patterns of failures that are indicative of software bugs. We demonstrate our methodology on an ML-based context-aware Speller. Our coverage-adequate, approx. 1 million regression test cases, automatically authored and maintained for Speller (1) are virtually maintenance free, (2) detect a higher number of Speller failures than previous manually-curated tests, (3) have better coverage of previously unknown functional boundaries of the ML component, and (4) lend themselves to automatic failure triaging by clustering and prioritizing subcategories of tests with over-represented failures. We identify several systematic failure patterns which were due to previously undetected bugs in the Speller, e.g., (1) when the user misses the first letter in a short word, and (2) when the user mistakenly inserts a character in the last token of an address; these have since been fixed.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {374–383},
numpages = {10},
keywords = {ML testing, ML-based testing, spelling correction},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1145/3318299.3318345,
author = {Li, ZhanJun and Shao, Yan},
title = {A Survey of Feature Selection for Vulnerability Prediction Using Feature-based Machine Learning},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318345},
doi = {10.1145/3318299.3318345},
abstract = {This paper summarized the basic process of software vulnerability prediction using feature-based machine learning for the first time. In addition to sorting out the related types and basis of vulnerability features definition, the advantages and disadvantages of different methods are compared. Finally, this paper analyzed the difficulties and challenges in this research field, and put forward some suggestions for future work.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {36–42},
numpages = {7},
keywords = {Software vulnerability prediction, feature, machine learning},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@inproceedings{10.1145/3510003.3510091,
author = {Li, Yanhui and Meng, Linghan and Chen, Lin and Yu, Li and Wu, Di and Zhou, Yuming and Xu, Baowen},
title = {Training data debugging for the fairness of machine learning software},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510091},
doi = {10.1145/3510003.3510091},
abstract = {With the widespread application of machine learning (ML) software, especially in high-risk tasks, the concern about their unfairness has been raised towards both developers and users of ML software. The unfairness of ML software indicates the software behavior affected by the sensitive features (e.g., sex), which leads to biased and illegal decisions and has become a worthy problem for the whole software engineering community.According to the "data-driven" programming paradigm of ML software, we consider the root cause of the unfairness as biased features in training data. Inspired by software debugging, we propose a novel method, Linear-regression based Training Data Debugging (LTDD), to debug feature values in training data, i.e., (a) identify which features and which parts of them are biased, and (b) exclude the biased parts of such features to recover as much valuable and unbiased information as possible to build fair ML software. We conduct an extensive study on nine data sets and three classifiers to evaluate the effect of our method LTDD compared with four baseline methods. Experimental results show that (a) LTDD can better improve the fairness of ML software with less or comparable damage to the performance, and (b) LTDD is more actionable for fairness improvement in realistic scenarios.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {2215–2227},
numpages = {13},
keywords = {ML software, debugging, fairness, training data},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3338906.3342484,
author = {Moghadam, Mahshid Helali},
title = {Machine learning-assisted performance testing},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3342484},
doi = {10.1145/3338906.3342484},
abstract = {Automated testing activities like automated test case generation imply a reduction in human effort and cost, with the potential to impact the test coverage positively. If the optimal policy, i.e., the course of actions adopted, for performing the intended test activity could be learnt by the testing system, i.e., a smart tester agent, then the learnt policy could be reused in analogous situations which leads to even more efficiency in terms of required efforts. Performance testing under stress execution conditions, i.e., stress testing, which involves providing extreme test conditions to find the performance breaking points, remains a challenge, particularly for complex software systems. Some common approaches for generating stress test conditions are based on source code or system model analysis, or use-case based design approaches. However, source code or precise system models might not be easily available for testing. Moreover, drawing a precise performance model is often difficult, particularly for complex systems. In this research, I have used model-free reinforcement learning to build a self-adaptive autonomous stress testing framework which is able to learn the optimal policy for stress test case generation without having a model of the system under test. The conducted experimental analysis shows that the proposed smart framework is able to generate the stress test conditions for different software systems efficiently and adaptively without access to performance models.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1187–1189},
numpages = {3},
keywords = {Autonomous testing, Performance testing, Reinforcement learning, Stress testing, Test case generation},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3266003.3266004,
author = {de Santiago, Valdivino Alexandre and da Silva, Leoni Augusto Romain and de Andrade Neto, Pedro Ribeiro},
title = {Testing Environmental Models supported by Machine Learning},
year = {2018},
isbn = {9781450365550},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266003.3266004},
doi = {10.1145/3266003.3266004},
abstract = {In this paper we present a new methodology, DaOBML, to test environmental models whose outputs are complex artifacts such as images (maps) or plots. Our approach suggests several test data generation techniques (Combinatorial Interaction Testing, Model-Based Testing, Random Testing) and digital image processing methods to drive the creation of Knowledge Bases (KBs). Considering such KBs and Machine Learning (ML) algorithms, a test oracle assigns the verdicts of new test data. Our methodology is supported by a tool and we applied it to models developed via the TerraME product. A controlled experiment was carried out and we conclude that Random Testing is the most feasible test data generation approach for developing the KBs, Artificial Neural Networks present the best performance out of six ML algorithms, and the larger the KB, in terms of size, the better.},
booktitle = {Proceedings of the III Brazilian Symposium on Systematic and Automated Software Testing},
pages = {3–12},
numpages = {10},
keywords = {Combinatorial Interaction Testing, Digital Image Processing, Empirical Software Engineering, Environmental Modeling, Machine Learning, Model-Based Testing, Random Testing},
location = {SAO CARLOS, Brazil},
series = {SAST '18}
}

@inproceedings{10.1145/3348445.3348453,
author = {Cynthia, Shamse Tasnim and Ripon, Shamim H.},
title = {Predicting and Classifying Software Faults: A Data Mining Approach},
year = {2019},
isbn = {9781450371957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3348445.3348453},
doi = {10.1145/3348445.3348453},
abstract = {In the field of software engineering, the detection of fault in the software has become a major topic to explore. With the help of data mining and machine learning approaches, this paper aims to denote whether a software is fault prone or not. In order to accomplish that this paper gives importance to compare between different machine learning approaches and by observing their performances we can conclude which models perform better to detect fault in the selected software modules. The dataset we have chosen to work on has imbalanced data. This paper research also worked with the imbalanced dataset and what results the imbalanced dataset gave when examined. The accuracy comparison, the performance of the different metrics can broadly help in software defect detection mechanism.},
booktitle = {Proceedings of the 7th International Conference on Computer and Communications Management},
pages = {143–147},
numpages = {5},
keywords = {Adaboost, SVM, Software faults, association rules, data mining, prediction},
location = {Bangkok, Thailand},
series = {ICCCM '19}
}

@inproceedings{10.1145/3452383.3452400,
author = {Misra, Janardan and Podder, Sanjay},
title = {Association of Defect Log Suitability for Machine Learning with Performance: An Experience Report},
year = {2021},
isbn = {9781450390460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452383.3452400},
doi = {10.1145/3452383.3452400},
abstract = {Machine learning (ML) based solutions utilizing textual details in defect logs have been shown to enable automation of defect management process and make it cost effective. In this work, we assess effectiveness of apriori manual analysis of the suitability of applying ML to problems encountered during defect management process. We consider problems of mapping defects to service engineers and business processes for designing experiments. Experimental analysis on these problems using multiple defect logs from practice reveals that a systematic analysis of the defect log data by project experts can provide approximate indication of the eventual performance of the ML model even before they are actually built. We discuss practical significance of the conclusions for designing ML based solutions in-practice.},
booktitle = {Proceedings of the 14th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {17},
numpages = {5},
keywords = {Assignee Recommendation, Business Process Mapping, Defect Management Life-Cycle, Machine Learning Suitability, Mining Defect Repositories, Text Analysis},
location = {Bhubaneswar, Odisha, India},
series = {ISEC '21}
}

@inproceedings{10.1145/3220267.3220286,
author = {El-Shorbagy, Sara Adel and El-Gammal, Wael Mohamed and Abdelmoez, Walid M.},
title = {Using SMOTE and Heterogeneous Stacking in Ensemble learning for Software Defect Prediction},
year = {2018},
isbn = {9781450364690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220267.3220286},
doi = {10.1145/3220267.3220286},
abstract = {Nowadays, there are a lot of classifications models used for predictions in the software engineering field such as effort estimation and defect prediction. One of these models is the ensemble learning machine that improves model performance by combining multiple models in different ways to get a more powerful model.One of the problems facing the prediction model is the misclassification of the minority samples. This problem mainly appears in the case of defect prediction. Our aim is the classification of defects which are considered minority samples during the training phase. This can be improved by implementing the Synthetic Minority Over-Sampling Technique (SMOTE) before the implementation of the ensemble model which leads to over-sample the minority class instances.In this paper, our work propose applying a new ensemble model by combining the SMOTE technique with the heterogeneous stacking ensemble to get the most benefit and performance in training a dataset that focus on the minority subset as in the software prediction study. Our proposed model shows better performance that overcomes other techniques results applied on the minority samples of the defect prediction.},
booktitle = {Proceedings of the 7th International Conference on Software and Information Engineering},
pages = {44–47},
numpages = {4},
keywords = {Classification, Defect Prediction, Ensemble, Heterogeneous, Machine Learning, SMOTE, Software Engineering, Stacking},
location = {Cairo, Egypt},
series = {ICSIE '18}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00066,
author = {Wan, Chengcheng and Liu, Shicheng and Hoffmann, Henry and Maire, Michael and Lu, Shan},
title = {A replication of are machine learning cloud APIs used correctly},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00066},
doi = {10.1109/ICSE-Companion52605.2021.00066},
abstract = {This artifact aims to provide benchmark suite, data, and script used in our study "Are Machine Learning Cloud APIs Used Correctly?". We collected a suite of 360 non-trivial applications that use ML cloud APIs for manual study. We also developed checkers and tool to detect and fix API mis-uses. We hope this artifact can motivate and help future research to further tackle ML API mis-uses. All related data are available online.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {158–159},
numpages = {2},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/2961111.2962610,
author = {Petri\'{c}, Jean and Bowes, David and Hall, Tracy and Christianson, Bruce and Baddoo, Nathan},
title = {Building an Ensemble for Software Defect Prediction Based on Diversity Selection},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962610},
doi = {10.1145/2961111.2962610},
abstract = {Background: Ensemble techniques have gained attention in various scientific fields. Defect prediction researchers have investigated many state-of-the-art ensemble models and concluded that in many cases these outperform standard single classifier techniques. Almost all previous work using ensemble techniques in defect prediction rely on the majority voting scheme for combining prediction outputs, and on the implicit diversity among single classifiers. Aim: Investigate whether defect prediction can be improved using an explicit diversity technique with stacking ensemble, given the fact that different classifiers identify different sets of defects. Method: We used classifiers from four different families and the weighted accuracy diversity (WAD) technique to exploit diversity amongst classifiers. To combine individual predictions, we used the stacking ensemble technique. We used state-of-the-art knowledge in software defect prediction to build our ensemble models, and tested their prediction abilities against 8 publicly available data sets. Conclusion: The results show performance improvement using stacking ensembles compared to other defect prediction models. Diversity amongst classifiers used for building ensembles is essential to achieving these performance improvements.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {46},
numpages = {10},
keywords = {Software defect prediction, diversity, ensembles of learning machines, software faults, stacking},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@inproceedings{10.1145/2810146.2810150,
author = {Mahmood, Zaheed and Bowes, David and Lane, Peter C. R. and Hall, Tracy},
title = {What is the Impact of Imbalance on Software Defect Prediction Performance?},
year = {2015},
isbn = {9781450337151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810146.2810150},
doi = {10.1145/2810146.2810150},
abstract = {Software defect prediction performance varies over a large range. Menzies suggested there is a ceiling effect of 80% Recall [8]. Most of the data sets used are highly imbalanced. This paper asks, what is the empirical effect of using different datasets with varying levels of imbalance on predictive performance? We use data synthesised by a previous meta-analysis of 600 fault prediction models and their results. Four model evaluation measures (the Mathews Correlation Coefficient (MCC), F-Measure, Precision and Recall) are compared to the corresponding data imbalance ratio. When the data are imbalanced, the predictive performance of software defect prediction studies is low. As the data become more balanced, the predictive performance of prediction models increases, from an average MCC of 0.15, until the minority class makes up 20% of the instances in the dataset, where the MCC reaches an average value of about 0.34. As the proportion of the minority class increases above 20%, the predictive performance does not significantly increase. Using datasets with more than 20% of the instances being defective has not had a significant impact on the predictive performance when using MCC. We conclude that comparing the results of defect prediction studies should take into account the imbalance of the data.},
booktitle = {Proceedings of the 11th International Conference on Predictive Models and Data Analytics in Software Engineering},
articleno = {4},
numpages = {4},
keywords = {Data Imbalance, Defect Prediction, Machine Learning},
location = {Beijing, China},
series = {PROMISE '15}
}

@inproceedings{10.1145/3196398.3196459,
author = {Bulmer, Tyson and Montgomery, Lloyd and Damian, Daniela},
title = {Predicting developers' IDE commands with machine learning},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196459},
doi = {10.1145/3196398.3196459},
abstract = {When a developer is writing code they are usually focused and in a state-of-mind which some refer to as flow. Breaking out of this flow can cause the developer to lose their train of thought and have to start their thought process from the beginning. This loss of thought can be caused by interruptions and sometimes slow IDE interactions. Predictive functionality has been harnessed in user applications to speed up load times, such as in Google Chrome's browser which has a feature called "Predicting Network Actions". This will pre-load web-pages that the user is most likely to click through. This mitigates the interruption that load times can introduce. In this paper we seek to make the first step towards predicting user commands in the IDE. Using the MSR 2018 Challenge Data of over 3000 developer session and over 10 million recorded events, we analyze and cleanse the data to be parsed into event series, which can then be used to train a variety of machine learning models, including a neural network, to predict user induced commands. Our highest performing model is able to obtain a 5 cross-fold validation prediction accuracy of 64%.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {82–85},
numpages = {4},
keywords = {IDE monitoring, developer commands, machine learning, neural network},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1145/3482909.3482911,
author = {Santos, Sebasti\~{a}o and Silveira, Beatriz and Durelli, Vinicius and Durelli, Rafael and Souza, Simone and Delamaro, Marcio},
title = {On Using Decision Tree Coverage Criteria forTesting Machine Learning Models},
year = {2021},
isbn = {9781450385039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482909.3482911},
doi = {10.1145/3482909.3482911},
abstract = {Over the past decade, there has been a growing interest in applying machine learning (ML) to address a myriad of tasks. Owing to this interest, the adoption of ML-based systems has gone mainstream. However, this widespread adoption of ML-based systems poses new challenges for software testers that must improve the quality and reliability of these ML-based solutions. To cope with the challenges of testing ML-based systems, we propose novel test adequacy criteria based on decision tree models. Differently from the traditional approach to testing ML models, which relies on manual collection and labelling of data, our criteria leverage the internal structure of decision tree models to guide the selection of test inputs. Thus, we introduce decision tree coverage (DTC) and boundary value analysis (BVA) as approaches to systematically guide the creation of effective test data that exercises key structural elements of a given decision tree model. To evaluate these criteria, we carried out an experiment using 12 datasets. We measured the effectiveness of test inputs in terms of the difference in model’s behavior between the test input and the training data. The experiment results indicate that our testing criteria can be used to guide the generation of effective test data.},
booktitle = {Proceedings of the 6th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {1–9},
numpages = {9},
keywords = {Decision Tree, Software Testing, Testing Criterion},
location = {Joinville, Brazil},
series = {SAST '21}
}

@inproceedings{10.1145/3266237.3266273,
author = {Braga, Rony\'{e}rison and Neto, Pedro Santos and Rab\^{e}lo, Ricardo and Santiago, Jos\'{e} and Souza, Matheus},
title = {A machine learning approach to generate test oracles},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266273},
doi = {10.1145/3266237.3266273},
abstract = {One of the essential activities for quality assurance in software development is the software testing. Studies report that Software Testing is one of the most costly activities in the development process, can reach up to 50 percent of its total cost. One of the great challenges of conducting software testing is related to the automation of a mechanism known as "test oracle". This work presents an approach based on machine learning (ML) for automation of the test oracle mechanism in software. The approach uses historical usage data from an application captured by inserting a capture component into the application under test. These data go through a Knowledge Discovery in Database step and are then used for training to generate an oracle suitable for the application under test. Four experiments were executed with web applications to evaluate the proposed approach. The first and second experiments were performed with a fictitious application, with faults inserted randomly in the first experiment, inserted by a developer in the second one and inserted by mutation tests in third one. The fourth experiment was carried out with a large real application in order to assure the results of the preliminary experiments. The experiments presented indications of the suitability of the approach to the solution of the problem.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {142–151},
numpages = {10},
keywords = {machine learning, test oracle, testing automation},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@inproceedings{10.5555/3507788.3507810,
author = {Korlepara, Piyush and Grigoriou, Marios and Kontogiannis, Kostas and Brealey, Chris and Giammaria, Alberto},
title = {Combining domain expert knowledge and machine learning for the identification of error prone files},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {Identifying as early as possible fault prone modules in order to facilitate continuous delivery in large software systems, has been an area where significant attention has been paid over the past few years. Recent efforts consider source code metrics and process metrics for training machine learning models to predict whether a software source code file is fault prone or not. In such prediction frameworks the accuracy of the trained model relies heavily on the features selected and the profiles of the metrics used for training the model which are unique to each system. Furthermore, these models act as black-boxes, where the end-user does not know how a specific prediction was reached. In this paper, we propose an approach which allows for domain expert knowledge to be combined with machine learning in order to yield fault-proneness prediction models that both exhibit high levels of recall and at the same time are able to provide explanations to the developers as to how and why these predictions were reached. For this paper we apply two rule-based inferencing techniques namely, Fuzzy reasoning, and Markov Logic Networks. The main contribution of this work is that it allows for expert developers to identify in the form of if-then rules domain logic that pertains to the fault-proneness of a source code file in the specific system being analysed. Results obtained from 19 open source systens indicate that MLNs perform better than Fuzzy Logic models and that project-customized rules achieve better results than generic rules. Furthermore, results indicate that its possible to compile a common set of rules that yields consistently acceptable results across different projects.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {153–162},
numpages = {10},
keywords = {continuous software engineering, fault-proneness prediction, process metrics, software repositories},
location = {Toronto, Canada},
series = {CASCON '21}
}

@proceedings{10.1145/3416505,
title = {MaLTeSQuE 2020: Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation},
year = {2020},
isbn = {9781450381246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the fourth edition of the workshop on Machine Learning Techniques for Software Quality Evaluation (MaLTeSQuE 2020) to be held virtually on November 16, 2020, co-located with the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2020).},
location = {Virtual, USA}
}

@inproceedings{10.1145/3409501.3409543,
author = {Yan, Ziyue and Zong, Lu},
title = {Spatial Prediction of Housing Prices in Beijing Using Machine Learning Algorithms},
year = {2020},
isbn = {9781450375603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409501.3409543},
doi = {10.1145/3409501.3409543},
abstract = {The real estate industry places key influence on almost every aspect of social economy given its great financing capacity and prolonged upstream and downstream industry chain. Therefore, predicting housing prices is regarded as an emerging topic in the recent decades. Hedonic Regression and Machine Learning Algorithms are two main methods in this field. This study aims to explore the important explanatory features and determine an accurate mechanism to implement spatial prediction of housing prices in Beijing by incorporating a list of machine learning techniques, including XGBoost, linear regression, Random Forest Regression, Ridge and Lasso Model, bagging and boosting, based on the housing price and features data in Beijing, China. Our result shows that compared to traditional hedonic method, machine learning methods demonstrate significant improvements on the accuracy of estimation despite that they are more time-costly. Moreover, it is found that XGBoost is the most accurate model in explaining and prediciting the spatial dynamics of housing prices in Beijing.},
booktitle = {Proceedings of the 2020 4th High Performance Computing and Cluster Technologies Conference &amp; 2020 3rd International Conference on Big Data and Artificial Intelligence},
pages = {64–71},
numpages = {8},
keywords = {Housing Price, Machine Learning Algorithms, Prediction, Spatial Modeling},
location = {Qingdao, China},
series = {HPCCT &amp; BDAI '20}
}

@inproceedings{10.1109/RAISE.2019.00010,
author = {Ferenc, Rudolf and Hegedundefineds, P\'{e}ter and Gyimesi, P\'{e}ter and Antal, G\'{a}bor and B\'{a}n, D\'{e}nes and Gyim\'{o}thy, Tibor},
title = {Challenging machine learning algorithms in predicting vulnerable JavaScript functions},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/RAISE.2019.00010},
doi = {10.1109/RAISE.2019.00010},
abstract = {The rapid rise of cyber-crime activities and the growing number of devices threatened by them place software security issues in the spotlight. As around 90% of all attacks exploit known types of security issues, finding vulnerable components and applying existing mitigation techniques is a viable practical approach for fighting against cyber-crime. In this paper, we investigate how the state-of-the-art machine learning techniques, including a popular deep learning algorithm, perform in predicting functions with possible security vulnerabilities in JavaScript programs.We applied 8 machine learning algorithms to build prediction models using a new dataset constructed for this research from the vulnerability information in public databases of the Node Security Project and the Snyk platform, and code fixing patches from GitHub. We used static source code metrics as predictors and an extensive grid-search algorithm to find the best performing models. We also examined the effect of various re-sampling strategies to handle the imbalanced nature of the dataset.The best performing algorithm was KNN, which created a model for the prediction of vulnerable functions with an F-measure of 0.76 (0.91 precision and 0.66 recall). Moreover, deep learning, tree and forest based classifiers, and SVM were competitive with F-measures over 0.70. Although the F-measures did not vary significantly with the re-sampling strategies, the distribution of precision and recall did change. No re-sampling seemed to produce models preferring high precision, while resampling strategies balanced the IR measures.},
booktitle = {Proceedings of the 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
pages = {8–14},
numpages = {7},
keywords = {JavaScript, code metrics, dataset, deep learning, machine learning, vulnerability},
location = {Montreal, Quebec, Canada},
series = {RAISE '19}
}

@inproceedings{10.5555/3432601.3432618,
author = {Grigoriou, Marios-Stavros and Kontogiannis, Kostas and Giammaria, Alberto and Brealey, Chris},
title = {Report on evaluation experiments using different machine learning techniques for defect prediction},
year = {2020},
publisher = {IBM Corp.},
address = {USA},
abstract = {With the emergence of AI, it is of no surprise that the application of Machine Learning techniques has attracted the attention of numerous software maintenance groups around the world. For defect proneness classification in particular, the use of Machine Learning classifiers has been touted as a promising approach. As a consequence, a large volume of research works has been published in the related research literature, utilizing either proprietary data sets or the PROMISE data repository which, for the purposes of this study, focuses only on the use of source code metrics as defect prediction training features. It has been argued though by several researchers, that process metrics may provide a better option as training features than source code metrics. For this paper, we have conducted a detailed extraction of GitHub process metrics from 148 open source systems, and we report on the findings of experiments conducted by using different Machine Learning classification algorithms for defect proneness classification. The main purpose of the paper is not to propose yet another Machine Learning technique for defect proneness classification, but to present to the community a very large data set using process metrics as opposed to source code metrics, and draw some initial interesting conclusions from this statistically significant data set.},
booktitle = {Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering},
pages = {123–132},
numpages = {10},
location = {Toronto, Ontario, Canada},
series = {CASCON '20}
}

@article{10.1145/3407183,
author = {Jagtap, Pushpak and Abdi, Fardin and Rungger, Matthias and Zamani, Majid and Caccamo, Marco},
title = {Software Fault Tolerance for Cyber-Physical Systems via Full System Restart},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2378-962X},
url = {https://doi.org/10.1145/3407183},
doi = {10.1145/3407183},
abstract = {The article addresses the issue of reliability of complex embedded control systems in the safety-critical environment. In this article, we propose a novel approach to design controller that (i) guarantees the safety of nonlinear physical systems, (ii) enables safe system restart during runtime, and (iii) allows the use of complex, unverified controllers (e.g., neural networks) that drive the physical systems toward complex specifications. We use abstraction-based controller synthesis approach to design a formally verified controller that provides application and system-level fault tolerance along with safety guarantee. Moreover, our approach is implementable using a commercial-off-the-shelf (COTS) processing unit. To demonstrate the efficacy of our solution and to verify the safety of the system under various types of faults injected in applications and in the underlying real-time operating system (RTOS), we implemented the proposed controller for the inverted pendulum and three degrees-of-freedom (3-DOF) helicopter.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = aug,
articleno = {47},
numpages = {20},
keywords = {Cyber-physical systems, abstraction-based control, fault-tolerance, full system restart, nonlinear systems}
}

@inproceedings{10.1145/3387940.3391490,
author = {Liem, Cynthia C. S. and Panichella, Annibale},
title = {Oracle Issues in Machine Learning and Where to Find Them},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391490},
doi = {10.1145/3387940.3391490},
abstract = {The rise in popularity of machine learning (ML), and deep learning in particular, has both led to optimism about achievements of artificial intelligence, as well as concerns about possible weaknesses and vulnerabilities of ML pipelines. Within the software engineering community, this has led to a considerable body of work on ML testing techniques, including white- and black-box testing for ML models. This means the oracle problem needs to be addressed. For supervised ML applications, oracle information is indeed available in the form of dataset 'ground truth', that encodes input data with corresponding desired output labels. However, while ground truth forms a gold standard, there still is no guarantee it is truly correct. Indeed, syntactic, semantic, and conceptual framing issues in the oracle may negatively affect the ML system's integrity. While syntactic issues may automatically be verified and corrected, the higher-level issues traditionally need human judgment and manual analysis. In this paper, we employ two heuristics based on information entropy and semantic analysis on well-known computer vision models and benchmark data from ImageNet. The heuristics are used to semi-automatically uncover potential higher-level issues in (i) the label taxonomy used to define the ground truth oracle (labels), and (ii) data encoding and representation. In doing this, beyond existing ML testing efforts, we illustrate the need for software engineering strategies that especially target and assess the oracle.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {483–488},
numpages = {6},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3338906.3338937,
author = {Aggarwal, Aniya and Lohia, Pranay and Nagar, Seema and Dey, Kuntal and Saha, Diptikalyan},
title = {Black box fairness testing of machine learning models},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338937},
doi = {10.1145/3338906.3338937},
abstract = {Any given AI system cannot be accepted unless its trustworthiness is proven. An important characteristic of a trustworthy AI system is the absence of algorithmic bias. 'Individual discrimination' exists when a given individual different from another only in 'protected attributes' (e.g., age, gender, race, etc.) receives a different decision outcome from a given machine learning (ML) model as compared to the other individual. The current work addresses the problem of detecting the presence of individual discrimination in given ML models. Detection of individual discrimination is test-intensive in a black-box setting, which is not feasible for non-trivial systems. We propose a methodology for auto-generation of test inputs, for the task of detecting individual discrimination. Our approach combines two well-established techniques - symbolic execution and local explainability for effective test case generation. We empirically show that our approach to generate test cases is very effective as compared to the best-known benchmark systems that we examine.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {625–635},
numpages = {11},
keywords = {Fairness Testing, Individual Discrimination, Local Explainability, Symbolic Execution},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1109/ICSE.2019.00029,
author = {He, Zhijian and Chen, Yao and Huang, Enyan and Wang, Qixin and Pei, Yu and Yuan, Haidong},
title = {A system identification based Oracle for control-CPS software fault localization},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00029},
doi = {10.1109/ICSE.2019.00029},
abstract = {Control-CPS software fault localization (SFL, aka bug localization) is of critical importance as bugs may cause major failures, even injuries/deaths. To locate the bugs in control-CPSs, SFL tools often demand many labeled ("correct"/"incorrect") source code execution traces as inputs. To label the correctness of these traces, we must judge the corresponding control-CPS physical trajectories' correctness. However, unlike discrete outputs, the boundaries between correct and incorrect physical trajectories are often vague. The mechanism (aka oracle) to judge the physical trajectories' correctness thus becomes a major challenge. So far, the ad hoc practice of "human oracles" is still widely used, whose qualities heavily depend on the human experts' expertise and availability. This paper proposes an oracle based on the well adopted autoregressive system identification (AR-SI). With proven success for controlling black-box physical systems, AR-SI is adapted by us to identify the buggy control-CPS as a black-box. We use this identification result as an oracle to judge the control-CPS's behaviors, and propose a methodology to prepare traces for control-CPS debugging. Comprehensive evaluations on classic control-CPSs with injected real-life and artificial bugs show that our proposed approach significantly outperforms the human oracle approach in SFL accuracy (recall) and latency, and in oracle false positive/negative rates. Our approach also helps discover a new real-life bug in a consumer-grade control-CPS.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {116–127},
numpages = {12},
keywords = {cyber-physical system, debug, oracle, testing},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1145/3343440,
author = {Kaur, Harsurinder and Pannu, Husanbir Singh and Malhi, Avleen Kaur},
title = {A Systematic Review on Imbalanced Data Challenges in Machine Learning: Applications and Solutions},
year = {2019},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3343440},
doi = {10.1145/3343440},
abstract = {In machine learning, the data imbalance imposes challenges to perform data analytics in almost all areas of real-world research. The raw primary data often suffers from the skewed perspective of data distribution of one class over the other as in the case of computer vision, information security, marketing, and medical science. The goal of this article is to present a comparative analysis of the approaches from the reference of data pre-processing, algorithmic and hybrid paradigms for contemporary imbalance data analysis techniques, and their comparative study in lieu of different data distribution and their application areas.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {79},
numpages = {36},
keywords = {Data imbalance, data analysis, machine learning, sampling}
}

@inproceedings{10.1109/ASE.2019.00164,
author = {Zhang, Kevin},
title = {A machine learning based approach to identify SQL injection vulnerabilities},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00164},
doi = {10.1109/ASE.2019.00164},
abstract = {This paper presents a machine learning classifier designed to identify SQL injection vulnerabilities in PHP code. Both classical and deep learning based machine learning algorithms were used to train and evaluate classifier models using input validation and sanitization features extracted from source code files. On ten-fold cross validations a model trained using Convolutional Neural Network(CNN) achieved the highest precision (95.4%), while a model based on Multilayer Perceptron (MLP) achieved the highest recall (63.7%) and the highest fmeasure (0.746).},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1286–1288},
numpages = {3},
keywords = {SQL injection, deep learning, prediction model, vulnerability},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/2961111.2962620,
author = {Shippey, Thomas and Hall, Tracy and Counsell, Steve and Bowes, David},
title = {So You Need More Method Level Datasets for Your Software Defect Prediction? Voil\`{a}!},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962620},
doi = {10.1145/2961111.2962620},
abstract = {Context: Defect prediction research is based on a small number of defect datasets and most are at class not method level. Consequently our knowledge of defects is limited. Identifying defect datasets for prediction is not easy and extracting quality data from identified datasets is even more difficult. Goal: Identify open source Java systems suitable for defect prediction and extract high quality fault data from these datasets. Method: We used the Boa to identify candidate open source systems. We reduce 50,000 potential candidates down to 23 suitable for defect prediction using a selection criteria based on the system's software repository and its defect tracking system. We use an enhanced SZZ algorithm to extract fault information and calculate metrics using JHawk. Result: We have produced 138 fault and metrics datasets for the 23 identified systems. We make these datasets (the ELFF datasets) and our data extraction tools freely available to future researchers. Conclusions: The data we provide enables future studies to proceed with minimal effort. Our datasets significantly increase the pool of systems currently being used in defect analysis studies.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {12},
numpages = {6},
keywords = {Boa, Data Mining, Defect Prediction, Defect linking, Defects},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@inproceedings{10.1145/3411564.3411610,
author = {Prado, Fernando F. and Digiampietri, Luciano A.},
title = {A systematic review of automated feature engineering solutions in machine learning problems},
year = {2020},
isbn = {9781450388733},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411564.3411610},
doi = {10.1145/3411564.3411610},
abstract = {During the last decades, machine learning has played an important role in building data-driven experiments. Based on information extracted from a variety of sources, new patterns can be identified, predictions can be made more easily and decisions made faster and more effective. The specialized application of machine learning solutions requires specific knowledge in areas such as math, computation, and statistics, as well as being extremely costly in time and having a high chance of incurring any kind of human error during the process. Automated Machine Learning Techniques (AutoML) seek to automate parts of the process of building machine learning applications, allowing non-experts to perform this process. An important part of this kind of problem is the feature engineering part which creates a transformation in the data, making it more representative for the final model. This paper presents a systematic review of automated feature engineering solutions in machine learning problems. With the main objective of identifying and analyzing the existing methods and techniques for performing the automated feature engineering step within the framework of machine learning problems.},
booktitle = {Proceedings of the XVI Brazilian Symposium on Information Systems},
articleno = {12},
numpages = {7},
keywords = {Auto Learning, Feature Engineering, Feature Generation, Feature Selection},
location = {S\~{a}o Bernardo do Campo, Brazil},
series = {SBSI '20}
}

@inproceedings{10.1145/2024445.2024455,
author = {Giger, Emanuel and Pinzger, Martin and Gall, Harald},
title = {Using the gini coefficient for bug prediction in eclipse},
year = {2011},
isbn = {9781450308489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2024445.2024455},
doi = {10.1145/2024445.2024455},
abstract = {The Gini coefficient is a prominent measure to quantify the inequality of a distribution. It is often used in the field of economy to describe how goods, e.g., wealth or farmland, are distributed among people. We use the Gini coefficient to measure code ownership by investigating how changes made to source code are distributed among the developer population. The results of our study with data from the Eclipse platform show that less bugs can be expected if a large share of all changes are accumulated, i.e., carried out, by relatively few developers.},
booktitle = {Proceedings of the 12th International Workshop on Principles of Software Evolution and the 7th Annual ERCIM Workshop on Software Evolution},
pages = {51–55},
numpages = {5},
keywords = {bug prediction, code ownership, gini coefficient},
location = {Szeged, Hungary},
series = {IWPSE-EVOL '11}
}

@inproceedings{10.5555/2486788.2486838,
author = {Lewis, Chris and Lin, Zhongpeng and Sadowski, Caitlin and Zhu, Xiaoyan and Ou, Rong and Whitehead Jr., E. James},
title = {Does bug prediction support human developers? findings from a google case study},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {While many bug prediction algorithms have been developed by academia, they're often only tested and verified in the lab using automated means. We do not have a strong idea about whether such algorithms are useful to guide human developers. We deployed a bug prediction algorithm across Google, and found no identifiable change in developer behavior. Using our experience, we provide several characteristics that bug prediction algorithms need to meet in order to be accepted by human developers and truly change how developers evaluate their code.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {372–381},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/3511430.3511463,
author = {Oishie, Naz Zarreen Zarreen and Roy, Banani},
title = {Commit-Checker: A human-centric approach for adopting bug inducing commit detection using machine learning models},
year = {2022},
isbn = {9781450396189},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511430.3511463},
doi = {10.1145/3511430.3511463},
booktitle = {Proceedings of the 15th Innovations in Software Engineering Conference},
articleno = {36},
numpages = {3},
location = {Gandhinagar, India},
series = {ISEC '22}
}

@inproceedings{10.1109/MSR.2019.00017,
author = {Dam, Hoa Khanh and Pham, Trang and Ng, Shien Wee and Tran, Truyen and Grundy, John and Ghose, Aditya and Kim, Taeksu and Kim, Chul-Joo},
title = {Lessons learned from using a deep tree-based model for software defect prediction in practice},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00017},
doi = {10.1109/MSR.2019.00017},
abstract = {Defects are common in software systems and cause many problems for software users. Different methods have been developed to make early prediction about the most likely defective modules in large codebases. Most focus on designing features (e.g. complexity metrics) that correlate with potentially defective code. Those approaches however do not sufficiently capture the syntax and multiple levels of semantics of source code, a potentially important capability for building accurate prediction models. In this paper, we report on our experience of deploying a new deep learning tree-based defect prediction model in practice. This model is built upon the tree-structured Long Short Term Memory network which directly matches with the Abstract Syntax Tree representation of source code. We discuss a number of lessons learned from developing the model and evaluating it on two datasets, one from open source projects contributed by our industry partner Samsung and the other from the public PROMISE repository.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {46–57},
numpages = {12},
keywords = {deep learning, defect prediction},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3395363.3404540,
author = {Tizpaz-Niari, Saeid and \v{C}ern\'{y}, Pavol and Trivedi, Ashutosh},
title = {Detecting and understanding real-world differential performance bugs in machine learning libraries},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3404540},
doi = {10.1145/3395363.3404540},
abstract = {Programming errors that degrade the performance of systems are widespread, yet there is very little tool support for finding and diagnosing these bugs. We present a method and a tool based on differential performance analysis---we find inputs for which the performance varies widely, despite having the same size. To ensure that the differences in the performance are robust (i.e. hold also for large inputs), we compare the performance of not only single inputs, but of classes of inputs, where each class has similar inputs parameterized by their size. Thus, each class is represented by a performance function from the input size to performance. Importantly, we also provide an explanation for why the performance differs in a form that can be readily used to fix a performance bug. The two main phases in our method are discovery with fuzzing and explanation with decision tree classifiers, each of which is supported by clustering. First, we propose an evolutionary fuzzing algorithm to generate inputs that characterize different performance functions. For this fuzzing task, the unique challenge is that we not only need the input class with the worst performance, but rather a set of classes exhibiting differential performance. We use clustering to merge similar input classes which significantly improves the efficiency of our fuzzer. Second, we explain the differential performance in terms of program inputs and internals (e.g., methods and conditions). We adapt discriminant learning approaches with clustering and decision trees to localize suspicious code regions. We applied our techniques on a set of micro-benchmarks and real-world machine learning libraries. On a set of micro-benchmarks, we show that our approach outperforms state-of-the-art fuzzers in finding inputs to characterize differential performance. On a set of case-studies, we discover and explain multiple performance bugs in popular machine learning frameworks, for instance in implementations of logistic regression in scikit-learn. Four of these bugs, reported first in this paper, have since been fixed by the developers.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {189–199},
numpages = {11},
keywords = {Debugging, Differential Performance Bugs, ML Libraries, Testing},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/3106237.3106291,
author = {Ma, Shiqing and Aafer, Yousra and Xu, Zhaogui and Lee, Wen-Chuan and Zhai, Juan and Liu, Yingqi and Zhang, Xiangyu},
title = {LAMP: data provenance for graph based machine learning algorithms through derivative computation},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106291},
doi = {10.1145/3106237.3106291},
abstract = {Data provenance tracking determines the set of inputs related to a given output. It enables quality control and problem diagnosis in data engineering. Most existing techniques work by tracking program dependencies. They cannot quantitatively assess the importance of related inputs, which is critical to machine learning algorithms, in which an output tends to depend on a huge set of inputs while only some of them are of importance. In this paper, we propose LAMP, a provenance computation system for machine learning algorithms. Inspired by automatic differentiation (AD), LAMP quantifies the importance of an input for an output by computing the partial derivative. LAMP separates the original data processing and the more expensive derivative computation to different processes to achieve cost-effectiveness. In addition, it allows quantifying importance for inputs related to discrete behavior, such as control flow selection. The evaluation on a set of real world programs and data sets illustrates that LAMP produces more precise and succinct provenance than program dependence based techniques, with much less overhead. Our case studies demonstrate the potential of LAMP in problem diagnosis in data engineering.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {786–797},
numpages = {12},
keywords = {Data Provenance, Debugging, Machine Learning},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/2070821.2070823,
author = {Roychowdhury, Shounak and Khurshid, Sarfraz},
title = {Software fault localization using feature selection},
year = {2011},
isbn = {9781450310222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070821.2070823},
doi = {10.1145/2070821.2070823},
abstract = {Manually locating and fixing faults can be tedious and hard. Recent years have seen much progress in automated techniques for fault localization. A particularly promising approach is to analyze passing and failing runs to compute how likely each statement is to be faulty. Techniques based on this approach have so far largely focused on either using statistical analysis or similarity based algorithms, which have a natural application in evaluating such runs. We present a novel approach to fault localization using feature selection techniques from machine learning. Our insight is that each additional failing or passing run can provide significantly diverse amount of information, which can help localize faults in code -- the statements with maximum feature diversity information can point to most suspicious lines of code. Experimental results show that our approach outperforms state-of-the-art approaches for localizing faults in most subject programs of the Siemens suite, which have previously been used to evaluate several fault localization techniques.},
booktitle = {Proceedings of the International Workshop on Machine Learning Technologies in Software Engineering},
pages = {11–18},
numpages = {8},
keywords = {RELIEF, automated debugging, fault localization, feature selection, machine learning, statistical debugging},
location = {Lawrence, Kansas, USA},
series = {MALETS '11}
}

@inproceedings{10.1145/3617572.3617879,
author = {Pei, Yulong and Alamir, Salwa and Dolga, Rares and Shah, Sameena},
title = {Code Revert Prediction with Graph Neural Networks: A Case Study at J.P. Morgan Chase},
year = {2023},
isbn = {9798400703775},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617572.3617879},
doi = {10.1145/3617572.3617879},
abstract = {Code revert prediction, a specialized form of software defect detection, aims to forecast or predict the likelihood of code changes being reverted or rolled back in software development. This task is very important in practice because by identifying code changes that are more prone to being reverted, developers and project managers can proactively take measures to prevent issues, improve code quality, and optimize development processes. However, compared to code defect detection, code revert prediction has been rarely studied in previous research. Additionally, many previous methods for code defect detection relied on independent features but ignored relationships between code scripts. Moreover, new challenges are introduced due to constraints in an industry setting such as company regulation, limited features and large-scale codebase. To overcome these limitations, this paper presents a systematic empirical study for code revert prediction that integrates the code import graph with code features. Different strategies to address anomalies and data imbalance have been implemented including graph neural networks with imbalance classification and anomaly detection. We conduct the experiments on real-world code commit data within J.P. Morgan Chase which is extremely imbalanced in order to make a comprehensive comparison of these different approaches for the code revert prediction problem.},
booktitle = {Proceedings of the 1st International Workshop on Software Defect Datasets},
pages = {1–5},
numpages = {5},
keywords = {Code revert prediction, anomaly detection, graph neural networks, imbalanced classification},
location = {San Francisco, CA, USA},
series = {SDD 2023}
}

@inproceedings{10.1145/3236024.3236055,
author = {Hu, Gang and Zhu, Linjie and Yang, Junfeng},
title = {AppFlow: using machine learning to synthesize robust, reusable UI tests},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236055},
doi = {10.1145/3236024.3236055},
abstract = {UI testing is known to be difficult, especially as today’s development cycles become faster. Manual UI testing is tedious, costly and error- prone. Automated UI tests are costly to write and maintain. This paper presents AppFlow, a system for synthesizing highly robust, highly reusable UI tests. It leverages machine learning to automatically recognize common screens and widgets, relieving developers from writing ad hoc, fragile logic to use them in tests. It enables developers to write a library of modular tests for the main functionality of an app category (e.g., an “add to cart” test for shopping apps). It can then quickly test a new app in the same category by synthesizing full tests from the modular ones in the library. By focusing on the main functionality, AppFlow provides “smoke testing” requiring little manual work. Optionally, developers can customize AppFlow by adding app-specific tests for completeness. We evaluated AppFlow on 60 popular apps in the shopping and the news category, two case studies on the BBC news app and the JackThreads shopping app, and a user-study of 15 subjects on the Wish shopping app. Results show that AppFlow accurately recognizes screens and widgets, synthesizes highly robust and reusable tests, covers 46.6% of all automatable tests for Jackthreads with the tests it synthesizes, and reduces the effort to test a new app by up to 90%. Interestingly, it found eight bugs in the evaluated apps, including seven functionality bugs, despite that they were publicly released and supposedly went through thorough testing.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {269–282},
numpages = {14},
keywords = {UI recognition, UI testing, machine learning, mobile testing, test reuse, test synthesis},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3371158.3371233,
author = {Mannarswamy, Sandya and Roy, Shourya and Chidambaram, Saravanan},
title = {Tutorial on Software Testing &amp; Quality Assurance for Machine Learning Applications from research bench to real world},
year = {2020},
isbn = {9781450377386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371158.3371233},
doi = {10.1145/3371158.3371233},
abstract = {Rapid progress in Machine Learning (ML) has seen a swift translation to real world commercial deployment. While research and development of ML applications have progressed at an exponential pace, the required software engineering process for ML applications and the corresponding eco-system of testing and quality assurance tools which enable software reliable, trustworthy and safe and easy to deploy, have sadly lagged behind. Specifically, the challenges and gaps in quality assurance (QA) and testing of AI applications have largely remained unaddressed contributing to a poor translation rate of ML applications from research to real world. Unlike traditional software, which has a well-defined software testing methodology, ML applications have largely taken an ad-hoc approach to testing. ML researchers and practitioners either fall back to traditional software testing approaches, which are inadequate for this domain, due to its inherent probabilistic and data dependent nature, or rely largely on non-rigorous self-defined QA methodologies. These issues have driven the ML and Software Engineering research communities to develop of newer tools and techniques designed specifically for ML. These research advances need to be publicized and practiced in real world in ML development and deployment for enabling successful translation of ML from research prototypes to real world. This tutorial intends to address this need.This tutorial aims to:[1] Provide a comprehensive overview of testing of ML applications[2] Provide practical insights and share community best practices for testing ML softwareBesides scientific literature, we derive our insights from our conversations with industry experts in ML.},
booktitle = {Proceedings of the 7th ACM IKDD CoDS and 25th COMAD},
pages = {373–374},
numpages = {2},
keywords = {Machine Learning, Quality Assurance, Software Testing},
location = {Hyderabad, India},
series = {CoDS COMAD 2020}
}

@inproceedings{10.1145/2811681.2811699,
author = {Hussain, Shahid and Keung, Jacky and Khan, Arif Ali and Bennin, Kwabena Ebo},
title = {Performance Evaluation of Ensemble Methods For Software Fault Prediction: An Experiment},
year = {2015},
isbn = {9781450337960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811681.2811699},
doi = {10.1145/2811681.2811699},
abstract = {In object-oriented software development, a plethora of studies have been carried out to present the application of machine learning algorithms for fault prediction. Furthermore, it has been empirically validated that an ensemble method can improve classification performance as compared to a single classifier. But, due to the inherent differences among machine learning and data mining approaches, the classification performance of ensemble methods will be varied. In this study, we investigated and evaluated the performance of different ensemble methods with itself and base-level classifiers, in predicting the faults proneness classes. Subsequently, we used three ensemble methods AdaboostM1, Vote and StackingC with five base-level classifiers namely Naivebayes, Logistic, J48, VotedPerceptron and SMO in Weka tool. In order to evaluate the performance of ensemble methods, we retrieved twelve datasets of open source projects from PROMISE repository. In this experiment, we used k-fold (k=10) cross-validation and ROC analysis for validation. Besides, we used recall, precision, accuracy, F-value measures to evaluate the performance of ensemble methods and base-level Classifiers. Finally, we observed significant performance improvement of applying ensemble methods as compared to its base-level classifier, and among ensemble methods we observed StackingC outperformed other selected ensemble methods for software fault prediction.},
booktitle = {Proceedings of the ASWEC 2015 24th Australasian Software Engineering Conference},
pages = {91–95},
numpages = {5},
keywords = {Chidamber and Kemerer (CK) Metrics, Classifiers, Ensemble Methods, Measures, Performance, Weka},
location = {Adelaide, SA, Australia},
series = {ASWEC ' 15 Vol. II}
}

@inproceedings{10.1145/2491411.2494581,
author = {Zhang, Hongyu and Cheung, S. C.},
title = {A cost-effectiveness criterion for applying software defect prediction models},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2494581},
doi = {10.1145/2491411.2494581},
abstract = {Ideally, software defect prediction models should help organize software quality assurance (SQA) resources and reduce cost of finding defects by allowing the modules most likely to contain defects to be inspected first. In this paper, we study the cost-effectiveness of applying defect prediction models in SQA and propose a basic cost-effectiveness criterion. The criterion implies that defect prediction models should be applied with caution. We also propose a new metric FN/(FN+TN) to measure the cost-effectiveness of a defect prediction model.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {643–646},
numpages = {4},
keywords = {Defect prediction, cost effectiveness, evaluation metrics},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inproceedings{10.1145/3213846.3213858,
author = {Dwarakanath, Anurag and Ahuja, Manish and Sikand, Samarth and Rao, Raghotham M. and Bose, R. P. Jagadeesh Chandra and Dubash, Neville and Podder, Sanjay},
title = {Identifying implementation bugs in machine learning based image classifiers using metamorphic testing},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213858},
doi = {10.1145/3213846.3213858},
abstract = {We have recently witnessed tremendous success of Machine Learning (ML) in practical applications. Computer vision, speech recognition and language translation have all seen a near human level performance. We expect, in the near future, most business applications will have some form of ML. However, testing such applications is extremely challenging and would be very expensive if we follow today's methodologies. In this work, we present an articulation of the challenges in testing ML based applications. We then present our solution approach, based on the concept of Metamorphic Testing, which aims to identify implementation bugs in ML based image classifiers. We have developed metamorphic relations for an application based on Support Vector Machine and a Deep Learning based application. Empirical validation showed that our approach was able to catch 71% of the implementation bugs in the ML applications.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {118–128},
numpages = {11},
keywords = {Metamorphic Testing, Testing Machine Learning based applications},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

@inproceedings{10.1145/2875913.2875944,
author = {Qing, He and Biwen, Li and Beijun, Shen and Xia, Yong},
title = {Cross-Project Software Defect Prediction Using Feature-Based Transfer Learning},
year = {2015},
isbn = {9781450336413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2875913.2875944},
doi = {10.1145/2875913.2875944},
abstract = {Cross-project defect prediction is taken as an effective means of predicting software defects when the data shortage exists in the early phase of software development. Unfortunately, the precision of cross-project defect prediction is usually poor, largely because of the differences between the reference and the target projects. Having realized the project differences, this paper proposes CPDP, a feature-based transfer learning approach to cross-project defect prediction. The core insight of CPDP is to (1) filter and transfer highly-correlated data based on data samples in the target projects, and (2) evaluate and choose learning schemas for transferring data sets. Models are then built for predicting defects in the target projects. We have also conducted an evaluation of the proposed approach on PROMISE datasets. The evaluation results show that, the proposed approach adapts to cross-project defect prediction in that f-measure of 81.8% of projects can get improved, and AUC of 54.5% projects improved. It also achieves similar f-measure and AUC as some inner-project defect prediction approaches.},
booktitle = {Proceedings of the 7th Asia-Pacific Symposium on Internetware},
pages = {74–82},
numpages = {9},
keywords = {cross-project defect prediction, feature-based transfer, transfer learning},
location = {Wuhan, China},
series = {Internetware '15}
}

@inproceedings{10.1145/2351676.2351734,
author = {Lu, Huihua and Cukic, Bojan and Culp, Mark},
title = {Software defect prediction using semi-supervised learning with dimension reduction},
year = {2012},
isbn = {9781450312042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351676.2351734},
doi = {10.1145/2351676.2351734},
abstract = {Accurate detection of fault prone modules offers the path to high quality software products while minimizing non essential assurance expenditures. This type of quality modeling requires the availability of software modules with known fault content developed in similar environment. Establishing whether a module contains a fault or not can be expensive. The basic idea behind semi-supervised learning is to learn from a small number of software modules with known fault content and supplement model training with modules for which the fault information is not available. In this study, we investigate the performance of semi-supervised learning for software fault prediction. A preprocessing strategy, multidimensional scaling, is embedded in the approach to reduce the dimensional complexity of software metrics. Our results show that the semi-supervised learning algorithm with dimension-reduction preforms significantly better than one of the best performing supervised learning algorithms, random forest, in situations when few modules with known fault content are available for training.},
booktitle = {Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering},
pages = {314–317},
numpages = {4},
keywords = {Software fault prediction, dimension reduction, semi-supervised learning, software metrics},
location = {Essen, Germany},
series = {ASE '12}
}

@proceedings{10.1145/3340482,
title = {MaLTeSQuE 2019: Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tallinn, Estonia}
}

@inproceedings{10.5555/2486788.2486840,
author = {Herzig, Kim and Just, Sascha and Zeller, Andreas},
title = {It's not a bug, it's a feature: how misclassification impacts bug prediction},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {In a manual examination of more than 7,000 issue reports from the bug databases of five open-source projects, we found 33.8% of all bug reports to be misclassified---that is, rather than referring to a code fix, they resulted in a new feature, an update to documentation, or an internal refactoring. This misclassification introduces bias in bug prediction models, confusing bugs and features: On average, 39% of files marked as defective actually never had a bug. We discuss the impact of this misclassification on earlier studies and recommend manual data validation for future studies.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {392–401},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/1985441.1985456,
author = {Giger, Emanuel and Pinzger, Martin and Gall, Harald C.},
title = {Comparing fine-grained source code changes and code churn for bug prediction},
year = {2011},
isbn = {9781450305747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985441.1985456},
doi = {10.1145/1985441.1985456},
abstract = {A significant amount of research effort has been dedicated to learning prediction models that allow project managers to efficiently allocate resources to those parts of a software system that most likely are bug-prone and therefore critical. Prominent measures for building bug prediction models are product measures, e.g., complexity or process measures, such as code churn. Code churn in terms of lines modified (LM) and past changes turned out to be significant indicators of bugs. However, these measures are rather imprecise and do not reflect all the detailed changes of particular source code entities during maintenance activities. In this paper, we explore the advantage of using fine-grained source code changes (SCC) for bug prediction. SCC captures the exact code changes and their semantics down to statement level. We present a series of experiments using different machine learning algorithms with a dataset from the Eclipse platform to empirically evaluate the performance of SCC and LM. The results show that SCC outperforms LM for learning bug prediction models.},
booktitle = {Proceedings of the 8th Working Conference on Mining Software Repositories},
pages = {83–92},
numpages = {10},
keywords = {code churn, nonlinear regression, prediction models, software bugs, source code changes},
location = {Waikiki, Honolulu, HI, USA},
series = {MSR '11}
}

@inproceedings{10.1145/3293882.3330580,
author = {Cordy, Maxime and Muller, Steve and Papadakis, Mike and Le Traon, Yves},
title = {Search-based test and improvement of machine-learning-based anomaly detection systems},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3330580},
doi = {10.1145/3293882.3330580},
abstract = {Machine-learning-based anomaly detection systems can be vulnerable to new kinds of deceptions, known as training attacks, which exploit the live learning mechanism of these systems by progressively injecting small portions of abnormal data. The injected data seamlessly swift the learned states to a point where harmful data can pass unnoticed. We focus on the systematic testing of these attacks in the context of intrusion detection systems (IDS). We propose a search-based approach to test IDS by making training attacks. Going a step further, we also propose searching for countermeasures, learning from the successful attacks and thereby increasing the resilience of the tested IDS. We evaluate our approach on a denial-of-service attack detection scenario and a dataset recording the network traffic of a real-world system. Our experiments show that our search-based attack scheme generates successful attacks bypassing the current state-of-the-art defences. We also show that our approach is capable of generating attack patterns for all configuration states of the studied IDS and that it is capable of providing appropriate countermeasures. By co-evolving our attack and defence mechanisms we succeeded at improving the defence of the IDS under test by making it resilient to 49 out of 50 independently generated attacks.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {158–168},
numpages = {11},
keywords = {clustering, intrusion detection systems, software testing},
location = {Beijing, China},
series = {ISSTA 2019}
}

@inproceedings{10.1109/ICSE-C.2017.72,
author = {Wu, Fei and Jing, Xiao-Yuan and Dong, Xiwei and Cao, Jicheng and Xu, Mingwei and Zhang, Hongyu and Ying, Shi and Xu, Baowen},
title = {Cross-project and within-project semi-supervised software defect prediction problems study using a unified solution},
year = {2017},
isbn = {9781538615898},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-C.2017.72},
doi = {10.1109/ICSE-C.2017.72},
abstract = {When there exists not enough historical defect data for building accurate prediction model, semi-supervised defect prediction (SSDP) and cross-project defect prediction (CPDP) are two feasible solutions. Existing CPDP methods assume that the available source data is well labeled. However, due to expensive human efforts for labeling a large amount of defect data, usually, we can only make use of the suitable unlabeled source data to help build the prediction model. We call CPDP in this scenario as cross-project semi-supervised defect prediction (CSDP). As to within-project semi-supervised defect prediction (WSDP), although some WSDP methods have been developed in recent years, there still exists much room for improvement. In this paper, we aim to provide an effective solution for both CSDP and WSDP problems. We introduce the semi-supervised dictionary learning technique, an effective machine learning technique, into defect prediction and propose a semi-supervised structured dictionary learning (SSDL) approach for CSDP and WSDP. SSDL can make full use of the useful information in limited labeled defect data and a large amount of unlabeled data. Experiments on two public datasets indicate that SSDL can obtain better prediction performance than related SSDP methods in the CSDP scenario.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering Companion},
pages = {195–197},
numpages = {3},
keywords = {cross-project semi-supervised defect prediction, semi-supervised structured dictionary learning, within-project semi-supervised defect prediction},
location = {Buenos Aires, Argentina},
series = {ICSE-C '17}
}

@inproceedings{10.1145/2532443.2532461,
author = {Chen, Jiaqiang and Liu, Shulong and Chen, Xiang and Gu, Qing and Chen, Daoxu},
title = {Empirical studies on feature selection for software fault prediction},
year = {2013},
isbn = {9781450323697},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2532443.2532461},
doi = {10.1145/2532443.2532461},
abstract = {Classification based software fault prediction methods aim to classify the modules into either fault-prone or non-fault-prone. Feature selection is a preprocess step used to improve the data quality. However most of previous research mainly focus on feature relevance analysis, there is little work focusing on feature redundancy analysis. Therefore we propose a two-stage framework for feature selection to solve this issue. In particular, during the feature relevance phase, we adopt three different relevance measures to obtain the relevant feature subset. Then during the feature redundancy analysis phase, we use a cluster-based method to eliminate redundant features. To verify the effectiveness of our proposed framework, we choose typical real-world software projects, including Eclipse projects and NASA software project KC1. Final empirical result shows the effectiveness of our proposed framework.},
booktitle = {Proceedings of the 5th Asia-Pacific Symposium on Internetware},
articleno = {26},
numpages = {4},
keywords = {feature selection, redundancy analysis, relevance analysis, software fault prediction},
location = {Changsha, China},
series = {Internetware '13}
}

@inproceedings{10.1145/2701126.2701207,
author = {Kim, Jeongho and Park, Jonghee and Lee, Eunseok},
title = {A new hybrid algorithm for software fault localization},
year = {2015},
isbn = {9781450333771},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701126.2701207},
doi = {10.1145/2701126.2701207},
abstract = {We previously presented a spectrum-based fault localization (SFL) technique, which we named Hybrid, that localizes a bug by using the program hit spectra and test results. We also proposed a distinct mechanism for test data that enables the SFL algorithms to localize fault in a more precise manner than what would be possible with the original test data. However, there was a limitation of the Hybrid algorithm. In that the technique only showed better performance when using distinct test data. Therefore, in the current work, we improve over Hybrid by analyzing more than 30 types of existing algorithms. After choosing the appropriate algorithms, we adopted their specific strengths through experimentation. Finally, we developed the novel Combination Algorithms (CAL). In our experimental study, we used the Siemens test program to confirm that our technique was more precise than the state-of-the-art SFL algorithm D Star and Heuristic III for both original and distinct test data. In particular, our technique localizes a fault under 2 percent on average as well as decreases the coverage of the reading code by a third of the source code.},
booktitle = {Proceedings of the 9th International Conference on Ubiquitous Information Management and Communication},
articleno = {50},
numpages = {8},
keywords = {execution trace, fault localization, program debugging, spectrum based fault localization, suspicious code},
location = {Bali, Indonesia},
series = {IMCOM '15}
}

@inproceedings{10.1145/2601248.2601294,
author = {Rodriguez, Daniel and Herraiz, Israel and Harrison, Rachel and Dolado, Javier and Riquelme, Jos\'{e} C.},
title = {Preliminary comparison of techniques for dealing with imbalance in software defect prediction},
year = {2014},
isbn = {9781450324762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2601248.2601294},
doi = {10.1145/2601248.2601294},
abstract = {Imbalanced data is a common problem in data mining when dealing with classification problems, where samples of a class vastly outnumber other classes. In this situation, many data mining algorithms generate poor models as they try to optimize the overall accuracy and perform badly in classes with very few samples. Software Engineering data in general and defect prediction datasets are not an exception and in this paper, we compare different approaches, namely sampling, cost-sensitive, ensemble and hybrid approaches to the problem of defect prediction with different datasets preprocessed differently. We have used the well-known NASA datasets curated by Shepperd et al. There are differences in the results depending on the characteristics of the dataset and the evaluation metrics, especially if duplicates and inconsistencies are removed as a preprocessing step.Further Results and replication package: http://www.cc.uah.es/drg/ease14},
booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {43},
numpages = {10},
keywords = {data quality, defect prediction, imbalanced data},
location = {London, England, United Kingdom},
series = {EASE '14}
}

@inproceedings{10.1145/3330204.3330275,
author = {Luiz, Frederico Caram and de Oliveira Rodrigues, Bruno Rafael and Parreiras, Fernando Silva},
title = {Machine learning techniques for code smells detection: an empirical experiment on a highly imbalanced setup},
year = {2019},
isbn = {9781450372374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330204.3330275},
doi = {10.1145/3330204.3330275},
booktitle = {Proceedings of the XV Brazilian Symposium on Information Systems},
articleno = {65},
numpages = {8},
location = {Aracaju, Brazil},
series = {SBSI '19}
}

@inproceedings{10.1145/2970276.2970364,
author = {Li, Xin and Liang, Yongjuan and Qian, Hong and Hu, Yi-Qi and Bu, Lei and Yu, Yang and Chen, Xin and Li, Xuandong},
title = {Symbolic execution of complex program driven by machine learning based constraint solving},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970364},
doi = {10.1145/2970276.2970364},
abstract = {Symbolic execution is a widely-used program analysis technique. It collects and solves path conditions to guide the program traversing. However, due to the limitation of the current constraint solvers, it is difficult to apply symbolic execution on programs with complex path conditions, like nonlinear constraints and function calls. In this paper, we propose a new symbolic execution tool MLB to handle such problem. Instead of relying on the classical constraint solving, in MLB, the feasibility problems of the path conditions are transformed into optimization problems, by minimizing some dissatisfaction degree. The optimization problems are then handled by the underlying optimization solver through machine learning guided sampling and validation. MLB is implemented on the basis of Symbolic PathFinder and encodes not only the simple linear path conditions, but also nonlinear arithmetic operations, and even black-box function calls of library methods, into symbolic path conditions. Experiment results show that MLB can achieve much better coverage on complex real-world programs.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {554–559},
numpages = {6},
keywords = {Complicated Path Condition, Constraint Solving, Machine Learning, Symbolic Execution},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.1145/2593833.2593842,
author = {Malhotra, Ruchika},
title = {Search based techniques for software fault prediction: current trends and future directions},
year = {2014},
isbn = {9781450328524},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593833.2593842},
doi = {10.1145/2593833.2593842},
abstract = {The effective allocation of the resources is crucial and essential in the testing phase of the software development life cycle so that the weak areas in the software can be verified and validated efficiently. The prediction of fault prone classes in the early phases of software development can help software developers to focus the limited available resources on those portions of software, which are more prone to fault. Recently, the search based techniques have been successfully applied in the software engineering domain. In this study, we analyze the position of search based techniques for use in software fault prediction by collecting relevant studies from the literature which were conducted during the period January 1991 to October 2013. We further summarize current trends by assessing the performance capability of the search based techniques in the existing research and suggest future directions.},
booktitle = {Proceedings of the 7th International Workshop on Search-Based Software Testing},
pages = {35–36},
numpages = {2},
keywords = {Search Based Techniques, Software Fault proneness, Software Quality},
location = {Hyderabad, India},
series = {SBST 2014}
}

@inproceedings{10.1145/1831708.1831743,
author = {Ostrand, Thomas J. and Weyuker, Elaine J.},
title = {Software fault prediction tool},
year = {2010},
isbn = {9781605588230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1831708.1831743},
doi = {10.1145/1831708.1831743},
abstract = {We have developed an interactive tool that predicts fault likelihood for the individual files of successive releases of large, long-lived, multi-developer software systems. Predictions are the result of a two-stage process: first, the extraction of current and historical properties of the system, and second, application of a negative binomial regression model to the extracted data. The prediction model is presented to the user as a GUI-based tool that requires minimal input from the user, and delivers its output as an ordered list of the system's files together with an expected percent of faults each file will have in the release about to undergo system test. The predictions can be used to prioritize testing efforts, to plan code or design reviews, to allocate human and computer resources, and to decide if files should be rewritten.},
booktitle = {Proceedings of the 19th International Symposium on Software Testing and Analysis},
pages = {275–278},
numpages = {4},
keywords = {defect, fault, gui tool, negative binomial, prediction},
location = {Trento, Italy},
series = {ISSTA '10}
}

@inproceedings{10.1145/1982595.1982599,
author = {Mousavian, Zaynab and Vahidi-Asl, Mojtaba and Parsa, Saeed},
title = {Scalable graph analyzing approach for software fault-localization},
year = {2011},
isbn = {9781450305921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982595.1982599},
doi = {10.1145/1982595.1982599},
abstract = {In this paper, a new approach for analyzing program behavioral graphs to detect fault relevant paths is presented. The existing graph mining approaches for bug localization merely detect discriminative sub-graphs between failing and passing runs. However, they are not applicable when the context of a failure is not appeared in a discriminative pattern. In our proposed method, the suspicious transitions are identified by contrasting nearest neighbor failing and passing dynamic behavioral graphs. For finding similar failing and passing graphs, we first convert the graphs into adequate vectors. Then, a combination of Jacard-Cosine similarity measures is applied to identify the nearest graphs. The new scoring formula takes advantage of null hypothesis testing for ranking weighted transitions. The main advantage of the proposed technique is its scalability which makes it work on large and complex programs with huge number of predicates. Another main capability of our approach is providing the faulty paths constructed from fault suspicious transitions. Considering the weighted execution graphs in the analysis enables us to find those types of bugs which reveal themselves in specific number of transitions between two particular predicates. The experimental results on Siemens test suite and Space program manifest the effectiveness of the proposed method on weighted execution graphs for locating bugs in comparison with other methods.},
booktitle = {Proceedings of the 6th International Workshop on Automation of Software Test},
pages = {15–21},
numpages = {7},
keywords = {graph mining, null hypothesis testing, software fault localization, weighted graphs},
location = {Waikiki, Honolulu, HI, USA},
series = {AST '11}
}

@inproceedings{10.1145/3617572.3617881,
author = {Dolga, Rares and Zmigrod, Ran and Silva, Rui and Alamir, Salwa and Shah, Sameena},
title = {Log Summarisation for Defect Evolution Analysis},
year = {2023},
isbn = {9798400703775},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617572.3617881},
doi = {10.1145/3617572.3617881},
abstract = {Log analysis and monitoring are essential aspects in software maintenance and identifying defects. In particular, the temporal nature and vast size of log data leads to an interesting and important research question: How can logs be summarised and monitored over time? While this has been a fundamental topic of research in the software engineering community, work has typically focused on heuristic-, syntax-, or static-based methods. In this work, we suggest an online semantic-based clustering approach to error logs that dynamically updates the log clusters to enable monitoring code error life-cycles. We also introduce a novel metric to evaluate the performance of temporal log clusters. We test our system and evaluation metric with an industrial dataset and find that our solution outperforms similar systems. We hope that our work encourages further temporal exploration in defect datasets.},
booktitle = {Proceedings of the 1st International Workshop on Software Defect Datasets},
pages = {11–16},
numpages = {6},
keywords = {Clustering, Defect Detection, Log Analysis, NLP},
location = {San Francisco, CA, USA},
series = {SDD 2023}
}

@inproceedings{10.1145/3510003.3510037,
author = {Liu, Changlin and Wang, Hanlin and Liu, Tianming and Gu, Diandian and Ma, Yun and Wang, Haoyu and Xiao, Xusheng},
title = {ProMal: precise window transition graphs for android via synergy of program analysis and machine learning},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510037},
doi = {10.1145/3510003.3510037},
abstract = {Mobile apps have been an integral part in our daily life. As these apps become more complex, it is critical to provide automated analysis techniques to ensure the correctness, security, and performance of these apps. A key component for these automated analysis techniques is to create a graphical user interface (GUI) model of an app, i.e., a window transition graph (WTG), that models windows and transitions among the windows. While existing work has provided both static and dynamic analysis to build the WTG for an app, the constructed WTG misses many transitions or contains many infeasible transitions due to the coverage issues of dynamic analysis and over-approximation of the static analysis. We propose ProMal, a "tribrid" analysis that synergistically combines static analysis, dynamic analysis, and machine learning to construct a precise WTG. Specifically, ProMal first applies static analysis to build a static WTG, and then applies dynamic analysis to verify the transitions in the static WTG. For the unverified transitions, ProMal further provides machine learning techniques that leverage runtime information (i.e., screenshots, UI layouts, and text information) to predict whether they are feasible transitions. Our evaluations on 40 real-world apps demonstrate the superiority of ProMal in building WTGs over static analysis, dynamic analysis, and machine learning techniques when they are applied separately.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1755–1767},
numpages = {13},
keywords = {deep learning, mobile apps, static analysis, window transition graph},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@article{10.1145/3587094,
author = {B\"{a}r, Dominik and Pr\"{o}llochs, Nicolas and Feuerriegel, Stefan},
title = {New Threats to Society from Free-Speech Social Media Platforms},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/3587094},
doi = {10.1145/3587094},
abstract = {Understanding emerging threats from social media platforms.},
journal = {Commun. ACM},
month = sep,
pages = {37–40},
numpages = {4}
}

@inproceedings{10.1109/MSR.2007.17,
author = {Joshi, Hemant and Zhang, Chuanlei and Ramaswamy, S. and Bayrak, Coskun},
title = {Local and Global Recency Weighting Approach to Bug Prediction},
year = {2007},
isbn = {076952950X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MSR.2007.17},
doi = {10.1109/MSR.2007.17},
abstract = {Finding and fixing software bugs is a challenging maintenance task, and a significant amount of effort is invested by software development companies on this issue. In this paper, we use the Eclipse project's recorded software bug history to predict occurrence of future bugs. The history contains information on when bugs have been reported and subsequently fixed.},
booktitle = {Proceedings of the Fourth International Workshop on Mining Software Repositories},
pages = {33},
series = {MSR '07}
}

@article{10.1145/1317471.1317478,
author = {Ploski, Jan and Rohr, Matthias and Schwenkenberg, Peter and Hasselbring, Wilhelm},
title = {Research issues in software fault categorization},
year = {2007},
issue_date = {November 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/1317471.1317478},
doi = {10.1145/1317471.1317478},
abstract = {Software faults are a major threat for the dependability of software systems. When we intend to study the impact of software faults on software behavior, examine the quality of fault tolerance mechanisms, or evaluate diagnostic techniques, the issue of distinguishing fault categories and their frequency distribution arises immediately. This article surveys the literature that provides quantitative data on categories of software faults and discusses the applicability of these software fault category distributions to fault injection case studies.},
journal = {SIGSOFT Softw. Eng. Notes},
month = nov,
pages = {6–es},
numpages = {8},
keywords = {bugs, injection of software faults, software fault categorization, software faults, software reliability}
}

@article{10.5555/3648699.3649077,
author = {Magesh, Akshayaa and Veeravalli, Venugopal V. and Roy, Anirban and Jha, Susmit},
title = {Principled out-of-distribution detection via multiple testing},
year = {2023},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {We study the problem of out-of-distribution (OOD) detection, that is, detecting whether a machine learning (ML) model's output can be trusted at inference time. While a number of tests for OOD detection have been proposed in prior work, a formal framework for studying this problem is lacking. We propose a definition for the notion of OOD that includes both the input distribution and the ML model, which provides insights for the construction of powerful tests for OOD detection. We also propose a multiple hypothesis testing inspired procedure to systematically combine any number of different statistics from the ML model using conformal p-values. We further provide strong guarantees on the probability of incorrectly classifying an in-distribution sample as OOD. In our experiments, we find that threshold-based tests proposed in prior work perform well in specific settings, but not uniformly well across different OOD instances. In contrast, our proposed method that combines multiple statistics performs uniformly well across different datasets and neural networks architectures.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {378},
numpages = {35},
keywords = {OOD characterization, conformal p-values, conditional false alarm guarantees, Benjamini-Hochberg procedure}
}

@inproceedings{10.5555/2666527.2666533,
author = {Morgado, In\^{e}s Coimbra and Paiva, Ana C. R. and Faria, Jo\~{a}o Pascoal and Camacho, Rui},
title = {GUI reverse engineering with machine learning},
year = {2012},
isbn = {9781467317535},
publisher = {IEEE Press},
abstract = {This paper proposes a new approach to reduce the effort of building formal models representative of the structure and behaviour of Graphical User Interfaces (GUI). The main goal is to automatically extract the GUI model with a dynamic reverse engineering process, consisting in an exploration phase, that extracts information by interacting with the GUI, and in a model generation phase that, making use of machine learning techniques, uses the extracted information of the first step to generate a state-machine model of the GUI, including guard conditions to remove ambiguity in transitions.},
booktitle = {Proceedings of the First International Workshop on Realizing AI Synergies in Software Engineering},
pages = {27–31},
numpages = {5},
keywords = {inductive logic programming, machine learning, model-based testing, reverse engineering},
location = {Zurich, Switzerland},
series = {RAISE '12}
}

@inproceedings{10.1145/1414004.1414066,
author = {Tosun, Ayse and Turhan, Burak and Bener, Ayse},
title = {Ensemble of software defect predictors: a case study},
year = {2008},
isbn = {9781595939715},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1414004.1414066},
doi = {10.1145/1414004.1414066},
abstract = {In this paper, we present a defect prediction model based on ensemble of classifiers, which has not been fully explored so far in this type of research. We have conducted several experiments on public datasets. Our results reveal that ensemble of classifiers considerably improve the defect detection capability compared to Naive Bayes algorithm. We also conduct a cost-benefit analysis for our ensemble, where it turns out that it is enough to inspect 32% of the code on the average, for detecting 76% of the defects.},
booktitle = {Proceedings of the Second ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {318–320},
numpages = {3},
keywords = {defect prediction, ensemble of classifiers, static code attributes},
location = {Kaiserslautern, Germany},
series = {ESEM '08}
}

@inproceedings{10.1145/2365324.2365335,
author = {Lu, Huihua and Cukic, Bojan},
title = {An adaptive approach with active learning in software fault prediction},
year = {2012},
isbn = {9781450312417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2365324.2365335},
doi = {10.1145/2365324.2365335},
abstract = {Background: Software quality prediction plays an important role in improving the quality of software systems. By mining software metrics, predictive models can be induced that provide software managers with insights into quality problems they need to tackle as effectively as possible.Objective: Traditional, supervised learning approaches dominate software quality prediction. Resulting models tend to be project specific. On the other hand, in situations where there are no previous releases, supervised learning approaches are not very useful because large training data sets are needed to develop accurate predictive models.Method: This paper eases the limitations of supervised learning approaches and offers good prediction performance. We propose an adaptive approach in which supervised learning and active learning are coupled together. NaiveBayes classifier is used as the base learner.Results: We track the performance at each iteration of the adaptive learning algorithm and compare it with the performance of supervised learning. Our results show that proposed scheme provides good fault prediction performance over time, i.e., it eventually outperforms the corresponding supervised learning approach. On the other hand, adaptive learning classification approach reduces the variance in prediction performance in comparison with the corresponding supervised learning algorithm.Conclusion: The adaptive approach outperforms the corresponding supervised learning approach when both use Naive-Bayes as base learner. Additional research is needed to investigate whether this observation remains valid with other base classifiers.},
booktitle = {Proceedings of the 8th International Conference on Predictive Models in Software Engineering},
pages = {79–88},
numpages = {10},
keywords = {active learning, adaptive learning, software fault prediction},
location = {Lund, Sweden},
series = {PROMISE '12}
}

@inproceedings{10.1145/2597073.2597081,
author = {Gupta, Monika and Sureka, Ashish and Padmanabhuni, Srinivas},
title = {Process mining multiple repositories for software defect resolution from control and organizational perspective},
year = {2014},
isbn = {9781450328630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597073.2597081},
doi = {10.1145/2597073.2597081},
abstract = {Issue reporting and resolution is a software engineering process supported by tools such as Issue Tracking System (ITS), Peer Code Review (PCR) system and Version Control System (VCS). Several open source software projects such as Google Chromium and Android follow process in which a defect or feature enhancement request is reported to an issue tracker followed by source-code change or patch review and patch commit using a version control system. We present an application of process mining three software repositories (ITS, PCR and VCS) from control flow and organizational perspective for effective process management. ITS, PCR and VCS are not explicitly linked so we implement regular expression based heuristics to integrate data from three repositories for Google Chromium project. We define activities such as bug reporting, bug fixing, bug verification, patch submission, patch review, and source code commit and create an event log of the bug resolution process. The extracted event log contains audit trail data such as caseID, timestamp, activity name and performer. We discover runtime process model for bug resolution process spanning three repositories using process mining tool, Disco, and conduct process performance and efficiency analysis. We identify bottlenecks, define and detect basic and composite anti-patterns. In addition to control flow analysis, we mine event log to perform organizational analysis and discover metrics such as handover of work, subcontracting, joint cases and joint activities.},
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
pages = {122–131},
numpages = {10},
keywords = {Empirical Software Engineering and Measurements, Issue Tracking System, Peer Code Review System, Process Mining, Social Network Analysis, Software Maintenance},
location = {Hyderabad, India},
series = {MSR 2014}
}

@inproceedings{10.1145/2568225.2568307,
author = {Lee, Sangho and Jung, Changhee and Pande, Santosh},
title = {Detecting memory leaks through introspective dynamic behavior modelling using machine learning},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568307},
doi = {10.1145/2568225.2568307},
abstract = {This paper expands staleness-based memory leak detection by presenting a machine learning-based framework. The proposed framework is based on an idea that object staleness can be better leveraged in regard to similarity of objects; i.e., an object is more likely to have leaked if it shows significantly high staleness not observed from other similar objects with the same allocation context.  A central part of the proposed framework is the modeling of heap objects. To this end, the framework observes the staleness of objects during a representative run of an application. From the observed data, the framework generates training examples, which also contain instances of hypothetical leaks. Via machine learning, the proposed framework replaces the error-prone user-definable staleness predicates used in previous research with a model-based prediction.  The framework was tested using both synthetic and real-world examples. Evaluation with synthetic leakage workloads of SPEC2006 benchmarks shows that the proposed method achieves the optimal accuracy permitted by staleness-based leak detection. Moreover, by incorporating allocation context into the model, the proposed method achieves higher accuracy than is possible with object staleness alone. Evaluation with real-world memory leaks demonstrates that the proposed method is effective for detecting previously reported bugs with high accuracy.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {814–824},
numpages = {11},
keywords = {Machine learning, Memory leak detection, Runtime analysis},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@article{10.1145/2020976.2020991,
author = {Malhotra, Ruchika and Jain, Ankita},
title = {Software fault prediction for object oriented systems: a literature review},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/2020976.2020991},
doi = {10.1145/2020976.2020991},
abstract = {There always has been a demand to produce efficient and high quality software. There are various object oriented metrics that measure various properties of the software like coupling, cohesion, inheritance etc. which affect the software to a large extent. These metrics can be used in predicting important quality attributes such as fault proneness, maintainability, effort, productivity and reliability. Early prediction of fault proneness will help us to focus on testing resources and use them only on the classes which are predicted to be fault-prone. Thus, this will help in early phases of software development to give a measurement of quality assessment.This paper provides the review of the previous studies which are related to software metrics and the fault proneness. In other words, it reviews several journals and conference papers on software fault prediction. There is large number of software metrics proposed in the literature. Each study uses a different subset of these metrics and performs the analysis using different datasets. Also, the researchers have used different approaches such as Support vector machines, naive bayes network, random forest, artificial neural network, decision tree, logistic regression etc. Thus, this study focuses on the metrics used, dataset used and the evaluation or analysis method used by various authors. This review will be beneficial for the future studies as various researchers and practitioners can use it for comparative analysis.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {1–6},
numpages = {6},
keywords = {empirical validation, fault proneness, metrics, object oriented, software quality}
}

@inproceedings{10.1145/2020390.2020405,
author = {Lu, Huihua and Cukic, Bojan and Culp, Mark},
title = {An iterative semi-supervised approach to software fault prediction},
year = {2011},
isbn = {9781450307093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020390.2020405},
doi = {10.1145/2020390.2020405},
abstract = {Background: Many statistical and machine learning techniques have been implemented to build predictive fault models. Traditional methods are based on supervised learning. Software metrics for a module and corresponding fault information, available from previous projects, are used to train a fault prediction model. This approach calls for a large size of training data set and enables the development of effective fault prediction models. In practice, data collection costs, the lack of data from earlier projects or product versions may make large fault prediction training data set unattainable. Small size of the training set that may be available from the current project is known to deteriorate the performance of the fault predictive model. In semi-supervised learning approaches, software modules with known or unknown fault content can be used for training.Aims: To implement and evaluate a semi-supervised learning approach in software fault prediction.Methods: We investigate an iterative semi-supervised approach to software quality prediction in which a base supervised learner is used within a semi-supervised application.Results: We varied the size of labeled software modules from 2% to 50% of all the modules in the project. After tracking the performance of each iteration in the semi-supervised algorithm, we observe that semi-supervised learning improves fault prediction if the number of initially labeled software modules exceeds 5%.Conclusion: The semi-supervised approach outperforms the corresponding supervised learning approach when both use random forest as base classification algorithm.},
booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
articleno = {15},
numpages = {10},
keywords = {fault prediction, semi-supervised learning},
location = {Banff, Alberta, Canada},
series = {Promise '11}
}

@inproceedings{10.1145/3183440.3194992,
author = {Guo, Yuchen and Shepperd, Martin and Li, Ning},
title = {Bridging effort-aware prediction and strong classification: a just-in-time software defect prediction study},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3194992},
doi = {10.1145/3183440.3194992},
abstract = {Context: Most research into software defect prediction ignores the differing amount of effort entailed in searching for defects between software components. The result is sub-optimal solutions in terms of allocating testing resources. Recently effort-aware (EA) defect prediction has sought to redress this deficiency. However, there is a gap between previous classification research and EA prediction.Objective: We seek to transfer strong defect classification capability to efficient effort-aware software defect prediction.Method: We study the relationship between classification performance and the cost-effectiveness curve experimentally (using six open-source software data sets).Results: We observe extremely skewed distributions of change size which contributes to the lack of relationship between classification performance and the ability to find efficient test orderings for defect detection. Trimming allows all effort-aware approaches bridging high classification capability to efficient effort-aware performance.Conclusion: Effort distributions dominate effort-aware models. Trimming is a practical method to handle this problem.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {325–326},
numpages = {2},
keywords = {defect prediction, effort-aware, just-in-time, software},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3511430.3511444,
author = {Cynthia, Shamse Tasnim and Roy, Banani and Mondal, Debajyoti},
title = {Feature Transformation for Improved Software Bug Detection Models},
year = {2022},
isbn = {9781450396189},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511430.3511444},
doi = {10.1145/3511430.3511444},
abstract = {Testing software is considered to be one of the most crucial phases in software development life cycle. Software bug fixing requires a significant amount of time and effort. A rich body of recent research explored ways to predict bugs in software artifacts using machine learning based techniques. For a reliable and trustworthy prediction, it is crucial to also consider the explainability aspects of such machine learning models. In this paper, we show how the feature transformation techniques can significantly improve the prediction accuracy and build confidence in building bug prediction models. We propose a novel approach for improved bug prediction that first extracts the features, then finds a weighted transformation of these features using a genetic algorithm that best separates bugs from non-bugs when plotted in a low-dimensional space, and finally, trains the machine learning model using the transformed dataset. In our experiment with real-life bug datasets, the random forest and k-nearest neighbor classifier models that leveraged feature transformation showed 4.25% improvement in recall values on an average of over 8 software systems when compared to the models built on original data.},
booktitle = {Proceedings of the 15th Innovations in Software Engineering Conference},
articleno = {16},
numpages = {10},
keywords = {genetic algorithm, machine learning, software bug, t-SNE},
location = {Gandhinagar, India},
series = {ISEC '22}
}

@inproceedings{10.1145/1370750.1370759,
author = {Ratzinger, Jacek and Sigmund, Thomas and Gall, Harald C.},
title = {On the relation of refactorings and software defect prediction},
year = {2008},
isbn = {9781605580241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370750.1370759},
doi = {10.1145/1370750.1370759},
abstract = {This paper analyzes the influence of evolution activities such as refactoring on software defects. In a case study of five open source projects we used attributes of software evolution to predict defects in time periods of six months. We use versioning and issue tracking systems to extract 110 data mining features, which are separated into refactoring and non-refactoring related features. These features are used as input into classification algorithms that create prediction models for software defects. We found out that refactoring related features as well as non-refactoring related features lead to high quality prediction models. Additionally, we discovered that refactorings and defects have an inverse correlation: The number of software defects decreases, if the number of refactorings increased in the preceding time period. As a result, refactoring should be a significant part of both bug fixes and other evolutionary changes to reduce software defects.},
booktitle = {Proceedings of the 2008 International Working Conference on Mining Software Repositories},
pages = {35–38},
numpages = {4},
keywords = {mining, software analysis, software evolution},
location = {Leipzig, Germany},
series = {MSR '08}
}

@inproceedings{10.1145/3696687.3696688,
author = {Chen, Zhanhua and Li, Guoyong and Cao, Yanun and Du, Chenxin},
title = {Research on the Application of Agile Testing in Satellite Ground Control System},
year = {2024},
isbn = {9798400709876},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696687.3696688},
doi = {10.1145/3696687.3696688},
abstract = {Traditional software testing has gradually exposed many drawbacks in multi-project development, through a multi-dimensional comparative analysis of the limitations of traditional testing and the characteristics of agile testing, based on the characteristics of the satellite ground control system and the development mode, a targeted and operable agile software testing model is designed to increase the cyclic iteration of the test, test-driven development and exploratory testing are introduced into the testing methodology, and the test implementation enhances and clarifies interface automation testing and accurate regression testing based on impact factors, utilizes integration testing based on micro-service groups of business domains and script testing of system operation scenarios, and provides a feasible method for solving the testing of large-scale information systems. The test results show that the test method alleviates the pressure of the centralized outbreak of traditional testing, digs out the hidden dangers of the system early and effectively, improves the testing efficiency, ensures the product quality, and enhances the users' confidence in the software products.},
booktitle = {Proceedings of the International Conference on Machine Learning, Pattern Recognition and Automation Engineering},
pages = {1–8},
numpages = {8},
location = {Singapore, Singapore},
series = {MLPRAE '24}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00061,
author = {Liu, Changlin and Xiao, Xusheng},
title = {ProMal: precise window transition graphs for Android via synergy of program analysis and machine learning},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00061},
doi = {10.1109/ICSE-Companion52605.2021.00061},
abstract = {Mobile apps have been an integral part in our daily life. As these apps become more complex, it is critical to provide automated analysis techniques to ensure the correctness, security, and performance of these apps. A key component for these automated analysis techniques is to create a graphical user interface (GUI) model of an app, i.e., a window transition graph (WTG), that models windows and transitions among the windows. While existing work has provided both static and dynamic analysis to build the WTG for an app, the constructed WTG misses many transitions or contains many infeasible transitions due to the coverage issues of dynamic analysis and over-approximation of the static analysis. We propose ProMal, a "tribrid" analysis that synergistically combines static analysis, dynamic analysis, and machine learning to construct a precise WTG. Specifically, ProMal first applies static analysis to build a static WTG, and then applies dynamic analysis to verify the transitions in the static WTG. For the unverified transitions, ProMal further provides machine learning techniques that leverage runtime information (i.e., screenshots, UI layouts, and text information) to predict whether they are feasible transitions. Our evaluations on 40 real-world apps demonstrate the superiority of ProMal in building WTGs over static analysis, dynamic analysis, and machine learning techniques when they are applied separately.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {144–146},
numpages = {3},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3555776.3577809,
author = {Rebro, Dominik Arne and Chren, Stanislav and Rossi, Bruno},
title = {Source Code Metrics for Software Defects Prediction},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577809},
doi = {10.1145/3555776.3577809},
abstract = {In current research, there are contrasting results about the applicability of software source code metrics as features for defect prediction models. The goal of the paper is to evaluate the adoption of software metrics in models for software defect prediction, identifying the impact of individual source code metrics. With an empirical study on 275 release versions of 39 Java projects mined from GitHub, we compute 12 software metrics and collect software defect information. We train and compare three defect classification models. The results across all projects indicate that Decision Tree (DT) and Random Forest (RF) classifiers show the best results. Among the highest-performing individual metrics are NOC, NPA, DIT, and LCOM5. While other metrics, such as CBO, do not bring significant improvements to the models.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1469–1472},
numpages = {4},
keywords = {software defect prediction, software metrics, mining software repositories, software quality},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.5555/2486788.2487006,
author = {Jonsson, Leif},
title = {Increasing anomaly handling efficiency in large organizations using applied machine learning},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Maintenance costs can be substantial for large organizations (several hundreds of programmers) with very large and complex software systems. By large we mean lines of code in the range of hundreds of thousands or millions. Our research objective is to improve the process of handling anomaly reports for large organizations. Specifically, we are addressing the problem of the manual, laborious and time consuming process of assigning anomaly reports to the correct design teams and the related issue of localizing faults in the system architecture. In large organizations, with complex systems, this is particularly problematic because the receiver of an anomaly report may not have detailed knowledge of the whole system. As a consequence, anomaly reports may be assigned to the wrong team in the organization, causing delays and unnecessary work. We have so far developed two machine learning prototypes to validate our approach. The latest, a re-implementation and extension, of the first is being evaluated on four large systems at Ericsson AB. Our main goal is to investigate how large software development organizations can significantly improve development efficiency by replacing manual anomaly report assignment and fault localization with machine learning techniques. Our approach focuses on training machine learning systems on anomaly report databases; this is in contrast to many other approaches that are based on test case execution combined with program sampling and/or source code analysis.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1361–1364},
numpages = {4},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/3639479.3639521,
author = {Liu, Zhihan and Zhang, Zhonglin},
title = {An improved under-sampling algorithm based on density Peak clustering},
year = {2024},
isbn = {9798400709241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639479.3639521},
doi = {10.1145/3639479.3639521},
abstract = {Aiming at the noise in unbalanced data sets and the imbalance between classes, an undersampling algorithm based on density peak clustering is proposed. First of all, the density peak clustering is used for all the majority of samples to eliminate the noise points; then the sampling weight is assigned according to the different sparsity of each cluster after clustering, and the sampling number is obtained; finally, undersampling is carried out in each cluster. The proposed undersampling algorithm is compared with five commonly used sampling algorithms on six unbalanced data sets. The experimental results show that this method can effectively avoid the generation of noise in unbalanced data sets, overcome the imbalance between classes, and achieve better classification performance on the whole.},
booktitle = {Proceedings of the 2023 6th International Conference on Machine Learning and Natural Language Processing},
pages = {204–208},
numpages = {5},
keywords = {Unbalanced data, classification, density peak clustering, sparsity, under-sampling},
location = {Sanya, China},
series = {MLNLP '23}
}

@inproceedings{10.1145/3555228.3555269,
author = {Santos, Geanderson and Veloso, Adriano and Figueiredo, Eduardo},
title = {Understanding Thresholds of Software Features for Defect Prediction},
year = {2022},
isbn = {9781450397353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555228.3555269},
doi = {10.1145/3555228.3555269},
abstract = {Software defect prediction is a subject of study involving the interplay of the software engineering and machine learning areas. The current literature proposed numerous machine learning models to predict software defects from software data, such as commits and code metrics. However, existing machine learning models are more valuable when we can understand the prediction. Otherwise, software developers cannot reason why a machine learning model made such predictions, generating many questions about the model’s applicability in software projects. As explainable machine learning models for the defect prediction problem remain a recent research topic, it leaves room for exploration. In this paper, we propose a preliminary analysis of an extensive dataset to predict software defects. The dataset includes 47,618 classes from 53 open-source projects and covers 66 software features related to numerous features of the code. Therefore, we offer contributions on explaining how each selected software feature favors the prediction of software defects in Java projects. Our initial results suggest that developers should keep the values of some specific software features small to avoid software defects. We hope our approach can guide more discussions about explainable machine learning for defect prediction and its impact on software development.},
booktitle = {Proceedings of the XXXVI Brazilian Symposium on Software Engineering},
pages = {305–310},
numpages = {6},
keywords = {defect prediction, explainable machine learning, software features for defect prediction},
location = {Virtual Event, Brazil},
series = {SBES '22}
}

@inproceedings{10.1145/3609437.3609458,
author = {Yang, Peixin and Zhu, Lin and Hu, Wenhua and Keung, Jacky Wai and Lu, Liping and Xiang, Jianwen},
title = {The Impact of the bug number on Effort-Aware Defect Prediction: An Empirical Study},
year = {2023},
isbn = {9798400708947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609437.3609458},
doi = {10.1145/3609437.3609458},
abstract = {Previous research have utilized public software defect datasets such as NASA, RELINK, and SOFTLAB, which only contain class label information. Almost all Effort-Aware Defect Prediction (EADP) studies are carried out around these datasets. However, EADP studies typically relying on bug density (i.e., the ratio between bug numbers and the lines of code) for ranking software modules. In order to investigate the impact of neglecting bug number information in software defect datasets on the performance of EADP models, we examine the performance degradation of the best-performing learning to rank methods when class labels are utilized instead of bug numbers. The experimental results show that neglecting bug number information in building EADP models results in an increase in the detected bugs. However, it also leads to a significant increase in the initial false alarms, ranging from 45.5% to 90.9% of the datasets, and an significant increase in the modules that need to be inspected, ranging from 5.2% to 70.4%. Therefore, we recommend not only the class labels but also the bug number information should be disclosed when publishing software defect datasets, in order to construct more accurate EADP models.},
booktitle = {Proceedings of the 14th Asia-Pacific Symposium on Internetware},
pages = {67–78},
numpages = {12},
keywords = {Bug Number, Effort-Aware, Learning to Rank, Software Defect Prediction},
location = {Hangzhou, China},
series = {Internetware '23}
}

@inproceedings{10.1145/1868328.1868350,
author = {Zhang, Hongyu and Nelson, Adam and Menzies, Tim},
title = {On the value of learning from defect dense components for software defect prediction},
year = {2010},
isbn = {9781450304047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868328.1868350},
doi = {10.1145/1868328.1868350},
abstract = {BACKGROUND: Defect predictors learned from static code measures can isolate code modules with a higher than usual probability of defects.AIMS: To improve those learners by focusing on the defect-rich portions of the training sets.METHOD: Defect data CM1, KC1, MC1, PC1, PC3 was separated into components. A subset of the projects (selected at random) were set aside for testing. Training sets were generated for a NaiveBayes classifier in two ways. In sample the dense treatment, the components with higher than the median number of defective modules were used for training. In the standard treatment, modules from any component were used for training. Both samples were run against the test set and evaluated using recall, probability of false alarm, and precision. In addition, under sampling and over sampling was performed on the defect data. Each method was repeated in a 10-by-10 cross-validation experiment.RESULTS: Prediction models learned from defect dense components out-performed standard method, under sampling, as well as over sampling. In statistical rankings based on recall, probability of false alarm, and precision, models learned from dense components won 4--5 times more often than any other method, and also lost the least amount of times.CONCLUSIONS: Given training data where most of the defects exist in small numbers of components, better defect predictors can be trained from the defect dense components.},
booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
articleno = {14},
numpages = {9},
keywords = {ceiling effect, defect dense components, defect prediction, sampling},
location = {Timi\c{s}oara, Romania},
series = {PROMISE '10}
}

@inproceedings{10.1145/956750.956795,
author = {Last, Mark and Friedman, Menahem and Kandel, Abraham},
title = {The data mining approach to automated software testing},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956795},
doi = {10.1145/956750.956795},
abstract = {In today's industry, the design of software tests is mostly based on the testers' expertise, while test automation tools are limited to execution of pre-planned tests only. Evaluation of test outputs is also associated with a considerable effort by human testers who often have imperfect knowledge of the requirements specification. Not surprisingly, this manual approach to software testing results in heavy losses to the world's economy. The costs of the so-called "catastrophic" software failures (such as Mars Polar Lander shutdown in 1999) are even hard to measure. In this paper, we demonstrate the potential use of data mining algorithms for automated induction of functional requirements from execution data. The induced data mining models of tested software can be utilized for recovering missing and incomplete specifications, designing a minimal set of regression tests, and evaluating the correctness of software outputs when testing new, potentially flawed releases of the system. To study the feasibility of the proposed approach, we have applied a novel data mining algorithm called Info-Fuzzy Network (IFN) to execution data of a general-purpose code for solving partial differential equations. After being trained on a relatively small number of randomly generated input-output examples, the model constructed by the IFN algorithm has shown a clear capability to discriminate between correct and faulty versions of the program.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {388–396},
numpages = {9},
keywords = {automated software testing, finite element solver, info-fuzzy networks, input-output analysis, regression testing},
location = {Washington, D.C.},
series = {KDD '03}
}

@article{10.1145/2347696.2347709,
author = {Rashid, Ekbal and Patnayak, Srikanta and Bhattacherjee, Vandana},
title = {A survey in the area of machine learning and its application for software quality prediction},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/2347696.2347709},
doi = {10.1145/2347696.2347709},
abstract = {This paper explores software quality improvement through early prediction of error patterns. It summarizes a variety of techniques for software quality prediction in the domain of software engineering. The objective of this research is to apply the various machine learning approaches, such as Case-Based Reasoning and Fuzzy logic, to predict software quality. The system predicts the error after accepting the values of certain parameters of the software. This paper advocates the use of case-based reasoning (i.e., CBR) to build a software quality prediction system with the help of human experts. The prediction is based on analogy. We have used different similarity measures to find the best method that increases reliability. This software is compiled using Turbo C++ 3.0 and hence it is very compact and standalone. It can be readily deployed on any configuration without affecting its performance.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {1–7},
numpages = {7},
keywords = {CBR, analogy, erffort estimation, function, machine learning, similarity, software quality}
}

@inproceedings{10.1145/3544902.3546246,
author = {Cui, Di and Li, Xingyu and Liu, Feiyang and Wang, Siqi and Dai, Jie and Wang, Lu and Li, Qingshan},
title = {Towards Demystifying the Impact of Dependency Structures on Bug Locations in Deep Learning Libraries},
year = {2022},
isbn = {9781450394277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544902.3546246},
doi = {10.1145/3544902.3546246},
abstract = {Background: Many safety-critical industrial applications have turned to deep learning systems as a fundamental component. Most of these systems rely on deep learning libraries, and bugs of such libraries can have irreparable consequences. Aims: Over the years, dependency structure has shown to be a practical indicator of software quality, widely used in numerous bug prediction techniques. The problem is that when analyzing bugs in deep learning libraries, researchers are unclear whether dependency structures still have a high correlation and which forms of dependency structures perform the best. Method: In this paper, we present a systematic investigation of the above question and implement a dependency structure-centric bug analysis tool: Depend4BL, capturing the interaction between dependency structures and bug locations in deep learning libraries. Results: We employ Depend4BL to analyze the top 5 open-source deep learning libraries on Github in terms of stars and forks, with 279,788 revision commits and 8,715 bug fixes. The results demonstrate the significant differences among syntactic, history, and semantic structures, and their vastly different impacts on bug locations. Their combinations have the potential to further improve bug prediction for deep learning libraries. Conclusions: In summary, our work provides a new perspective regarding to the correlation between dependency structures and bug locations in deep learning libraries. We release a large set of benchmarks and a prototype toolkit to automatically detect various forms of dependency structures for deep learning libraries. Our study also unveils useful findings based on quantitative and qualitative analysis that benefit bug prediction techniques for deep learning libraries.},
booktitle = {Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {249–260},
numpages = {12},
keywords = {Bug Prediction., Deep Learning System, Dependency Structure},
location = {Helsinki, Finland},
series = {ESEM '22}
}

@inproceedings{10.1109/ICSE48619.2023.00076,
author = {Li, Jiawei and Ahmed, Iftekhar},
title = {Commit Message Matters: Investigating Impact and Evolution of Commit Message Quality},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00076},
doi = {10.1109/ICSE48619.2023.00076},
abstract = {Commit messages play an important role in communication among developers. To measure the quality of commit messages, researchers have defined what semantically constitutes a Good commit message: it should have both the summary of the code change (What) and the motivation/reason behind it (Why). The presence of the issue report/pull request links referenced in a commit message has been treated as a way of providing Why information. In this study, we found several quality issues that could hamper the links' ability to provide Why information. Based on this observation, we developed a machine learning classifier for automatically identifying whether a commit message has What and Why information by considering both the commit messages and the link contents. This classifier outperforms state-of-the-art machine learning classifiers by 12 percentage points improvement in the F1 score. With the improved classifier, we conducted a mixed method empirical analysis and found that: (1) Commit message quality has an impact on software defect proneness, and (2) the overall quality of the commit messages decreases over time, while developers believe they are writing better commit messages. All the research artifacts (i.e., tools, scripts, and data) of this study are available on the accompanying website [2].},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {806–817},
numpages = {12},
keywords = {commit message quality, software defect proneness, empirical analysis},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/2530290,
author = {Khan, Mohammad Maifi Hasan and Le, Hieu Khac and Ahmadi, Hossein and Abdelzaher, Tarek F. and Han, Jiawei},
title = {Troubleshooting interactive complexity bugs in wireless sensor networks using data mining techniques},
year = {2014},
issue_date = {January 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1550-4859},
url = {https://doi.org/10.1145/2530290},
doi = {10.1145/2530290},
abstract = {This article presents a tool for uncovering bugs due to interactive complexity in networked sensing applications. Such bugs are not localized to one component that is faulty, but rather result from complex and unexpected interactions between multiple often individually nonfaulty components. Moreover, the manifestations of these bugs are often not repeatable, making them particularly hard to find, as the particular sequence of events that invokes the bug may not be easy to reconstruct. Because of the distributed nature of failure scenarios, our tool looks for sequences of events that may be responsible for faulty behavior, as opposed to localized bugs such as a bad pointer in a module. We identified several challenges in applying discriminative sequence mining for root cause analysis when the system fails to perform as expected and presented our solutions to those challenges. We also present two alternative schemes, namely, two-stage mining and the progressive discriminative sequence mining to address the scalability challenge. An extensible framework is developed where a front-end collects runtime data logs of the system being debugged and an offline back-end uses frequent discriminative pattern mining to uncover likely causes of failure. We provided several case studies where we applied our tool successfully to troubleshoot the cause of the problem. We uncovered a kernel-level race condition bug in the LiteOS operating system and a protocol design bug in the directed diffusion protocol. We also presented a case study of debugging a multichannel MAC protocol that was found to exhibit corner cases of poor performance (worse than single-channel MAC). The tool helped to uncover event sequences that lead to a highly degraded mode of operation. Fixing the problem significantly improved the performance of the protocol. We also evaluated the extensions presented in this article. Finally, we provided a detailed analysis of tool overhead in terms of memory requirements and impact on the running application.},
journal = {ACM Trans. Sen. Netw.},
month = jan,
articleno = {31},
numpages = {35},
keywords = {Distributed protocol debugging, wireless sensor networks}
}

@inproceedings{10.1145/3172871.3172880,
author = {Singh, Maninder and Anu, Vaibhav and Walia, Gursimran S. and Goswami, Anurag},
title = {Validating Requirements Reviews by Introducing Fault-Type Level Granularity: A Machine Learning Approach},
year = {2018},
isbn = {9781450363983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172871.3172880},
doi = {10.1145/3172871.3172880},
abstract = {Inspections are a proven approach for improving software requirements quality. Owing to the fact that inspectors report both faults and non-faults (i.e., false-positives) in their inspection reports, a major chunk of work falls on the person who is responsible for consolidating the reports received from multiple inspectors. We aim at automation of fault-consolidation step by using supervised machine learning algorithms that can effectively isolate faults from non-faults. Three different inspection studies were conducted in controlled environments to obtain real inspection data from inspectors belonging to both industry and from academic backgrounds. Next, we devised a methodology to separate faults from non-faults by first using ten individual classifiers from five different classification families to categorize different fault-types (e.g., omission, incorrectness, and inconsistencies). Based on the individual performance of classifiers for each fault-type, we created targeted ensembles that are suitable for identification of each fault-type. Our analysis showed that our selected ensemble classifiers were able to separate faults from non-faults with very high accuracy (as high as 85-89% for some fault-types), with a notable result being that in some cases, individual classifiers performed better than ensembles. In general, our approach can significantly reduce effort required to isolate faults from false-positives during the fault consolidation step of requirements inspections. Our approach also discusses the percentage possibility of correctly classifying each fault-type.},
booktitle = {Proceedings of the 11th Innovations in Software Engineering Conference},
articleno = {10},
numpages = {11},
keywords = {Ensemble, Fault types, Inspection reviews, Machine learning, Supervised learning},
location = {Hyderabad, India},
series = {ISEC '18}
}

@inproceedings{10.1145/1188895.1188910,
author = {Di Fatta, Giuseppe and Leue, Stefan and Stegantova, Evghenia},
title = {Discriminative pattern mining in software fault detection},
year = {2006},
isbn = {1595935843},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1188895.1188910},
doi = {10.1145/1188895.1188910},
abstract = {We present a method to enhance fault localization for software systems based on a frequent pattern mining algorithm. Our method is based on a large set of test cases for a given set of programs in which faults can be detected. The test executions are recorded as function call trees. Based on test oracles the tests can be classified into successful and failing tests. A frequent pattern mining algorithm is used to identify frequent subtrees in successful and failing test executions. This information is used to rank functions according to their likelihood of containing a fault. The ranking suggests an order in which to examine the functions during fault analysis. We validate our approach experimentally using a subset of Siemens benchmark programs.},
booktitle = {Proceedings of the 3rd International Workshop on Software Quality Assurance},
pages = {62–69},
numpages = {8},
keywords = {automated debugging, fault isolation},
location = {Portland, Oregon},
series = {SOQUA '06}
}

@inproceedings{10.1145/3611643.3616308,
author = {Nicolae, Maria-Irina and Eisele, Max and Zeller, Andreas},
title = {Revisiting Neural Program Smoothing for Fuzzing},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616308},
doi = {10.1145/3611643.3616308},
abstract = {Testing with randomly generated inputs (fuzzing) has gained significant traction due to its capacity to expose program vulnerabilities automatically. Fuzz testing campaigns generate large amounts of data, making them ideal for the application of machine learning (ML). Neural program smoothing, a specific family of ML-guided fuzzers, aims to use a neural network as a smooth approximation of the program target for new test case generation. In this paper, we conduct the most extensive evaluation of neural program smoothing (NPS) fuzzers against standard gray-box fuzzers (&gt;11 CPU years and &gt;5.5 GPU years), and make the following contributions:  We find that the original performance claims for NPS fuzzers do not hold; a gap we relate to fundamental, implementation, and experimental limitations of prior works. We contribute the first in-depth analysis of the contribution of machine learning and gradient-based mutations in NPS. We implement Neuzz++, which shows that addressing the practical limitations of NPS fuzzers improves performance, but that standard gray-box fuzzers almost always surpass NPS-based fuzzers. As a consequence, we propose new guidelines targeted at benchmarking fuzzing based on machine learning, and present MLFuzz, a platform with GPU access for easy and reproducible evaluation of ML-based fuzzers.  Neuzz++, MLFuzz, and all our data are public.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {133–145},
numpages = {13},
keywords = {fuzzing, machine learning, neural networks, neural program smoothing},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3549034.3561176,
author = {Hasabnis, Niranjan},
title = {Are machine programming systems using right source-code measures to select code repositories?},
year = {2022},
isbn = {9781450394567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549034.3561176},
doi = {10.1145/3549034.3561176},
abstract = {Machine programming (MP) is an emerging field at the intersection of  
deterministic and probabilistic computing, and it aims to assist software and  
hardware engineers, among other applications. Along with powerful compute  
resources, MP systems often rely on vast amount of open-source code to learn  
interesting properties about code and programming and solve problems in the  
areas of debugging, code recommendation, auto-completion, etc. Unfortunately,  
several of the existing MP systems either do not consider quality of code  
repositories or use atypical quality measures than those typically used in  
software engineering community to select them. As such, impact of quality of  
code repositories on the performance of these systems needs to be studied.  

In this preliminary paper, we evaluate impact of different quality repositories  
on the performance of a candidate MP system. Towards that objective, we develop  
a framework, named GitRank, to rank open-source repositories on quality,  
maintainability, and popularity by leveraging existing research on this topic.  
We then apply GitRank to evaluate correlation between the quality measures  
used by the candidate MP system and the quality measures used by our framework.  
Our preliminary results reveal some correlation between the quality measures  
used in GitRank and ControlFlag's performance, suggesting that some of the  
measures used in GitRank are applicable to ControlFlag. But it also raises  
questions around right quality measures for code repositories used in MP  
systems. We believe that our findings also generate interesting insights towards  
code quality measures that affect performance of MP systems.},
booktitle = {Proceedings of the 6th International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {11–16},
numpages = {6},
keywords = {AI, code quality, code repositories, machine learning, machine programming, software engineering},
location = {Singapore, Singapore},
series = {MaLTeSQuE 2022}
}

@inproceedings{10.1145/3661167.3661217,
author = {Fehrer, Therese and Cabrera Lozoya, Rocio and Sabetta, Antonino and Di Nucci, Dario and Tamburri, Damian A.},
title = {Detecting Security Fixes in Open-Source Repositories using Static Code Analyzers},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661217},
doi = {10.1145/3661167.3661217},
abstract = {The sources of reliable, code-level information about vulnerabilities that affect open-source software (OSS) are scarce, which hinders a broad adoption of advanced tools that provide code-level detection and assessment of vulnerable OSS dependencies. In this paper, we report our findings from using features extracted from four (PMD, Checkstyle, CK, Progex) off-the-shelf static code analyzers relying on pattern matching, software metrics or program analysis in a machine-learning pipeline to identify source code commits that contain vulnerability fixes. We show that successful machine learning models based on base classifiers and ensemble techniques can be trained on the combination of the features.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {429–432},
numpages = {4},
keywords = {commit representation, machine learning, software vulnerability analysis, source code representation},
location = {Salerno, Italy},
series = {EASE '24}
}

@inproceedings{10.1145/3640912.3640931,
author = {Wang, Juanning and Li, Qiang},
title = {Automatic testing of computerized interlocks based on recognition of the interlocking host computer interface},
year = {2024},
isbn = {9798400716683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640912.3640931},
doi = {10.1145/3640912.3640931},
abstract = {Contending with the current status quo of computerized interlock automatic test, in the test result determination module of computerized interlock automatic test system, OpenCV computer vision library is introduced to realize the test result determination based on interlock interface recognition. In the automatic test system, the interlocking software under test receives the simulated push-button operation commands and the simulated station signaling equipment status, and outputs the test results after the interlocking operation. OpenCV recognizes the signaling status changes of signaling machines, turnouts, track sections and other equipment in the test results, and then compares the results with the expected results to determine whether the test is passed or not. The results show that the automatic test result determination based on image recognition is more versatile, more visualized, and significantly more efficient.},
booktitle = {Proceedings of the 2023 International Conference on Communication Network and Machine Learning},
pages = {97–100},
numpages = {4},
location = {Zhengzhou, China},
series = {CNML '23}
}

@inproceedings{10.1145/1085130.1085144,
author = {Tron\c{c}on, Remko and Janssens, Gerda},
title = {Analyzing &amp; debugging ILP data mining query execution},
year = {2005},
isbn = {1595930507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1085130.1085144},
doi = {10.1145/1085130.1085144},
abstract = {We present a trace based approach for analyzing the runs of Inductive Logic Programming Data Mining systems, without needing to modify the actual implementation of the ILP mining algorithms. We discuss the use of traces as the basis for easy and fast, semi-automated debugging of the underlying (query) execution engine of the ILP system. Our approach also provides a way to monitor the behavior of queries generated by the ILP algorithm, allowing an evaluation and comparison of the impact of different execution mechanisms. The traces can be extended further, and as such be useful for visualization and monitoring of other aspects of ILP data mining systems.},
booktitle = {Proceedings of the Sixth International Symposium on Automated Analysis-Driven Debugging},
pages = {105–110},
numpages = {6},
keywords = {debugging, inductive logic programming, monitoring, traces},
location = {Monterey, California, USA},
series = {AADEBUG'05}
}

@inproceedings{10.1145/3616901.3616929,
author = {Yuan, Yuan},
title = {Software Technology Project Defect Prediction Based on AI},
year = {2024},
isbn = {9798400707544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616901.3616929},
doi = {10.1145/3616901.3616929},
abstract = {Abstract—In the rapidly developing information age, software development has become an important component of modern society. However, due to the complexity of the development process, project defects would inevitably occur in the software development process, leading to the extension of the software development cycle and the increase of costs. Therefore, this article explores and studies the issue of defect prediction in software technology projects based on artificial intelligence (AI). By analyzing and mining relevant data on project defects, a defect prediction model based on machine learning was established, and the effectiveness of the model was evaluated and analyzed. The results show that the average defect detection rate of strategy 3 is 0.88. The AI based software technology project defect prediction method proposed in this article can effectively improve the quality and efficiency of software development.},
booktitle = {Proceedings of the 2023 International Conference on Frontiers of Artificial Intelligence and Machine Learning},
pages = {126–130},
numpages = {5},
keywords = {Keywords: Artificial Intelligence, Prediction Systems, Project Defects, Software Technology},
location = {Beijing, China},
series = {FAIML '23}
}

@inproceedings{10.1145/3640912.3640919,
author = {Li, Zeru and Qin, Zhongyuan and Sun, Xin and Wang, Wen},
title = {Efficient fuzzing testcases generation based on SAGAN},
year = {2024},
isbn = {9798400716683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640912.3640919},
doi = {10.1145/3640912.3640919},
abstract = {Generative adversarial networks can be used for the generation of testcases in fuzzing, but the structural information of the testcases is rarely attended to. In this paper, we adopt a SAGAN-based testcases generation technique, to learn and utilize the structural information of the testcases and give attention to the important parts. We selectively improve the network structure so that the model can be more adapted to the structural information of the fuzzing testcases. We used gradient penalty and spectral normalization to stabilize the training of the network. The results show that our approach has higher efficiency on the lava-m dataset. In addition, the fuzzing testcases generated by SAGAN can find more crashes and hangs compared to those mutated by AFL++.},
booktitle = {Proceedings of the 2023 International Conference on Communication Network and Machine Learning},
pages = {33–38},
numpages = {6},
location = {Zhengzhou, China},
series = {CNML '23}
}

@article{10.5555/3648699.3649063,
author = {Laberge, Gabriel and Pequignot, Yann and Mathieu, Alexandre and Khomh, Foutse and Marchand, Mario},
title = {Partial order in chaos: consensus on feature attributions in the rashomon set},
year = {2023},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {Post-hoc global/local feature attribution methods are progressively being employed to understand the decisions of complex machine learning models. Yet, because of limited amounts of data, it is possible to obtain a diversity of models with good empirical performance but that provide very different explanations for the same prediction, making it hard to derive insight from them. In this work, instead of aiming at reducing the underspecification of model explanations, we fully embrace it and extract logical statements about feature attributions that are consistent across all models with good empirical performance (i.e. all models in the Rashomon Set). We show that partial orders of local/global feature importance arise from this methodology enabling more nuanced interpretations by allowing pairs of features to be incomparable when there is no consensus on their relative importance. We prove that every relation among features present in these partial orders also holds in the rankings provided by existing approaches. Finally, we present three use cases employing hypothesis spaces with tractable Rashomon Sets (Additive models, Kernel Ridge, and Random Forests) and show that partial orders allow one to extract consistent local and global interpretations of models despite their under-specification.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {364},
numpages = {50},
keywords = {XAI, feature attribution, under-specification, Rashomon set, uncertainty}
}

@article{10.1145/3708532,
author = {H\"{a}m\"{a}l\"{a}inen, Joonas and Das, Teerath and Mikkonen, Tommi},
title = {A Systematic Literature Review of Multi-Label Learning in Software Engineering},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708532},
doi = {10.1145/3708532},
abstract = {In this paper, we provide the first systematic literature review of the intersection of two research areas, Multi-Label Learning (MLL) and Software Engineering (SE). We refer to this intersection as MLL4SE. In recent years, MLL problems have increased in many applications and research areas because real-world datasets often have a multi-label nature. For multi-label data, simplifying the assumption of traditional classification approaches that an instance can only be associated with one class only leads to worse accuracy. Thus, a better match of methods and assumptions about the data is required. We identified 50 primary studies in our systematic literature review in the MLL4SE domain. Based on this review, we identified six main SE application domains where MLL has been applied. These domains include Software Requirement Engineering, Issue Tracking and Management, Community and Knowledge Management, API Usage and Management, Code Quality and Maintenance, and Mobile Application Development. We summarized the methods used and the data nature of the MLL4SE applications. Moreover, we separately provide taxonomies of future work directions from machine learning and software engineering perspectives. In general, we highlight current trends, research gaps, and shortcomings.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {Machine Learning, Multi-Label Learning, Software Engineering, Systematic Literature Review, Software Development Life Cycle (SDLC) Activities}
}

@inproceedings{10.1145/568760.568824,
author = {Denaro, Giovanni and Morasca, Sandro and Pezz\`{e}, Mauro},
title = {Deriving models of software fault-proneness},
year = {2002},
isbn = {1581135564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/568760.568824},
doi = {10.1145/568760.568824},
abstract = {The effectiveness of the software testing process is a key issue for meeting the increasing demand of quality without augmenting the overall costs of software development. The estimation of software fault-proneness is important for assessing costs and quality and thus better planning and tuning the testing process. Unfortunately, no general techniques are available for estimating software fault-proneness and the distribution of faults to identify the correct level of test for the required quality. Although software complexity and testing thoroughness are intuitively related to the costs of quality assurance and the quality of the final product, single software metrics and coverage criteria provide limited help in planning the testing process and assuring the required quality.By using logistic regression, this paper shows how models can be built that relate software measures and software fault-proneness for classes of homogeneous software products. It also proposes the use of cross-validation for selecting valid models even for small data sets.The early results show that it is possible to build statistical models based on historical data for estimating fault-proneness of software modules before testing, and thus better planning and monitoring the testing activities.},
booktitle = {Proceedings of the 14th International Conference on Software Engineering and Knowledge Engineering},
pages = {361–368},
numpages = {8},
keywords = {cross-validation, fault-proneness models, logistic regression, software faultiness, software metrics, software testing process},
location = {Ischia, Italy},
series = {SEKE '02}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00098,
author = {Chimalakonda, Sridhar and Das, Debeshee and Mathai, Alex and Tamilselvam, Srikanth and Kumar, Atul},
title = {The Landscape of Source Code Representation Learning in AI-Driven Software Engineering Tasks},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00098},
doi = {10.1109/ICSE-Companion58688.2023.00098},
abstract = {Appropriate representation of source code and its relevant properties form the backbone of Artificial Intelligence (AI)/ Machine Learning (ML) pipelines for various software engineering (SE) tasks such as code classification, bug prediction, code clone detection, and code summarization. In the literature, researchers have extensively experimented with different kinds of source code representations (syntactic, semantic, integrated, customized) and ML techniques such as pre-trained BERT models. In addition, it is common for researchers to create hand-crafted and customized source code representations for an appropriate SE task. In a 2018 survey [1], Allamanis et al. listed nearly 35 different ways of of representing source code for different SE tasks like Abstract Syntax Trees (ASTs), customized ASTs, Control Flow Graphs (CFGs), Data Flow Graphs (DFGs) and so on. The main goal of this tutorial is two-fold (i) Present an overview of the state-of-the-art of source code representations and corresponding ML pipelines with an explicit focus on the merits and demerits of each of the representations (ii) Practical challenges in infusing different code-views in the state-of-the-art ML models and future research directions.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {342–343},
numpages = {2},
keywords = {code representation, machine learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3617555.3617871,
author = {Karakas, Umutcan and Tosun, Ayse},
title = {Automated Fairness Testing with Representative Sampling},
year = {2023},
isbn = {9798400703751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617555.3617871},
doi = {10.1145/3617555.3617871},
abstract = {The issue of fairness testing in machine learning models has become popular due to rising concerns about potential bias and discrimination, as these models continue to permeate end-user applications. However, achieving an accurate and reliable measurement of the fairness performance of machine learning models remains a substantial challenge. Representative sampling plays a pivotal role in ensuring accurate fairness assessments and providing insight into the underlying dynamics of data, unlike biased or random sampling approaches. In our study, we introduce our approach, namely RSFair, which adopts the representative sampling method to comprehensively evaluate the fairness performance of a trained machine learning model. Our research findings on two datasets indicate that RSFair yields more accurate and reliable results, thus improving the efficiency of subsequent search steps, and ultimately the fairness performance of the model. With the usage of Orthogonal Matching Pursuit (OMP) and K-Singular Value Decomposition (K-SVD) algorithms for representative sampling, RSFair significantly improves the detection of discriminatory inputs by 76% and the fairness performance by 53% compared to other search-based approaches in the literature.},
booktitle = {Proceedings of the 19th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {54–63},
numpages = {10},
keywords = {fairness testing, machine learning, representative sampling},
location = {San Francisco, CA, USA},
series = {PROMISE 2023}
}

@inproceedings{10.1145/3330204.3330230,
author = {de Macedo, Charles Mendes and Ruela, Andr\'{e} Siqueira and Delgado, Karina Valdivia},
title = {Application of Clustering Algorithms for Discovering Bug Patterns in JavaScript Software},
year = {2019},
isbn = {9781450372374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330204.3330230},
doi = {10.1145/3330204.3330230},
abstract = {Applications developed with JavaScript language are increasing every day, not only for client-side, but also for server-side and for mobile devices. In this context, the existence of tools to identify faults is fundamental in order to assist developers during the evolution of their applications. Different tools and approaches have been proposed over the years, however they have limitations to evolve over time, becoming obsolete quickly. The reason for this is the use of a fixed list of pre-defined faults that are searched in the code. The BugAID tool implements a semiautomatic strategy for discovering bug patterns by grouping the changes made during the project development. The objective of this work is to contribute to the BugAID tool, extending this tool with improvements in the extraction of characteristics to be used by the clustering algorithm. The extended module of the BugAID extraction module (BE) that extracts the characteristics is called BE+. Additionally, an evaluation of the clustering algorithms used for discovering fault patterns in JavaScript software is performed. The results show that the DBScan and Optics algorithms with BE+ presented the best results for the Rand, Jaccard and Adjusted Rand indexes, while HDBScan with BE and BE+ presented the worst result.},
booktitle = {Proceedings of the XV Brazilian Symposium on Information Systems},
articleno = {21},
numpages = {8},
keywords = {Bug Discovery, Data Mining, Machine Learning, Pattern Recognition, Software Quality},
location = {Aracaju, Brazil},
series = {SBSI '19}
}

@inproceedings{10.1145/3613372.3613412,
author = {Martins, Vin\'{\i}Cius and T. Ramalho, Camila and Cordeiro Marques, Lucas and Alves Pereira, Juliana and Garcia, Alessandro and Lucena, Carlos and Feij\'{o}, Bruno and L. Furtado, Antonio},
title = {Analyzing a Semantics-Aware Bug Seeding Tool's Efficacy: A qualitative study with the SemSeed tool},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613372.3613412},
doi = {10.1145/3613372.3613412},
abstract = {Software developers can benefit from machine learning solutions to predict bugs. Machine learning solutions usually require a lot of data to train a model in order to achieve reliable results. In this context, developers use bug-seeding approaches to generate synthetic bugs, which should be similar to human-made bugs. A recent state-of-the-art tool, called SemSeed, uses a semantics-aware bug seeding approach in order to hopefully achieve more realistic bugs. In this study, we report on the investigation of SemSeed’s efficacy. We create a survey that shows developers a bug and asks whether it is a Real or Synthetic bug. We collected and analyzed the answers from 47 developers, and we show that SemSeed can be very accurate in seeding realistic bugs.},
booktitle = {Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
pages = {246–256},
numpages = {11},
keywords = {bug detection., bug seeding, machine learning, software engineering},
location = {Campo Grande, Brazil},
series = {SBES '23}
}

@inproceedings{10.1145/3691620.3695481,
author = {Zhao, Zhenjiang and Toda, Takahisa and Kitamura, Takashi},
title = {Approximation-guided Fairness Testing through Discriminatory Space Analysis},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695481},
doi = {10.1145/3691620.3695481},
abstract = {As machine learning (ML) systems are increasingly used in various fields, including tasks with high social impact, concerns about their fairness are growing. To address these concerns, individual fairness testing (IFT) has been introduced to identify individual discriminatory instances (IDIs) that indicate the violation of individual fairness in a given ML classifier. In this paper, we propose a black-box testing algorithm for IFT, named Aft (short for Approximation-guided Fairness Testing). Aft constructs approximate models based on decision trees, and generates test cases by sampling paths of the decision trees. Our evaluation by experiments confirms that Aft outperforms the state-of-the-art black-box IFT algorithm ExpGA both in efficiency (by 3.42 times) and diversity of IDIs identified by algorithms (by 1.16 times).},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1007–1018},
numpages = {12},
keywords = {machine learning, algorithmic fairness, fairness testing, decision tree, sampling algorithm},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@article{10.1145/3542944,
author = {Ding, Zishuo and Li, Heng and Shang, Weiyi and Chen, Tse-Hsun (Peter)},
title = {Towards Learning Generalizable Code Embeddings Using Task-agnostic Graph Convolutional Networks},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3542944},
doi = {10.1145/3542944},
abstract = {Code embeddings have seen increasing applications in software engineering (SE) research and practice recently. Despite the advances in embedding techniques applied in SE research, one of the main challenges is their generalizability. A recent study finds that code embeddings may not be readily leveraged for the downstream tasks that the embeddings are not particularly trained for. Therefore, in this article, we propose GraphCodeVec, which represents the source code as graphs and leverages the Graph Convolutional Networks to learn more generalizable code embeddings in a task-agnostic manner. The edges in the graph representation are automatically constructed from the paths in the abstract syntax trees, and the nodes from the tokens in the source code. To evaluate the effectiveness of GraphCodeVec , we consider three downstream benchmark tasks (i.e., code comment generation, code authorship identification, and code clones detection) that are used in a prior benchmarking of code embeddings and add three new downstream tasks (i.e., source code classification, logging statements prediction, and software defect prediction), resulting in a total of six downstream tasks that are considered in our evaluation. For each downstream task, we apply the embeddings learned by GraphCodeVec and the embeddings learned from four baseline approaches and compare their respective performance. We find that GraphCodeVec outperforms all the baselines in five out of the six downstream tasks, and its performance is relatively stable across different tasks and datasets. In addition, we perform ablation experiments to understand the impacts of the training context (i.e., the graph context extracted from the abstract syntax trees) and the training model (i.e., the Graph Convolutional Networks) on the effectiveness of the generated embeddings. The results show that both the graph context and the Graph Convolutional Networks can benefit GraphCodeVec in producing high-quality embeddings for the downstream tasks, while the improvement by Graph Convolutional Networks is more robust across different downstream tasks and datasets. Our findings suggest that future research and practice may consider using graph-based deep learning methods to capture the structural information of the source code for SE tasks.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {48},
numpages = {43},
keywords = {Machine learning, source code representation, code embeddings, neural network}
}

@inproceedings{10.1145/3653644.3653652,
author = {Wang, Xin and Shi, Kaiming and Chen, Mingliang and He, Lulu},
title = {Diagnostic Determination in Real-Time Systems},
year = {2024},
isbn = {9798400709777},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3653644.3653652},
doi = {10.1145/3653644.3653652},
abstract = {In the field of automatic control, particularly within complex systems, fault diagnosis is a critical yet challenging task. The effectiveness of fault diagnosis relies on a system property known as diagnosability. Diagnosability refers to a system's capability to accurately identify the occurrence of faults through a series of observations, i.e., it enables the differentiation between erroneous and correct observations. To date, there has been limited research focused on analyzing the diagnosability of real-time systems. In this paper, we propose an algorithm aimed at assessing the diagnosability of real-time systems. Our approach involves modeling real-time systems using timed automata and redefining diagnosability for real-time systems by accounting for the delays between observable events. Additionally, we explore how time constraints influence diagnosability. This paper has two main contributions. Firstly, we provide and prove a sufficient and necessary condition for diagnosability based on timed automata. Secondly, we propose an efficient algorithm for verifying diagnosability.},
booktitle = {Proceedings of the 2024 3rd International Conference on Frontiers of Artificial Intelligence and Machine Learning},
pages = {211–214},
numpages = {4},
keywords = {Diagnosability, Model, Time constraints},
location = {Yichang, China},
series = {FAIML '24}
}

@article{10.1145/3597202,
author = {Suneja, Sahil and Zhuang, Yufan and Zheng, Yunhui and Laredo, Jim and Morari, Alessandro and Khurana, Udayan},
title = {Incorporating Signal Awareness in Source Code Modeling: An Application to Vulnerability Detection},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3597202},
doi = {10.1145/3597202},
abstract = {AI models of code have made significant progress over the past few years. However, many models are actually not learning task-relevant source code features. Instead, they often fit non-relevant but correlated data, leading to a lack of robustness and generalizability, and limiting the subsequent practical use of such models. In this work, we focus on improving the model quality through signal awareness, i.e., learning the relevant signals in the input for making predictions. We do so by leveraging the heterogeneity of code samples in terms of their signal-to-noise content. We perform an end-to-end exploration of model signal awareness, comprising: (i) uncovering the reliance of AI models of code on task-irrelevant signals, via prediction-preserving input minimization; (ii) improving models’ signal awareness by incorporating the notion of code complexity during model training, via curriculum learning; (iii) improving models’ signal awareness by generating simplified signal-preserving programs and augmenting them to the training dataset; and (iv) presenting a novel interpretation of the model learning behavior from the perspective of the dataset, using its code complexity distribution. We propose a new metric to measure model signal awareness, Signal-aware Recall, which captures how much of the model’s performance is attributable to task-relevant signal learning. Using a software vulnerability detection use-case, our model probing approach uncovers a significant lack of signal awareness in the models, across three different neural network architectures and three datasets. Signal-aware Recall is observed to be in the sub-50s for models with traditional Recall in the high 90s, suggesting that the models are presumably picking up a lot of noise or dataset nuances while learning their logic. With our code-complexity-aware model learning enhancement techniques, we are able to assist the models toward more task-relevant learning, recording up-to 4.8\texttimes{} improvement in model signal awareness. Finally, we employ our model learning introspection approach to uncover the aspects of source code where the model is facing difficulty, and we analyze how our learning enhancement techniques alleviate it.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {145},
numpages = {40},
keywords = {Machine learning, neural networks, reliability, signal awareness, curriculum learning, data augmentation, explainability}
}

@inproceedings{10.1145/3549034.3561179,
author = {Chao, Liu and Qiaoluan, Xie and Yong, Li and Yang, Xu and Hyun-Deok, Choi},
title = {DeepCrash: deep metric learning for crash bucketing based on stack trace},
year = {2022},
isbn = {9781450394567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549034.3561179},
doi = {10.1145/3549034.3561179},
abstract = {Some software projects collect vast crash reports from testing and end users, then organize them in groups to efficiently fix bugs. This task is crash report bucketing. In particular, a high precision and fast speed crash similarity measurement approach is the critical constraint for large-scale crash bucketing. In this paper, we propose a deep learning-based crash bucketing method which maps stack trace to feature vectors and groups these feature vectors into buckets. First, we develop a frame tokenization method for stack trace, called frame2vec, to extract frame representations based on frame segmentation. Second, we propose a deep metric model to map the sequential stack trace representations into feature vectors whose similarity can represent the similarity of crashes. Third, a clustering algorithm is used to rapidly group similar feature vectors into same buckets to get the final result. Additionally, we evaluate our approach with the other seven competing methods on both private and public data sets. The results reveal that our method can speed up clustering and maintain high competitive precision.},
booktitle = {Proceedings of the 6th International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {29–34},
numpages = {6},
keywords = {Crash Report Bucketing, Crash Reports, Crash Stack, Deep Learning, Duplicate Bug Report, Stack Trace},
location = {Singapore, Singapore},
series = {MaLTeSQuE 2022}
}

@article{10.1145/3641541,
author = {Ollando, Rapha\"{e}l and Shin, Seung Yeob and Briand, Lionel C.},
title = {Learning Failure-Inducing Models for Testing Software-Defined Networks},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3641541},
doi = {10.1145/3641541},
abstract = {Software-defined networks (SDN) enable flexible and effective communication systems that are managed by centralized software controllers. However, such a controller can undermine the underlying communication network of an SDN-based system and thus must be carefully tested. When an SDN-based system fails, in order to address such a failure, engineers need to precisely understand the conditions under which it occurs. In this article, we introduce a machine learning-guided fuzzing method, named FuzzSDN, aiming at both (1)&nbsp;generating effective test data leading to failures in SDN-based systems and (2)&nbsp;learning accurate failure-inducing models that characterize conditions under which such system fails. To our knowledge, no existing work simultaneously addresses these two objectives for SDNs. We evaluate FuzzSDN by applying it to systems controlled by two open-source SDN controllers. Furthermore, we compare FuzzSDN with two state-of-the-art methods for fuzzing SDNs and two baselines for learning failure-inducing models. Our results show that (1)&nbsp;compared to the state-of-the-art methods, FuzzSDN generates at least 12 times more failures, within the same time budget, with a controller that is fairly robust to fuzzing and (2)&nbsp;our failure-inducing models have, on average, a precision of 98% and a recall of 86%, significantly outperforming the baselines.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {113},
numpages = {25},
keywords = {Software-defined networks, software testing, fuzzing, machine learning}
}

@inproceedings{10.1145/3665314.3670837,
author = {Shukla, Sanket and Pudukotai Dinakarrao, Sai Manoj},
title = {Energy Harvesting-Supported Efficient Low-Power ML Processing with Adaptive Checkpointing and Intermittent Computing},
year = {2024},
isbn = {9798400706882},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665314.3670837},
doi = {10.1145/3665314.3670837},
abstract = {The rise of ultra-low-power embedded processors has led to increased use of energy harvesting devices (EHDs), providing portability and extended lifespans, but also presenting challenges due to sporadic ambient energy and limited storage. This paper introduces "Micro-Controller Unit - Early Exit Neural Network" MCU-EENet, a framework utilizing kinetic energy harvesting to support machine learning tasks intermittently with early exits. The intermittent nature of ambient energy can lead to potential program interruptions, necessitating efficient state retention techniques within MCU-EENet. We propose "SmartCheck," a memory-optimized runtime checkpointing technique integrated with MCU-EENet to manage and utilize harvested energy efficiently. Through extensive experimentation, our framework demonstrates significant energy savings and performance enhancements, making it a promising solution for energy-constrained environments. Experimental results show a ~ 30% to 50% reduction in energy footprint and a 1.2X to 2.4X speed improvement over existing checkpointing methods.},
booktitle = {Proceedings of the 29th ACM/IEEE International Symposium on Low Power Electronics and Design},
pages = {1–6},
numpages = {6},
keywords = {machine learning, energy harvesting, intermittent computing},
location = {Newport Beach, CA, USA},
series = {ISLPED '24}
}

@article{10.5555/3722577.3722672,
author = {Fern\'{a}ndez, Tamara and Rivera, Nicol\'{a}s},
title = {A general framework for the analysis of kernel-based tests},
year = {2024},
issue_date = {January 2024},
publisher = {JMLR.org},
volume = {25},
number = {1},
issn = {1532-4435},
abstract = {Kernel-based tests provide a simple yet effective framework that uses the theory of reproducing kernel Hilbert spaces to design non-parametric testing procedures. In this paper, we propose new theoretical tools that can be used to study the asymptotic behaviour of kernel-based tests in various data scenarios and in different testing problems. Unlike current approaches, our methods avoid working with U and V-statistics expansions that usually lead to lengthy and tedious computations and asymptotic approximations. Instead, we work directly with random functionals on the Hilbert space to analyse kernel-based tests. By harnessing the use of random functionals, our framework leads to much cleaner analyses, involving less tedious computations. Additionally, it offers the advantage of accommodating pre-existing knowledge regarding test-statistics as many of the random functionals considered in applications are known statistics that have been studied comprehensively. To demonstrate the efficacy of our approach, we thoroughly examine two categories of kernel tests, along with three specific examples of kernel tests, including a novel kernel test for conditional independence testing.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {95},
numpages = {40},
keywords = {kernel methods, hypothesis testing, reproducing kernel Hilbert space}
}

@inproceedings{10.1145/3696443.3708959,
author = {Wang, Huanting and Lenihan, Patrick and Wang, Zheng},
title = {Enhancing Deployment-Time Predictive Model Robustness for Code Analysis and Optimization},
year = {2025},
isbn = {9798400712753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696443.3708959},
doi = {10.1145/3696443.3708959},
abstract = {Supervised machine learning techniques have shown promising results in code analysis and optimization problems. However, a learning-based solution can be brittle because minor changes in hardware or application workloads – such as facing a new CPU architecture or code pattern – may jeopardize decision accuracy, ultimately undermining model robustness. We introduce Prom, an open-source library to enhance the robustness and performance of predictive models against such changes during deployment. Prom achieves this by using statistical assessments to identify test samples prone to mispredictions and using feedback on these samples to improve a deployed model. We showcase Prom by applying it to 13 representative machine learning models across 5 code analysis and optimization tasks. Our extensive evaluation demonstrates that Prom can successfully identify an average of 96% (up to 100%) of mispredictions. By relabeling up to 5% of the Prom-identified samples through incremental learning, Prom can help a deployed model achieve a performance comparable to that attained during its model training phase.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {31–46},
numpages = {16},
keywords = {Machine learning, Model reliability, Statistical assessment},
location = {Las Vegas, NV, USA},
series = {CGO '25}
}

@inproceedings{10.1145/3624032.3624038,
author = {Silveira, Beatriz and Durelli, Vinicius and Santos, Sebasti\~{a}o and Durelli, Rafael and Delamaro, Marcio and Souza, Simone},
title = {Test Data Selection Based on Applying Mutation Testing to Decision Tree Models},
year = {2023},
isbn = {9798400716294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624032.3624038},
doi = {10.1145/3624032.3624038},
abstract = {Software testing is crucial to ensure software quality, verifying that it behaves as expected. This activity plays a crucial role in identifying defects from the early stages of the development process. Software testing is especially essential in complex or critical systems, such as those using Machine Learning (ML) techniques, since the models can present uncertainties and errors that affect their reliability. This work investigates the use of mutation testing to support the validation of ML applications. Our approach involves applying mutation analysis to the decision tree structure. The resulting mutated trees are a reference for selecting a test dataset that can effectively identify incorrect classifications in machine learning models. Preliminary results suggest that the proposed approach can successfully improve the test data selection for ML applications.},
booktitle = {Proceedings of the 8th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {38–46},
numpages = {9},
keywords = {Decision Tree, Machine Learning, Mutation Testing, Software Testing},
location = {Campo Grande, MS, Brazil},
series = {SAST '23}
}

@inproceedings{10.1145/1101908.1101979,
author = {Denmat, Tristan and Ducass\'{e}, Mireille and Ridoux, Olivier},
title = {Data mining and cross-checking of execution traces: a re-interpretation of Jones, Harrold and Stasko test information},
year = {2005},
isbn = {1581139934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1101908.1101979},
doi = {10.1145/1101908.1101979},
abstract = {The current trend in debugging and testing is to cross-check information collected during several executions. Jones et al., for example, propose to use the instruction coverage of passing and failing runs in order to visualize suspicious statements. This seems promising but lacks a formal justification. In this paper, we show that the method of Jones et al. can be re-interpreted as a data mining procedure. More particularly, they define an indicator which characterizes association rules between data. With this formal framework we are able to explain intrinsic limitations of the above indicator.},
booktitle = {Proceedings of the 20th IEEE/ACM International Conference on Automated Software Engineering},
pages = {396–399},
numpages = {4},
location = {Long Beach, CA, USA},
series = {ASE '05}
}

@inproceedings{10.1145/3587716.3587793,
author = {Li, Zhiwei and Pan, Zhongliang},
title = {Research of the Automatic Testing Software on Boundary-scan Test},
year = {2023},
isbn = {9781450398411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587716.3587793},
doi = {10.1145/3587716.3587793},
abstract = {With the rapid development of electronic science and technology, very large scale integrated circuites(VLSI) is more and more used in various electronic products. Therefore, the circuit test of such products has become a hot topic of research. In this paper, the method of boundary-scan test is studied systematically, and the automatic testing software basing on USB interface is made, and the actual testing effect is good.},
booktitle = {Proceedings of the 2023 15th International Conference on Machine Learning and Computing},
pages = {464–468},
numpages = {5},
keywords = {Automatic Test, Boundary-Scan Test, Cluster Test, Completeness Test, Interconnection Test, Testing Software},
location = {Zhuhai, China},
series = {ICMLC '23}
}

@inproceedings{10.1145/3643656.3643897,
author = {Soha, Peter Attila and Vancsics, Bela and Gergely, Tam\'{a}s and Beszedes, Arpad},
title = {Flaky Tests in the AI Domain},
year = {2024},
isbn = {9798400705588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643656.3643897},
doi = {10.1145/3643656.3643897},
abstract = {In this position paper, we investigate how frequently is randomness the cause of flakiness in the traditional and in the AI-enabled software domains. Based on previous works, it seems that while in the general domain flakiness rarely stems from randomness, in the AI domain it is a frequent phenomenon. Thus, we urge a discussion about a classification scheme of flaky tests based on whether they are caused by the inherent randomness of the AI-enabled SUT or some other reason. This way, better identification, classification and proper handling of flakiness in such systems will be possible.},
booktitle = {Proceedings of the 1st International Workshop on Flaky Tests},
pages = {20–21},
numpages = {2},
keywords = {flaky test, artificial intelligence, machine learning, randomness},
location = {Lisbon, Portugal},
series = {FTW '24}
}

@inproceedings{10.1145/3638530.3648419,
author = {Smith-Miles, Kate and Mu\~{n}oz, Mario Andr\'{e}s and Kandanaarachchi, Sevvandi},
title = {Instance Space Analysis and Item Response Theory for Algorithm Testing},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638530.3648419},
doi = {10.1145/3638530.3648419},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1369–1390},
numpages = {22},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@inproceedings{10.1145/3318299.3318337,
author = {Zhang, Zongtang and Chen, Zhe and Dai, Weiguo and Cheng, Yusheng},
title = {An Over-sampling Method Based on Margin Theory},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318337},
doi = {10.1145/3318299.3318337},
abstract = {Imbalanced data widely exists in real life, while the traditional classification method usually takes accuracy as the classification criterion, which is not suitable for the classification of imbalanced data. Resampling is an important method to deal with imbalanced data classification. In this paper, a margin based random over-sampling (MRO) method is proposed, and then MROBoost algorithm is proposed by combining the AdaBoost algorithm. Experimental results on the UCI dataset show that the MROBoost algorithm is superior to AdaBoost for imbalanced data classification problem.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {506–510},
numpages = {5},
keywords = {AdaBoost, Machine learning, imbalanced data, over-sampling},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@inproceedings{10.1109/ICSE48619.2023.00100,
author = {Laudato, Gennaro and Scalabrino, Simone and Novielli, Nicole and Lanubile, Filippo and Oliveto, Rocco},
title = {Predicting Bugs by Monitoring Developers during Task Execution},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00100},
doi = {10.1109/ICSE48619.2023.00100},
abstract = {Knowing which parts of the source code will be defective can allow practitioners to better allocate testing resources. For this reason, many approaches have been proposed to achieve this goal. Most state-of-the-art predictive models rely on product and process metrics, i.e., they predict the defectiveness of a component by considering what developers did. However, there is still limited evidence of the benefits that can be achieved in this context by monitoring how developers complete a development task. In this paper, we present an empirical study in which we aim at understanding whether measuring human aspects on developers while they write code can help predict the introduction of defects. First, we introduce a new developer-based model which relies on behavioral, psychophysical, and control factors that can be measured during the execution of development tasks. Then, we run a controlled experiment involving 20 software developers to understand if our developer-based model is able to predict the introduction of bugs. Our results show that a developer-based model is able to achieve a similar accuracy compared to a state-of-the-art code-based model, i.e., a model that uses only features measured from the source code. We also observed that by combining the models it is possible to obtain the best results (~84% accuracy).},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1110–1122},
numpages = {13},
keywords = {bug prediction, human aspects of software engineering, biometric sensors, empirical software engineering},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.5555/3507788.3507847,
author = {Kontogiannis, Kostas and Grigoriou, Marios and Brealey, Chris and Giammaria, Alberto},
title = {Compliance by design: software analytics and AIOps},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {Large software systems encompass complex interactions among their components and are subjected to frequent maintenance activities applied in order to fix bugs, add new functionality, port to new platforms, or interoperate with other systems. An important aspect to consider, is how such maintenance activities can be achieved in a way that first minimizes the risk of failures, and second how these maintenance activities can be integrated in a continuous deployment (CD) / continuous integration (CI) DevOps process. One major aspect on achieving this objective is to identify and remediate early on, possible vulnerabilities which are manifested as violations of known published controls and policies. The workshop brought together researchers and practitioners to discuss technological advances in the areas of requirements modeling, systems modeling, logging, event processing, and system verification.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {294–295},
numpages = {2},
keywords = {code repositories, machine learning, software bug prediction, software metrics},
location = {Toronto, Canada},
series = {CASCON '21}
}

@inproceedings{10.1109/SC.2014.65,
author = {Chen, Zhengzhang and Son, Seung Woo and Hendrix, William and Agrawal, Ankit and Liao, Wei-keng and Choudhary, Alok},
title = {NUMARCK: machine learning algorithm for resiliency and checkpointing},
year = {2014},
isbn = {9781479955008},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC.2014.65},
doi = {10.1109/SC.2014.65},
abstract = {Data checkpointing is an important fault tolerance technique in High Performance Computing (HPC) systems. As the HPC systems move towards exascale, the storage space and time costs of checkpointing threaten to overwhelm not only the simulation but also the post-simulation data analysis. One common practice to address this problem is to apply compression algorithms to reduce the data size. However, traditional lossless compression techniques that look for repeated patterns are ineffective for scientific data in which high-precision data is used and hence common patterns are rare to find. This paper exploits the fact that in many scientific applications, the relative changes in data values from one simulation iteration to the next are not very significantly different from each other. Thus, capturing the distribution of relative changes in data instead of storing the data itself allows us to incorporate the temporal dimension of the data and learn the evolving distribution of the changes. We show that an order of magnitude data reduction becomes achievable within guaranteed user-defined error bounds for each data point.We propose NUMARCK, Northwestern University Machine learning Algorithm for Resiliency and ChecKpointing, that makes use of the emerging distributions of data changes between consecutive simulation iterations and encodes them into an indexing space that can be concisely represented. We evaluate NUMARCK using two production scientific simulations, FLASH and CMIP5, and demonstrate a superior performance in terms of compression ratio and compression accuracy. More importantly, our algorithm allows users to specify the maximum tolerable error on a per point basis, while compressing the data by an order of magnitude.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
pages = {733–744},
numpages = {12},
location = {New Orleans, Louisana},
series = {SC '14}
}

@inproceedings{10.1145/2979779.2979783,
author = {Maheshwari, Suchi and Agarwal, Sonali},
title = {Three-way decision based Defect Prediction for Object Oriented Software},
year = {2016},
isbn = {9781450342131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2979779.2979783},
doi = {10.1145/2979779.2979783},
abstract = {Early prediction of defective software module plays critical role in the software project development to reduce the overall development time, budgets and increases the customer satisfaction. The bug prediction based on two-way classification method classifies the software module as defective or non-defective. This method provides good accuracy measure but this metric is not sufficient in case if misclassification cost is concerned. Classifying the defective module as non-defective will lead to higher cost of entire software project at the end. In this study, three-way decision based classification method and Random Forest ensemble are used to predict the defect in Object Oriented Software to reduce the misclassification cost which will lead to avoid the cost overrun. The eclipse bug prediction dataset is used and experimental results show that the decision cost is reduced and accuracy is increased using our proposed method.},
booktitle = {Proceedings of the International Conference on Advances in Information Communication Technology &amp; Computing},
articleno = {4},
numpages = {6},
keywords = {Eclipse Bug Prediction dataset, Na\"{\i}ve Bayes, Random Forest, Software defect prediction, Three-way decision},
location = {Bikaner, India},
series = {AICTC '16}
}

@inproceedings{10.1109/ASE56229.2023.00169,
author = {Xue, Zhipeng and Gao, Zhipeng and Hu, Xing and Li, Shanping},
title = {ACWRecommender: A Tool for Validating Actionable Warnings with Weak Supervision},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00169},
doi = {10.1109/ASE56229.2023.00169},
abstract = {Static analysis tools have gained popularity among developers for finding potential bugs, but their widespread adoption is hindered by the accomnpanying high false alarm rates (up to 90%). To address this challenge, previous studies proposed the concept of actionable warnings, and apply machine-learning methods to distinguish actionable warnings from false alarms. Despite these efforts, our preliminary study suggests that the current methods used to collect actionable warnings are rather shaky and unreliable, resulting in a large proportion of invalid actionable warnings. In this work, we mined 68,274 reversions from Top-500 Github C repositories to create a substantia actionable warning dataset and assigned weak labels to each warning's likelihood of being a real bug. To automatically identify actionable warnings and recommend those with a high probability of being real bugs (AWHB), we propose a two-stage framework called ACWRecommender. In the first stage, our tool use a pre-trained model, i.e., UniXcoder, to identify actionable warnings from a huge number of SA tool's reported warnings. In the second stage, we rerank valid actionable warnings to the top by using weakly supervised learning. Experimental results showed that our tool outperformed several baselines for actionable warning detection (in terms of F1-score) and performed better for AWHB recommendation (in terms of nDCG and MRR). Additionaly, we also performed an in-the-wild evaluation, we manually validated 24 warnings out of 2,197 reported warnings on 10 randomly selected projects, 22 of which were confirmed by developers as real bugs, demonstrating the practical usage of our tool.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1876–1880},
numpages = {5},
keywords = {actionable warning recommendation, static analysis, weak supervision, data mining},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3629479.3629514,
author = {Silva, Sara and De Fran\c{c}a, Breno Bernard Nicolau},
title = {A Case Study on Data Science Processes in an Academia-Industry Collaboration},
year = {2023},
isbn = {9798400707865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629479.3629514},
doi = {10.1145/3629479.3629514},
abstract = {Data Science (DS) is emerging in major software development projects and often needs to follow software development practices. Therefore, DS processes will likely continue to attract Software Engineering (SE) practices and vice-versa. This case study aims to map and describe a software development process for Machine Learning(ML)-enabled applications and associated practices used in a real DS project at the Recod.ai laboratory in collaboration with an industrial partner. The focus was to analyze the process and identify the strengths and primary challenges, considering their expertise in robust ML practices and how they can contribute to general software quality. To achieve this, we conducted semi-structured interviews and analyzed them using procedures from the Straussian Grounded Theory. The results showed that the DS development process is iterative, with feedback between activities, which differs from the processes in the literature. Additionally, this process presents a greater involvement of domain experts. Besides, the team prioritizes software quality characteristics (attributes) in these DS projects to ensure some aspects of the final product’s quality, i.e., functional correctness and robustness. To achieve those, they use regular accuracy metrics and include explainability and data leakage as quality metrics during training. Finally, the software engineer’s role and its responsibilities differ from those of a traditional industry software engineer, as s/he is involved in most of the process steps. These characteristics can contribute to high-quality models achieving the partner needs and, consequently, relevant contributions to the intersection between SE and DS.},
booktitle = {Proceedings of the XXII Brazilian Symposium on Software Quality},
pages = {1–10},
numpages = {10},
keywords = {case study, data science, machine learning, software processes},
location = {Bras\'{\i}lia, Brazil},
series = {SBQS '23}
}

@article{10.5555/3648699.3648775,
author = {Bo, Di and Hwangbo, Hoon and Sharma, Vinit and Arndt, Corey and TerMaath, Stephanie},
title = {A randomized subspace-based approach for dimensionality reduction and important variable selection},
year = {2023},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {An analysis of high-dimensional data can offer a detailed description of a system but is often challenged by the curse of dimensionality. General dimensionality reduction techniques can alleviate such difficulty by extracting a few important features, but they are limited due to the lack of interpretability and connectivity to actual decision making associated with each physical variable. Variable selection techniques, as an alternative, can maintain the interpretability, but they often involve a greedy search that is susceptible to failure in capturing important interactions or a metaheuristic search that requires extensive computations. This research proposes a novel method that identifies critical subspaces, reduceddimensional physical spaces, to achieve dimensionality reduction and variable selection. We apply a randomized search for subspace exploration and leverage ensemble techniques to enhance model performance. When applied to high-dimensional data collected from the failure prediction of a composite/metal hybrid structure exhibiting complex progressive damage failure under loading, the proposed method outperforms the existing and potential alternatives in prediction and important variable selection.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {76},
numpages = {31},
keywords = {subspace-based modeling, randomized algorithms, feature selection, hybrid material analysis, damage tolerance modeling}
}

@inproceedings{10.1145/3177457.3191709,
author = {Ren, Yidan and Zhu, Zhengzhou and Chen, Xiangzhou and Ding, Huixia and Zhang, Geng},
title = {Research on Defect Detection Technology of Trusted Behavior Decision Tree Based on Intelligent Data Semantic Analysis of Massive Data},
year = {2018},
isbn = {9781450363396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3177457.3191709},
doi = {10.1145/3177457.3191709},
abstract = {With the rapid development of information technology, software systems' scales and complexity are showing a trend of expansion. The users' needs for the software security, software security reliability and software stability are growing increasingly. At present, the industry has applied machine learning methods to the fields of defect detection to repair and improve software defects through the massive data intelligent semantic analysis or code scanning. The model in machine learning is faced with big difficulty of model building, understanding, and the poor visualization in the field of traditional software defect detection. In view of the above problems, we present a point of view that intelligent semantic analysis technology based on massive data, and using the trusted behavior decision tree model to analyze the soft behavior by layered detection technology. At the same time, it is equipped related test environment to compare the tested software. The result shows that the defect detection technology based on intelligent semantic analysis of massive data is superior to other techniques at the cost of building time and error reported ratio.},
booktitle = {Proceedings of the 10th International Conference on Computer Modeling and Simulation},
pages = {168–175},
numpages = {8},
keywords = {software defect detection, intelligent semantic analysis, decision tree, Massive data},
location = {Sydney, Australia},
series = {ICCMS '18}
}

@inproceedings{10.1145/3534678.3539099,
author = {Lindon, Michael and Sanden, Chris and Shirikian, Vach\'{e}},
title = {Rapid Regression Detection in Software Deployments through Sequential Testing},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539099},
doi = {10.1145/3534678.3539099},
abstract = {The practice of continuous deployment has enabled companies to reduce time-to-market by increasing the rate at which software can be deployed. However, deploying more frequently bears the risk that occasionally defective changes are released. For Internet companies, this has the potential to degrade the user experience and increase user abandonment. Therefore, quality control gates are an important component of the software delivery process. These are used to build confidence in the reliability of a release or change. Towards this end, a common approach is to perform a canary test to evaluate new software under production workloads. Detecting defects as early as possible is necessary to reduce exposure and to provide immediate feedback to the developer.We present a statistical framework for rapidly detecting regressions in software deployments. Our approach is based on sequential tests of stochastic order and of equality in distribution. This enables canary tests to be continuously monitored, permitting regressions to be rapidly detected while strictly controlling the false detection probability throughout. The utility of this approach is demonstrated based on two case studies at Netflix.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3336–3346},
numpages = {11},
keywords = {A/B testing, anytime-valid inference, canary release, canary testing, confidence sequences, experimentation, regression detection, sequential testing, software delivery},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3551349.3559546,
author = {Mukhtar, Adil and Hofer, Birgit and Jannach, Dietmar and Wotawa, Franz and Schekotihin, Konstantin},
title = {Boosting Spectrum-Based Fault Localization for Spreadsheets with Product Metrics in a Learning Approach},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3559546},
doi = {10.1145/3551349.3559546},
abstract = {Faults in spreadsheets are not uncommon and they can have significant negative consequences in practice. Various approaches for fault localization were proposed in recent years, among them techniques that transferred ideas from spectrum-based fault localization (SFL) to the spreadsheet domain. Applying SFL to spreadsheets proved to be effective, but has certain limitations. Specifically, the constrained computational structures of spreadsheets may lead to large sets of cells that have the same assumed fault probability according to SFL and thus have to be inspected manually. In this work, we propose to combine SFL with a fault prediction method based on spreadsheet metrics in a machine learning (ML) approach. In particular, we train supervised ML models using two orthogonal types of features: (i) variables that are used to compute similarity coefficients in SFL and (ii) spreadsheet metrics that have shown to be good predictors for faulty formulas in previous work. Experiments with a widely-used corpus of faulty spreadsheets indicate that the combined model helps to significantly improve fault localization performance in terms of wasted effort and accuracy.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {175},
numpages = {5},
keywords = {Spreadsheets, Spectrum-based Fault Localization, Machine Learning, Artificial Intelligence},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3650212.3680315,
author = {Yang, Xiaoyi and Wang, Yuxing and Rafi, Tahmid and Liu, Dongfang and Wang, Xiaoyin and Zhang, Xueling},
title = {Towards Automatic Oracle Prediction for AR Testing: Assessing Virtual Object Placement Quality under Real-World Scenes},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680315},
doi = {10.1145/3650212.3680315},
abstract = {Augmented Reality (AR) technology opens up exciting possibilities in various fields, such as education, work guidance, shopping, communication, and gaming. However, users often encounter usability and user experience issues in current AR apps, often due to the imprecise placement of virtual objects. Detecting these inaccuracies is crucial for AR app testing, but automating the process is challenging due to its reliance on human perception and validation. This paper introduces VOPA (Virtual Object Placement Assessment), a novel approach that automatically identifies imprecise virtual object placements in real-world AR apps. VOPA involves instrumenting real-world AR apps to collect screenshots representing various object placement scenarios and their corresponding metadata under real-world scenes. The collected data are then labeled through crowdsourcing and used to train a hybrid neural network that identifies object placement errors. VOPA aims to enhance AR app testing by automating the assessment of virtual object placement quality and detecting imprecise instances. In our evaluation of a test set of 304 screenshots, VOPA achieved an accuracy of 99.34%, precision of 96.92% and recall of 100%. Furthermore, VOPA successfully identified 38 real-world object placement errors, including instances where objects were hovering between two surfaces or appearing embedded in the wall.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {717–729},
numpages = {13},
keywords = {Augmented Reality, Automated Test Oracle, Machine Learning},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3218603.3218624,
author = {Zhang, Sheng and Tang, Adrian and Jiang, Zhewei and Sethumadhavan, Simha and Seok, Mingoo},
title = {Blacklist Core: Machine-Learning Based Dynamic Operating-Performance-Point Blacklisting for Mitigating Power-Management Security Attacks},
year = {2018},
isbn = {9781450357043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3218603.3218624},
doi = {10.1145/3218603.3218624},
abstract = {Most modern computing devices make available fine-grained control of operating frequency and voltage for power management. These interfaces, as demonstrated by recent attacks, open up a new class of software fault injection attacks that compromise security on commodity devices. CLKSCREW, a recently-published attack that stretches the frequency of devices beyond their operational limits to induce faults, is one such attack. Statically and permanently limiting frequency and voltage modulation space, i.e., guard-banding, could mitigate such attacks but it incurs large performance degradation and long testing time. Instead, in this paper, we propose a run-time technique which dynamically blacklists unsafe operating performance points using a neural-net model. The model is first trained offline in the design time and then subsequently adjusted at run-time by inspecting a selected set of features such as power management control registers, timing-error signals, and core temperature. We designed the algorithm and hardware, titled a BlackList (BL) core, which is capable of detecting and mitigating such power management-based security attack at high accuracy. The BL core incurs a reasonably small amount of overhead in power, delay, and area.},
booktitle = {Proceedings of the International Symposium on Low Power Electronics and Design},
articleno = {5},
numpages = {6},
keywords = {power management, operating performance point, blacklist, Security},
location = {Seattle, WA, USA},
series = {ISLPED '18}
}

@inproceedings{10.1145/3266237.3266271,
author = {Ara\'{u}jo, Cristiano Werner and Zapalowski, Vanius and Nunes, Ingrid},
title = {Using code quality features to predict bugs in procedural software systems},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266271},
doi = {10.1145/3266237.3266271},
abstract = {A wide range of metrics have been used as features to build bug (or fault) predictors. However, most of the existing predictors focus mostly on object-oriented (OO) systems, either because they rely on OO metrics or were evaluated mainly with OO systems. Procedural software systems (PSS), less addressed in bug prediction research, often suffer from maintainability problems because they typically consist of low-level applications, using for example preprocessors to cope with variability. Previous work evaluated sets of features (composed of static code metrics) proposed in existing approaches in the PSS context. However, explored metrics are limited to those that are part of traditional metric suites, being often associated with structural code properties. A type of information explored to a smaller extent in this context is the output of code quality tools that statically analyse source code, providing hints of code problems. In this paper, we investigate the use of information collected from quality tools to build bug predictors dedicated to PSS. We specify four features derived from code quality tools or associated with poor programming practices and evaluate the effectiveness of these features. Our evaluation shows that our proposed features improve bug predictors in our investigated context.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {122–131},
numpages = {10},
keywords = {procedural languanges, code metrics, bug prediction},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@inproceedings{10.1145/3615366.3615423,
author = {Campos, Jo\~{a}o R. and Machado, Rodrigo and Vieira, Marco},
title = {Leveraging Time Series Autocorrelation Through Numerical Differentiation for Improving Failure Prediction},
year = {2023},
isbn = {9798400708442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615366.3615423},
doi = {10.1145/3615366.3615423},
abstract = {Given the complexity of modern software systems, it is no longer possible to detect every fault before deployment. Such faults can eventually lead to failures at runtime, compromising the business process and causing significant risk or losses. Online Failure Prediction (OFP) is a complementary fault-tolerance technique that tries to predict failures in the near future, by using past data and the current state of the system. However, modern systems are comprised of many components and thus a proper characterization of its state requires hundreds of system metrics. As the system evolves through time, these data can be seen as multivariate time series, where the value of a system metric at a given time is related to its previous value. Although various techniques exist for leveraging this autocorrelation, they are often either simplistic (e.g., sliding-window), or too complex (e.g., Long-Short Term Memory (LSTM)). In this paper we propose the use of numerical differentiation, computing the first and second derivative, as a means to extract information concerning the underlying function of each system metric to support the development of predictive models for OFP. We conduct a comprehensive case using a Linux failure dataset that was generated through fault injection. Results suggest that numerical differentiation can be a promising approach to improve the performance of Machine Learning (ML) models for dependability-related problems with similar sequential characteristics (e.g., intrusion detection).},
booktitle = {Proceedings of the 12th Latin-American Symposium on Dependable and Secure Computing},
pages = {70–79},
numpages = {10},
keywords = {Online Failure Prediction, Numerical Differentiation, Machine Learning, Dependability},
location = {La Paz, Bolivia},
series = {LADC '23}
}

@article{10.1145/3702992,
author = {Zhang, Xiaoyu and Zhai, Juan and Ma, Shiqing and Guan, Xiaohong and Shen, Chao},
title = {DREAM: Debugging and Repairing AutoML Pipelines},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3702992},
doi = {10.1145/3702992},
abstract = {Deep Learning models have become an integrated component of modern software systems. In response to the challenge of model design, researchers proposed Automated Machine Learning (AutoML) systems, which automatically search for model architecture and hyperparameters for a given task. Like other software systems, existing AutoML systems have shortcomings in their design. We identify two common and severe shortcomings in AutoML, performance issue (i.e., searching for the desired model takes an unreasonably long time) and ineffective search issue (i.e., AutoML systems are not able to find an accurate enough model). After analyzing the workflow of AutoML, we observe that existing AutoML systems overlook potential opportunities in search space, search method, and search feedback, which results in performance and ineffective search issues. Based on our analysis, we design and implement DREAM, an automatic and general-purpose tool to alleviate and repair the shortcomings of AutoML pipelines and conduct effective model searches for diverse tasks. It monitors the process of AutoML to collect detailed feedback and automatically repairs shortcomings by expanding search space and leveraging a feedback-driven search strategy. Our evaluation results show that DREAM can be applied on two state-of-the-art AutoML pipelines and effectively and efficiently repair their shortcomings.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
keywords = {Automated Machine Learning, Software Testing and Debugging, AutoML Systems, DL Model Testing and Repair}
}

@article{10.1145/3664812,
author = {Feng, Xiaoning and Han, Xiaohong and Chen, Simin and Yang, Wei},
title = {LLMEffiChecker: Understanding and Testing Efficiency Degradation of Large Language Models},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3664812},
doi = {10.1145/3664812},
abstract = {Large Language Models (LLMs) have received much recent attention due to their human-level accuracy. While existing works mostly focus on either improving accuracy or testing accuracy robustness, the computation efficiency of LLMs, which is of paramount importance due to often vast generation demands and real-time requirements, has surprisingly received little attention. In this article, we make the first attempt to understand and test potential computation efficiency robustness in state-of-the-art LLMs. By analyzing the working mechanism and implementation of 20,543 public-accessible LLMs, we observe a fundamental property in LLMs that could be manipulated in an adversarial manner to reduce computation efficiency significantly. Our interesting observation is that the output length determines the computation efficiency of LLMs instead of the input, where the output length depends on two factors: an often sufficiently large yet pessimistic pre-configured threshold controlling the max number of iterations and a runtime-generated end of sentence (EOS) token. Our key motivation is to generate test inputs that could sufficiently delay the generation of EOS such that LLMs would have to go through enough iterations to satisfy the pre-configured threshold. We present LLMEffiChecker, which can work under both white-box setting and black-box setting. In the white-box scenario, LLMEffiChecker develops a gradient-guided technique that searches for a minimal and unnoticeable perturbation at character-level, token-level, and structure-level. In the black-box scenario, LLMEffiChecker employs a causal inference-based approach to find critical tokens and similarly applies three levels of imperceptible perturbation to them. Both the white-box and black-box settings effectively delay the appearance of EOS, compelling these inputs to reach the naturally unreachable threshold. To demonstrate the effectiveness of LLMEffiChecker, we conduct a systematic evaluation on nine publicly available LLMs: Google T5, AllenAI WMT14, Helsinki-NLP translator, Facebook FairSeq, UNICAMP-DL translator, MarianMT, Google FLAN-T5, MBZUAI LaMini-GPT, and Salesforce CodeGen. Experimental results show that LLMEffiChecker can increase on average LLMs’ response latency and energy consumption by 325% to 3,244% and 344% to 3,616%, respectively, by perturbing just one character or token in the input sentence. Our case study shows that inputs generated by LLMEffiChecker significantly affect the battery power in real-world mobile devices (i.e., drain more than 30 times battery power than normal inputs).},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {186},
numpages = {38},
keywords = {Machine learning, software testing, large language model}
}

@inproceedings{10.5555/3432601.3432636,
author = {Kontogiannis, Kostas and Giammaria, Alberto and Brealey, Chris and Grigoriou, Marios},
title = {1st workshop on AIOps and systems compliance},
year = {2020},
publisher = {IBM Corp.},
address = {USA},
abstract = {Over the past two years we have seen a growth on AIOps. AIOps is the area of software engineering which aims to apply Artificial Intelligence on IT operations and on the big data which are harvested from different IT systems across an organisation. AIOps comes as a direct response to the dire need to break the siloed approach to IT operations in order to support a holistic system analysis which allows not only for the identification and proactive or even predictive resolution of issues, but also for assessing the compliance of a system against certain policies such as policies related to safety, security, and regulatory. This workshop brought together researchers and practitioners to discuss the latest developments on AIOps and system compliance and identified key challenge issues.},
booktitle = {Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering},
pages = {254–255},
numpages = {2},
keywords = {software metrics, software bug prediction, machine learning, code repositories},
location = {Toronto, Ontario, Canada},
series = {CASCON '20}
}

@inproceedings{10.1145/3649329.3655688,
author = {Roquet, Lucas and Fernandes dos Santos, Fernando and Rech, Paolo and Traiola, Marcello and Sentieys, Olivier and Kritikakou, Angeliki},
title = {Cross-Layer Reliability Evaluation and Efficient Hardening of Large Vision Transformers Models},
year = {2024},
isbn = {9798400706011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649329.3655688},
doi = {10.1145/3649329.3655688},
abstract = {Vision Transformers (ViTs) are highly accurate Machine Learning (ML) models. However, their large size and complexity increase the expected error rate due to hardware faults. Measuring the error rate of large ViT models is challenging, as conventional microarchitectural fault simulations can take years to produce statistically significant data. This paper proposes a two-level evaluation based on data collected through more than 70 hours of neutron beam experiments and more than 600 hours of software fault simulation. We consider 12 ViT models executed in 2 NVIDIA GPU architectures. We first characterize the fault model in ViT's kernels to identify the faults more likely to propagate to the output. We then design dedicated procedures efficiently integrated into the ViT to locate and correct these faults. We propose Maximum corrupted Malicious values (MaxiMals), an experimentally tuned low-cost mitigation solution to reduce the impact of transient faults on ViTs. We demonstrate that MaxiMals can correct 90.7% of critical failures, with execution time overheads as low as 5.61%.},
booktitle = {Proceedings of the 61st ACM/IEEE Design Automation Conference},
articleno = {291},
numpages = {6},
location = {San Francisco, CA, USA},
series = {DAC '24}
}

@article{10.1145/3712190,
author = {Fu, Michael and Pasuksmit, Jirat and Tantithamthavorn, Chakkrit},
title = {AI for DevSecOps: A Landscape and Future Opportunities},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3712190},
doi = {10.1145/3712190},
abstract = {DevOps has emerged as one of the most rapidly evolving software development paradigms. With the growing concerns surrounding security in software systems, the DevSecOps paradigm has gained prominence, urging practitioners to incorporate security practices seamlessly into the DevOps workflow. However, integrating security into the DevOps workflow can impact agility and impede delivery speed. Recently, the advancement of artificial intelligence (AI) has revolutionized automation in various software domains, including software security. AI-driven security approaches, particularly those leveraging machine learning or deep learning, hold promise in automating security workflows. They have the potential to reduce manual efforts and can be incorporated into DevOps practices to support consistent delivery speed while aligning with the principles of the DevSecOps paradigm. This paper seeks to contribute to the critical intersection of AI and DevSecOps by presenting a comprehensive landscape of AI-driven security techniques applicable to DevOps and identifying avenues for enhancing security, trust, and efficiency in software development processes. We analyzed 99 research papers spanning from 2017 to 2023. Specifically, we address two key research questions (RQs). In RQ1, we identified 12 security tasks associated with the DevSecOps process and reviewed existing AI-driven security approaches, the problems they addressed, and the 65 benchmarks used to evaluate those approaches. Drawing insights from our findings, in RQ2, we discussed state-of-the-art AI-driven security approaches, highlighted 15 challenges in existing research, and proposed 15 corresponding avenues for future opportunities.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {DevOps, DevSecOps, Artificial Intelligence, Deep Learning, Machine Learning, AI Security, Vulnerability, Supply Chain Security}
}

@inproceedings{10.1145/3611643.3616320,
author = {Vegas, Sira and Elbaum, Sebastian},
title = {Pitfalls in Experiments with DNN4SE: An Analysis of the State of the Practice},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616320},
doi = {10.1145/3611643.3616320},
abstract = {Software engineering (SE) techniques are increasingly relying on deep learning approaches to support many SE tasks, from bug triaging to code generation. To assess the efficacy of such techniques researchers typically perform controlled experiments. Conducting these experiments, however, is particularly challenging given the complexity of the space of variables involved, from specialized and intricate architectures and algorithms to a large number of training hyper-parameters and choices of evolving datasets, all compounded by how rapidly the machine learning technology is advancing, and the inherent sources of randomness in the training process. In this work we conduct a mapping study, examining 194 experiments with techniques that rely on deep neural networks (DNNs) appearing in 55 papers published in premier SE venues to provide a characterization of the state of the practice, pinpointing experiments’ common trends and pitfalls. Our study reveals that most of the experiments, including those that have received ACM artifact badges, have fundamental limitations that raise doubts about the reliability of their findings. More specifically, we find: 1) weak analyses to determine that there is a true relationship between independent and dependent variables (87% of the experiments), 2) limited control over the space of DNN relevant variables, which can render a relationship between dependent variables and treatments that may not be causal but rather correlational (100% of the experiments), and 3) lack of specificity in terms of what are the DNN variables and their values utilized in the experiments (86% of the experiments) to define the treatments being applied, which makes it unclear whether the techniques designed are the ones being assessed, or how the sources of extraneous variation are controlled. We provide some practical recommendations to address these limitations.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {528–540},
numpages = {13},
keywords = {deep learning, machine learning for software engineering, software engineering experimentation},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3243127.3243129,
author = {Khosrowjerdi, Hojat and Meinke, Karl},
title = {Learning-based testing for autonomous systems using spatial and temporal requirements},
year = {2018},
isbn = {9781450359726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243127.3243129},
doi = {10.1145/3243127.3243129},
abstract = {Cooperating cyber-physical systems-of-systems (CO-CPS) such as vehicle platoons, robot teams or drone swarms usually have strict safety requirements on both spatial and temporal behavior. Learning-based testing is a combination of machine learning and model checking that has been successfully used for black-box requirements testing of cyber-physical systems-of-systems. We present an overview of research in progress to apply learning-based testing to evaluate spatio-temporal requirements on autonomous systems-of-systems through modeling and simulation.},
booktitle = {Proceedings of the 1st International Workshop on Machine Learning and Software Engineering in Symbiosis},
pages = {6–15},
numpages = {10},
keywords = {spatio-temporal logic, requirements testing, model-based testing, machine learning, learning-based testing, black-box testing, Automotive software},
location = {Montpellier, France},
series = {MASES 2018}
}

@inproceedings{10.1145/3629479.3629512,
author = {Pereira Cesar, Durval and Barbosa Vieira da Silva, Caio and Pedrosa Leite, Leonardo and Fonseca, Baldoino and De Medeiros Baia, Davy},
title = {Is your code harmful too? Understanding harmful code through transfer learning},
year = {2023},
isbn = {9798400707865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629479.3629512},
doi = {10.1145/3629479.3629512},
abstract = {Code smells are indicators of poor design implementation and decision-making that can potentially harm the quality of software. Therefore, detecting these smells is crucial to prevent such issues. Some studies aim to comprehend the impact of code smells on software quality, while others propose rules or machine learning-based approaches to identify code smells. Previous research has focused on labeling and analyzing code snippets that significantly impair software quality using machine learning techniques. These snippets are classified as Clean, Smelly, Buggy, and Harmful Code. Harmful Code refers to Smelly code segments that have one or more reported bugs, whether fixed or not. Consequently, the presence of a Harmful Code increases the risk of introducing new defects and/or design issues during the remediation process. While generating useful results for harmful code detection, none of the prior work has considered, through the use of transfer learning, train a model to identify harmful snippets in one programming language and being able to identify similar harmfulness in another programming language. We perform our study on this scope with 5 smell types, 258,035 versions of 23 open-source projects, 8,181 bugs and 11,506 code smells. The findings revealed promising transferability of knowledge between Java and C# in the presence of various code smells types, while C++ exhibited more challenging transferability. Also, our study discovered that a sample size of 32 demonstrated favorable outcomes for most harmful codes, underscoring the efficiency of transfer learning even with limited data.},
booktitle = {Proceedings of the XXII Brazilian Symposium on Software Quality},
pages = {42–51},
numpages = {10},
keywords = {code smells, machine learning, software quality, transfer learning},
location = {Bras\'{\i}lia, Brazil},
series = {SBQS '23}
}

@article{10.1145/3698109,
author = {Bal, Pravas and Kumar, Sandeep},
title = {Cross Project Defect Prediction using Dropout Regularized Deep Learning and Unique Matched Metrics},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2158-656X},
url = {https://doi.org/10.1145/3698109},
doi = {10.1145/3698109},
abstract = {The primary goal of software defect prediction (SDP) is to predict the software defects for a specific software using historical data or data from past releases of software projects. The existing state of arts on SDP primarily discusses two prediction scenarios: Within Project Defect Prediction (WPDP) and Cross Project Defect Prediction (CPDP). The prediction model belongs to the WPDP scenario, which means that the model is trained and tested on different parts of the same dataset or trained on the dataset belonging to the previous version of the same project. While in the CPDP scenario, training and testing occur on different software project datasets. Due to the unavailability of historical datasets or prior releases of software defect datasets, CPDP is more useful in real-life scenarios. So, CPDP analysis is a very challenging issue in the SDP domain. Sometimes, machine learning (ML) models perform poorly due to inadequate training in the CPDP scenario. To support better CPDP performance, we must carefully build an ML model focusing on lower training error and overfitting issues. To address these issues, we have proposed a cross-project data preprocessing method to correlate the metrics of different project datasets, namely Unique Selection of Matched Metrics (USMM), using the KS test and Hungarian method. To further improve the CPDP performance, we have also used the dropout regularized deep learning (DRDL) model. We have deployed 34 software defect datasets to validate the DRDL model and USMM method. The experimental results demonstrate that the DRDL model using the USMM method (DRDL-USMM) is a promising model to enhance the prediction accuracy, and an improvement in the range of 3.3% to 8.5% as compared to the existing works in the CPDP scenario has been found.},
note = {Just Accepted},
journal = {ACM Trans. Manage. Inf. Syst.},
month = sep,
keywords = {Deep learning, cross project defect prediction, correlated matched metrics, dropout regularization.}
}

@inproceedings{10.1145/3548660.3561332,
author = {Cernau, Laura Diana and Dio\c{s}an, Laura Silvia and undefinederban, Camelia},
title = {A pedagogical approach in interleaving software quality concerns at an artificial intelligence course},
year = {2022},
isbn = {9781450394536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548660.3561332},
doi = {10.1145/3548660.3561332},
abstract = {The software engineering industry is an everchanging domain requiring professionals to have a good knowledge base and adaptability skills.Artificial Intelligence (AI) has achieved substantial success in enhancing program analysis techniques and applications, including bug prediction. It is a promising direction by applying advanced Machine Learning techniques into suitable software engineering tasks.  

The main goal of this paper is to propose a pedagogical interdisciplinary approach that pave the path for developing an e-learning platform serving to check the quality of the source code that students wrote by means of Artificial Intelligence techniques. By putting into practice this proposal, we are planning to show the students how to combine concepts learned from two different courses. The first step of this approach would be part of the Advanced Programming Methods, a Software Engineering related course, where students learn about the importance of writing good quality code and use software metrics as a mean of software quality assessment. Then, the following steps will be integrated into the Artificial Intelligence course, where students learn about different Machine Learning algorithms and how to apply them to solve practical problems. Thus, as an applicability in this respect, students use the metric values calculated for their projects developed at Advanced Programming Methods course as lab assignments and also to train (at Artificial Intelligence class) a bug detection model able to estimate the quality of new codebases.  

The proposed approach is helpful for both students and teachers. On one side, it helps the students understand the importance of writing clean, high-quality code. And on the other side, it helps teachers in their evaluation process by giving them time to focus on different aspects of homework than the code quality.},
booktitle = {Proceedings of the 4th International Workshop on Education through Advanced Software Engineering and Artificial Intelligence},
pages = {18–24},
numpages = {7},
keywords = {software metrics, software engineering, code quality},
location = {Singapore, Singapore},
series = {EASEAI 2022}
}

@inproceedings{10.1109/ASE.2011.6100070,
author = {Chen, Ning and Hoi, Steven C. H. and Xiao, Xiaokui},
title = {Software process evaluation: A machine learning approach},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100070},
doi = {10.1109/ASE.2011.6100070},
abstract = {Software process evaluation is essential to improve software development and the quality of software products in an organization. Conventional approaches based on manual qualitative evaluations (e.g., artifacts inspection) are deficient in the sense that (i) they are time-consuming, (ii) they suffer from the authority constraints, and (iii) they are often subjective. To overcome these limitations, this paper presents a novel semi-automated approach to software process evaluation using machine learning techniques. In particular, we formulate the problem as a sequence classification task, which is solved by applying machine learning algorithms. Based on the framework, we define a new quantitative indicator to objectively evaluate the quality and performance of a software process. To validate the efficacy of our approach, we apply it to evaluate the defect management process performed in four real industrial software projects. Our empirical results show that our approach is effective and promising in providing an objective and quantitative measurement for software process evaluation.},
booktitle = {Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {333–342},
numpages = {10},
series = {ASE '11}
}

@inproceedings{10.1145/3474624.3477070,
author = {Martins, Luana and Bezerra, Carla and Costa, Heitor and Machado, Ivan},
title = {Smart prediction for refactorings in the software test code},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3477070},
doi = {10.1145/3474624.3477070},
abstract = {Test smells are bad practices to either design or implement a test code. Their presence may reduce the test code quality, harming the software testing activities, primarily from a maintenance perspective. Therefore, defining strategies and tools to handle test smells and improve the test code quality is necessary. State-of-the-art strategies encompass automated support mainly based on hard thresholds of rules, static and dynamic metrics to identify the test smells. Such thresholds are subjective to interpretation and may not consider the complexity of the software projects. Moreover, they are limited as they do not automate test refactoring but only count on developers’ expertise and intuition. In this context, a technique that uses historical implicit or tacit data to generate knowledge could assist the identification and refactoring of test smells. This study aims to establish a novel approach based on machine learning techniques to suggest developers refactoring strategies for test smells. As an expected result, we could understand the applicability of the machine learning techniques to handle test smells and a framework proposal that helps developers in decision-making regarding the refactoring of test smells.},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {115–120},
numpages = {6},
keywords = {Test Smells, Software Quality, Machine Learning},
location = {Joinville, Brazil},
series = {SBES '21}
}

@article{10.1145/3688844,
author = {Pan, Weifeng and Wu, Wei and Ming, Hua and Kim, Dae-Kyoo and Yang, Zijiang and Ma, Yutao},
title = {Toward the Fractal Dimension of Classes},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3688844},
doi = {10.1145/3688844},
abstract = {The fractal property has been regarded as a fundamental property of complex networks, characterizing the self-similarity of a network. Such a property is usually numerically characterized by the fractal dimension metric, and it not only helps the understanding of the relationship between the structure and function of complex networks but also finds a wide range of applications in complex systems. The existing literature shows that class-level software networks (i.e., class dependency networks) are complex networks with the fractal property. However, the fractal property at the feature (i.e., methods and fields) level has never been investigated, although it is useful for measuring class complexity and predicting bugs in classes. Furthermore, existing studies on the fractal property of software systems were all performed on un-weighted software networks and have not been used in any practical quality assurance tasks such as bug prediction. Generally, considering the weights on edges can give us more accurate representations of the software structure and thus help us obtain more accurate results. The illustration of an approach’s practical use can promote its adoption in practice. In this article, we examine the fractal property of classes by proposing a new metric. Specifically, we build a Feature-Level Software Network (FLSN) for each class to represent the methods/fields and their couplings (including coupling frequencies) within the class and propose a new metric, Fractal Dimension for Classes (FDC), to numerically describe the fractal property of classes using FLSNs, which captures class complexity. We evaluate FDC theoretically against Weyuker’s nine properties, and the results show that FDC adheres to eight of the nine properties. Empirical experiments performed on a set of 12 large open source Java systems show that (i) for most classes (larger than  (96%) ), there exists the fractal property in their FLSNs, (ii) FDC is capable of capturing additional aspects of class complexity that have not been addressed by existing complexity metrics, (iii) FDC significantly correlates with both the existing class-level complexity metrics and the number of bugs in classes, and (iv) FDC, when used together with existing class-level complexity metrics, can significantly improve bug prediction in classes in three scenarios (i.e., bug-count, bug-classification, and effort-aware) of the cross-project context, but in the within-project context, it cannot.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {30},
numpages = {50},
keywords = {key classes, network metrics, complex networks, static analysis, program comprehension}
}

@inproceedings{10.1145/3412841.3441894,
author = {Gartziandia, Aitor and Arrieta, Aitor and Agirre, Aitor and Sagardui, Goiuria and Arratibel, Maite},
title = {Using regression learners to predict performance problems on software updates: a case study on elevators dispatching algorithms},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3441894},
doi = {10.1145/3412841.3441894},
abstract = {Remote software deployment and updating has long been commonplace in many different fields, but now, the increasing expansion of IoT and CPSoS (Cyber-Physcal System of Systems) has highlighted the need for additional mechanisms in these systems, to ensure the correct behaviour of the deployed software version after deployment. In this sense, this paper investigates the use of Machine Learning algorithms to predict acceptable behaviour in system performance of a new software release. By monitoring the real performance, eventual unexpected problems can be identified. Based on previous knowledge and actual run-time information, the proposed approach predicts the response time that can be considered acceptable for the new software release, and this information is used to identify problematic releases. The mechanism has been applied to the post-deployment monitoring of traffic algorithms in elevator systems. To evaluate the approach, we have used performance mutation testing, obtaining good results. This paper makes two contributions. First, it proposes several regression learners that have been trained with different types of traffic profiles to efficiently predict response time of the traffic dispatching algorithm. This prediction is then compared with the actual response time of the new algorithm release, and provides a verdict about its performance. Secondly, a comparison of the different learners is performed.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {135–144},
numpages = {10},
keywords = {cyber-physical systems, machine learning, performance bugs},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@article{10.1145/3705301,
author = {Xie, Ruilin and Chen, Xiang and He, Qifan and Li, Bixin and Cui, Zhanqi},
title = {IATT: Interpretation Analysis based Transferable Test Generation for Convolutional Neural Networks},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3705301},
doi = {10.1145/3705301},
abstract = {Convolutional neural networks (CNNs) have been widely used in various fields. However, it is essential to perform sufficient testing to detect internal defects before deploying CNNs, especially in security-sensitive scenarios. Generating error-inducing inputs to trigger erroneous behavior is the primary way to detect CNN model defects. However, in practice, when the model under test is a black-box CNN model without accessible internal information, in some scenarios it is still necessary to generate high-quality test inputs within a limited testing budget. In such a new scenario, a potential approach is to generate transferable test inputs by analyzing the internal knowledge of other white-box CNN models similar to the model under test, and then use transferable test inputs to test the black-box CNN model. The main challenge in generating transferable test inputs is how to improve their error-inducing capability for different CNN models without changing the test oracle. We found that different CNN models make predictions based on features of similar important regions in images. Adding targeted perturbations to important regions will generate transferable test inputs with high realism. Therefore, we propose the Interpretable Analysis based Transferable Test Generation method for CNNs (IATT), which employs interpretation methods of CNN models to explain and localize important regions in test inputs, using backpropagation optimizer and perturbation mask process to add targeted perturbations to these important regions, thereby generating transferable test inputs. This process is repeated to iteratively optimize the transferability and realism of the test inputs. To verify the effectiveness of IATT, we perform experimental studies on nine deep learning models, including ResNet-50 and Vit-B/16, and commercial computer vision system Google Cloud Vision, and compared our method with four state-of-the-art baseline methods. Experimental results show that transferable test inputs generated by IATT can effectively cause black-box target models to output incorrect results. Compared to existing testing and adversarial attack methods, the average error-inducing success rate (ESR) in different testing scenarios is 18.1% (sim) 52.7% greater than the baseline methods. Additionally, the test inputs generated by IATT achieve high ESR while maintaining high realism.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
keywords = {Convolutional neural network, Transfer Testing, Interpretability, Test Input Generation}
}

@article{10.1145/3643729,
author = {Song, Zirui and Chen, Jiongyi and Zhang, Kehuan},
title = {Bin2Summary: Beyond Function Name Prediction in Stripped Binaries with Functionality-Specific Code Embeddings},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643729},
doi = {10.1145/3643729},
abstract = {Nowadays, closed-source software only with stripped binaries still dominates the ecosystem, which brings obstacles to understanding the functionalities of the software and further conducting the security analysis. With such an urgent need, research has traditionally focused on predicting function names, which can only provide fragmented and abbreviated information about functionality.  To advance the state-of-the-art, this paper presents Bin2Summary to automatically summarize the functionality of the function in stripped binaries with natural language sentences. Specifically, the proposed framework includes a functionality-specific code embedding module to facilitate fine-grained similarity detection and an attention-based seq2seq model to generate summaries in natural language. Based on 16 widely-used projects (e.g., Coreutils), we have evaluated Bin2Summary with 38,167 functions, which are filtered from 162,406 functions, and all of them have a high-quality comment. Bin2Summary achieves 0.728 in precision and 0.729 in recall on our datasets, and the functionality-specific embedding module can improve the existing assembly language model by up to 109.5% and 109.9% in precision and recall. Meanwhile, the experiments demonstrated that Bin2Summary has outstanding transferability in analyzing the cross-architecture (i.e., in x64 and x86) and cross-environment (i.e., in Cygwin and MSYS2) binaries. Finally, the case study illustrates how Bin2Summary outperforms the existing works in providing functionality summaries with abundant semantics beyond function names.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {3},
numpages = {23},
keywords = {Code Summarization, Machine Learning for Program Analysis, Reverse Engineering, Transfer Learning}
}

@inproceedings{10.1145/3704137.3704166,
author = {Costa, Samuel Sampaio and Pato, Matilde and Datia, Nuno},
title = {An empirical study on the application of KANs for classification},
year = {2025},
isbn = {9798400718014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704137.3704166},
doi = {10.1145/3704137.3704166},
abstract = {Kolmogorov-Arnold Networks (KANs) represent a breakthrough in deep learning, diverging from Multi-Layer Perceptrons (MLPs) by generalizing the Kolmogorov-Arnold representation theorem (KAT) to networks of arbitrary depth and width. This theorem facilitates the decomposition of multivariate functions into constituent one-dimensional elements, with learnable activation functions on weights and the sum operator on nodes. KANs have been shown to exhibit robust performance in function approximation, validated across mathematical, physical, and practical domains such as traffic prediction and medical diagnostics. Our study investigates KANs' efficacy through comprehensive evaluations on OpenML, Kaggle and UCI datasets, with a focus on enhancing Human Activity Recognition systems. They demonstrate high classification performance compared to conventional machine learning approaches and MLPs. These findings underscore KANs' potential as scalable, interpretable tools in modern machine learning applications given their favorable neural scaling laws.},
booktitle = {Proceedings of the 2024 8th International Conference on Advances in Artificial Intelligence},
pages = {308–314},
numpages = {7},
keywords = {Classification, Human Activity Recognition, Kernel function, Kolmogorov-Arnold Networks, Multivariate functions},
location = {
},
series = {ICAAI '24}
}

@article{10.1145/3550271,
author = {Attaoui, Mohammed and Fahmy, Hazem and Pastore, Fabrizio and Briand, Lionel},
title = {Black-box Safety Analysis and Retraining of DNNs based on Feature Extraction and Clustering},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3550271},
doi = {10.1145/3550271},
abstract = {Deep neural networks (DNNs) have demonstrated superior performance over classical machine learning to support many features in safety-critical systems. Although DNNs are now widely used in such systems (e.g., self driving cars), there is limited progress regarding automated support for functional safety analysis in DNN-based systems. For example, the identification of root causes of errors, to enable both risk analysis and DNN retraining, remains an open problem. In this article, we propose SAFE, a black-box approach to automatically characterize the root causes of DNN errors. SAFE relies on a transfer learning model pre-trained on ImageNet to extract the features from error-inducing images. It then applies a density-based clustering algorithm to detect arbitrary shaped clusters of images modeling plausible causes of error. Last, clusters are used to effectively retrain and improve the DNN. The black-box nature of SAFE is motivated by our objective not to require changes or even access to the DNN internals to facilitate adoption. Experimental results show the superior ability of SAFE in identifying different root causes of DNN errors based on case studies in the automotive domain. It also yields significant improvements in DNN accuracy after retraining, while saving significant execution time and memory when compared to alternatives.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {79},
numpages = {40},
keywords = {transfer learning, clustering, DNN debugging, DNN functional safety analysis, DNN explanation}
}

@inproceedings{10.1145/3526073.3527591,
author = {Matsui, Beatriz M. A. and Goya, Denise H.},
title = {MLOps: a guide to its adoption in the context of responsible AI},
year = {2023},
isbn = {9781450393195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526073.3527591},
doi = {10.1145/3526073.3527591},
abstract = {DevOps practices have increasingly been applied to software development as well as the machine learning lifecycle, in a process known as MLOps. Currently, many professionals have written about this topic, but still few results can be found in the academic and scientific literature on MLOps and how to to implement it effectively. Considering aspects of responsible AI, this number is even lower, opening up a field of research with many possibilities. This article presents five steps to guide the understanding and adoption of MLOps in the context of responsible AI. The study aims to serve as a reference guide for all those who wish to learn more about the topic and intend to implement MLOps practices to develop their systems, following responsible AI principles.},
booktitle = {Proceedings of the 1st Workshop on Software Engineering for Responsible AI},
pages = {45–49},
numpages = {5},
keywords = {responsible AI, model, machine learning, development, MLOps, DevOps},
location = {Pittsburgh, Pennsylvania},
series = {SE4RAI '22}
}

@article{10.1145/3660773,
author = {Hossain, Soneya Binta and Jiang, Nan and Zhou, Qiang and Li, Xiaopeng and Chiang, Wen-Hao and Lyu, Yingjun and Nguyen, Hoan and Tripp, Omer},
title = {A Deep Dive into Large Language Models for Automated Bug Localization and Repair},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660773},
doi = {10.1145/3660773},
abstract = {Large language models (LLMs) have shown impressive effectiveness in various software engineering tasks,
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
including automated program repair (APR). In this study, we take a deep dive into automated bug localization
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
and repair utilizing LLMs. In contrast to many deep learning-based APR methods that assume known bug
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
locations, rely on line-level localization tools, or address bug prediction and fixing in one step, our approach
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
uniquely employs LLMs to predict bug location at the token level and subsequently utilizes them for bug
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
fixing. This methodological separation of bug localization and fixing using different LLMs enables effective
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
integration of diverse contextual information and improved incorporation of inductive biases. We introduce
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Toggle: Token-Granulated Bug Localization and Repair, a comprehensive program repair framework
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
that integrates a bug localization model, an adjustment model to address tokenizer inconsistencies, and a
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
bug-fixing model. Toggle takes a buggy function as input and generates a complete corrected function. We
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
investigate various styles of prompting to the bug fixing model to identify the most effective prompts that
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
better utilize the inductive bias and significantly outperform others. Toggle achieves the new state-of-the-art
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(SOTA) performance on the CodeXGLUE code refinement benchmark, and exhibits better and comparable
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
performance on several other widely-used APR datasets, including Defects4J. In the Defects4J benchmark, our
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
approach consistently ranks above other methods, achieving superior results in the Top-10, Top-30, Top-50,
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
and Top-100 metrics. Besides examining Toggle’s generalizability to unseen data, evaluating the effectiveness
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
of various prompts, we also investigate the impact of additional contextual information such as buggy lines
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
and code comments on bug localization, and explore the importance of the adjustment model. Our extensive
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
experiments offer valuable insights and answers to critical research questions.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {66},
numpages = {23},
keywords = {Automated Bug Localization and Reapir, Large Language Models}
}

@inproceedings{10.1109/MSR.2017.4,
author = {Rajbahadur, Gopi Krishnan and Wang, Shaowei and Kamei, Yasutaka and Hassan, Ahmed E.},
title = {The impact of using regression models to build defect classifiers},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.4},
doi = {10.1109/MSR.2017.4},
abstract = {It is common practice to discretize continuous defect counts into defective and non-defective classes and use them as a target variable when building defect classifiers (discretized classifiers). However, this discretization of continuous defect counts leads to information loss that might affect the performance and interpretation of defect classifiers. Another possible approach to build defect classifiers is through the use of regression models then discretizing the predicted defect counts into defective and non-defective classes (regression-based classifiers).In this paper, we compare the performance and interpretation of defect classifiers that are built using both approaches (i.e., discretized classifiers and regression-based classifiers) across six commonly used machine learning classifiers (i.e., linear/logistic regression, random forest, KNN, SVM, CART, and neural networks) and 17 datasets. We find that: i) Random forest based classifiers outperform other classifiers (best AUC) for both classifier building approaches; ii) In contrast to common practice, building a defect classifier using discretized defect counts (i.e., discretized classifiers) does not always lead to better performance.Hence we suggest that future defect classification studies should consider building regression-based classifiers (in particular when the defective ratio of the modeled dataset is low). Moreover, we suggest that both approaches for building defect classifiers should be explored, so the best-performing classifier can be used when determining the most influential features.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {135–145},
numpages = {11},
keywords = {random forest, non-discretization, model interpretation, discretization, classification via regression, bug prediction},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@inproceedings{10.1145/3293882.3338985,
author = {Kudjo, Patrick Kwaku and Chen, Jinfu},
title = {A cost-effective strategy for software vulnerability prediction based on bellwether analysis},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3338985},
doi = {10.1145/3293882.3338985},
abstract = {Vulnerability Prediction Models (VPMs) aims to identify vulnerable and non-vulnerable components in large software systems. Consequently, VPMs presents three major drawbacks (i) finding an effective method to identify a representative set of features from which to construct an effective model. (ii) the way the features are utilized in the machine learning setup (iii) making an implicit assumption that parameter optimization would not change the outcome of VPMs. To address these limitations, we investigate the significant effect of the Bellwether analysis on VPMs. Specifically, we first develop a Bellwether algorithm to identify and select an exemplary subset of data to be considered as the Bellwether to yield improved prediction accuracy against the growing portfolio benchmark. Next, we build a machine learning approach with different parameter settings to show the improvement of performance of VPMs. The prediction results of the suggested models were assessed in terms of precision, recall, F-measure, and other statistical measures. The preliminary result shows the Bellwether approach outperforms the benchmark technique across the applications studied with F-measure values ranging from 51.1%-98.5%.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {424–427},
numpages = {4},
keywords = {Tuning, Software vulnerability, Machine learning, Bellwether},
location = {Beijing, China},
series = {ISSTA 2019}
}

@article{10.1145/1233321.1233338,
author = {Zadrozny, Bianca and Weiss, Gary and Saar-Tsechansky, Maytal},
title = {UBDM 2006: Utility-Based Data Mining 2006 workshop report},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/1233321.1233338},
doi = {10.1145/1233321.1233338},
abstract = {In this report we provide a summary of the Second International Workshop on Utility-Based Data Mining (UBDM-06) held in conjunction with the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. The workshop was held on August 20, 2006 in Philadelphia, PA, USA. As was the case for the first UBDM workshop, this workshop brought together researchers who currently investigate how to incorporate economic utility aspects into the data mining process and practitioners who have real-world experience with how these factors influence data mining applications.},
journal = {SIGKDD Explor. Newsl.},
month = dec,
pages = {98–101},
numpages = {4},
keywords = {utility-based data mining, cost-sensitive learning, active learning, active information acquisition}
}

@inproceedings{10.1109/ICSE43902.2021.00086,
author = {Ma, Wei and Chekam, Thierry Titcheu and Papadakis, Mike and Harman, Mark},
title = {MuDelta: Delta-Oriented Mutation Testing at Commit Time},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00086},
doi = {10.1109/ICSE43902.2021.00086},
abstract = {To effectively test program changes using mutation testing, one needs to use mutants that are relevant to the altered program behaviours. We introduce MuDelta, an approach that identifies commit-relevant mutants; mutants that affect and are affected by the changed program behaviours. Our approach uses machine learning applied on a combined scheme of graph and vector-based representations of static code features. Our results, from 50 commits in 21 Coreutils programs, demonstrate a strong prediction ability of our approach; yielding 0.80 (ROC) and 0.50 (PR-Curve) AUC values with 0.63 and 0.32 precision and recall values. These predictions are significantly higher than random guesses, 0.20 (PR-Curve) AUC, 0.21 and 0.21 precision and recall, and subsequently lead to strong relevant tests that kill 45% more relevant mutants than randomly sampled mutants (either sampled from those residing on the changed component(s) or from the changed lines). Our results also show that MuDelta selects mutants with 27% higher fault revealing ability in fault introducing commits. Taken together, our results corroborate the conclusion that commit-based mutation testing is suitable and promising for evolving software.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {897–909},
numpages = {13},
keywords = {regression testing, mutation testing, machine learning, continuous integration, commit-relevant mutants},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3611643.3613880,
author = {Feng, Sidong and Lu, Haochuan and Xiong, Ting and Deng, Yuetang and Chen, Chunyang},
title = {Towards Efficient Record and Replay: A Case Study in WeChat},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613880},
doi = {10.1145/3611643.3613880},
abstract = {WeChat, a widely-used messenger app boasting over 1 billion monthly active users, requires effective app quality assurance for its complex features. Record-and-replay tools are crucial in achieving this goal. Despite the extensive development of these tools, the impact of waiting time between replay events has been largely overlooked. On one hand, a long waiting time for executing replay events on fully-rendered GUIs slows down the process. On the other hand, a short waiting time can lead to events executing on partially-rendered GUIs, negatively affecting replay effectiveness. An optimal waiting time should strike a balance between effectiveness and efficiency. We introduce WeReplay, a lightweight image-based approach that dynamically adjusts inter-event time based on the GUI rendering state. Given the real-time streaming on the GUI, WeReplay employs a deep learning model to infer the rendering state and synchronize with the replaying tool, scheduling the next event when the GUI is fully rendered. Our evaluation shows that our model achieves 92.1% precision and 93.3% recall in discerning GUI rendering states in the WeChat app. Through assessing the performance in replaying 23 common WeChat usage scenarios, WeReplay successfully replays all scenarios on the same and different devices more efficiently than the state-of-the-practice baselines.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1681–1692},
numpages = {12},
keywords = {Efficient record and replay, GUI rendering, Machine Learning},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3472674.3473981,
author = {Pontillo, Valeria and Palomba, Fabio and Ferrucci, Filomena},
title = {Toward static test flakiness prediction: a feasibility study},
year = {2021},
isbn = {9781450386258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472674.3473981},
doi = {10.1145/3472674.3473981},
abstract = {Flaky tests are tests that exhibit both a passing and failing behavior when run against the same code. While the research community has attempted to define automated approaches for detecting and addressing test flakiness, most of them suffer from scalability issues and uncertainty as they require test cases to be run multiple times. This limitation has been recently targeted by means of machine learning solutions that could predict the flakiness of tests using a set of both static and dynamic metrics that would avoid the re-execution of tests. Recognizing the effort spent so far, this paper poses the first steps toward an orthogonal view of the problem, namely the classification of flaky tests using only statically computable software metrics. We propose a feasibility study on 72 projects of the iDFlakies dataset, and investigate the differences between flaky and non-flaky tests in terms of 25 test and production code metrics and smells. First, we statistically assess those differences. Second, we build a logistic regression model to verify the extent to which the differences observed are still significant when the metrics are considered together. The results show a relation between test flakiness and a number of test and production code factors, indicating the possibility to build classification approaches that exploit those factors to predict test flakiness.},
booktitle = {Proceedings of the 5th International Workshop on Machine Learning Techniques for Software Quality Evolution},
pages = {19–24},
numpages = {6},
keywords = {Software Quality Evaluation, Flaky Tests, Empirical Studies},
location = {Athens, Greece},
series = {MaLTESQuE 2021}
}

@inproceedings{10.1145/3193977.3193985,
author = {Hardin, Bonnie and Kanewala, Upulee},
title = {Using semi-supervised learning for predicting metamorphic relations},
year = {2018},
isbn = {9781450357296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3193977.3193985},
doi = {10.1145/3193977.3193985},
abstract = {Software testing is difficult to automate, especially in programs which have no oracle, or method of determining which output is correct. Metamorphic testing is a solution this problem. Metamorphic testing uses metamorphic relations to define test cases and expected outputs. A large amount of time is needed for a domain expert to determine which metamorphic relations can be used to test a given program. Metamorphic relation prediction removes this need for such an expert. We propose a method using semi-supervised machine learning to detect which metamorphic relations are applicable to a given code base. We compare this semi-supervised model with a supervised model, and show that the addition of unlabeled data improves the classification accuracy of the MR prediction model.},
booktitle = {Proceedings of the 3rd International Workshop on Metamorphic Testing},
pages = {14–17},
numpages = {4},
keywords = {semi-supervised learning, metamorphic testing, metamorphic relations, machine learning},
location = {Gothenburg, Sweden},
series = {MET '18}
}

@inproceedings{10.1145/3387904.3389295,
author = {Lenarduzzi, Valentina and Palomba, Fabio and Taibi, Davide and Tamburri, Damian Andrew},
title = {OpenSZZ: A Free, Open-Source, Web-Accessible Implementation of the SZZ Algorithm},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389295},
doi = {10.1145/3387904.3389295},
abstract = {The accurate identification of defect-inducing commits represents a key problem for researchers interested in studying the naturalness of defects and defining defect prediction models. To tackle this problem, software engineering researchers have relied on and proposed several implementations of the well-known Sliwerski-Zimmermann-Zeller (SZZ) algorithm. Despite its popularity and wide usage, no open-source, publicly available, and web-accessible implementation of the algorithm has been proposed so far. In this paper, we prototype and make available one such implementation for further use by practitioners and researchers alike. The evaluation of the proposed prototype showed competitive results and lays the foundation for future work. This paper outlines our prototype, illustrating its usage and reporting on its evaluation in action.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {446–450},
numpages = {5},
keywords = {Web APIs, Software Defect Proneness, Software Defect Prediction, Open-Source Tools},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@inproceedings{10.5555/998675.999452,
author = {Brun, Yuriy and Ernst, Michael D.},
title = {Finding Latent Code Errors via Machine Learning over Program Executions},
year = {2004},
isbn = {0769521630},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper proposes a technique for identifying programproperties that indicate errors. The technique generates machinelearning models of program properties known to resultfrom errors, and applies these models to program propertiesof user-written code to classify and rank propertiesthat may lead the user to errors. Given a set of propertiesproduced by the program analysis, the technique selectssubset of properties that are most likely to reveal an error.An implementation, the Fault Invariant Classifier,demonstrates the efficacy of the technique. The implementationuses dynamic invariant detection to generate programproperties. It uses support vector machine and decision treelearning tools to classify those properties. In our experimentalevaluation, the technique increases the relevance(the concentration of fault-revealing properties) by a factorof 50 on average for the C programs, and 4.8 for the Javaprograms. Preliminary experience suggests that most of thefault-revealing properties do lead a programmer to an error.},
booktitle = {Proceedings of the 26th International Conference on Software Engineering},
pages = {480–490},
numpages = {11},
series = {ICSE '04}
}

@inproceedings{10.1109/ASE56229.2023.00202,
author = {Peng, Bo and Liang, Pingjia and Han, Tingchen and Luo, Weilin and Du, Jianfeng and Wan, Hai and Ye, Rongzhen and Zheng, Yuhang},
title = {PURLTL: Mining LTL Specification from Imperfect Traces in Testing},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00202},
doi = {10.1109/ASE56229.2023.00202},
abstract = {Formal specifications are widely used in software testing approaches, while writing such specifications is a time-consuming job. Recently, a number of methods have been proposed to mine specifications from execution traces, typically in the form of linear temporal logic (LTL). However, existing works have the following disadvantages: (1) ignoring the negative impact of imperfect traces, which come from partial profiling, missing context information, or buggy programs; (2) relying on templates, resulting in limited expressiveness; (3) requesting negative traces, which are usually unavailable in practice.In this paper, we propose PURLTL, which is able to mine arbitrary LTL specifications from imperfect traces. To alleviate the search space explosion and the wrong search bias, we propose a neural-based method to search LTL formulae, which, intuitively, simulates LTL path checking through differentiable parameter operations. To solve the problem of lacking negative traces, we transform the problem into learning from positive and unlabeled samples, by means of data augmentation and applying positive and unlabeled learning to the training process. Experiments show that our approach surpasses the previous start-of-the-art (SOTA) approach by a large margin. Besides, the results suggest that our approach is not only robust with imperfect traces, but also does not rely on formula templates.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1766–1770},
numpages = {5},
keywords = {specification mining, machine learning},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3665348.3665391,
author = {Xia, Yuying and Shao, Haijian and Deng, Xing},
title = {VulCoBERT: A CodeBERT-Based System for Source Code Vulnerability Detection},
year = {2024},
isbn = {9798400709562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665348.3665391},
doi = {10.1145/3665348.3665391},
abstract = {As we advance in time, with the evolution of the computer industry and the escalating intricacy of diverse software, the demand for source code defects is also increasing.&nbsp;Traditional deep learning-based software defect detection methods perform well on synthetic defect datasets, but their performance on real software defect datasets is unsatisfactory. At the same time, pre-trained models derived from extensive data training are extensively employed in various NLP tasks and have achieved excellent results.&nbsp;Based on this background, this study introduces a software defect detection system leveraging CodeBERT and Bi-LSTM. This method first preprocesses and standardizes the C source code to minimize the impact of redundant information and reduce noise;&nbsp;Secondly, coding sequences are segmented and encoded using the CodeBERT pre-trained model, capturing the semantic features within the program's code and converting them into vectors containing code feature information and output;&nbsp;Then, the output of the received CodeBERT is processed through the Bi-LSTM network to acquire the structural composition inherent to the code and the semantic information of the positive and negative terms;&nbsp;Finally, the vectors containing source code features are classified using a fully connected network to determine whether the code segment has defects.&nbsp;To verify the impact of this approach, we used the cross-language benchmark test set CodeXGLUE proposed by Microsoft Research Institute for evaluating code tasks for validation. The results showed that this method had higher accuracy in detecting real software defect samples than other methods, indicating that the proposed method can effectively improve software defect detection capabilities.},
booktitle = {Proceedings of the 2024 International Conference on Generative Artificial Intelligence and Information Security},
pages = {249–252},
numpages = {4},
location = {Kuala Lumpur, Malaysia},
series = {GAIIS '24}
}

@inproceedings{10.1145/2915970.2915979,
author = {Petri\'{c}, Jean},
title = {Using different characteristics of machine learners to identify different defect families},
year = {2016},
isbn = {9781450336918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2915970.2915979},
doi = {10.1145/2915970.2915979},
abstract = {Background: Software defect prediction has been an active area of research for the last few decades. Many models have been developed with aim to find locations in code likely to contain defects. As of yet, these prediction models are of limited use and rarely used in the software industry.Problem: Current modelling techniques are too coarse grained and fail in finding some defects. Most of the prediction models do not look for targeted defect characteristics, but rather treat them as a black box and homogeneous. No study has investigated in greater detail how well certain defect characteristics work with different prediction modelling techniques.Methodology: This PhD will address three major tasks. First, the relation among software defects, prediction models and static code metrics will be analysed. Second, the possibility of a mapping function between prediction models and defect characteristics shall be investigated. Third, an optimised ensemble model that searches for targeted defects will be developed.Contribution: A few contributions will yield from this work. Characteristics of defects will be identified, allowing other researchers to build on this work to produce more efficient prediction models in future. New modelling techniques that better suit state-of-the-art knowledge in defect prediction shall be designed. Such prediction models should be transformed in a tool that can be used by our industrial collaborator in the real industry environment.},
booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {5},
numpages = {4},
keywords = {software defect prediction, prediction modeling, machine learning},
location = {Limerick, Ireland},
series = {EASE '16}
}

@article{10.1145/3715005,
author = {Zhang, Shenglin and Xia, Sibo and Fan, Wenzhao and Shi, Binpeng and Xiong, Xiao and Zhong, Zhenyu and Ma, Minghua and Sun, Yongqian and Pei, Dan},
title = {Failure Diagnosis in Microservice Systems: A Comprehensive Survey and Analysis},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715005},
doi = {10.1145/3715005},
abstract = {Widely adopted for their scalability and flexibility, modern microservice systems present unique failure diagnosis challenges due to their independent deployment and dynamic interactions. This complexity can lead to cascading failures that negatively impact operational efficiency and user experience. Recognizing the critical role of fault diagnosis in improving the stability and reliability of microservice systems, researchers have conducted extensive studies and achieved a number of significant results. This survey provides an exhaustive review of 98 scientific papers from 2003 to the present, including a thorough examination and elucidation of the fundamental concepts, system architecture, and problem statement. It also includes a qualitative analysis of the dimensions, providing an in-depth discussion of current best practices and future directions, aiming to further its development and application. In addition, this survey compiles publicly available datasets, toolkits, and evaluation metrics to facilitate the selection and validation of techniques for practitioners.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Microservice, failure diagnosis, root cause localization, failure classification, multimodal data}
}

@inproceedings{10.1109/ASE56229.2023.00021,
author = {Hanna, Carol and Petke, Justyna},
title = {Hot Patching Hot Fixes: Reflection and Perspectives},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00021},
doi = {10.1109/ASE56229.2023.00021},
abstract = {With our reliance on software continuously increasing, it is of utmost importance that it be reliable. However, complete prevention of bugs in live systems is unfortunately an impossible task due to time constraints, incomplete testing, and developers not having knowledge of the full stack. As a result, mitigating risks for systems in production through hot patching and hot fixing has become an integral part of software development. In this paper, we first give an overview of the terminology used in the literature for research on this topic. Subsequently, we build upon these findings and present our vision for an automated framework for predicting and mitigating critical software issues at runtime. Our framework combines hot patching and hot fixing research from multiple fields, in particular: software defect and vulnerability prediction, automated test generation and repair, as well as runtime patching. We hope that our vision inspires research collaboration between the different communities.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1781–1786},
numpages = {6},
keywords = {software engineering, software maintenance, predictive maintenance, prediction methods, repair},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3558489.3559068,
author = {Bludau, Peter and Pretschner, Alexander},
title = {Feature sets in just-in-time defect prediction: an empirical evaluation},
year = {2022},
isbn = {9781450398602},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3558489.3559068},
doi = {10.1145/3558489.3559068},
abstract = {Just-in-time defect prediction assigns a defect risk to each new change to a software repository in order to prioritize review and testing efforts. Over the last decades different approaches were proposed in literature to craft more accurate prediction models. However, defect prediction is still not widely used in industry, due to predictions with varying performance. In this study, we evaluate existing features on six open-source projects and propose two new features sets, not yet discussed in literature. By combining all feature sets, we improve MCC by on average 21%, leading to the best performing models when compared to state-of-the-art approaches. We also evaluate effort-awareness and find that on average 14% more defects can be identified, inspecting 20% of changed lines.},
booktitle = {Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {22–31},
numpages = {10},
keywords = {machine learning, empirical evaluation, JIT defect prediction},
location = {Singapore, Singapore},
series = {PROMISE 2022}
}

@article{10.1145/3511662,
author = {Neville-Neil, George V.},
title = {Getting Off the Mad Path: Debuggers and assertions},
year = {2022},
issue_date = {November-December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {6},
issn = {1542-7730},
url = {https://doi.org/10.1145/3511662},
doi = {10.1145/3511662},
abstract = {KV continues to grind his teeth as he sees code loaded with debugging statements that would be totally unnecessary if the programmers who wrote the code could be both confident in and proficient with their debuggers. If one is lucky enough to have access to a good debugger, one should give extreme thanks to whatever they normally give thanks to and use the damn thing!},
journal = {Queue},
month = jan,
pages = {18–21},
numpages = {4}
}

@inproceedings{10.1145/3460620.3460754,
author = {Cheruvu, Aravind and Vangipuram, Radhakrishna},
title = {A Survey of Similarity Measures for Time stamped Temporal Datasets},
year = {2021},
isbn = {9781450388382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460620.3460754},
doi = {10.1145/3460620.3460754},
abstract = {Temporal transactional databases are transactional databases which store data in a temporal aspect. Usage of similarity of measures in temporal data mining tasks have gained significant importance to retrieve information and interesting patterns in data. It is always crucial to understand and decide what similarity measure we should use while performing a data mining task and this is always driven by the actual data and nature of the temporal data sets. The main objective of this research is to perform a detailed survey of the various similarity measures used in the temporal data mining in recent research contributions. This paper also provides insights on how these similarity measures are used in the Temporal association rule mining algorithms based on the works carried out in the literature.},
booktitle = {International Conference on Data Science, E-Learning and Information Systems 2021},
pages = {193–197},
numpages = {5},
keywords = {optimization, industry manufacturing, digitization, algorithms},
location = {Ma'an, Jordan},
series = {DATA'21}
}

@inproceedings{10.1145/3677182.3677264,
author = {Li, Na and Wang, Jun and Chen, Chen and Hu, Hongfei},
title = {Application of API automation testing based on microservice mode in industry software},
year = {2024},
isbn = {9798400709784},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677182.3677264},
doi = {10.1145/3677182.3677264},
abstract = {Set against the backdrop of a corporate cloud billing system testing initiative, this paper delves into the pragmatic approach to API automation testing within a microservices architectural context. It commences by underscoring the significance and the inherent challenges posed by API testing in a microservices ecosystem, with a particular focus on the quandaries encountered when managing intricate and voluminous test data. To surmount these obstacles and enhance both the efficacy and scope of testing, the research advocates for an innovative paradigm in test data administration and procreation. This paradigm harnesses machine learning techniques to automate the generation of high-fidelity test data. By leveraging machine learning algorithms to dissect historical data and discern patterns of application utilization, the methodology affords the creation of test datasets that mirror authentic operational scenarios. Such an approach substantially elevates the pertinence and exhaustiveness of the test data, concurrently diminishing the demand for labor-intensive manual test case design. During the regression testing phase, the expounded microservices-based API automation testing strategy has demonstrated its efficacy in bolstering software quality and refining the testing process's efficiency. The paper concludes by encapsulating best practices for API automation testing within microservices architectures and suggests avenues for future research aimed at further streamlining software testing protocols and propelling ongoing advancements in industry software quality assurance.},
booktitle = {Proceedings of the International Conference on Algorithms, Software Engineering, and Network Security},
pages = {460–464},
numpages = {5},
location = {Nanchang, China},
series = {ASENS '24}
}

@inproceedings{10.1145/3617555.3617875,
author = {Bhandari, Pragya and Rodr\'{\i}guez-P\'{e}rez, Gema},
title = {BuggIn: Automatic Intrinsic Bugs Classification Model using NLP and ML},
year = {2023},
isbn = {9798400703751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617555.3617875},
doi = {10.1145/3617555.3617875},
abstract = {Recent studies have shown that bugs can be categorized into in-  
trinsic and extrinsic types. Intrinsic bugs can be backtracked to  
specific changes in the version control system (VCS), while extrin-  
sic bugs originate from external changes to the VCS and lack a  
direct bug-inducing change. Using only intrinsic bugs to train bug  
prediction models has been reported as beneficial to improve the  
performance of such models. However, there is currently no auto-  
mated approach to identify intrinsic bugs. To bridge this gap, our  
study employs Natural Language Processing (NLP) techniques to  
automatically identify intrinsic bugs. Specifically, we utilize two  
embedding techniques, seBERT and TF-IDF, applied to the title and  
description text of bug reports. The resulting embeddings are fed  
into well-established machine learning algorithms such as Support  
Vector Machine, Logistic Regression, Decision Tree, Random Forest,  
and K-Nearest Neighbors. The primary objective of this paper is  
to assess the performance of various NLP and machine learning  
techniques in identifying intrinsic bugs using the textual informa-  
tion extracted from bug reports. The results demonstrate that both  
seBERT and TF-IDF can be effectively utilized for intrinsic bug  
identification. The highest performance scores were achieved by  
combining TF-IDF with the Decision Tree algorithm and utilizing  
the bug titles (yielding an F1 score of 78%). This was closely fol-  
lowed by seBERT, Support Vector Machine, and bug titles (with an  
F1 score of 77%). In summary, this paper introduces an innovative  
approach that automates the identification of intrinsic bugs using  
textual information derived from bug reports.},
booktitle = {Proceedings of the 19th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {2–11},
numpages = {10},
keywords = {software bugs, natural language processing, intrinsic bugs, extrinsic bugs, classification},
location = {San Francisco, CA, USA},
series = {PROMISE 2023}
}

@article{10.1145/3631340,
author = {Deokuliar, Harsh and Sangwan, Raghvinder S. and Badr, Youakim and Srinivasan, Satish M.},
title = {Improving Testing of Deep-learning Systems: A combination of differential and mutation testing results in better test data.},
year = {2023},
issue_date = {September/October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {5},
issn = {1542-7730},
url = {https://doi.org/10.1145/3631340},
doi = {10.1145/3631340},
abstract = {We used differential testing to generate test data to improve diversity of data points in the test dataset and then used mutation testing to check the quality of the test data in terms of diversity. Combining differential and mutation testing in this fashion improves mutation score, a test data quality metric, indicating overall improvement in testing effectiveness and quality of the test data when testing deep learning systems.},
journal = {Queue},
month = nov,
pages = {54–65},
numpages = {12}
}

@inproceedings{10.1145/3510454.3517066,
author = {Olsthoorn, Mitchell},
title = {More effective test case generation with multiple tribes of AI},
year = {2022},
isbn = {9781450392235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510454.3517066},
doi = {10.1145/3510454.3517066},
abstract = {Software testing is a critical activity in the software development life cycle for quality assurance. Automated Test Case Generation (TCG) can assist developers by speeding up this process. It accomplishes this by evolving an initial set of randomly generated test cases over time to optimize for predefined coverage criteria. One of the key challenges for automated TCG approaches is navigating the large input space. Existing state-of-the-art TCG algorithms struggle with generating highly-structured input data and preserving patterns in test structures, among others. I hypothesize that combining multiple tribes of AI can improve the effectiveness and efficiency of automated TCG. To test this hypothesis, I propose using grammar-based fuzzing and machine learning to augment evolutionary algorithms for generating more structured input data and preserving promising patterns within test cases. Additionally, I propose to use behavioral modeling and interprocedural control dependency analysis to improve test effectiveness. Finally, I propose integrating these novel approaches into a testing framework to promote the adoption of automated TCG in industry.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings},
pages = {286–290},
numpages = {5},
keywords = {test case generation, software testing, search-based software testing, machine learning, fuzzing},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@article{10.1145/3643671,
author = {Attaoui, Mohammed Oualid and Fahmy, Hazem and Pastore, Fabrizio and Briand, Lionel},
title = {Supporting Safety Analysis of Image-processing DNNs through Clustering-based Approaches},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3643671},
doi = {10.1145/3643671},
abstract = {The adoption of deep neural networks (DNNs) in safety-critical contexts is often prevented by the lack of effective means to explain their results, especially when they are erroneous. In our previous work, we proposed a white-box approach (HUDD) and a black-box approach (SAFE) to automatically characterize DNN failures. They both identify clusters of similar images from a potentially large set of images leading to DNN failures. However, the analysis pipelines for HUDD and SAFE were instantiated in specific ways according to common practices, deferring the analysis of other pipelines to future work.In this article, we report on an empirical evaluation of 99 different pipelines for root cause analysis of DNN failures. They combine transfer learning, autoencoders, heatmaps of neuron relevance, dimensionality reduction techniques, and different clustering algorithms. Our results show that the best pipeline combines transfer learning, DBSCAN, and UMAP. It leads to clusters almost exclusively capturing images of the same failure scenario, thus facilitating root cause analysis. Further, it generates distinct clusters for each root cause of failure, thus enabling engineers to detect all the unsafe scenarios. Interestingly, these results hold even for failure scenarios that are only observed in a small percentage of the failing images.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {130},
numpages = {48},
keywords = {DNN explanation, DNN functional safety analysis, DNN debugging, clustering, transfer learning}
}

@inproceedings{10.1145/3482909.3482916,
author = {Camara, Bruno and Silva, Marco and Endo, Andre and Vergilio, Silvia},
title = {On the use of test smells for prediction of flaky tests},
year = {2021},
isbn = {9781450385039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482909.3482916},
doi = {10.1145/3482909.3482916},
abstract = {Regression testing is an important phase to deliver software with quality. However, flaky tests hamper the evaluation of test results and can increase costs. This is because a flaky test may pass or fail non-deterministically and to identify properly the flakiness of a test requires rerunning the test suite multiple times. To cope with this challenge, approaches have been proposed based on prediction models and machine learning. Existing approaches based on the use of the test case vocabulary may be context-sensitive and prone to overfitting, presenting low performance when executed in a cross-project scenario. To overcome these limitations, we investigate the use of test smells as predictors of flaky tests. We conducted an empirical study to understand if test smells have good performance as a classifier to predict the flakiness in the cross-project context, and analysed the information gain of each test smell. We also compared the test smell-based approach with the vocabulary-based one. As a result, we obtained a classifier that had a reasonable performance (Random Forest, 0.83%) to predict the flakiness in the testing phase. This classifier presented better performance than vocabulary-based model for cross-project prediction. The Assertion Roulette and Sleepy Test test smell types are the ones associated with the best information gain values.},
booktitle = {Proceedings of the 6th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {46–54},
numpages = {9},
keywords = {test smells, test flakiness, regression testing, machine learning},
location = {Joinville, Brazil},
series = {SAST '21}
}

@inproceedings{10.1145/1162678.1162681,
author = {Le, Franck and Lee, Sihyung and Wong, Tina and Kim, Hyong S. and Newcomb, Darrell},
title = {Minerals: using data mining to detect router misconfigurations},
year = {2006},
isbn = {159593569X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1162678.1162681},
doi = {10.1145/1162678.1162681},
abstract = {Recent studies have shown that router misconfigurations are common and have dramatic consequences for the operations of networks. Not only can misconfigurations compromise the security of a single network, they can even cause global disruptions in Internet connectivity. Several solutions have been proposed that can detect a number of problems in real configuration files. However, these solutions share a common limitation: they are rule-based. Rules are assumed to be known beforehand, and violations of these rules are deemed misconfigurations. As policies typically differ among networks, rule-based approaches are limited in the scope of mistakes they can detect. In this paper, we address the problem of router misconfigurations using data mining. We apply association rules mining to the configuration files of routers across an administrative domain to discover local, network-specific policies. Deviations from these local policies are potential misconfigurations. We have evaluated our scheme on configuration files from a large state-wide network provider, a large university campus and a high-performance research network, and found promising results. We discovered a number of errors that were confirmed and later corrected by the network engineers. These errors would have been difficult to detect with current rule-based approaches.},
booktitle = {Proceedings of the 2006 SIGCOMM Workshop on Mining Network Data},
pages = {293–298},
numpages = {6},
keywords = {static analysis, routers, network misconfiguration, association rules mining},
location = {Pisa, Italy},
series = {MineNet '06}
}

@inproceedings{10.1145/3373477.3373486,
author = {Aggarwal, Simran},
title = {Software code analysis using ensemble learning techniques},
year = {2020},
isbn = {9781450372916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373477.3373486},
doi = {10.1145/3373477.3373486},
abstract = {Ensuing the advent of advancements in software systems, the probability of them containing high severity defects is exponentially on the rise. With each technological addition, the complexity of software is increasing. Reproduction and rectification of a defect requires time and effort. Current state of the art analysis tools cater to the investigation of static aspects of a production level code. However, it is imperative to assess the dynamic development process of a system so as to be able to timely detect erroneous components early on in the development life cycle of a software. A novel automated defect prediction feature enhancement is proposed that analyses the static structure of the current code and state of the software in past releases to extract relevant static and dynamic feature sets. Data generated is modelled for defect trends in the future release of the software by four ensemble classifiers. Results demonstrate the superiority of Voting algorithm for the problem of defect prediction.},
booktitle = {Proceedings of the 1st International Conference on Advanced Information Science and System},
articleno = {9},
numpages = {7},
keywords = {software quality, object-oriented metrics, machine learning, ensemble learning, empirical validation, defect prediction},
location = {Singapore, Singapore},
series = {AISS '19}
}

@inproceedings{10.1145/3524842.3528472,
author = {Gao, Yuxiang and Zhu, Yi and Yu, Qiao},
title = {Evaluating the effectiveness of local explanation methods on source code-based defect prediction models},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528472},
doi = {10.1145/3524842.3528472},
abstract = {Interpretation has been considered as one of key factors for applying defect prediction in practice. As one way for interpretation, local explanation methods has been widely used for certain predictions on datasets of traditional features. There are also attempts to use local explanation methods on source code-based defect prediction models, but unfortunately, it will get poor results. Since it is unclear how effective those local explanation methods are, we evaluate such methods with automatic metrics which focus on local faithfulness and explanation precision. Based on the results of experiments, we find that the effectiveness of local explanation methods depends on the adopted defect prediction models. They are effective on token frequency-based models, while they may not be effective enough to explain all predictions of deep learning-based models. Besides, we also find that the hyperparameter of local explanation methods should be carefully optimized to get more precise and meaningful explanation.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {640–645},
numpages = {6},
keywords = {software defect prediction, local explanation, explainable machine learning, LIME},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1145/3106237.3117766,
author = {Volf, Zahy and Shmueli, Edi},
title = {Screening heuristics for project gating systems},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3117766},
doi = {10.1145/3106237.3117766},
abstract = {Continuous Integration (CI) is a hot topic. Yet, little attention is payed to how CI systems work and what impacts their behavior. In parallel, bug prediction in software is gaining high attention. But this is done mostly in the context of software engineering, and the relation to the realm of CI and CI systems engineering has not been established yet. In this paper we describe how Project Gating systems operate, which are a specific type of CI systems used to keep the mainline of development always clean. We propose and evaluate three heuristics for improving Gating performance and demonstrate their trade-offs. The third heuristic, which leverages state-of-the-art bug prediction achieves the best performance across the entire spectrum of workload conditions.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {872–877},
numpages = {6},
keywords = {Project Gating, Machine Learning, Continuous Integration},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/3383219.3383268,
author = {Lenz, Luca and Felderer, Michael and Schwedes, Sascha and M\"{u}ller, Kai},
title = {Explainable Priority Assessment of Software-Defects using Categorical Features at SAP HANA},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383268},
doi = {10.1145/3383219.3383268},
abstract = {We want to automate priority assessment of software defects. To do so we provide a tool which uses an explainability-driven framework and classical machine learning algorithms to keep the decisions transparent. Differing from other approaches we only use objective and categorical fields from the bug tracking system as features. This makes our approach lightweight and extremely fast. We perform binary classification with priority labels corresponding to deadlines. Additionally, we evaluate the tool on real data to ensure good performance in the practical use case.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {366–367},
numpages = {2},
keywords = {software quality, machine learning, defect assessment, bug priority},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1145/3524459.3527350,
author = {Lajk\'{o}, M\'{a}rk and Csuvik, Viktor and Vid\'{a}cs, L\'{a}szl\'{o}},
title = {Towards JavaScript program repair with generative pre-trained transformer (GPT-2)},
year = {2022},
isbn = {9781450392853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524459.3527350},
doi = {10.1145/3524459.3527350},
abstract = {The goal of Automated Program Repair (APR) is to find a fix to software bugs, without human intervention. The so-called Generate and Validate (G&amp;V) approach deemed to be the most popular method in the last few years, where the APR tool creates a patch and it is validated against an oracle. Recent years for Natural Language Processing (NLP) were of great interest, with new pre-trained models shattering records on tasks ranging from sentiment analysis to question answering. Usually these deep learning models inspire the APR community as well. These approaches usually require a large dataset on which the model can be trained (or fine-tuned) and evaluated. The criterion to accept a patch depends on the underlying dataset, but usually the generated patch should be exactly the same as the one created by a human developer. As NLP models are more and more capable to form sentences, and the sentences will form coherent paragraphs, the APR tools are also better and better at generating syntactically and semantically correct source code. As the Generative Pre-trained Transformer (GPT) model is now available to everyone thanks to the NLP and AI research community, it can be fine-tuned to specific tasks (not necessarily on natural language). In this work we use the GPT-2 model to generate source code, to the best of our knowledge, the GPT-2 model was not used for Automated Program Repair so far. The model is fine-tuned for a specific task: it has been taught to fix JavaScript bugs automatically. To do so, we trained the model on 16863 JS code snippets, where it could learn the nature of the observed programming language. In our experiments we observed that the GPT-2 model was able to learn how to write syntactically correct source code almost on every attempt, although it failed to learn good bug-fixes in some cases. Nonetheless it was able to generate the correct fixes in most of the cases, resulting in an overall accuracy up to 17.25%.},
booktitle = {Proceedings of the Third International Workshop on Automated Program Repair},
pages = {61–68},
numpages = {8},
keywords = {machine learning, code refinement, automated program repair, JavaScript, GPT},
location = {Pittsburgh, Pennsylvania},
series = {APR '22}
}

@article{10.1145/3508362,
author = {Chen, Junjie and Suo, Chenyao},
title = {Boosting Compiler Testing via Compiler Optimization Exploration},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3508362},
doi = {10.1145/3508362},
abstract = {Compilers are a kind of important software, and similar to the quality assurance of other software, compiler testing is one of the most widely-used ways of guaranteeing their quality. Compiler bugs tend to occur in compiler optimizations. Detecting optimization bugs needs to consider two main factors: (1) the optimization flags controlling the accessability of the compiler buggy code should be turned on; and (2) the test program should be able to trigger the buggy code. However, existing compiler testing approaches only consider the latter to generate effective test programs, but just run them under several pre-defined optimization levels (e.g., -O0, -O1, -O2, -O3, -Os in GCC).To better understand the influence of compiler optimizations on compiler testing, we conduct the first empirical study, and find that (1) all the bugs detected under the widely-used optimization levels are also detected under the explored optimization settings (we call a combination of optimization flags turned on for compilation an optimization setting), while 83.54% of bugs are only detected under the latter; (2) there exist both inhibition effect and promotion effect among optimization flags for compiler testing, indicating the necessity and challenges of considering the factor of compiler optimizations in compiler testing.We then propose the first approach, called COTest, by considering both factors to test compilers. Specifically, COTest first adopts machine-learning (the XGBoost algorithm) to model the relationship between test programs and optimization settings, to predict the bug-triggering probability of a test program under an optimization setting. Then, it designs a diversity augmentation strategy to select a set of diverse candidate optimization settings for prediction for a test program. Finally, Top-K optimization settings are selected for compiler testing according to the predicted bug-triggering probabilities. Then, it designs a diversity augmentation strategy to select a set of diverse candidate optimization settings for prediction for a test program. Finally, Top-K optimization settings are selected for compiler testing according to the predicted bug-triggering probabilities. The experiments on GCC and LLVM demonstrate its effectiveness, especially COTest detects 17 previously unknown bugs, 11 of which have been fixed or confirmed by developers.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {72},
numpages = {33},
keywords = {machine learning, compiler optimization, Compiler testing}
}

@inproceedings{10.1145/3491102.3517474,
author = {Balayn, Agathe and Rikalo, Natasa and Lofi, Christoph and Yang, Jie and Bozzon, Alessandro},
title = {How can Explainability Methods be Used to Support Bug Identification in Computer Vision Models?},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517474},
doi = {10.1145/3491102.3517474},
abstract = {Deep learning models for image classification suffer from dangerous issues often discovered after deployment. The process of identifying bugs that cause these issues remains limited and understudied. Especially, explainability methods are often presented as obvious tools for bug identification. Yet, the current practice lacks an understanding of what kind of explanations can best support the different steps of the bug identification process, and how practitioners could interact with those explanations. Through a formative study and an iterative co-creation process, we build an interactive design probe providing various potentially relevant explainability functionalities, integrated into interfaces that allow for flexible workflows. Using the probe, we perform 18 user-studies with a diverse set of machine learning practitioners. Two-thirds of the practitioners engage in successful bug identification. They use multiple types of explanations, e.g. visual and textual ones, through non-standardized sequences of interactions including queries and exploration. Our results highlight the need for interactive, guiding, interfaces with diverse explanations, shedding light on future research directions.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {184},
numpages = {16},
keywords = {computer vision, machine learning explainability, machine learning model debugging, user interface},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3524481.3527230,
author = {Zhang, Jiyang and Liu, Yu and Gligoric, Milos and Legunsen, Owolabi and Shi, August},
title = {Comparing and combining analysis-based and learning-based regression test selection},
year = {2022},
isbn = {9781450392860},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524481.3527230},
doi = {10.1145/3524481.3527230},
abstract = {Regression testing---rerunning tests on each code version to detect newly-broken functionality---is important and widely practiced. But, regression testing is costly due to the large number of tests and the high frequency of code changes. Regression test selection (RTS) optimizes regression testing by only rerunning a subset of tests that can be affected by changes. Researchers showed that RTS based on program analysis can save substantial testing time for (medium-sized) open-source projects. Practitioners also showed that RTS based on machine learning (ML) works well on very large code repositories, e.g., in Facebook's monorepository. We combine analysis-based RTS and ML-based RTS by using the latter to choose a subset of tests selected by the former. We first train several novel ML models to learn the impact of code changes on test outcomes using a training dataset that we obtain via mutation analysis. Then, we evaluate the benefits of combining ML models with analysis-based RTS on 10 projects, compared with using each technique alone. Combining ML-based RTS with two analysis-based RTS techniques-Ekstazi and STARTS-selects 25.34% and 21.44% fewer tests, respectively.},
booktitle = {Proceedings of the 3rd ACM/IEEE International Conference on Automation of Software Test},
pages = {17–28},
numpages = {12},
keywords = {regression testing, regression test selection, program analysis, machine learning},
location = {Pittsburgh, Pennsylvania},
series = {AST '22}
}

@inproceedings{10.1145/3691620.3695260,
author = {Feng, Sidong and Lu, Haochuan and Jiang, Jianqin and Xiong, Ting and Huang, Likun and Liang, Yinglin and Li, Xiaoqin and Deng, Yuetang and Aleti, Aldeida},
title = {Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695260},
doi = {10.1145/3691620.3695260},
abstract = {UI automation tests play a crucial role in ensuring the quality of mobile applications. Despite the growing popularity of machine learning techniques to generate these tests, they still face several challenges, such as the mismatch of UI elements. The recent advances in Large Language Models (LLMs) have addressed these issues by leveraging their semantic understanding capabilities. However, a significant gap remains in applying these models to industrial-level app testing, particularly in terms of cost optimization and knowledge limitation. To address this, we introduce CAT to create cost-effective UI automation tests for industry apps by combining machine learning and LLMs with best practices. Given the task description, CAT employs Retrieval Augmented Generation (RAG) to source examples of industrial app usage as the few-shot learning context, assisting LLMs in generating the specific sequence of actions. CAT then employs machine learning techniques, with LLMs serving as a complementary optimizer, to map the target element on the UI screen. Our evaluations on the WeChat testing dataset demonstrate the CAT's performance and cost-effectiveness, achieving 90% UI automation with $0.34 cost, outperforming the state-of-the-art. We have also integrated our approach into the real-world WeChat testing platform, demonstrating its usefulness in detecting 141 bugs and enhancing the developers' testing process.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1973–1978},
numpages = {6},
keywords = {UI automation test, large language model, retrieval-augmented generation, cost optimization},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1109/ICSE43902.2021.00067,
author = {Li, Yi and Wang, Shaohua and Nguyen, Tien N.},
title = {Fault Localization with Code Coverage Representation Learning},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00067},
doi = {10.1109/ICSE43902.2021.00067},
abstract = {In this paper, we propose DEEPRL4FL, a deep learning fault localization (FL) approach that locates the buggy code at the statement and method levels by treating FL as an image pattern recognition problem. DEEPRL4FL does so via novel code coverage representation learning (RL) and data dependencies RL for program statements. Those two types of RL on the dynamic information in a code coverage matrix are also combined with the code representation learning on the static information of the usual suspicious source code. This combination is inspired by crime scene investigation in which investigators analyze the crime scene (failed test cases and statements) and related persons (statements with dependencies), and at the same time, examine the usual suspects who have committed a similar crime in the past (similar buggy code in the training data).For the code coverage information, DEEPRL4FL first orders the test cases and marks error-exhibiting code statements, expecting that a model can recognize the patterns discriminating between faulty and non-faulty statements/methods. For dependencies among statements, the suspiciousness of a statement is seen taking into account the data dependencies to other statements in execution and data flows, in addition to the statement by itself. Finally, the vector representations for code coverage matrix, data dependencies among statements, and source code are combined and used as the input of a classifier built from a Convolution Neural Network to detect buggy statements/methods. Our empirical evaluation shows that DEEPRL4FL improves the top-1 results over the state-of-the-art statement-level FL baselines from 173.1% to 491.7%. It also improves the top-1 results over the existing method-level FL baselines from 15.0% to 206.3%.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {661–673},
numpages = {13},
keywords = {representation learning, machine learning, fault localization, deep learning, code coverage},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3416505.3423561,
author = {Vasiliev, Roman and Koznov, Dmitrij and Chernishev, George and Khvorov, Aleksandr and Luciv, Dmitry and Povarov, Nikita},
title = {TraceSim: a method for calculating stack trace similarity},
year = {2020},
isbn = {9781450381246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416505.3423561},
doi = {10.1145/3416505.3423561},
abstract = {Many contemporary software products have subsystems for automatic crash reporting. However, it is well-known that the same bug can produce slightly different reports. To manage this problem, reports are usually grouped, often manually by developers. Manual triaging, however, becomes infeasible for products that have large userbases, which is the reason for many different approaches to automating this task. Moreover, it is important to improve quality of triaging due to a large volume of reports that needs to be processed properly. Therefore, even a relatively small improvement could play a significant role in the overall accuracy of report bucketing. The majority of existing studies use some kind of a stack trace similarity metric, either based on information retrieval techniques or string matching methods. However, it should be stressed that the quality of triaging is still insufficient.  In this paper, we describe TraceSim — a novel approach to this problem which combines TF-IDF, Levenshtein distance, and machine learning to construct a similarity metric. Our metric has been implemented inside an industrial-grade report triaging system. The evaluation on a manually labeled dataset shows significantly better results compared to baseline approaches.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation},
pages = {25–30},
numpages = {6},
keywords = {Stack Trace, Software Repositories, Software Engineering, Information Retrieval, Duplicate Crash Report, Duplicate Bug Report, Deduplication, Crash Stack, Crash Reports, Crash Report Deduplication, Automatic Problem Reporting Tools, Automatic Crash Reporting},
location = {Virtual, USA},
series = {MaLTeSQuE 2020}
}

@inproceedings{10.1145/3468264.3468545,
author = {Suneja, Sahil and Zheng, Yunhui and Zhuang, Yufan and Laredo, Jim A. and Morari, Alessandro},
title = {Probing model signal-awareness via prediction-preserving input minimization},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468545},
doi = {10.1145/3468264.3468545},
abstract = {This work explores the signal awareness of AI models for source code understanding. Using a software vulnerability detection use case, we evaluate the models' ability to capture the correct vulnerability signals to produce their predictions. Our prediction-preserving input minimization (P2IM) approach systematically reduces the original source code to a minimal snippet which a model needs to maintain its prediction. The model's reliance on incorrect signals is then uncovered when the vulnerability in the original code is missing in the minimal snippet, both of which the model however predicts as being vulnerable. We measure the signal awareness of models using a new metric we propose -- Signal-aware Recall (SAR). We apply P2IM on three different neural network architectures across multiple datasets. The results show a sharp drop in the model's Recall from the high 90s to sub-60s with the new metric, highlighting that the models are presumably picking up a lot of noise or dataset nuances while learning their vulnerability detection logic. Although the drop in model performance may be perceived as an adversarial attack, but this isn't P2IM's objective. The idea is rather to uncover the signal-awareness of a black-box model in a data-driven manner via controlled queries. SAR's purpose is to measure the impact of task-agnostic model training, and not to suggest a shortcoming in the Recall metric. The expectation, in fact, is for SAR to match Recall in the ideal scenario where the model truly captures task-specific signals.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {945–955},
numpages = {11},
keywords = {signal-aware recall, model signal-awareness, machine learning},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1109/ASE56229.2023.00094,
author = {Shar, Lwin Khin and Goknil, Arda and Husom, Erik Johannes and Sen, Sagar and Tun, Yan Naing and Kim, Kisub},
title = {AutoConf: Automated Configuration of Unsupervised Learning Systems Using Metamorphic Testing and Bayesian Optimization},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00094},
doi = {10.1109/ASE56229.2023.00094},
abstract = {Unsupervised learning systems using clustering have gained significant attention for numerous applications due to their unique ability to discover patterns and structures in large unlabeled datasets. However, their effectiveness highly depends on their configuration, which requires domain-specific expertise and often involves numerous manual trials. Specifically, selecting appropriate algorithms and hyperparameters adds to the complexity of the configuration process. In this paper, we propose, apply, and assess an automated approach (AutoConf) for configuring unsupervised learning systems using clustering, leveraging metamorphic testing and Bayesian optimization. Metamorphic testing is utilized to verify the configurations of unsupervised learning systems by applying a series of input transformations. We use Bayesian optimization guided by metamorphic-testing output to automatically identify the optimal configuration. The approach aims to streamline the configuration process and enhance the effectiveness of unsupervised learning systems. It has been evaluated through experiments on six datasets from three domains for anomaly detection. The evaluation results show that our approach can find configurations outperforming the baseline approaches as they achieved a recall of 0.89 and a precision of 0.84 (on average).},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1326–1338},
numpages = {13},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@article{10.1145/3672457,
author = {Tambon, Florian and Khomh, Foutse and Antoniol, Giuliano},
title = {GIST: Generated Inputs Sets Transferability in Deep Learning},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {8},
issn = {1049-331X},
url = {https://doi.org/10.1145/3672457},
doi = {10.1145/3672457},
abstract = {To foster the verifiability and testability of deep neural networks (DNN), an increasing number of methods for test case generation techniques are being developed.When confronted with testing DNN models, the user can apply any existing test generation technique. However, it needs to do so for each technique and each DNN model under test, which can be expensive. Therefore, a paradigm shift could benefit this testing process: rather than regenerating the test set independently for each DNN model under test, we could transfer from existing DNN models.This article introduces Generated Inputs Sets Transferability (GIST), a novel approach for the efficient transfer of test sets. Given a property selected by a user (e.g., neurons covered, faults), GIST enables the selection of good test sets from the point of view of this property among available test sets. This allows the user to recover similar properties on the transferred test sets as he would have obtained by generating the test set from scratch with a test cases generation technique. Experimental results show that GIST can select effective test sets for the given property to transfer. Moreover, GIST scales better than reapplying test case generation techniques from scratch on DNN models under test.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {214},
numpages = {38},
keywords = {Test sets generation, deep learning, DNN, testing, transferability}
}

@inproceedings{10.1145/3524842.3528009,
author = {Tufano, Michele and Deng, Shao Kun and Sundaresan, Neel and Svyatkovskiy, Alexey},
title = {Methods2Test: a dataset of focal methods mapped to test cases},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528009},
doi = {10.1145/3524842.3528009},
abstract = {Unit testing is an essential part of the software development process, which helps to identify issues with source code in early stages of development and prevent regressions. Machine learning has emerged as viable approach to help software developers generate automated unit tests. However, generating reliable unit test cases that are semantically correct and capable of catching software bugs or unintended behavior via machine learning requires large, metadata-rich, datasets. In this paper we present Methods2Test: a large, supervised dataset of test cases mapped to corresponding methods under test (i.e., focal methods). This dataset contains 780,944 pairs of JUnit tests and focal methods, extracted from a total of 91,385 Java open source projects hosted on GitHub with licenses permitting re-distribution. The main challenge behind the creation of the Methods2Test was to establish a reliable mapping between a test case and the relevant focal method. To this aim, we designed a set of heuristics, based on developers' best practices in software testing, which identify the likely focal method for a given test case. To facilitate further analysis, we store a rich set of metadata for each method-test pair in JSON-formatted files. Additionally, we extract textual corpus from the dataset at different context levels, which we provide both in raw and tokenized forms, in order to enable researchers to train and evaluate machine learning models for Automated Test Generation. Methods2Test is publicly available at: https://github.com/microsoft/methods2test},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {299–303},
numpages = {5},
keywords = {software testing, datasets},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1145/3475716.3475781,
author = {Croft, Roland and Newlands, Dominic and Chen, Ziyu and Babar, M. Ali},
title = {An Empirical Study of Rule-Based and Learning-Based Approaches for Static Application Security Testing},
year = {2021},
isbn = {9781450386654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475716.3475781},
doi = {10.1145/3475716.3475781},
abstract = {Background: Static Application Security Testing (SAST) tools purport to assist developers in detecting security issues in source code. These tools typically use rule-based approaches to scan source code for security vulnerabilities. However, due to the significant shortcomings of these tools (i.e., high false positive rates), learning-based approaches for Software Vulnerability Prediction (SVP) are becoming a popular approach. Aims: Despite the similar objectives of these two approaches, their comparative value is unexplored. We provide an empirical analysis of SAST tools and SVP models, to identify their relative capabilities for source code security analysis. Method: We evaluate the detection and assessment performance of several common SAST tools and SVP models on a variety of vulnerability datasets. We further assess the viability and potential benefits of combining the two approaches. Results: SAST tools and SVP models provide similar detection capabilities, but SVP models exhibit better overall performance for both detection and assessment. Unification of the two approaches is difficult due to lacking synergies. Conclusions: Our study generates 12 main findings which provide insights into the capabilities and synergy of these two approaches. Through these observations we provide recommendations for use and improvement.},
booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {8},
numpages = {12},
keywords = {Static Application Security Testing, Security, Machine Learning},
location = {Bari, Italy},
series = {ESEM '21}
}

@inproceedings{10.5555/3615924.3615927,
author = {Nascimento, Nathalia and Alencar, Paulo and Cowan, Donald},
title = {Artificial Intelligence vs. Software Engineers: An Empirical Study on Performance and Efficiency using ChatGPT},
year = {2023},
publisher = {IBM Corp.},
address = {USA},
abstract = {In the realm of Software Engineering (SE), automation has become a tangible reality. Artificial Intelligence (AI) has suc-cessfully addressed challenges in project management, mod-eling, testing, and development. Among the latest innova-tions is ChatGPT, an ML-infused chatbot capable of gen-erating programming codes and software testing strategies. Although there is speculation that AI-based computation can boost productivity and even substitute software engineers in software development, empirical evidence supporting such claims is lacking. Moreover, questions remain about their po-tential to address overlooked evaluation metrics like energy efficiency, vulnerability, fairness (i.e., human bias), and safety. This paper probes into these issues with an empirical study, comparing ChatGPT with both novice and expert program-mers using LeetCode contest problems. The investigation focuses on performance and memory-efficiency, while also acknowledging the need for a broader assessment of non-functional requirements. The results suggest that ChatGPT is better than beginners at solving easy and medium prob-lems, but it is not yet proven to beat expert programmers. This paper posits that a comprehensive comparison of soft-ware engineers and AI-based solutions, considering various evaluation criteria, is pivotal in fostering human-machine collaboration, enhancing the reliability of AI-based meth-ods, and understanding task suitability for humans or AI. Furthermore, it facilitates the effective implementation of co-operative work structures and human-in-the-loop processes.},
booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering},
pages = {24–33},
numpages = {10},
keywords = {Software Engineering, AI-based solutions, Performance Evaluation, ChatGPT, Machine Learning},
location = {Las Vegas, NV, USA},
series = {CASCON '23}
}

@inproceedings{10.1145/3643991.3644928,
author = {Shahini, Xhulja and Metzger, Andreas and Pohl, Klaus},
title = {An Empirical Study on Just-in-time Conformal Defect Prediction},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644928},
doi = {10.1145/3643991.3644928},
abstract = {Code changes can introduce defects that affect software quality and reliability. Just-in-time (JIT) defect prediction techniques provide feedback at check-in time on whether a code change is likely to contain defects. This immediate feedback allows practitioners to make timely decisions regarding potential defects. However, a prediction model may deliver false predictions, that may negatively affect practitioners' decisions. False positive predictions lead to unnecessarily spending resources on investigating clean code changes, while false negative predictions may result in overlooking defective changes. Knowing how uncertain a defect prediction is, would help practitioners to avoid wrong decisions. Previous research in defect prediction explored different approaches to quantify prediction uncertainty for supporting decision-making activities. However, these approaches only offer a heuristic quantification of uncertainty and do not provide guarantees.In this study, we use conformal prediction (CP) as a rigorous uncertainty quantification approach on top of JIT defect predictors. We assess how often CP can provide guarantees for JIT defect predictions. We also assess how many false JIT defect predictions CP can filter out. We experiment with two state-of-the-art JIT defect prediction techniques (DeepJIT and CC2Vec) and two widely used datasets (Qt and OpenStack).Our experiments show that CP can ensure correctness with a 95% probability, for only 27% (for DeepJIT) and 9% (for CC2Vec) of the JIT defect predictions. Additionally, our experiments indicate that CP might be a valuable technique for filtering out the false predictions of JIT defect predictors. CP can filter out up to 100% of false negative predictions and 90% of false positives generated by CC2Vec, and up to 86% of false negative predictions and 83% of false positives generated by DeepJIT.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {88–99},
numpages = {12},
keywords = {defect prediction, quality assurance, conformal prediction, machine learning, deep learning, correctness guarantees, uncertainty},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@article{10.1145/3564821,
author = {Di Sorbo, Andrea and Zampetti, Fiorella and Visaggio, Aaron and Di Penta, Massimiliano and Panichella, Sebastiano},
title = {Automated Identification and Qualitative Characterization of Safety Concerns Reported in UAV Software Platforms},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3564821},
doi = {10.1145/3564821},
abstract = {Unmanned Aerial Vehicles (UAVs) are nowadays used in a variety of applications. Given the cyber-physical nature of UAVs, software defects in these systems can cause issues with safety-critical implications. An important aspect of the lifecycle of UAV software is to minimize the possibility of harming humans or damaging properties through a continuous process of hazard identification and safety risk management. Specifically, safety-related concerns typically emerge during the operation of UAV systems, reported by end-users and developers in the form of issue reports and pull requests. However, popular UAV systems daily receive tens or hundreds of reports of varying types and quality. To help developers timely identify and triage safety-critical UAV issues, we (i) experiment with automated approaches (previously used for issue classification) for detecting the safety-related matters appearing in the titles and descriptions of issues and pull requests reported in UAV platforms and (ii) propose a categorization of the main hazards and accidents discussed in such issues. Our results (i) show that shallow machine learning (ML)-based approaches can identify safety-related sentences with precision, recall, and F-measure values of about 80%; and (ii) provide a categorization and description of the relationships between safety issue hazards and accidents.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {67},
numpages = {37},
keywords = {empirical study, machine learning, safety issues, issue management, Unmanned aerial vehicles}
}

@inproceedings{10.1145/3522664.3528596,
author = {Song, Qunying and Borg, Markus and Engstr\"{o}m, Emelie and Ard\"{o}, H\r{a}kan and Rico, Sergio},
title = {Exploring ML testing in practice: lessons learned from an interactive rapid review with axis communications},
year = {2022},
isbn = {9781450392754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3522664.3528596},
doi = {10.1145/3522664.3528596},
abstract = {There is a growing interest in industry and academia in machine learning (ML) testing. We believe that industry and academia need to learn together to produce rigorous and relevant knowledge. In this study, we initiate a collaboration between stakeholders from one case company, one research institute, and one university. To establish a common view of the problem domain, we applied an interactive rapid review of the state of the art. Four researchers from Lund University and RISE Research Institutes and four practitioners from Axis Communications reviewed a set of 180 primary studies on ML testing. We developed a taxonomy for the communication around ML testing challenges and results and identified a list of 12 review questions relevant for Axis Communications. The three most important questions (data testing, metrics for assessment, and test generation) were mapped to the literature, and an in-depth analysis of the 35 primary studies matching the most important question (data testing) was made. A final set of the five best matches were analysed and we reflect on the criteria for applicability and relevance for the industry. The taxonomies are helpful for communication but not final. Furthermore, there was no perfect match to the case company's investigated review question (data testing). However, we extracted relevant approaches from the five studies on a conceptual level to support later context-specific improvements. We found the interactive rapid review approach useful for triggering and aligning communication between the different stakeholders.},
booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
pages = {10–21},
numpages = {12},
keywords = {AI engineering, interactive rapid review, machine learning, taxonomy, testing},
location = {Pittsburgh, Pennsylvania},
series = {CAIN '22}
}

@inproceedings{10.1145/3452383.3452392,
author = {Mondal, Saikat and Saifullah, C M Khaled and Bhattacharjee, Avijit and Rahman, Mohammad Masudur and Roy, Chanchal K.},
title = {Early Detection and Guidelines to Improve Unanswered Questions on Stack Overflow},
year = {2021},
isbn = {9781450390460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452383.3452392},
doi = {10.1145/3452383.3452392},
abstract = {Stack Overflow is one of the largest and most popular question-answering (Q&amp;A) websites. It accumulates millions of programming related questions and answers to support the developers in software development. Unfortunately, a large number of questions are not answered at all, which might hurt the quality or purpose of this community-oriented knowledge base. Up to 29% of Stack Overflow questions do not have any answers. There have been existing attempts in detecting the unanswered questions. Unfortunately, they primarily rely on the question attributes (e.g., score, view count) that are not available during the submission of a question. Detection of the potentially unanswered questions in advance during question submission could help one improve the question and thus receive the answers in time. In this paper, we compare unanswered and answered questions quantitatively and qualitatively by analyzing a total of 4.8 million questions from Stack Overflow. We find that topics discussed in the question, the experience of the question submitter, and readability of question texts could often determine whether a question would be answered or not. Our qualitative study also reveals several other non-trivial factors that not only explain (partially) why the questions remain unanswered but also guide the novice users to improve their questions. We develop four machine learning models to predict the unanswered questions during their submission. According to the experiments, our models predict the unanswered questions with a maximum of about 79% accuracy and significantly outperform the state-of-the-art prediction models.},
booktitle = {Proceedings of the 14th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {9},
numpages = {11},
keywords = {Stack Overflow, machine learning, prediction model, question attributes, unanswered questions},
location = {Bhubaneswar, Odisha, India},
series = {ISEC '21}
}

@article{10.1145/3660793,
author = {Rafi, Md Nakhla and Kim, Dong Jae and Chen, An Ran and Chen, Tse-Hsun (Peter) and Wang, Shaowei},
title = {Towards Better Graph Neural Network-Based Fault Localization through Enhanced Code Representation},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660793},
doi = {10.1145/3660793},
abstract = {Automatic software fault localization plays an important role in software quality assurance by pinpointing faulty locations for easier debugging. Coverage-based fault localization is a commonly used technique, which applies statistics on coverage spectra to rank faulty code based on suspiciousness scores. However, statistics- based approaches based on formulae are often rigid, which calls for learning-based techniques. Amongst all, Grace, a graph-neural network (GNN) based technique has achieved state-of-the-art due to its capacity to preserve coverage spectra, i.e., test-to-source coverage relationships, as precise abstract syntax-enhanced graph representation, mitigating the limitation of other learning-based technique which compresses the feature representation. However, such representation is not scalable due to the increasing complexity of software, correlating with increasing coverage spectra and AST graph, making it challenging to extend, let alone train the graph neural network in practice. In this work, we proposed a new graph representation, DepGraph, that reduces the complexity of the graph representation by 70% in nodes and edges by integrating the interprocedural call graph in the graph representation of the code. Moreover, we integrate additional features—code change information—into the graph as attributes so the model can leverage rich historical project data. We evaluate DepGraph using Defects4j 2.0.0, and it outperforms Grace by locating 20% more faults in Top-1 and improving the Mean First Rank (MFR) and the Mean Average Rank (MAR) by over 50% while decreasing GPU memory usage by 44% and training/inference time by 85%. Additionally, in cross-project settings, DepGraph surpasses the state-of-the-art baseline with a 42% higher Top-1 accuracy, and 68% and 65% improvement in MFR and MAR, respectively. Our study demonstrates DepGraph’s robustness, achieving state-of-the-art accuracy and scalability for future extension and adoption.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {86},
numpages = {23},
keywords = {Debugging, Fault Localization, Graph Neural Networks}
}

@inproceedings{10.1145/3639477.3639712,
author = {Hrusto, Adha and Runeson, Per and Ohlsson, Magnus C},
title = {Autonomous Monitors for Detecting Failures Early and Reporting Interpretable Alerts in Cloud Operations},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639712},
doi = {10.1145/3639477.3639712},
abstract = {Detecting failures early in cloud-based software systems is highly significant as it can reduce operational costs, enhance service reliability, and improve user experience. Many existing approaches include anomaly detection in metrics or a blend of metric and log features. However, such approaches tend to be very complex and hardly explainable, and consequently non-trivial for implementation and evaluation in industrial contexts. In collaboration with a case company and their cloud-based system in the domain of PIM (Product Information Management), we propose and implement autonomous monitors for proactive monitoring across multiple services of distributed software architecture, fused with anomaly detection in performance metrics and log analysis using GPT-3. We demonstrated that operations engineers tend to be more efficient by having access to interpretable alert notifications based on detected anomalies that contain information about implications and potential root causes. Additionally, proposed autonomous monitors turned out to be beneficial for the timely identification and revision of potential issues before they propagate and cause severe consequences.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {47–57},
numpages = {11},
keywords = {cloud, monitoring, anomaly detection, failures},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3278186.3278194,
author = {Santiago, Dionny and Clarke, Peter J. and Alt, Patrick and King, Tariq M.},
title = {Abstract flow learning for web application test generation},
year = {2018},
isbn = {9781450360531},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278186.3278194},
doi = {10.1145/3278186.3278194},
abstract = {Achieving high software quality today involves manual analysis, test planning, documentation of testing strategy and test cases, and the development of scripts to support automated regression testing. To keep pace with software evolution, test artifacts must also be frequently updated. Although test automation practices help mitigate the cost of regression testing, a large gap exists between the current paradigm and fully automated software testing. Researchers and practitioners are realizing the potential for artificial intelligence and machine learning (ML) to help bridge the gap between the testing capabilities of humans and those of machines. This paper presents an ML approach that combines a language specification that includes a grammar that can be used to describe test flows, and a trainable test flow generation model, in order to generate tests in a way that is trainable, reusable across different applications, and generalizable to new applications.},
booktitle = {Proceedings of the 9th ACM SIGSOFT International Workshop on Automating TEST Case Design, Selection, and Evaluation},
pages = {49–55},
numpages = {7},
keywords = {Testing, Test generation, Machine learning, Language, Automation},
location = {Lake Buena Vista, FL, USA},
series = {A-TEST 2018}
}

@inproceedings{10.1145/3583131.3590379,
author = {Applis, Leonhard and Panichella, Annibale and Marang, Ruben},
title = {Searching for Quality: Genetic Algorithms and Metamorphic Testing for Software Engineering ML},
year = {2023},
isbn = {9798400701191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583131.3590379},
doi = {10.1145/3583131.3590379},
abstract = {More machine learning (ML) models are introduced to the field of Software Engineering (SE) and reached a stage of maturity to be considered for real-world use; But the real world is complex, and testing these models lacks often in explainability, feasibility and computational capacities. Existing research introduced meta-morphic testing to gain additional insights and certainty about the model, by applying semantic-preserving changes to input-data while observing model-output. As this is currently done at random places, it can lead to potentially unrealistic datapoints and high computational costs. With this work, we introduce genetic search as an aid for metamorphic testing in SE ML. Exploiting the delta in output as a fitness function, the evolutionary intelligence optimizes the transformations to produce higher deltas with less changes. We perform a case study minimizing F1 and MRR for Code2Vec on a representative sample from java-small with both genetic and random search. Our results show that within the same amount of time, genetic search was able to achieve a decrease of 10% in F1 while random search produced 3% drop.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1490–1498},
numpages = {9},
location = {Lisbon, Portugal},
series = {GECCO '23}
}

@article{10.1145/3384517,
author = {Kapur, Ritu and Sodhi, Balwinder},
title = {A Defect Estimator for Source Code: Linking Defect Reports with Programming Constructs Usage Metrics},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3384517},
doi = {10.1145/3384517},
abstract = {An important issue faced during software development is to identify defects and the properties of those defects, if found, in a given source file. Determining defectiveness of source code assumes significance due to its implications on software development and maintenance cost.We present a novel system to estimate the presence of defects in source code and detect attributes of the possible defects, such as the severity of defects. The salient elements of our system are: (i) a dataset of newly introduced source code metrics, called PROgramming CONstruct (PROCON) metrics, and (ii) a novel Machine-Learning (ML)-based system, called Defect Estimator for Source Code (DESCo), that makes use of PROCON dataset for predicting defectiveness in a given scenario. The dataset was created by processing 30,400+ source files written in four popular programming languages, viz., C, C++, Java, and Python.The results of our experiments show that DESCo system outperforms one of the state-of-the-art methods with an improvement of 44.9%. To verify the correctness of our system, we compared the performance of 12 different ML algorithms with 50+ different combinations of their key parameters. Our system achieves the best results with SVM technique with a mean accuracy measure of 80.8%.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {12},
numpages = {35},
keywords = {source code mining, software metrics, software faults and failures, software defect prediction, automated software engineering, Maintaining software, AI in software engineering}
}

@inproceedings{10.1145/3338906.3340450,
author = {Chen, Jianfeng and Chakraborty, Joymallya and Clark, Philip and Haverlock, Kevin and Cherian, Snehit and Menzies, Tim},
title = {Predicting breakdowns in cloud services (with SPIKE)},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3340450},
doi = {10.1145/3338906.3340450},
abstract = {Maintaining web-services is a mission-critical task where any down- time means loss of revenue and reputation (of being a reliable service provider). In the current competitive web services market, such a loss of reputation causes extensive loss of future revenue.  To address this issue, we developed SPIKE, a data mining tool which can predict upcoming service breakdowns, half an hour into the future. Such predictions let an organization alert and assemble the tiger team to address the problem (e.g. by reconguring cloud hardware in order to reduce the likelihood of that breakdown).  SPIKE utilizes (a) regression tree learning (with CART); (b) synthetic minority over-sampling (to handle how rare spikes are in our data); (c) hyperparameter optimization (to learn best settings for our local data) and (d) a technique we called “topology sampling” where training vectors are built from extensive details of an individual node plus summary details on all their neighbors.  In the experiments reported here, SPIKE predicted service spikes 30 minutes into future with recalls and precision of 75% and above. Also, SPIKE performed relatively better than other widely-used learning methods (neural nets, random forests, logistic regression).},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {916–924},
numpages = {9},
keywords = {parameter tuning, optimization, data mining, Cloud},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3651640.3651644,
author = {Sch\"{u}tz, Martin and Pl\"{o}sch, Reinhold},
title = {A Practical Failure Prediction Model based on Code Smells and Software Development Metrics},
year = {2024},
isbn = {9798400708817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3651640.3651644},
doi = {10.1145/3651640.3651644},
abstract = {Making errors during software development is unavoidable. Developers inevitably make errors that take additional time to fix later. Consequently, efforts for bug fixing compete with implementing new features. Typically, the later bugs are found, the higher the cost for remediation. To address this concern, software testing should start as early as possible in software development lifecycle. For this purpose, static analysis is proposed, but typically shows too many findings and hence do not support development teams appropriately. So, it would be a benefit to premature detect those findings in static analysis that will result in failures to reduce subsequent efforts notably. The purpose of the paper is to analyze failure data from issue tracking systems that are correlated to findings from static analysis. Thereupon an artificial intelligence-based approach is used to train practicable models for business environment that enables effective prediction of software faults. The results from static analysis show that predefined complexity measures encompassed the most defects. While there are commonalities in relevant defect findings in static analysis reports, meaningful prediction models cannot be expected based solely on this data. In addition to the findings of the static analysis, metrics like code changes in a time period or number of authors involved in code changes were considered for building the prediction models. Two of the developed prediction models have a high accuracy and excellent utility rate. These resulting prediction models are currently used at Raiffeisen Software GmbH for a long-term study on failure prediction based on code smells.},
booktitle = {Proceedings of the 4th European Symposium on Software Engineering},
pages = {14–22},
numpages = {9},
keywords = {change metrics and failure prediction, failure prediction, machine learning for failure prediction, static analysis, technical debt},
location = {Napoli, Italy},
series = {ESSE '23}
}

@inproceedings{10.1145/2070821.2070829,
author = {Zhang, Dongmei and Dang, Yingnong and Lou, Jian-Guang and Han, Shi and Zhang, Haidong and Xie, Tao},
title = {Software analytics as a learning case in practice: approaches and experiences},
year = {2011},
isbn = {9781450310222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070821.2070829},
doi = {10.1145/2070821.2070829},
abstract = {Software analytics is to enable software practitioners to perform data exploration and analysis in order to obtain insightful and actionable information for data-driven tasks around software and services. In this position paper, we advocate that when applying analytic technologies in practice of software analytics, one should (1) incorporate a broad spectrum of domain knowledge and expertise, e.g., management, machine learning, large-scale data processing and computing, and information visualization; and (2) investigate how practitioners take actions on the produced information, and provide effective support for such information-based action taking. Our position is based on our experiences of successful technology transfer on software analytics at Microsoft Research Asia.},
booktitle = {Proceedings of the International Workshop on Machine Learning Technologies in Software Engineering},
pages = {55–58},
numpages = {4},
keywords = {technology transfer, software analytics, machine learning},
location = {Lawrence, Kansas, USA},
series = {MALETS '11}
}

@article{10.1145/3576039,
author = {Tian, Haoye and Liu, Kui and Li, Yinghua and Kabor\'{e}, Abdoul Kader and Koyuncu, Anil and Habib, Andrew and Li, Li and Wen, Junhao and Klein, Jacques and Bissyand\'{e}, Tegawend\'{e} F.},
title = {The Best of Both Worlds: Combining Learned Embeddings with Engineered Features for Accurate Prediction of Correct Patches},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3576039},
doi = {10.1145/3576039},
abstract = {A large body of the literature on automated program repair develops approaches where patches are automatically generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. While the state-of-the-art explores research directions that require dynamic information or rely on manually-crafted heuristics, we study the benefit of learning code representations in order to learn deep features that may encode the properties of patch correctness. Our empirical work investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations of patch correctness identification, and assess the possibility of accurate classification of correct patch by combining learned embeddings with engineered features. Experimental results demonstrate the potential of learned embeddings to empower Leopard (a patch correctness predicting framework implemented in this work) with learning algorithms in reasoning about patch correctness: a machine learning predictor with BERT transformer-based learned embeddings associated with XGBoost achieves an AUC value of about 0.803 in the prediction of patch correctness on a new dataset of 2,147 labeled patches that we collected for the experiments. Our investigations show that deep learned embeddings can lead to complementary/better performance when comparing against the state-of-the-art, PATCH-SIM, which relies on dynamic information. By combining deep learned embeddings and engineered features, Panther (the upgraded version of Leopard implemented in this work) outperforms Leopard with higher scores in terms of AUC, +Recall and -Recall, and can accurately identify more (in)correct patches that cannot be predicted by the classifiers only with learned embeddings or engineered features. Finally, we use an explainable ML technique, SHAP, to empirically interpret how the learned embeddings and engineered features are contributed to the patch correctness prediction.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {92},
numpages = {34},
keywords = {explanation, features combination, embeddings, machine learning, distributed representation learning, patch correctness, Program repair}
}

@inproceedings{10.1145/2931037.2931039,
author = {Bowes, David and Hall, Tracy and Harman, Mark and Jia, Yue and Sarro, Federica and Wu, Fan},
title = {Mutation-aware fault prediction},
year = {2016},
isbn = {9781450343909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2931037.2931039},
doi = {10.1145/2931037.2931039},
abstract = {We introduce mutation-aware fault prediction, which leverages additional guidance from metrics constructed in terms of mutants and the test cases that cover and detect them. We report the results of 12 sets of experiments, applying 4 different predictive modelling techniques to 3 large real-world systems (both open and closed source). The results show that our proposal can significantly (p ≤ 0.05) improve fault prediction performance. Moreover, mutation-based metrics lie in the top 5% most frequently relied upon fault predictors in 10 of the 12 sets of experiments, and provide the majority of the top ten fault predictors in 9 of the 12 sets of experiments.},
booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
pages = {330–341},
numpages = {12},
keywords = {Software Metrics, Software Fault Prediction, Software Defect Prediction, Mutation Testing, Empirical Study},
location = {Saarbr\"{u}cken, Germany},
series = {ISSTA 2016}
}

@inproceedings{10.1145/3243127.3243130,
author = {Ognawala, Saahil and Amato, Ricardo Nales and Pretschner, Alexander and Kulkarni, Pooja},
title = {Automatically assessing vulnerabilities discovered by compositional analysis},
year = {2018},
isbn = {9781450359726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243127.3243130},
doi = {10.1145/3243127.3243130},
abstract = {Testing is the most widely employed method to find vulnerabilities in real-world software programs. Compositional analysis, based on symbolic execution, is an automated testing method to find vulnerabilities in medium- to large-scale programs consisting of many interacting components. However, existing compositional analysis frameworks do not assess the severity of reported vulnerabilities. In this paper, we present a framework to analyze vulnerabilities discovered by an existing compositional analysis tool and assign CVSS3 (Common Vulnerability Scoring System v3.0) scores to them, based on various heuristics such as interaction with related components, ease of reachability, complexity of design and likelihood of accepting unsanitized input. By analyzing vulnerabilities reported with CVSS3 scores in the past, we train simple machine learning models. By presenting our interactive framework to developers of popular open-source software and other security experts, we gather feedback on our trained models and further improve the features to increase the accuracy of our predictions. By providing qualitative (based on community feedback) and quantitative (based on prediction accuracy) evidence from 21 open-source programs, we show that our severity prediction framework can effectively assist developers with assessing vulnerabilities.},
booktitle = {Proceedings of the 1st International Workshop on Machine Learning and Software Engineering in Symbiosis},
pages = {16–25},
numpages = {10},
keywords = {vulnerability assessment, symbolic execution, software testing, compositional analysis},
location = {Montpellier, France},
series = {MASES 2018}
}

@article{10.1145/3345628,
author = {Kim, Yunho and Mun, Seokhyeon and Yoo, Shin and Kim, Moonzoo},
title = {Precise Learn-to-Rank Fault Localization Using Dynamic and Static Features of Target Programs},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3345628},
doi = {10.1145/3345628},
abstract = {Finding the root cause of a bug requires a significant effort from developers. Automated fault localization techniques seek to reduce this cost by computing the suspiciousness scores (i.e., the likelihood of program entities being faulty). Existing techniques have been developed by utilizing input features of specific types for the computation of suspiciousness scores, such as program spectrum or mutation analysis results. This article presents a novel learn-to-rank fault localization technique called PRecise machINe-learning-based fault loCalization tEchnique (PRINCE). PRINCE uses genetic programming (GP) to combine multiple sets of localization input features that have been studied separately until now. For dynamic features, PRINCE encompasses both Spectrum Based Fault Localization (SBFL) and Mutation Based Fault Localization (MBFL) techniques. It also uses static features, such as dependency information and structural complexity of program entities. All such information is used by GP to train a ranking model for fault localization. The empirical evaluation on 65 real-world faults from CoREBench, 84 artificial faults from SIR, and 310 real-world faults from Defects4J shows that PRINCE outperforms the state-of-the-art SBFL, MBFL, and learn-to-rank techniques significantly. PRINCE localizes a fault after reviewing 2.4% of the executed statements on average (4.2 and 3.0 times more precise than the best of the compared SBFL and MBFL techniques, respectively). Also, PRINCE ranks 52.9% of the target faults within the top ten suspicious statements.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = oct,
articleno = {23},
numpages = {34},
keywords = {source file characteristics, mutation analysis, machine learning, Fault localization}
}

@article{10.1145/3695996,
author = {Wang, Guoqing and Sun, Zeyu and Dong, Jinhao and Zhang, Yuxia and Zhu, Mingxuan and Liang, Qingyuan and Hao, Dan},
title = {Is It Hard to Generate Holistic Commit Message?},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3695996},
doi = {10.1145/3695996},
abstract = {Commit messages are important for developers to understand the content and the reason for code changes. However, poor and even empty commit messages widely exist. To improve the quality of commit messages and development efficiency, many commit message generation methods have been proposed. Nevertheless, previous methods mainly focus on a brief generation problem, where both the input code change and the output commit messages are restricted to short. This may initiate a debate on the performance of these methods in practice. In this article, we attempt to remove the restrictions and move the needle forward to a holistic commit message generation problem. In particular, we conduct experiments to evaluate the performance of existing commit message generation methods in holistic commit message generation. In the experiments, we choose seven state-of-the-art commit generation methods and focus on two important scenarios in commit message generation (i.e., the within-project scenario and the cross-project scenario). To conduct our experiments, we publish a holistic commit message dataset HORDA with test data manually labeled. In our evaluations, we find that in generating holistic commit messages, the IR-based method has a better performance than non-pre-trained generation-based methods in the within-project scenario, contradicting previous research findings. Further, while the pre-trained generation-based methods are better than non-pre-trained generation-based methods, they are still constrained by the limitations of generation models.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {36},
numpages = {28},
keywords = {Empirical Study, Commit Message Generation, Holistic Commit Message Generation, Machine Learning}
}

@inproceedings{10.1145/3591569.3591612,
author = {Nguyen Thi, Hien and Phan, Thi-Thu-Hong and Tran, Cao Truong},
title = {Genetic Programming for Bee Audio Classification},
year = {2023},
isbn = {9781450399616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591569.3591612},
doi = {10.1145/3591569.3591612},
abstract = {Honey bees (Apis mellifera) play a very important role in agriculture thanks to their ability of plants’ pollination. However, the number of honey bees decreases every year because of the effects of climate change, environmental pollution, and so on. As a result, finding a useful solution to this problem has been more and more attracting scientists and companies. Applying machine learning (ML) methods based on audio data recording inside the hive is a promising solution to detect changes in the beehive. In this study, we investigate the genetic programming (GP) method, one of the powerful ML methods, for identifying bee sound data. We also compare our proposal with the results from a previous study. The experiment results show that with the right configuration of parameters, GP can achieve better results than well-known methods for the task of classifying bee sound samples.},
booktitle = {Proceedings of the 2023 8th International Conference on Intelligent Information Technology},
pages = {246–250},
numpages = {5},
keywords = {Parameter setting, Machine learning, Genetic programming, Convolutional neural networks, Audio classification},
location = {Da Nang, Vietnam},
series = {ICIIT '23}
}

@inproceedings{10.1109/ASE56229.2023.00161,
author = {Kur, Justin and Chen, Jingshu and Huang, Jun},
title = {Scalable Industrial Control System Analysis via XAI-Based Gray-Box Fuzzing},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00161},
doi = {10.1109/ASE56229.2023.00161},
abstract = {Conventional approaches to analyzing industrial control systems have relied on either white-box analysis or blackbox fuzzing. However, white-box methods rely on sophisticated domain expertise, while black-box methods suffers from state explosion and thus scales poorly when analyzing real ICS involving a large number of sensors and actuators. To address these limitations, we propose XAI-based gray-box fuzzing, a novel approach that leverages explainable AI and machine learning modeling of ICS to accurately identify a small set of actuators critical to ICS safety, which result in significant reduction of state space without relying on domain expertise. Experiment results show that our method accurately explains the ICS model and significantly speeds-up fuzzing by 64x when compared to conventional black-box methods.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1803–1807},
numpages = {5},
keywords = {fuzzing, industrial control systems, learning based approaches, explainable AI, security attack},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@article{10.1145/3447876,
author = {Lyu, Yingzhe and Li, Heng and Sayagh, Mohammed and Jiang, Zhen Ming (Jack) and Hassan, Ahmed E.},
title = {An Empirical Study of the Impact of Data Splitting Decisions on the Performance of AIOps Solutions},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3447876},
doi = {10.1145/3447876},
abstract = {AIOps (Artificial Intelligence for IT Operations) leverages machine learning models to help practitioners handle the massive data produced during the operations of large-scale systems. However, due to the nature of the operation data, AIOps modeling faces several data splitting-related challenges, such as imbalanced data, data leakage, and concept drift. In this work, we study the data leakage and concept drift challenges in the context of AIOps and evaluate the impact of different modeling decisions on such challenges. Specifically, we perform a case study on two commonly studied AIOps applications: (1) predicting job failures based on trace data from a large-scale cluster environment and (2) predicting disk failures based on disk monitoring data from a large-scale cloud storage environment. First, we observe that the data leakage issue exists in AIOps solutions. Using a time-based splitting of training and validation datasets can significantly reduce such data leakage, making it more appropriate than using a random splitting in the AIOps context. Second, we show that AIOps solutions suffer from concept drift. Periodically updating AIOps models can help mitigate the impact of such concept drift, while the performance benefit and the modeling cost of increasing the update frequency depend largely on the application data and the used models. Our findings encourage future studies and practices on developing AIOps solutions to pay attention to their data-splitting decisions to handle the data leakage and concept drift challenges.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {54},
numpages = {38},
keywords = {model maintenance, machine learning engineering, failure prediction, data leakage, concept drift, AIOps}
}

@inproceedings{10.1145/3558489.3559069,
author = {Coskun, Tugce and Halepmollasi, Rusen and Hanifi, Khadija and Fouladi, Ramin Fadaei and De Cnudde, Pinar Comak and Tosun, Ayse},
title = {Profiling developers to predict vulnerable code changes},
year = {2022},
isbn = {9781450398602},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3558489.3559069},
doi = {10.1145/3558489.3559069},
abstract = {Software vulnerability prediction and management have caught the interest of researchers and practitioners, recently. Various techniques that are usually based on characteristics of the code artefacts are also offered to predict software vulnerabilities. While other studies achieve promising results, the role of developers in inducing vulnerabilities has not been studied yet. We aim to profile the vulnerability inducing and vulnerability fixing behaviors of developers in software projects using Heterogeneous Information Network (HIN) analysis. We also investigate the impact of developer profiles in predicting vulnerability inducing commits, and compare the findings against the approach based on the code metrics. We adopt Random Walk with Restart (RWR) algorithm on HIN and the aggregation of code metrics for extracting all the input features. We utilize traditional machine learning algorithms namely, Naive Bayes (NB), Support Vector Machine (SVM), Random Forest (RF) and eXtreme Gradient Boosting (XGBoost) to build the prediction models.We report our empirical analysis to predict vulnerability inducing commits of four Apache projects. The technique based on code metrics achieves 90% success for the recall measure, whereas the technique based on profiling developer behavior achieves 71% success. When we use the feature sets obtained with the two techniques together, we achieve 89% success.},
booktitle = {Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {32–41},
numpages = {10},
keywords = {vulnerability prediction, vulnerability, technical debt, profiling developers},
location = {Singapore, Singapore},
series = {PROMISE 2022}
}

@inproceedings{10.1145/3402842.3407158,
author = {Sun, Jun and Yang, Zijiang},
title = {ObjSim: efficient testing of cyber-physical systems},
year = {2020},
isbn = {9781450380324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3402842.3407158},
doi = {10.1145/3402842.3407158},
abstract = {Cyber-physical systems (CPSs) play a critical role in automating public infrastructure and thus attract wide range of attacks. Assessing the effectiveness of defense mechanisms is challenging as realistic sets of attacks to test them against are not always available. In this short paper, we briefly describe smart fuzzing, an automated, machine learning guided technique for systematically producing test suites of CPS network attacks. Our approach uses predictive ma- chine learning models and meta-heuristic search algorithms to guide the fuzzing of actuators so as to drive the CPS into different unsafe physical states. The approach has been proven effective on two real-world CPS testbeds.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Testing, Analysis, and Verification of Cyber-Physical Systems and Internet of Things},
pages = {1–2},
numpages = {2},
keywords = {testing, network, machine learning, fuzzing, cyber-physical system},
location = {Virtual Event, USA},
series = {TAV-CPS/IoT 2020}
}

@inproceedings{10.1145/3460319.3464840,
author = {Pan, Cong and Pradel, Michael},
title = {Continuous test suite failure prediction},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464840},
doi = {10.1145/3460319.3464840},
abstract = {Continuous integration advocates to run the test suite of a project frequently, e.g., for every code change committed to a shared repository. This process imposes a high computational cost and sometimes also a high human cost, e.g., when developers must wait for the test suite to pass before a change appears in the main branch of the shared repository. However, only 4% of all test suite invocations turn a previously passing test suite into a failing test suite. The question arises whether running the test suite for each code change is really necessary. This paper presents continuous test suite failure prediction, which reduces the cost of continuous integration by predicting whether a particular code change should trigger the test suite at all. The core of the approach is a machine learning model based on features of the code change, the test suite, and the development history. We also present a theoretical cost model that describes when continuous test suite failure prediction is worthwhile. Evaluating the idea with 15k test suite runs from 242 open-source projects shows that the approach is effective at predicting whether running the test suite is likely to reveal a test failure. Moreover, we find that our approach improves the AUC over baselines that use features proposed for just-in-time defect prediction and test case failure prediction by 13.9% and 2.9%, respectively. Overall, continuous test suite failure prediction can significantly reduce the cost of continuous integration.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {553–565},
numpages = {13},
keywords = {machine learning, cost model, continuous test suite failure prediction, continuous integration},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1145/3650212.3680401,
author = {Pinconschi, Eduard and Gopinath, Divya and Abreu, Rui and P\u{a}s\u{a}reanu, Corina S.},
title = {Evaluating Deep Neural Networks in Deployment: A Comparative Study (Replicability Study)},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680401},
doi = {10.1145/3650212.3680401},
abstract = {As deep neural networks (DNNs) are increasingly used in safety-critical applications, there is a growing concern for their reliability. Even highly trained, high-performant networks are not 100% accurate. However, it is very difficult to predict their behavior during deployment without ground truth. In this paper, we provide a comparative and replicability study on recent approaches that have been proposed to evaluate the reliability of DNNs in deployment. We find that it is hard to run and reproduce the results for these approaches on their replication packages and even more difficult to run them on artifacts other than their own. Further, it is difficult to compare the effectiveness of the approaches, due to the lack of clearly defined evaluation metrics. Our results indicate that more effort is needed in our research community to obtain sound techniques for evaluating the reliability of neural networks in safety-critical domains. To this end, we contribute an evaluation framework that incorporates the considered approaches and enables evaluation on common benchmarks, using common metrics.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1300–1311},
numpages = {12},
keywords = {Neural Networks, Testing, Trustworthy AI},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@article{10.1145/3640335,
author = {Neelofar, Neelofar and Aleti, Aldeida},
title = {Identifying and Explaining Safety-critical Scenarios for Autonomous Vehicles via Key Features},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3640335},
doi = {10.1145/3640335},
abstract = {Ensuring the safety of autonomous vehicles (AVs) is of utmost importance, and testing them in simulated environments is a safer option than conducting in-field operational tests. However, generating an exhaustive test suite to identify critical test scenarios is computationally expensive, as the representation of each test is complex and contains various dynamic and static features, such as the AV under test, road participants (vehicles, pedestrians, and static obstacles), environmental factors (weather and light), and the road’s structural features (lanes, turns, road speed, etc.). In this article, we present a systematic technique that uses Instance Space Analysis (ISA) to identify the significant features of test scenarios that affect their ability to reveal the unsafe behaviour of AVs. ISA identifies the features that best differentiate safety-critical scenarios from normal driving and visualises the impact of these features on test scenario outcomes (safe/unsafe) in two dimensions. This visualisation helps to identify untested regions of the instance space and provides an indicator of the quality of the test suite in terms of the percentage of feature space covered by testing. To test the predictive ability of the identified features, we train five Machine Learning classifiers to classify test scenarios as safe or unsafe. The high precision, recall, and F1 scores indicate that our proposed approach is effective in predicting the outcome of a test scenario without executing it and can be used for test generation, selection, and prioritisation.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {94},
numpages = {32},
keywords = {Testing autonomous vehicles, feature-impact analysis, instance space analysis, search-based software testing}
}

@inproceedings{10.1109/ASE56229.2023.00148,
author = {Feldt, Robert and Kang, Sungmin and Yoon, Juyeon and Yoo, Shin},
title = {Towards Autonomous Testing Agents via Conversational Large Language Models},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00148},
doi = {10.1109/ASE56229.2023.00148},
abstract = {Software testing is an important part of the development cycle, yet it requires specialized expertise and substantial developer effort to adequately test software. Recent discoveries of the capabilities of large language models (LLMs) suggest that they can be used as automated testing assistants, and thus provide helpful information and even drive the testing process. To highlight the potential of this technology, we present a taxonomy of LLM-based testing agents based on their level of autonomy, and describe how a greater level of autonomy can benefit developers in practice. An example use of LLMs as a testing assistant is provided to demonstrate how a conversational framework for testing can help developers. This also highlights how the often criticized "hallucination" of LLMs can be beneficial for testing. We identify other tangible benefits that LLM-driven testing agents can bestow, and also discuss potential limitations.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1688–1693},
numpages = {6},
keywords = {software testing, machine learning, large language model, artificial intelligence, test automation},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3460620.3460752,
author = {M.S.B, Phridviraj and C.V, Guru Rao and Vangipuram, Radhakrishna and Cheruvu, Aravind},
title = {Similarity Association Pattern Mining in Transaction Databases},
year = {2021},
isbn = {9781450388382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460620.3460752},
doi = {10.1145/3460620.3460752},
abstract = {Association pattern mining is a method of finding interesting relationships or patterns between item sets present in each of the transactions of the transactional databases. Current researchers in this area are focusing on the data mining task of finding frequent patterns among the item sets based on the interestingness measures like the support and confidence which is called as Frequent pattern mining. Till date, in existing frequent pattern mining algorithms, an itemset is said to be frequent if the support of the itemset satisfies the minimum support input. In this paper, the objective of our algorithm is to find interesting patterns among the item sets based on a Gaussian similarity for an input reference threshold which is first of its kind in the research literature. This study is limited to outlining na\"{\i}ve approach of mining frequent itemsets which requires validating every itemset to verify if the itemset is frequent or not.},
booktitle = {International Conference on Data Science, E-Learning and Information Systems 2021},
pages = {180–184},
numpages = {5},
keywords = {optimization, industry manufacturing, digitization, algorithms},
location = {Ma'an, Jordan},
series = {DATA'21}
}

@inproceedings{10.1145/3382494.3410694,
author = {Huang, Yuekai and Wang, Junjie and Wang, Song and Liu, Zhe and Hu, Yuanzhe and Wang, Qing},
title = {Quest for the Golden Approach: An Experimental Evaluation of Duplicate Crowdtesting Reports Detection},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410694},
doi = {10.1145/3382494.3410694},
abstract = {Background: Given the invisibility and unpredictability of distributed crowdtesting processes, there is a large number of duplicate reports, and detecting these duplicate reports is an important task to help save testing effort. Although, many approaches have been proposed to automatically detect the duplicates, the comparison among them and the practical guidelines to adopt these approaches in crowdtesting remain vague.Aims: We aim at conducting the first experimental evaluation of the commonly-used and state-of-the-art approaches for duplicate detection in crowdtesting reports, and exploring which is the golden approach.Method: We begin with a systematic review of approaches for duplicate detection, and select ten state-of-the-art approaches for our experimental evaluation. We conduct duplicate detection with each approach on 414 crowdtesting projects with 59,289 reports collected from one of the largest crowdtesting platforms.Results: Machine learning based approach, i.e., ML-REP, and deep learning based approach, i.e., DL-BiMPM, are the best two approaches for duplicate reports detection in crowdtesting, while the later one is more sensitive to the size of training data and more time-consuming for model training and prediction.Conclusions: This paper provides new insights and guidelines to select appropriate duplicate detection techniques for duplicate crowdtesting reports detection.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {17},
numpages = {12},
keywords = {machine learning, information retrieval, duplicate detection, deep learning, Crowdtesting},
location = {Bari, Italy},
series = {ESEM '20}
}

@inproceedings{10.1145/3377816.3381738,
author = {Arokiam, Jude and Bradbury, Jeremy S.},
title = {Automatically predicting bug severity early in the development process},
year = {2020},
isbn = {9781450371261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377816.3381738},
doi = {10.1145/3377816.3381738},
abstract = {Bug severity is an important factor in prioritizing which bugs to fix first. The process of triaging bug reports and assigning a severity requires developer expertise and knowledge of the underlying software. Methods to automate the assignment of bug severity have been developed to reduce the developer cost, however, many of these methods require 70-90% of the project's bug reports as training data and delay their use until later in the development process. Not being able to automatically predict a bug report's severity early in a project can greatly reduce the benefits of automation. We have developed a new bug report severity prediction method that leverages how bug reports are written rather than what the bug reports contain. Our method allows for the prediction of bug severity at the beginning of the project by using an organization's historical data, in the form of bug reports from past projects, to train the prediction classifier. In validating our approach, we conducted over 1000 experiments on a dataset of five NASA robotic mission software projects. Our results demonstrate that our method was not only able to predict the severity of bugs earlier in development, but it was also able to outperform an existing keyword-based classifier for a majority of the NASA projects.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {17–20},
numpages = {4},
keywords = {natural language processing, machine learning, bug severity},
location = {Seoul, South Korea},
series = {ICSE-NIER '20}
}

@inproceedings{10.1145/3379597.3387448,
author = {Tsay, Jason and Braz, Alan and Hirzel, Martin and Shinnar, Avraham and Mummert, Todd},
title = {AIMMX: Artificial Intelligence Model Metadata Extractor},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387448},
doi = {10.1145/3379597.3387448},
abstract = {Despite all of the power that machine learning and artificial intelligence (AI) models bring to applications, much of AI development is currently a fairly ad hoc process. Software engineering and AI development share many of the same languages and tools, but AI development as an engineering practice is still in early stages. Mining software repositories of AI models enables insight into the current state of AI development. However, much of the relevant metadata around models are not easily extractable directly from repositories and require deduction or domain knowledge. This paper presents a library called AIMMX that enables simplified AI Model Metadata eXtraction from software repositories. The extractors have five modules for extracting AI model-specific metadata: model name, associated datasets, references, AI frameworks used, and model domain. We evaluated AIMMX against 7,998 open-source models from three sources: model zoos, arXiv AI papers, and state-of-the-art AI papers. Our platform extracted metadata with 87% precision and 83% recall. As preliminary examples of how AI model metadata extraction enables studies and tools to advance engineering support for AI development, this paper presents an exploratory analysis for data and method reproducibility over the models in the evaluation dataset and a catalog tool for discovering and managing models. Our analysis suggests that while data reproducibility may be relatively poor with 42% of models in our sample citing their datasets, method reproducibility is more common at 72% of models in our sample, particularly state-of-the-art models. Our collected models are searchable in a catalog that uses existing metadata to enable advanced discovery features for efficiently finding models.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {81–92},
numpages = {12},
keywords = {Model Mining, Model Metadata, Model Catalog, Metadata Extraction, Machine Learning, Artificial Intelligence},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3676641.3716019,
author = {Gong, Sishuai and Rui, Wang and Altinb\"{u}ken, Deniz and Fonseca, Pedro and Maniatis, Petros},
title = {Snowplow: Effective Kernel Fuzzing with a Learned White-box Test Mutator},
year = {2025},
isbn = {9798400710797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676641.3716019},
doi = {10.1145/3676641.3716019},
abstract = {Kernel fuzzers rely heavily on program mutation to automatically generate new test programs based on existing ones. In particular, program mutation can alter the test's control and data flow inside the kernel by inserting new system calls, changing the values of call arguments, or performing other program mutations. However, due to the complexity of the kernel code and its user-space interface, finding the effective mutation that can lead to the desired outcome such as increasing the coverage and reaching a target code location is extremely difficult, even with the widespread use of manually-crafted heuristics.This work proposes Snowplow, a kernel fuzzer that uses a learned white-box test mutator to enhance test mutation. The core of Snowplow is an efficient machine learning model that can learn to predict promising mutations given the test program to mutate, its kernel code coverage, and the desired coverage. Snowplow is demonstrated on argument mutations of the kernel tests, and evaluated on recent Linux kernel releases. When fuzzing the kernels for 24 hours, Snowplow shows a significant speedup of discovering new coverage (4.8x~5.2x) and achieves higher overall coverage (7.0%~8.6%). In a 7-day fuzzing campaign, Snowplow discovers 86 previously-unknown crashes. Furthermore, the learned mutator is shown to accelerate directed kernel fuzzing by reaching 19 target code locations 8.5x faster and two additional locations that are missed by the state-of-the-art directed kernel fuzzer.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {1124–1138},
numpages = {15},
keywords = {kernel fuzzing, operating systems reliability and security, software testing and debugging},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/3691620.3695472,
author = {Chen, Jialuo and Wang, Jingyi and Zhang, Xiyue and Sun, Youcheng and Kwiatkowska, Marta and Chen, Jiming and Cheng, Peng},
title = {FAST: Boosting Uncertainty-based Test Prioritization Methods for Neural Networks via Feature Selection},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695472},
doi = {10.1145/3691620.3695472},
abstract = {Due to the vast testing space, the increasing demand for effective and efficient testing of deep neural networks (DNNs) has led to the development of various DNN test case prioritization techniques. However, the fact that DNNs can deliver high-confidence predictions for incorrectly predicted examples, known as the over-confidence problem, causes these methods to fail to reveal high-confidence errors. To address this limitation, in this work, we propose FAST, a method that boosts existing prioritization methods through guided FeAture SelecTion. FAST is based on the insight that certain features may introduce noise that affects the model's output confidence, thereby contributing to high-confidence errors. It quantifies the importance of each feature for the model's correct predictions, and then dynamically prunes the information from the noisy features during inference to derive a new probability vector for the uncertainty estimation. With the help of FAST, the high-confidence errors and correctly classified examples become more distinguishable, resulting in higher APFD (Average Percentage of Fault Detection) values for test prioritization, and higher generalization ability for model enhancement. We conduct extensive experiments to evaluate FAST across a diverse set of model structures on multiple benchmark datasets to validate the effectiveness, efficiency, and scalability of FAST compared to the state-of-the-art prioritization techniques.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {895–906},
numpages = {12},
keywords = {deep neural networks, test input prioritization},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@article{10.1145/3660808,
author = {Zhou, Shasha and Huang, Mingyu and Sun, Yanan and Li, Ke},
title = {Evolutionary Multi-objective Optimization for Contextual Adversarial Example Generation},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660808},
doi = {10.1145/3660808},
abstract = {The emergence of the 'code naturalness' concept, which suggests that software code shares statistical properties with natural language, paves the way for deep neural networks (DNNs) in software engineering (SE). However, DNNs can be vulnerable to certain human imperceptible variations in the input, known as adversarial examples (AEs), which could lead to adverse model performance. Numerous attack strategies have been proposed to generate AEs in the context of computer vision and natural language processing, but the same is less true for source code of programming languages in SE. One of the challenges is derived from various constraints including syntactic, semantics and minimal modification ratio. These constraints, however, are subjective and can be conflicting with the purpose of fooling DNNs. This paper develops a multi-objective adversarial attack method (dubbed MOAA), a tailored NSGA-II, a powerful evolutionary multi-objective (EMO) algorithm, integrated with CodeT5 to generate high-quality AEs based on contextual information of the original code snippet. Experiments on 5 source code tasks with 10 datasets of 6 different programming languages show that our approach can generate a diverse set of high-quality AEs with promising transferability. In addition, using our AEs, for the first time, we provide insights into the internal behavior of pre-trained models.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {101},
numpages = {24},
keywords = {adversarial example, multi-objective optimization, neural networks}
}

@article{10.1145/3616372,
author = {Amalfitano, Domenico and Faralli, Stefano and Hauck, Jean Carlo Rossa and Matalonga, Santiago and Distante, Damiano},
title = {Artificial Intelligence Applied to Software Testing: A Tertiary Study},
year = {2023},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3616372},
doi = {10.1145/3616372},
abstract = {Context: Artificial intelligence (AI) methods and models have extensively been applied to support different phases of the software development lifecycle, including software testing (ST). Several secondary studies investigated the interplay between AI and ST but restricted the scope of the research to specific domains or sub-domains within either area.Objective: This research aims to explore the overall contribution of AI to ST, while identifying the most popular applications and potential paths for future research directions.Method: We executed a tertiary study following well-established guidelines for conducting systematic literature mappings in software engineering and for answering nine research questions.Results: We identified and analyzed 20 relevant secondary studies. The analysis was performed by drawing from well-recognized AI and ST taxonomies and mapping the selected studies according to them. The resulting mapping and discussions provide extensive and detailed information on the interplay between AI and ST.Conclusion: The application of AI to support ST is a well-consolidated and growing interest research topic. The mapping resulting from our study can be used by researchers to identify opportunities for future research, and by practitioners looking for evidence-based information on which AI-supported technology to possibly adopt in their testing processes.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {58},
numpages = {38},
keywords = {Systematic mapping study, Systematic literature review, Tertiary study, Taxonomy, Software testing, Artificial intelligence}
}

@article{10.1145/3708521,
author = {Li, Rui and Liu, Huai and Poon, Pak-Lok and Towey, Dave and Sun, Chang-Ai and Zheng, Zheng and Zhou, Zhi Quan and Chen, Tsong Yueh},
title = {Metamorphic Relation Generation: State of the Art and Research Directions},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708521},
doi = {10.1145/3708521},
abstract = {Metamorphic testing has become one mainstream technique to address the notorious oracle problem in software testing, thanks to its great successes in revealing real-life bugs in a wide variety of software systems. Metamorphic relations, the core component of metamorphic testing, have continuously attracted research interests from both academia and industry. In the last decade, a rapidly increasing number of studies have been conducted to systematically generate metamorphic relations from various sources and for different application domains. In this article, based on the systematic review on the state of the art for metamorphic relations’ generation, we summarize and highlight visions for further advancing the theory and techniques for identifying and constructing metamorphic relations, and discuss promising research directions in related areas.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {Metamorphic testing, Metamorphic relation, Metamorphic relation generation}
}

@inproceedings{10.1145/3368089.3409723,
author = {She, Dongdong and Krishna, Rahul and Yan, Lu and Jana, Suman and Ray, Baishakhi},
title = {MTFuzz: fuzzing with a multi-task neural network},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409723},
doi = {10.1145/3368089.3409723},
abstract = {Fuzzing is a widely used technique for detecting software bugs and vulnerabilities. Most popular fuzzers generate new inputs using an evolutionary search to maximize code coverage. Essentially, these fuzzers start with a set of seed inputs, mutate them to generate new inputs, and identify the promising inputs using an evolutionary fitness function for further mutation.Despite their success, evolutionary fuzzers tend to get stuck in long sequences of unproductive mutations. In recent years, machine learning (ML) based mutation strategies have reported promising results. However, the existing ML-based fuzzers are limited by the lack of quality and diversity of the training data. As the input space of the target programs is high dimensional and sparse, it is prohibitively expensive to collect many diverse samples demonstrating successful and unsuccessful mutations to train the model.In this paper, we address these issues by using a Multi-Task Neural Network that can learn a compact embedding of the input space based on diverse training samples for multiple related tasks (i.e.,predicting for different types of coverage). The compact embedding can guide the mutation process by focusing most of the mutations on the parts of the embedding where the gradient is high. MTFuzz uncovers 11 previously unseen bugs and achieves an average of 2\texttimes{} more edge coverage compared with 5 state-of-the-art fuzzer on 10 real-world programs},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {737–749},
numpages = {13},
keywords = {Multi-task learning, Machine learning, Fuzzing},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3338906.3340442,
author = {Barash, Guy and Farchi, Eitan and Jayaraman, Ilan and Raz, Orna and Tzoref-Brill, Rachel and Zalmanovici, Marcel},
title = {Bridging the gap between ML solutions and their business requirements using feature interactions},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3340442},
doi = {10.1145/3338906.3340442},
abstract = {Machine Learning (ML) based solutions are becoming increasingly popular and pervasive. When testing such solutions, there is a tendency to focus on improving the ML metrics such as the F1-score and accuracy at the expense of ensuring business value and correctness by covering business requirements. In this work, we adapt test planning methods of classical software to ML solutions. We use combinatorial modeling methodology to define the space of business requirements and map it to the ML solution data, and use the notion of data slices to identify the weaker areas of the ML solution and strengthen them. We apply our approach to three real-world case studies and demonstrate its value.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1048–1058},
numpages = {11},
keywords = {Software Testing, Machine Learning, Combinatorial Modeling},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3238147.3238165,
author = {Udeshi, Sakshi and Arora, Pryanshu and Chattopadhyay, Sudipta},
title = {Automated directed fairness testing},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238165},
doi = {10.1145/3238147.3238165},
abstract = {Fairness is a critical trait in decision making. As machine-learning models are increasingly being used in sensitive application domains (e.g. education and employment) for decision making, it is crucial that the decisions computed by such models are free of unintended bias. But how can we automatically validate the fairness of arbitrary machine-learning models? For a given machine-learning model and a set of sensitive input parameters, our Aeqitas approach automatically discovers discriminatory inputs that highlight fairness violation. At the core of Aeqitas are three novel strategies to employ probabilistic search over the input space with the objective of uncovering fairness violation. Our Aeqitas approach leverages inherent robustness property in common machine-learning models to design and implement scalable test generation methodologies. An appealing feature of our generated test inputs is that they can be systematically added to the training set of the underlying model and improve its fairness. To this end, we design a fully automated module that guarantees to improve the fairness of the model. We implemented Aeqitas and we have evaluated it on six stateof- the-art classifiers. Our subjects also include a classifier that was designed with fairness in mind. We show that Aeqitas effectively generates inputs to uncover fairness violation in all the subject classifiers and systematically improves the fairness of respective models using the generated test inputs. In our evaluation, Aeqitas generates up to 70% discriminatory inputs (w.r.t. the total number of inputs generated) and leverages these inputs to improve the fairness up to 94%.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {98–108},
numpages = {11},
keywords = {Software Fairness, Machine Learning, Directed Testing},
location = {Montpellier, France},
series = {ASE '18}
}

@article{10.1145/3660809,
author = {Oueslati, Khouloud and Laberge, Gabriel and Lamothe, Maxime and Khomh, Foutse},
title = {Mining Action Rules for Defect Reduction Planning},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660809},
doi = {10.1145/3660809},
abstract = {Defect reduction planning plays a vital role in enhancing software quality and minimizing software maintenance costs. By training a black box machine learning model and “explaining” its predictions, explainable AI for software engineering aims to identify the code characteristics that impact maintenance risks. However, post-hoc explanations do not always faithfully reflect what the original model computes. In this paper, we introduce CounterACT, a Counterfactual ACTion rule mining approach that can generate defect reduction plans without black-box models. By leveraging action rules, CounterACT provides a course of action that can be considered as a counterfactual explanation for the class (e.g., buggy or not buggy) assigned to a piece of code. We compare the effectiveness of CounterACT with the original action rule mining algorithm and six established defect reduction approaches on 9 software projects. Our evaluation is based on (a) overlap scores between proposed code changes and actual developer modifications; (b) improvement scores in future releases; and (c) the precision, recall, and F1-score of the plans. Our results show that, compared to competing approaches, CounterACT’s explainable plans achieve higher overlap scores at the release level (median 95%) and commit level (median 85.97%), and they offer better trade-off between precision and recall (median F1-score 88.12%). Finally, we venture beyond planning and explore leveraging Large Language models (LLM) for generating code edits from our generated plans. Our results show that suggested LLM code edits supported by our plans are actionable and are more likely to pass relevant test cases than vanilla LLM code recommendations.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {102},
numpages = {23},
keywords = {Action rule mining, Counterfactual explanations, Defect reduction planning, Explainability, Software analytics}
}

@inproceedings{10.1145/3609437.3609441,
author = {Chen, Wei and Chen, Wu and Liu, Jiamou and Zhao, Kaiqi and Zhang, Mingyue},
title = {SupConFL: Fault Localization with Supervised Contrastive Learning},
year = {2023},
isbn = {9798400708947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609437.3609441},
doi = {10.1145/3609437.3609441},
abstract = {Recent years have seen a growing interest in deep learning-based approaches to localize faults in software. However, existing methods have not reached a satisfying level of accuracy. The main reason is that the feature extraction of faulty code elements is insufficient. Namely, these deep learning-based methods will learn some features that are not relevant to fault localization, and thus ignore the features related to fault localization. We propose SupConFL, a new framework for statement-level fault localization. Our framework combines the statement-level abstract syntax tree with the statement sequence, and adopt controllable attention-based LSTM to locate the faulty elements. The training is done through contrastive learning between the faulty code and its fixed version. By comparing the faulty code with the fixed code, the model can learn richer features of the faulty code elements. Our experiments on Defects4j-1.2.0 dataset show that our method outperforms the current state-of-the-art. Specifically, SupConFL improves Top-1 score by 7.96% in comparison with the current state-of-the-art. In addition, our method has also achieved good results in cross-project experiments.},
booktitle = {Proceedings of the 14th Asia-Pacific Symposium on Internetware},
pages = {44–54},
numpages = {11},
keywords = {fault localization, contrastive learning, code representation},
location = {Hangzhou, China},
series = {Internetware '23}
}

@inproceedings{10.1145/3649329.3657355,
author = {Jung, Jinhyo and So, Hwisoo and Ko, Woobin and Joshi, Sumedh Shridhar and Kim, Yebon and Ko, Yohan and Shrivastava, Aviral and Lee, Kyoungwoo},
title = {Maintaining Sanity: Algorithm-based Comprehensive Fault Tolerance for CNNs},
year = {2024},
isbn = {9798400706011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649329.3657355},
doi = {10.1145/3649329.3657355},
abstract = {As the deployment of neural networks in safety-critical applications proliferates, it becomes imperative that they exhibit consistent and dependable performance amidst hardware malfunctions. Several protection schemes have been proposed to protect neural networks, but they suffer from huge overheads or insufficient fault coverage. This paper presents Maintaining Sanity, a comprehensive and efficient protection technique for CNNs. Maintaining Sanity extends the state-of-the-art algorithm-based fault tolerance for CNN, utilizing hamming codes and checkpointing to correct over 99.6% of critical faults with about 72% runtime overhead and minimal memory overhead compared to traditional triple modular redundancy (TMR) techniques.},
booktitle = {Proceedings of the 61st ACM/IEEE Design Automation Conference},
articleno = {59},
numpages = {6},
keywords = {reliability, soft errors, transient faults, fault injection, ABFT, CNN, reliable machine learning},
location = {San Francisco, CA, USA},
series = {DAC '24}
}

@inproceedings{10.1145/3657604.3664676,
author = {Deshpande, Gururaj and Cheekati, Shravan and Patel, Shail and Raj, Pranav and Singh, Madhuri and Pindur, Mark and Al Soghyar, Nouf and Zhao, Bryan and Babolhavaeji, Parisa and Taher, Mohammad and Nathan, Krish and Spaeth, Will and Roozbahani, Max Mahdi},
title = {Transforming CS Education with DevOps: Streamlined Assignment Validation and Delivery @ Scale},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3664676},
doi = {10.1145/3657604.3664676},
abstract = {The surge in interest and demand for Artificial Intelligence (AI) skills has significantly increased student enrollment in AI and Machine Learning (ML) classes. In a large ML course at Georgia Institute of Technology, multi-week assignments encourage students to critically think about various ML algorithms theoretically and in an applied setting. Given the complexity of these large assignments, there exists the potential for bugs to remain undetected even after the verification process. These bugs lead to a significant increase in student questions and concerns, necessitate re-releasing assignment, and degrade the homework experience. To reduce and even prevent bugs in assignments, we adopt the DevOps methodology and implement a novel CI/CD pipeline along with Gitflow to automate the validation process of an assignment, from creation to release. An analysis of our classroom forum across semesters demonstrates that integrating a CI/CD pipeline with Gitflow effectively reduces the number of bug-related posts, allowing the instructional team to refocus on enhancing the student learning experience.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {259–264},
numpages = {6},
keywords = {CICD, automation, devops, gitflow, large classrooms},
location = {Atlanta, GA, USA},
series = {L@S '24}
}

@inproceedings{10.1145/3508352.3549422,
author = {Chiu, I-Wei and Chen, Xin-Ping and Hu, Jennifer Shueh-Inn and Li, James Chien-Mo},
title = {Automatic Test Configuration and Pattern Generation (ATCPG) for Neuromorphic Chips},
year = {2022},
isbn = {9781450392174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508352.3549422},
doi = {10.1145/3508352.3549422},
abstract = {The demand for low-power, high-performance neuromorphic chips is increasing. However, conventional testing is not applicable to neuromorphic chips due to three reasons: (1) lack of scan DfT, (2) stochastic characteristic, and (3) configurable functionality. In this paper, we present an automatic test configuration and pattern generation (ATCPG) method for testing a configurable stochastic neuromorphic chip without using scan DfT. We use machine learning to generate test configurations. Then, we apply a modified fast gradient sign method to generate test patterns. Finally, we determine test repetitions with statistical power of test. We conduct experiments on one of the neuromorphic architectures, spiking neural network, to evaluate the effectiveness of our ATCPG. The experimental results show that our ATCPG can achieve 100% fault coverage for the five fault models we use. For testing a 3-layer model at 0.05 significant level, we produce 5 test configurations and 67 test patterns. The average test repetitions of neuron faults and synapse faults are 2,124 and 4,557, respectively. Besides, our simulation results show that the overkill matched our significance level perfectly.},
booktitle = {Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design},
articleno = {30},
numpages = {7},
keywords = {neuromorphic chip, spiking neural network, test pattern generation},
location = {San Diego, California},
series = {ICCAD '22}
}

@inproceedings{10.1145/3664646.3664772,
author = {Kouemo Ngassom, Sylvain and Moradi Dakhel, Arghavan and Tambon, Florian and Khomh, Foutse},
title = {Chain of Targeted Verification Questions to Improve the Reliability of Code Generated by LLMs},
year = {2024},
isbn = {9798400706851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664646.3664772},
doi = {10.1145/3664646.3664772},
abstract = {LLM-based assistants, such as GitHub Copilot and ChatGPT, have the potential to generate code that fulfills a programming task described in a natural language description, referred to as a prompt. The widespread accessibility of these assistants enables users with diverse backgrounds to generate code and integrate it into software projects. However, studies show that code generated by LLMs is prone to bugs and may miss various corner cases in task specifications. Presenting such buggy code to users can impact their reliability and trust in LLM-based assistants. Moreover, significant efforts are required by the user to detect and repair any bug present in the code, especially if no test cases are available. In this study, we propose a self-refinement method aimed at improving the reliability of code generated by LLMs by minimizing the number of bugs before execution, without human intervention, and in the absence of test cases. Our approach is based on targeted Verification Questions (VQs) to identify potential bugs within the initial code. These VQs target various nodes within the Abstract Syntax Tree (AST) of the initial code, which have the potential to trigger specific types of bug patterns commonly found in LLM-generated code. Finally, our method attempts to repair these potential bugs by re-prompting the LLM with the targeted VQs and the initial code. Our evaluation, based on programming tasks in the CoderEval dataset, demonstrates that our proposed method outperforms state-of-the-art methods by decreasing the number of targeted errors in the code between 21% to 62% and improving the number of executable code instances to 13%.},
booktitle = {Proceedings of the 1st ACM International Conference on AI-Powered Software},
pages = {122–130},
numpages = {9},
keywords = {Code Generation, Hallucination, Large Language Model, Reliability, Software Development},
location = {Porto de Galinhas, Brazil},
series = {AIware 2024}
}

@inproceedings{10.1145/3460319.3464819,
author = {Zeng, Zhengran and Zhang, Yuqun and Zhang, Haotian and Zhang, Lingming},
title = {Deep just-in-time defect prediction: how far are we?},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464819},
doi = {10.1145/3460319.3464819},
abstract = {Defect prediction aims to automatically identify potential defective code with minimal human intervention and has been widely studied in the literature. Just-in-Time (JIT) defect prediction focuses on program changes rather than whole programs, and has been widely adopted in continuous testing. CC2Vec, state-of-the-art JIT defect prediction tool, first constructs a hierarchical attention network (HAN) to learn distributed vector representations of both code additions and deletions, and then concatenates them with two other embedding vectors representing commit messages and overall code changes extracted by the existing DeepJIT approach to train a model for predicting whether a given commit is defective. Although CC2Vec has been shown to be the state of the art for JIT defect prediction, it was only evaluated on a limited dataset and not compared with all representative baselines. Therefore, to further investigate the efficacy and limitations of CC2Vec, this paper performs an extensive study of CC2Vec on a large-scale dataset with over 310,370 changes (8.3 X larger than the original CC2Vec dataset). More specifically, we also empirically compare CC2Vec against DeepJIT and representative traditional JIT defect prediction techniques. The experimental results show that CC2Vec cannot consistently outperform DeepJIT, and neither of them can consistently outperform traditional JIT defect prediction. We also investigate the impact of individual traditional defect prediction features and find that the added-line-number feature outperforms other traditional features. Inspired by this finding, we construct a simplistic JIT defect prediction approach which simply adopts the added-line-number feature with the logistic regression classifier. Surprisingly, such a simplistic approach can outperform CC2Vec and DeepJIT in defect prediction, and can be 81k X/120k X faster in training/testing. Furthermore, the paper also provides various practical guidelines for advancing JIT defect prediction in the near future.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {427–438},
numpages = {12},
keywords = {Software Defect Prediction, Just-In-Time Prediction, Deep Learning},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1145/3368089.3409754,
author = {Harel-Canada, Fabrice and Wang, Lingxiao and Gulzar, Muhammad Ali and Gu, Quanquan and Kim, Miryung},
title = {Is neuron coverage a meaningful measure for testing deep neural networks?},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409754},
doi = {10.1145/3368089.3409754},
abstract = {Recent effort to test deep learning systems has produced an intuitive and compelling test criterion called neuron coverage (NC), which resembles the notion of traditional code coverage. NC measures the proportion of neurons activated in a neural network and it is implicitly assumed that increasing NC improves the quality of a test suite. In an attempt to automatically generate a test suite that increases NC, we design a novel diversity promoting regularizer that can be plugged into existing adversarial attack algorithms. We then assess whether such attempts to increase NC could generate a test suite that (1) detects adversarial attacks successfully, (2) produces natural inputs, and (3) is unbiased to particular class predictions. Contrary to expectation, our extensive evaluation finds that increasing NC actually makes it harder to generate an effective test suite: higher neuron coverage leads to fewer defects detected, less natural inputs, and more biased prediction preferences. Our results invoke skepticism that increasing neuron coverage may not be a meaningful objective for generating tests for deep neural networks and call for a new test generation technique that considers defect detection, naturalness, and output impartiality in tandem.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {851–862},
numpages = {12},
keywords = {Testing, Software Engineering, Neuron Coverage, Machine Learning, Adversarial Attack},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1145/3688842,
author = {Nourry, Olivier and Kashiwa, Yutaro and Shang, Weiyi and Shu, Honglin and Kamei, Yasutaka},
title = {My Fuzzers Won’t Build: An Empirical Study of Fuzzing Build Failures},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3688842},
doi = {10.1145/3688842},
abstract = {Fuzzing is an automated software testing technique used to find software vulnerabilities that works by sending large amounts of inputs to a software system to trigger bad behaviors. In recent years, the open source software ecosystem has seen a significant increase in the adoption of fuzzing to avoid spreading vulnerabilities throughout the ecosystem. While fuzzing can uncover vulnerabilities, there is currently a lack of knowledge regarding the challenges of conducting fuzzing activities over time. Specifically, fuzzers are very complex tools to set up and build before they can be used.We set out to empirically find out how challenging is build maintenance in the context of fuzzing. We mine over 1.2 million build logs from Google’s OSS-Fuzz service to investigate fuzzing build failures. We first conduct a quantitative analysis to quantify the prevalence of fuzzing build failures. We then manually investigate 677 failing fuzzing builds logs and establish a taxonomy of 25 root causes of build failures. We finally train a machine learning model to recognize common failure patterns in failing build logs. Our taxonomy can serve as a reference for practitioners conducting fuzzing build maintenance. Our modeling experiment shows the potential of using automation to simplify the process of fuzzing.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {29},
numpages = {30},
keywords = {Fuzzing, Empirical Study, Build Maintenance}
}

@inproceedings{10.1145/3540250.3558941,
author = {Zhu, Junjie and Long, Teng and Wang, Wei and Memon, Atif},
title = {Improving ML-based information retrieval software with user-driven functional testing and defect class analysis},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558941},
doi = {10.1145/3540250.3558941},
abstract = {Machine Learning (ML) has become the cornerstone of information retrieval (IR) software, as it can drive better user experience by leveraging information-rich data and complex models. However, evaluating the emergent behavior of ML-based IR software can be challenging with traditional software testing approaches: when developers modify the software, they cannot often extract useful information from individual test instances; rather, they seek to holistically verify whether—and where—their modifications caused significant regressions or improvements at scale. In this paper, we introduce not only such a holistic approach to evaluate the system-level behavior of the software, but also the concept of a defect class, which represents a partition of the input space on which the ML-based software does measurably worse for an existing feature or on which the ML task is more challenging for a new feature. We leverage large volumes of functional test cases, automatically obtained, to derive these defect classes, and propose new ways to improve the IR software from an end-user’s perspective. Applying our approach on a real production Search-AutoComplete system that contains a query interpretation ML component, we demonstrate that (1) our holistic metrics successfully identified two regressions and one improvement, where all 3 were independently verified with retrospective A/B experiments, (2) the automatically obtained defect classes provided actionable insights during early-stage ML development, and (3) we also detected defect classes at the finer sub-component level for which there were significant regressions, which we blocked prior to different releases.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1291–1301},
numpages = {11},
keywords = {Relevance Search, Query Interpretation, Machine Learning Testing, Information Retrieval System Testing, AutoComplete Search},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00010,
author = {Saini, Nishrith and Britto, Ricardo},
title = {Using machine intelligence to prioritise code review requests},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00010},
doi = {10.1109/ICSE-SEIP52600.2021.00010},
abstract = {Modern Code Review (MCR) is the process of reviewing new code changes that need to be merged with an existing codebase. As a developer, one may receive many code review requests every day, i.e., the review requests need to be prioritised. Manually prioritising review requests is a challenging and time-consuming process. To address the above problem, we conducted an industrial case study at Ericsson aiming at developing a tool called Pineapple, which uses a Bayesian Network to prioritise code review requests. To validate our approach/tool, we deployed it in a live software development project at Ericsson, wherein more than 150 developers develop a telecommunication product. We focused on evaluating the predictive performance, feasibility, and usefulness of our approach. The results indicate that Pineapple has competent predictive performance (RMSE = 0.21 and MAE = 0.15). Furthermore, around 82.6% of Pineapple's users believe the tool can support code review request prioritisation by providing reliable results, and around 56.5% of the users believe it helps reducing code review lead time. As future work, we plan to evaluate Pineapple's predictive performance, usefulness, and feasibility through a longitudinal investigation.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {11–20},
numpages = {10},
keywords = {prioritisation, modern code review, machine reasoning, machine learning, machine intelligence, bayesian networks},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1145/3568364.3568380,
author = {Matcha, Wyao and Toure, Fadel and Badri, Mourad and Badri, Linda},
title = {Identifying Candidate Classes for Unit Testing Using Deep Learning Classifiers: An Empirical Validation},
year = {2022},
isbn = {9781450396950},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568364.3568380},
doi = {10.1145/3568364.3568380},
abstract = {This paper aims at investigating the use of deep learning to suggest (prioritize) classes to be tested rigorously during unit testing of object-oriented systems. We relied on software unit testing information history and source code metrics. We conducted an empirical study using data collected from two Apache open-source Java software systems (POI and ANT). For each software system, we extracted the source code of five different versions. For each version, we collected various metrics from the source code of the Java classes. Then, for all software classes, we extracted testing coverage measures at instruction and method levels of granularity. We used the existing JUnit test cases developed for these systems. Based on the different datasets we collected, we trained several deep neural network models. We validated the obtained classifiers using four validation techniques: (1) CV: Cross Version validation, (2) CPV: Combined Previous Version validation, (3) CSPV: Combined System and Previous Version validation, and (4) LOSO: Leave One System Out validation. The obtained results in terms of classifiers’ performance vary between 70% and 80% of accuracy and strongly support the viability of our approach.},
booktitle = {Proceedings of the 4th World Symposium on Software Engineering},
pages = {98–107},
numpages = {10},
keywords = {Unit Testing, Tests Prioritization, Testing Coverage Measures, Source Code, Metrics, Machine Learning, Deep Learning},
location = {Xiamen, China},
series = {WSSE '22}
}

@inproceedings{10.5555/3432601.3432619,
author = {Jahanshahi, Hadi and Cevik, Mucahit and Ba\c{s}ar, Ay\c{s}e},
title = {Moving from cross-project defect prediction to heterogeneous defect prediction: a partial replication study},
year = {2020},
publisher = {IBM Corp.},
address = {USA},
abstract = {Software defect prediction heavily relies on the metrics collected from software projects. Earlier studies often used machine learning techniques to build, validate, and improve bug prediction models using either a set of metrics collected within a project or across different projects. However, techniques applied and conclusions derived by those models are restricted by how identical those metrics are. Knowledge coming from those models will not be extensible to a target project if no sufficient overlapping metrics have been collected in the source projects. To explore the feasibility of transferring knowledge across projects without common labeled metrics, we systematically integrated Heterogeneous Defect Prediction (HDP) by replicating and validating the obtained results. Our main goal is to extend prior research and explore the feasibility of HDP and finally to compare its performance with that of its predecessor, Cross-Project Defect Prediction. We construct an HDP model on different publicly available datasets. Moreover, we propose a new ensemble voting approach in the HDP context to utilize the predictive power of multiple available datasets. The result of our experiment is comparable to that of the original study. However, we also explored the feasibility of HDP in real cases. Our results shed light on the infeasibility of many cases for the HDP algorithm due to its sensitivity to the parameter selection. In general, our analysis gives a deep insight into why and how to perform transfer learning from one domain to another, and in particular, provides a set of guidelines to help researchers and practitioners to disseminate knowledge to the defect prediction domain.},
booktitle = {Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering},
pages = {133–142},
numpages = {10},
keywords = {transfer learning, software quality, heterogeneous metrics, defect prediction},
location = {Toronto, Ontario, Canada},
series = {CASCON '20}
}

@inproceedings{10.1145/3650212.3680312,
author = {Demir, Demet and Betin Can, Aysu and Surer, Elif},
title = {Test Selection for Deep Neural Networks using Meta-Models with Uncertainty Metrics},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680312},
doi = {10.1145/3650212.3680312},
abstract = {With the use of Deep Learning (DL) in safety-critical domains, the systematic testing of these systems has become a critical issue for human life. Due to the data-driven nature of Deep Neural Networks (DNNs), the effectiveness of tests is closely related to the adequacy of test datasets. Test data need to be labeled, which requires manual human effort and sometimes expert knowledge. DL system testers aim to select the test data that will be most helpful in identifying the weaknesses of the DNN model by using resources efficiently.
 

 
To help achieve this goal, we propose a test data prioritization approach based on using a meta-model that gets uncertainty metrics as input, which are derived from outputs of other base models. Integrating different uncertainty metrics helps overcome individual limitations of these metrics and be effective in a wider range of scenarios. We train the meta-models with the objective of predicting whether a test input will lead the tested model to make an incorrect prediction or not. We conducted an experimental evaluation with popular image classification datasets and DNN models to evaluate the proposed approach. The results of the experiments demonstrate that our approach effectively prioritizes the test datasets and outperforms existing state-of-the-art test prioritization methods used in comparison. In the experiments, we evaluated the test prioritization approach from a distribution-aware perspective by generating test datasets with and without out-of-distribution data.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {678–690},
numpages = {13},
keywords = {Data Distribution, Deep Neural Network Testing, Test Selection and Prioritization, Uncertainty},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3661167.3661199,
author = {Esposito, Matteo and Falaschi, Valentina and Falessi, Davide},
title = {An Extensive Comparison of Static Application Security Testing Tools},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661199},
doi = {10.1145/3661167.3661199},
abstract = {Context: Static Application Security Testing Tools (SASTTs) identify software vulnerabilities to support the security and reliability of software applications. Interestingly, several studies have suggested that alternative solutions may be more effective than SASTTs due to their tendency to generate false alarms, commonly referred to as low Precision. Aim: We aim to comprehensively evaluate SASTTs, setting a reliable benchmark for assessing and finding gaps in vulnerability identification mechanisms based on SASTTs or alternatives. Method: Our SASTTs evaluation is based on a controlled, though synthetic, Java codebase. It involves an assessment of 1.5 million test executions, and it features innovative methodological features such as effort-aware accuracy metrics and method-level analysis. Results: Our findings reveal that SASTTs detect a tiny range of vulnerabilities. In contrast to prevailing wisdom, SASTTs exhibit high Precision while falling short in Recall. Conclusions: Our findings suggest that enhancing Recall, alongside expanding the spectrum of detected vulnerability types, should be the primary focus for improving SASTTs or alternative approaches, such as machine learning-based vulnerability identification solutions.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {69–78},
numpages = {10},
keywords = {Common Vulnerability Exposure, Common Weakness Enumeration, Security Assessment Tool, Static Application Security Testing},
location = {Salerno, Italy},
series = {EASE '24}
}

@article{10.1145/3705302,
author = {Zhang, Lehuan and Guo, Shikai and Guo, Yi and Li, Hui and Chai, Yu and Chen, Rong and Li, Xiaochen and Jiang, He},
title = {Context-based Transfer Learning for Structuring Fault Localization and Program Repair Automation},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3705302},
doi = {10.1145/3705302},
abstract = {Automated software debugging plays a crucial role in aiding software developers to swiftly identify and attempt to rectify faults, thereby significantly reducing developers’ workload. Previous researches have predominantly relied on simplistic semantic deep learning or statistical analysis methods to locate faulty statements in diverse projects. However, code repositories often consist of lengthy sequences with long-distance dependencies, posing challenges for accurately modeling fault localization using these methods. In addition, the lack of joint reasoning among various faults prevents existing models from deeply capturing fault information. To address these challenges, we propose a method named CodeHealer to achieve accurate fault localization and program repair. CodeHealer comprises three components: a Deep Semantic Information Extraction Component that effectively extracts deep semantic features from suspicious code statements using classifiers based on Joint-attention mechanisms; a Suspicious Statement Ranking Component that combines various fault localization features and employs multilayer perceptrons to derive multidimensional vectors of suspicion values; and a Fault Repair Component that, based on ranked suspicious statements generated by fault localization, adopts a top-down approach using multiple classifiers based on Co-teaching mechanisms to select repair templates and generate patches. The experimental results indicate that when applied to fault localization, CodeHealer outperforms the best baseline method with improvements of 11.4%, 2.7%, and 1.6% on Top-1/3/5 metrics, respectively. It also reduces the MFR and MAR by 9.8% and 2.1%, where lower values denote better fault localization effectiveness. Additionally, in automated software debugging, CodeHealer fixes an additional 6 faults compared to the current best method, totaling 53 faults repaired.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
keywords = {Software debugging, Fault Localization, Transfer learning}
}

@inproceedings{10.1145/3691620.3695347,
author = {Smytzek, Marius and Eberlein, Martin and Werk, Kai and Grunske, Lars and Zeller, Andreas},
title = {FixKit: A Program Repair Collection for Python},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695347},
doi = {10.1145/3691620.3695347},
abstract = {In recent years, automatic program repair has gained much attention in the research community. Generally, program repair approaches consider a faulty program and a test suite that captures the program's intended behavior. The goal is automatically generating a patch that corrects the fault by identifying the faulty code locations, suggesting a candidate fix, and validating it against the provided tests. However, most existing program repair tools focus on Java or C programs, while Python, one of the most popular programming languages, lacks approaches that work on it.We present FixKit, a collection of five program repair approaches for Python programs. Moreover, our framework allows for easy integration of new repair approaches and swapping individual components, such as fault localization. Our framework enables researchers to compare and investigate various repair, fault localization effortlessly, and validation approaches on a common set of techniques.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {2342–2345},
numpages = {4},
keywords = {python, program repair, genetic programming},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3468264.3473131,
author = {Ma, Yu-Seung and Yoo, Shin and Kim, Taeho},
title = {Selecting test inputs for DNNs using differential testing with subspecialized model instances},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3473131},
doi = {10.1145/3468264.3473131},
abstract = {Testing of Deep Learning (DL) models is difficult due to the lack of automated test oracle and the high cost of human labelling. Differential testing has been used as a surrogate oracle, but there is no systematic guide on how to choose the reference model to use for differential testing. We propose a novel differential testing approach based on subspecialized models, i.e., models that are trained on sliced training data only (hence specialized for the slice). A preliminary evaluation of our approach with an CNN-based EMNIST image classifier shows that it can achieve higher error detection rate with selected inputs compared to using more advanced ResNet and LeNet as the reference model for differential testing. Our approach also outperforms N-version testing, i.e., the use of the same DL model architecture trained separately but using the same data.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1467–1470},
numpages = {4},
keywords = {Test Oracle, Machine Learning, Diffrential Testing},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3383972.3384012,
author = {Cheng, Jing and Zhang, Hao and Qin, Kai},
title = {Safety Critical Software Reliability Model Considering Multiple Influencing Factors},
year = {2020},
isbn = {9781450376426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383972.3384012},
doi = {10.1145/3383972.3384012},
abstract = {Safety-critical software accounts for a large proportion of computer applications in high-tech fields such as aerospace, aerospace, and nuclear power. Once a failure occurs, it can seriously endanger human life, cause large-scale environmental damage, or cause huge economic losses. How to accurately evaluate the reliability of safety-critical software has become one of the problems that software reliability engineering needs to solve. Therefore, this paper takes safety-critical software as the research object. Based on the in-depth analysis of the characteristics of safety-critical software, combined with the characteristics of safety-critical software and the common model optimization and improvement ideas, the total number of software failures, the failure detection rate changes, and the troubleshooting rate is less than the correlation between faults and errors is analyzed. The fault detection and elimination process are analyzed. The influencing factors that should be considered in the reliability modeling process of safety critical software are summarized and mathematically described. A multi-influence factor is considered. Safety Critical Software Reliability Model (SCSRM). Based on the safety critical software failure data set, the comparison experiment between SCSRM and the other four software reliability models was completed. The experimental results show that SCSRM can obtain better data fitting and prediction effects in the reliability assessment scenarios of safety-critical software, and proves the validity and stability of the proposed model.},
booktitle = {Proceedings of the 2020 12th International Conference on Machine Learning and Computing},
pages = {560–566},
numpages = {7},
keywords = {Software Reliability Model, Software Reliability, Safety Critical Software},
location = {Shenzhen, China},
series = {ICMLC '20}
}

@inproceedings{10.1145/3460120.3484813,
author = {He, Jingxuan and Sivanrupan, Gishor and Tsankov, Petar and Vechev, Martin},
title = {Learning to Explore Paths for Symbolic Execution},
year = {2021},
isbn = {9781450384544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460120.3484813},
doi = {10.1145/3460120.3484813},
abstract = {Symbolic execution is a powerful technique that can generate tests steering program execution into desired paths. However, the scalability of symbolic execution is often limited by path explosion, i.e., the number of symbolic states representing the paths under exploration quickly explodes as execution goes on. Therefore, the effectiveness of symbolic execution engines hinges on the ability to select and explore the right symbolic states.In this work, we propose a novel learning-based strategy, called Learch, able to effectively select promising states for symbolic execution to tackle the path explosion problem. Learch directly estimates the contribution of each state towards the goal of maximizing coverage within a time budget, as opposed to relying on manually crafted heuristics based on simple statistics as a crude proxy for the objective. Moreover, Learch leverages existing heuristics in training data generation and feature extraction, and can thus benefit from any new expert-designed heuristics. We instantiated Learch in KLEE, a widely adopted symbolic execution engine. We evaluated Learch on a diverse set of programs, showing that Learch is practically effective: it covers more code and detects more security violations than existing manual heuristics, as well as combinations of those heuristics. We also show that using tests generated by Learch as initial fuzzing seeds enables the popular fuzzer AFL to find more paths and security violations.},
booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2526–2540},
numpages = {15},
keywords = {symbolic execution, program testing, machine learning, fuzzing},
location = {Virtual Event, Republic of Korea},
series = {CCS '21}
}

@inproceedings{10.1145/3464968.3468409,
author = {Jafarinejad, Foad and Narasimhan, Krishna and Mezini, Mira},
title = {NerdBug: automated bug detection in neural networks},
year = {2021},
isbn = {9781450385411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3464968.3468409},
doi = {10.1145/3464968.3468409},
abstract = {Despite the exponential growth of deep learning software during the last decade, there is a lack of tools to test and debug issues in deep learning programs. Current static analysis tools do not address challenges specific to deep learning as observed by past research on bugs specific to this area. Existing deep learning bug detection tools focus on specific issues like shape mismatches. In this paper, we present a vision for an abstraction-based approach to detect deep learning bugs and the plan to evaluate our approach. The motivation behind the abstraction-based approach is to be able to build an intermediate version of the neural network that can be analyzed in development time to provide live feedback programmers are used to with other kind of bugs.},
booktitle = {Proceedings of the 1st ACM International Workshop on AI and Software Testing/Analysis},
pages = {13–16},
numpages = {4},
keywords = {Machine Learning, Deep Learning, Debugging, Bug Detection},
location = {Virtual, Denmark},
series = {AISTA 2021}
}

@inproceedings{10.1145/3460319.3464834,
author = {Elsner, Daniel and Hauer, Florian and Pretschner, Alexander and Reimer, Silke},
title = {Empirically evaluating readily available information for regression test optimization in continuous integration},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464834},
doi = {10.1145/3460319.3464834},
abstract = {Regression test selection (RTS) and prioritization (RTP) techniques aim to reduce testing efforts and developer feedback time after a change to the code base. Using various information sources, including test traces, build dependencies, version control data, and test histories, they have been shown to be effective. However, not all of these sources are guaranteed to be available and accessible for arbitrary continuous integration (CI) environments. In contrast, metadata from version control systems (VCSs) and CI systems are readily available and inexpensive. Yet, corresponding RTP and RTS techniques are scattered across research and often only evaluated on synthetic faults or in a specific industrial context. It is cumbersome for practitioners to identify insights that apply to their context, let alone to calibrate associated parameters for maximum cost-effectiveness. This paper consolidates existing work on RTP and unsafe RTS into an actionable methodology to build and evaluate such approaches that exclusively rely on CI and VCS metadata. To investigate how these approaches from prior research compare in heterogeneous settings, we apply the methodology in a large-scale empirical study on a set of 23 projects covering 37,000 CI logs and 76,000 VCS commits. We find that these approaches significantly outperform established RTP baselines and, while still triggering 90% of the failures, we show that practitioners can expect to save on average 84% of test execution time for unsafe RTS. We also find that it can be beneficial to limit training data, features from test history work better than change-based features, and, somewhat surprisingly, simple and well-known heuristics often outperform complex machine-learned models.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {491–504},
numpages = {14},
keywords = {software testing, regression test optimization, machine learning},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1145/3691620.3695500,
author = {Li, Zhong and Zhang, Chong and Pan, Minxue and Zhang, Tian and Li, Xuandong},
title = {AACEGEN: Attention Guided Adversarial Code Example Generation for Deep Code Models},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695500},
doi = {10.1145/3691620.3695500},
abstract = {Adversarial code examples are important to investigate the robustness of deep code models. Existing work on adversarial code example generation has shown promising results yet still falls short in practical applications due to either the high number of model invocations or the limited naturalness of generated examples. In this paper, we propose AaceGEN, an attention-guided adversarial code example generation method for deep code models. The key idea of AaceGEN is to utilize the attention distributions behind deep code models to guide the generation of adversarial code examples. As such, the code elements critical for model predictions could be prioritized for exploration, enhancing the effectiveness and efficiency of adversarial code example generation. In addition, AaceGEN implements a code transformation library providing diverse semantic-preserving code transformations for various code elements, and further conducts a search under the constraint of a maximum number of allowable code transformations to generate adversarial code examples with subtlety and stealth. Our extensive experiments on 9 diverse subjects, taking into account different software engineering tasks and varied deep code models, demonstrate that AaceGEN outperforms 3 baseline approaches under comprehensive evaluation.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1245–1257},
numpages = {13},
keywords = {adversarial example, deep code model, code transformation, search-based testing},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3664646.3664764,
author = {Hussain, Aftab and Rabin, Md Rafiqul Islam and Alipour, Mohammad Amin},
title = {Measuring Impacts of Poisoning on Model Parameters and Embeddings for Large Language Models of Code},
year = {2024},
isbn = {9798400706851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664646.3664764},
doi = {10.1145/3664646.3664764},
abstract = {Large language models (LLMs) have revolutionized software development practices, yet concerns about their safety have arisen, particularly regarding hidden backdoors, aka trojans. Backdoor attacks involve the insertion of triggers into training data, allowing attackers to manipulate the behavior of the model maliciously. In this paper, we focus on analyzing the model parameters to detect potential backdoor signals in code models. Specifically, we examine attention weights and biases, and context embeddings of the clean and poisoned CodeBERT and CodeT5 models. Our results suggest noticeable patterns in context embeddings of poisoned samples for both the poisoned models; however, attention weights and biases do not show any significant differences. This work contributes to ongoing efforts in white-box detection of backdoor signals in LLMs of code through the analysis of parameters and embeddings.},
booktitle = {Proceedings of the 1st ACM International Conference on AI-Powered Software},
pages = {59–64},
numpages = {6},
keywords = {LLMs of Code, Safe AI, Trojan Detection},
location = {Porto de Galinhas, Brazil},
series = {AIware 2024}
}

@inproceedings{10.1145/3551349.3556914,
author = {Tian, Haoye and Tang, Xunzhu and Habib, Andrew and Wang, Shangwen and Liu, Kui and Xia, Xin and Klein, Jacques and Bissyand\'{E}, Tegawend\'{E} F.},
title = {Is this Change the Answer to that Problem? Correlating Descriptions of Bug and Code Changes for Evaluating Patch Correctness},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556914},
doi = {10.1145/3551349.3556914},
abstract = {Patch correctness has been the focus of automated program repair (APR) in recent years due to the propensity of APR tools to generate overfitting patches. Given a generated patch, the oracle (e.g., test suites) is generally weak in establishing correctness. Therefore, the literature has proposed various approaches of leveraging machine learning with engineered and deep learned features, or exploring dynamic execution information, to further explore the correctness of APR-generated patches. In this work, we propose a novel perspective to the problem of patch correctness assessment: a correct patch implements changes that “answer” to a problem posed by buggy behavior. Concretely, we turn the patch correctness assessment into a Question Answering problem. To tackle this problem, our intuition is that natural language processing can provide the necessary representations and models for assessing the semantic correlation between a bug (question) and a patch (answer). Specifically, we consider as inputs the bug reports as well as the natural language description of the generated patches. Our approach, Quatrain, first considers state-of-the-art commit message generation models to produce the relevant inputs associated to each generated patch. Then we leverage a neural network architecture to learn the semantic correlation between bug reports and commit messages. Experiments on a large dataset of 9&nbsp;135 patches generated for three bug datasets (Defects4j, Bugs.jar and Bears) show that Quatrain achieves an AUC of 0.886 on predicting patch correctness, and recalling 93% correct patches while filtering out 62% incorrect patches. Our experimental results further demonstrate the influence of inputs quality on prediction performance. We further perform experiments to highlight that the model indeed learns the relationship between bug reports and code change descriptions for the prediction. Finally, we compare against prior work and discuss the benefits of our approach.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {59},
numpages = {13},
keywords = {Question Answering, Program Repair, Patch Correctness, Machine Learning},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3551349.3556918,
author = {Yang, Chenyang and Brower-Sinning, Rachel A and Lewis, Grace and K\"{A}Stner, Christian},
title = {Data Leakage in Notebooks: Static Detection and Better Processes},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556918},
doi = {10.1145/3551349.3556918},
abstract = {Data science pipelines to train and evaluate models with machine learning may contain bugs just like any other code. Leakage between training and test data can lead to overestimating the model’s accuracy during offline evaluations, possibly leading to deployment of low-quality models in production. Such leakage can happen easily by mistake or by following poor practices, but may be tedious and challenging to detect manually. We develop a static analysis approach to detect common forms of data leakage in data science code. Our evaluation shows that our analysis accurately detects data leakage and that such leakage is pervasive among over 100,000 analyzed public notebooks. We discuss how our static analysis approach can help both practitioners and educators, and how leakage prevention can be designed into the development process.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {30},
numpages = {12},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3474624.3474634,
author = {Collins, Eliane and Neto, Arilo and Vincenzi, Auri and Maldonado, Jos\'{e}},
title = {Deep Reinforcement Learning based Android Application GUI Testing},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3474634},
doi = {10.1145/3474624.3474634},
abstract = {The advances in mobile computing and the market demand for new products which meet an increasingly public represent the importance to assure the quality of mobile applications. In this context, automated GUI testing has become highlighted in research. However, studies indicate that there are still limitations to achieve a large number of possible combinations of operations, transitions, functionality coverage, and failures reproduction. In this paper, a Deep Q-Network-based android application GUI testing tool (DeepGUIT) is proposed to test case generation for android mobile apps, guiding the exploration by code coverage value and new activities. The tool was evaluated with 15 open-source mobile applications. The obtained results showed higher code coverage than the state-of-the-art tools Monkey (61% average higher) and Q-testing (47% average higher), in addition, a greater number of failures.},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {186–194},
numpages = {9},
keywords = {reinforcement learning, machine learning, and automated testing, Mobile application testing},
location = {Joinville, Brazil},
series = {SBES '21}
}

@inproceedings{10.1145/3698062.3698067,
author = {Bai, Jiacheng and Dohi, Tadashi and Zheng, Junjun and Okamura, Hiroyuki},
title = {Comparison of Kernel-based Nonparametric and Parametric Regression Approaches for Software Reliability Prediction},
year = {2024},
isbn = {9798400717086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698062.3698067},
doi = {10.1145/3698062.3698067},
abstract = {This paper presents kernel-based nonparametric regression approaches for software reliability prediction without specifying a specific fault count distribution. Unlike the traditional models, the proposed method does not require the underlying stochastic process describing software fault-counts during the system testing phase. We apply two kernel estimators–the Nadaraya-Watson estimator and the local linear estimator–along with four types of kernel functions: Gaussian, triangular, biweight, and Epanechnikov kernels. To address the bandwidth selection issue, we employ the well-established least-squares cross-validation algorithm. Through numerical experiments on six real software fault count datasets, we compare the prediction performance of kernel-based regression methods with there of traditional parametric models for the cumulative number of software faults. Our results demonstrate the potential of nonparametric software reliability models to provide accurate predictions without predefined software intensity or mean value functions.},
booktitle = {Proceedings of the 2024 The 6th World Symposium on Software Engineering (WSSE)},
pages = {32–37},
numpages = {6},
keywords = {software reliability, kernel regression, nonparametric approach, Nadaraya-Watson estimator, local linear estimator, predictive performance.},
location = {
},
series = {WSSE '24}
}

@inproceedings{10.1145/3578527.3578530,
author = {Jain, Ridhi and Gervasoni, Nicole and Ndhlovu, Mthandazo and Rawat, Sanjay},
title = {A Code Centric Evaluation of C/C++ Vulnerability Datasets for Deep Learning Based Vulnerability Detection Techniques},
year = {2023},
isbn = {9798400700644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578527.3578530},
doi = {10.1145/3578527.3578530},
abstract = {Recent years have witnessed tremendous progress in NLP-based code comprehension via deep neural networks (DNN) learning, especially Large Language Models (LLMs). While the original application of LLMs is focused on code generation, there have been attempts to extend the application to more specialized tasks, like code similarity, author attribution, code repairs, and so on. As data plays an important role in the success of any machine learning approach, researchers have also proposed several benchmarks which are coupled with a specific task at hand. It is well known in the machine learning (ML) community that the presence of biases in the dataset affects the quality of the ML algorithm in a real-world scenario. This paper evaluates several existing datasets from DNN’s application perspective. We specifically focus on training datasets of C/C++ language code. Our choice of language stems from the fact that while LLM-based techniques have been applied and evaluated on programming languages like Python, JavaScript, and Ruby, there is not much LLM research for C/C++. As a result, datasets generated synthetically or from real-world codes are in individual research work. Consequently, in the absence of a uniform dataset, such works are hard to compare with each other. In this work, we aim to achieve two main objectives– 1. propose code-centric features that are relevant to security program analysis tasks like vulnerability detection; 2. a thorough (qualitative and quantitative) examination of the existing code datasets that demonstrate the main characteristics of the individual datasets to have a clear comparison. Our evaluation finds exciting facts about existing datasets highlighting gaps that need to be addressed.},
booktitle = {Proceedings of the 16th Innovations in Software Engineering Conference},
articleno = {6},
numpages = {10},
keywords = {software vulnerability, software metrics, program graphs, datasets},
location = {Allahabad, India},
series = {ISEC '23}
}

@article{10.1145/3548684,
author = {Cruz-Carlon, Juan and Varshosaz, Mahsa and Le Goues, Claire and Wasowski, Andrzej},
title = {Patching Locking Bugs Statically with Crayons},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3548684},
doi = {10.1145/3548684},
abstract = {The Linux Kernel is a world-class operating system controlling most of our computing infrastructure: mobile devices, Internet routers and services, and most of the supercomputers. Linux is also an example of low-level software with no comprehensive regression test suite (for good reasons). The kernel’s tremendous societal importance imposes strict stability and correctness requirements. These properties make Linux a challenging and relevant target for static automated program repair (APR). Over the past decade, a significant progress has been made in dynamic APR. However, dynamic APR techniques do not translate naturally to systems without tests. We present a static APR technique addressing sequential locking API misuse bugs in the Linux Kernel. We attack the key challenge of static APR, namely, the lack of detailed program specification, by combining static analysis with machine learning to complement the information presented by the static analyzer. In experiments on historical real-world bugs in the kernel, we were able to automatically re-produce or propose equivalent patches in 85% of the human-made patches, and automatically rank them among the top three candidates for 64% of the cases and among the top five for 74%.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {56},
numpages = {28},
keywords = {api misuse, static program repair, Automated repair}
}

@inproceedings{10.1145/3650212.3652132,
author = {Chen, Jialuo and Wang, Jingyi and Sun, Youcheng and Cheng, Peng and Chen, Jiming},
title = {Isolation-Based Debugging for Neural Networks},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3652132},
doi = {10.1145/3650212.3652132},
abstract = {Neural networks (NNs) are known to have diverse defects such as adversarial examples, backdoor and discrimination, raising great concerns about their reliability. While NN testing can effectively expose these defects to a significant degree, understanding their root causes within the network requires further examination. In this work, inspired by the idea of debugging in traditional software for failure isolation, we propose a novel unified neuron-isolation-based framework for debugging neural networks, shortly IDNN. Given a buggy NN that exhibits certain undesired properties (e.g., discrimination), the goal of IDNN is to identify the most critical and minimal set of neurons that are responsible for exhibiting these properties. Notably, such isolation is conducted with the objective that by simply ‘freezing’ these neurons, the model’s undesired properties can be eliminated, resulting in a much more efficient model repair compared to computationally expensive retraining or weight optimization as in existing literature. We conduct extensive experiments to evaluate IDNN across a diverse set of NN structures on five benchmark datasets, for solving three debugging tasks, including backdoor, unfairness, and weak class. As a lightweight framework, IDNN outperforms state-of-the-art baselines by successfully identifying and isolating a very small set of responsible neurons, demonstrating superior generalization performance across all tasks.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {338–349},
numpages = {12},
keywords = {Debugging, Fault Isolation, Neural Network},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3453483.3454045,
author = {He, Jingxuan and Lee, Cheng-Chun and Raychev, Veselin and Vechev, Martin},
title = {Learning to find naming issues with big code and small supervision},
year = {2021},
isbn = {9781450383912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453483.3454045},
doi = {10.1145/3453483.3454045},
abstract = {We introduce a new approach for finding and fixing naming issues in source code. The method is based on a careful combination of unsupervised and supervised procedures: (i) unsupervised mining of patterns from Big Code that express common naming idioms. Program fragments violating such idioms indicates likely naming issues, and (ii) supervised learning of a classifier on a small labeled dataset which filters potential false positives from the violations.  We implemented our method in a system called Namer and evaluated it on a large number of Python and Java programs. We demonstrate that Namer is effective in finding naming mistakes in real world repositories with high precision (~70%). Perhaps surprisingly, we also show that existing deep learning methods are not practically effective and achieve low precision in finding naming issues (up to ~16%).},
booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {296–311},
numpages = {16},
keywords = {Static analysis, Name-based program analysis, Machine learning, Bug detection, Anomaly detection},
location = {Virtual, Canada},
series = {PLDI 2021}
}

@inproceedings{10.1145/3661167.3661216,
author = {Siddiq, Mohammed Latif and Da Silva Santos, Joanna Cecilia and Tanvir, Ridwanul Hasan and Ulfat, Noshin and Al Rifat, Fahmid and Carvalho Lopes, Vin\'{\i}cius},
title = {Using Large Language Models to Generate JUnit Tests: An Empirical Study},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661216},
doi = {10.1145/3661167.3661216},
abstract = {A code generation model generates code by taking a prompt from a code comment, existing code, or a combination of both. Although code generation models (e.g., GitHub Copilot) are increasingly being adopted in practice, it is unclear whether they can successfully be used for unit test generation without fine-tuning for a strongly typed language like Java. To fill this gap, we investigated how well three models (Codex, GPT-3.5-Turbo, and StarCoder) can generate unit tests. We used two benchmarks (HumanEval and Evosuite SF110) to investigate the effect of context generation on the unit test generation process. We evaluated the models based on compilation rates, test correctness, test coverage, and test smells. We found that the Codex model achieved above 80% coverage for the HumanEval dataset, but no model had more than 2% coverage for the EvoSuite SF110 benchmark. The generated tests also suffered from test smells, such as Duplicated Asserts and Empty Tests.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {313–322},
numpages = {10},
keywords = {junit, large language models, test generation, test smells, unit testing},
location = {Salerno, Italy},
series = {EASE '24}
}

@inproceedings{10.1145/3558489.3559066,
author = {Al Debeyan, Fahad and Hall, Tracy and Bowes, David},
title = {Improving the performance of code vulnerability prediction using abstract syntax tree information},
year = {2022},
isbn = {9781450398602},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3558489.3559066},
doi = {10.1145/3558489.3559066},
abstract = {The recent emergence of the Log4jshell vulnerability demonstrates the importance of detecting code vulnerabilities in software systems. Software Vulnerability Prediction Models (VPMs) are a promising tool for vulnerability detection. Recent studies have focused on improving the performance of models to predict whether a piece of code is vulnerable or not (binary classification). However, such approaches are limited because they do not provide developers with information on the type of vulnerability that needs to be patched. We present our multiclass classification approach to improve the performance of vulnerability prediction models. Our approach uses abstract syntax tree n-grams to identify code clusters related to specific vulnerabilities. We evaluated our approach using real-world Java software vulnerability data. We report increased predictive performance compared to a variety of other models, for example, F-measure increases from 55% to 75% and MCC increases from 48% to 74%. Our results suggest that clustering software vulnerabilities using AST n-gram information is a promising approach to improve vulnerability prediction and enable specific information about the vulnerability type to be provided.},
booktitle = {Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {2–11},
numpages = {10},
keywords = {Software Vulnerability, Software Security, Machine learning},
location = {Singapore, Singapore},
series = {PROMISE 2022}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00027,
author = {Usman, Muhammad and Noller, Yannic and P\u{a}s\u{a}reanu, Corina S. and Sun, Youcheng and Gopinath, Divya},
title = {NeuroSPF: a tool for the symbolic analysis of neural networks},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00027},
doi = {10.1109/ICSE-Companion52605.2021.00027},
abstract = {This paper presents NeuroSPF, a tool for the symbolic analysis of neural networks. Given a trained neural network model, the tool extracts the architecture and model parameters and translates them into a Java representation that is amenable for analysis using the Symbolic PathFinder symbolic execution tool. Notably, NeuroSPF encodes specialized peer classes for parsing the model's parameters, thereby enabling efficient analysis. With NeuroSPF the user has the flexibility to specify either the inputs or the network internal parameters as symbolic, promoting the application of program analysis and testing approaches from software engineering to the field of machine learning. For instance, NeuroSPF can be used for coverage-based testing and test generation, finding adversarial examples and also constraint-based repair of neural networks, thus improving the reliability of neural networks and of the applications that use them. Video URL: https://youtu.be/seal8fG78LI},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {25–28},
numpages = {4},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3368089.3409687,
author = {Kampmann, Alexander and Havrikov, Nikolas and Soremekun, Ezekiel O. and Zeller, Andreas},
title = {When does my program do this? learning circumstances of software behavior},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409687},
doi = {10.1145/3368089.3409687},
abstract = {A program fails. Under which circumstances does the failure occur? Our Alhazenapproach starts with a run that exhibits a particular behavior and automatically determines input features associated with the behavior in question: (1) We use a grammar to parse the input into individual elements. (2) We use a decision tree learner to observe and learn which input elements are associated with the behavior in question. (3) We use the grammar to generate additional inputs to further strengthen or refute hypotheses as learned associations. (4) By repeating steps 2&nbsp;and&nbsp;3, we obtain a theory that explains and predicts the given behavior. In our evaluation using inputs for find, grep, NetHack, and a JavaScript transpiler, the theories produced by Alhazen predict and produce failures with high accuracy and allow developers to focus on a small set of input features: “grep fails whenever the --fixed-strings option is used in conjunction with an empty search string.”},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1228–1239},
numpages = {12},
keywords = {software behavior, machine learning, error diagnosis, debugging},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3597926.3598073,
author = {Bao, Shenglin and Sha, Chaofeng and Chen, Bihuan and Peng, Xin and Zhao, Wenyun},
title = {In Defense of Simple Techniques for Neural Network Test Case Selection},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598073},
doi = {10.1145/3597926.3598073},
abstract = {Although deep learning (DL) software has been pervasive in various applications, the brittleness of deep neural networks (DNN) hinders their deployment in many tasks especially high-stake ones. To mitigate the risk accompanied with DL software fault, a variety of DNN testing techniques have been proposed such as test case selection. Among those test case selection or prioritization methods, the uncertainty-based ones such as DeepGini have demonstrated their effectiveness in finding DNN’s faults. Recently, TestRank, a learning based test ranking method has shown their out-performance over simple uncertainty-based test selection methods. However, this is achieved with a more complicated design which needs to train a graph convolutional network and a multi-layer Perceptron. In this paper, we propose a novel and lightweight DNN test selection method to enhance the effectiveness of existing simple ones. Besides the DNN model’s uncertainty on test case itself, we take into account model’s uncertainty on its neighbors. This could diversify the selected test cases and improve the effectiveness of existing uncertainty-based test selection methods. Extensive experiments on 5 datasets demonstrate the effectiveness of our approach.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {501–513},
numpages = {13},
keywords = {test case selection, k-nearest neighbor, deep learning},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@article{10.1145/3712187,
author = {Pian, Weiguo and Li, Yinghua and Tian, Haoye and Sun, Tiezhu and Song, Yewei and Tang, Xunzhu and Habib, Andrew and Klein, Jacques and Bissyand\'{e}, Tegawend\'{e} F.},
title = {You Don’t Have to Say Where to Edit! jLED – Joint Learning to Localize and Edit Source Code},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3712187},
doi = {10.1145/3712187},
abstract = {Learning to edit code automatically is becoming more and more feasible. Thanks to recent advances in Neural Machine Translation (NMT), various case studies are being investigated where patches are automatically produced and assessed either automatically (using test suites) or by developers themselves. An appealing setting remains when the developer must provide a natural language input of the requirement for the code change. A recent proof of concept in the literature showed that it is indeed feasible to translate these natural language requirements into code changes. A recent advancement, MODIT [8], has shown promising results in code editing by leveraging natural language, code context, and location information as input. However, it struggles when location information is unavailable. While several studies [29, 81] have demonstrated the ability to edit source code without explicitly specifying the edit location, they still tend to generate edits with less accuracy at the line level. In this work, we address the challenge of generating code edits without precise location information, a scenario we consider crucial for the practical adoption of NMT in code development. To that end, we develop a novel joint training approach for both localization and source code editions. Building a benchmark based on over 70k commits (patches and messages), we demonstrate that our jLED (joint Localize and EDit) approach is effective. An ablation study further demonstrates the importance of our design choice in joint training.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Source Code Edition, Joint Learning, Automated Programming, Neural Machine Translation}
}

@inproceedings{10.1145/3416505.3423563,
author = {Palma, Stefano Dalla and Mohammadi, Majid and Di Nucci, Dario and Tamburri, Damian A.},
title = {Singling the odd ones out: a novelty detection approach to find defects in infrastructure-as-code},
year = {2020},
isbn = {9781450381246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416505.3423563},
doi = {10.1145/3416505.3423563},
abstract = {Infrastructure-as-Code (IaC) is increasingly adopted. However, little is known about how to best maintain and evolve it. Previous studies focused on defining Machine-Learning models to predict defect-prone blueprints using supervised binary classification. This class of techniques uses both defective and non-defective instances in the training phase. Furthermore, the high imbalance between defective and non-defective samples makes the training more difficult and leads to unreliable classifiers. In this work, we tackle the defect-prediction problem from a different perspective using novelty detection and evaluate the performance of three techniques, namely OneClassSVM, LocalOutlierFactor, and IsolationForest, and compare their performance with a baseline RandomForest binary classifier. Such models are trained using only non-defective samples: defective data points are treated as novelty because the number of defective samples is too little compared to defective ones. We conduct an empirical study on an extremely-imbalanced dataset consisting of 85 real-world Ansible projects containing only small amounts of defective instances. We found that novelty detection techniques can recognize defects with a high level of precision and recall, an AUC-PR up to 0.86, and an MCC up to 0.31. We deem our results can influence the current trends in defect detection and put forward a new research path toward dealing with this problem.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation},
pages = {31–36},
numpages = {6},
keywords = {Novelty Detection, Infrastructure-as-Code, Defect Prediction},
location = {Virtual, USA},
series = {MaLTeSQuE 2020}
}

@inproceedings{10.1145/3650212.3680342,
author = {He, Yifeng and Huang, Jiabo and Rong, Yuyang and Guo, Yiwen and Wang, Ethan and Chen, Hao},
title = {UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large Language Models for Program Testing},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680342},
doi = {10.1145/3650212.3680342},
abstract = {The remarkable capability of large language models (LLMs) in 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
generating high-quality code has drawn increasing attention 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
in the software testing community.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
However, existing code LLMs often demonstrate unsatisfactory capabilities in generating accurate, complete tests
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
since they were trained on code snippets collected without 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
differentiating between code for testing and for other purposes.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
In this paper, we present a large-scale dataset, UniTSyn, which can enhance LLMs for Unit Test Synthesis. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Associating tests with the tested functions is crucial for LLMs to infer the expected behavior and the logic paths to be verified.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
By leveraging Language Server Protocol, UniTSyn achieves the challenging goal of collecting focal-test pairs without per-project execution setups or per-language heuristics, which tend to be fragile and difficult to scale.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Containing 2.7 million focal-test pairs across five mainstream programming languages, it can enhance the test generation ability of LLMs.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Our experiments demonstrate that, 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
by building an autoregressive LLM based on UniTSyn,
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
we can achieve significant benefits in learning and understanding unit test representations, 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
resulting in improved generation accuracy and code coverage 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
across all the evaluated programming languages.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1061–1072},
numpages = {12},
keywords = {Large language models, dataset, software testing, test case generation},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3658644.3670329,
author = {Amjad, Abdul Haddi and Munir, Shaoor and Shafiq, Zubair and Gulzar, Muhammad Ali},
title = {Blocking Tracking JavaScript at the Function Granularity},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3670329},
doi = {10.1145/3658644.3670329},
abstract = {Modern websites extensively rely on JavaScript to implement both functionality and tracking. Existing privacy-enhancing content-blocking tools struggle against mixed scripts, which simultaneously implement both functionality and tracking. Blocking such scripts would break functionality, and not blocking them would allow tracking. We propose Not.js, a fine-grained JavaScript blocking tool that operates at the function-level granularity. Not.js's strengths lie in analyzing the dynamic execution context, including the call stack and calling context of each JavaScript function, and then encoding this context to build a rich graph representation. Not.js trains a supervised machine learning classifier on a webpage's graph representation to first detect tracking at the function-level and then automatically generates surrogate scripts that preserve functionality while removing tracking. Our evaluation of Not.js on the top-10K websites demonstrates that it achieves high precision (94%) and recall (98%) in detecting tracking functions, outperforming the state-of-the-art while being robust against off-the-shelf JavaScript obfuscation. Fine-grained detection of tracking functions allows Not.js to automatically generate surrogate scripts, which our evaluation shows that successfully remove tracking functions without causing major breakage. Our deployment of Not.js shows that mixed scripts are present on 62.3% of the top-10K websites, with 70.6% of the mixed scripts being third-party that engage in tracking activities such as cookie ghostwriting.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {2177–2191},
numpages = {15},
keywords = {code refactoring, privacy, software engineering, web},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@inproceedings{10.1145/3649329.3656526,
author = {Li, Wenxing and Lyu, Hongqin and Liang, Shengwen and Wang, Tiancheng and Li, Huawei},
title = {SmartATPG: Learning-based Automatic Test Pattern Generation with Graph Convolutional Network and Reinforcement Learning},
year = {2024},
isbn = {9798400706011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649329.3656526},
doi = {10.1145/3649329.3656526},
abstract = {Automatic test pattern generation (ATPG) is a critical technology in integrated circuit testing. It searches for effective test vectors to detect all possible faults in the circuit as entirely as possible, thereby ensuring chip yield and improving chip quality. However, the process of searching for test vectors is NP-complete. At the same time, the large amount of backtracking generated during the search for test vectors can directly affect the performance of ATPG. In this paper, a learning-based ATPG framework, SmartATPG, is proposed to search for high-quality test vectors, reduce the number of backtracking during the search process, and thereby improve the performance of ATPG. SmartATPG utilizes graph convolutional network (GCN) to fully extract circuit feature information and efficiently explore the ATPG search space through reinforcement learning (RL). Experimental results indicate that the proposed SmartATPG outperforms traditional and artificial neural network (ANN)-based heuristic strategies on most benchmark circuits.},
booktitle = {Proceedings of the 61st ACM/IEEE Design Automation Conference},
articleno = {296},
numpages = {6},
location = {San Francisco, CA, USA},
series = {DAC '24}
}

@inproceedings{10.1145/3387940.3392250,
author = {Rahman, Karishma and Kahanda, Indika and Kanewala, Upulee},
title = {MRpredT: Using Text Mining for Metamorphic Relation Prediction},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392250},
doi = {10.1145/3387940.3392250},
abstract = {Metamorphic relations (MRs) are an essential component of metamorphic testing (MT) that highly affects its fault detection effectiveness. MRs are usually identified with the help of a domain expert, which is a labor-intensive task. In this work, we explore the feasibility of a text classification-based machine learning approach to predict MRs using their program documentation as the sole input. We compare our method to our previously developed graph kernelbased machine learning approach and demonstrate that textual features extracted from program documentation are highly effective for predicting metamorphic relations for matrix calculation programs.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {420–424},
numpages = {5},
keywords = {Text classification, Metamorphic testing, Metamorphic relations},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3700838.3703656,
author = {Garu, Amit and Gupta, Arya Tanmay and Nguyen, Duong Ngoc and Kulkarni, Sandeep},
title = {Quantitative characterization of consistency violating faults at individual distributed computing processes},
year = {2025},
isbn = {9798400710629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3700838.3703656},
doi = {10.1145/3700838.3703656},
abstract = {Consistency Violation Fault (cvf) is a type of fault where stale information causes a distributed computation to execute incorrect transitions. Since cvf s are common in practice, understanding cvf s is important in designing resilient distributed programs. Some properties of cvf s in a few case-study self-stabilizing distributed programs have been investigated in previous studies. However, more analyses are still needed to obtain a more comprehensive quantitative characterization of this class of faults.In this study, we focus on factors that could influence the impact of cvf s, such as the location and timing of cvf s. We also extend our analysis to more case-study problems, such as distributed machine learning. Our preliminary results show that location has an important role in determining the impact of cvf.},
booktitle = {Proceedings of the 26th International Conference on Distributed Computing and Networking},
pages = {274–275},
numpages = {2},
keywords = {Consistency violation fault, Stabilization, Fault-tolerance, Distributed monitoring, Static analysis},
location = {
},
series = {ICDCN '25}
}

@inproceedings{10.1145/3661167.3661288,
author = {La Gamba, Davide and Iuliano, Gerardo and Recupito, Gilberto and Giordano, Giammaria and Ferrucci, Filomena and Di Nucci, Dario and Palomba, Fabio},
title = {Toward a Search-Based Approach to Support the Design of Security Tests for Malicious Network Traffic},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661288},
doi = {10.1145/3661167.3661288},
abstract = {IoT devices generate and exchange large amounts of data daily, creating significant security and privacy challenges. Security testing, particularly using Machine Learning (ML), helps identify and classify potential malicious network traffic. Previous research has shown how ML can aid in designing security tests for IoT attacks. This ongoing paper introduces a search-based approach using Genetic Algorithms (GAs) to evolve detection rules and detect intrusion attacks. We build on existing GA methods for intrusion detection and compare them with leading ML models. We propose 17 detection rules and demonstrate that while GAs do not fully replace ML, they perform well with ample attack examples and enhance the usability and implementation of deterministic test cases by security testers.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {624–628},
numpages = {5},
keywords = {Genetic Algorithms, Internet-Of-Things, Intrusion Detection Attacks, Security Test Code Design, Security Testing.},
location = {Salerno, Italy},
series = {EASE '24}
}

@article{10.1145/3527317,
author = {Liu, Jiawei and Wei, Yuxiang and Yang, Sen and Deng, Yinlin and Zhang, Lingming},
title = {Coverage-guided tensor compiler fuzzing with joint IR-pass mutation},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3527317},
doi = {10.1145/3527317},
abstract = {In the past decade, Deep Learning (DL) systems have been widely deployed in various application domains to facilitate our daily life, e.g., natural language processing, healthcare, activity recognition, and autonomous driving. Meanwhile, it is extremely challenging to ensure the correctness of DL systems (e.g., due to their intrinsic nondeterminism), and bugs in DL systems can cause serious consequences and may even threaten human lives. In the literature, researchers have explored various techniques to test, analyze, and verify DL models, since their quality directly affects the corresponding system behaviors. Recently, researchers have also proposed novel techniques for testing the underlying operator-level DL libraries (such as TensorFlow and PyTorch), which provide general binary implementations for each high-level DL operator and are the foundation for running DL models on different hardware platforms. However, there is still limited work targeting the reliability of the emerging tensor compilers (also known as DL compilers), which aim to automatically compile high-level tensor computation graphs directly into high-performance binaries for better efficiency, portability, and scalability than traditional operator-level libraries. Therefore, in this paper, we target the important problem of tensor compiler testing, and have proposed Tzer, a practical fuzzing technique for the widely used TVM tensor compiler. Tzer focuses on mutating the low-level Intermediate Representation (IR) for TVM due to the limited mutation space for the high-level IR. More specifically, Tzer leverages both general-purpose and tensor-compiler-specific mutators guided by coverage feedback for diverse and evolutionary IR mutation; furthermore, since tensor compilers provide various passes (i.e., transformations) for IR optimization, Tzer also performs pass mutation in tandem with IR mutation for more effective fuzzing. Our experimental results show that Tzer substantially outperforms existing fuzzing techniques on tensor compiler testing, with 75% higher coverage and 50% more valuable tests than the 2nd-best technique. Also, different components of Tzer have been validated via ablation study. To date, Tzer has detected 49 previously unknown bugs for TVM, with 37 bugs confirmed and 25 bugs fixed (PR merged).},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {73},
numpages = {26},
keywords = {Machine Learning Systems, Fuzzing, Compiler Testing}
}

@inproceedings{10.1145/3510003.3510153,
author = {Kharkar, Anant and Moghaddam, Roshanak Zilouchian and Jin, Matthew and Liu, Xiaoyu and Shi, Xin and Clement, Colin and Sundaresan, Neel},
title = {Learning to reduce false positives in analytic bug detectors},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510153},
doi = {10.1145/3510003.3510153},
abstract = {Due to increasingly complex software design and rapid iterative development, code defects and security vulnerabilities are prevalent in modern software. In response, programmers rely on static analysis tools to regularly scan their codebases and find potential bugs. In order to maximize coverage, however, these tools generally tend to report a significant number of false positives, requiring developers to manually verify each warning. To address this problem, we propose a Transformer-based learning approach to identify false positive bug warnings. We demonstrate that our models can improve the precision of static analysis by 17.5%. In addition, we validated the generalizability of this approach across two major bug types: null dereference and resource leak.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1307–1316},
numpages = {10},
keywords = {datasets, gaze detection, neural networks, text tagging},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1109/ASE56229.2023.00133,
author = {Huo, Yintong and Li, Yichen and Su, Yuxin and He, Pinjia and Xie, Zifan and Lyu, Michael R.},
title = {AutoLog: A Log Sequence Synthesis Framework for Anomaly Detection},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00133},
doi = {10.1109/ASE56229.2023.00133},
abstract = {The rapid progress of modern computing systems has led to a growing interest in informative run-time logs. Various log-based anomaly detection techniques have been proposed to ensure software reliability. However, their implementation in the industry has been limited due to the lack of high-quality public log resources as training datasets.While some log datasets are available for anomaly detection, they suffer from limitations in (1) comprehensiveness of log events; (2) scalability over diverse systems; and (3) flexibility of log utility. To address these limitations, we propose AutoLog, the first automated log generation methodology for anomaly detection. AutoLog uses program analysis to generate runtime log sequences without actually running the system. AutoLog starts with probing comprehensive logging statements associated with the call graphs of an application. Then, it constructs execution graphs for each method after pruning the call graphs to find log-related execution paths in a scalable manner. Finally, AutoLog propagates the anomaly label to each acquired execution path based on human knowledge. It generates flexible log sequences by walking along the log execution paths with controllable parameters. Experiments on 50 popular Java projects show that AutoLog acquires significantly more (9x-58x) log events than existing log datasets from the same system, and generates log messages much faster (15x) with a single machine than existing passive data collection approaches. AutoLog also provides hyper-parameters to adjust the data size, anomaly rate, and component indicator for simulating different real-world scenarios. We further demonstrate AutoLog's practicality by showing that AutoLog enables log-based anomaly detectors to achieve better performance (1.93%) compared to existing log datasets. We hope AutoLog can facilitate the benchmarking and adoption of automated log analysis techniques.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {497–509},
numpages = {13},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1109/ASE51524.2021.9678586,
author = {Stallenberg, Dimitri and Olsthoorn, Mitchell and Panichella, Annibale},
title = {Improving test case generation for REST APIs through hierarchical clustering},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678586},
doi = {10.1109/ASE51524.2021.9678586},
abstract = {With the ever-increasing use of web APIs in modern-day applications, it is becoming more important to test the system as a whole. In the last decade, tools and approaches have been proposed to automate the creation of system-level test cases for these APIs using evolutionary algorithms (EAs). One of the limiting factors of EAs is that the genetic operators (crossover and mutation) are fully randomized, potentially breaking promising patterns in the sequences of API requests discovered during the search. Breaking these patterns has a negative impact on the effectiveness of the test case generation process. To address this limitation, this paper proposes a new approach that uses Agglomerative Hierarchical Clustering (AHC) to infer a linkage tree model, which captures, replicates, and preserves these patterns in new test cases. We evaluate our approach, called LT-MOSA, by performing an empirical study on 7 real-world benchmark applications w.r.t. branch coverage and real-fault detection capability. We also compare LT-MOSA with the two existing state-of-the-art white-box techniques (MIO, MOSA) for REST API testing. Our results show that LT-MOSA achieves a statistically significant increase in test target coverage (i.e., lines and branches) compared to MIO and MOSA in 4 and 5 out of 7 applications, respectively. Furthermore, LT-MOSA discovers 27 and 18 unique real-faults that are left undetected by MIO and MOSA, respectively.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {117–128},
numpages = {12},
keywords = {test case generation, system-level testing, search-based software engineering, machine learning},
location = {Melbourne, Australia},
series = {ASE '21}
}

@article{10.14778/3712221.3712251,
author = {Ni, Shengquan and Huang, Yicong and Wang, Zuozhi and Li, Chen},
title = {IcedTea: Efficient and Responsive Time-Travel Debugging in Dataflow Systems},
year = {2024},
issue_date = {November 2024},
publisher = {VLDB Endowment},
volume = {18},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3712221.3712251},
doi = {10.14778/3712221.3712251},
abstract = {Dataflow systems have an increasing need to support a wide range of tasks in data-centric applications using latest techniques such as machine learning. These tasks often involve custom functions with complex internal states. Consequently, users need enhanced debugging support to understand runtime behaviors and investigate internal states of dataflows. Traditional forward debuggers allow users to follow the chronological order of operations in an execution. Therefore, a user cannot easily identify a past runtime behavior after an unexpected result is produced. In this paper, we present a novel time-travel debugging paradigm called IcedTea, which supports reverse debugging. In particular, in a dataflow's execution, which is inherently distributed across multiple operators, the user can periodically interact with the job and retrieve the global states of the operators. After the execution, the system allows the user to roll back the dataflow state to any past interactions. The user can use step instructions to repeat the past execution to understand how data was processed in the original execution. We give a full specification of this powerful paradigm, study how to reduce its runtime overhead and develop techniques to support debugging instructions responsively. Our experiments on real-world datasets and workflows show that IcedTea can support responsive time-travel debugging with low time and space overhead.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {902–914},
numpages = {13}
}

@inproceedings{10.1145/3626246.3654756,
author = {Huang, Yicong and Wang, Zuozhi and Li, Chen},
title = {Demonstration of Udon: Line-by-line Debugging of User-Defined Functions in Data Workflows},
year = {2024},
isbn = {9798400704222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626246.3654756},
doi = {10.1145/3626246.3654756},
abstract = {Many big data systems are written in languages such as C, C++, Java, and Scala for high efficiency, whereas data analysts often use Python to conduct data wrangling, statistical analysis, and machine learning. User-defined functions (UDFs) are commonly used in these systems to bridge the gap between the two ecosystems. Debugging complex UDFs in data-processing systems is challenging due to the required coordination between language debuggers and the data-processing engine, as well as the debugging overhead on large volumes of data. In this paper, we showcase Udon, a novel debugger to support line-by-line debugging of UDFs in data-processing systems. Udon encapsulates modern line-by-line debugging primitives, such as those to set breakpoints, perform code inspections, and make code modifications while executing a UDF on a single tuple. In this demonstration, we use real-world scenarios to showcase the experience of using Udon for line-by-line debugging of a UDF.},
booktitle = {Companion of the 2024 International Conference on Management of Data},
pages = {476–479},
numpages = {4},
keywords = {data workflows, debugging, python udf, user-defined functions},
location = {Santiago AA, Chile},
series = {SIGMOD '24}
}

@article{10.1145/3122787,
author = {Balkan, Ayca and Tabuada, Paulo and Deshmukh, Jyotirmoy V. and Jin, Xiaoqing and Kapinski, James},
title = {Underminer: A Framework for Automatically Identifying Nonconverging Behaviors in Black-Box System Models},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1539-9087},
url = {https://doi.org/10.1145/3122787},
doi = {10.1145/3122787},
abstract = {Evaluation of industrial embedded control system designs is a time-consuming and imperfect process. While an ideal process would apply a formal verification technique such as model checking or theorem proving, these techniques do not scale to industrial design problems, and it is often difficult to use these techniques to verify performance aspects of control system designs, such as stability or convergence. For industrial designs, engineers rely on testing processes to identify critical or unexpected behaviors. We propose a novel framework called Underminer to improve the testing process; this is an automated technique to identify nonconverging behaviors in embedded control system designs. Underminer treats the system as a black box and lets the designer indicate the model parameters, inputs, and outputs that are of interest. It differentiates convergent from nonconvergent behaviors using Convergence Classifier Functions (CCFs).The tool can be applied in the context of testing models created late in the controller development stage, where it assumes that the given model displays mostly convergent behavior and learns a CCF in an unsupervised fashion from such convergent model behaviors. This CCF is then used to guide a thorough exploration of the model with the help of optimization-guided techniques or adaptive sampling techniques, with the goal of identifying rare nonconvergent model behaviors. Underminer can also be used early in the development stage, where models may have some significant nonconvergent behaviors. Here, the framework permits designers to indicate their mental model for convergence by labeling behaviors as convergent/nonconvergent and then constructs a CCF using a supervised learning technique. In this use case, the goal is to use the CCF to test an improved design for the model. Underminer supports a number of convergence-like notions, such as those based on Lyapunov analysis and temporal logic, and also CCFs learned directly from labeled output behaviors using machine-learning techniques such as support vector machines and neural networks. We demonstrate the efficacy of Underminer by evaluating its performance on several academic as well as industrial examples.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = dec,
articleno = {20},
numpages = {28},
keywords = {stability, machine learning, formal methods, Automatic testing}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00036,
author = {Beller, Moritz and Wong, Chu-Pan and Bader, Johannes and Scott, Andrew and Machalica, Mateusz and Chandra, Satish and Meijer, Erik},
title = {What it would take to use mutation testing in industry: a study at Facebook},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00036},
doi = {10.1109/ICSE-SEIP52600.2021.00036},
abstract = {Traditionally, mutation testing generates an abundance of small deviations of a program, called mutants. At industrial systems the scale and size of Facebook's, doing this is infeasible. We should not create mutants that the test suite would likely fail on or that give no actionable signal to developers. To tackle this problem, in this paper, we semi-automatically learn error-inducing patterns from a corpus of common Java coding errors and from changes that caused operational anomalies at Facebook specifically. We combine the mutations with instrumentation that measures which tests exactly visited the mutated piece of code. Results on more than 15,000 generated mutants show that more than half of the generated mutants survive Facebook's rigorous test suite of unit, integration, and system tests. Moreover, in a case study with 26 developers, all but two expressed that the mutation exposed a lack of testing in principle. As such, almost half of the 26 would actually act on the mutant presented to them by adapting an existing or creating a new test. The others did not for a variety of reasons often outside the scope of mutation testing. It remains a practical challenge how we can include such external information to increase the actionability rate on mutants.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {268–277},
numpages = {10},
keywords = {mutation testing, mutation monkey, machine learning, getafix},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1145/2499393.2499395,
author = {Herbold, Steffen},
title = {Training data selection for cross-project defect prediction},
year = {2013},
isbn = {9781450320160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499393.2499395},
doi = {10.1145/2499393.2499395},
abstract = {Software defect prediction has been a popular research topic in recent years and is considered as a means for the optimization of quality assurance activities. Defect prediction can be done in a within-project or a cross-project scenario. The within-project scenario produces results with a very high quality, but requires historic data of the project, which is often not available. For the cross-project prediction, the data availability is not an issue as data from other projects is readily available, e.g., in repositories like PROMISE. However, the quality of the defect prediction results is too low for practical use. Recent research showed that the selection of appropriate training data can improve the quality of cross-project defect predictions. In this paper, we propose distance-based strategies for the selection of training data based on distributional characteristics of the available data. We evaluate the proposed strategies in a large case study with 44 data sets obtained from 14 open source projects. Our results show that our training data selection strategy improves the achieved success rate of cross-project defect predictions significantly. However, the quality of the results still cannot compete with within-project defect prediction.},
booktitle = {Proceedings of the 9th International Conference on Predictive Models in Software Engineering},
articleno = {6},
numpages = {10},
keywords = {cross-project prediction, defect-prediction, machine learning},
location = {Baltimore, Maryland, USA},
series = {PROMISE '13}
}

@inproceedings{10.1145/3540250.3549095,
author = {Alon, Yoav and David, Cristina},
title = {Using graph neural networks for program termination},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549095},
doi = {10.1145/3540250.3549095},
abstract = {Termination analyses investigate the termination behavior of programs, intending to detect nontermination, which is known to cause a variety of program bugs (e.g. hanging programs,  
denial-of-service vulnerabilities). Beyond formal approaches, various attempts have been made to estimate the termination behavior of programs using neural networks. However, the majority of these  
approaches continue to rely on formal methods to provide strong soundness guarantees and consequently suffer from similar limitations. In this paper, we move away from formal methods and embrace the stochastic nature of machine learning models. Instead of aiming for rigorous guarantees  
that can be interpreted by solvers, our objective is to provide an estimation of a program's termination behavior and of the likely reason for nontermination (when applicable) that a programmer can use for debugging purposes. Compared to previous approaches using neural networks for program termination, we also take advantage of the graph representation of programs by employing Graph Neural Networks. To further assist programmers in understanding and debugging nontermination bugs, we adapt the notions of attention and semantic segmentation, previously used for other application domains, to programs. Overall, we designed and implemented classifiers for program termination based on Graph Convolutional Networks and Graph Attention Networks, as well as a semantic segmentation Graph Neural Network that localizes AST nodes likely to cause nontermination. We also  
illustrated how the information provided by semantic segmentation can be combined with program slicing to further aid debugging.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {910–921},
numpages = {12},
keywords = {Program Termination, Program Nontermination, Graph Neural Networks, Graph Attention Networks},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/1150402.1150530,
author = {Ling, Charles X. and Sheng, Victor S. and Bruckhaus, Tilmann and Madhavji, Nazim H.},
title = {Maximum profit mining and its application in software development},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150530},
doi = {10.1145/1150402.1150530},
abstract = {While most software defects (i.e., bugs) are corrected and tested as part of the lengthy software development cycle, enterprise software vendors often have to release software products before all reported defects are corrected, due to deadlines and limited resources. A small number of these defects will be escalated by customers and they must be resolved immediately by the software vendors at a very high cost. In this paper, we develop an Escalation Prediction (EP) system that mines historic defect report data and predict the escalation risk of the defects for maximum net profit. More specifically, we first describe a simple and general framework to convert the maximum net profit problem to cost-sensitive learning. We then apply and compare several well-known cost-sensitive learning approaches for EP. Our experiments suggest that the cost-sensitive decision tree is the best method for producing the highest positive net profit and comprehensible results. The EP system has been deployed successfully in the product group of an enterprise software vendor.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {929–934},
numpages = {6},
keywords = {escalation prediction, data mining, cost-sensitive learning},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/3512850.3512862,
author = {Wang, Binbin and Wen, Mi and Song, Yan and Wang, Liangliang and Wang, Zihan and Mao, Qifan},
title = {MLPNeuzz: A Novel Neural Program Smoothing Method Based on Multi-Layer Perceptron},
year = {2022},
isbn = {9781450395717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512850.3512862},
doi = {10.1145/3512850.3512862},
abstract = {In recent years, using fuzzy methods to mine network security vulnerabilities has become a mainstream. Fuzzing is an effective vulnerability mining technology, which can find the potential vulnerability trigger point by traversing the program branch through some key algorithms. However, the traditional fuzzing methods exist some problems, such as redundant test cases, inefficient mutation strategy and so on. Therefore, a method combining machine learning with fuzzing has been proposed, which provides solutions to the above problems. Recently, someone proposes an effective fuzzer called NEUZZ, which uses a simple feedforward neural network (FNN) for neural program smoothing to model the branching behavior of the target program and improve the utilization of test cases. However, the traditional FNN model is easy to cause low learning efficiency and poor generalization ability and other problems. In order to solve these problems, a novel neural program smoothing method based on Multi-Layer Perceptron (MLP) is proposed in this paper, and we name the fuzzer as MLPNeuzz. MLPNeuzz can further collect edge coverage information and improve the smoothing effect of neural programs. In addition, we refine the original NEUZZ fuzzy method to make its fuzzy process more reasonable. Experiments on several real-world application programs show that the MLPNeuzz method proposed in this paper can achieve higher edge coverage than NEUZZ under the same time overhead.},
booktitle = {Proceedings of the 2022 8th International Conference on Computing and Data Engineering},
pages = {92–97},
numpages = {6},
location = {Bangkok, Thailand},
series = {ICCDE '22}
}

@inproceedings{10.1145/3468264.3468623,
author = {Patra, Jibesh and Pradel, Michael},
title = {Semantic bug seeding: a learning-based approach for creating realistic bugs},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468623},
doi = {10.1145/3468264.3468623},
abstract = {When working on techniques to address the wide-spread problem of software bugs, one often faces the need for a large number of realistic bugs in real-world programs. Such bugs can either help evaluate an approach, e.g., in form of a bug benchmark or a suite of program mutations, or even help build the technique, e.g., in learning-based bug detection. Because gathering a large number of real bugs is difficult, a common approach is to rely on automatically seeded bugs. Prior work seeds bugs based on syntactic transformation patterns, which often results in unrealistic bugs and typically cannot introduce new, application-specific code tokens.  This paper presents SemSeed, a technique for automatically seeding bugs in a semantics-aware way. The key idea is to imitate how a given real-world bug would look like in other programs by semantically adapting the bug pattern to the local context. To reason about the semantics of pieces of code, our approach builds on learned token embeddings that encode the semantic similarities of identifiers and literals. Our evaluation with real-world JavaScript software shows that the approach effectively reproduces real bugs and clearly outperforms a semantics-unaware approach. The seeded bugs are useful as training data for learning-based bug detection, where they significantly improve the bug detection ability. Moreover, we show that SemSeed-created bugs complement existing mutation testing operators, and that our approach is efficient enough to seed hundreds of thousands of bugs within an hour.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {906–918},
numpages = {13},
keywords = {token embeddings, machine learning, dataset, bugs, bug injection},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3382025.3414960,
author = {Str\"{u}der, Stefan and Mukelabai, Mukelabai and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Feature-oriented defect prediction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414960},
doi = {10.1145/3382025.3414960},
abstract = {Software errors are a major nuisance in software development and can lead not only to reputation damages, but also to considerable financial losses for companies. Therefore, numerous techniques for predicting software defects, largely based on machine learning methods, have been developed over the past decades. These techniques usually rely on code and process metrics in order to predict defects at the granularity of typical software assets, such as subsystems, components, and files. In this paper, we present the first systematic investigation of feature-oriented defect prediction: the prediction of defects at the granularity of features---domain-oriented entities abstractly representing (and often cross-cutting) typical software assets. Feature-oriented prediction can be beneficial, since: (i) particular features might be more error-prone than others, (ii) characteristics of features known as defective might be useful to predict other error-prone features, (iii) feature-specific code might be especially prone to faults arising from feature interactions. We present a dataset derived from 12 software projects and introduce two metric sets for feature-oriented defect prediction. We evaluated seven machine learning classifiers with three different attribute sets each, using our two new metric sets as well as an existing metric set from the literature. We observe precision and recall values of around 85% and better robustness when more diverse metrics sets with richer feature information are used.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {21},
numpages = {12},
keywords = {prediction, feature, defect, classification},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3609437.3609438,
author = {Shi, Chaoxuan and Zhu, Tingwei and Zhang, Tian and Pang, Jun and Pan, Minxue},
title = {Structural-semantics Guided Program Simplification for Understanding Neural Code Intelligence Models},
year = {2023},
isbn = {9798400708947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609437.3609438},
doi = {10.1145/3609437.3609438},
abstract = {Neural code intelligence models are cutting-edge automated code understanding technologies that have achieved remarkable performance in various software engineering tasks. However, the lack of deep learning models’ interpretability hinders the application of deep learning based code intelligence models in real-world scenarios, particularly in security-critical domains. Previous studies use program simplification to understand neural code intelligence models, but they have overlooked the fact that the most significant difference between source code and natural language is the code’s structural semantics. In this paper, we first conduct an empirical study to identify the critical code structural semantic features valued by neural code intelligence models, and then we propose a novel program simplification method called SSGPS (Structural-Semantics Guided Program Simplification). Results on three code summarization models show that SSGPS can reduce training and testing time by 20-40% while controlling the decrease in model performance by less than 4%, demonstrating that our method can retain the critical code structural semantics for understanding neural code intelligence models.},
booktitle = {Proceedings of the 14th Asia-Pacific Symposium on Internetware},
pages = {1–11},
numpages = {11},
keywords = {Program Simplification, Neural Code Intelligence Model, Interpretable AI, Code Structural Semantics},
location = {Hangzhou, China},
series = {Internetware '23}
}

@inproceedings{10.1145/3468264.3473931,
author = {Lampel, Johannes and Just, Sascha and Apel, Sven and Zeller, Andreas},
title = {When life gives you oranges: detecting and diagnosing intermittent job failures at Mozilla},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3473931},
doi = {10.1145/3468264.3473931},
abstract = {Continuous delivery of cloud systems requires constant running of jobs (build processes, tests, etc.). One issue that plagues this continuous integration (CI) process are intermittent failures - non-deterministic, false alarms that do not result from a bug in the software or job specification, but rather from issues in the underlying infrastructure. At Mozilla, such intermittent failures are called oranges as a reference to the color of the build status indicator. As such intermittent failures disrupt CI and lead to failures, they erode the developers' trust in the jobs. We present a novel approach that automatically classifies failing jobs to determine whether job execution failures arise from an actual software bug or were caused by flakiness in the job (e.g., test) or the underlying infrastructure. For this purpose, we train classification models using job telemetry data to diagnose failure patterns involving features such as runtime, cpu load, operating system version, or specific platform with high precision. In an evaluation on a set of Mozilla CI jobs, our approach achieves precision scores of 73%, on average, across all data sets with some test suites achieving precision scores good enough for fully automated classification (i.e., precision scores of up to 100%), and recall scores of 82% on average (up to 94%).},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1381–1392},
numpages = {12},
keywords = {machine learning, intermittent failures, flaky tests, continuous integration, Software testing},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3637882.3637894,
author = {Goodall, William and Wallace, Flynt and Jackson, Melody},
title = {KitBit: An Instrumented Collar for Indoor Pets},
year = {2024},
isbn = {9798400716560},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637882.3637894},
doi = {10.1145/3637882.3637894},
abstract = {This paper presents KitBit, a wearable activity tracker for cats, aiming to improve communication between domestic cats and their owners by monitoring feline behavior. We present a compact, lightweight, collar-mounted device with an onboard IMU (accelerometer/gyroscope) and wireless data transmission capabilities. This device was evaluated in a set of hour-long play sessions in the homes of 11 different domestic cats, where it showed good cat-ergonomics and a high degree of reliability while collecting IMU data. An LSTM model trained on the data achieved an overall accuracy of 80.13% in determining the cats’ level of activity compared to a manually-labeled ground-truth dataset. This result shows the feasibility of tracking domestic cats’ activities, and the potential for future developments in feline wearable technology.},
booktitle = {Proceedings of the Tenth International Conference on Animal-Computer Interaction},
articleno = {8},
numpages = {8},
keywords = {activity tracking, cats, machine learning, wearable technology},
location = {Raleigh, NC, USA},
series = {ACI '23}
}

@inproceedings{10.1145/3442167.3442177,
author = {Spring, Jonathan M. and Galyardt, April and Householder, Allen D. and VanHoudnos, Nathan},
title = {On managing vulnerabilities in AI/ML systems},
year = {2021},
isbn = {9781450389952},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442167.3442177},
doi = {10.1145/3442167.3442177},
abstract = {This paper explores how the current paradigm of vulnerability management might adapt to include machine learning systems through a thought experiment: what if flaws in machine learning (ML) were assigned Common Vulnerabilities and Exposures (CVE) identifiers (CVE-IDs)? We consider both ML algorithms and model objects. The hypothetical scenario is structured around exploring the changes to the six areas of vulnerability management: discovery, report intake, analysis, coordination, disclosure, and response. While algorithm flaws are well-known in academic research community, there is no apparent clear line of communication between this research community and the operational communities that deploy and manage systems that use ML. The thought experiments identify some ways in which CVE-IDs may establish some useful lines of communication between these two communities. In particular, it would start to introduce the research community to operational security concepts, which appears to be a gap left by existing efforts.},
booktitle = {Proceedings of the New Security Paradigms Workshop 2020},
pages = {111–126},
numpages = {16},
keywords = {vulnerability management, prioritization, machine learning, CVE-ID},
location = {Online, USA},
series = {NSPW '20}
}

@inproceedings{10.1145/3510003.3510214,
author = {Kang, Hong Jin and Aw, Khai Loong and Lo, David},
title = {Detecting false alarms from automatic static analysis tools: how far are we?},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510214},
doi = {10.1145/3510003.3510214},
abstract = {Automatic static analysis tools (ASATs), such as Findbugs, have a high false alarm rate. The large number of false alarms produced poses a barrier to adoption. Researchers have proposed the use of machine learning to prune false alarms and present only actionable warnings to developers. The state-of-the-art study has identified a set of "Golden Features" based on metrics computed over the characteristics and history of the file, code, and warning. Recent studies show that machine learning using these features is extremely effective and that they achieve almost perfect performance.We perform a detailed analysis to better understand the strong performance of the "Golden Features". We found that several studies used an experimental procedure that results in data leakage and data duplication, which are subtle issues with significant implications. Firstly, the ground-truth labels have leaked into features that measure the proportion of actionable warnings in a given context. Secondly, many warnings in the testing dataset appear in the training dataset. Next, we demonstrate limitations in the warning oracle that determines the ground-truth labels, a heuristic comparing warnings in a given revision to a reference revision in the future. We show the choice of reference revision influences the warning distribution. Moreover, the heuristic produces labels that do not agree with human oracles. Hence, the strong performance of these techniques previously seen is overoptimistic of their true performance if adopted in practice. Our results convey several lessons and provide guidelines for evaluating false alarm detectors.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {698–709},
numpages = {12},
keywords = {data duplication, data leakage, false alarms, static analysis},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@article{10.1145/3714463,
author = {Ji, Shunhui and Huang, Changrong and Ren, Bin and Dong, Hai and Grunske, Lars and Xiao, Yan and Zhang, Pengcheng},
title = {TAEFuzz: Automatic Fuzzing for Image-based Deep Learning Systems via Transferable Adversarial Examples},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3714463},
doi = {10.1145/3714463},
abstract = {Deep learning (DL) components have been broadly applied in diverse applications. Similar to traditional software engineering, effective test case generation methods are needed by industry to enhance the quality and robustness of these deep learning components. To this end, we propose a novel automatic software testing technique, TAEFuzz (Automatic Fuzz-Testing via Transferable Adversarial Examples), which aims to automatically assess and enhance the robustness of image-based deep learning (DL) systems based on test cases generated by transferable adversarial examples. TAEFuzz alleviates the over-fitting problem during optimized test case generation and prevents test cases from prematurely falling into local optima. In addition, TAEFuzz enhances the visual quality of test cases through constraining perturbations inserted into sensitive areas of the images. For a system with low robustness, TAEFuzz trains a low-cost denoising module to reduce the impact of perturbations in transferable adversarial examples on the system. Experimental results demonstrate that the test cases generated by TAEFuzz can discover up to 46.1% more errors in the targeted systems, and ensure the visual quality of test cases. Compared to existing techniques, TAEFuzz also enhances the robustness of the target systems against transferable adversarial examples with the perturbation denoising module.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Deep learning, fuzzing, transferable adversarial examples, robustness}
}

@article{10.1145/3503509,
author = {Yang, Yanming and Xia, Xin and Lo, David and Bi, Tingting and Grundy, John and Yang, Xiaohu},
title = {Predictive Models in Software Engineering: Challenges and Opportunities},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3503509},
doi = {10.1145/3503509},
abstract = {Predictive models are one of the most important techniques that are widely applied in many areas of software engineering. There have been a large number of primary studies that apply predictive models and that present well-performed studies in various research domains, including software requirements, software design and development, testing and debugging, and software maintenance. This article is a first attempt to systematically organize knowledge in this area by surveying a body of 421 papers on predictive models published between 2009 and 2020. We describe the key models and approaches used, classify the different models, summarize the range of key application areas, and analyze research results. Based on our findings, we also propose a set of current challenges that still need to be addressed in future work and provide a proposed research road map for these opportunities.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {56},
numpages = {72},
keywords = {survey, software engineering, deep learning, machine learning, Predictive models}
}

@inproceedings{10.1145/3422392.3422420,
author = {Lima, Rodrigo and Souza, Jairo and Fonseca, Baldoino and Teixeira, Leopoldo and Gheyi, Rohit and Ribeiro, M\'{a}rcio and Garcia, Alessandro and de Mello, Rafael},
title = {Understanding and Detecting Harmful Code},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422420},
doi = {10.1145/3422392.3422420},
abstract = {Code smells typically indicate poor design implementation and choices that may degrade software quality. Hence, they need to be carefully detected to avoid such poor design. In this context, some studies try to understand the impact of code smells on the software quality, while others propose rules or machine learning-based techniques to detect code smells. However, none of those studies or techniques focus on analyzing code snippets that are really harmful to software quality. This paper presents a study to understand and classify code harmfulness. We analyze harmfulness in terms of CLEAN, SMELLY, BUGGY, and HARMFUL code. By HARMFUL CODE, we define a SMELLY code element having one or more bugs reported. These bugs may have been fixed or not. Thus, the incidence of HARMFUL CODE may represent a increased risk of introducing new defects and/or design problems during its fixing. We perform our study with 22 smell types, 803 versions of 13 open-source projects, 40,340 bugs and 132,219 code smells. The results show that even though we have a high number of code smells, only 0.07% of those smells are harmful. The Abstract Function Call From Constructor is the smell type more related to HARMFUL CODE. To cross-validate our results, we also perform a survey with 60 developers. Most of them (98%) consider code smells harmful to the software, and 85% of those developers believe that code smells detection tools are important. But, those developers are not concerned about selecting tools that are able to detect HARMFUL CODE. We also evaluate machine learning techniques to classify code harmfulness: they reach the effectiveness of at least 97% to classify HARMFUL CODE. While the Random Forest is effective in classifying both SMELLY and HARMFUL CODE, the Gaussian Naive Bayes is the less effective technique. Our results also suggest that both software and developers' metrics are important to classify HARMFUL CODE.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {223–232},
numpages = {10},
keywords = {Code Smells, Machine Learning, Software Quality},
location = {Natal, Brazil},
series = {SBES '20}
}

@article{10.1145/3625290,
author = {Huang, Wei and Zhao, Xingyu and Banks, Alec and Cox, Victoria and Huang, Xiaowei},
title = {Hierarchical Distribution-aware Testing of Deep Learning},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3625290},
doi = {10.1145/3625290},
abstract = {With its growing use in safety/security-critical applications, Deep Learning (DL) has raised increasing concerns regarding its dependability. In particular, DL has a notorious problem of lacking robustness. Input added with adversarial perturbations, i.e., Adversarial Examples (AEs), are easily mispredicted by the DL model. Despite recent efforts made in detecting AEs via state-of-the-art attack and testing methods, they are normally input distribution–agnostic and/or disregard the perceptual quality of adversarial perturbations. Consequently, the detected AEs are irrelevant inputs in the application context or noticeably unrealistic to humans. This may lead to a limited effect on improving the DL model’s dependability, as the testing budget is likely to be wasted on detecting AEs that are encountered very rarely in its real-life operations. In this article, we propose a new robustness testing approach for detecting AEs that considers both the feature-level distribution and the pixel-level distribution, capturing the perceptual quality of adversarial perturbations. The two considerations are encoded by a novel hierarchical mechanism. First, we select test seeds based on the density of feature-level distribution and the vulnerability of adversarial robustness. The vulnerability of test seeds is indicated by the auxiliary information, which are highly correlated with local robustness. Given a test seed, we then develop a novel genetic algorithm–based local test case generation method, in which two fitness functions work alternatively to control the perceptual quality of detected AEs. Finally, extensive experiments confirm that our holistic approach considering hierarchical distributions is superior to the state-of-the-arts that either disregard any input distribution or only consider a single (non-hierarchical) distribution, in terms of not only detecting imperceptible AEs but also improving the overall robustness of the DL model under testing.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {42},
numpages = {35},
keywords = {safe AI, robustness growth, distribution-aware testing, natural perturbations, adversarial examples detection, Deep learning robustness}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00026,
author = {Li, Paul Luo and Chai, Xiaoyu and Campbell, Frederick and Liao, Jilong and Abburu, Neeraja and Kang, Minsuk and Niculescu, Irina and Brake, Greg and Patil, Siddharth and Dooley, James and Paddock, Brandon},
title = {Evolving software to be ML-driven utilizing real-world A/B testing: experiences, insights, challenges},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00026},
doi = {10.1109/ICSE-SEIP52600.2021.00026},
abstract = {ML-driven software is heralded as the next major advancement in software engineering; existing software today can benefit from being evolved to be ML-driven. In this paper, we contribute practical knowledge about evolving software to be ML-driven, utilizing real-world A/B testing. We draw on experiences evolving two software features from the Windows operating system to be ML-driven, with more than ten realworld A/B tests on millions of PCs over more than two years. We discuss practical reasons for using A/B testing to engineer ML-driven software, insights for success, as well as on-going realworld challenges. This knowledge may help practitioners, as well as help direct future research and innovations.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {170–179},
numpages = {10},
keywords = {software quality, software engineering, software development management, predictive models, machine learning algorithms, machine learning, learning (artificial intelligence), data analysis, big data applications},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1145/3387940.3391464,
author = {Briem, J\'{o}n Arnar and Smit, Jordi and Sellik, Hendrig and Rapoport, Pavel and Gousios, Georgios and Aniche, Maur\'{\i}cio},
title = {OffSide: Learning to Identify Mistakes in Boundary Conditions},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391464},
doi = {10.1145/3387940.3391464},
abstract = {Mistakes in boundary conditions are the cause of many bugs in software. These mistakes happen when, e.g., developers make use of '&lt;' or '&gt;' in cases where they should have used '&lt;=' or '&gt;='. Mistakes in boundary conditions are often hard to find and manually detecting them might be very time-consuming for developers. While researchers have been proposing techniques to cope with mistakes in the boundaries for a long time, the automated detection of such bugs still remains a challenge. We conjecture that, for a tool to be able to precisely identify mistakes in boundary conditions, it should be able to capture the overall context of the source code under analysis. In this work, we propose a deep learning model that learn mistakes in boundary conditions and, later, is able to identify them in unseen code snippets. We train and test a model on over 1.5 million code snippets, with and without mistakes in different boundary conditions. Our model shows an accuracy from 55% up to 87%. The model is also able to detect 24 out of 41 real-world bugs; however, with a high false positive rate. The existing state-of-the-practice linter tools are not able to detect any of the bugs. We hope this paper can pave the road towards deep learning models that will be able to support developers in detecting mistakes in boundary conditions.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {203–208},
numpages = {6},
keywords = {software testing, software engineering, machine learning for software testing, machine learning for software engineering, deep learning for software testing, boundary testing},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3377811.3380369,
author = {Bertolino, Antonia and Guerriero, Antonio and Miranda, Breno and Pietrantuono, Roberto and Russo, Stefano},
title = {Learning-to-rank vs ranking-to-learn: strategies for regression testing in continuous integration},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380369},
doi = {10.1145/3377811.3380369},
abstract = {In Continuous Integration (CI), regression testing is constrained by the time between commits. This demands for careful selection and/or prioritization of test cases within test suites too large to be run entirely. To this aim, some Machine Learning (ML) techniques have been proposed, as an alternative to deterministic approaches. Two broad strategies for ML-based prioritization are learning-to-rank and what we call ranking-to-learn (i.e., reinforcement learning). Various ML algorithms can be applied in each strategy. In this paper we introduce ten of such algorithms for adoption in CI practices, and perform a comprehensive study comparing them against each other using subjects from the Apache Commons project. We analyze the influence of several features of the code under test and of the test process. The results allow to draw criteria to support testers in selecting and tuning the technique that best fits their context.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1–12},
numpages = {12},
keywords = {continuous integration, machine learning, regression testing, test prioritization, test selection},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3611643.3616370,
author = {Zhang, Fuyuan and Hu, Xinwen and Ma, Lei and Zhao, Jianjun},
title = {DeepRover: A Query-Efficient Blackbox Attack for Deep Neural Networks},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616370},
doi = {10.1145/3611643.3616370},
abstract = {Deep neural networks (DNNs) achieved a significant performance breakthrough over the past decade and have been widely adopted in various industrial domains. However, a fundamental problem regarding DNN robustness is still not adequately addressed, which can potentially lead to many quality issues after deployment, e.g., safety, security, and reliability. An adversarial attack is one of the most commonly investigated techniques to penetrate a DNN by misleading the DNN’s decision through the generation of minor perturbations in the original inputs. More importantly, the adversarial attack is a crucial way to assess, estimate, and understand the robustness boundary of a DNN. Intuitively, a stronger adversarial attack can help obtain a tighter robustness boundary, allowing us to understand the potential worst-case scenario when a DNN is deployed. To push this further, in this paper, we propose DeepRover, a fuzzing-based blackbox attack for deep neural networks used for image classification. We show that DeepRover is more effective and query-efficient in generating adversarial examples than state-of-the-art blackbox attacks. Moreover, DeepRover can find adversarial examples at a finer-grained level than other approaches.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1384–1394},
numpages = {11},
keywords = {Adversarial Attacks, Blackbox Fuzzing, Deep Neural Networks},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3631991.3631996,
author = {Irie, Shinnosuke and Aman, Hirohisa and Amasaki, Sousuke and Yokogawa, Tomoyuki and Kawahara, Minoru},
title = {A Comparative Study of Hybrid Fault-Prone Module Prediction Models Using Association Rule and Random Forest},
year = {2023},
isbn = {9798400708053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631991.3631996},
doi = {10.1145/3631991.3631996},
abstract = {Many fault-prone module prediction methods are implemented using machine learning algorithms, and the random forest is well known as the simple and powerful one. However, since the random forest uses an ensemble of decision trees, it is hard to explain why the module is predicted as “fault-prone.” In order to compensate for such a weakness, there have been studies of hybrid prediction methods combining the association rule mining technique with the random forest. In the hybrid method, a module’s fault-proneness is first assessed by the association rules. Then, when the module’s feature does not match any rules, its fault-proneness is evaluated by the random forest model. This paper focuses on how to combine the two techniques and conducts a comparative study to explore a better hybrid prediction method. The empirical results show: (1) it is better to use both association rules of “faulty” and “non-faulty” rather than using only “faulty” rules; (2) it is better to train the random forest classifiers using all data regardless of whether or not they matched association rules.},
booktitle = {Proceedings of the 2023 5th World Symposium on Software Engineering},
pages = {33–38},
numpages = {6},
keywords = {association rule, fault-prone module prediction, random forest},
location = {Tokyo, Japan},
series = {WSSE '23}
}

@inproceedings{10.1145/3416505.3423564,
author = {Borovits, Nemania and Kumara, Indika and Krishnan, Parvathy and Palma, Stefano Dalla and Di Nucci, Dario and Palomba, Fabio and Tamburri, Damian A. and van den Heuvel, Willem-Jan},
title = {DeepIaC: deep learning-based linguistic anti-pattern detection in IaC},
year = {2020},
isbn = {9781450381246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416505.3423564},
doi = {10.1145/3416505.3423564},
abstract = {Linguistic anti-patterns are recurring poor practices concerning inconsistencies among the naming, documentation, and implementation of an entity. They impede readability, understandability, and maintainability of source code. This paper attempts to detect linguistic anti-patterns in infrastructure as code (IaC) scripts used to provision and manage computing environments. In particular, we consider inconsistencies between the logic/body of IaC code units and their names. To this end, we propose a novel automated approach that employs word embeddings and deep learning techniques. We build and use the abstract syntax tree of IaC code units to create their code embedments. Our experiments with a dataset systematically extracted from open source repositories show that our approach yields an accuracy between 0.785 and 0.915 in detecting inconsistencies.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation},
pages = {7–12},
numpages = {6},
keywords = {Word2Vec, Linguistic Anti-patterns, Infrastructure Code, IaC, Defects, Deep Learning, Code Embedding},
location = {Virtual, USA},
series = {MaLTeSQuE 2020}
}

@article{10.1145/3542946,
author = {Khanfir, Ahmed and Koyuncu, Anil and Papadakis, Mike and Cordy, Maxime and Bissyand\'{e}, Tegawende F. and Klein, Jacques and Le Traon, Yves},
title = {iBiR: Bug-report-driven Fault Injection},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3542946},
doi = {10.1145/3542946},
abstract = {Much research on software engineering relies on experimental studies based on fault injection. Fault injection, however, is not often relevant to emulate real-world software faults since it “blindly” injects large numbers of faults. It remains indeed challenging to inject few but realistic faults that target a particular functionality in a program. In this work, we introduce iBiR , a fault injection tool that addresses this challenge by exploring change patterns associated to user-reported faults. To inject realistic faults, we create mutants by re-targeting a bug-report-driven automated program repair system, i.e., reversing its code transformation templates. iBiR is further appealing in practice since it requires deep knowledge of neither code nor tests, just of the program’s relevant bug reports. Thus, our approach focuses the fault injection on the feature targeted by the bug report. We assess iBiR by considering the Defects4J dataset. Experimental results show that our approach outperforms the fault injection performed by traditional mutation testing in terms of semantic similarity with the original bug, when applied at either system or class levels of granularity, and provides better, statistically significant estimations of test effectiveness (fault detection). Additionally, when injecting 100 faults, iBiR injects faults that couple with the real ones in around 36% of the cases, while mutation testing achieves less than 4%.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {33},
numpages = {31},
keywords = {information retrieval, bug reports, mutation, Fault injection}
}

@article{10.1145/3660815,
author = {Yoon, Jaehan and Cha, Sooyoung},
title = {FeatMaker: Automated Feature Engineering for Search Strategy of Symbolic Execution},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660815},
doi = {10.1145/3660815},
abstract = {We present FeatMaker, a novel technique that automatically generates state features to enhance the search strategy of symbolic execution. Search strategies, designed to address the well-known state-explosion problem, prioritize which program states to explore. These strategies typically depend on a ”state feature” that describes a specific property of program states, using this feature to score and rank them. Recently, search strategies employing multiple state features have shown superior performance over traditional strategies that use a single, generic feature. However, the process of designing these features remains largely manual.   Moreover, manually crafting state features is both time-consuming and prone to yielding unsatisfactory results. The goal of this paper is to fully automate the process of generating state features for search strategies from scratch. The key idea is to leverage path-conditions, which are basic but vital information maintained by symbolic execution, as state features. A challenge arises when employing all path-conditions as state features, as it results in an excessive number of state features. To address this, we present a specialized algorithm that iteratively generates and refines state features based on data accumulated during symbolic execution. Experimental results on 15 open-source C programs show that FeatMaker significantly outperforms existing search strategies that rely on manually-designed features, both in terms of branch coverage and bug detection. Notably, FeatMaker achieved an average of 35.3% higher branch coverage than state-of-the-art strategies and discovered 15 unique bugs. Of these, six were detected exclusively by FeatMaker.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {108},
numpages = {22},
keywords = {Software Testing, Symbolic Execution}
}

@proceedings{10.1145/3694715,
title = {SOSP '24: Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles},
year = {2024},
isbn = {9798400712517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 30th ACM Symposium on Operating Systems Principles (SOSP 2024)! We are delighted to present these 43 papers that reflect today's broad range of topics that comprise modern computer systems research, including file and storage systems, memory systems, distributed systems, verification, security, system support for machine learning, microservices, fault-tolerance and reliability, debugging, and, of course, operating systems.},
location = {Austin, TX, USA}
}

@inproceedings{10.1145/3650212.3680368,
author = {Xue, Zhipeng and Gao, Zhipeng and Wang, Shaohua and Hu, Xing and Xia, Xin and Li, Shanping},
title = {SelfPiCo: Self-Guided Partial Code Execution with LLMs},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680368},
doi = {10.1145/3650212.3680368},
abstract = {Code executability plays a vital role in software debugging and testing (e.g., detecting runtime exceptions or assertion violations). However, code execution, especially partial or arbitrary code execution, is a non-trivial task due to missing definitions and complex third-party dependencies. To make partial code (such as code snippets posted on the web or code fragments deep inside complex software projects) executable, the existing study has proposed a machine learning model to predict the undefined element types and inject the pre-defined dummy values into execution. However, the performance of their tool is limited due to its simply designed dummy values and the inability to continue learning. In this paper, we design and implement a novel framework, named SelfPiCo (Self-Guided Partial Code Executor), to dynamically guide partial code execution by incorporating the open-source LLM (i.e., Code Llama) within an interactive loop. Particularly, SelfPiCo leverages few-shot in-context learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning based on fine-tuning the Code Llama model. SelfPiCo continuously learns from code execution results and refines its predictions step after step. Our evaluations demonstrate that SelfPiCo can execute 72.7% and 83.3% of all lines in the open-source code and Stack Overflow snippets, outperforming the most recent state-of-the-art Lexecutor by 37.9% and 33.5%, respectively. Moreover, SelfPiCo successfully detected 18 and 33 runtime type error issues by executing the partial code from eight GitHub software projects and 43 Stack Overflow posts, demonstrating the practical usage and potential application of our framework in practice.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1389–1401},
numpages = {13},
keywords = {Dynamic Analysis, Large Language Model, Partial Code Execution, Prompt Engineering},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3460319.3464813,
author = {Luo, Sicheng and Xu, Hui and Bi, Yanxiang and Wang, Xin and Zhou, Yangfan},
title = {Boosting symbolic execution via constraint solving time prediction (experience paper)},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464813},
doi = {10.1145/3460319.3464813},
abstract = {Symbolic execution is an essential approach for automated test case generation. However, the approach is generally not scalable to large programs. One critical reason is that the constraint solving problems in symbolic execution are generally hard. Consequently, the symbolic execution process may get stuck in solving such hard problems. To mitigate this issue, symbolic execution tools generally rely on a timeout threshold to terminate the solving. Such a timeout is generally set to a fixed, predefined value, e.g., five minutes in angr. Nevertheless, how to set a proper timeout is critical to the tool’s efficiency. This paper proposes an approach to tackle the problem by predicting the time required for solving a constraint model so that the symbolic execution engine could base on the information to determine whether to continue the current solving process. Due to the cost of the prediction itself, our approach triggers the predictor only when the solving time has exceeded a relatively small value. We have shown that such a predictor can achieve promising performance with several different machine learning models and datasets. By further employing an adaptive design, the predictor can achieve an F1-score ranging from 0.743 to 0.800 on these datasets. We then apply the predictor to eight programs and conduct simulation experiments. Results show that the efficiency of constraint solving for symbolic execution can be improved by 1.25x to 3x, depending on the distribution of the hardness of their constraint models.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {336–347},
numpages = {12},
keywords = {Symbolic execution, SMT solving, Adaptive machine learning},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1145/3540250.3549175,
author = {Le-Cong, Thanh and Kang, Hong Jin and Nguyen, Truong Giang and Haryono, Stefanus Agus and Lo, David and Le, Xuan-Bach D. and Huynh, Quyet Thang},
title = {AutoPruner: transformer-based call graph pruning},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549175},
doi = {10.1145/3540250.3549175},
abstract = {Constructing a static call graph requires trade-offs between soundness and precision.  
Program analysis techniques for constructing call graphs are unfortunately usually imprecise.  
To address this problem, researchers have recently proposed call graph pruning empowered by machine learning to post-process call graphs constructed by static analysis. A machine learning model is built to capture information from the call graph by extracting structural features for use in a random forest classifier. It then removes edges that are predicted to be false positives. Despite the improvements shown by machine learning models, they are still limited as they do not consider the source code semantics and thus often are not able to effectively distinguish true and false positives.  

In this paper, we present a novel call graph pruning technique, AutoPruner, for eliminating false positives in call graphs via both statistical semantic and structural analysis.  
Given a call graph constructed by traditional static analysis tools, AutoPruner takes a Transformer-based approach to capture the semantic relationships between the caller and callee functions associated with each edge in the call graph. To do so, AutoPruner fine-tunes a model of code that was pre-trained on a large corpus to represent source code based on descriptions of its semantics.  
Next, the model is used to extract semantic features from the functions related to each edge in the call graph. AutoPruner uses these semantic features together with the structural features extracted from the call graph to classify each edge via a feed-forward neural network. Our empirical evaluation on a benchmark dataset of real-world programs shows that AutoPruner outperforms the state-of-the-art baselines, improving on F-measure by up to 13% in identifying false-positive edges in a static call graph. Moreover, AutoPruner achieves improvements on two client analyses, including halving the false alarm rate on null pointer analysis and over 10% improvements on monomorphic call-site detection. Additionally, our ablation study and qualitative analysis show that the semantic features extracted by AutoPruner capture a remarkable amount of information for distinguishing between true and false positives.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {520–532},
numpages = {13},
keywords = {Transformer, Static Analysis, Pretrained Language Model, Call Graph Pruning},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@article{10.1145/3715106,
author = {Ramalho, Neilson C. L. and Amario de Souza, Higor and Lordello Chaim, Marcos},
title = {Testing and Debugging Quantum Programs: The Road to 2030},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715106},
doi = {10.1145/3715106},
abstract = {Quantum computing has existed in the theoretical realm for several decades. Recently, quantum computing has re-emerged as a promising technology to solve problems that a classical computer could take hundreds of years to solve. However, there are challenges and opportunities for academics and practitioners regarding software engineering practices for testing and debugging quantum programs. This paper presents a roadmap for addressing these challenges, pointing out the existing gaps in the literature and suggesting research directions. We discuss the limitations caused by noise, the no-cloning theorem, the lack of a standard architecture for quantum computers, among others. Regarding testing, we highlight gaps and opportunities related to transpilation, mutation analysis, input states with hybrid interfaces, program analysis, and coverage. For debugging, we present the current strategies, including classical techniques applied to quantum programs, quantum-specific assertions, and quantum-related bug patterns. We introduce a conceptual model to illustrate concepts regarding the testing and debugging of quantum programs and the relationship between them. Those concepts are used to identify and discuss research challenges to cope with quantum programs through 2030, focusing on the interfaces between classical and quantum computing and on creating testing and debugging techniques that take advantage of the unique quantum computing characteristics.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Quantum Software Testing, Quantum Software Debugging, Quantum Software Engineering}
}

@article{10.1145/3467895,
author = {Falessi, Davide and Ahluwalia, Aalok and Penta, Massimiliano DI},
title = {The Impact of Dormant Defects on Defect Prediction: A Study of 19 Apache Projects},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3467895},
doi = {10.1145/3467895},
abstract = {Defect prediction models can be beneficial to prioritize testing, analysis, or code review activities, and has been the subject of a substantial effort in academia, and some applications in industrial contexts. A necessary precondition when creating a defect prediction model is the availability of defect data from the history of projects. If this data is noisy, the resulting defect prediction model could result to be unreliable. One of the causes of noise for defect datasets is the presence of “dormant defects,” i.e., of defects discovered several releases after their introduction. This can cause a class to be labeled as defect-free while it is not, and is, therefore “snoring.” In this article, we investigate the impact of snoring on classifiers' accuracy and the effectiveness of a possible countermeasure, i.e., dropping too recent data from a training set. We analyze the accuracy of 15 machine learning defect prediction classifiers, on data from more than 4,000 defects and 600 releases of 19 open source projects from the Apache ecosystem. Our results show that on average across projects (i) the presence of dormant defects decreases the recall of defect prediction classifiers, and (ii) removing from the training set the classes that in the last release are labeled as not defective significantly improves the accuracy of the classifiers. In summary, this article provides insights on how to create defects datasets by mitigating the negative effect of dormant defects on defect prediction.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {4},
numpages = {26},
keywords = {dataset bias, fix-inducing changes, Defect prediction}
}

@inproceedings{10.1145/3624032.3624035,
author = {Guilherme, Vitor and Vincenzi, Auri},
title = {An initial investigation of ChatGPT unit test generation capability},
year = {2023},
isbn = {9798400716294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624032.3624035},
doi = {10.1145/3624032.3624035},
abstract = {Context: Software testing ensures software quality, but developers often disregard it. The use of automated testing generation is pursued to reduce the consequences of overlooked test cases in a software project. Problem: In the context of Java programs, several tools can completely automate generating unit test sets. Additionally, studies are conducted to offer evidence regarding the quality of the generated test sets. However, it is worth noting that these tools rely on machine learning and other AI algorithms rather than incorporating the latest advancements in Large Language Models (LLMs). Solution: This work aims to evaluate the quality of Java unit tests generated by an OpenAI LLM algorithm, using metrics like code coverage and mutation test score. Method: For this study, 33 programs used by other researchers in the field of automated test generation were selected. This approach was employed to establish a baseline for comparison purposes. For each program, 33 unit test sets were generated automatically, without human interference, by changing Open AI API parameters. After executing each test set, metrics such as code line coverage, mutation score, and success rate of test execution were collected to evaluate the efficiency and effectiveness of each set. Summary of Results: Our findings revealed that the OpenAI LLM test set demonstrated similar performance across all evaluated aspects compared to traditional automated Java test generation tools used in the previous research. These results are particularly remarkable considering the simplicity of the experiment and the fact that the generated test code did not undergo human analysis.},
booktitle = {Proceedings of the 8th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {15–24},
numpages = {10},
keywords = {testing tools, software testing, mutation testing, experimental software engineering, coverage testing, automated test generation},
location = {Campo Grande, MS, Brazil},
series = {SAST '23}
}

@inproceedings{10.1145/3671016.3671398,
author = {Liu, Di and Yan, Yanyan and Fan, Hongcheng and Feng, Yang},
title = {Mining Fix Patterns for System Interaction Bugs},
year = {2024},
isbn = {9798400707056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3671016.3671398},
doi = {10.1145/3671016.3671398},
abstract = {System interaction is a fundamental aspect of software development. It involves direct engagement between developers and operating systems, covering tasks such as file management, permission handling, environment dependencies, and parallel development. Accurate system interaction can boost software performance and enhance user experience. On the other hand, improper use often leads to software issues, impacting reliability and stability. Meanwhile, most system interaction bugs typically involve only a minor size of code and follow similar fix patterns. In this paper, we designed a technique to uncover common fix patterns for system interaction bugs. The technique converts bug-fixing behaviors into edit actions, then establishes feature vectors to complete their clustering. Based on this, we present a large-scale study on over 7,800 commits from 37 real Github repositories. We analyzed the results and summarized 19 common fix patterns across 9 categories. Further, we discuss the implications that can support related development, testing, and improvements. These findings will contribute to understanding the essence of system interaction bugs and provide insights for future studies.},
booktitle = {Proceedings of the 15th Asia-Pacific Symposium on Internetware},
pages = {367–376},
numpages = {10},
keywords = {Bug study, Fix patterns, Mining software repositories, System Interaction},
location = {Macau, China},
series = {Internetware '24}
}

@article{10.1145/3576038,
author = {Jin, Xianhao and Servant, Francisco},
title = {HybridCISave: A Combined Build and Test Selection Approach in Continuous Integration},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3576038},
doi = {10.1145/3576038},
abstract = {Continuous Integration (CI) is a popular practice in modern software engineering. Unfortunately, it is also a high-cost practice—Google and Mozilla estimate their CI systems in millions of dollars. To reduce the computational cost in CI, researchers developed approaches to selectively execute builds or tests that are likely to fail (and skip those likely to pass). In this article, we present a novel hybrid technique (HybridCISave) to improve on the limitations of existing techniques: to provide higher cost savings and higher safety. To provide higher cost savings, HybridCISave combines techniques to predict and skip executions of both full builds that are predicted to pass and partial ones (only the tests in them predicted to pass). To provide higher safety, HybridCISave combines the predictions of multiple techniques to obtain stronger certainty before it decides to skip a build or test. We evaluated HybridCISave by comparing its effectiveness with the existing build selection techniques over 100 projects and found that it provided higher cost savings at the highest safety. We also evaluated each design decision in HybridCISave and found that skipping both full and partial builds increased its cost savings and that combining multiple test selection techniques made it safer.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {93},
numpages = {39},
keywords = {test selection, build selection, Continuous Integration, Software maintenance}
}

@article{10.5555/3722479.3722531,
author = {Saben, Clark and Zeitz, Jessica and Chandrasekar, Prashant},
title = {Enabling Blind and Low-Vision (BLV) Developers with LLM-Driven Code Debugging},
year = {2024},
issue_date = {October 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {3},
issn = {1937-4771},
abstract = {BLVRUN is a command line shell script designed to offer developers within the blind and low-vision (BLV) community a succinct and insightful overview of traceback errors. Its primary function involves parsing errors and utilizing a refined large language model to generate informative error summaries. In terms of performance, our model rivals that of well-known models like ChatGPT or AI-chatbot plug-ins tailored for specific Integrated Development Environments (IDEs). Importantly, BLV users can seamlessly integrate this tool into their existing development workflows, eliminating the need for any modifications or adaptations to facilitate debugging tasks.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {204–215},
numpages = {12}
}

@proceedings{10.1145/3643788,
title = {APR '24: Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the fifth International Workshop on Automated Program Repair (APR 2024), hosted by International Conference on Software Engineering (ICSE) 2024. Since its inception in 2020, APR has become a central event of the program repair community, reflecting a growing interest in the field among the software engineering, programming language, machine learning and formal methods communities.APR 2024 continues the tradition of fostering interaction among researchers in program repair. As always, we are particularly focused on narrowing the divide between academic research and real-world industry applications.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/2950290.2950308,
author = {Hanam, Quinn and Brito, Fernando S. de M. and Mesbah, Ali},
title = {Discovering bug patterns in JavaScript},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2950308},
doi = {10.1145/2950290.2950308},
abstract = {JavaScript has become the most popular language used by developers for client and server side programming. The language, however, still lacks proper support in the form of warnings about potential bugs in the code. Most bug finding tools in use today cover bug patterns that are discovered by reading best practices or through developer intuition and anecdotal observation. As such, it is still unclear which bugs happen frequently in practice and which are important for developers to be fixed. We propose a novel semi-automatic technique, called BugAID, for discovering the most prevalent and detectable bug patterns. BugAID is based on unsupervised machine learning using language-construct-based changes distilled from AST differencing of bug fixes in the code. We present a large-scale study of common bug patterns by mining 105K commits from 134 server-side JavaScript projects. We discover 219 bug fixing change types and discuss 13 pervasive bug patterns that occur across multiple projects and can likely be prevented with better tool support. Our findings are useful for improving tools and techniques to prevent common bugs in JavaScript, guiding tool integration for IDEs, and making developers aware of common mistakes involved with programming in JavaScript.},
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {144–156},
numpages = {13},
keywords = {static analysis, data mining, Node.js, JavaScript, Bug patterns},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1145/3663529.3663832,
author = {Tao, Zhu and Gao, Yongqiang and Qi, Jiayi and Peng, Chao and Wu, Qinyun and Chen, Xiang and Yang, Ping},
title = {Neat: Mobile App Layout Similarity Comparison Based on Graph Convolutional Networks},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663832},
doi = {10.1145/3663529.3663832},
abstract = {A wide variety of device models, screen resolutions and operating systems have emerged with recent advances in mobile devices. As a result, the graphical user interface (GUI) layout in mobile apps has become increasingly complex due to this market fragmentation, with rapid iterations being the norm. Testing page layout issues under these circumstances hence becomes a resource-intensive task, requiring significant manpower and effort due to the vast number of device models and screen resolution adaptations. One of the most challenging issues to cover manually is multi-model and cross-version layout verification for the same GUI page. To address this issue, we propose Neat, a non-intrusive end-to-end mobile app layout similarity measurement tool that utilizes computer vision techniques for GUI element detection, layout feature extraction, and similarity metrics. Our empirical evaluation and industrial application have demonstrated that our approach is effective in improving the efficiency of layout assertion testing and ensuring application quality.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {104–114},
numpages = {11},
keywords = {CNN, GCN, Graphical User Interface, Mobile App, OCR, YOLOX},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@article{10.1145/3696450,
author = {Huang, Kai and Xu, Zhengzi and Yang, Su and Sun, Hongyu and Li, Xuejun and Yan, Zheng and Zhang, Yuqing},
title = {Evolving Paradigms in Automated Program Repair: Taxonomy, Challenges, and Opportunities},
year = {2024},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3696450},
doi = {10.1145/3696450},
abstract = {With the rapid development and large-scale popularity of program software, modern society increasingly relies on software systems. However, the problems exposed by software have also come to the fore. The software bug has become an important factor troubling developers. In this context, Automated Program Repair (APR) techniques have emerged, aiming to automatically fix software bug problems and reduce manual debugging work. In particular, benefiting from the advances in deep learning, numerous learning-based APR techniques have emerged in recent years, which also bring new opportunities for APR research. To give researchers a quick overview of APR techniques’ complete development and future opportunities, we review the evolution of APR techniques and discuss in depth the latest advances in APR research. In this article, the development of APR techniques is introduced in terms of four different patch generation schemes: search-based, constraint-based, template-based, and learning-based. Moreover, we propose a uniform set of criteria to review and compare each APR tool and then discuss the current state of APR development. Finally, we analyze current challenges and future directions, especially highlighting the critical opportunities that large language models bring to APR research.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {36},
numpages = {43},
keywords = {Automated program repair}
}

@inproceedings{10.1145/3671016.3671400,
author = {Liu, Zixi and Feng, Yang and Xu, Jiali and Xu, Baowen},
title = {ObjTest: Object-Level Mutation for Testing Object Detection Systems},
year = {2024},
isbn = {9798400707056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3671016.3671400},
doi = {10.1145/3671016.3671400},
abstract = {With the tremendous advancement of deep learning techniques, object detection (OD) systems have achieved significant development. These systems, powered by deep neural networks, are now widely employed in diverse applications, including autonomous driving, intelligent video surveillance, and industrial inspection. Despite their impressive capabilities, OD systems, being complex software entities, can manifest erroneous behaviors that potentially lead to substantial losses. Moreover, the inherent complexity of detecting and localizing multiple objects in an image adds to the challenges of data annotation and system testing. To alleviate these challenges, in this paper, we propose ObjTest, an object-level mutation approach for testing OD systems. We generate large-scale test data by inserting, replacing, and removing target objects in the images while preserving their oracle information properly. We further propose an uncertainty evaluation metric for the prediction of test cases and adopt them to guide the test generation. Our comprehensive evaluation of ObjTest across three well-known OD datasets reveals that it effectively identifies numerous recognition failures. The results demonstrate that our object-level mutation approach yields more naturalistic alterations compared to traditional image-level transformations. Furthermore, the tests derived from our uncertainty metric-driven guidance enhance error detection efficiency and offer substantial benefits for guiding the retraining of OD systems to boost their performance.},
booktitle = {Proceedings of the 15th Asia-Pacific Symposium on Internetware},
pages = {61–70},
numpages = {10},
keywords = {deep neural networks, metamorphic testing, object detection, test generation},
location = {Macau, China},
series = {Internetware '24}
}

@inproceedings{10.1145/3568813.3600130,
author = {Koutcheme, Charles and Sarsa, Sami and Leinonen, Juho and Haaranen, Lassi and Hellas, Arto},
title = {Evaluating Distance Measures for Program Repair},
year = {2023},
isbn = {9781450399760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568813.3600130},
doi = {10.1145/3568813.3600130},
abstract = {Background and Context: Struggling with programming assignments while learning to program is a common phenomenon in programming courses around the world. Supporting struggling students is a common theme in Computing Education Research (CER), where a wide variety of support methods have been created and evaluated. An important stream of research here focuses on program repair, where methods for automatically fixing erroneous code are used for supporting students as they debug their code. Work in this area has so far assessed the performance of the methods by evaluating the closeness of the proposed fixes to the original erroneous code. The evaluations have mainly relied on the use of edit distance measures such as the sequence edit distance and there is a lack of research on which distance measure is the most appropriate. Objectives: Provide insight into measures for quantifying the distance between erroneous code written by a student and a proposed change. We conduct the evaluation in an introductory programming context, where insight into the distance measures can provide help in choosing a suitable metric that can inform which fixes should be suggested to novices. Method: A team of five experts annotated a subset of the Dublin dataset, creating solutions for over a thousand erroneous programs written by students. We evaluated how the prominent edit distance measures from the CER literature compare against measures used in Natural Language Processing (NLP) tasks for retrieving the experts’ solutions from a pool of proposed solutions. We also evaluated how the expert-generated solutions compare against the solutions proposed by common program repair algorithms. The annotated dataset and the evaluation code are published as part of the work. Findings: Our results highlight that the ROUGE score, classically used for evaluating the performance of machine summarization tasks, performs well as an evaluation and selection metric for program repair. We also highlight the practical utility of NLP metrics, which allow an easier interpretation and comparison of the performance of repair techniques when compared to the classic methods used in the CER literature. Implications: Our study highlights the variety of distance metrics used for comparing source codes. We find issues with the classically used distance measures that can be combated by using NLP metrics. Based on our findings, we recommend including NLP metrics, and in particular, the ROUGE metric, in evaluations when considering new program repair methodologies. We also suggest incorporating NLP metrics into other areas where source codes are compared, including plagiarism detection.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1},
pages = {495–507},
numpages = {13},
keywords = {BLEU, ROUGE, automated program repair, automatic program repair, bug fixing, computing education, dataset, distance measures, distance metrics, educational data mining, feedback, natural language processing, program repair},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{10.1145/3474624.3474627,
author = {Barros, Daniel and Horita, Flavio and Wiese, Igor and Silva, Kanan},
title = {A Mining Software Repository Extended Cookbook: Lessons learned from a literature review},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3474627},
doi = {10.1145/3474624.3474627},
abstract = {The main purpose of Mining Software Repositories (MSR) is to discover the latest enhancements and provide an insight into how to make improvements in a software project. In light of it, this paper updates the MSR findings of the original MSR Cookbook, by first conducting a systematic mapping study to elicit and analyze the state-of-the-art, and then proposing an extended version of the Cookbook. This extended Cookbook was built on four high-level themes, which were derived from the analysis of a list of 112 selected studies. Hence, it was used to consolidate the extended Cookbook as a contribution to practice and research in the following areas by: 1) including studies published in all available and relevant publication venues; 2) including and updating recommendations in all four high-level themes, with an increase of 84% in comments in this study when compared with the original MSR Cookbook; 3) summarizing the tools employed for each high-level theme; and 4) providing lessons learned for future studies. Thus, the extended Cookbook examined in this work can support new research projects, as upgraded recommendations and the lessons learned are available with the aid of samples and tools.},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {1–10},
numpages = {10},
keywords = {Software Repositories, Mining Software Repositories, Data Mining, Artificial Intelligence},
location = {Joinville, Brazil},
series = {SBES '21}
}

@article{10.1145/3729225,
author = {Lu, Yifei and Pan, Minxue and Lu, Haochuan and Deng, Yuetang and Zhang, Tian and Wang, Linzhang and Li, Xuandong},
title = {Improving Test Efficacy for Large-Scale Android Applications by Exploiting GUI and Functional Equivalence},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3729225},
doi = {10.1145/3729225},
abstract = {Large-scale Android apps that provide complex functions are gradually becoming the mainstream in Android app markets. They tend to display many GUI widgets on a single GUI page, which, unfortunately, can cause more redundant test actions—actions with similar functions—to automatic testing approaches. The effectiveness of existing testing approaches is still limited, suggesting the necessity of reducing the test effort on redundant actions. In this paper, we first identify three types of GUI structures that can cause redundant actions and then propose a novel approach, called action equivalence evaluation, to find the actions with similar functions by exploiting both GUI structure and functionality. By integrating this approach with existing testing tools, the test efficacy can be improved.We conducted experiments on 17 large-scale Android apps, including three industrial apps Google News, Messenger, and WeChat. The results show that more instructions can be covered, and more crashes can be detected, compared to the state-of-the-art Android testing tools. 29 real bugs were found in our experiment, and moreover, 760 bugs over 40 versions of WeChat had been detected in the real test environment during a three-month testing period.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
keywords = {Android testing, GUI testing, equivalent action evaluation}
}

@article{10.1145/3708534,
author = {Sartaj, Hassan and Ali, Shaukat and Gj\o{}by, Julie Marie},
title = {MeDeT: Medical Device Digital Twins Creation with Few-shot Meta-learning},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708534},
doi = {10.1145/3708534},
abstract = {Testing healthcare Internet of Things (IoT) applications at system and integration levels necessitates integrating numerous medical devices. Challenges of incorporating medical devices are: (i) their continuous evolution, making it infeasible to include all device variants, and (ii) rigorous testing at scale requires multiple devices and their variants, which is time-intensive, costly, and impractical. Our collaborator, Oslo City’s health department, faced these challenges in developing automated test infrastructure, which our research aims to address. In this context, we propose a meta-learning-based approach (MeDeT) to generate digital twins (DTs) of medical devices and adapt DTs to evolving devices. We evaluate MeDeT in Oslo City’s context using five widely-used medical devices integrated with a real-world healthcare IoT application. Our evaluation assesses MeDeT’s ability to generate and adapt DTs across various devices and versions using different few-shot methods, the fidelity of these DTs, the scalability of operating 1000 DTs concurrently, and the associated time costs. Results show that MeDeT can generate DTs with over 96% fidelity, adapt DTs to different devices and newer versions with reduced time cost (around one minute), and operate 1000 DTs in a scalable manner while maintaining the fidelity level, thus serving in place of physical devices for testing.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {Digital Twins, Meta-Learning, Few-shot Learning, Internet of Things (IoT), Medical Devices, System Testing}
}

@article{10.1145/3705307,
author = {Lyu, Deyun and Zhang, Zhenya and Arcaini, Paolo and Zhang, Xiao-Yi and Ishikawa, Fuyuki and Zhao, Jianjun},
title = {SpectAcle: Fault Localisation of AI-Enabled CPS by Exploiting Sequences of DNN Controller Inferences},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3705307},
doi = {10.1145/3705307},
abstract = {Cyber-Physical Systems (CPSs) are increasingly adopting deep neural networks (DNNs) as controllers, giving birth to AI-enabled CPSs. Despite their advantages, many concerns arise about the safety of DNN controllers. Numerous efforts have been made to detect system executions that violate safety specifications; however, once a violation is detected, to fix the issue, it is necessary to localise the parameters of the DNN controller responsible for the wrong decisions leading to the violation. This is particularly challenging, as it requires to consider a sequence of control decisions, rather than a single one, preceding the violation. To tackle this problem, we propose SpectAcle, that can localise the faulty parameters in DNN controllers. SpectAcle considers the DNN inferences preceding the specification violation and uses forward impact to determine the DNN parameters that are more relevant to the DNN outputs. Then, it identifies which of these parameters are responsible for the specification violation, by adapting classic suspiciousness metrics. Moreover, we propose two versions of SpectAcle, that consider differently the timestamps that precede the specification violation. We experimentally evaluate the effectiveness of SpectAcle on 6067 faulty benchmarks, spanning over different application domains. The results show that SpectAcle can detect most of the faults.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
keywords = {fault localisation, neural network controllers, cyber-physical systems}
}

@inproceedings{10.1145/3575879.3575964,
author = {Katsadouros, Evangelos and Patrikakis, Charalampos},
title = {A Survey on Vulnerability Prediction using GNNs},
year = {2023},
isbn = {9781450398541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575879.3575964},
doi = {10.1145/3575879.3575964},
abstract = {The massive release of software products has led to critical incidents in the software industry due to low-quality software. Software engineers lack security knowledge which causes the development of insecure software. Traditional solutions for analysing code for vulnerabilities suffer from high false positives and negative rates. Researchers over the last decade have proposed mechanisms for analysing code for vulnerabilities using machine learning. The results are promising and could replace traditional static analysis tools or accompany them in the foreseeable future to produce more reliable results. This survey presents the work done so far in vulnerability detection using Graph Neural Networks (GNNs). Presents the GNNs architectures, the graph representations, the datasets, and the results of these studies.},
booktitle = {Proceedings of the 26th Pan-Hellenic Conference on Informatics},
pages = {38–43},
numpages = {6},
keywords = {Software Quality, Software, Privacy},
location = {Athens, Greece},
series = {PCI '22}
}

@inproceedings{10.5555/3432601.3432605,
author = {Krishnakumar, Sanjena and Abdou, Tamer},
title = {Towards interpretable and maintainable supervised learning using shapley values in arrhythmia},
year = {2020},
publisher = {IBM Corp.},
address = {USA},
abstract = {This paper investigates the application of a model-agnostic interpretability technique, Shapley Additive Explanations (SHAP), to understand and hence, enhance machine learning classification models using Shapley values in the prediction of arrhythmias1. Using the Arrhythmia dataset2, three different feature selection techniques, Information Gain (IG), Recursive Feature Elimination-Random Forest (RFE-RF), and AutoSpearman, were used to select features for machine learning models to predict the arrhythmia class. Four multi-class classification models, Na\"{\i}ve Bayes (NB), k-Nearest Neighbours (kNN), Random Forest (RF), and stacking heterogeneous ensemble (Ensemble) were built, evaluated, and compared. SHAP interpretation method was applied to find reliable explanations for the predictions of the classification models. Additionally, SHAP values were used to find `bellwether' instances to enhance the training of our models in order to improve their performances in the prediction of arrhythmia. The most stable and top-performing classification model was RF, followed by Ensemble in comparison to NB and kNN. SHAP provided robust and reliable explanations for the classification models. Furthermore, improving the training of our models with `bellwether' instances, found using SHAP values, enhanced the overall model performances in terms of accuracy, AUC, and F1 score. In conclusion, we recommend using SHAP value explanations as a robust and reliable method for local model-agnostic interpretability and to enhance machine learning models for arrhythmia prediction.},
booktitle = {Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering},
pages = {23–32},
numpages = {10},
keywords = {shapley value, multi-class classification, machine learning, local model-agnostic interpretation, healthcare, bellwether, arrhythmia, SHAP, LIME},
location = {Toronto, Ontario, Canada},
series = {CASCON '20}
}

@article{10.1145/3563330,
author = {Sakkas, Georgios and Endres, Madeline and Guo, Philip J. and Weimer, Westley and Jhala, Ranjit},
title = {Seq2Parse: neurosymbolic parse error repair},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3563330},
doi = {10.1145/3563330},
abstract = {We present Seq2Parse, a language-agnostic neurosymbolic approach to automatically repairing parse errors. Seq2Parse is based on the insight that Symbolic Error Correcting (EC) Parsers can, in principle, synthesize repairs, but, in practice, are overwhelmed by the many error-correction rules that are not relevant to the particular program that requires repair. In contrast, Neural approaches are fooled by the large space of possible sequence level edits, but can precisely pinpoint the set of EC-rules that are relevant to a particular program. We show how to combine their complementary strengths by using neural methods to train a sequence classifier that predicts the small set of relevant EC-rules for an ill-parsed program, after which, the symbolic EC-parsing algorithm can make short work of generating useful repairs. We train and evaluate our method on a dataset of 1,100,000 Python programs, and show that Seq2Parse is accurate and efficient: it can parse 94% of our tests within 2.1 seconds, while generating the exact user fix in 1 out 3 of the cases; and useful: humans perceive both Seq2Parse-generated error locations and repairs to be almost as good as human-generated ones in a statistically-significant manner.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {167},
numpages = {27},
keywords = {Machine Learning, Error-Correcting Parsers, Automated Program Repair}
}

@inproceedings{10.1145/2915970.2916004,
author = {Pfahl, Dietmar and Karus, Siim and Stavnycha, Myroslava},
title = {Improving expert prediction of issue resolution time},
year = {2016},
isbn = {9781450336918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2915970.2916004},
doi = {10.1145/2915970.2916004},
abstract = {Predicting the resolution times of issue reports in software development is important, because it helps allocate resources adequately. However, issue resolution time (IRT) prediction is difficult and prediction quality is limited. A common approach in industry is to base predictions on expert knowledge. While this manual approach requires the availability and effort of experts, automated approaches using data mining and machine learning techniques require a small upfront investment for setting up the data collection and analysis infrastructure as well as the availability of sufficient past data for model building. Several approaches for automated IRT prediction have been proposed and evaluated. The aim of our study was (1) to compare the prediction quality of expert-based IRT prediction in a software company located in Estonia with that of various fully automated IRT prediction approaches proposed and used by other researchers, including k-means clustering, k-nearest neighbor classification, Na\"{\i}ve Bayes classification, decision trees, random forest (RF) and ordered logistic regression (OLR), and (2) to improve the current IRT prediction quality in the company at hand. For our study, we analyzed issue reports collected by the company in the period from April 2011 to January 2015. Regarding our first goal, we found that experts in the case company were able to predict IRTs approximately 50% of the time within the range of ±10% of the actual IRTs. In addition, 67% of the experts' predictions have an absolute error that is less or equal 0.5 hours. When applying the automated approaches used by other researchers to the company's data, we observed lower predictive quality as compared to IRT predictions made by the company's experts, even for the best-performing approaches RF and OLR. Regarding our second goal, after unsuccessfully experimenting with improvements to the RF and OLR based approaches, we managed to develop models based on text analysis that achieved a prediction quality at par or better than that achieved by company experts.},
booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {42},
numpages = {6},
keywords = {resolution time, random forest, ordered logistic regression, na\"{\i}ve bayes classifier, machine learning, latent semantic analysis, k-nearest neighbors, k-means, issue report, expert prediction},
location = {Limerick, Ireland},
series = {EASE '16}
}

@article{10.1145/3569935,
author = {Fahmy, Hazem and Pastore, Fabrizio and Briand, Lionel and Stifter, Thomas},
title = {Simulator-based Explanation and Debugging of Hazard-triggering Events in DNN-based Safety-critical Systems},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3569935},
doi = {10.1145/3569935},
abstract = {When Deep Neural Networks (DNNs) are used in safety-critical systems, engineers should determine the safety risks associated with failures (i.e., erroneous outputs) observed during testing. For DNNs processing images, engineers visually inspect all failure-inducing images to determine common characteristics among them. Such characteristics correspond to hazard-triggering events (e.g., low illumination) that are essential inputs for safety analysis. Though informative, such activity is expensive and error prone.To support such safety analysis practices, we propose Simulator-based Explanations for DNN failurEs (SEDE), a technique that generates readable descriptions for commonalities in failure-inducing, real-world images and improves the DNN through effective retraining. SEDE leverages the availability of simulators, which are commonly used for cyber-physical systems. It relies on genetic algorithms to drive simulators toward the generation of images that are similar to failure-inducing, real-world images in the test set; it then employs rule learning algorithms to derive expressions that capture commonalities in terms of simulator parameter values. The derived expressions are then used to generate additional images to retrain and improve the DNN.With DNNs performing in-car sensing tasks, SEDE successfully characterized hazard-triggering events leading to a DNN accuracy drop. Also, SEDE enabled retraining leading to significant improvements in DNN accuracy, up to 18 percentage points.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {104},
numpages = {47},
keywords = {explainable AI, heatmaps, DNN debugging, DNN functional safety analysis, DNN explanation}
}

@inproceedings{10.1145/1985441.1985443,
author = {Whitehead, Jim},
title = {Fantasy, farms, and freemium: what game data mining teaches us about retention, conversion, and virality (keynote abstract)},
year = {2011},
isbn = {9781450305747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985441.1985443},
doi = {10.1145/1985441.1985443},
abstract = {In December of 2010, the new game CityVille achieved 6 million daily active users in just 8 days. Clearly the success of CityVille owes something to the fun gameplay experience it provides. That said, it was far from the best game released in 2010. Why did it grow so fast? In this talk the key factors behind the dramatic success of social network games are explained. Social network games build word-of-mouth player acquisition directly into the gameplay experience via friend invitations and game mechanics that require contributions by friends to succeed. Software analytics (mined data about player sessions) yield detailed models of factors that affect player retention and engagement. Player engagement is directly related to conversion, shifting a free player into a paying player, the critical move in a freemium business model. Analytics also permit tracking of player virality, the degree to which one player invites other players into the game. Social network games offer multiple lessons for software engineers in general, and software mining researchers in particular. Since software is in competition for people's attention along with a wide range of other media and software, it is important to design software for high engagement and retention. Retention engineering requires constant attention to mined user experience data, and this data is easiest to acquire with web-based software. Building user acquisition directly into software provides powerful benefits, especially when it is integrated deeply into the experience delivered by the software. Since retention engineering and viral user acquisition are much easier with web-based software, the trend of software applications migrating to the web will accelerate.},
booktitle = {Proceedings of the 8th Working Conference on Mining Software Repositories},
pages = {1},
numpages = {1},
keywords = {post-mortem analysis, languages, java, generics},
location = {Waikiki, Honolulu, HI, USA},
series = {MSR '11}
}

@article{10.1145/3688840,
author = {Shi, Jinjing and Xiao, Zimeng and Shi, Heyuan and Jiang, Yu and Li, Xuelong},
title = {QuanTest: Entanglement-Guided Testing of Quantum Neural Network Systems},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3688840},
doi = {10.1145/3688840},
abstract = {Quantum Neural Network (QNN) combines the deep learning (DL) principle with the fundamental theory of quantum mechanics to achieve machine learning tasks with quantum acceleration. Recently, QNN systems have been found to manifest robustness issues similar to classical DL systems. There is an urgent need for ways to test their correctness and security. However, QNN systems differ significantly from traditional quantum software and classical DL systems, posing critical challenges for QNN testing. These challenges include the inapplicability of traditional quantum software testing methods to QNN systems due to differences in programming paradigms and decision logic representations, the dependence of quantum test sample generation on perturbation operators, and the absence of effective information in quantum neurons. In this article, we propose QuanTest, a quantum entanglement-guided adversarial testing framework to uncover potential erroneous behaviors in QNN systems. We design a quantum entanglement adequacy criterion to quantify the entanglement acquired by the input quantum states from the QNN system, along with two similarity metrics to measure the proximity of generated quantum adversarial examples to the original inputs. Subsequently, QuanTest formulates the problem of generating test inputs that maximize the quantum entanglement adequacy and capture incorrect behaviors of the QNN system as a joint optimization problem and solves it in a gradient-based manner to generate quantum adversarial examples. Experimental results demonstrate that QuanTest possesses the capability to capture erroneous behaviors in QNN systems (generating 67.48–96.05% more high-quality test samples than the random noise under the same perturbation size constraints). The entanglement-guided approach proves effective in adversarial testing, generating more adversarial examples (maximum increase reached 21.32%).},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {48},
numpages = {32},
keywords = {Quantum neural network, deep neural network, adversarial testing, quantum entanglement}
}

@inproceedings{10.1145/2889160.2889260,
author = {Le, Duc and Medvidovic, Nenad},
title = {Architectural-based speculative analysis to predict bugs in a software system},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889260},
doi = {10.1145/2889160.2889260},
abstract = {Over time, a software system's code and its underlying design tend to decay steadily and, in turn, to complicate the system's maintenance. In order to address that phenomenon, many researchers tried to help engineers predict parts of a system that are most likely to create problems while or even before they are modifying the system. Problems that creep into a system may manifest themselves as bugs, in which case engineers have no choice but to fix them or develop workarounds. However, these problems may also be more subtle, such as code clones, circular dependencies among system elements, very large APIs, individual elements that implement multiple diffuse concerns, etc. Even though such architectural and code "smells" may not crash a system outright, they impose real costs in terms of engineers' time and effort, as well as system correctness and performance. Along the time, implicit problems may be revealed as explicit problems. However, most current techniques predict explicit problems of a system only based on explicit problems themselves. Our research takes a further step by using implicit problems, e.g., architectural- and code-smells, in combination with explicit problems to provide an accurate, systematic and in depth approach to predict potential system problems, particularly bugs.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {807–810},
numpages = {4},
keywords = {speculative analysis, bug prediction, architectural-based analysis, architectural decay},
location = {Austin, Texas},
series = {ICSE '16}
}

@article{10.1145/3488280,
author = {Sowah, Robert A. and Kuditchar, Bernard and Mills, Godfrey A. and Acakpovi, Amevi and Twum, Raphael A. and Buah, Gifty and Agboyi, Robert},
title = {HCBST: An Efficient Hybrid Sampling Technique for Class Imbalance Problems},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3488280},
doi = {10.1145/3488280},
abstract = {Class imbalance problem is prevalent in many real-world domains. It has become an active area of research. In binary classification problems, imbalance learning refers to learning from a dataset with a high degree of skewness to the negative class. This phenomenon causes classification algorithms to perform woefully when predicting positive classes with new examples. Data resampling, which involves manipulating the training data before applying standard classification techniques, is among the most commonly used techniques to deal with the class imbalance problem. This article presents a new hybrid sampling technique that improves the overall performance of classification algorithms for solving the class imbalance problem significantly. The proposed method called the Hybrid Cluster-Based Undersampling Technique (HCBST) uses a combination of the cluster undersampling technique to under-sample the majority instances and an oversampling technique derived from Sigma Nearest Oversampling based on Convex Combination, to oversample the minority instances to solve the class imbalance problem with a high degree of accuracy and reliability. The performance of the proposed algorithm was tested using 11 datasets from the National Aeronautics and Space Administration Metric Data Program data repository and University of California Irvine Machine Learning data repository with varying degrees of imbalance. Results were compared with classification algorithms such as the K-nearest neighbours, support vector machines, decision tree, random forest, neural network, AdaBoost, na\"{\i}ve Bayes, and quadratic discriminant analysis. Tests results revealed that for the same datasets, the HCBST performed better with average performances of 0.73, 0.67, and 0.35 in terms of performance measures of area under curve, geometric mean, and Matthews Correlation Coefficient, respectively, across all the classifiers used for this study. The HCBST has the potential of improving the performance of the class imbalance problem, which by extension, will improve on the various applications that rely on the concept for a solution.},
journal = {ACM Trans. Knowl. Discov. Data},
month = nov,
articleno = {57},
numpages = {37},
keywords = {classification, clustering, cluster undersampling technique, data sampling, Class imbalance}
}

@article{10.1145/3712189,
author = {Zhao, Yifan and Sun, Zeyu and Wang, Guoqing and Liang, Qingyuan and Zhang, Yakun and Lou, Yiling and Hao, Dan and Zhang, Lu},
title = {Automatically Learning a Precise Measurement for Fault Diagnosis Capability of Test Cases},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3712189},
doi = {10.1145/3712189},
abstract = {Prevalent Fault Localization (FL) techniques rely on tests to localize buggy program elements. Tests could be treated as fuel to further boost FL by providing more debugging information. Therefore, it is highly valuable to measure the Fault Diagnosis Capability (FDC) of a test for diagnosing faults, so as to select or generate tests to better help FL (i.e., FL-oriented test selection or FL-oriented test generation). To this end, researchers have proposed many FDC metrics, which serve as the selection criterion in FL-oriented test selection or the fitness function in FL-oriented test generation. Existing FDC metrics can be classified into result-agnostic and result-aware metrics depending on whether they take test results (i.e., passing or failing) as input. Although result-aware metrics perform better in test selection, they have restricted applications due to the input of test results, e.g., they cannot be applied to guide test generation. Moreover, all the existing FDC metrics are designed based on some predefined heuristics and have achieved limited FL performance due to their inaccuracy. To address these issues, in this paper, we reconsider result-agnostic metrics (i.e., metrics that do not take test results as input), and propose a novel result-agnostic metric RLFDC which predicts FDC values of tests through reinforcement learning. In particular, we treat FL results as reward signals, and train an FDC prediction model with the direct FL feedback to automatically learn a more accurate measurement rather than design one based on predefined heuristics. Finally, we evaluate the proposed RLFDC on Defects4J by applying the studied metrics to test selection and generation. According to the experimental results, the proposed RLFDC outperforms all the result-agnostic metrics in both test selection and generation, e.g., when applied to selecting human-written tests, RLFDC achieves 28.2% and 21.6% higher acc@1 and mAP values compared to the state-of-the-art result-agnostic metric TfD. Besides, RLFDC even achieves competitive performance compared to the state-of-the-art result-aware metric FDG in test selection.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Fault localization, Fault diagnosability, Reinforcement learning}
}

@inproceedings{10.1145/3510003.3510187,
author = {Kaufman, Samuel J. and Featherman, Ryan and Alvin, Justin and Kurtz, Bob and Ammann, Paul and Just, Ren\'{e}},
title = {Prioritizing mutants to guide mutation testing},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510187},
doi = {10.1145/3510003.3510187},
abstract = {Mutation testing offers concrete test goals (mutants) and a rigorous test efficacy criterion, but it is expensive due to vast numbers of mutants, many of which are neither useful nor actionable. Prior work has focused on selecting representative and sufficient mutant subsets, measuring whether a test set that is mutation-adequate for the subset is equally adequate for the entire set. However, no known industrial application of mutation testing uses or even computes mutation adequacy, instead focusing on iteratively presenting very few mutants as concrete test goals for developers to write tests.This paper (1) articulates important differences between mutation analysis, where measuring mutation adequacy is of interest, and mutation testing, where mutants are of interest insofar as they serve as concrete test goals to elict effective tests; (2) introduces a new measure of mutant usefulness, called test completeness advancement probability (TCAP); (3) introduces an approach to prioritizing mutants by incrementally selecting mutants based on their predicted TCAP; and (4) presents simulations showing that TCAP-based prioritization of mutants advances test completeness more rapidly than prioritization with the previous state-of-the-art.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1743–1754},
numpages = {12},
keywords = {TCAP, machine learning, mutant selection, mutant utility, mutation testing, test completeness advancement probability},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3691620.3695015,
author = {Lahiri, Sumit and Kalita, Pankaj Kumar and Chittora, Akshay Kumar and Vankudre, Varun and Roy, Subhajit},
title = {Program Synthesis Meets Visual What-Comes-Next Puzzles},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695015},
doi = {10.1145/3691620.3695015},
abstract = {What-Comes-Next (WCN) puzzles challenge us to identify the next figure that "logically follows" a provided sequence of figures. WCN puzzles are a favorite of interviewers and examiners---there is hardly any aptitude test that misses WCN puzzles. In this work, we propose to automatically synthesize WCN puzzles. The key insight to our methodology is that generation of WCN problems can be posed as a program synthesis problem. We design a small yet expressive language, PuzzlerLang, to capture solutions to WCN puzzles. PuzzlerLang is expressive enough to explain almost all human generated WCN puzzles that we collected, and yet, small enough to allow synthesis in a reasonable time. To ensure that the generated puzzles are appealing to humans, we infer a machine learning model to approximate the appeal factor of given WCN puzzle to humans. We use this model within our puzzle synthesizer as an optimization function to generate highly appealing and correct-by-construction WCN puzzles. We implemented our ideas in a tool, PuzzleGen; we found that PuzzleGen is fast, clocking an average time of about 3.4s per puzzle. Further, statistical tests over the responses from a user-study supported that the PuzzleGen generated puzzles were indistinguishable from puzzles created by humans.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {418–429},
numpages = {12},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/2695664.2695959,
author = {Xuan, Xiao and Lo, David and Xia, Xin and Tian, Yuan},
title = {Evaluating defect prediction approaches using a massive set of metrics: an empirical study},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695959},
doi = {10.1145/2695664.2695959},
abstract = {To evaluate the performance of a within-project defect prediction approach, people normally use precision, recall, and F-measure scores. However, in machine learning literature, there are a large number of evaluation metrics to evaluate the performance of an algorithm, (e.g., Matthews Correlation Coefficient, G-means, etc.), and these metrics evaluate an approach from different aspects. In this paper, we investigate the performance of within-project defect prediction approaches on a large number of evaluation metrics. We choose 6 state-of-the-art approaches including naive Bayes, decision tree, logistic regression, kNN, random forest and Bayesian network which are widely used in defect prediction literature. And we evaluate these 6 approaches on 14 evaluation metrics (e.g., G-mean, F-measure, balance, MCC, J-coefficient, and AUC). Our goal is to explore a practical and sophisticated way for evaluating the prediction approaches comprehensively. We evaluate the performance of defect prediction approaches on 10 defect datasets from PROMISE repository. The results show that Bayesian network achieves a noteworthy performance. It achieves the best recall, FN-R, G-mean1 and balance on 9 out of the 10 datasets, and F-measure and J-coefficient on 7 out of the 10 datasets.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1644–1647},
numpages = {4},
keywords = {machine learning, evaluation metric, defect prediction},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/3650212.3680390,
author = {Ghanbari, Ali},
title = {Decomposition of Deep Neural Networks into Modules via Mutation Analysis},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680390},
doi = {10.1145/3650212.3680390},
abstract = {Recently, several approaches have been proposed for decomposing deep neural network (DNN) classifiers into binary classifier modules to facilitate modular development and repair of such models. These approaches concern only the problem of decomposing classifier models, and some of them rely on the activation patterns of the neurons, thereby limiting their applicability.
 

 
In this paper, we propose a DNN decomposition technique, named Incite, that uses neuron mutation to quantify the contributions of the neurons to a given output of a model. Then, for each model output, a subgraph induced by the nodes with highest contribution scores for that output are selected and extracted as a module. Incite is agnostic to the type of the model and the activation functions used in its construction, and is applicable to not just classifiers, but to regression models as well. Furthermore, the costs of mutation analysis in Incite has been reduced by heuristic clustering of neurons, enabling its application to models with millions of parameters. Lastly, Incite prunes away the neurons that do not contribute to the outcome of the modules, producing compressed, efficient modules.
 

 
We have evaluated Incite using 16 DNN models for well-known classification and regression problems and report its effectiveness along combined accuracy (and MAE) of the modules, the overlap in model elements between the modules, and the compression ratio. We observed that, for classification models, Incite, on average, incurs 3.44% loss in accuracy, and the average overlap between the modules is 71.76%, while the average compression ratio is 1.89X. Meanwhile, for regression models, Incite, on average, incurs 18.56% gain in MAE, and the overlap between modules is 80.14%, while the average compression ratio is 1.83X.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1669–1681},
numpages = {13},
keywords = {Decomposition, Deep Neural Network, Module, Mutation Analysis},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3324884.3416532,
author = {Tian, Haoye and Liu, Kui and Kabor\'{e}, Abdoul Kader and Koyuncu, Anil and Li, Li and Klein, Jacques and Bissyand\'{e}, Tegawend\'{e} F.},
title = {Evaluating representation learning of code changes for predicting patch correctness in program repair},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416532},
doi = {10.1145/3324884.3416532},
abstract = {A large body of the literature of automated program repair develops approaches where patches are generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. While the state of the art explore research directions that require dynamic information or that rely on manually-crafted heuristics, we study the benefit of learning code representations in order to learn deep features that may encode the properties of patch correctness. Our empirical work mainly investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations. We report on findings based on embeddings produced by pre-trained and re-trained neural networks. Experimental results demonstrate the potential of embeddings to empower learning algorithms in reasoning about patch correctness: a machine learning predictor with BERT transformer-based embeddings associated with logistic regression yielded an AUC value of about 0.8 in the prediction of patch correctness on a deduplicated dataset of 1000 labeled patches. Our investigations show that learned representations can lead to reasonable performance when comparing against the state-of-the-art, PATCH-SIM, which relies on dynamic information. These representations may further be complementary to features that were carefully (manually) engineered in the literature.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {981–992},
numpages = {12},
keywords = {distributed representation learning, embeddings, machine learning, patch correctness, program repair},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3533767.3534219,
author = {Yuan, Wei and Zhang, Quanjun and He, Tieke and Fang, Chunrong and Hung, Nguyen Quoc Viet and Hao, Xiaodong and Yin, Hongzhi},
title = {CIRCLE: continual repair across programming languages},
year = {2022},
isbn = {9781450393799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533767.3534219},
doi = {10.1145/3533767.3534219},
abstract = {Automatic Program Repair (APR) aims at fixing buggy source code with less manual debugging efforts, which plays a vital role in improving software reliability and development productivity. Recent APR works have achieved remarkable progress via applying deep learning (DL), particularly neural machine translation (NMT) techniques. However, we observe that existing DL-based APR models suffer from at least two severe drawbacks: (1) Most of them can only generate patches for a single programming language, as a result, to repair multiple languages, we have to build and train many repairing models. (2) Most of them are developed offline. Therefore, they won’t function when there are new-coming requirements. To address the above problems, a T5-based APR framework equipped with continual learning ability across multiple programming languages is proposed, namely ContInual Repair aCross Programming LanguagEs (CIRCLE). Specifically, (1) CIRCLE utilizes a prompting function to narrow the gap between natural language processing (NLP) pre-trained tasks and APR. (2) CIRCLE adopts a difficulty-based rehearsal strategy to achieve lifelong learning for APR without access to the full historical data. (3) An elastic regularization method is employed to strengthen CIRCLE’s continual learning ability further, preventing it from catastrophic forgetting. (4) CIRCLE applies a simple but effective re-repairing method to revise generated errors caused by crossing multiple programming languages. We train CIRCLE for four languages (i.e., C, JAVA, JavaScript, and Python) and evaluate it on five commonly used benchmarks. The experimental results demonstrate that CIRCLE not only effectively and efficiently repairs multiple programming languages in continual learning settings, but also achieves state-of-the-art performance (e.g., fixes 64 Defects4J bugs) with a single repair model.},
booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {678–690},
numpages = {13},
keywords = {Neural Machine Translation, Lifelong Learning, Automatic Program Repair, AI and Software Engineering},
location = {Virtual, South Korea},
series = {ISSTA 2022}
}

@article{10.1145/3722105,
author = {Yu, Shengcheng and Fang, Chunrong and Liu, Jia and Chen, Zhenyu},
title = {Test Script Intention Generation for Mobile Application via GUI Image and Code Understanding},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3722105},
doi = {10.1145/3722105},
abstract = {Testing is the most direct and effective technique to ensure software quality. Test scripts always play a more important role in mobile app testing than test cases for source code, due to the GUI-intensive and event-driven characteristics of mobile applications (app). Test scripts focus on user interactions and the corresponding response events, which is significant for testing the target app functionalities. Therefore, it is critical to understand the test scripts for better script maintenance and modification. There exist some mature code understanding (i.e., code comment generation, code summarization) technologies that can be directly applied to functionality source code with business logic. However, such technologies will have difficulties when being applied to test scripts, because test scripts are loosely linked to apps under test (AUT) by widget selectors, and do not contain business logic themselves.In order to solve the test script understanding gap, this paper presents a novel approach, namely TestIntention, to infer the intention of GUI test scripts. Test intention refers to the user expectations of app behaviors for specific operations. TestIntention formalizes test scripts with an operation sequence model. For each operation within the sequence, TestIntention extracts the target widget selector and links the selector to the GUI layout information or the corresponding response events. For widgets identified by XPath, TestIntention utilizes the image understanding technologies to explore the detailed information of the widget images, the intention of which is understood with a deep learning model. For widgets identified by ID, TestIntention first maps the selectors to the response methods with business logic, and then adopts code understanding technologies to describe code in natural language form. Results of all operations are combined to generate test intention for test scripts. An empirical experiment including different metrics proves the outstanding performance of TestIntention, outperforming baselines by much. Also, it is shown that TestIntention can save about 80% developers’ time to understand test scripts.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
keywords = {Mobile App Testing, GUI Understanding, Code Understanding}
}

@inproceedings{10.1145/3643788.3648021,
author = {Lajko, Mark and Csuvik, Viktor and Gyimothy, Tibor and Vidacs, Laszlo},
title = {Automated Program Repair with the GPT Family, including GPT-2, GPT-3 and CodeX},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648021},
doi = {10.1145/3643788.3648021},
abstract = {Automated Program Repair (APR) is a promising approach for addressing software defects and improving software reliability. There are various approaches to APR, including using Machine Learning (ML) techniques such as neural networks and evolutionary algorithms, as well as more traditional methods such as static analysis and symbolic execution. In recent years, there has been growing interest in using ML techniques for APR, including the use of large language models such as GPT-2 and GPT-3. These models have the ability to generate human-like text and code, making them well-suited for tasks such as generating repair patches for defective programs. In this paper, we explore the use of the GPT family (including GPT-2, GPT-J-6B, GPT-3 and Codex) for APR of JavaScript programs and evaluate their performance in terms of the number and quality of repair patches generated. Our results show that these state-of-the-art language models are able to generate repair patches that successfully fix the defects in the JavaScript programs, with Codex performing slightly better overall. To be precise, in our self-assembled dataset, Codex was able to generate 108 repair patches that are exactly the same as the developer fix for the first try. If we consider multiple patch generations, up to 201 buggy programs are being repaired automatically from the 1559 evaluation dataset (12.89%).},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {34–41},
numpages = {8},
keywords = {automated program repair, transformers, GPT-3, codex, JavaScript},
location = {Lisbon, Portugal},
series = {APR '24}
}

@article{10.1145/3625293,
author = {Ismayilzada, Elkhan and Rahman, Md Mazba Ur and Kim, Dongsun and Yi, Jooyong},
title = {Poracle: Testing Patches under Preservation Conditions to Combat the Overfitting Problem of Program Repair},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3625293},
doi = {10.1145/3625293},
abstract = {To date, the users of test-driven program repair tools suffer from the overfitting problem; a generated patch may pass all available tests without being correct. In the existing work, users are treated as merely passive consumers of the tests. However, what if they are willing to modify the test to better assess the patches obtained from a repair tool? In this work, we propose a novel semi-automatic patch-classification methodology named Poracle. Our key contributions are three-fold. First, we design a novel lightweight specification method that reuses the existing test. Specifically, the users extend the existing failing test with a preservation condition—the condition under which the patched and pre-patched versions should produce the same output. Second, we develop a fuzzer that performs differential fuzzing with a test containing a preservation condition. Once we find an input that satisfies a specified preservation condition but produces different outputs between the patched and pre-patched versions, we classify the patch as incorrect with high confidence. We show that our approach is more effective than the four state-of-the-art patch classification approaches. Last, we show through a user study that the users find our semi-automatic patch assessment method more effective and preferable than the manual assessment.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {44},
numpages = {39},
keywords = {preservation condition, patch classification, patch validation, overfitting problem, Automated program repair}
}

@inproceedings{10.1109/ASE56229.2023.00194,
author = {Missaoui, Sondess and Gerasimou, Simos and Matragkas, Nicholas},
title = {Semantic Data Augmentation for Deep Learning Testing Using Generative AI},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00194},
doi = {10.1109/ASE56229.2023.00194},
abstract = {The performance of state-of-the-art Deep Learning models heavily depends on the availability of well-curated training and testing datasets that sufficiently capture the operational domain. Data augmentation is an effective technique in alleviating data scarcity, reducing the time-consuming and expensive data collection and labelling processes. Despite their potential, existing data augmentation techniques primarily focus on simple geometric and colour space transformations, like noise, flipping and resizing, producing datasets with limited diversity. When the augmented dataset is used for testing the Deep Learning models, the derived results are typically uninformative about the robustness of the models. We address this gap by introducing GenFuzzer, a novel coverage-guided data augmentation fuzzing technique for Deep Learning models underpinned by generative AI. We demonstrate our approach using widely-adopted datasets and models employed for image classification, illustrating its effectiveness in generating informative datasets leading up to a 26% increase in widely-used coverage criteria.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1694–1698},
numpages = {5},
keywords = {generative AI, deep learning testing, coverage guided fuzzing, data augmentation, safe AI},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@article{10.1145/3690634,
author = {Sun, Xinyu and Liu, Wanwei and Wang, Shangwen and Chen, Tingyu and Tao, Ye and Mao, Xiaoguang},
title = {AutoRIC: Automated Neural Network Repairing Based on Constrained Optimization},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3690634},
doi = {10.1145/3690634},
abstract = {Neural networks are important computational models used in the domains of artificial intelligence and software engineering. Parameters of a neural network are obtained via training it against a specific dataset with a standard process, which guarantees each sample within that set is mapped to the correct class. In general, for a trained neural network, there is no warranty of high-level properties, such as fairness, robustness, and so forth. In this case, one need to tune the parameters in an alternative manner, and it is called repairing. In this paper, we present AutoRIC (Automated Repair wIth Constraints), an analytical-approach-based white-box repairing framework against general properties that could be quantitatively measured. Our approach is mainly based on constrained optimization, namely, we treat the properties of neural network as the optimized objective described by a quadratic formula about the faulty parameters. To ensure the classification accuracy of the repaired neural network, we impose linear inequality constraints to the inputs that obtain incorrect outputs from the neural network. In general, this may generate a huge amount of constraints, resulting in the prohibitively high cost in the problem solving, or even making the problem unable to be solved by the constraint solver. To circumvent this, we present a selection strategy to diminish the restrictions, i.e., we always select the most ‘strict’ ones into the constraint set each time. Experimental results show that repairing with constraints performs efficiently and effectively. AutoRIC tends to achieve a satisfactory repairing result whereas brings in a negligible accuracy drop. AutoRIC enjoys a notable time advantage and this advantage becomes increasingly evident as the network complexity rises. Moreover, experiment results also demonstrate that repairing based on unconstrained optimizations are not stable, which embodies the necessity of constraints.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {50},
numpages = {29},
keywords = {Deep Neural Networks, Repair, Constrained Optimization}
}

@article{10.1145/3719345,
author = {Kong, Jiaolong and Xie, Xiaofei and Cheng, Mingfei and Liu, Shangqing and Du, Xiaoning and Guo, Qi},
title = {ContrastRepair: Enhancing Conversation-Based Automated Program Repair via Contrastive Test Case Pairs},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3719345},
doi = {10.1145/3719345},
abstract = {Automated Program Repair (APR) aims to automatically generate patches for rectifying software bugs. Recent strides in Large Language Models (LLM), such as ChatGPT, have yielded encouraging outcomes in APR, especially within the conversation-driven APR framework. Nevertheless, the efficacy of conversation-driven APR is contingent on the quality of the feedback information. In this paper, we propose ContrastRepair, a novel conversation-based APR approach that augments conversation-driven APR by providing LLMs with contrastive test pairs. A test pair consists of a failing test and a passing test, which offer contrastive feedback to the LLM. Our key insight is to minimize the difference between the generated passing test and the given failing test, which can better isolate the root causes of bugs. By providing such informative feedback, ContrastRepair enables the LLM to produce effective bug fixes. The implementation of ContrastRepair is based on the state-of-the-art LLM, ChatGPT, and it iteratively interacts with ChatGPT until plausible patches are generated. We evaluate ContrastRepair on multiple benchmark datasets, including Defects4J, QuixBugs, and HumanEval-Java. The results demonstrate that ContrastRepair significantly outperforms existing methods, achieving a new state-of-the-art in program repair. For instance, among Defects4J 1.2 and 2.0, ContrastRepair correctly repairs 143 out of all 337 bug cases, while the best-performing baseline fixes 124 bugs.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
keywords = {Program Repair, Large Language Model}
}

@article{10.1145/3709356,
author = {Fan, Fu and Jiang, Yanjie and Chen, Tianyi and Zhang, Hengshun and Zhang, Yuxia and Niu, Nan and Liu, Hui},
title = {An Empirical Study on Common Sense-Violating Bugs in Mobile Apps},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3709356},
doi = {10.1145/3709356},
abstract = {Mobile applications are widely used by billions of users in their daily work and life. Such GUI software is prone to bugs, potentially degrading user experience. Notably, many bugs in mobile apps are reported by end-users who cannot access the requirements of the app or test cases accompanied by explicitly specified test oracles. It may suggest that such bugs are not identified in the traditional way, i.e., by comparing the actual behaviors of the apps against their expected behaviors explicitly specified in the requirements or test cases. Instead, such bugs are often identified by comparing the actual behaviors against users’ common knowledge of apps, noted as common sense. We refer to such bugs as common sense-violating bugs. Although it is well-known that common sense-violating bugs are common in mobile apps, it remains unclear how popular they are and what kind of common sense principles are violated by them, let alone the relationship among the violated common sense principles. To this end, in this paper, we conduct the first large-scale empirical study on common sense-violating bugs in open-source mobile apps. We manually analyzed 2,808 real-world bug reports across 948 open-sourced mobile apps on GitHub. Our analysis results suggest that 1,006 (35.8%) out of the 2,808 bugs pertain to common sense-violating bugs. From those common sense-violating bugs, we identified a set of common sense principles violated by the buggy behaviors, and built a taxonomy for the common sense principles. Such principles fall into three categories: UI content-related common sense principles, UI layout-related common sense principles, and interaction-related common sense principles. By analyzing the frequency of the common sense principles being violated, we observed that a small set of common sense principles were frequently violated by the majority of common sense-violating bugs: 18 common sense principles, accounting for only 5% of the violated common sense principles, were violated by more than half of the common sense-violating bugs. These findings suggest that identifying the most frequent common sense-violating bugs could be achieved by using a small set of critical common sense principles, which may significantly reduce the cost of common sense-based bug detection. Finally, to demonstrate the feasibility of automated bug detection with common sense-based test oracles, we propose an automated approach to validating whether a given test run violates the most frequently violated common sense principle: No raw error message. Our evaluation results suggest that the automated approach is accurate, whose precision and recall are 91.3% and 91.6%, respectively.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {UI testing, Mobile applications, Android applications, Empirical study}
}

@inproceedings{10.1145/3148453.3306274,
author = {Ali, Kamran and Xia, Xiaoling},
title = {Revisiting and Rethinking on the Technical Aspects of Software Requirements},
year = {2018},
isbn = {9781450363525},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148453.3306274},
doi = {10.1145/3148453.3306274},
abstract = {Software Requirement specification document is the building block in software development. SRS plays a significantly important role for developing any type of Software. This is the first deliverable to the client. This paper presents the key features and technical aspects of SRS document which help in resolving myriad problems which occur during designing and developing of Software Application. In this study, we will cover major areas of SRS document which will be of crucial help to whole project team for programming any sort of application like web application, mobile application and desktop application.},
booktitle = {Proceedings of the International Conference on Information Technology and Electrical Engineering 2018},
articleno = {35},
numpages = {4},
keywords = {Software requirement specification, Software fault, Software Engineering, Human errors},
location = {Xiamen, Fujian, China},
series = {ICITEE '18}
}

@inproceedings{10.1145/3551349.3556929,
author = {Liu, Zixi and Feng, Yang and Yin, Yining and Sun, Jingyu and Chen, Zhenyu and Xu, Baowen},
title = {QATest: A Uniform Fuzzing Framework for Question Answering Systems},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556929},
doi = {10.1145/3551349.3556929},
abstract = {The tremendous advancements in deep learning techniques have empowered question answering(QA) systems with the capability of dealing with various tasks. Many commercial QA systems, such as Siri, Google Home, and Alexa, have been deployed to assist people in different daily activities. However, modern QA systems are often designed to deal with different topics and task formats, which makes both the test collection and labeling tasks difficult and thus threats their quality. To alleviate this challenge, in this paper, we design and implement a fuzzing framework for QA systems, namely QATest, based on the metamorphic testing theory. It provides the first uniform solution to generate tests with oracle information automatically for various QA systems, such as machine reading comprehension, open-domain QA, and QA on knowledge bases. To further improve testing efficiency and generate more tests detecting erroneous behaviors, we design N-Gram coverage and perplexity priority based on the features of the question data to guide the generation process. To evaluate the performance of QATest, we experiment with it on four QA systems that are designed for different tasks. The experiment results show that the tests generated by QATest detect hundreds of erroneous behaviors of QA systems efficiently. Also, the results confirm that the testing criteria can improve test diversity and fuzzing efficiency.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {81},
numpages = {12},
keywords = {question answering systems, natural language processing, fuzz testing, automated testing},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@article{10.1145/3721978,
author = {Li, Jiapeng and Zheng, Zheng and Du, Xiaoting and Wang, Haoyu and Liu, Yanwen},
title = {DRLMutation: A Comprehensive Framework for Mutation Testing in Deep Reinforcement Learning Systems},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3721978},
doi = {10.1145/3721978},
abstract = {Deep reinforcement learning (DRL) systems have been increasingly applied in various domains. Testing them, however, remains a major open research problem. Mutation testing is a popular test suite evaluation technique that analyzes the extent to which test suites detect injected faults. It has been widely researched in both traditional software and the field of deep learning. However, due to the fundamental differences between DRL systems and traditional software, as well as deep learning systems, in aspects such as environment interaction, network decision-making, and data efficiency, previous mutation testing techniques cannot be directly applied to DRL systems. In this paper, we proposed a comprehensive mutation testing framework specifically designed for DRL systems, DRLMutation, to further fill this gap. We first considered the characteristics of DRL, and based on both the training process and the model of trained agent, examined combinations from three dimensions: objects, operation methods, and injection methods. This approach led to a more comprehensive design methodology for DRL mutation operators. After filtering, we identified a total of 107 applicable DRL mutation operators. Then, in the realm of evaluation, we formulated a set of metrics tailored to assess test suites. Finally, we validated the stealthiness and effectiveness of the proposed mutation operators in the Cart Pole, Mountain Car Continuous, Lunar Lander, Breakout and CARLA environments. We show inspiring findings that the majority of these designed DRL mutation operators potentially undermine the decision-making capabilities of the agent without affecting normal training. The varying degrees of disruption achieved by these mutation operators can be used to assess the quality of different test suites.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
keywords = {Mutation Testing, Mutation Operators, Software Testing, Deep Reinforcement Learning}
}

@article{10.1145/3637230,
author = {Biringa, Chidera and Kul, G\"{o}khan},
title = {PACE: A Program Analysis Framework for Continuous Performance Prediction},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3637230},
doi = {10.1145/3637230},
abstract = {Software development teams establish elaborate continuous integration pipelines containing automated test cases to accelerate the development process of software. Automated tests help to verify the correctness of code modifications decreasing the response time to changing requirements. However, when the software teams do not track the performance impact of pending modifications, they may need to spend considerable time refactoring existing code. This article presents PACE, a program analysis framework that provides continuous feedback on the performance impact of pending code updates. We design performance microbenchmarks by mapping the execution time of functional test cases given a code update. We map microbenchmarks to code stylometry features and feed them to predictors for performance predictions. Our experiments achieved significant performance in predicting code performance, outperforming current state-of-the-art by 75% on neural-represented code stylometry features.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {85},
numpages = {23},
keywords = {Current Code State, Code Stylometry Features, Microbenchmarking}
}

@inproceedings{10.1145/3611643.3616265,
author = {Hossain, Soneya Binta and Filieri, Antonio and Dwyer, Matthew B. and Elbaum, Sebastian and Visser, Willem},
title = {Neural-Based Test Oracle Generation: A Large-Scale Evaluation and Lessons Learned},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616265},
doi = {10.1145/3611643.3616265},
abstract = {Defining test oracles is crucial and central to test development, but 
manual construction of oracles is expensive. While recent neural-based automated test oracle generation techniques have shown 
promise, their real-world effectiveness remains a compelling question requiring further exploration and understanding. This paper 
investigates the effectiveness of TOGA, a recently developed neural-based method for automatic test oracle generation. TOGA utilizes 
EvoSuite-generated test inputs and generates both exception and assertion oracles. In a Defects4j study, TOGA outperformed specification, search, and neural-based techniques, detecting 57 bugs, including 30 unique bugs not detected by other methods. To gain a deeper understanding of its applicability in real-world settings, we conducted a series of external, extended, and conceptual replication studies of TOGA. 

In a large-scale study involving 25 real-world Java systems, 223.5K test cases, and 51K injected faults, we evaluate TOGA’s 
ability to improve fault-detection effectiveness relative to the state-of-the-practice and the state-of-the-art. We find that TOGA misclassifies the type of oracle needed 24% of the time and that when it classifies correctly around 62% of the time it is not confident enough to generate any assertion oracle. When it does generate an assertion oracle, more than 47% of them are false positives, and the true positive assertions only increase fault detection by 0.3% 
relative to prior work. These findings expose limitations of the state-of-the-art neural-based oracle generation technique, provide 
valuable insights for improvement, and offer lessons for evaluating future automated oracle generation methods.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {120–132},
numpages = {13},
keywords = {EvoSuite, Mutation Testing, Neural Test Oracle Generation, TOGA},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1109/ASE56229.2023.00147,
author = {Louloudakis, Nikolaos and Gibson, Perry and Cano, Jos\'{e} and Rajan, Ajitha},
title = {Fault Localization for Buggy Deep Learning Framework Conversions in Image Recognition},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00147},
doi = {10.1109/ASE56229.2023.00147},
abstract = {When deploying Deep Neural Networks (DNNs), developers often convert models from one deep learning framework to another (e.g., TensorFlow to PyTorch). However, this process is error-prone and can impact target model accuracy. To identify the extent of such impact, we perform and briefly present a differential analysis against three DNNs widely used for image recognition (MobileNetV2, ResNet101, and InceptionV3) converted across four well-known deep learning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which revealed numerous model crashes and output label discrepancies of up to 72%. To mitigate such errors, we present a novel approach towards fault localization and repair of buggy deep learning framework conversions, focusing on pre-trained image recognition models. Our technique consists of four stages of analysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters, and 4) graph representation. In addition, we propose various strategies towards fault repair of the faults detected. We implement our technique on top of the Apache TVM deep learning compiler, and we test it by conducting a preliminary fault localization analysis for the conversion of InceptionV3 from TF to TFLite. Our approach detected a fault in a common DNN converter tool, which introduced precision errors in weights, reducing model accuracy. After our fault localization, we repaired the issue, reducing our conversion error to zero.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1795–1799},
numpages = {5},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/1081870.1081969,
author = {Jeske, Daniel R. and Samadi, Behrokh and Lin, Pengyue J. and Ye, Lan and Cox, Sean and Xiao, Rui and Younglove, Ted and Ly, Minh and Holt, Douglas and Rich, Ryan},
title = {Generation of synthetic data sets for evaluating the accuracy of knowledge discovery systems},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081969},
doi = {10.1145/1081870.1081969},
abstract = {Information Discovery and Analysis Systems (IDAS) are designed to correlate multiple sources of data and use data mining techniques to identify potential significant events. Application domains for IDAS are numerous and include the emerging area of homeland security.Developing test cases for an IDAS requires background data sets into which hypothetical future scenarios can be overlaid. The IDAS can then be measured in terms of false positive and false negative error rates. Obtaining the test data sets can be an obstacle due to both privacy issues and also the time and cost associated with collecting a diverse set of data sources.In this paper, we give an overview of the design and architecture of an IDAS Data Set Generator (IDSG) that enables a fast and comprehensive test of an IDAS. The IDSG generates data using statistical and rule-based algorithms and also semantic graphs that represent interdependencies between attributes. A credit card transaction application is used to illustrate the approach.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {756–762},
numpages = {7},
keywords = {information discovery, data mining, data generation},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/3663529.3663807,
author = {McQueary, Wren and Mim, Sadia Afrin and Raihan, Md Nishat and Smith, Justin and Johnson, Brittany},
title = {Py-holmes: Causal Testing for Deep Neural Networks in Python},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663807},
doi = {10.1145/3663529.3663807},
abstract = {Deep learning has become a go-to solution for many problems. This increases the importance of our ability to understand and improve these technologies. While many tools exist to support debugging deep learning models (e.g., DNNs), few attempt to provide support for understanding the root cause of unexpected behavior. Causal testing is a technique that has been shown to help developers understand and fix the root causes of defects. Causal testing may be particularly valuable in DNNs, where causality is often hard to understand due to the abstractions DNNs create to represent data. In theory, causal testing is capable of supporting root cause debugging in various types of programs and software systems. However, the only implementation that exists is in Java and was not implemented as an end-to-end tool or for use on DNNs, making validation of this theory difficult. In this paper, we introduce py-holmes, a prototype tool that supports causal testing on Python programs, for both DNNs and shallow programs. For more information about py-holmes' internal process, see our GitHub repository: https://go.gmu.edu/pyHolmes_Public_Repo. Our demo video can be found here: https://go.gmu.edu/pyholmes_demo_2024.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {602–606},
numpages = {5},
keywords = {Adversarial Sample Generation, Causal Testing, Counterfactual Explanation, Deep Learning},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/3617574.3617858,
author = {Gill, Waris and Anwar, Ali and Gulzar, Muhammad Ali},
title = {FedDefender: Backdoor Attack Defense in Federated Learning},
year = {2023},
isbn = {9798400703799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617574.3617858},
doi = {10.1145/3617574.3617858},
abstract = {Federated Learning (FL) is a privacy-preserving distributed machine learning technique that enables individual clients (e.g., user participants, edge devices, or organizations) to train a model on their local data in a secure environment and then share the trained model with an aggregator to build a global model collaboratively. In this work, we propose FedDefender, a defense mechanism against targeted poisoning attacks in FL by leveraging differential testing. FedDefender first applies differential testing on clients’ models using a synthetic input. Instead of comparing the output (predicted label), which is unavailable for synthetic input, FedDefender fingerprints the neuron activations of clients’ models to identify a potentially malicious client containing a backdoor. We evaluate FedDefender using MNIST and FashionMNIST datasets with 20 and 30 clients, and our results demonstrate that FedDefender effectively mitigates such attacks, reducing the attack success rate (ASR) to 10% without deteriorating the global model performance.},
booktitle = {Proceedings of the 1st International Workshop on Dependability and Trustworthiness of Safety-Critical Systems with Machine Learned Components},
pages = {6–9},
numpages = {4},
keywords = {testing, poisoning attack, federated learning, fault localization, differential testing, deep learning, backdoor attack},
location = {San Francisco, CA, USA},
series = {SE4SafeML 2023}
}

@article{10.1145/3695992,
author = {Guglielmi, Emanuela and Bavota, Gabriele and Oliveto, Rocco and Scalabrino, Simone},
title = {Automatic Identification of Game Stuttering via Gameplay Videos Analysis},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3695992},
doi = {10.1145/3695992},
abstract = {Modern video games are extremely complex software systems and, as such, they might suffer from several types of post-release issues. A particularly insidious issue is constituted by drops in the frame rate (i.e., stuttering events), which might have a negative impact on the user experience. Stuttering events are frequently documented in the million of hours of gameplay videos shared by players on platforms such as Twitch or YouTube. From the developers’ perspective, these videos represent a free source of documented “testing activities.” However, especially for popular games, the quantity and length of these videos make impractical their manual inspection. We introduce HASTE, an approach for the automatic detection of stuttering events in gameplay videos that can be exploited to generate candidate bug reports. HASTE firstly splits a given video into visually coherent slices, with the goal of filtering-out those that not representing actual gameplay (e.g., navigating the game settings). Then, it identifies the subset of pixels in the video frames which actually show the game in action excluding additional elements on screen such as the logo of the YouTube channel, on-screen chats, and so forth. In this way, HASTE can exploit state-of-the-art image similarity metrics to identify candidate stuttering events, namely subsequent frames being almost identical in the pixels depicting the game. We evaluate the different steps behind HASTE on a total of 105 videos showing that it can correctly extract video slices with a 76% precision, and can correctly identify the slices related to gameplay with a recall and precision higher than 77%. Overall, HASTE achieves 71% recall and 89% precision for the identification of stuttering events in gameplay videos.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {38},
numpages = {29},
keywords = {Video game, Performance}
}

@inproceedings{10.1145/3464298.3493400,
author = {Tak, Byungchul and Han, Wook-Shin},
title = {Lognroll: discovering accurate log templates by iterative filtering},
year = {2021},
isbn = {9781450385343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3464298.3493400},
doi = {10.1145/3464298.3493400},
abstract = {Modern IT systems rely heavily on log analytics for critical operational tasks. Since the volume of logs produced from numerous distributed components is overwhelming, it requires us to employ automated processing. The first step of automated log processing is to convert streams of log lines into the sequence of log format IDs, called log templates. A log template serves as a base string with unfilled parts from which logs are generated during runtime by substitution of contextual information. The problem of log template discovery from the volume of collected logs poses a great challenge due to the semi-structured nature of the logs and the computational overheads. Our investigation reveals that existing techniques show various limitations. We approach the log template discovery problem as search-based learning by applying the ILP (Inductive Logic Programming) framework. The algorithm core consists of narrowing down the logs into smaller sets by analyzing value compositions on selected log column positions. Our evaluation shows that it produces accurate log templates from diverse application logs with small computational costs compared to existing methods. With the quality metric we defined, we obtained about 21%-51% improvements of log template quality.},
booktitle = {Proceedings of the 22nd International Middleware Conference},
pages = {273–285},
numpages = {13},
keywords = {sequential covering, log template, log analysis},
location = {Qu\'{e}bec city, Canada},
series = {Middleware '21}
}

@inproceedings{10.1145/3551349.3556920,
author = {Zhang, Yingyi and Wang, Zan and Jiang, Jiajun and You, Hanmo and Chen, Junjie},
title = {Toward Improving the Robustness of Deep Learning Models via Model Transformation},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556920},
doi = {10.1145/3551349.3556920},
abstract = {Deep learning (DL) techniques have attracted much attention in recent years, and have been applied to many application scenarios, including those that are safety-critical. Improving the universal robustness of DL models is vital and many approaches have been proposed in the last decades aiming at such a purpose. Among existing approaches, adversarial training is the most representative. It advocates a post model tuning process via incorporating adversarial samples. Although successful, they still suffer from the challenge of generalizability issues in the face of various attacks with unsatisfactory effectiveness. Targeting this problem, in this paper we propose a novel model training framework, which aims at improving the universal robustness of DL models via model transformation incorporated with a data augmentation strategy in a delta debugging fashion. We have implemented our approach in a tool, called Dare, and conducted an extensive evaluation on 9 DL models. The results show that our approach significantly outperforms existing adversarial training techniques. Specifically, Dare has achieved the highest Empirical Robustness in 29 of 45 testing scenarios under various attacks, while the number drops to 5 of 45 for the best baseline approach.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {104},
numpages = {13},
keywords = {Model Robustness, Delta Debugging, Deep Neural Network},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@article{10.1145/3626712,
author = {Huang, Yicong and Wang, Zuozhi and Li, Chen},
title = {Udon: Efficient Debugging of User-Defined Functions in Big Data Systems with Line-by-Line Control},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
url = {https://doi.org/10.1145/3626712},
doi = {10.1145/3626712},
abstract = {Many big data systems are written in languages such as C, C++, Java, and Scala to process large amounts of data efficiently, while data analysts often use Python to conduct data wrangling, statistical analysis, and machine learning. User-defined functions (UDFs) are commonly used in these systems to bridge the gap between the two ecosystems. In this paper, we propose Udon, a novel debugger to support fine-grained debugging of UDFs. Udon encapsulates the modern line-by-line debugging primitives, such as the ability to set breakpoints, perform code inspections, and make code modifications while executing a UDF on a single tuple. It includes a novel debug-aware UDF execution model to ensure the responsiveness of the operator during debugging. It utilizes advanced state-transfer techniques to satisfy breakpoint conditions that span across multiple UDFs. It incorporates various optimization techniques to reduce the runtime overhead. We conduct experiments with multiple UDF workloads on various datasets and show its high efficiency and scalability.},
journal = {Proc. ACM Manag. Data},
month = dec,
articleno = {225},
numpages = {26},
keywords = {big data systems, debugging, user-defined functions (UDFs)}
}

@inproceedings{10.1145/3643658.3643919,
author = {Mastain, Vincent and Petrillo, Fabio},
title = {A Behavior-driven Development and Reinforcement Learning approach for videogame automated testing},
year = {2024},
isbn = {9798400705618},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643658.3643919},
doi = {10.1145/3643658.3643919},
abstract = {Video game development has undergone a significant transformation in the last decade, with modern games becoming increasingly complex and sophisticated. Testing these games for quality assurance is challenging and time-consuming, often relying on manual testers. In this paper, we introduce an automated testing approach that combines Behavior-Driven Development (BDD) with Reinforcement Learning (RL) to streamline the testing process. We present a framework that uses natural language-based test cases to describe game behaviors and expected outcomes, combined with RL, to test games automatically. We validated our approach through tests on four distinct Python-based games. We analyzed the impact of game complexity on training duration and discussed the challenges of defining optimal reward functions. Our framework provides a structured approach to address RL complexities, simplifying the process of creating test scenarios. Combining BDD and RL offers a promising solution to test complex modern video games more efficiently and ensure higher game quality upon release.},
booktitle = {Proceedings of the ACM/IEEE 8th International Workshop on Games and Software Engineering},
pages = {1–8},
numpages = {8},
keywords = {videogame, testing, reinforcement learning, behavior-driven development, software quality, software engineering},
location = {Lisbon, Portugal},
series = {GAS '24}
}

@proceedings{10.1145/3663529,
title = {FSE 2024: Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to FSE 2024, the ACM International Conference on the Foundations of Software Engineering (FSE) 2024. The conference now has a shorter name! FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {Porto de Galinhas, Brazil}
}

@inproceedings{10.1145/3650212.3680354,
author = {Shin, Jiho and Hashtroudi, Sepehr and Hemmati, Hadi and Wang, Song},
title = {Domain Adaptation for Code Model-Based Unit Test Case Generation},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680354},
doi = {10.1145/3650212.3680354},
abstract = {Recently, deep learning-based test case generation approaches have been proposed to automate the generation of unit test cases. In this study, we leverage Transformer-based code models to generate
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
unit tests with the help of Domain Adaptation (DA) at a project level. Specifically, we use CodeT5, a relatively small language model trained on source code data, and fine-tune it on the test generation
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
task. Then, we apply domain adaptation to each target project data to learn project-specific knowledge (project-level DA). We use the Methods2test dataset to fine-tune CodeT5 for the test generation
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
task and the Defects4j dataset for project-level domain adaptation and evaluation. We compare our approach with (a) CodeT5 fine-tuned on the test generation without DA, (b) the A3Test tool, and (c)
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
GPT-4 on five projects from the Defects4j dataset. The results show that tests generated using DA can increase the line coverage by 18.62%, 19.88%, and 18.02% and mutation score by 16.45%, 16.01%,
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
and 12.99% compared to the above (a), (b), and (c) baselines, respectively. The overall results show consistent improvements in metrics such as parse rate, compile rate, BLEU, and CodeBLEU. In addition,
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
we show that our approach can be seen as a complementary solution alongside existing search-based test generation tools such as EvoSuite, to increase the overall coverage and mutation scores with an average of 34.42% and 6.8%, for line coverage and mutation score, respectively.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1211–1222},
numpages = {12},
keywords = {Code Model, Domain Adaption, GPT, LLM, Test generation, Transformers},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@article{10.1145/3583565,
author = {C., Shrikanth N. and Menzies, Tim},
title = {Assessing the Early Bird Heuristic (for Predicting Project Quality)},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3583565},
doi = {10.1145/3583565},
abstract = {Before researchers rush to reason across all available data or try complex methods, perhaps it is prudent to first check for simpler alternatives. Specifically, if the historical data has the most information in some small region, then perhaps a model learned from that region would suffice for the rest of the project.To support this claim, we offer a case study with 240 projects, where we find that the information in those projects “clumps” towards the earliest parts of the project. A quality prediction model learned from just the first 150 commits works as well, or better than state-of-the-art alternatives. Using just this “early bird” data, we can build models very quickly and very early in the project life cycle. Moreover, using this early bird method, we have shown that a simple model (with just a few features) generalizes to hundreds of projects.Based on this experience, we doubt that prior work on generalizing quality models may have needlessly complicated an inherently simple process. Further, prior work that focused on later-life cycle data needs to be revisited, since their conclusions were drawn from relatively uninformative regions.Replication note: All our data and scripts are available here: https://github.com/snaraya7/early-bird.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {116},
numpages = {39},
keywords = {data-lite, early, defects, Quality prediction}
}

@inproceedings{10.1145/3686812.3686814,
author = {Irshad, Ayesha and Azam, Farooque and Anwar, Muhammad Waseem},
title = {A Framework for Clone Detection in UML Models (UMCD)},
year = {2024},
isbn = {9798400717215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686812.3686814},
doi = {10.1145/3686812.3686814},
abstract = {Clone detection plays a vital role in managing the quality and maintainability of software systems. In the process of software development, the initial phase is to specify and visualize the software design using UML models. These models serve as a blueprint to guide through all the phases of the software development process. Therefore, if there are clones in these UML models they will induce clones in further stages of software development as well. Subsequently, these clones will propagate and amplify the clone-related issues throughout the software development process. For this reason, detection, tracking, and removal of the clones in UML models is as crucial as in code. Furthermore, Model Driven Software Engineering (MDSE) aims to automatically generate code from models such as UML models. Consequently, increasing the importance of Model clone detection. This study focuses on the application of Natural Language Processing (NLP) to detect clones within UML models especially targeting UML state-machine models. Initially, a UML model is created, and exported in XML format, to represent the model in textual form. Since the XML code of UML diagrams carries a lot of structural information that is irrelevant for clone detection and is also not balanced. Therefore, the XML code is parsed to extract the relevant features of the model. The extracted features are further preprocessed to represent them in a suitable format. Furthermore, the extracted data is labeled to represent clone and nonclone pairs. Moreover, for the detection of clones Natural Language processing techniques are used since, UML models have a lot of textual information e.g., names of elements, constraints, etc. Therefore, NLP techniques can efficiently identify duplicates in UML Models. The proposed framework is applied to several case studies. These case studies validate the effectiveness of our approach in model clone detection.},
booktitle = {Proceedings of the 2024 16th International Conference on Computer Modeling and Simulation},
pages = {7–14},
numpages = {8},
keywords = {Extensible Markup Language (XML), MDSE (Model Driven Software Engineering), NLP (Natural Language Processing), State Machine (SM), UMCD (UML Model Clone Detection), UML (Unified modeling language)},
location = {Dalian, China},
series = {ICCMS '24}
}

@article{10.1145/2557833.2560586,
author = {Peiris, Manjula and Hill, James H.},
title = {Towards detecting software performance anti-patterns using classification techniques},
year = {2014},
issue_date = {January 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2557833.2560586},
doi = {10.1145/2557833.2560586},
abstract = {This paper presents a non-intrusive machine learning approach called Non-intrusive Performance Anti-pattern Detecter (NiPAD) for identifying and classifying software performance anti-patterns. NiPAD uses only system performance metrics-as opposed to analyzing application level performance metrics or source code and the design of a software application to identify and classify software performance anti-patterns within an application. The results of applying NiPAD to an example application show that NiPAD is able to predict the One Lane Bridge software performance anti-pattern within a software application with 0.94 accuracy.},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–4},
numpages = {4},
keywords = {software performance anti-patterns, machine learning, dynamic software analysis, classification}
}

@inproceedings{10.1145/3551349.3556932,
author = {Xiao, Yan and Lin, Yun and Beschastnikh, Ivan and Sun, Changsheng and Rosenblum, David and Dong, Jin Song},
title = {Repairing Failure-inducing Inputs with Input Reflection},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556932},
doi = {10.1145/3551349.3556932},
abstract = {Trained with a sufficiently large training and testing dataset, Deep Neural Networks (DNNs) are expected to generalize. However, inputs may deviate from the training dataset distribution in real deployments. This is a fundamental issue with using a finite dataset, which may lead deployed DNNs to mis-predict in production. Inspired by input-debugging techniques for traditional software systems, we propose a runtime approach to identify and fix failure-inducing inputs in deep learning systems. Specifically, our approach targets DNN mis-predictions caused by unexpected (deviating and out-of-distribution) runtime inputs. Our approach has two steps. First, it recognizes and distinguishes deviating (“unseen” semantically-preserving) and out-of-distribution inputs from in-distribution inputs. Second, our approach fixes the failure-inducing inputs by transforming them into inputs from the training set that have similar semantics. We call this process input reflection and formulate it as a search problem over the embedding space on the training set. We implemented a tool called InputReflector based on the above two-step approach and evaluated it with experiments on three DNN models trained on CIFAR-10, MNIST, and FMNIST image datasets. The results show that InputReflector can effectively distinguish deviating inputs that retain semantics of the distribution (e.g., zoomed images) and out-of-distribution inputs from in-distribution inputs. InputReflector repairs deviating inputs and achieves 30.78% accuracy improvement over original models. We also illustrate how InputReflector can be used to evaluate tests generated by deep learning testing tools.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {85},
numpages = {13},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@article{10.1109/TNET.2023.3234931,
author = {Shukla, Apoorv and Hudemann, Kevin and V\'{a}gi, Zsolt and H\"{u}gerich, Lily and Smaragdakis, Georgios and Hecker, Artur and Schmid, Stefan and Feldmann, Anja},
title = {Runtime Verification for Programmable Switches},
year = {2023},
issue_date = {Aug. 2023},
publisher = {IEEE Press},
volume = {31},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2023.3234931},
doi = {10.1109/TNET.2023.3234931},
abstract = {We introduce a runtime verification framework for programmable switches that complements static analysis. To evaluate our approach, we design and develop &lt;monospace&gt;P6&lt;/monospace&gt;, a runtime verification system that automatically detects, localizes, and patches software bugs in P4 programs. Bugs are reported via a violation of pre-specified expected behavior that is captured by &lt;monospace&gt;P6&lt;/monospace&gt;. &lt;monospace&gt;P6&lt;/monospace&gt; is based on machine learning-guided fuzzing that tests P4 switch non-intrusively, i.e., without modifying the P4 program for detecting runtime bugs. This enables an automated and real-time localization and patching of bugs. We used a &lt;monospace&gt;P6&lt;/monospace&gt; prototype to detect and patch existing bugs in various publicly available P4 application programs deployed on two different switch platforms, namely, behavioral model (bmv2) and Tofino. Our evaluation shows that &lt;monospace&gt;P6&lt;/monospace&gt; significantly outperforms bug detection baselines while generating fewer packets and patches bugs in large P4 programs, e.g., &lt;monospace&gt;switch.p4&lt;/monospace&gt; without triggering any regressions.},
journal = {IEEE/ACM Trans. Netw.},
month = jan,
pages = {1822–1837},
numpages = {16}
}

@article{10.1145/3708473,
author = {Manke, Ruchira and Wardat, Mohammad and Khomh, Foutse and Rajan, Hridesh},
title = {Leveraging Data Characteristics for Bug Localization in Deep Learning Programs},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708473},
doi = {10.1145/3708473},
abstract = {Deep Learning (DL) is a class of machine learning algorithms that are used in a wide variety of applications. Like any software system, DL programs can have bugs. To support bug localization in DL programs, several tools have been proposed in the past. As most of the bugs that occur due to improper model structure known as structural bugs lead to inadequate performance during training, it is challenging for developers to identify the root cause and address these bugs. To support bug detection and localization in DL programs, in this paper, we propose Theia, which detects and localizes structural bugs in DL programs. Unlike the previous works, Theia considers the training dataset characteristics to automatically detect bugs in DL programs developed using two deep learning libraries, Keras and PyTorch. Since training the DL models is a time-consuming process, Theia detects these bugs at the beginning of the training process and alerts the developer with informative messages containing the bug's location and actionable fixes which will help them to improve the structure of the model. We evaluated Theia on a benchmark of 40 real-world buggy DL programs obtained from Stack Overflow. Our results show that Theia successfully localizes 57/75 structural bugs in 40 buggy programs, whereas NeuraLint, a state-of-the-art approach capable of localizing structural bugs before training localizes 17/75 bugs.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {deep learning bugs, bug localization, debugging, program analysis}
}

@inproceedings{10.1145/3468264.3473494,
author = {Chakraborty, Mohna},
title = {Does reusing pre-trained NLP model propagate bugs?},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3473494},
doi = {10.1145/3468264.3473494},
abstract = {In this digital era, the textual content has become a seemingly ubiquitous part of our life. Natural Language Processing (NLP) empowers machines to comprehend the intricacies of textual data and eases human-computer interaction. Advancement in language modeling, continual learning, availability of a large amount of linguistic data, and large-scale computational power have made it feasible to train models for downstream tasks related to text analysis, including safety-critical ones, e.g., medical, airlines, etc. Compared to other deep learning (DL) models, NLP-based models are widely reused for various tasks. However, the reuse of pre-trained models in a new setting is still a complex task due to the limitations of the training dataset, model structure, specification, usage, etc. With this motivation, we study BERT, a vastly used language model (LM), from the direction of reusing in the code. We mined 80 posts from Stack Overflow related to BERT and found 4 types of bugs observed in clients’ code. Our results show that 13.75% are fairness, 28.75% are parameter, 15% are token, and 16.25% are version-related bugs.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1686–1688},
numpages = {3},
keywords = {Reuse, NLP, Deep Learning, Bug, BERT},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3395363.3397355,
author = {Liu, Hui and Shen, Mingzhu and Jin, Jiahao and Jiang, Yanjie},
title = {Automated classification of actions in bug reports of mobile apps},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397355},
doi = {10.1145/3395363.3397355},
abstract = {When users encounter problems with mobile apps, they may commit such problems to developers as bug reports. To facilitate the processing of bug reports, researchers proposed approaches to validate the reported issues automatically according to the steps to reproduce specified in bug reports. Although such approaches have achieved high success rate in reproducing the reported issues, they often rely on a predefined vocabulary to identify and classify actions in bug reports. However, such manually constructed vocabulary and classification have significant limitations. It is challenging for the vocabulary to cover all potential action words because users may describe the same action with different words. Besides that, classification of actions solely based on the action words could be inaccurate because the same action word, appearing in different contexts, may have different meaning and thus belongs to different action categories. To this end, in this paper we propose an automated approach, called MaCa, to identify and classify action words in Mobile apps’ bug reports. For a given bug report, it first identifies action words based on natural language processing. For each of the resulting action words, MaCa extracts its contexts, i.e., its enclosing segment, the associated UI target, and the type of its target element by both natural language processing and static analysis of the associated app. The action word and its contexts are then fed into a machine learning based classifier that predicts the category of the given action word in the given context. To train the classifier, we manually labelled 1,202 actions words from 525 bug reports that are associated with 207 apps. Our evaluation results on manually labelled data suggested that MaCa was accurate with high accuracy varying from 95% to 96.7%. We also investigated to what extent MaCa could further improve existing approaches (i.e., Yakusu and ReCDroid) in reproducing bug reports. Our evaluation results suggested that integrating MaCa into existing approaches significantly improved the success rates of ReCDroid and Yakusu by 22.7% = (69.2%-56.4%)/56.4% and 22.9%= (62.7%-51%)/51%, respectively.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {128–140},
numpages = {13},
keywords = {Test Case Generation, Mobile Testing, Classification, Bug report},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/2884781.2884857,
author = {Tantithamthavorn, Chakkrit and McIntosh, Shane and Hassan, Ahmed E. and Matsumoto, Kenichi},
title = {Automated parameter optimization of classification techniques for defect prediction models},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884857},
doi = {10.1145/2884781.2884857},
abstract = {Defect prediction models are classifiers that are trained to identify defect-prone software modules. Such classifiers have configurable parameters that control their characteristics (e.g., the number of trees in a random forest classifier). Recent studies show that these classifiers may underperform due to the use of suboptimal default parameter settings. However, it is impractical to assess all of the possible settings in the parameter spaces. In this paper, we investigate the performance of defect prediction models where Caret --- an automated parameter optimization technique --- has been applied. Through a case study of 18 datasets from systems that span both proprietary and open source domains, we find that (1) Caret improves the AUC performance of defect prediction models by as much as 40 percentage points; (2) Caret-optimized classifiers are at least as stable as (with 35% of them being more stable than) classifiers that are trained using the default settings; and (3) Caret increases the likelihood of producing a top-performing classifier by as much as 83%. Hence, we conclude that parameter settings can indeed have a large impact on the performance of defect prediction models, suggesting that researchers should experiment with the parameters of the classification techniques. Since automated parameter optimization techniques like Caret yield substantially benefits in terms of performance improvement and stability, while incurring a manageable additional computational cost, they should be included in future defect prediction studies.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {321–332},
numpages = {12},
keywords = {software defect prediction, parameter optimization, experimental design, classification techniques},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/3540250.3558907,
author = {Khanfir, Ahmed},
title = {Effective and scalable fault injection using bug reports and generative language models},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558907},
doi = {10.1145/3540250.3558907},
abstract = {Previous research has shown that artificial faults can be useful in many software engineering tasks such as testing, fault-tolerance assessment, debugging, dependability evaluation, risk analysis, etc. However, such artificial-fault-based applications can be questioned or inaccurate when the considered faults misrepresent real bugs. Since typically, fault injection techniques (i.e. mutation testing) produce a large number of faults by altering ”blindly” the code in arbitrary locations, they are unlikely capable to produce few but relevant real-like faults. In our work, we tackle this challenge by guiding the injection towards resembling bugs that have been previously introduced by developers. For this purpose, we propose iBiR, the first fault injection approach that leverages information from bug reports to inject ”realistic” faults. iBiR injects faults on the locations that are more likely to be related to a given bug-report by applying appropriate inverted fix-patterns, which are manually or automatically crafted by automated-program-repair researchers. We assess our approach using bugs from the Defects4J dataset and show that iBiR outperforms significantly conventional mutation testing in terms of injecting faults that semantically resemble and couple with real ones, in the vast majority of the cases. Similarly, the faults produced by iBiR give significantly better fault-tolerance estimates than conventional mutation testing in around 80% of the cases.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1790–1794},
numpages = {5},
keywords = {Naturalness, Mutation, Language models, Information Retrieval, Fault Injection, Bug Reports},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@article{10.1145/3689627,
author = {Sharief, Farhana and Ijaz, Humaira and Shojafar, Mohammad and Naeem, Muhammad Asif},
title = {Multi-Class Imbalanced Data Handling with Concept Drift in Fog Computing: A Taxonomy, Review, and Future Directions},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3689627},
doi = {10.1145/3689627},
abstract = {A network of actual physical objects or “IoT components” linked to the internet and equipped with sensors, electronics, software, and network connectivity is known as the Internet of Things (IoT). This ability of the IoT components to gather and share data is made possible by this network connectivity. Many IoT devices are currently operating, which generate a lot of data. When these IoT devices started collecting data, the cloud was the only place to analyze, filter, pre-process, and aggregate it. However, when it comes to IoT, the cloud has restrictions regarding latency and a more centralized method of distributing programs. A new form of computing called Fog computing has been proposed to address the shortcomings of current cloud computing. In an IoT context, sensors regularly communicate signal information, and edge devices process the data obtained from these sensors using Fog computing. The sensors’ internal or external problems, security breaches, or the integration of heterogeneous equipment contribute to the imbalanced data, i.e., comparatively speaking, one class has more instances than the other. As a result of this data, the pattern extraction is imbalanced. Recent attempts have concentrated heavily on binary-class imbalanced concerns with exactly two classes. However, the classification of multi-class imbalanced data is an issue that needs to be fixed in Fog computing, even if it is widespread in other fields, including text categorization, human activity detection, and medical diagnosis. The study intends to deal with this problem. It presents a systematic, thorough, and in-depth comparative analysis of several binary-class and multi-class imbalanced data handling strategies for batch and streaming data in IoT networks and Fog computing. There are five major objectives in this study. First, reviewing the Fog computing concept. Second, outlining the optimization metric used in Fog computing. Third, focusing on binary and multi-class batch data handling for IoT networks and Fog computing. Fourth, reviewing and comparing the current imbalanced data handling methodologies for multi-class data streams. Fifth, explaining how to cope with the concept drift, including novel and recurring classes, targeted optimization measures, and evaluation tools. Finally, the best performance metrics and tools for concept drift, binary-class (batch and stream) data, and multi-class (batch and stream) data are highlighted.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {16},
numpages = {48},
keywords = {Cloud computing, fog computing, Internet of Things (IoT), multi-class imbalanced data stream, concept drift}
}

@inproceedings{10.1145/3639478.3643124,
author = {Liu, Changshu and Cetin, Pelin and Patodia, Yogesh and Ray, Baishakhi and Chakraborty, Saikat and Ding, Yangruibo},
title = {Automated Code Editing with Search-Generate-Modify},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643124},
doi = {10.1145/3639478.3643124},
abstract = {Code editing is essential in evolving software development. In literature, several automated code editing tools are proposed, which leverage Information Retrieval-based techniques and Machine Learning-based code generation and code editing models.A patch that is obtained by search &amp; retrieval, even if incorrect, can provide helpful guidance to a code generation model. However, a retrieval-guided patch produced by a code generation model can still be a few tokens off from the intended patch. Such generated patches can be slightly modified to create the intended patches. We propose SarGaM which mimics a developer's behavior - search for related patches, generate or write code and then modify to adapt it to the right context. Our evaluation of SarGaM on edit generation shows superior performance w.r.t. the current state-of-the-art techniques. SarGaM also shows its effectiveness on automated program repair tasks.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {398–399},
numpages = {2},
keywords = {bug fixing, automated program repair, edit-based neural network},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@article{10.1145/3714467,
author = {Bartocci, Ezio and Mariani, Leonardo and Ni\v{c}kovi\'{c}, Dejan and Yadav, Drishti},
title = {Signal Feature Coverage and Testing for CPS Dataflow Models},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3714467},
doi = {10.1145/3714467},
abstract = {Design of cyber-physical systems (CPS) typically involves dataflow modelling. The structure of dataflow models differs from the traditional software, making standard coverage metrics not appropriate for measuring the thoroughness of testing. To address this limitation, this paper proposes signal feature coverage as a new coverage metric for systematically testing CPS dataflow models. We derive signal feature coverage by leveraging signal features. We developed a testing framework in Simulink®, a popular dataflow modelling and simulation environment, that automates the generation and execution of test cases based on the defined coverage metric. We evaluated the effectiveness of our approach by carrying out experiments on five Simulink®models tested against ten Signal Temporal Logic specifications. We compared our coverage-based testing approach to adaptive random testing, falsification testing, output diversity-based approaches, and testing using MathWorks’ Simulink® Design Verifier™. The results demonstrate that our coverage-based testing approach outperforms the conventional techniques regarding fault detection capability.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Cyber-Physical Systems, Simulink models, Coverage criteria, Testing, Signal Temporal Logic (STL)}
}

@article{10.1145/3715006,
author = {Chowdhury, Shaiful},
title = {The Good, the Bad, and the Monstrous: Predicting Highly Change-Prone Source Code Methods at Their Inception},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715006},
doi = {10.1145/3715006},
abstract = {The cost of software maintenance often surpasses the initial development expenses, making it a significant concern for the software industry. A key strategy for alleviating future maintenance burdens is the early prediction and identification of change-prone code components, which allows for timely optimizations. While prior research has largely concentrated on predicting change-prone files and classes—an approach less favored by practitioners—this paper shifts focus to predicting highly change-prone methods, aligning with the preferences of both practitioners and researchers. We analyzed 774,051 source code methods from 49 prominent open-source Java projects. Our findings reveal that approximately 80% of changes are concentrated in just 20% of the methods, demonstrating the Pareto 80/20 principle. Moreover, this subset of methods is responsible for the majority of the identified bugs in these projects. After establishing their critical role in mitigating software maintenance costs, our study shows that machine learning models can effectively identify these highly change-prone methods from their inception. Additionally, we conducted a thorough manual analysis to uncover common patterns (or concepts) among the more difficult-to-predict methods. These insights can help future research develop new features and enhance prediction accuracy.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {change-proneness, bug-proneness, code metrics, maintenance, McCabe, code complexity}
}

@inproceedings{10.1145/3597926.3605231,
author = {Downing, Mara},
title = {Quantitative Robustness Analysis of Neural Networks},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3605231},
doi = {10.1145/3597926.3605231},
abstract = {Neural networks are an increasingly common tool for solving problems that require complex analysis and pattern matching, such as identifying stop signs or processing medical imagery. Accordingly, verification of neural networks for safety and correctness is of great importance, as mispredictions can have catastrophic results in safety critical domains. One metric for verification is robustness, which answers whether or not a misclassified input exists in a given input neighborhood. I am focusing my research at quantitative robustness---finding not only if there exist misclassified inputs within a given neighborhood but also how many exist as a proportion of the neighborhood size. My overall goal is to expand the research on quantitative neural network robustness verification and create a variety of quantitative verification tools geared towards expanding our understanding of neural network robustness.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1527–1531},
numpages = {5},
keywords = {Robustness, Quantitative Verification, Neural Network Verification},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@article{10.1145/3678167,
author = {Liu, Yue and Tantithamthavorn, Chakkrit and Liu, Yonghui and Thongtanunam, Patanamon and Li, Li},
title = {Automatically Recommend Code Updates: Are We There Yet?},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {8},
issn = {1049-331X},
url = {https://doi.org/10.1145/3678167},
doi = {10.1145/3678167},
abstract = {In recent years, large pre-trained Language Models of Code (CodeLMs) have shown promising results on various software engineering tasks. One such task is automatic code update recommendation, which transforms outdated code snippets into their approved and revised counterparts. Although many CodeLM-based approaches have been proposed, claiming high accuracy, their effectiveness and reliability on real-world code update tasks remain questionable. In this article, we present the first extensive evaluation of state-of-the-art CodeLMs for automatically recommending code updates. We assess their performance on two diverse datasets of paired updated methods, considering factors such as temporal evolution, project specificity, method size, and update complexity. Our results reveal that while CodeLMs exhibit higher performance in settings that ignore temporal information, they struggle in more realistic time-wise scenarios and generalize poorly to new projects. Furthermore, CodeLM performance decreases significantly for larger methods and more complex updates. Furthermore, we observe that many CodeLM-generated “updates” are actually null, especially in time-wise settings, and meaningful edits remain challenging. Our findings highlight the significant gap between the perceived and actual effectiveness of CodeLMs for real-world code update recommendation and emphasize the need for more research on improving their practicality, robustness, and generalizability.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {217},
numpages = {27},
keywords = {Code updates, neural machine translation}
}

@inproceedings{10.1145/3194104.3194110,
author = {Young, Steven and Abdou, Tamer and Bener, Ayse},
title = {A replication study: just-in-time defect prediction with ensemble learning},
year = {2018},
isbn = {9781450357234},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194104.3194110},
doi = {10.1145/3194104.3194110},
abstract = {Just-in-time defect prediction, which is also known as change-level defect prediction, can be used to efficiently allocate resources and manage project schedules in the software testing and debugging process. Just-in-time defect prediction can reduce the amount of code to review and simplify the assignment of developers to bug fixes. This paper reports a replicated experiment and an extension comparing the prediction of defect-prone changes using traditional machine learning techniques and ensemble learning. Using datasets from six open source projects, namely Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL we replicate the original approach to verify the results of the original experiment and use them as a basis for comparison for alternatives in the approach. Our results from the replicated experiment are consistent with the original. The original approach uses a combination of data preprocessing and a two-layer ensemble of decision trees. The first layer uses bagging to form multiple random forests. The second layer stacks the forests together with equal weights. Generalizing the approach to allow the use of any arbitrary set of classifiers in the ensemble, optimizing the weights of the classifiers, and allowing additional layers, we apply a new deep ensemble approach, called deep super learner, to test the depth of the original study. The deep super learner achieves statistically significantly better results than the original approach on five of the six projects in predicting defects as measured by F1 score.},
booktitle = {Proceedings of the 6th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
pages = {42–47},
numpages = {6},
keywords = {defect prediction, deep learning},
location = {Gothenburg, Sweden},
series = {RAISE '18}
}

@inproceedings{10.1145/3361242.3361243,
author = {Wang, Yuehuan and Li, Zenan and Xu, Jingwei and Yu, Ping and Ma, Xiaoxing},
title = {Fast Robustness Prediction for Deep Neural Network},
year = {2019},
isbn = {9781450377010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361242.3361243},
doi = {10.1145/3361242.3361243},
abstract = {Deep neural networks (DNNs) have achieved impressive performance in many difficult tasks. However, DNN models are essentially uninterpretable to humans, and unfortunately prone to adversarial attacks, which hinders their adoption in security and safety-critical scenarios. The robustness of a DNN model, which measures its stableness against adversarial attacks, becomes an important topic in both the machine learning and the software engineering communities. Analytical evaluation of DNN robustness is difficult due to the high-dimensionality of inputs, the huge amount of parameters, and the nonlinear network structure. In practice, the degree of robustness of DNNs is empirically approximated with adversarial searching, which is computationally expensive and cannot be applied in resource constrained settings such as embedded computing. In this paper, we propose to predict the robustness of a DNN model for each input with another DNN model, which takes the output of neurons of the former model as input. We train a regression model to encode the connections between output of the penultimate layer of a DNN model and its robustness. With this trained model, the robustness for an input can be predicted instantaneously. Experiments with MNIST and CIFAR10 datasets and LeNet, VGG and ResNet DNN models were conducted to evaluate the efficacy of the proposed approach. The results indicated that our approach achieved 0.05-0.21 mean absolute errors and significantly outperformed confidence and surprise adequacy-based approaches.},
booktitle = {Proceedings of the 11th Asia-Pacific Symposium on Internetware},
articleno = {11},
numpages = {10},
keywords = {Robustness, Prediction, Deep Neural Networks},
location = {Fukuoka, Japan},
series = {Internetware '19}
}

@inproceedings{10.1145/3540250.3558931,
author = {Orvalho, Pedro and Janota, Mikol\'{a}\v{s} and Manquinho, Vasco},
title = {MultIPAs: applying program transformations to introductory programming assignments for data augmentation},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558931},
doi = {10.1145/3540250.3558931},
abstract = {There has been a growing interest, over the last few years, in the topic of automated program repair applied to fixing introductory programming assignments (IPAs).  
However, the datasets of IPAs publicly available tend to be small and with no valuable annotations about the defects of each program. Small datasets are not very useful for program repair tools that rely on machine learning models. Furthermore, a large diversity of correct implementations allows computing a smaller set of repairs to fix a given incorrect program rather than always using the same set of correct implementations for a given IPA. For these reasons, there has been an increasing demand for the task of augmenting IPAs benchmarks.  

This paper presents MultIPAs, a program transformation tool that can augment IPAs benchmarks by: (1) applying six syntactic mutations that conserve the program's semantics and (2) applying three semantic mutilations that introduce faults in the IPAs.  
Moreover, we demonstrate the usefulness of MultIPAs by augmenting with millions of programs  
two publicly available benchmarks of programs written in the C language, and also by generating an extensive benchmark of semantically incorrect programs.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1657–1661},
numpages = {5},
keywords = {Program Transformation, MOOCs, Introductory Programming Assignments, Data Augmentation, Automated Program Repair},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3578244.3583738,
author = {Nayrolles, Mathieu},
title = {Pushing the Limits of Video Game Performance: A Performance Engineering Perspective},
year = {2023},
isbn = {9798400700682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578244.3583738},
doi = {10.1145/3578244.3583738},
abstract = {Ubisoft constantly pushes the boundaries of game development to create immersive worlds that capture the imagination of millions of players worldwide. To achieve this, performance engineering plays a crucial role in ensuring that games run smoothly on various platforms and devices. In this talk, we will explore the latest advancements in the field of performance engineering for video games, focusing on runtime performance, network optimization, backend and database optimization, and cloud gaming. We will discuss how machine learning techniques enhance classical profiling and optimize game engine scheduling.Additionally, we will address the challenges of deterministic replication of assets between clients and optimizing micro-services for cloud gaming experiences. Lastly, we will touch on the importance of performance engineering for non-code aspects of game development, such as animation, textures, props, and assets.},
booktitle = {Proceedings of the 2023 ACM/SPEC International Conference on Performance Engineering},
pages = {1},
numpages = {1},
keywords = {performance engineering, video games},
location = {Coimbra, Portugal},
series = {ICPE '23}
}

@inproceedings{10.1145/2896921.2896923,
author = {Felbinger, Hermann and Wotawa, Franz and Nica, Mihai},
title = {Empirical study of correlation between mutation score and model inference based test suite adequacy assessment},
year = {2016},
isbn = {9781450341516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896921.2896923},
doi = {10.1145/2896921.2896923},
abstract = {In this paper we investigate a method for test suite evaluation that is based on an inferred model from the test suite. The idea is to use the similarity between the inferred model and the system under test as a measure of test suite adequacy, which is the ability of a test suite to expose errors in the system under test. We define similarity using the root mean squared error computed from the differences of the system under test output and the model output for certain inputs not used for model inference. In the paper we introduce the approach and provide results of an experimental evaluation where we compare the similarity with the mutation score. We used the Pearson Correlation coefficient to calculate whether a linear correlation between mutation score and root mean squared error exists. As a result we obtain that in certain cases the computed similarity strongly correlates with the mutation score.},
booktitle = {Proceedings of the 11th International Workshop on Automation of Software Test},
pages = {43–49},
numpages = {7},
keywords = {software test, mutation score, machine learning},
location = {Austin, Texas},
series = {AST '16}
}

@article{10.1145/3529318,
author = {Ben Braiek, Houssem and Khomh, Foutse},
title = {Testing Feedforward Neural Networks Training Programs},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3529318},
doi = {10.1145/3529318},
abstract = {At present, we are witnessing an increasing effort to improve the performance and trustworthiness of Deep Neural Networks (DNNs), with the aim to enable their adoption in safety critical systems such as self-driving cars or aircraft collision-avoidance systems. Multiple testing techniques are proposed to generate test cases that can expose inconsistencies in the behavior of DNN models. These techniques assume implicitly that the training program is bug-free and appropriately configured. However, satisfying this assumption for a novel problem requires significant engineering work to prepare the data, design the DNN, implement the training program, and tune the hyperparameters to produce the model for which current automated test data generators search for corner-case behaviors. All these model training steps can be error prone. Therefore, it is crucial to detect and correct errors throughout all the engineering steps of DNN-based software systems and not only on the resulting DNN model. In this article, we gather a catalog of training issues and based on their symptoms and their effects on the behavior of the training program, we propose practical verification routines to detect the aforementioned issues, automatically, by continuously validating that some important properties of the learning dynamics hold during the training. Then, we design TheDeepChecker, an end-to-end property-based debugging approach for DNN training programs and implement it as a TensorFlow-based library. As an empirical evaluation, we conduct a case study to assess the effectiveness of TheDeepChecker on synthetic and real-world buggy DL programs and compare its performance to that of the Amazon SageMaker Debugger (SMD). Results show that TheDeepChecker’s on-execution validation of DNN-based program’s properties through three sequential phases (pre-, on-, and post-fitting) succeeds in revealing several coding bugs and system misconfigurations errors early on and at a low cost. Moreover, our property-based approach outperforms the SMD’s offline rules verification on training logs in terms of detection accuracy for unstable learning issues and coverage of additional DL bugs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {105},
numpages = {61},
keywords = {property-based debugging, training programs, Neural networks}
}

@inproceedings{10.1109/ASE56229.2023.00029,
author = {Li, Yi and Wang, Shaohua and Nguyen, Tien N.},
title = {Contextuality of Code Representation Learning},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00029},
doi = {10.1109/ASE56229.2023.00029},
abstract = {Advanced machine learning models (ML) have been successfully leveraged in several software engineering (SE) applications. The existing SE techniques have used the embedding models ranging from static to contextualized ones to build the vectors for program units. The contextualized vectors address a phenomenon in natural language texts called polysemy, which is the coexistence of different meanings of a word/phrase. However, due to different nature, program units exhibit the nature of mixed polysemy. Some code tokens and statements exhibit polysemy while other tokens (e.g., keywords, separators, and operators) and statements maintain the same meaning in different contexts. A natural question is whether static or contextualized embeddings fit better with the nature of mixed polysemy in source code. The answer to this question is helpful for the SE researchers in selecting the right embedding model.We conducted experiments on 12 popular sequence-/tree-/graph-based embedding models and on the samples of a dataset of 10,222 Java projects with +14M methods. We present several contextuality evaluation metrics adapted from natural-language texts to code structures to evaluate the embeddings from those models. Among several findings, we found that the models with higher contextuality help a bug detection model perform better than the static ones. Neither static nor contextualized embedding models fit well with the mixed polysemy nature of source code. Thus, we develop HyCode, a hybrid embedding model that fits better with the nature of mixed polysemy in source code.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {548–559},
numpages = {12},
keywords = {code representation learning, contextualized embedding, contextuality of code embedding},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3287921.3287958,
author = {Van Thuy, Hoang and Anh, Phan Viet and Hoai, Nguyen Xuan},
title = {Automated Large Program Repair based on Big Code},
year = {2018},
isbn = {9781450365390},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287921.3287958},
doi = {10.1145/3287921.3287958},
abstract = {The task of automatic program repair is to automatically localize and generate the correct patches for the bugs. A prominent approach is to produce a space of candidate patches, then find and validate candidates on test case sets. However, searching for the correct candidates is really challenging, since the search space is dominated by incorrect patches and its size is huge.This paper presents several methods to improve the automated program repair system Prophet, called Prophet+. Our approach contributes three improvements over Prophet: 1) extract twelve relations of statements and blocks for Bi-gram model using Big code, 2) prune the search space, 3) develop an algorithm to re-rank candidate patches in the search space. The experimental results show that our proposed system enhances the performance of Prophet, recognized as the state-of-the-art system, significantly. Specifically, for the top 1, our system generates the correct patches for 17 over 69 bugs while the number achieved by Prophet is 15.},
booktitle = {Proceedings of the 9th International Symposium on Information and Communication Technology},
pages = {375–381},
numpages = {7},
keywords = {N-gram, Machine Learning, Bigcode, Automated Program Repair},
location = {Danang City, Viet Nam},
series = {SoICT '18}
}

@inproceedings{10.1145/3611643.3616356,
author = {Du, Xiaohu and Wen, Ming and Wei, Zichao and Wang, Shangwen and Jin, Hai},
title = {An Extensive Study on Adversarial Attack against Pre-trained Models of Code},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616356},
doi = {10.1145/3611643.3616356},
abstract = {Transformer-based pre-trained models of code (PTMC) have been widely utilized and have achieved state-of-the-art performance in many mission-critical applications. However, they can be vulnerable to adversarial attacks through identifier substitution or coding style transformation, which can significantly degrade accuracy and may further incur security concerns. Although several approaches have been proposed to generate adversarial examples for PTMC, the effectiveness and efficiency of such approaches, especially on different code intelligence tasks, has not been well understood. To bridge this gap, this study systematically analyzes five state-of-the-art adversarial attack approaches from three perspectives: effectiveness, efficiency, and the quality of generated examples. The results show that none of the five approaches balances all these perspectives. Particularly, approaches with a high attack success rate tend to be time-consuming; the adversarial code they generate often lack naturalness, and vice versa. To address this limitation, we explore the impact of perturbing identifiers under different contexts and find that identifier substitution within for and if statements is the most effective. Based on these findings, we propose a new approach that prioritizes different types of statements for various tasks and further utilizes beam search to generate adversarial examples. Evaluation results show that it outperforms the state-of-the-art ALERT in terms of both effectiveness and efficiency while preserving the naturalness of the generated adversarial examples.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {489–501},
numpages = {13},
keywords = {Adversarial Attack, Deep Learning, Pre-Trained Model},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3427228.3427269,
author = {Das, Sanjeev and James, Kedrian and Werner, Jan and Antonakakis, Manos and Polychronakis, Michalis and Monrose, Fabian},
title = {A Flexible Framework for Expediting Bug Finding by Leveraging Past (Mis-)Behavior to Discover New Bugs},
year = {2020},
isbn = {9781450388580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427228.3427269},
doi = {10.1145/3427228.3427269},
abstract = {Among various fuzzing approaches, coverage-guided grey-box fuzzing is perhaps the most prominent, due to its ease of use and effectiveness. Using this approach, the selection of inputs focuses on maximizing program coverage, e.g., in terms of the different branches that have been traversed. In this work, we begin with the observation that selecting any input that explores a new path, and giving equal weight to all paths, can lead to severe inefficiencies. For instance, although seemingly “new” crashes involving previously unexplored paths may be discovered, these often have the same root cause and actually correspond to the same bug. To address these inefficiencies, we introduce a framework that incorporates a tighter feedback loop to guide the fuzzing process in exploring truly diverse code paths. Our framework employs (i) a vulnerability-aware selection of coverage metrics for enhancing the effectiveness of code exploration, (ii) crash deduplication information for early feedback, and (iii) a configurable input culling strategy that interleaves multiple strategies to achieve comprehensiveness. A novel aspect of our work is the use of hardware performance counters to derive coverage metrics. We present an approach for assessing and selecting the hardware events that can be used as a meaningful coverage metric for a target program. The results of our empirical evaluation using real-world programs demonstrate the effectiveness of our approach: in some cases, we explore fewer than 50% of the paths compared to a base fuzzer (AFL, MOpt, and Fairfuzz), yet on average, we improve new bug discovery by 31%, and find the same bugs (as the base) 3.3 times faster. Moreover, although we specifically chose applications that have been subject to recent fuzzing campaigns, we still discovered 9 new vulnerabilities.},
booktitle = {Proceedings of the 36th Annual Computer Security Applications Conference},
pages = {345–359},
numpages = {15},
keywords = {Machine Learning, Hardware Performance Counters, Fuzzing},
location = {Austin, USA},
series = {ACSAC '20}
}

@inproceedings{10.1109/ASE56229.2023.00052,
author = {Xiao, Mingxuan and Xiao, Yan and Dong, Hai and Ji, Shunhui and Zhang, Pengcheng},
title = {LEAP: Efficient and Automated Test Method for NLP Software},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00052},
doi = {10.1109/ASE56229.2023.00052},
abstract = {The widespread adoption of DNNs in NLP software has highlighted the need for robustness. Researchers proposed various automatic testing techniques for adversarial test cases. However, existing methods suffer from two limitations: weak error-discovering capabilities, with success rates ranging from 0% to 24.6% for BERT-based NLP software, and time inefficiency, taking 177.8s to 205.28s per test case, making them challenging for time-constrained scenarios.To address these issues, this paper proposes LEAP, an automated test method that uses LEvy flight-based Adaptive Particle swarm optimization integrated with textual features to generate adversarial test cases. Specifically, we adopt Levy flight for population initialization to increase the diversity of generated test cases. We also design an inertial weight adaptive update operator to improve the efficiency of LEAP's global optimization of high-dimensional text examples and a mutation operator based on the greedy strategy to reduce the search time.We conducted a series of experiments to validate LEAP's ability to test NLP software and found that the average success rate of LEAP in generating adversarial test cases is 79.1%, which is 6.1% higher than the next best approach (PSOattack). While ensuring high success rates, LEAP significantly reduces time overhead by up to 147.6s compared to other heuristic-based methods. Additionally, the experimental results demonstrate that LEAP can generate more transferable test cases and significantly enhance the robustness of DNN-based systems.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1136–1148},
numpages = {13},
keywords = {NLP software testing, particle swarm optimization},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3510454.3522680,
author = {Pontillo, Valeria},
title = {Static test flakiness prediction},
year = {2022},
isbn = {9781450392235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510454.3522680},
doi = {10.1145/3510454.3522680},
abstract = {The problem of flakiness occurs when a test case is non-deterministic and exhibits both a passing and failing behavior when run against the same code. Over the last years, the software engineering research community has been working toward defining approaches for detecting and addressing test flakiness, but most of these approaches suffer from scalability issues. Recently, this limitation has been targeted through machine learning solutions that could predict flaky tests using various features, both static and dynamic. Unfortunately, the proposed solutions involve features that could be costly to compute. In this paper, I perform a step forward and predict test flakiness only using statically computable metrics. I conducted an experiment on 18 Java projects coming from the FlakeFlagger dataset. First, I statistically assess the differences between flaky and non-flaky tests in terms of 25 static metrics in an individual and combined way. Then, I experimented with a machine learning approach that predicts flakiness based on the previously evaluated factors. The results show that static features can be used to characterize flaky tests: this is especially true for metrics and smells connected to source code complexity. In addition, this new static approach has performance comparable to the machine learning models already in the literature in terms of F-Measure.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings},
pages = {325–327},
numpages = {3},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@article{10.1145/2421648.2421653,
author = {Basak, Jayanta and Wadhwani, Kushal and Voruganti, Kaladhar and Narayanamurthy, Srinivasan and Mathur, Vipul and Nandi, Siddhartha},
title = {Model building for dynamic multi-tenant provider environments},
year = {2012},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {3},
issn = {0163-5980},
url = {https://doi.org/10.1145/2421648.2421653},
doi = {10.1145/2421648.2421653},
abstract = {Increasingly, storage vendors are finding it difficult to leverage existing white-box and black-box modeling techniques to build robust system models that can predict system behavior in the emerging dynamic and multi-tenant data centers. White-box models are becoming brittle because the model builders are not able to keep up with the innovations in the storage system stack, and black-box models are becoming brittle because it is increasingly difficult to a priori train the model for the dynamic and multi-tenant data center environment. Thus, there is a need for innovation in system model building area.In this paper we present a machine learning based blackbox modeling algorithm called M-LISP that can predict system behavior in untrained region for these emerging multitenant and dynamic data center environments. We have implemented and analyzed M-LISP in real environments and the initial results look very promising. We also provide a survey of some common machine learning algorithms and how they fare with respect to satisfying the modeling needs of the new data center environments.},
journal = {SIGOPS Oper. Syst. Rev.},
month = dec,
pages = {20–31},
numpages = {12},
keywords = {storage management, resource modeling, machine learning, black-box}
}

@inproceedings{10.1145/3194718.3194730,
author = {Sarro, Federica},
title = {Predictive analytics for software testing: keynote paper},
year = {2018},
isbn = {9781450357418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194718.3194730},
doi = {10.1145/3194718.3194730},
abstract = {This keynote discusses the use of Predictive Analytics for Software Engineering, and in particular for Software Defect Prediction and Software Testing, by presenting the latest results achieved in these fields leveraging Artificial Intelligence, Search-based and Machine Learning methods, and by giving some directions for future work.},
booktitle = {Proceedings of the 11th International Workshop on Search-Based Software Testing},
pages = {1},
numpages = {1},
keywords = {search-based predictive modelling, predictive analytics},
location = {Gothenburg, Sweden},
series = {SBST '18}
}

@article{10.1145/3660804,
author = {Yang, Haoran and Nong, Yu and Zhang, Tao and Luo, Xiapu and Cai, Haipeng},
title = {Learning to Detect and Localize Multilingual Bugs},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660804},
doi = {10.1145/3660804},
abstract = {Increasing studies have shown bugs in multi-language software as a critical loophole in modern software quality assurance, especially those induced by language interactions (i.e., multilingual bugs). Yet existing tool support for bug detection/localization remains largely limited to single-language software, despite the long-standing prevalence of multi-language systems in various real-world software domains. Extant static/dynamic analysis and deep learning (DL) based approaches all face major challenges in addressing multilingual bugs. In this paper, we present xLoc, a DL-based technique/tool for detecting and localizing multilingual bugs. Motivated by results of our bug-characteristics study on top locations of multilingual bugs, xLoc first learns the general knowledge relevant to differentiating various multilingual control-flow structures. This is achieved by pre-training a Transformer model with customized position encoding against novel objectives. Then, xLoc learns task-specific knowledge for the task of multilingual bug detection/localization, through another new position encoding scheme (based on cross-language API vicinity) that allows for the model to attend particularly to control-flow constructs that bear most multilingual bugs during fine-tuning. We have implemented xLoc for Python-C software and curated a dataset of 3,770 buggy and 15,884 non-buggy Python-C samples, which enabled our extensive evaluation of xLoc against two state-of-the-art baselines: fine-tuned CodeT5 and zero-shot ChatGPT. Our results show that xLoc achieved 94.98% F1 and 87.24%@Top-1 accuracy, which are significantly (up to 162.88% and 511.75%) higher than the baselines. Ablation studies further confirmed significant contributions of each of the novel design elements in xLoc. With respective bug-location characteristics and labeled bug datasets for fine-tuning, our design may be applied to other language combinations beyond Python-C.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {97},
numpages = {24},
keywords = {Multi-language software, bug detection, fault localization, multilingual bugs}
}

@inproceedings{10.1145/3520312.3534869,
author = {Rabin, Md Rafiqul Islam and Hussain, Aftab and Alipour, Mohammad Amin},
title = {Syntax-guided program reduction for understanding neural code intelligence models},
year = {2022},
isbn = {9781450392730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520312.3534869},
doi = {10.1145/3520312.3534869},
abstract = {Neural code intelligence (CI) models are opaque black-boxes and offer little insight on the features they use in making predictions. This opacity may lead to distrust in their prediction and hamper their wider adoption in safety-critical applications. Recently, input program reduction techniques have been proposed to identify key features in the input programs to improve the transparency of CI models. However, this approach is syntax-unaware and does not consider the grammar of the programming language.  

In this paper, we apply a syntax-guided program reduction technique that considers the grammar of the input programs during reduction. Our experiments on multiple models across different types of input programs show that the syntax-guided program reduction technique is faster and provides smaller sets of key tokens in reduced programs. We also show that the key tokens could be used in generating adversarial examples for up to 65% of the input programs.},
booktitle = {Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming},
pages = {70–79},
numpages = {10},
keywords = {Transparency, Program Reduction, Neural Models of Source Code, Interpretability, Feature Engineering},
location = {San Diego, CA, USA},
series = {MAPS 2022}
}

@inproceedings{10.1145/3650212.3680382,
author = {Mazouni, Quentin and Spieker, Helge and Gotlieb, Arnaud and Acher, Mathieu},
title = {Policy Testing with MDPFuzz (Replicability Study)},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680382},
doi = {10.1145/3650212.3680382},
abstract = {In recent years, following tremendous achievements in Reinforcement Learning, a great deal of interest has been devoted to ML models for sequential decision-making. Together with these scientific breakthroughs/advances, research has been conducted to develop automated functional testing methods for finding faults in black-box Markov decision processes. Pang et al. (ISSTA 2022) presented a black-box fuzz testing framework called MDPFuzz. The method consists of a fuzzer whose main feature is to use Gaussian Mixture Models (GMMs) to compute coverage of the test inputs as the likelihood to have already observed their results. This guidance through coverage evaluation aims at favoring novelty during testing and fault discovery in the decision model.
 
 
 
Pang et al. evaluated their work with four use cases, by comparing the number of failures found after twelve-hour testing campaigns with or without the guidance of the GMMs (ablation study). In this paper, we verify some of the key findings of the original paper and explore the limits of MDPFuzz through reproduction and replication. We re-implemented the proposed methodology and evaluated our replication in a large-scale study that extends the original four use cases with three new ones. Furthermore, we compare MDPFuzz and its ablated counterpart with a random testing baseline. We also assess the effectiveness of coverage guidance for different parameters, something that has not been done in the original evaluation. Despite this parameter analysis and unlike Pang et al.’s original conclusions, we find that in most cases, the aforementioned ablated Fuzzer outperforms MDPFuzz, and conclude that the coverage model proposed does not lead to finding more faults.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1567–1578},
numpages = {12},
keywords = {Reinforcement Learning, Replicability, Software Testing},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@article{10.1145/3511701,
author = {Biagiola, Matteo and Tonella, Paolo},
title = {Testing the Plasticity of Reinforcement Learning-based Systems},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3511701},
doi = {10.1145/3511701},
abstract = {The dataset available for pre-release training of a machine-learning based system is often not representative of all possible execution contexts that the system will encounter in the field. Reinforcement Learning (RL) is a prominent approach among those that support continual learning, i.e., learning continually in the field, in the post-release phase. No study has so far investigated any method to test the plasticity of RL-based systems, i.e., their capability to adapt to an execution context that may deviate from the training one.We propose an approach to test the plasticity of RL-based systems. The output of our approach is a quantification of the adaptation and anti-regression capabilities of the system, obtained by computing the adaptation frontier of the system in a changed environment. We visualize such frontier as an adaptation/anti-regression heatmap in two dimensions, or as a clustered projection when more than two dimensions are involved. In this way, we provide developers with information on the amount of changes that can be accommodated by the continual learning component of the system, which is key to decide if online, in-the-field learning can be safely enabled or not.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {80},
numpages = {46},
keywords = {empirical software engineering, reinforcement learning, Software testing}
}

@article{10.1145/3702971,
author = {Zem\'{\i}n, Luciano and Godio, Ariel and Cornejo, C\'{e}sar and Degiovanni, Renzo and Guti\'{e}rrez Brida, Sim\'{o}n and Regis, Germ\'{a}n and Aguirre, Nazareno and Frias, Marcelo Fabi\'{a}n},
title = {An Empirical Study on the Suitability of Test-based Patch Acceptance Criteria},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3702971},
doi = {10.1145/3702971},
abstract = {In this article, we empirically study the suitability of tests as acceptance criteria for automated program fixes, by checking patches produced by automated repair tools using a bug-finding tool, as opposed to previous works that used tests or manual inspections. We develop a number of experiments in which faulty programs from IntroClass, a known benchmark for program repair techniques, are fed to the program repair tools GenProg, Angelix, AutoFix, and Nopol, using test suites of varying quality, including those accompanying the benchmark. We then check the produced patches against formal specifications using a bug-finding tool. Our results show that, in the studied scenarios, automated program repair tools are significantly more likely to accept a spurious program fix than producing an actual one. Using bounded-exhaustive suites larger than the originally given ones (with about 100 and 1,000 tests) we verify that overfitting is reduced but (a) few new correct repairs are generated and (b) some tools see their performance reduced by the larger suites and fewer correct repairs are produced. Finally, by comparing with previous work, we show that overfitting is underestimated in semantics-based tools and that patches not discarded using held-out tests may be discarded using a bug-finding tool.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {57},
numpages = {20},
keywords = {automatic program repair, formal specifications, testing, oracle}
}

@inproceedings{10.1145/3551349.3556939,
author = {Feldmeier, Patric and Fraser, Gordon},
title = {Neuroevolution-Based Generation of Tests and Oracles for Games},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556939},
doi = {10.1145/3551349.3556939},
abstract = {Game-like programs have become increasingly popular in many software engineering domains such as mobile apps, web applications, or programming education. However, creating tests for programs that have the purpose of challenging human players is a daunting task for automatic test generators. Even if test generation succeeds in finding a relevant sequence of events to exercise a program, the randomized nature of games means that it may neither be possible to reproduce the exact program behavior underlying this sequence, nor to create test assertions checking if observed randomized game behavior is correct. To overcome these problems, we propose Neatest, a novel test generator based on the NeuroEvolution of Augmenting Topologies (NEAT) algorithm. Neatest systematically explores a program’s statements, and creates neural networks that operate the program in order to reliably reach each statement—that is, Neatest learns to play the game in a way to reliably cover different parts of the code. As the networks learn the actual game behavior, they can also serve as test oracles by evaluating how surprising the observed behavior of a program under test is compared to a supposedly correct version of the program. We evaluate this approach in the context of Scratch, an educational programming environment. Our empirical study on 25 non-trivial Scratch games demonstrates that our approach can successfully train neural networks that are not only far more resilient to random influences than traditional test suites consisting of static input sequences, but are also highly effective with an average mutation score of more than 65%.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {72},
numpages = {13},
keywords = {Scratch, Neuroevolution, Game Testing, Automated Testing},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3551349.3559549,
author = {Wang, Xin and Liu, Xiao and Zhou, Pingyi and Liu, Qixia and Liu, Jin and Wu, Hao and Cui, Xiaohui},
title = {Test-Driven Multi-Task Learning with Functionally Equivalent Code Transformation for Neural Code Generation},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3559549},
doi = {10.1145/3551349.3559549},
abstract = {Automated code generation is a longstanding challenge in both communities of software engineering and artificial intelligence. Currently, some works have started to investigate the functional correctness of code generation, where a code snippet is considered correct if it passes a set of test cases. However, most existing works still model code generation as text generation without considering program-specific information, such as functionally equivalent code snippets and test execution feedback. To address the above limitations, this paper proposes a method combining program analysis with deep learning for neural code generation, where functionally equivalent code snippets and test execution feedback will be considered at the training stage. Concretely, we firstly design several code transformation heuristics to produce different variants of the code snippet satisfying the same functionality. In addition, we employ the test execution feedback and design a test-driven discriminative task to train a novel discriminator, aiming to let the model distinguish whether the generated code is correct or not. The preliminary results on a newly published dataset demonstrate the effectiveness of our proposed framework for code generation. Particularly, in terms of the pass@1 metric, we achieve 8.81 and 11.53 gains compared with CodeGPT and CodeT5, respectively.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {188},
numpages = {6},
keywords = {Program Analysis, Neural Code Generation, Multi-Task Learning, Execution Feedback, Code Transformation},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1109/ASE56229.2023.00136,
author = {Li, Zhenhao and Chen, An Ran and Hu, Xing and Xia, Xin and Chen, Tse-Hsun (Peter) and Shang, Weiyi},
title = {Are They All Good? Studying Practitioners' Expectations on the Readability of Log Messages},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00136},
doi = {10.1109/ASE56229.2023.00136},
abstract = {Developers write logging statements to generate logs that provide run-time information for various tasks. The readability of log messages in the logging statements (i.e., the descriptive text) is rather crucial to the value of the generated logs. Immature log messages may slow down or even obstruct the process of log analysis. Despite the importance of log messages, there is still a lack of standards on what constitutes good readability of log messages and how to write them. In this paper, we conduct a series of interviews with 17 industrial practitioners to investigate their expectations on the readability of log messages. Through the interviews, we derive three aspects related to the readability of log messages, including Structure, Information, and Wording, along with several specific practices to improve each aspect. We validate our findings through a series of online questionnaire surveys and receive positive feedback from the participants. We then manually investigate the readability of log messages in large-scale open source systems and find that a large portion (38.1%) of the log messages have inadequate readability. Motivated by such observation, we further explore the potential of automatically classifying the readability of log messages using deep learning and machine learning models. We find that both deep learning and machine learning models can effectively classify the readability of log messages with a balanced accuracy above 80.0% on average. Our study provides comprehensive guidelines for composing log messages to further improve practitioners' logging practices.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {129–140},
numpages = {12},
keywords = {software logging, log messages, empirical study},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1109/ASE51524.2021.9678580,
author = {Tantithamthavorn, Chakkrit (Kla) and Jiarpakdee, Jirayus},
title = {Explainable AI for software engineering},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678580},
doi = {10.1109/ASE51524.2021.9678580},
abstract = {The success of software engineering projects largely depends on complex decision-making. For example, which tasks should a developer do first, who should perform this task, is the software of high quality, is a software system reliable and resilient enough to deploy, etc. However, erroneous decision-making for these complex questions is costly in terms of money and reputation. Thus, Artificial Intelligence/Machine Learning (AI/ML) techniques have been widely used in software engineering for developing software analytics tools and techniques to improve decision-making, developer productivity, and software quality. However, the predictions of such AI/ML models for software engineering are still not practical (i.e., coarse-grained), not explainable, and not actionable. These concerns often hinder the adoption of AI/ML models in software engineering practices. In addition, many recent studies still focus on improving the accuracy, while a few of them focus on improving explainability. Are we moving in the right direction? How can we better improve the SE community (both research and education)?In this tutorial, we first provide a concise yet essential introduction to the most important aspects of Explainable AI and a hands-on tutorial of Explainable AI tools and techniques. Then, we introduce the fundamental knowledge of defect prediction (an example application of AI for Software Engineering). Finally, we demonstrate three successful case studies on how Explainable AI techniques can be used to address the aforementioned challenges by making the predictions of software defect prediction models more practical, explainable, and actionable. The materials are available at https://xai4se.github.io.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1–2},
numpages = {2},
keywords = {software engineering, explainable AI},
location = {Melbourne, Australia},
series = {ASE '21}
}

@inproceedings{10.5555/3370272.3370296,
author = {Sabor, Korosh K. and Hamou-Lhadj, Abdelwahab and Trabelsi, Abdelaziz and Hassine, Jameleddine},
title = {Predicting bug report fields using stack traces and categorical attributes},
year = {2019},
publisher = {IBM Corp.},
address = {USA},
abstract = {Studies have shown that the lack of information about a bug often delays the bug report (BR) resolution process. Existing approaches rely mainly on BR descriptions as the main features for predicting BR fields. BR descriptions, however, tend to be informal and not always reliable. In this study, we show that the use of stack traces, a more formal source, and categorical features of BRs provides better accuracy than BR descriptions. We focus on the prediction of faulty components and products, two important BR fields, often used by developers to investigate a bug. Our method relies on mining historical BRs in order to predict faulty components and products of new incoming bugs. We map stack traces of historical BRs to feature vectors, weighted using TF-IDF. The vectors, together with a selected set of BR categorical information, are then fed to a classification algorithm. The method also tackles the problem of unbalanced data. Our approach achieves an average accuracy of 58% (when predicting faulty components) and 60% (when predicting faulty products) on Eclipse dataset and 70% (when predicting faulty components) and 70% (when predicting faulty products) on Gnome dataset. For both datasets, our approach improves over the method that uses BR descriptions by a large margin, up to an average of 46%.},
booktitle = {Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering},
pages = {224–233},
numpages = {10},
keywords = {software maintenance and evolution, software bugs reports, mining software repositories, machine learning},
location = {Toronto, Ontario, Canada},
series = {CASCON '19}
}

@inproceedings{10.1145/3551349.3563240,
author = {Pham, Phuoc and Nguyen, Vu and Nguyen, Tien},
title = {A Review of AI-augmented End-to-End Test Automation Tools},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3563240},
doi = {10.1145/3551349.3563240},
abstract = {Software testing is a process of evaluating and verifying whether a software product still works as expected, and it is repetitive, laborious, and time-consuming. To address this problem, automation tools have been developed to automate testing activities and enhance quality and delivery time. However, automation tools become less effective with continuous integration and continuous delivery (CI/CD) pipelines when the system under test is constantly changing. Recent advances in artificial intelligence and machine learning (AI/ML) present the potential for addressing important challenges in test automation. AI/ML can be applied to automate various testing activities such as detecting bugs and errors, maintaining existing test cases, or generating new test cases much faster than humans. In this study, we will outline testing activities where AI has significantly impacted and greatly enhanced the testing process. Based on that, we identify primary AI techniques that are used in each testing activity. Further, we conduct a comprehensive study of test automation tools to provide a clear look at the role of AI/ML technology in industrial testing tools. The results of this paper help researchers and practitioners understand the current state of AI/ML applied to software testing, which is the first important step towards achieving successful and efficient software testing.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {214},
numpages = {4},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3193977.3193978,
author = {Xu, Liming and Towey, Dave and French, Andrew P. and Benford, Steve and Zhou, Zhi Quan and Chen, Tsong Yueh},
title = {Enhancing supervised classifications with metamorphic relations},
year = {2018},
isbn = {9781450357296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3193977.3193978},
doi = {10.1145/3193977.3193978},
abstract = {We report on a novel use of metamorphic relations (MRs) in machine learning: instead of conducting metamorphic testing, we use MRs for the augmentation of the machine learning algorithms themselves. In particular, we report on how MRs can enable enhancements to an image classification problem of images containing hidden visual markers ("Artcodes").Working on an original classifier, and using the characteristics of two different categories of images, two MRs, based on separation and occlusion, were used to improve the performance of the classifier. Our experimental results show that the MR-augmented classifier achieves better performance than the original classifier, algorithms, and extending the use of MRs beyond the context of software testing.},
booktitle = {Proceedings of the 3rd International Workshop on Metamorphic Testing},
pages = {46–53},
numpages = {8},
keywords = {supervised classification, random forests, metamorphic testing, metamorphic relations, artcodes},
location = {Gothenburg, Sweden},
series = {MET '18}
}

@article{10.1145/3689779,
author = {Borgarelli, Andrea and Enea, Constantin and Majumdar, Rupak and Nagendra, Srinidhi},
title = {Reward Augmentation in Reinforcement Learning for Testing Distributed Systems},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689779},
doi = {10.1145/3689779},
abstract = {Bugs in popular distributed protocol implementations have been the source of many downtimes in popular internet services. We describe a randomized testing approach for distributed protocol implementations based on reinforcement learning. Since the natural reward structure is very sparse, the key to successful exploration in reinforcement learning is reward augmentation. We show two different techniques that build on one another. First, we provide a decaying exploration bonus based on the discovery of new states---the reward decays as the same state is visited multiple times. The exploration bonus captures the intuition from coverage-guided fuzzing of prioritizing new coverage points; in contrast to other schemes, we show that taking the maximum of the bonus and the Q-value leads to more effective exploration. Second, we provide waypoints to the algorithm as a sequence of predicates that capture interesting semantic scenarios. Waypoints exploit designer insight about the protocol and guide the exploration to "interesting" parts of the state space. Our reward structure ensures that new episodes can reliably get to deep interesting states even without execution caching. We have implemented our algorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and RSL) shows that our algorithm can significantly outperform baseline approaches in terms of coverage and bug finding.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {339},
numpages = {27},
keywords = {Distributed Systems, Reactive Systems Testing, Reinforcement Learning}
}

@inproceedings{10.1145/3629527.3651841,
author = {Panahandeh, Mahsa and Ezzati-Jivan, Naser and Hamou-Lhadj, Abdelwahab and Miller, James},
title = {Efficient Unsupervised Latency Culprit Ranking in Distributed Traces with GNN and Critical Path Analysis},
year = {2024},
isbn = {9798400704451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629527.3651841},
doi = {10.1145/3629527.3651841},
abstract = {Microservices offer the benefits of scalable flexibility and rapid deployment, making them a preferred architecture in today's IT industry. However, their dynamic nature increases their susceptibility to failures, highlighting the need for effective troubleshooting strategies. Current methods for pinpointing issues in microservices often depend on impractical supervision or rest on unrealistic assumptions. We propose a novel approach using graph unsupervised neural networks and critical path analysis to address these limitations. Our experiments on four open-source microservice benchmarks show significant results, with top-1 accuracy ranging from 86.4% to 96%, over 6% enhancement compared to existing methods. Moreover, our approach reduces training time by 5.6 times compared to similar works on the same datasets.},
booktitle = {Companion of the 15th ACM/SPEC International Conference on Performance Engineering},
pages = {62–66},
numpages = {5},
keywords = {critical path analysis, culprit identification, distributed traces, firm dataset, graph neural network},
location = {London, United Kingdom},
series = {ICPE '24 Companion}
}

@inproceedings{10.1145/2884781.2884804,
author = {Wang, Song and Liu, Taiyue and Tan, Lin},
title = {Automatically learning semantic features for defect prediction},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884804},
doi = {10.1145/2884781.2884804},
abstract = {Software defect prediction, which predicts defective code regions, can help developers find bugs and prioritize their testing efforts. To build accurate prediction models, previous studies focus on manually designing features that encode the characteristics of programs and exploring different machine learning algorithms. Existing traditional features often fail to capture the semantic differences of programs, and such a capability is needed for building accurate prediction models.To bridge the gap between programs' semantics and defect prediction features, this paper proposes to leverage a powerful representation-learning algorithm, deep learning, to learn semantic representation of programs automatically from source code. Specifically, we leverage Deep Belief Network (DBN) to automatically learn semantic features from token vectors extracted from programs' Abstract Syntax Trees (ASTs).Our evaluation on ten open source projects shows that our automatically learned semantic features significantly improve both within-project defect prediction (WPDP) and cross-project defect prediction (CPDP) compared to traditional features. Our semantic features improve WPDP on average by 14.7% in precision, 11.5% in recall, and 14.2% in F1. For CPDP, our semantic features based approach outperforms the state-of-the-art technique TCA+ with traditional features by 8.9% in F1.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {297–308},
numpages = {12},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1109/ICSE.2019.00054,
author = {Philip, Adithya Abraham and Bhagwan, Ranjita and Kumar, Rahul and Maddila, Chandra Sekhar and Nagappan, Nachiappan},
title = {FastLane: test minimization for rapidly deployed large-scale online services},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00054},
doi = {10.1109/ICSE.2019.00054},
abstract = {Today, we depend on numerous large-scale services for basic operations such as email. These services, built on the basis of Continuous Integration/Continuous Deployment (CI/CD) processes, are extremely dynamic: developers continuously commit code and introduce new features, functionality and fixes. Hundreds of commits may enter the code-base in a single day. Therefore one of the most time-critical, yet resource-intensive tasks towards ensuring code-quality is effectively testing such large code-bases.This paper presents FastLane, a system that performs data-driven test minimization. FastLane uses light-weight machine-learning models built upon a rich history of test and commit logs to predict test outcomes. Tests for which we predict outcomes need not be explicitly run, thereby saving us precious test-time and resources. Our evaluation on a large-scale email and collaboration platform service shows that our techniques can save 18.04%, i.e., almost a fifth of test-time while obtaining a test outcome accuracy of 99.99%.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {408–418},
numpages = {11},
keywords = {test prioritization, machine learning, commit risk},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1145/3718083,
author = {Jiang, Yanjie and Liu, Hui and Liu, Jinyan and Zhang, Yuxia and Ji, Weixing and Zhong, Hao and Zhang, Lu},
title = {An Empirical Study on the Relationship Between Defects and Source Code’s Unnaturalness},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3718083},
doi = {10.1145/3718083},
abstract = {Natural languages are “natural” in that texts in natural languages are repetitive and predictable. Recent research indicates that programming languages share similar characteristics (naturalness), with source code displaying patterns of repetition and predictability. Notably, studies have shown that buggy code deviates from these natural patterns in that buggy code is significantly less natural than bug-free one. In this paper, we conduct a large-scale and extensive empirical study to investigate whether code defects lead to unnaturalness of source code. Different from existing studies, we leverage multiple large-scale and high-quality bug repositories where bug-irrelevant changes in bug-fixing commits have been explicitly excluded. The leveraged software applications cover different programming languages, and the empirical study involves real-world software defects as well as defects injected automatically with well-known mutation operators. On one side, our evaluation results confirm existing studies in that buggy source code lines are often less natural than bug-free ones. On the other side, our evaluation reveals some interesting new findings. First, fixing bugs does not significantly improve the naturalness of code lines and the fixed lines on average are as unnatural as buggy ones. This finding may suggest that software defects are not the root causes of source code’s unnaturalness although there does existing statistically significant correlation between software defects and source code’s naturalness. Second, defects in different programming languages have similar effect on source code’s naturalness. The conclusions (i.e., buggy code is less natural but fixing the bugs cannot improve source code’s naturalness) hold regardless of the programming languages. Third, injecting defects automatically by well-known mutation operators does not significantly reduce the naturalness of involved source code lines. This suggests that automatically injected defects may have a similar impact on the naturalness of source code as real-world defects inadvertently introduced by developers. Fourth, the detects’ impact on source code’s naturalness varies slightly among different categories of software defects. Although fixing bugs on average does not significantly improve the naturalness of involved source code, fixing ”checking” related bugs does significantly improve the naturalness of source code. Finally, locating buggy code lines according to naturalness alone is inaccurate, resulting in extremely low precision (less than one percent).},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
keywords = {Code Entropy, Bugs, Source Code, Bug Fixing, Naturalness}
}

@inproceedings{10.1109/ESEM.2017.19,
author = {Fan, Qiang and Yu, Yue and Yin, Gang and Wang, Tao and Wang, Huaimin},
title = {Where is the road for issue reports classification based on text mining?},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.19},
doi = {10.1109/ESEM.2017.19},
abstract = {Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt.In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {121–130},
numpages = {10},
keywords = {mining software repositories, machine learning technique, issue tracking system},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@inproceedings{10.1145/3477314.3507059,
author = {Henriksen, Patrick and Leofante, Francesco and Lomuscio, Alessio},
title = {Repairing misclassifications in neural networks using limited data},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3507059},
doi = {10.1145/3477314.3507059},
abstract = {We present a novel and computationally efficient method for repairing a feed-forward neural network with respect to a finite set of inputs that are misclassified. The method assumes no access to the training set. We present a formal characterisation for repairing the neural network and study its resulting properties in terms of soundness and minimality. We introduce a gradient-based algorithm that performs localised modifications to the network's weights such that misclassifications are repaired while marginally affecting network accuracy on correctly classified inputs. We introduce an implementation, I-REPAIR, and show it is able to repair neural networks while reducing accuracy drops by up to 90% when compared to other state-of-the-art approaches for repair.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {1031–1038},
numpages = {8},
keywords = {deep neural networks, model repair, safe AI},
location = {Virtual Event},
series = {SAC '22}
}

@article{10.1145/3375572.3375582,
author = {Fontana, Francesca Arcelli and Perrouin, Gilles and Ampatzoglou, Apostolos and Archer, Mathieu and Walter, Bartosz and Cordy, Maxime and Palomba, Fabio and Devroey, Xavier},
title = {MALTESQUE 2019 Workshop Summary},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3375572.3375582},
doi = {10.1145/3375572.3375582},
abstract = {Welcome to the third edition of the workshop on Machine Learn- ing Techniques for Software Quality Evaluation (MaLTeSQuE 2019), held in Tallinn, Estonia, August 27th, 2019, co-located with ESEC / FSE 2019. This year MALTESQUE merged with the MASES (Machine Learning and Software Engineering in Symbiosis) work- shop, co-located with the ASE 2018 conference. Ten papers from all over the world were submitted, seven of them were accepted. The program also featured a keynote by Lionel Briand on the use of machine learning to improve software testing.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {34–35},
numpages = {2}
}

@inproceedings{10.1145/3460319.3464801,
author = {Dunn, Isaac and Pouget, Hadrien and Kroening, Daniel and Melham, Tom},
title = {Exposing previously undetectable faults in deep neural networks},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464801},
doi = {10.1145/3460319.3464801},
abstract = {Existing methods for testing DNNs solve the oracle problem by constraining the raw features (e.g. image pixel values) to be within a small distance of a dataset example for which the desired DNN output is known. But this limits the kinds of faults these approaches are able to detect. In this paper, we introduce a novel DNN testing method that is able to find faults in DNNs that other methods cannot. The crux is that, by leveraging generative machine learning, we can generate fresh test inputs that vary in their high-level features (for images, these include object shape, location, texture, and colour). We demonstrate that our approach is capable of detecting deliberately injected faults as well as new faults in state-of-the-art DNNs, and that in both cases, existing methods are unable to find these faults.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {56–66},
numpages = {11},
keywords = {Robustness, Generative Adversarial Networks, Deep Learning},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@proceedings{10.1145/3617555,
title = {PROMISE 2023: Proceedings of the 19th International Conference on Predictive Models and Data Analytics in Software Engineering},
year = {2023},
isbn = {9798400703751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our pleasure to welcome you to the 19th ACM International Conference on Predictive Models and Data Analytics in Software Engineering (PROMISE 2023), to be held in presence on December 8th, 2023, co-located with the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2023).},
location = {San Francisco, CA, USA}
}

@inproceedings{10.1145/3510454.3517063,
author = {Grishina, Anastasiia},
title = {Enabling automatic repair of source code vulnerabilities using data-driven methods},
year = {2022},
isbn = {9781450392235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510454.3517063},
doi = {10.1145/3510454.3517063},
abstract = {Users around the world rely on software-intensive systems in their day-to-day activities. These systems regularly contain bugs and security vulnerabilities. To facilitate bug fixing, data-driven models of automatic program repair use pairs of buggy and fixed code to learn transformations that fix errors in code. However, automatic repair of security vulnerabilities remains under-explored. In this work, we propose ways to improve code representations for vulnerability repair from three perspectives: input data type, data-driven models, and downstream tasks. The expected results of this work are improved code representations for automatic program repair and, specifically, fixing security vulnerabilities.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings},
pages = {275–277},
numpages = {3},
keywords = {static analysis, software security, natural language processing, graph-based machine learning, automatic program repair, ML4Code},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@article{10.1145/3715105,
author = {Shi, Jieke and Yang, Zhou and He, Junda and Xu, Bowen and Kim, Dongsun and Han, DongGyun and Lo, David},
title = {Finding Safety Violations of AI-Enabled Control Systems through the Lens of Synthesized Proxy Programs},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715105},
doi = {10.1145/3715105},
abstract = {Given the increasing adoption of modern AI-enabled control systems, ensuring their safety and reliability has become a critical task in software testing. One prevalent approach to testing control systems is falsification, which aims to find an input signal that causes the control system to violate a formal safety specification using optimization algorithms. However, applying falsification to AI-enabled control systems poses two significant challenges: (1)&nbsp;it requires the system to execute numerous candidate test inputs, which can be time-consuming, particularly for systems with AI models that have many parameters, and (2)&nbsp;multiple safety requirements are typically defined as a conjunctive specification, which is difficult for existing falsification approaches to comprehensively cover.This paper introduces Synthify, a falsification framework tailored for AI-enabled control systems, i.e., control systems equipped with AI controllers. Our approach performs falsification in a two-phase process. At the start, Synthify synthesizes a program that implements one or a few linear controllers to serve as a proxy for the AI controller. This proxy program mimics the AI controller's functionality but is computationally more efficient. Then, Synthify employs the  (epsilon) -greedy strategy to sample a promising sub-specification from the conjunctive safety specification. It then uses a Simulated Annealing-based falsification algorithm to find violations of the sampled sub-specification for the control system. To evaluate Synthify, we compare it to PSY-TaLiRo, a state-of-the-art and industrial-strength falsification tool, on 8 publicly available control systems. On average, Synthify achieves a 83.5% higher success rate in falsification compared to PSY-TaLiRo with the same budget of falsification trials. Additionally, our method is 12.8 (times)  faster in finding a single safety violation than the baseline. The safety violations found by Synthify are also more diverse than those found by PSY-TaLiRo, covering 137.7% more sub-specifications.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Falsification, Search-based Testing, AI-enabled Control Systems, Program Synthesis}
}

@article{10.1145/3505243,
author = {Yang, Yanming and Xia, Xin and Lo, David and Grundy, John},
title = {A Survey on Deep Learning for Software Engineering},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {10s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3505243},
doi = {10.1145/3505243},
abstract = {In 2006, Geoffrey Hinton proposed the concept of training “Deep Neural Networks (DNNs)” and an improved model training method to break the bottleneck of neural network development. More recently, the introduction of AlphaGo in 2016 demonstrated the powerful learning ability of deep learning and its enormous potential. Deep learning has been increasingly used to develop state-of-the-art software engineering (SE) research tools due to its ability to boost performance for various SE tasks. There are many factors, e.g., deep learning model selection, internal structure differences, and model optimization techniques, that may have an impact on the performance of DNNs applied in SE. Few works to date focus on summarizing, classifying, and analyzing the application of deep learning techniques in SE. To fill this gap, we performed a survey to analyze the relevant studies published since 2006. We first provide an example to illustrate how deep learning techniques are used in SE. We then conduct a background analysis (BA) of primary studies and present four research questions to describe the trend of DNNs used in SE (BA), summarize and classify different deep learning techniques (RQ1), and analyze the data processing including data collection, data classification, data pre-processing, and data representation (RQ2). In RQ3, we depicted a range of key research topics using DNNs and investigated the relationships between DL-based model adoption and multiple factors (i.e., DL architectures, task types, problem types, and data types). We also summarized commonly used datasets for different SE tasks. In RQ4, we summarized the widely used optimization algorithms and provided important evaluation metrics for different problem types, including regression, classification, recommendation, and generation. Based on our findings, we present a set of current challenges remaining to be investigated and outline a proposed research road map highlighting key opportunities for future work.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {206},
numpages = {73},
keywords = {survey, software engineering, machine learning, neural network, Deep learning}
}

@inproceedings{10.1145/3183440.3183487,
author = {Saha, Ripon K. and Yoshida, Hiroaki and Prasad, Mukul R. and Tokumoto, Susumu and Takayama, Kuniharu and Nanba, Isao},
title = {Elixir: an automated repair tool for Java programs},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3183487},
doi = {10.1145/3183440.3183487},
abstract = {Object-oriented (OO) languages, by design, make heavy use of method invocations (MI). Unsurprisingly, a large fraction of OO-program bug patches also involve method invocations. However, current program repair techniques incorporate MIs in very limited ways, ostensibly to avoid searching the huge repair space that method invocations afford. To address this challenge, in previous work, we proposed a generate-and-validate repair technique which can effectively synthesize patches from a repair space rich in method invocation expressions, by using a machine-learned model to rank the space of concrete repairs. In this paper we describe the tool Elixir that instantiates this technique for the repair of Java programs. We describe the architecture, user-interface, and salient features of Elixir, and specific use-cases it can be applied in. We also report on our efforts towards practical deployment of Elixir within our organization, including the initial results of a trial of Elixir on a project of interest to potential customers. A video demonstrating Elixir is available at: https://elixir-tool.github.io/demo-video.html},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {77–80},
numpages = {4},
keywords = {OOP, automatic program repair, machine learning},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3524842.3528446,
author = {Croft, Roland and Babar, M. Ali and Chen, Huaming},
title = {Noisy label learning for security defects},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528446},
doi = {10.1145/3524842.3528446},
abstract = {Data-driven software engineering processes, such as vulnerability prediction heavily rely on the quality of the data used. In this paper, we observe that it is infeasible to obtain a noise-free security defect dataset in practice. Despite the vulnerable class, the non-vulnerable modules are difficult to be verified and determined as truly exploit free given the limited manual efforts available. It results in uncertainty, introduces labeling noise in the datasets and affects conclusion validity. To address this issue, we propose novel learning methods that are robust to label impurities and can leverage the most from limited label data; noisy label learning. We investigate various noisy label learning methods applied to software vulnerability prediction. Specifically, we propose a two-stage learning method based on noise cleaning to identify and remediate the noisy samples, which improves AUC and recall of baselines by up to 8.9% and 23.4%, respectively. Moreover, we discuss several hurdles in terms of achieving a performance upper bound with semi-omniscient knowledge of the label noise. Overall, the experimental results show that learning from noisy labels can be effective for data-driven software and security analytics.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {435–447},
numpages = {13},
keywords = {software vulnerabilities, noisy label learning, machine learning},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@article{10.1145/3720417,
author = {Zhang, Guanqin and Zhang, Zhenya and Bandara, H.M.N. Dilum and Chen, Shiping and Zhao, Jianjun and Sui, Yulei},
title = {Efficient Incremental Verification of Neural Networks Guided by Counterexample Potentiality},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3720417},
doi = {10.1145/3720417},
abstract = {Incremental verification is an emerging neural network verification approach that aims to accelerate the verification of a neural network N* by reusing the existing verification result (called a template) of a similar neural network N. To date, the state-of-the-art incremental verification approach leverages the problem splitting history produced by branch and bound (BaB in verification of N, to select only a part of the sub-problems for verification of N*, thus more efficient than verifying N* from scratch. While this approach identifies whether each sub-problem should be re-assessed, it neglects the information of how necessary each sub-problem should be re-assessed, in the sense that the sub-problems that are more likely to contain counterexamples should be prioritized, in order to terminate the verification process as soon as a counterexample is detected.                 To bridge this gap, we first define a counterexample potentiality order over different sub-problems based on the template, and then we propose Olive, an incremental verification approach that explores the sub-problems of verifying N* orderly guided by counterexample potentiality. Specifically, Olive has two variants, including Oliveg, a greedy strategy that always prefers to exploit the sub-problems that are more likely to contain counterexamples, and Oliveb, a balanced strategy that also explores the sub-problems that are less likely, in case the template is not sufficiently precise. We experimentally evaluate the efficiency of Olive on 1445 verification problem instances derived from 15 neural networks spanning over two datasets MNIST and CIFAR-10. Our evaluation demonstrates significant performance advantages of Olive over state-of-the-art classic verification and incremental approaches. In particular, Olive shows evident superiority on the problem instances that contain counterexamples, and performs as well as Ivan on the certified problem instances.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {83},
numpages = {28},
keywords = {branch and bound, counterexample potentiality, incremental verification, neural network verification}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00038,
author = {Murali, Vijayaraghavan and Yao, Edward and Mathur, Umang and Chandra, Satish},
title = {Scalable statistical root cause analysis on app telemetry},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00038},
doi = {10.1109/ICSE-SEIP52600.2021.00038},
abstract = {Despite engineering workflows that aim to prevent buggy code from being deployed, bugs still make their way into the Facebook app. When symptoms of these bugs, such as user submitted reports and automatically captured crashes, are reported, finding their root causes is an important step in resolving them. However, at Facebook's scale of billions of users, a single bug can manifest as several different symptoms according to the various user and execution environments in which the software is deployed. Root cause analysis (RCA) therefore requires tedious manual investigation and domain expertise to extract out common patterns that are observed in groups of reports and use them for debugging.We propose Minesweeper, a technique for RCA that moves towards automatically identifying the root cause of bugs from their symptoms. The method is based on two key aspects: (i) a scalable algorithm to efficiently mine patterns from telemetric information that is collected along with the reports, and (ii) statistical notions of precision and recall of patterns that help point towards root causes. We evaluate Minesweeper's scalability and effectiveness in finding root causes from symptoms on real world bug and crash reports from Facebook's apps. Our evaluation demonstrates that Minesweeper can perform RCA for tens of thousands of reports in less than 3 minutes, and is more than 85% accurate in identifying the root cause of regressions.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {288–297},
numpages = {10},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1145/259526.259548,
author = {Cheatham, Thomas J. and Yoo, Jungsoon P. and Wahl, Nancy J.},
title = {Software testing: a machine learning experiment},
year = {1995},
isbn = {0897917375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/259526.259548},
doi = {10.1145/259526.259548},
booktitle = {Proceedings of the 1995 ACM 23rd Annual Conference on Computer Science},
pages = {135–141},
numpages = {7},
location = {Nashville, Tennessee, USA},
series = {CSC '95}
}

@article{10.1145/3680464,
author = {Wang, Haipeng and Wei, Zhengyuan and Zhou, Qilin and Chan, Wing-Kwong},
title = {Context-Aware Fuzzing for Robustness Enhancement of Deep Learning Models},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3680464},
doi = {10.1145/3680464},
abstract = {In the testing-retraining pipeline for enhancing the robustness property of deep learning (DL) models, many state-of-the-art robustness-oriented fuzzing techniques are metric-oriented. The pipeline generates adversarial examples as test cases via such a DL testing technique and retrains the DL model under test with test suites that contain these test cases. On the one hand, the strategies of these fuzzing techniques tightly integrate the key characteristics of their testing metrics. On the other hand, they are often unaware of whether their generated test cases are different from the samples surrounding these test cases and whether there are relevant test cases of other seeds when generating the current one. We propose a novel testing metric called Contextual Confidence (CC). CC measures a test case through the surrounding samples of a test case in terms of their mean probability predicted to the prediction label of the test case. Based on this metric, we further propose a novel fuzzing technique Clover as a DL testing technique for the pipeline. In each fuzzing round, Clover first finds a set of seeds whose labels are the same as the label of the seed under fuzzing. At the same time, it locates the corresponding test case that achieves the highest CC values among the existing test cases of each seed in this set of seeds and shares the same prediction label as the existing test case of the seed under fuzzing that achieves the highest CC value. Clover computes the piece of difference between each such pair of a seed and a test case. It incrementally applies these pieces of differences to perturb the current test case of the seed under fuzzing that achieves the highest CC value and to perturb the resulting samples along the gradient to generate new test cases for the seed under fuzzing. Clover finally selects test cases among the generated test cases of all seeds as much as possible and with a preference to select test cases with higher CC values for improving model robustness. The experiments show that Clover outperforms the state-of-the-art coverage-based technique Adapt and loss-based fuzzing technique RobOT by 67%–129% and 48%–100% in terms of robustness improvement ratio, respectively, delivered through the same testing-retraining pipeline. For test case generation, in terms of numbers of unique adversarial labels and unique categories for the constructed test suites, Clover outperforms Adapt by  (2.0times)  and  (3.5times)  and RobOT by  (1.6times)  and  (1.7times)  on fuzzing clean models, and also outperforms Adapt by  (3.4times)  and  (4.5times)  and RobOT by  (9.8times)  and  (11.0times)  on fuzzing adversarially trained models, respectively.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {8},
numpages = {68},
keywords = {context-awareness, fuzzing algorithm, robustness, assessment, metric}
}

@inproceedings{10.1109/ASE56229.2023.00156,
author = {Hu, Shengyou and Wu, Huayao and Wang, Peng and Chang, Jing and Tu, Yongjun and Jiang, Xiu and Niu, Xintao and Nie, Changhai},
title = {ATOM: Automated Black-Box Testing of Multi-Label Image Classification Systems},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00156},
doi = {10.1109/ASE56229.2023.00156},
abstract = {Multi-label Image Classification Systems (MICSs) developed based on Deep Neural Networks (DNNs) are extensively used in people's daily life. Currently, although there are a variety of approaches to test DNN-based systems, they typically rely on the internals of DNNs to design test cases, and do not take the core specification of MICS (i.e., correctly recognizing multiple objects in a given image) into account. In this paper, we propose ATOM, an automated and systematic black-box testing framework for testing MICS. Specifically, ATOM exploits the label combination as the testing adequacy criteria, hoping to systematically examine the impact of correlations between a fixed number of labels on the classification ability of MICS. Then, ATOM leverages image search engine and natural language processing to find test images that are not only common to the real-world, but also relevant to target label combinations. Finally, ATOM combines metamorphic testing and label information to realize test oracle identification, based on which the ability of MICS in classifying different label combinations is evaluated. To evaluate the effectiveness of ATOM, we have performed experiments on two popular datasets of MICS, VOC and COCO (each with five state-of-the-art DNN models), and one real-world photo tagging application from our industrial partner. The experimental results reveal that the performance of current DNN-based MICSs remains less satisfactory even in recognizing correlations between only two labels, as ATOM triggers a total number of 6,049 such label combination related errors for all MICSs studied. In particular, ATOM reports 587 error-revealing images for the industrial MICS, in which 92% of them are confirmed by the developers.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {230–242},
numpages = {13},
keywords = {multi-label image classification testing, black-box testing, metamorphic testing},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1109/ICSE43902.2021.00109,
author = {Bui, Nghi D. Q. and Yu, Yijun and Jiang, Lingxiao},
title = {InferCode: Self-Supervised Learning of Code Representations by Predicting Subtrees},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00109},
doi = {10.1109/ICSE43902.2021.00109},
abstract = {Learning code representations has found many uses in software engineering, such as code classification, code search, comment generation, and bug prediction, etc. Although representations of code in tokens, syntax trees, dependency graphs, paths in trees, or the combinations of their variants have been proposed, existing learning techniques have a major limitation that these models are often trained on datasets labeled for specific downstream tasks, and as such the code representations may not be suitable for other tasks. Even though some techniques generate representations from unlabeled code, they are far from being satisfactory when applied to the downstream tasks. To overcome the limitation, this paper proposes InferCode, which adapts the self-supervised learning idea from natural language processing to the abstract syntax trees (ASTs) of code. The novelty lies in the training of code representations by predicting subtrees automatically identified from the contexts of ASTs. With InferCode, subtrees in ASTs are treated as the labels for training the code representations without any human labelling effort or the overhead of expensive graph construction, and the trained representations are no longer tied to any specific downstream tasks or code units.We have trained an instance of InferCode model using Tree-Based Convolutional Neural Network (TBCNN) as the encoder of a large set of Java code. This pre-trained model can then be applied to downstream unsupervised tasks such as code clustering, code clone detection, cross-language code search, or be reused under a transfer learning scheme to continue training the model weights for supervised tasks such as code classification and method name prediction. Compared to prior techniques applied to the same downstream tasks, such as code2vec, code2seq, ASTNN, using our pre-trained InferCode model higher performance is achieved with a significant margin for most of the tasks, including those involving different programming languages. The implementation of InferCode and the trained embeddings are available at the link: https://github.com/bdqnghi/infercode.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1186–1197},
numpages = {12},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3493612.3520471,
author = {Aljedaani, Wajdi and Mkaouer, Mohamed Wiem and Ludi, Stephanie and Ouni, Ali and Jenhani, Ilyes},
title = {On the identification of accessibility bug reports in open source systems},
year = {2022},
isbn = {9781450391702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493612.3520471},
doi = {10.1145/3493612.3520471},
abstract = {Today, mobile devices provide support to disabled people to make their life easier due to their high accessibility and capability, e.g., finding accessible locations, picture and voice-based communication, customized user interfaces and vocabulary levels. These accessibility frameworks are directly integrated, as libraries, in various apps, providing them with accessibility functions. Just like any other software, these frameworks regularly encounter errors. These errors are reported by app developers in the form of bug reports. These bug reports related to accessibility faults need to be urgently fixed since their existence significantly hinders the usability of apps. In this context, the manual inspection of a large number of bug reports to identify accessibility-related ones is time-consuming and error-prone. Prior research has investigated mobile app user reviews classification for various purposes, including bug reports identification, feature request identification, app performance optimization etc. Yet, none of the prior research has investigated the identification of accessibility-related bug reports, making their prioritization and timely correction difficult for software developers. To support developers with this manual process, the goal of this paper is to automatically detect, for a given bug report, whether it is about accessibility or not. Thus, we tackle the identification of accessibility bug reports as a binary classification problem. To build our model, we rely on an existing dataset of manually curated accessibility bug reports, extracted from popular open-source projects, namely Mozilla Firefox and Google Chromium. We design our solution to learn from these reports the appropriate discriminative features i.e., keywords that properly represent accessibility issues. Our trained model is evaluating using stratified cross-validation, and the findings show that our classifier achieves high F1-scores of 93%.},
booktitle = {Proceedings of the 19th International Web for All Conference},
articleno = {19},
numpages = {11},
keywords = {open source, machine learning, bug repository, bug report, accessibility},
location = {Lyon, France},
series = {W4A '22}
}

@article{10.1145/2853073.2853083,
author = {Rathore, Santosh Singh and Kumar, Sandeep},
title = {A Decision Tree Regression based Approach for the Number of Software Faults Prediction},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853083},
doi = {10.1145/2853073.2853083},
abstract = {Software fault prediction is an important activity to make software quality assurance (SQA) process more efficient, economic and targeted. Most of earlier works related to software fault prediction have focused on classifying software modules as faulty or non-faulty. However, prediction of the number of faults in a given software module is not adequately investigated. In this paper, we explore the capability of decision tree regression (DTR) for the number of faults prediction in two different scenarios, intra-release prediction and inter-releases prediction for the given software system. The experimental study is performed over five open-source software projects with their nineteen releases collected from the PROMISE data repository. The predictive accuracy of DTR is evaluated using absolute error and relative error, prediction at level l and goodness-of-t measure. The results show that decision tree regression produced significant prediction accuracy for the number of faults prediction in both the considered scenarios. The relative comparison of intra-release and inter-releases fault prediction shows that intra-project prediction produced better accuracy compared to inter-releases prediction across all the datasets},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–6},
numpages = {6},
keywords = {Software Fault Prediction, Number of Faults Prediction, Decision Tree Regression}
}

@article{10.1145/3726524,
author = {Alonso, Juan C. and Ernst, Michael D. and Segura, Sergio and Ruiz-Cort\'{e}s, Antonio},
title = {Test Oracle Generation for REST APIs},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3726524},
doi = {10.1145/3726524},
abstract = {The number and complexity of test case generation tools for REST APIs have significantly increased in recent years. These tools excel in automating input generation but are limited by their test oracles, which can only detect crashes, regressions, and violations of API specifications or design best practices. This article introduces AGORA+, an approach for generating test oracles for REST APIs through the detection of invariants—output properties that should always hold. AGORA+ learns the expected behavior of an API by analyzing API requests and their corresponding responses. We enhanced the Daikon tool for dynamic detection of likely invariants, adding new invariant types and creating a front-end called Beet. Beet translates any OpenAPI specification and a set of API requests and responses into Daikon inputs. AGORA+ can detect 106 different types of invariants in REST APIs. We also developed PostmanAssertify, which converts the invariants identified by AGORA+ into executable JavaScript assertions. AGORA+ achieved a precision of 80% on 25 operations from 20 industrial APIs. It also identified 48% of errors systematically seeded in the outputs of the APIs under test. AGORA+ uncovered 32 bugs in popular APIs, including Amadeus, Deutschebahn, GitHub, Marvel, NYTimesBooks, and YouTube, leading to fixes and documentation updates.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
keywords = {REST APIs, test oracle, invariant detection, automated testing}
}

@inproceedings{10.1145/3324884.3416617,
author = {Li, Ke and Xiang, Zilin and Chen, Tao and Tan, Kay Chen},
title = {BiLO-CPDP: bi-level programming for automated model discovery in cross-project defect prediction},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416617},
doi = {10.1145/3324884.3416617},
abstract = {Cross-Project Defect Prediction (CPDP), which borrows data from similar projects by combining a transfer learner with a classifier, have emerged as a promising way to predict software defects when the available data about the target project is insufficient. However, developing such a model is challenge because it is difficult to determine the right combination of transfer learner and classifier along with their optimal hyper-parameter settings. In this paper, we propose a tool, dubbed BiLO-CPDP, which is the first of its kind to formulate the automated CPDP model discovery from the perspective of bi-level programming. In particular, the bi-level programming proceeds the optimization with two nested levels in a hierarchical manner. Specifically, the upper-level optimization routine is designed to search for the right combination of transfer learner and classifier while the nested lower-level optimization routine aims to optimize the corresponding hyper-parameter settings. To evaluate BiLO-CPDP, we conduct experiments on 20 projects to compare it with a total of 21 existing CPDP techniques, along with its single-level optimization variant and Auto-Sklearn, a state-of-the-art automated machine learning tool. Empirical results show that BiLO-CPDP champions better prediction performance than all other 21 existing CPDP techniques on 70% of the projects, while being overwhelmingly superior to Auto-Sklearn and its single-level optimization variant on all cases. Furthermore, the unique bi-level formalization in BiLO-CPDP also permits to allocate more budget to the upper-level, which significantly boosts the performance.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {573–584},
numpages = {12},
keywords = {automated parameter optimization, classification techniques, configurable software and tool, cross-project defect prediction, transfer learning},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1145/3672454,
author = {Huang, Dong and Bu, Qingwen and Fu, Yichao and Qing, Yuhao and Xie, Xiaofei and Chen, Junjie and Cui, Heming},
title = {Neuron Sensitivity-Guided Test Case Selection},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3672454},
doi = {10.1145/3672454},
abstract = {Deep neural networks (DNNs) have been widely deployed in software to address various tasks (e.g., autonomous driving, medical diagnosis). However, they can also produce incorrect behaviors that result in financial losses and even threaten human safety. To reveal and repair incorrect behaviors in DNNs, developers often collect rich, unlabeled datasets from the natural world and label them to test DNN models. However, properly labeling a large number of datasets is a highly expensive and time-consuming task.To address the above-mentioned problem, we propose neuron sensitivity-guided test case selection (NSS), which can reduce the labeling time by selecting valuable test cases from unlabeled datasets. NSS leverages the information of the internal neuron induced by the test cases to select valuable test cases, which have high confidence in causing the model to behave incorrectly. We evaluated NSS with four widely used datasets and four well-designed DNN models compared to the state-of-the-art (SOTA) baseline methods. The results show that NSS performs well in assessing the probability of failure triggering in test cases and in the improvement capabilities of the model. Specifically, compared to the baseline approaches, NSS achieves a higher fault detection rate (e.g., when selecting 5% of the test cases from the unlabeled dataset in the MNIST and LeNet1 experiment, NSS can obtain an 81.8% fault detection rate, which is a 20% increase compared with SOTA baseline strategies).},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {188},
numpages = {32},
keywords = {Deep learning testing, neuron sensitivity, model interpretation}
}

@article{10.1145/3477127.3477131,
author = {Won, Kwanghee and Choi, Hyung-do and Shin, Sung},
title = {Deep learning-based semantic classification of EMF-related scientific literature},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1559-6915},
url = {https://doi.org/10.1145/3477127.3477131},
doi = {10.1145/3477127.3477131},
abstract = {Semantic classification of scientific literature using machine learning approaches is challenging due to the difficulties in labeling data and the length of the texts [2, 7]. Most of the work has been done for keyword-based categorization tasks, which take care of occurrence of important terms, whereas semantic classification requires understanding of terms and the meaning of sentences in a context. In this study, we have evaluated neural network models on a semantic classification task using 1091 labeled EMF-related scientific papers listed in the Powerwatch study. The EMF-related papers are labeled into three categories: positive, null finding, and neither. We have conducted neural architecture and hyperparameter search to find the most suitable model for the task. In experiments, we compared the performance of several neural network models in terms of classification accuracy. In addition, we have tested two different types of attention mechanisms. First, a Fully Convolutional Neural Network (FCN) has been used to identify important sentences in the text for the semantic classification. Second, the Transformer, a self-attention-based model, has been tested on the dataset. The experimental result showed that the BiLSTM performed best on both unbalanced and balanced data and the FCN was able to identify important parts in input texts.},
journal = {SIGAPP Appl. Comput. Rev.},
month = jul,
pages = {48–56},
numpages = {9},
keywords = {semantic classification, fully convolutional network, deep neural network, attention model}
}

@inproceedings{10.1109/SC41406.2024.00105,
author = {Fu, Xiang and Zhang, Weiping and Meng, Shiman and Huang, Xin and Xu, Wubiao and Guo, Luanzheng and Sato, Kento},
title = {AutoCheck: Automatically Identifying Variables for Checkpointing by Data Dependency Analysis},
year = {2024},
isbn = {9798350352917},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC41406.2024.00105},
doi = {10.1109/SC41406.2024.00105},
abstract = {Checkpoint/Restart (C/R) has been widely deployed in numerous HPC systems, Clouds, and industrial data centers, which are typically operated by system engineers. Nevertheless, there is no existing approach that helps system engineers without domain expertise, and domain scientists without system fault tolerance knowledge identify those critical variables accounted for correct application execution restoration in a failure for C/R. To address this problem, we propose an analytical model and a tool (AutoCheck) that can automatically identify critical variables to checkpoint for C/R. AutoCheck relies on first, analytically tracking and optimizing data dependency between variables and other application execution state, and second, a set of heuristics that identify critical variables for checkpointing from the refined data dependency graph (DDG). AutoCheck allows programmers to pinpoint critical variables to checkpoint quickly within a few minutes. We evaluate AutoCheck on 14 representative HPC benchmarks, demonstrating that AutoCheck can efficiently identify correct critical variables to checkpoint.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {99},
numpages = {16},
keywords = {Checkpoint/Restart, Data dependency analysis, Fault tolerance, LLVM-Trace},
location = {Atlanta, GA, USA},
series = {SC '24}
}

@inproceedings{10.1145/2856636.2856637,
author = {Lal, Sangeeta and Sureka, Ashish},
title = {LogOpt: Static Feature Extraction from Source Code for Automated Catch Block Logging Prediction},
year = {2016},
isbn = {9781450340182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2856636.2856637},
doi = {10.1145/2856636.2856637},
abstract = {Software logging is an important software development practice which is used to trace important software execution points. This execution information can provide important insight to developer while software debugging. Inspite of many benefits logging is often done in an ad-hoc manner based only on knowledge and experience of software developer because of lack of formal guidelines and training required for making strategic logging decision. It is known that appropriate logging is beneficial for developers but inappropriate logging can have adverse effect on the system. Excessive logging can not only cause performance and cost overhead, it can also lessen the benefit of logging by producing tons of useless logs. Sparse logging can make logging ineffective by leaving out important information. In order to lessen the load of software developers and to improve the quality of software logging, in this work we propose 'LogOpt' tool to help developers in making informed logging decision. LogOpt uses static features from source code to make catch block logging decision. LogOpt is a machine learning based framework which learns the characteristics of logged and unlogged training instance to make informed logging decision. We manually analyze snippets of logged and unlogged source code and extracted 46 distinguishing features important in making logging decision. We evaluated LogOpt on two large open source projects Apache Tomcat and CloudStack (nearly 1.41M LOC). Results show that LogOpt is effective for automated logging task.},
booktitle = {Proceedings of the 9th India Software Engineering Conference},
pages = {151–155},
numpages = {5},
keywords = {Tracing, Source Code Analysis, Machine Learning, Logging, Debugging},
location = {Goa, India},
series = {ISEC '16}
}

@inproceedings{10.1109/ICSE43902.2021.00046,
author = {Wang, Zan and You, Hanmo and Chen, Junjie and Zhang, Yingyi and Dong, Xuyuan and Zhang, Wenbin},
title = {Prioritizing Test Inputs for Deep Neural Networks via Mutation Analysis},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00046},
doi = {10.1109/ICSE43902.2021.00046},
abstract = {Deep Neural Network (DNN) testing is one of the most widely-used ways to guarantee the quality of DNNs. However, labeling test inputs to check the correctness of DNN prediction is very costly, which could largely affect the efficiency of DNN testing, even the whole process of DNN development. To relieve the labeling-cost problem, we propose a novel test input prioritization approach (called PRIMA) for DNNs via intelligent mutation analysis in order to label more bug-revealing test inputs earlier for a limited time, which facilitates to improve the efficiency of DNN testing. PRIMA is based on the key insight: a test input that is able to kill many mutated models and produce different prediction results with many mutated inputs, is more likely to reveal DNN bugs, and thus it should be prioritized higher. After obtaining a number of mutation results from a series of our designed model and input mutation rules for each test input, PRIMA further incorporates learning-to-rank (a kind of supervised machine learning to solve ranking problems) to intelligently combine these mutation results for effective test input prioritization. We conducted an extensive study based on 36 popular subjects by carefully considering their diversity from five dimensions (i.e., different domains of test inputs, different DNN tasks, different network structures, different types of test inputs, and different training scenarios). Our experimental results demonstrate the effectiveness of PRIMA, significantly outperforming the state-of-the-art approaches (with the average improvement of 8.50%~131.01% in terms of prioritization effectiveness). In particular, we have applied PRIMA to the practical autonomous-vehicle testing in a large motor company, and the results on 4 real-world scene-recognition models in autonomous vehicles further confirm the practicability of PRIMA.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {397–409},
numpages = {13},
keywords = {Test Prioritization, Mutation, Label, Deep Neural Network, Deep Learning Testing},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3533767.3534375,
author = {Weiss, Michael and Tonella, Paolo},
title = {Simple techniques work surprisingly well for neural network test prioritization and active learning (replicability study)},
year = {2022},
isbn = {9781450393799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533767.3534375},
doi = {10.1145/3533767.3534375},
abstract = {Test Input Prioritizers (TIP) for Deep Neural Networks (DNN) are an important technique to handle the typically very large test datasets efficiently, saving computation and labelling costs. This is particularly true for large scale, deployed systems, where inputs observed in production are recorded to serve as potential test or training data for next versions of the system. Feng et. al. propose DeepGini, a very fast and simple TIP and show that it outperforms more elaborate techniques such as neuron- and surprise coverage. In a large-scale study (4 case studies, 8 test datasets, 32’200 trained models) we verify their findings. However, we also find that other comparable or even simpler baselines from the field of uncertainty quantification, such as the predicted softmax likelihood or the entropy of the predicted softmax likelihoods perform equally well as DeepGini},
booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {139–150},
numpages = {12},
keywords = {uncertainty quantification, neural networks, Test prioritization},
location = {Virtual, South Korea},
series = {ISSTA 2022}
}

@inproceedings{10.1145/3345629.3345635,
author = {Wang, Song and Bansal, Chetan and Nagappan, Nachiappan and Philip, Adithya Abraham},
title = {Leveraging Change Intents for Characterizing and Identifying Large-Review-Effort Changes},
year = {2019},
isbn = {9781450372336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345629.3345635},
doi = {10.1145/3345629.3345635},
abstract = {Code changes to software occur due to various reasons such as bug fixing, new feature addition, and code refactoring. In most existing studies, the intent of the change is rarely leveraged to provide more specific, context aware analysis.In this paper, we present the first study to leverage change intent to characterize and identify Large-Review-Effort (LRE) changes regarding review effort---changes with large review effort. Specifically, we first propose a feedback-driven and heuristics-based approach to obtain change intents. We then characterize the changes regarding review effort by using various features extracted from change metadata and the change intents. We further explore the feasibility of automatically classifying LRE changes. We conduct our study on a large-scale project from Microsoft and three large-scale open source projects, i.e., Qt, Android, and OpenStack. Our results show that, (i) code changes with some intents are more likely to be LRE changes, (ii) machine learning based prediction models can efficiently help identify LRE changes, and (iii) prediction models built for code changes with some intents achieve better performance than prediction models without considering the change intent, the improvement in AUC can be up to 19 percentage points and is 7.4 percentage points on average. The tool developed in this study has already been used in Microsoft to provide the review effort and intent information of changes for reviewers to accelerate the review process.},
booktitle = {Proceedings of the Fifteenth International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {46–55},
numpages = {10},
keywords = {Code review, change intent, machine learning, review effort},
location = {Recife, Brazil},
series = {PROMISE'19}
}

@article{10.1145/3640329,
author = {Zhang, Quanjun and Zhai, Juan and Fang, Chunrong and Liu, Jiawei and Sun, Weisong and Hu, Haichuan and Wang, Qingyu},
title = {Machine Translation Testing via Syntactic Tree Pruning},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3640329},
doi = {10.1145/3640329},
abstract = {Machine translation systems have been widely adopted in our daily life, making life easier and more convenient. Unfortunately, erroneous translations may result in severe consequences, such as financial losses. This requires to improve the accuracy and the reliability of machine translation systems. However, it is challenging to test machine translation systems because of the complexity and intractability of the underlying neural models. To tackle these challenges, we propose a novel metamorphic testing approach by syntactic tree pruning (STP) to validate machine translation systems. Our key insight is that a pruned sentence should have similar crucial semantics compared with the original sentence. Specifically, STP (1) proposes a core semantics-preserving pruning strategy by basic sentence structures and dependency relations on the level of syntactic tree representation, (2) generates source sentence pairs based on the metamorphic relation, and (3) reports suspicious issues whose translations break the consistency property by a bag-of-words model. We further evaluate STP on two state-of-the-art machine translation systems (i.e., Google Translate and Bing Microsoft Translator) with 1,200 source sentences as inputs. The results show that STP accurately finds 5,073 unique erroneous translations in Google Translate and 5,100 unique erroneous translations in Bing Microsoft Translator (400% more than state-of-the-art techniques), with 64.5% and 65.4% precision, respectively. The reported erroneous translations vary in types and more than 90% of them are not found by state-of-the-art techniques. There are 9,393 erroneous translations unique to STP, which is 711.9% more than state-of-the-art techniques. Moreover, STP is quite effective in detecting translation errors for the original sentences with a recall reaching 74.0%, improving state-of-the-art techniques by 55.1% on average.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {125},
numpages = {39},
keywords = {Software testing, machine translation, metamorphic testing}
}

@inproceedings{10.1145/3273934.3273936,
author = {Ferenc, Rudolf and T\'{o}th, Zolt\'{a}n and Lad\'{a}nyi, Gergely and Siket, Istv\'{a}n and Gyim\'{o}thy, Tibor},
title = {A Public Unified Bug Dataset for Java},
year = {2018},
isbn = {9781450365932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3273934.3273936},
doi = {10.1145/3273934.3273936},
abstract = {Background: Bug datasets have been created and used by many researchers to build bug prediction models.Aims: In this work we collected existing public bug datasets and unified their contents.Method: We considered 5 public datasets which adhered to all of our criteria. We also downloaded the corresponding source code for each system in the datasets and performed their source code analysis to obtain a common set of source code metrics. This way we produced a unified bug dataset at class and file level that is suitable for further research (e.g. to be used in the building of new bug prediction models). Furthermore, we compared the metric definitions and values of the different bug datasets.Results: We found that (i) the same metric abbreviation can have different definitions or metrics calculated in the same way can have different names, (ii) in some cases different tools give different values even if the metric definitions coincide because (iii) one tool works on source code while the other calculates metrics on bytecode, or (iv) in several cases the downloaded source code contained more files which influenced the afferent metric values significantly.Conclusions: Apart from all these imprecisions, we think that having a common metric set can help in building better bug prediction models and deducing more general conclusions. We made the unified dataset publicly available for everyone. By using a public dataset as an input for different bug prediction related investigations, researchers can make their studies reproducible, thus able to be validated and verified.},
booktitle = {Proceedings of the 14th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {12–21},
numpages = {10},
keywords = {Bug dataset, code metrics, static code analysis},
location = {Oulu, Finland},
series = {PROMISE'18}
}

@inproceedings{10.1145/3540250.3558947,
author = {Fu, Ying and Yan, Meng and Xu, Jian and Li, Jianguo and Liu, Zhongxin and Zhang, Xiaohong and Yang, Dan},
title = {Investigating and improving log parsing in practice},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558947},
doi = {10.1145/3540250.3558947},
abstract = {Logs are widely used for system behavior diagnosis by automatic log mining. Log parsing is an important data preprocessing step that converts semi-structured log messages into structured data as the feature input for log mining. Currently, many studies are devoted to proposing new log parsers. However, to the best of our knowledge, no previous study comprehensively investigates the effectiveness of log parsers in industrial practice. To investigate the effectiveness of the log parsers in industrial practice, in this paper, we conduct an empirical study on the effectiveness of six state-of-the-art log parsers on 10 microservice applications of Ant Group. Our empirical results highlight two challenges for log parsing in practice: 1) various separators. There are various separators in a log message, and the separators in different event templates or different applications are also various. Current log parsers cannot perform well because they do not consider various separators. 2) Various lengths due to nested objects. The log messages belonging to the same event template may also have various lengths due to nested objects. The log messages of 6 out of 10 microservice applications at Ant Group with various lengths due to nested objects. 4 out of 6 state-of-the-art log parsers cannot deal with various lengths due to nested objects. In this paper, we propose an improved log parser named Drain+ based on a state-of-the-art log parser Drain. Drain+ includes two innovative components to address the above two challenges: a statistical-based separators generation component, which generates separators automatically for log message splitting, and a candidate event template merging component, which merges the candidate event templates by a template similarity method. We evaluate the effectiveness of Drain+ on 10 microservice applications of Ant Group and 16 public datasets. The results show that Drain+ outperforms the six state-of-the-art log parsers on industrial applications and public datasets. Finally, we conclude the observations in the road ahead for log parsing to inspire other researchers and practitioners.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1566–1577},
numpages = {12},
keywords = {Log parsing, Log analysis, Industrial study},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3691620.3695281,
author = {Humeniuk, Dmytro and Ben Braiek, Houssem and Reid, Thomas and Khomh, Foutse},
title = {In-Simulation Testing of Deep Learning Vision Models in Autonomous Robotic Manipulators},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695281},
doi = {10.1145/3691620.3695281},
abstract = {Testing autonomous robotic manipulators is challenging due to the complex software interactions between vision and control components. A crucial element of modern robotic manipulators is the deep learning based object detection model. The creation and assessment of this model requires real world data, which can be hard to label and collect, especially when the hardware setup is not available. The current techniques primarily focus on using synthetic data to train deep neural networks (DDNs) and identifying failures through offline or online simulation-based testing. However, the process of exploiting the identified failures to uncover design flaws early on, and leveraging the optimized DNN within the simulation to accelerate the engineering of the DNN for real-world tasks remains unclear. To address these challenges, we propose the MARTENS (Manipulator Robot Testing and Enhancement in Simulation) framework, which integrates a photorealistic NVIDIA Isaac Sim simulator with evolutionary search to identify critical scenarios aiming at improving the deep learning vision model and uncovering system design flaws. Evaluation of two industrial case studies demonstrated that MARTENS effectively reveals robotic manipulator system failures, detecting 25% to 50% more failures with greater diversity compared to random test generation. The model trained and repaired using the MARTENS approach achieved mean average precision (mAP) scores of 0.91 and 0.82 on real-world images with no prior retraining. Further fine-tuning on real-world images for a few epochs (less than 10) increased the mAP to 0.95 and 0.89 for the first and second use cases, respectively. In contrast, a model trained solely on real-world data achieved mAPs of 0.8 and 0.75 for use case 1 and use case 2 after more than 25 epochs.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {2187–2198},
numpages = {12},
keywords = {simulation, on-line testing, DNN testing, autonomous robotic manipulators, evolutionary search},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3650212.3680331,
author = {Le Tolguenec, Paul-Antoine and Rachelson, Emmanuel and Besse, Yann and Teichteil-Koenigsbuch, Florent and Schneider, Nicolas and Waeselynck, H\'{e}l\`{e}ne and Wilson, Dennis},
title = {Exploration-Driven Reinforcement Learning for Avionic System Fault Detection (Experience Paper)},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680331},
doi = {10.1145/3650212.3680331},
abstract = {Critical software systems require stringent testing to identify possible failure cases, which can be difficult to find using manual testing. In this study, we report our industrial experience in testing a realistic R&amp;D flight control system using a heuristic based testing method. Our approach utilizes evolutionary strategies augmented with intrinsic motivation to yield a diverse range of test cases, each revealing different potential failure scenarios within the system. This diversity allows for a more comprehensive identification and understanding of the system’s vulnerabilities. We analyze the test cases found by evolution to identify the system’s weaknesses. The results of our study show that our approach can be used to improve the reliability and robustness of avionics systems by providing high-quality test cases in an efficient and cost-effective manner.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {920–931},
numpages = {12},
keywords = {Reinforcement learning, automated testing, critical software system, diversity, evolutionary strategies, genetic algorithms, intrinsic motivation, physical system, software reliability},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3643786.3648024,
author = {Mancu, Andrei and Laurent, Thomas and Rieger, Franz and Arcaini, Paolo and Ishikawa, Fuyuki and Rueckert, Daniel},
title = {More is Not Always Better: Exploring Early Repair of DNNs},
year = {2024},
isbn = {9798400705748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643786.3648024},
doi = {10.1145/3643786.3648024},
abstract = {DNN repair is an effective technique applied after training to enhance the class-specific accuracy of classifier models, where a low failure rate is required on specific classes. The repair methods introduced in recent studies assume that they are applied to fully trained models. In this paper, we argue that this could not always be the best choice. We analyse the performance of DNN models under various training times and repair combinations. Through meticulously designed experiments on two real-world datasets and a carefully curated assessment score, we show that applying DNN repair earlier in the training process, and not only at its end, can be beneficial. Thus, we encourage the research community to consider the idea of when to apply DNN repair in the model development.},
booktitle = {Proceedings of the 5th IEEE/ACM International Workshop on Deep Learning for Testing and Testing for Deep Learning},
pages = {13–16},
numpages = {4},
keywords = {deep neural networks, DNN repair, DNN training, safety-critical},
location = {Lisbon, Portugal},
series = {DeepTest '24}
}

@article{10.1145/3714468,
author = {Oldfield, Noah H. and Laaber, Christoph and Yue, Tao and Ali, Shaukat},
title = {Faster and Better Quantum Software Testing through Specification Reduction and Projective Measurements},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3714468},
doi = {10.1145/3714468},
abstract = {Quantum computing (QC) promises polynomial and exponential speedups in many domains, such as unstructured search and prime number factoring. However, quantum programs yield probabilistic outputs from exponentially growing distributions and are vulnerable to quantum-specific faults. Existing quantum software testing (QST) approaches treat quantum superpositions as classical distributions. This leads to two major limitations when applied to quantum programs: (1) an exponentially growing sample space distribution and (2) failing to detect quantum-specific faults such as phase flips. To overcome these limitations, we introduce a QST approach, which applies a reduction algorithm to a quantum program specification. The reduced specification alleviates the limitations (1) by enabling faster sampling through quantum parallelism and (2) by performing projective measurements in the mixed Hadamard basis. Our evaluation of 143 quantum programs across four categories demonstrates significant improvements in test runtimes and fault detection with our reduction approach. Average test runtimes improved from 169.9s to 11.8s, with notable enhancements in programs with large circuit depths (383.1s to 33.4s) and large program specifications (464.8s to 7.7s). Furthermore, our approach increases mutation scores from  (54.5%)  to  (74.7%) , effectively detecting phase flip faults that non-reduced specifications miss. These results underline our approach’s importance to improve QST efficiency and effectiveness},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Quantum computing, software testing, quantum program specification, projective measurements}
}

@inproceedings{10.1145/3468264.3473935,
author = {Ye, Jiaming and Chen, Ke and Xie, Xiaofei and Ma, Lei and Huang, Ruochen and Chen, Yingfeng and Xue, Yinxing and Zhao, Jianjun},
title = {An empirical study of GUI widget detection for industrial mobile games},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3473935},
doi = {10.1145/3468264.3473935},
abstract = {With the widespread adoption of smartphones in our daily life, mobile games experienced increasing demand over the past years. Meanwhile, the quality of mobile games has been continuously drawing more and more attention, which can greatly affect the player experience. For better quality assurance, general-purpose testing has been extensively studied for mobile apps. However, due to the unique characteristic of mobile games, existing mobile testing techniques may not be directly suitable and applicable. To better understand the challenges in mobile game testing, in this paper, we first initiate an early step to conduct an empirical study towards understanding the challenges and pain points of mobile game testing process at our industrial partner NetEase Games. Specifically, we first conduct a survey from the mobile test development team at NetEase Games via both scrum interviews and questionnaires. We found that accurate and effective GUI widget detection for mobile games could be the pillar to boost the automation of mobile game testing and other downstream analysis tasks in practice.  We then continue to perform comparative studies to investigate the effectiveness of state-of-the-art general-purpose mobile app GUI widget detection methods in the context of mobile games. To this end, we also develop a technique to automatically collect GUI widgets region information of industrial mobile games, which is equipped with a heuristic-based data cleaning method for quality refinement of the labeling results. Our evaluation shows that: (1) Existing GUI widget detection methods for general-purpose mobile apps cannot perform well on industrial mobile games. (2) Mobile game exhibits obvious difference from other general-purpose mobile apps in the perspective GUI widgets. Our further in-depth analysis reveals high diversity and density characteristics of mobile game GUI widgets could be the major reasons that post the challenges for existing methods, which calls for new research methods and better industry practices. To enable further research along this line, we construct the very first GUI widget detection benchmark, specially designed for mobile games, incorporating both our collected dataset and the state-of-the-art widget detection methods for mobile apps, which could also be the basis for further study of many downstream quality assurance tasks (e.g., testing and analysis) for mobile games.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1427–1437},
numpages = {11},
keywords = {Game Testing, GUI Detection, Deep Learning},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@article{10.1145/3630252,
author = {Lustosa, Andre and Menzies, Tim},
title = {Learning from Very Little Data: On the Value of Landscape Analysis for Predicting Software Project Health},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3630252},
doi = {10.1145/3630252},
abstract = {When data is scarce, software analytics can make many mistakes. For example, consider learning predictors for open source project health (e.g., the number of closed pull requests in 12 months time). The training data for this task may be very small (e.g., 5 years of data, collected every month means just 60 rows of training data). The models generated from such tiny datasets can make many prediction errors.Those errors can be tamed by a landscape analysis that selects better learner control parameters. Our niSNEAK tool (a)&nbsp;clusters the data to find the general landscape of the hyperparameters, then (b)&nbsp;explores a few representatives from each part of that landscape. niSNEAK is both faster and more effective than prior state-of-the-art hyperparameter optimization algorithms (e.g., FLASH, HYPEROPT, OPTUNA).The configurations found by niSNEAK have far less error than other methods. For example, for project health indicators such as C = number of commits, I = number of closed issues, and R = number of closed pull requests, niSNEAK’s 12-month prediction errors are {I=0%, R=33%&nbsp;C=47%}, whereas other methods have far larger errors of {I=61%,R=119%&nbsp;C=149%}. We conjecture that niSNEAK works so well since it finds the most informative regions of the hyperparameters, then jumps to those regions. Other methods (that do not reflect over the landscape) can waste time exploring less informative options.Based on the preceding, we recommend landscape analytics (e.g., niSNEAK) especially when learning from very small datasets. This article only explores the application of niSNEAK to project health. That said, we see nothing in principle that prevents the application of this technique to a wider range of problems.To assist other researchers in repeating, improving, or even refuting our results, all our scripts and data are available on GitHub at https://github.com/zxcv123456qwe/niSneak.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {58},
numpages = {22},
keywords = {Hyperparameter tuning, software health, indepedent variable clustering}
}

@inproceedings{10.1145/3510454.3516828,
author = {Wei, Zhengyuan and Wang, Haipeng and Yang, Zhen and Chan, W. K.},
title = {SEbox4DL: a modular software engineering toolbox for deep learning models},
year = {2022},
isbn = {9781450392235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510454.3516828},
doi = {10.1145/3510454.3516828},
abstract = {Deep learning (DL) models are widely used in software applications. Novel DL models and datasets are published from time to time. Developers may also tempt to apply new software engineering (SE) techniques on their DL models. However, no existing tool supports the applications of software testing and debugging techniques on new DL models and their datasets without modifying the code. Developers should manually write code to glue every combination of models, datasets, and SE technique and chain them together.We propose SEbox4DL, a novel and modular toolbox that automatically integrates models, datasets, and SE techniques into SE pipelines seen in developing DL models. SEbox4DL exemplifies six SE pipelines and can be extended with ease. Each user-defined task in the pipelines is to implement a SE technique within a function with a unified interface so that the whole design of SEbox4DL is generic, modular, and extensible. We have implemented several SE techniques as user-defined tasks to make SEbox4DL off-the-shelf. Our experiments demonstrate that SEbox4DL can simplify the applications of software testing and repair techniques on the latest or popular DL models and datasets. The toolbox is open-source and published at https://github.com/Wsine/SEbox4DL. A video for demonstration is available at: https://youtu.be/EYeFFi4lswc.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings},
pages = {193–196},
numpages = {4},
keywords = {toolbox, testing, software engineering, repair, neural networks},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/2889160.2889164,
author = {Luo, Qi and Poshyvanyk, Denys and Nair, Aswathy and Grechanik, Mark},
title = {FOREPOST: a tool for detecting performance problems with feedback-driven learning software testing},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889164},
doi = {10.1145/2889160.2889164},
abstract = {A goal of performance testing is to find situations when applications unexpectedly exhibit worsened characteristics for certain combinations of input values. A fundamental question of performance testing is how to select a manageable subset of the input data faster to find performance problems in applications automatically.We present a novel tool, FOREPOST, for finding performance problems in applications automatically using black-box software testing. In this paper, we demonstrate how FOREPOST extracts rules from execution traces of applications by using machine learning algorithms, and then uses these rules to select test input data automatically to steer applications towards computationally intensive paths and to find performance problems. FOREPOST is available in our online appendix (http://www.cs.wm.edu/semeru/data/ICSE16-FOREPOST), which contains the tool, source code and demo video.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {593–596},
numpages = {4},
keywords = {performance testing, machine learning, black-box testing},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/3010079.3010086,
author = {Garcia, Washington and Benson, Theophilus},
title = {A First Look at Bugs in OpenStack},
year = {2016},
isbn = {9781450346733},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3010079.3010086},
doi = {10.1145/3010079.3010086},
abstract = {An increasing amount of popular services are utilizing cloud infrastructure due to its convenience, low cost, and scalability. However, as more services turn to cloud as a means of storing and delivering data to consumers, the faults of cloud infrastructure become more apparent. When cloud infrastructure fails, the consequences are disastrous, with failures making national headlines. Popular services such as Amazon, Quora, Netflix, and many social media sites all rely on cloud computing at their core. Although new cloud infrastructures have sprouted in recent years, there is limited knowledge about what type of bugs they contain, and how these bugs affect quality of service of cloud components. We propose a system that can automatically classify bug tickets using the natural language descriptions provided by developers. We then utilize this system to classify a random sub-sample of 30k OpenStack bugs, and reveal trends related to OpenStack releases, priority assignments, and project characteristics. For example, we find that existing issues make up over 70% of bugs in OpenStack modules, with over half of these bugs corresponding to reliability.},
booktitle = {Proceedings of the 2016 ACM Workshop on Cloud-Assisted Networking},
pages = {67–72},
numpages = {6},
keywords = {software bugs, openstack, machine learning, cloud},
location = {Irvine, California, USA},
series = {CAN '16}
}

@article{10.1145/3711902,
author = {Gruner, Bernd and Brust, Clemens-Alexander and Zeller, Andreas},
title = {Finding Information Leaks with Information Flow Fuzzing},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3711902},
doi = {10.1145/3711902},
abstract = {We present information flow fuzzing, an approach that guides fuzzers towards detecting information leaks—information that reaches a third party, but should not. The approach detects information flow by means of mutations, checking whether and how mutations to (secret) data affect output and execution:

First, the fuzzer uses information flow as a leak oracle. To this end, for each input, the fuzzer first runs the program regularly. Then, it mutates secret data such as a certificate or a password, and re-runs the program giving the original input. If the output changes, the fuzzer has revealed an information leak.
Second, the fuzzer uses information flow as guidance. The fuzzer not only maximizes coverage, but also changes in coverage and changes in data between the two runs. This increases the likelihood that a mutation will spread to the output.We have implemented a tool named FLOWFUZZ that wraps around a C program under test to provide information-flow based oracles and guidance, allowing for integration with all common fuzzers for C &nbsp;programs. Using a set of subjects representing common information leaks, we investigate (1) whether oracles based on information flow detect information leaks in our subjects; and (2) whether guidance based on information flow improves over standard coverage guidance. All data and tools are available for replication and reproduction.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {fuzzing, information flow}
}

@inproceedings{10.1145/3678890.3678928,
author = {Wang, Yu and Xu, Yue},
title = {Beyond REST: Introducing APIF for Comprehensive API Vulnerability Fuzzing},
year = {2024},
isbn = {9798400709593},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678890.3678928},
doi = {10.1145/3678890.3678928},
abstract = {In modern software development, APIs play a crucial role as they facilitate platform interoperability and serve as conduits for data transmission. API fuzzing has emerged to explore errors and vulnerabilities in web applications, cloud services, and IoT systems. Its effectiveness highly depends on parameter structure analysis and fuzzing request generation. However, existing methods focus more on RESTful APIs, lacking generalizability for other protocols. Additionally, shortcomings in the effectiveness of test payloads and testing efficiency have limited the large-scale application of these methods in real-world scenarios. This paper introduces APIF, a novel API fuzzing framework that incorporates three innovative designs. Firstly, by adopting a tree-structured model for parsing and mutating parameters in different API protocols, APIF breaks the limitations of existing research that are only effective for RESTful APIs, thus broadening its applicability. Secondly, APIF utilizes a recursive decoder to tackle the complex encodings in API parameters, increasing the fuzzing effectiveness. Thirdly, APIF leverages a testing priority calculation algorithm together with a parameter independence analysis algorithm to enhance fuzzing efficiency, enabling this method to be widely applied in real-world, large-scale API vulnerability fuzzing. We evaluate APIF against the state-of-the-art fuzzers on 7 open-source projects via 412 APIs. The results demonstrate APIF’s superior precision, recall, and efficiency. Moreover, in real-world API vulnerability exploration, APIF discovered 188 bugs over 60 API projects, with 26 vulnerabilities confirmed by the software maintainers.},
booktitle = {Proceedings of the 27th International Symposium on Research in Attacks, Intrusions and Defenses},
pages = {435–449},
numpages = {15},
keywords = {API Fuzzing, Application Security, Vulnerability Testing, Web Security},
location = {Padua, Italy},
series = {RAID '24}
}

@inproceedings{10.1145/3597926.3598086,
author = {Liu, Yu and Zhang, Jiyang and Nie, Pengyu and Gligoric, Milos and Legunsen, Owolabi},
title = {More Precise Regression Test Selection via Reasoning about Semantics-Modifying Changes},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598086},
doi = {10.1145/3597926.3598086},
abstract = {Regression test selection (RTS) speeds up regression testing by only re-running tests that might be affected by code changes. Ideal RTS safely selects all affected tests and precisely selects only affected tests. But, aiming for this ideal is often slower than re-running all tests. So, recent RTS techniques use program analysis to trade precision for speed, i.e., lower regression testing time, or even use machine learning to trade safety for speed. We seek to make recent analysis-based RTS techniques more precise, to further speed up regression testing. Independent studies suggest that these techniques reached a “performance wall” in the speed-ups that they provide.  
We manually inspect code changes to discover those that do not require re-running tests that are only affected by such changes. We categorize 29 kinds of changes that we find from five projects into 13 findings, 11 of which are semantics-modifying. We enhance two RTS techniques---Ekstazi and STARTS---to reason about our findings. Using 1,150 versions of 23 projects, we evaluate the impact on safety and precision of leveraging such changes. We also evaluate if our findings from a few projects can speed up regression testing in other projects. The results show that our enhancements are effective and they can generalize. On average, they result in selecting 41.7% and 31.8% fewer tests, and take 33.7% and 28.7% less time than Ekstazi and STARTS, respectively, with no loss in safety.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {664–676},
numpages = {13},
keywords = {semantics-modifying changes, regression testing, change-impact analysis, Regression test selection},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@article{10.1109/TCBB.2018.2822803,
author = {Sevakula, Rahul K. and Singh, Vikas and Verma, Nishchal K. and Kumar, Chandan and Cui, Yan},
title = {Transfer Learning for Molecular Cancer Classification Using Deep Neural Networks},
year = {2019},
issue_date = {Nov.-Dec. 2019},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {16},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2018.2822803},
doi = {10.1109/TCBB.2018.2822803},
abstract = {The emergence of deep learning has impacted numerous machine learning based applications and research. The reason for its success lies in two main advantages: 1) it provides the ability to learn very complex non-linear relationships between features and 2) it allows one to leverage information from unlabeled data that does not belong to the problem being handled. This paper presents a transfer learning procedure for cancer classification, which uses feature selection and normalization techniques in conjunction with s sparse auto-encoders on gene expression data. While classifying any two tumor types, data of other tumor types were used in unsupervised manner to improve the feature representation. The performance of our algorithm was tested on 36 two-class benchmark datasets from the GEMLeR repository. On performing statistical tests, it is clearly ascertained that our algorithm statistically outperforms several generally used cancer classification approaches. The deep learning based molecular disease classification can be used to guide decisions made on the diagnosis and treatment of diseases, and therefore may have important applications in precision medicine.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = dec,
pages = {2089–2100},
numpages = {12}
}

@proceedings{10.1145/3679006,
title = {MET 2024: Proceedings of the 9th ACM International Workshop on Metamorphic Testing},
year = {2024},
isbn = {9798400711176},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 9th International Workshop on Metamorphic Testing (MET 2024), held in conjunction with the 2024 ACM SIGSOFT International Symposium on Software Testing and Analysis and European Conference on Object-Oriented Programming (ISSTA/ECOOP 2024).},
location = {Vienna, Austria}
}

@inproceedings{10.1109/ICSE43902.2021.00066,
author = {K\"{u}\c{c}\"{u}k, Yi\u{g}it and Henderson, Tim A. D. and Podgurski, Andy},
title = {Improving Fault Localization by Integrating Value and Predicate Based Causal Inference Techniques},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00066},
doi = {10.1109/ICSE43902.2021.00066},
abstract = {Statistical fault localization (SFL) techniques use execution profiles and success/failure information from software executions, in conjunction with statistical inference, to automatically score program elements based on how likely they are to be faulty. SFL techniques typically employ one type of profile data: either coverage data, predicate outcomes, or variable values. Most SFL techniques actually measure correlation, not causation, between profile values and success/failure, and so they are subject to confounding bias that distorts the scores they produce. This paper presents a new SFL technique, named UniVal, that uses causal inference techniques and machine learning to integrate information about both predicate outcomes and variable values to more accurately estimate the true failure-causing effect of program statements. UniVal was empirically compared to several coverage-based, predicate-based, and value-based SFL techniques on 800 program versions with real faults.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {649–660},
numpages = {12},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3624032.3624043,
author = {Marques, Jo\~{a}o Paulo and Lima, M\^{o}nica and Souza, Bruno and Miranda, Eloise and Santos, Andr\'{e} and Collins, Eliane},
title = {SelectNLTest - Selection and natural language rewriting of test cases generated by the DRL-MOBTEST tool},
year = {2023},
isbn = {9798400716294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624032.3624043},
doi = {10.1145/3624032.3624043},
abstract = {The software testing process is important to ensure quality, especially in mobile applications that have characteristics such as platform diversity, hardware limitations, portability, frequent updates, among others. Software companies need to deliver quickly with increasingly complex functionalities; therefore, the testing process must be efficient and avoid bottlenecks, such as the creation of test cases. Among the solutions found in the literature, the state-of-the-art tool DRL-MOBTEST aims to assist in the automatic generation of test cases for mobile applications using deep reinforcement learning. The experiments show promising results; however, the tool has some limitations, such as generating duplicate and less readable tests. In this article, we present SelectNLTest, a module developed to identify and remove similar test scripts and transcribe the test cases generated by the tool using Natural Language Processing techniques. This allows for the removal of similar tests and improves the readability and understanding of the generated test cases for professionals in the field. The results of the experiments showed that, in 10 Android applications used in the comparative analysis, the proposed module reduced the number of test cases by 58.3% while maintaining code coverage and application functionality.},
booktitle = {Proceedings of the 8th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {77–85},
numpages = {9},
keywords = {natural language, deep reinforcement learning, coverage analysis, automatic test generation},
location = {Campo Grande, MS, Brazil},
series = {SAST '23}
}

@article{10.1145/3604609,
author = {Wei, Zhengyuan and Wang, Haipeng and Ashraf, Imran and Chan, Wing-Kwong},
title = {DeepPatch: Maintaining Deep Learning Model Programs to Retain Standard Accuracy with Substantial Robustness Improvement},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3604609},
doi = {10.1145/3604609},
abstract = {Maintaining a deep learning (DL) model by making the model substantially more robust through retraining with plenty of adversarial examples of non-trivial perturbation strength often reduces the model’s standard accuracy. Many existing model repair or maintenance techniques sacrifice standard accuracy to produce a large gain in robustness or vice versa. This article proposes DeepPatch, a novel technique to maintain filter-intensive DL models. To the best of our knowledge, DeepPatch is the first work to address the challenge of standard accuracy retention while substantially improving the robustness of DL models with plenty of adversarial examples of non-trivial and diverse perturbation strengths. Rather than following the conventional wisdom to generalize all the components of a DL model over the union set of clean and adversarial samples, DeepPatch formulates a novel division of labor method to adaptively activate a subset of its inserted processing units to process individual samples. Its produced model can generate the original or replacement feature maps in each forward pass of the patched model, making the patched model carry an intrinsic property of behaving like the model under maintenance on demand. The overall experimental results show that DeepPatch successfully retains the standard accuracy of all pretrained models while improving the robustness accuracy substantially. However, the models produced by the peer techniques suffer from either large standard accuracy loss or small robustness improvement compared with the models under maintenance, rendering them unsuitable in general to replace the latter.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {150},
numpages = {49},
keywords = {accuracy recovery, maintenance, Model testing}
}

@inproceedings{10.1145/3524481.3527232,
author = {de Santiago J\'{u}nior, Valdivino Alexandre},
title = {A method and experiment to evaluate deep neural networks as test oracles for scientific software},
year = {2022},
isbn = {9781450392860},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524481.3527232},
doi = {10.1145/3524481.3527232},
abstract = {Testing scientific software is challenging because usually such type of systems have non-deterministic behaviours and, in addition, they generate non-trivial outputs such as images. Artificial intelligence (AI) is now a reality which is also helping in the development of the software testing activity. In this article, we evaluate seven deep neural networks (DNNs), precisely deep convolutional neural networks (CNNs) with up to 161 layers, playing the role of test oracle procedures for testing scientific models. Firstly, we propose a method, TOrC, which starts by generating training, validation, and test image datasets via combinatorial interaction testing applied to the original codes and second-order mutants. Within TOrC we also have classical steps such as transfer learning, a technique recommended for DNNs. Then, we verified the performance of the oracles (CNNs). The main conclusions of this research are: i) not necessarily a greater number of layers means that a CNN will present better performance; ii) transfer learning is a valuable technique but eventually we may need extended solutions to get better performances; iii) data-centric AI is an interesting path to follow; and iv) there is not a clear correlation between the software bugs, in the scientific models, and the errors (image misclassifications) presented by the CNNs.},
booktitle = {Proceedings of the 3rd ACM/IEEE International Conference on Automation of Software Test},
pages = {40–51},
numpages = {12},
keywords = {transfer learning, test oracles, explainable artificial intelligence, deep convolutional neural networks, data-centric artificial intelligence},
location = {Pittsburgh, Pennsylvania},
series = {AST '22}
}

@inproceedings{10.1145/3650212.3680328,
author = {Yang, Boyang and Tian, Haoye and Pian, Weiguo and Yu, Haoran and Wang, Haitao and Klein, Jacques and Bissyand\'{e}, Tegawend\'{e} F. and Jin, Shunfu},
title = {CREF: An LLM-Based Conversational Software Repair Framework for Programming Tutors},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680328},
doi = {10.1145/3650212.3680328},
abstract = {With the proven effectiveness of Large Language Models (LLMs) in code-related tasks, researchers have explored their potential for program repair. However, existing repair benchmarks might have influenced LLM training data, potentially causing data leakage. To evaluate LLMs’ realistic repair capabilities, (i) we introduce an extensive, non-crawled benchmark TutorCode, comprising 1,239 C++ defect codes and associated information such as tutor guidance, solution description, failing test cases, and the corrected code. Our work assesses LLM’s repair performance on TutorCode, measuring repair correctness (TOP-5 and AVG-5) and patch precision (RPSR). (ii) We then provide a comprehensive investigation into which types of extra information can help LLMs improve their repair performance. Among these types, tutor guidance was the most effective information. To fully harness LLMs’ conversational capabilities and the benefits of augmented information, (iii) we introduce a novel conversational semi-automatic repair framework CREF assisting human programming tutors. It demonstrates a remarkable AVG-5 improvement of 17.2%-24.6% compared to the baseline, achieving an impressive AVG-5 of 76.6% when utilizing GPT-4. These results highlight the potential for enhancing LLMs’ repair capabilities through tutor interactions and historical conversations. The successful application of CREF in a real-world educational setting demonstrates its effectiveness in reducing tutors’ workload and improving students’ learning experience, showing promise for code review and other software engineering tasks.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {882–894},
numpages = {13},
keywords = {Large Language Model, Open Source, Program Repair},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3533767.3534386,
author = {Wang, Jialai and Qiu, Han and Rong, Yi and Ye, Hengkai and Li, Qi and Li, Zongpeng and Zhang, Chao},
title = {BET: black-box efficient testing for convolutional neural networks},
year = {2022},
isbn = {9781450393799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533767.3534386},
doi = {10.1145/3533767.3534386},
abstract = {It is important to test convolutional neural networks (CNNs) to identify defects (e.g. error-inducing inputs) before deploying them in security-sensitive scenarios. Although existing white-box testing methods can effectively test CNN models with high neuron coverage, they are not applicable to privacy-sensitive scenarios where full knowledge of target CNN models is lacking. In this work, we propose a novel Black-box Efficient Testing (BET) method for CNN models. The core insight of BET is that CNNs are generally prone to be affected by continuous perturbations. Thus, by generating such continuous perturbations in a black-box manner, we design a tunable objective function to guide our testing process for thoroughly exploring defects in different decision boundaries of the target CNN models. We further design an efficiency-centric policy to find more error-inducing inputs within a fixed query budget. We conduct extensive evaluations with three well-known datasets and five popular CNN structures. The results show that BET significantly outperforms existing white-box and black-box testing methods considering the effective error-inducing inputs found in a fixed query/inference budget. We further show that the error-inducing inputs found by BET can be used to fine-tune the target model, improving its accuracy by up to 3%.},
booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {164–175},
numpages = {12},
keywords = {Convolutional Neural Networks, Black-box Testing},
location = {Virtual, South Korea},
series = {ISSTA 2022}
}

@inproceedings{10.1145/3643991.3644885,
author = {Liu, Zhipeng and Yan, Meng and Gao, Zhipeng and Li, Dong and Zhang, Xiaohong and Yang, Dan},
title = {AW4C: A Commit-Aware C Dataset for Actionable Warning Identification},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644885},
doi = {10.1145/3643991.3644885},
abstract = {Excessive non-actionable warnings generated by static program analysis tools can hinder developers from utilizing these tools effectively. Leveraging learning-based approaches for actionable warning identification has demonstrated promise in boosting developer productivity, minimizing the risk of bugs, and reducing code smells. However, the small sizes of existing datasets have limited the model choices for machine learning researchers, and the lack of aligned fix commits limits the scope of the dataset for research. In this paper, we present AW4C, an actionable warning C dataset that contains 38,134 actionable warnings mined from more than 500 repositories on GitHub. These warnings are generated via Cppcheck, and most importantly, each warning is precisely mapped to the commit where the corrective action occurred. To the best of our knowledge, this is the largest publicly available actionable warning dataset for C programming language to date. The dataset is suited for use in machine/deep learning models and can support a wide range of tasks, such as actionable warning identification and vulnerability detection. Furthermore, we have released our dataset1 and a general framework for collecting actionable warnings on GitHub2 to facilitate other researchers to replicate our work and validate their innovative ideas.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {133–137},
numpages = {5},
keywords = {static program analysis, actionable warning identification},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@article{10.1145/3607835,
author = {Varshosaz, Mahsa and Ghaffari, Mohsen and Johnsen, Einar Broch and W\k{a}sowski, Andrzej},
title = {Formal Specification and Testing for Reinforcement Learning},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {ICFP},
url = {https://doi.org/10.1145/3607835},
doi = {10.1145/3607835},
abstract = {The development process for reinforcement learning applications is still exploratory rather than systematic. This exploratory nature reduces reuse of specifications between applications and increases the chances of introducing programming errors. This paper takes a step towards systematizing the development of reinforcement learning applications. We introduce a formal specification of reinforcement learning problems and algorithms, with a particular focus on temporal difference methods and their definitions in backup diagrams. We further develop a test harness for a large class of reinforcement learning applications based on temporal difference learning, including SARSA and Q-learning. The entire development is rooted in functional programming methods; starting with pure specifications and denotational semantics, ending with property-based testing and using compositional interpreters for a domain-specific term language as a test oracle for concrete implementations. We demonstrate the usefulness of this testing method on a number of examples, and evaluate with mutation testing. We show that our test suite is effective in killing mutants (90% mutants killed for 75% of subject agents). More importantly, almost half of all mutants are killed by generic write-once-use-everywhere tests that apply to any reinforcement learning problem modeled using our library, without any additional effort from the programmer.},
journal = {Proc. ACM Program. Lang.},
month = aug,
articleno = {193},
numpages = {34},
keywords = {Scala, reinforcement learning, specification-based testing}
}

@article{10.1145/3607179,
author = {Rahman, Mohammad Masudur and Roy, Chanchal K.},
title = {A Systematic Review of Automated Query Reformulations in Source Code Search},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3607179},
doi = {10.1145/3607179},
abstract = {Fixing software bugs and adding new features are two of the major maintenance tasks. Software bugs and features are reported as change requests. Developers consult these requests and often choose a few keywords from them as an ad hoc query. Then they execute the query with a search engine to find the exact locations within software code that need to be changed. Unfortunately, even experienced developers often fail to choose appropriate queries, which leads to costly trials and errors during a code search. Over the years, many studies have attempted to reformulate the ad hoc queries from developers to support them. In this systematic literature review, we carefully select 70 primary studies on query reformulations from 2,970 candidate studies, perform an in-depth qualitative analysis (e.g., Grounded Theory), and then answer seven research questions with major findings. First, to date, eight major methodologies (e.g., term weighting, term co-occurrence analysis, thesaurus lookup) have been adopted to reformulate queries. Second, the existing studies suffer from several major limitations (e.g., lack of generalizability, the vocabulary mismatch problem, subjective bias) that might prevent their wide adoption. Finally, we discuss the best practices and future opportunities to advance the state of research in search query reformulations.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {159},
numpages = {79},
keywords = {systematic literature review, machine learning, query quality analysis, term weighting, automated query reformulation, Internet-scale code search, bug localization, Concept location}
}

@inproceedings{10.1145/3540250.3558959,
author = {Zlotchevski, Andrei and Drain, Dawn and Svyatkovskiy, Alexey and Clement, Colin B. and Sundaresan, Neel and Tufano, Michele},
title = {Exploring and evaluating personalized models for code generation},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558959},
doi = {10.1145/3540250.3558959},
abstract = {Large Transformer models achieved the state-of-the-art status for Natural Language Understanding tasks and are increasingly becoming the baseline model architecture for modeling source code. Transformers are usually pre-trained on large unsupervised corpora, learning token representations and transformations relevant to modeling generally available text, and are then fine-tuned on a particular downstream task of interest. While fine-tuning is a tried-and-true method for adapting a model to a new domain -- for example, question-answering on a given topic -- generalization remains an on-going challenge. In this paper, we explore and evaluate transformer model fine-tuning for personalization. In the context of generating unit tests for Java methods, we evaluate learning to personalize to a specific software project using several personalization techniques. We consider three key approaches: (i) custom fine-tuning, which allows all the model parameters to be tuned; (ii) lightweight fine-tuning, which freezes most of the model's parameters, allowing tuning of the token embeddings and softmax layer only or the final layer alone; (iii) prefix tuning, which keeps model parameters frozen, but optimizes a small project-specific prefix vector. Each of these techniques offers a trade-off in total compute cost and predictive performance, which we evaluate by code and task-specific metrics, training time, and total computational operations. We compare these fine-tuning strategies for code generation and discuss the potential generalization and cost benefits of each in various deployment scenarios.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1500–1508},
numpages = {9},
keywords = {Personalized Models, Code Generation},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00091,
author = {Dola, Swaroopa and Dwyer, Matthew B. and Soffa, Mary Lou},
title = {Artifact: distribution-aware testing of neural networks using generative models},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00091},
doi = {10.1109/ICSE-Companion52605.2021.00091},
abstract = {The artifact used for the experimental evaluation of Distribution-Aware Testing of Neural Networks Using Generative Models is publicly available on GitHub and it is reusable. The artifact consists of python scripts, trained deep neural network model files and data required for running the experiments. It is also provided as a VirtualBox VM image for reproducing the paper results. Users should be familiar with using VirtualBox software and Linux platform to reproduce or reuse the artifact.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {205–206},
numpages = {2},
keywords = {test generation, test coverage, input validation, deep neural networks},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3559744.3559750,
author = {Lopes Da Silva, Dennis and Lordello Chaim, Marcos and Amario De Souza, Higor},
title = {Data flow Subsumption and its Impact on Spectrum-based Fault Localization},
year = {2022},
isbn = {9781450397537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3559744.3559750},
doi = {10.1145/3559744.3559750},
abstract = {Debugging aims at finding and correcting software defects. To help the developer, fault localization techniques were developed using association metrics and code coverage data spectra to identify the most suspicious code snippets. They assist the developer by means of a ranking of the most suspicious spectra that guides the developer in his or her ”hunt” for defects. These techniques, when based on data flow spectra, use definition use associations (DUA) for ranking calculation. However, the coverage of given DUAs often guarantees the coverage of other DUAs, in a relationship between DUAs called subsumption. In practice, the subsumption relationship means that if a given DUA is covered, others are also guaranteed to be covered in certain conditions. Based on the subsumption property, this work presents an experiment in which fault localization effectiveness is assessed using only the spectra of the set of unconstrained DUAs, that is, the minimal set of DUAs that may guarantee coverage of all other DUAs of the software under test. For this experiment, we use a subset of programs of the Defects4J repository, data flow spectra, and the Ochiai association metric. Our results compare the rankings produced by the set of unconstrained DUAs against those produced by all DUAs for fault localization. They indicate that most of the faults reached by DUA spectra can be found by inspecting only the unconstrained DUAs.},
booktitle = {Proceedings of the 7th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {39–48},
numpages = {10},
keywords = {subsumption, software testing, fault localization, data flow spectra},
location = {Uberlandia, Brazil},
series = {SAST '22}
}

@inproceedings{10.1109/ASE51524.2021.9678706,
author = {Applis, Leonhard and Panichella, Annibale and van Deursen, Arie},
title = {Assessing robustness of ML-based program analysis tools using metamorphic program transformations},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678706},
doi = {10.1109/ASE51524.2021.9678706},
abstract = {Metamorphic testing is a well-established testing technique that has been successfully applied in various domains, including testing deep learning models to assess their robustness against data noise or malicious input. Currently, metamorphic testing approaches for machine learning (ML) models focused on image processing and object recognition tasks. Hence, these approaches cannot be applied to ML targeting program analysis tasks. In this paper, we extend metamorphic testing approaches for ML models targeting software programs. We present Lampion, a novel testing framework that applies (semantics preserving) meta-morphic transformations on the test datasets. Lampion produces new code snippets equivalent to the original test set but different in their identifiers or syntactic structure. We evaluate Lampion against CodeBERT, a state-of-the-art ML model for Code-To-Text tasks that creates Javadoc summaries for given Java methods. Our results show that simple transformations significantly impact the target model behavior, providing additional information on the models reasoning apart from the classic performance metric.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1377–1381},
numpages = {5},
location = {Melbourne, Australia},
series = {ASE '21}
}

@proceedings{10.1145/3641343,
title = {ICEITSA '23: Proceedings of the 3rd International Conference on Electronic Information Technology and Smart Agriculture},
year = {2023},
isbn = {9798400716775},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sanya, China}
}

@inproceedings{10.1145/3551349.3561160,
author = {Rafi, Tahmid and Zhang, Xueling and Wang, Xiaoyin},
title = {PredART: Towards Automatic Oracle Prediction of Object Placements in Augmented Reality Testing},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3561160},
doi = {10.1145/3551349.3561160},
abstract = {While the emerging Augmented Reality (AR) technique allows a lot of new application opportunities, from education and communication to gaming, current augmented apps often have complaints about their usability and/or user experience due to placement errors of virtual objects. Therefore, identifying noticeable placement errors is an important goal in the testing of AR apps. However, placement errors can only be perceived by human beings and may need to be confirmed by multiple users, making automatic testing very challenging. In this paper, we propose PredART, a novel approach to predict human ratings of virtual object placements that can be used as test oracles in automated AR testing. PredART is based on automatic screenshot sampling, crowd sourcing, and a hybrid neural network for image regression. The evaluation on a test set of 480 screenshots shows that our approach can achieve an accuracy of 85.0% and a mean absolute error, mean squared error, and root mean squared error of 0.047, 0.008, and 0.091, respectively.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {77},
numpages = {13},
keywords = {Virtual Objects, Placement Error, Augmented Reality},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3377812.3382175,
author = {Pan, Rangeet},
title = {Does fixing bug increase robustness in deep learning?},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3382175},
doi = {10.1145/3377812.3382175},
abstract = {Deep Learning (DL) based systems are utilized vastly. Developers update the code to fix the bugs in the system. How these code fixing techniques impacts the robustness of these systems has not been clear. Does fixing code increase the robustness? Do they deteriorate the learning capability of the DL based systems? To answer these questions, we studied 321 Stack Overflow posts based on a published dataset. In this study, we built a classification scheme to analyze how bug-fixes changed the robustness of the DL model and found that most of the bug-fixes can increase the robustness. We also found evidence of bug-fixing that decrease the robustness. Our preliminary result suggests that 12.5% and 2.4% of the bug-fixes in Stack Overflow posts caused the increase and the decrease of the robustness of DL models, respectively.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {146–148},
numpages = {3},
keywords = {robustness, deep neural networks, bugs, bug fix},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1145/3722229,
author = {AlOmar, Eman Abdullah},
title = {Nurturing Code Quality: Leveraging Static Analysis and Large Language Models for Software Quality in Education},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3722229},
doi = {10.1145/3722229},
abstract = {Large Language Models (LLMs), such as ChatGPT, have become widely popular for various software engineering tasks, including programming, testing, code review, and program comprehension. However, their impact on improving software quality in educational settings remains uncertain. This paper explores our experience teaching the use of Programming Mistake Detector (PMD) to foster a culture of bug fixing and leverage LLM to improve software quality in the classroom. This paper discusses the results of an experiment involving 155 submissions that carried out a code review activity of 1,658 rules. Our quantitative and qualitative analysis reveals that a set of PMD quality issues influences the acceptance or rejection of the issues, and design-related categories that take longer to resolve. Although students acknowledge the potential of using ChatGPT during code review, some skepticism persists. Further, constructing prompts for ChatGPT that possess clarity, complexity, and context nurtures vital learning outcomes, such as enhanced critical thinking, and among the 1,658 issues analyzed, 93% of students indicated that ChatGPT did not identify any additional issues beyond those detected by PMD. Conversations between students and ChatGPT encompass five categories, including ChatGPT’s use of affirmation phrases like ‘certainly’ regarding bug fixing decisions, and apology phrases such as ‘apologize’ when resolving challenges. Through this experiment, we demonstrate that code review can become an integral part of the educational computing curriculum. We envision our findings to enable educators to support students with effective code review strategies, increasing awareness of LLMs, and promoting software quality in education.},
note = {Just Accepted},
journal = {ACM Trans. Comput. Educ.},
month = mar,
keywords = {large language models, education, bugfix, static analysis, code quality}
}

@inproceedings{10.1145/3702138.3702146,
author = {Yuan, Zhidan and Wang, Zhuangzhuang and Wu, Enze and Huang, Tao and Chen, Yingying},
title = {Empirical Studies on Failure Prediction for Distributed Systems Based on Feature Selection},
year = {2025},
isbn = {9798400717543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702138.3702146},
doi = {10.1145/3702138.3702146},
abstract = {Distributed system failure prediction identifies potential failures by constructing machine learning models based on Key Performance Indicator (KPI) data. However, due to the diversity of KPI metrics, not all metrics are relevant to specific failures. This lead to redundancy in the KPI metrics used to construct the models. To address this issue, we investigate the impact of filter-based ranking feature selection methods on distributed system failure prediction. In our empirical study, we conduct experiments based on the ZTE dataset by using six feature selection methods and four tree-based models. We explore the effects of different feature selection methods and selection ratios on model performance. Additionally, we study whether feature selection methods can effectively identify important KPI metrics from the perspective of model interpretability. The results indicate that compared to not using feature selection methods, employing such methods allows for the construction of effective failure prediction models with only a subset of KPI metrics. Model performance improves with an increase in feature selection ratio until reaching a point of stabilization. Moreover, feature selection methods effectively identify important KPI metrics within the dataset.},
booktitle = {Proceeding of the 2024 5th Asia Service Sciences and Software Engineering Conference},
pages = {43–52},
numpages = {10},
keywords = {Empirical Studies, Feature Selection, KPI Metrics Data, Model Interpretability},
location = {
},
series = {ASSE '24}
}

@article{10.1145/3660826,
author = {Yan, Chuan and Meng, Mark Huasong and Xie, Fuman and Bai, Guangdong},
title = {Investigating Documented Privacy Changes in Android OS},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660826},
doi = {10.1145/3660826},
abstract = {Android has empowered third-party apps to access data and services on mobile devices since its genesis.This involves a wide spectrum of user privacy-sensitive data, such as the device ID and location. In recent years, Android has taken proactive measures to adapt its access control policies for such data, in response to the increasingly strict privacy protection regulations around the world. When each new Android version is released, its privacy changes induced by the version evolution are transparently disclosed, and we refer to them as documented privacy changes (DPCs). Implementing DPCs in Android OS is a non-trivial task, due to not only the dispersed nature of those access control points within the OS, but also the challenges posed by backward compatibility. As a result, whether the actual access control enforcement in the OS implementations aligns with the disclosed DPCs becomes a critical concern.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
In this work, we conduct the first systematic study on the consistency between the operational behaviors of the OS at runtime and the officially disclosed DPCs. We propose DopCheck, an automatic DPC-driven testing framework equipped with a large language model (LLM) pipeline. It features a serial of analysis to extract the ontology from the privacy change documents written in natural language, and then harnesses the few-shot capability of LLMs to construct test cases for the detection of DPC-compliance issues in OS implementations. We apply DopCheck with the latest versions (10 to 13) of Android Open Source Project (AOSP). Our evaluation involving 79 privacy-sensitive APIs demonstrates that DopCheck can effectively recognize DPCs from Android documentation and generate rigorous test cases. Our study reveals that the status quo of the DPC-compliance issues is concerning, evidenced by 19 bugs identified by DopCheck. Notably, 12 of them are discovered in Android 13 and 6 in Android 10 for the first time, posing more than 35% Android users to the risk of privacy leakage. Our findings should raise an alert to Android users and app developers on the DPC compliance issues when using or developing an app, and would also underscore the necessity for Google to comprehensively validate the actual implementation against its privacy documentation prior to the OS release.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {119},
numpages = {24},
keywords = {Android, documentation, privacy, testing}
}

@inproceedings{10.1109/ICSE-SEIP.2017.13,
author = {Ziftci, Celal and Reardon, Jim},
title = {Who broke the build? automatically identifying changes that induce test failures in continuous integration at Google scale},
year = {2017},
isbn = {9781538627174},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP.2017.13},
doi = {10.1109/ICSE-SEIP.2017.13},
abstract = {Quickly identifying and fixing code changes that introduce regressions is critical to keep the momentum on software development, especially in very large scale software repositories with rapid development cycles, such as at Google. Identifying and fixing such regressions is one of the most expensive, tedious, and time consuming tasks in the software development life-cycle. Therefore, there is a high demand for automated techniques that can help developers identify such changes while minimizing manual human intervention. Various techniques have recently been proposed to identify such code changes. However, these techniques have shortcomings that make them unsuitable for rapid development cycles as at Google. In this paper, we propose a novel algorithm to identify code changes that introduce regressions, and discuss case studies performed at Google on 140 projects. Based on our case studies, our algorithm automatically identifies the change that introduced the regression in the top-5 among thousands of candidates 82% of the time, and provides considerable savings on manual work developers need to perform.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering: Software Engineering in Practice Track},
pages = {113–122},
numpages = {10},
keywords = {software tools, software testing, software fault diagnosis, software debugging, software changes, ranking},
location = {Buenos Aires, Argentina},
series = {ICSE-SEIP '17}
}

@article{10.1145/3661484,
author = {Kr\"{u}ger, Jacob and Li, Yi and Lossev, Kirill and Zhu, Chenguang and Chechik, Marsha and Berger, Thorsten and Rubin, Julia},
title = {A Meta-Study of Software-Change Intentions},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3661484},
doi = {10.1145/3661484},
abstract = {Every software system undergoes changes, for example, to add new features, fix bugs, or refactor code. The importance of understanding software changes has been widely recognized, resulting in various techniques and studies, for instance, on change-impact analysis or classifying developers’ activities. Since changes are triggered by developers’ intentions—something they plan or want to change in the system—many researchers have studied intentions behind changes. While there appears to be a consensus among software-engineering researchers and practitioners that knowing the intentions behind software changes is important, it is not clear how developers can actually benefit from this knowledge. In fact, there is no consolidated, recent overview of the state of the art on software-change intentions (SCIs) and their relevance for software engineering. We present a meta-study of 122 publications, which we used to derive a categorization of SCIs and to discuss motivations, evidence, and techniques relating to SCIs. Unfortunately, we found that individual pieces of research are often disconnected from each other, because a common understanding is missing. Similarly, some publications showcase the potential of knowing SCIs, but more substantial research to understand the practical benefits of knowing SCIs is needed. Our contributions can help researchers and practitioners improve their understanding of SCIs and how SCIs can aid software engineering tasks.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {300},
numpages = {41},
keywords = {Intentions, software evolution, change management, version control}
}

@inproceedings{10.1145/3676536.3676755,
author = {Schlaegl, Manfred and Grosse, Daniel},
title = {Single Instruction Isolation for RISC-V Vector Test Failures},
year = {2025},
isbn = {9798400710773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676536.3676755},
doi = {10.1145/3676536.3676755},
abstract = {Testing complex RISC-V extensions such as RISC-V Vector (RVV) with its 600+ highly configurable instructions is crucial. For this reason, test suites have been developed over the last years, including both hand-written and automatically generated tests. Although the process of running these tests is often highly automated, a significant portion of the work, namely the result analysis, has to be conducted manually after the run.This paper introduces the modular, open-source framework RVVTS for positive and negative testing of RVV implementations, featuring a novel technique called Single Instruction Isolation with Code Minimization, which significantly reduces manual result analysis of failing tests. We demonstrate the effectiveness of RVVTS by automatically generating and applying test sets to the RISC-V VP++ Virtual Prototype and the QEMU emulator, achieving a functional coverage of &gt;94%. For RISC-V VP++, our framework detects and minimizes ~1, 849 failures and associate them with 10 isolated, failing instructions. Similarly, for QEMU, it detects ~19k failures and relates them to 168 instructions for debugging. Overall, we confirmed 3 new bugs in the RISC-V VP++ and 2 in QEMU (and 7 more are to be analyzed).},
booktitle = {Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design},
articleno = {156},
numpages = {9},
location = {Newark Liberty International Airport Marriott, New York, NY, USA},
series = {ICCAD '24}
}

@inproceedings{10.1109/ICSSP.2019.00011,
author = {Kapur, Ritu and Sodhi, Balwinder},
title = {Towards a knowledge warehouse and expert system for the automation of SDLC tasks},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSSP.2019.00011},
doi = {10.1109/ICSSP.2019.00011},
abstract = {Cost of a skilled and competent software developer is high, and it is desirable to minimize dependency on such costly human resources. One of the ways to minimize such costs is via automation of various software development tasks.Recent advances in Artificial Intelligence (AI) and the availability of a large volume of knowledge bearing data at various software development related venues present a ripe opportunity for building tools that can automate software development tasks. For instance, there is significant latent knowledge present in raw or unstructured data associated with items such as source files, code commit logs, defect reports, comments, and so on, available in the Open Source Software (OSS) repositories.We aim to leverage such knowledge-bearing data, the latest advances in AI and hardware to create knowledge warehouses and expert systems for the software development domain. Such tools can help in building applications for performing various software development tasks such as defect prediction, effort estimation, code review, etc.},
booktitle = {Proceedings of the International Conference on Software and System Processes},
pages = {5–8},
numpages = {4},
keywords = {supervised learning, software maintenance, data mining, automated software engineering},
location = {Montreal, Quebec, Canada},
series = {ICSSP '19}
}

@inproceedings{10.1145/3510003.3510146,
author = {Yang, Zhou and Shi, Jieke and He, Junda and Lo, David},
title = {Natural attack for pre-trained models of code},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510146},
doi = {10.1145/3510003.3510146},
abstract = {Pre-trained models of code have achieved success in many important software engineering tasks. However, these powerful models are vulnerable to adversarial attacks that slightly perturb model inputs to make a victim model produce wrong outputs. Current works mainly attack models of code with examples that preserve operational program semantics but ignore a fundamental requirement for adversarial example generation: perturbations should be natural to human judges, which we refer to as naturalness requirement.In this paper, we propose ALERT (Naturalness Aware Attack), a black-box attack that adversarially transforms inputs to make victim models produce wrong outputs. Different from prior works, this paper considers the natural semantic of generated examples at the same time as preserving the operational semantic of original inputs. Our user study demonstrates that human developers consistently consider that adversarial examples generated by ALERT are more natural than those generated by the state-of-the-art work by Zhang et al. that ignores the naturalness requirement. On attacking CodeBERT, our approach can achieve attack success rates of 53.62%, 27.79%, and 35.78% across three downstream tasks: vulnerability prediction, clone detection and code authorship attribution. On GraphCodeBERT, our approach can achieve average success rates of 76.95%, 7.96% and 61.47% on the three tasks. The above outperforms the baseline by 14.07% and 18.56% on the two pre-trained models on average. Finally, we investigated the value of the generated adversarial examples to harden victim models through an adversarial fine-tuning procedure and demonstrated the accuracy of CodeBERT and GraphCodeBERT against ALERT-generated adversarial examples increased by 87.59% and 92.32%, respectively.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1482–1493},
numpages = {12},
keywords = {adversarial attack, genetic algorithm, pre-trained models},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/1835804.1835822,
author = {Maxwell, Evan K. and Back, Godmar and Ramakrishnan, Naren},
title = {Diagnosing memory leaks using graph mining on heap dumps},
year = {2010},
isbn = {9781450300551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1835804.1835822},
doi = {10.1145/1835804.1835822},
abstract = {Memory leaks are caused by software programs that prevent the reclamation of memory that is no longer in use. They can cause significant slowdowns, exhaustion of available storage space and, eventually, application crashes. Detecting memory leaks is challenging because real-world applications are built on multiple layers of software frameworks, making it difficult for a developer to know whether observed references to objects are legitimate or the cause of a leak. We present a graph mining solution to this problem wherein we analyze heap dumps to automatically identify subgraphs which could represent potential memory leak sources. Although heap dumps are commonly analyzed in existing heap profiling tools, our work is the first to apply a graph grammar mining solution to this problem. Unlike classical graph mining work, we show that it suffices to mine the dominator tree of the heap dump, which is significantly smaller than the underlying graph. Our approach identifies not just leaking candidates and their structure, but also provides aggregate information about the access path to the leaks. We demonstrate several synthetic as well as real-world examples of heap dumps for which our approach provides more insight into the problem than state-of-the-art tools such as Eclipse's MAT.},
booktitle = {Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {115–124},
numpages = {10},
keywords = {memory leaks, heap profiling, graph mining, graph grammars, dominator tree},
location = {Washington, DC, USA},
series = {KDD '10}
}

@article{10.1145/3635709,
author = {Giamattei, Luca and Guerriero, Antonio and Pietrantuono, Roberto and Russo, Stefano},
title = {Causality-driven Testing of Autonomous Driving Systems},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3635709},
doi = {10.1145/3635709},
abstract = {Testing Autonomous Driving Systems (ADS) is essential for safe development of self-driving cars. For thorough and realistic testing, ADS are usually embedded in a simulator and tested in interaction with the simulated environment. However, their high complexity and the multiple safety requirements lead to costly and ineffective testing. Recent techniques exploit many-objective strategies and ML to efficiently search the huge input space. Despite the indubitable advances, the need for smartening the search keep being pressing. This article presents CART (CAusal-Reasoning-driven Testing), a new technique that formulates testing as a causal reasoning task. Learning causation, unlike correlation, allows assessing the effect of actively changing an input on the output, net of possible confounding variables. CART first infers the causal relations between test inputs and outputs, then looks for promising tests by querying the learnt model. Only tests suggested by the model are run on the simulator. An extensive empirical evaluation, using Pylot as ADS and CARLA as simulator, compares CART with state-of-the-art algorithms used recently on ADS. CART shows a significant gain in exposing more safety violations and does so more efficiently. More broadly, the work opens to a wider exploitation of causal learning beside (or on top of) ML for testing-related tasks.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {74},
numpages = {35},
keywords = {Self-driving cars, autonomous vehicles, AI testing, search-based software testing, causal reasoning}
}

@article{10.1145/3643727,
author = {Chen, Xiangping and Xu, Furen and Huang, Yuan and Zhang, Neng and Zheng, Zibin},
title = {JIT-Smart: A Multi-task Learning Framework for Just-in-Time Defect Prediction and Localization},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643727},
doi = {10.1145/3643727},
abstract = {Just-in-time defect prediction (JIT-DP) is used to predict the defect-proneness of a commit and just-in-time defect localization (JIT-DL) is used to locate the exact buggy positions (defective lines) in a commit. Recently, various JIT-DP and JIT-DL techniques have been proposed, while most of them use a post-mortem way (e.g., code entropy, attention weight, LIME) to achieve the JIT-DL goal based on the prediction results in JIT-DP. These methods do not utilize the label information of the defective code lines during model building. In this paper, we propose a unified model JIT-Smart, which makes the training process of just-in-time defect prediction and localization tasks a mutually reinforcing multi-task learning process. Specifically, we design a novel defect localization network (DLN), which explicitly introduces the label information of defective code lines for supervised learning in JIT-DL with considering the class imbalance issue. To further investigate the accuracy and cost-effectiveness of JIT-Smart, we compare JIT-Smart with 7 state-of-the-art baselines under 5 commit-level and 5 line-level evaluation metrics in JIT-DP and JIT-DL. The results demonstrate that JIT-Smart is statistically better than all the state-of-the-art baselines in JIT-DP and JIT-DL. In JIT-DP, at the median value, JIT-Smart achieves F1-Score of 0.475, AUC of 0.886, Recall@20%Effort of 0.823, Effort@20%Recall of 0.01 and Popt of 0.942 and improves the baselines by 19.89%-702.74%, 1.23%-31.34%, 9.44%-33.16%, 21.6%-53.82% and 1.94%-34.89%, respectively . In JIT-DL, at the median value, JIT-Smart achieves Top-5 Accuracy of 0.539 and Top-10 Accuracy of 0.396, Recall@20%Effortline of 0.726, Effort@20%Recallline of 0.087 and IFAline of 0.098 and improves the baselines by 101.83%-178.35%, 101.01%-277.31%, 257.88%-404.63%, 71.91%-74.31% and 99.11%-99.41%, respectively. Statistical analysis shows that our JIT-Smart performs more stably than the best-performing model. Besides, JIT-Smart also achieves the best performance compared with the state-of-the-art baselines in cross-project evaluation.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {1},
numpages = {23},
keywords = {Defect Localization, Defect Prediction, Just-In-Time, Multi-task Learning}
}

@inproceedings{10.5555/3507788.3507804,
author = {Ria and Grigoriou, Marios-Stavros and Kontogiannis, Kostas and Giammaria, Alberto and Brealey, Chris},
title = {Process-metrics trends analysis for evaluating file-level error proneness},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {Assessing the likelihood of a source code file being buggy or healthy in upcoming commits given its past behavior and its interaction with other files, has been an area where the software engineering community has paid significant attention over the years. Early efforts aimed on associating software metrics with maintainability indexes, while more recent efforts focused on the use of machine learning for classifying a software module as error prone or not. In most approaches to date, this analysis is primarily based on source code metrics or on information extracted from the system's source code, and to a lesser extend on information that relates to process metrics. In this paper, we propose a process-metrics based method for predicting the behavior of a file, based both on its GitHub commits and its interdependences with other co-committed files. More specifically, for each file and for each commit a file participates in, we compute a dependency score this file has with its other co-committed files. This score is appropriately amplified if the file is participating in a bug-fixing commit, or decayed over time if it does not. By examining, over several open source systems, the trend of that dependency score for every file as a product of time, for files whose outcome is known and that are used as gold standard, we report statistics which shed light on estimating the likelihood of whether these trends can predict the future behavior of a file or not.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {113–122},
numpages = {10},
location = {Toronto, Canada},
series = {CASCON '21}
}

@inproceedings{10.1109/ASE51524.2021.9678841,
author = {Li, Rui and Liu, Huai and Lou, Guannan and Zheng, Xi and Liu, Xiao and Chen, Tsong Yueh},
title = {Metamorphic testing on multi-module UAV systems},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678841},
doi = {10.1109/ASE51524.2021.9678841},
abstract = {Recent years have seen a rapid development of machine learning based multi-module unmanned aerial vehicle (UAV) systems. To address the oracle problem in autonomous systems, numerous studies have been conducted to use meta-morphic testing to automatically generate test scenes for various modules, e.g., those in self-driving cars. However, as most of the studies are based on unit testing including end-to-end model-based testing, a similar testing approach may not be equally effective for UAV systems where multiple modules are working closely together. Therefore, in this paper, instead of unit testing, we propose a novel metamorphic system testing framework for UAV, named MSTU, to detect the defects in multi-module UAV systems. A preliminary evaluation plan to apply MSTU on an emerging autonomous multi-module UAV system is also presented to demonstrate the feasibility of the proposed testing framework.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1171–1173},
numpages = {3},
keywords = {system testing, software testing and verification, multi-module UAV system, metamorphic testing},
location = {Melbourne, Australia},
series = {ASE '21}
}

@article{10.1145/3579851,
author = {Greca, Renan and Miranda, Breno and Bertolino, Antonia},
title = {State of Practical Applicability of Regression Testing Research: A Live Systematic Literature Review},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {13s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3579851},
doi = {10.1145/3579851},
abstract = {Context: Software regression testing refers to rerunning test cases after the system under test is modified, ascertaining that the changes have not (re-)introduced failures. Not all researchers’ approaches consider applicability and scalability concerns, and not many have produced an impact in practice. Objective: One goal is to investigate industrial relevance and applicability of proposed approaches. Another is providing a live review, open to continuous updates by the community. Method: A systematic review of regression testing studies that are clearly motivated by or validated against industrial relevance and applicability is conducted. It is complemented by follow-up surveys with authors of the selected papers and 23 practitioners. Results: A set of 79 primary studies published between 2016–2022 is collected and classified according to approaches and metrics. Aspects relative to their relevance and impact are discussed, also based on their authors’ feedback. All the data are made available from the live repository that accompanies the study. Conclusions: While widely motivated by industrial relevance and applicability, not many approaches are evaluated in industrial or large-scale open-source systems, and even fewer approaches have been adopted in practice. Some challenges hindering the implementation of relevant approaches are synthesized, also based on the practitioners’ feedback.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {274},
numpages = {36},
keywords = {systematic literature review, test suite amplification, test suite reduction, test case prioritization, test case selection, Regression Testing}
}

@inproceedings{10.1145/3611643.3616289,
author = {Jain, Kush and Alon, Uri and Groce, Alex and Le Goues, Claire},
title = {Contextual Predictive Mutation Testing},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616289},
doi = {10.1145/3611643.3616289},
abstract = {Mutation testing is a powerful technique for assessing and improving test suite quality that artificially introduces bugs and checks whether the test suites catch them. However, it is also computationally expensive and thus does not scale to large systems and projects. One promising recent approach to tackling this scalability problem uses machine learning to predict whether the tests will detect the synthetic bugs, without actually running those tests. However, existing predictive mutation testing approaches still misclassify 33% of detection outcomes on a randomly sampled set of mutant-test suite pairs. We introduce MutationBERT, an approach for predictive mutation testing that simultaneously encodes the source method mutation and test method, capturing key context in the input representation. Thanks to its higher precision, MutationBERT saves 33% of the time spent by a prior approach on checking/verifying live mutants. MutationBERT, also outperforms the state-of-the-art in both same project and cross project settings, with meaningful improvements in precision, recall, and F1 score. We validate our input representation, and aggregation approaches for lifting predictions from the test matrix level to the test suite level, finding similar improvements in performance. MutationBERT not only enhances the state-of-the-art in predictive mutation testing, but also presents practical benefits for real-world applications, both in saving developer time and finding hard to detect mutants that prior approaches do not.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {250–261},
numpages = {12},
keywords = {code coverage, mutation analysis, test oracles},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/2593929.2600116,
author = {Harman, Mark and Jia, Yue and Langdon, William B. and Petke, Justyna and Moghadam, Iman Hemati and Yoo, Shin and Wu, Fan},
title = {Genetic improvement for adaptive software engineering (keynote)},
year = {2014},
isbn = {9781450328647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593929.2600116},
doi = {10.1145/2593929.2600116},
abstract = {This paper presents a brief outline of an approach to online genetic improvement. We argue that existing progress in genetic improvement can be exploited to support adaptivity. We illustrate our proposed approach with a 'dreaming smart device' example that combines online and offline machine learning and optimisation.},
booktitle = {Proceedings of the 9th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {1–4},
numpages = {4},
keywords = {Search Based Software Engineering, Machine Learning, Genetic Improvement, Artificial Intelligence},
location = {Hyderabad, India},
series = {SEAMS 2014}
}

@inproceedings{10.1145/3639478.3639781,
author = {Duque-Torres, Alejandra},
title = {Selecting and Constraining Metamorphic Relations},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639781},
doi = {10.1145/3639478.3639781},
abstract = {Software testing is a critical aspect of ensuring the reliability and quality of software systems. However, it often poses challenges, particularly in determining the expected output of a System Under Test (SUT) for a given set of inputs, a problem commonly referred to as the test oracle problem. Metamorphic Testing (MT) offers a promising solution to the test oracle problem by examining the relations between input-output pairs in consecutive executions of the SUT. These relations, referred to as Metamorphic Relations (MRs), define the expected changes in the output when specific changes are made to the input. Our research is focused on developing methods and tools to assist testers in the selection of MRs, the definition of constraints, and providing explanations for MR outcomes. The research is divided in three parts. The first part focuses on MR collection and description, entailing the creation of a comprehensive repository of MRs from various sources. A standardised MR representation is devised to promote machine-readability and wide-ranging applicability. The second part introduces MetraTrimmer, a test-data-driven approach for systematically selecting and constraining MRs. This approach acknowledges that MRs may not be universally applicable to all test data space. The final part, evaluation and validation, encompasses empirical studies aimed at assessing the effectiveness of the developed methods and validating their suitability for real-world regression testing scenarios. Through this research, we aim to advance the automation of MR generation, enhance the understanding of MR violations, and facilitate their effective application in regression testing.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {212–216},
numpages = {5},
keywords = {test oracle, metamorphic testing, metamorphic relations, test data, pattern mining},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3549036.3562055,
author = {Garc\'{\i}a de la Barrera Amo, Antonio and Serrano, Manuel A. and Garc\'{\i}a Rodr\'{\i}guez de Guzm\'{a}n, Ignacio and Polo, Macario and Piattini, Mario},
title = {Automatic generation of test circuits for the verification of Quantum deterministic algorithms},
year = {2022},
isbn = {9781450394581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549036.3562055},
doi = {10.1145/3549036.3562055},
abstract = {Quantum computing will make it possible to exponentially accelerate the performance of a wide range of computational problems, such as cryptography, machine learning or chemical simulation. However, the quantum potential is not only a matter of hardware, but also of software. Therefore, this new paradigm has an impact yet to be explored on software development processes and techniques, and the adaptation of classical software engineering to the new classical/quantum hybrid systems raises a number of important challenges: a new Quantum Software Engineering is therefore needed. Specifically, and focusing on quantum software quality, software verification remains an open research question, as its novelty and complexity make quantum software development a particularly error-prone process. Most current approaches to test-driven verification rely heavily on simulations, which is a problem due to the lack of scalability of simulators running on classical computers. To address this shortcoming, we define the concept of a "Quantum Test Case", and then present a method to test quantum circuits on real machines, without using simulation test functionalities such as amplitude calculation or non-destructive measurement. This is achieved by automatically generating a Quantum Test Case, which wraps the circuit under test and performs the verification. We also present the process to run a set of tests on a circuit with this method, along with an example to illustrate the technique.},
booktitle = {Proceedings of the 1st International Workshop on Quantum Programming for Software Engineering},
pages = {1–6},
numpages = {6},
keywords = {Quantum Testing, Quantum Test Case, Quantum Software Engineering, Quantum Computing},
location = {Singapore, Singapore},
series = {QP4SE 2022}
}

@inproceedings{10.1145/3540250.3549086,
author = {Ibrahimzada, Ali Reza and Varli, Yigit and Tekinoglu, Dilara and Jabbarvand, Reyhaneh},
title = {Perfect is the enemy of test oracle},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549086},
doi = {10.1145/3540250.3549086},
abstract = {Automation of test oracles is one of the most challenging facets of software testing, but remains comparatively less addressed compared to automated test input generation. Test oracles rely on a ground-truth that can distinguish between the correct and buggy behavior to determine whether a test fails (detects a bug) or passes. What makes the oracle problem challenging and undecidable is the assumption that the ground-truth should know the exact expected, correct, or buggy behavior. However, we argue that one can still build an accurate oracle without knowing the exact correct or buggy behavior, but how these two might differ. This paper presents , a learning-based approach that in the absence of test assertions or other types of oracle, can determine whether a unit test passes or fails on a given method under test (MUT). To build the ground-truth, jointly embeds unit tests and the implementation of MUTs into a unified vector space, in such a way that the neural representation of tests are similar to that of MUTs they pass on them, but dissimilar to MUTs they fail on them. The classifier built on top of this vector representation serves as the oracle to generate “fail” labels, when test inputs detect a bug in MUT or “pass” labels, otherwise. Our extensive experiments on applying to more than 5K unit tests from a diverse set of open-source Java projects show that the produced oracle is (1) effective in predicting the fail or pass labels, achieving an overall accuracy, precision, recall, and F1 measure of 93%, 86%, 94%, and 90%, (2) generalizable, predicting the labels for the unit test of projects that were not in training or validation set with negligible performance drop, and (3) efficient, detecting the existence of bugs in only 6.5 milliseconds on average. Moreover, by interpreting the neural model and looking at it beyond a closed-box solution, we confirm that the oracle is valid, i.e., it predicts the labels through learning relevant features.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {70–81},
numpages = {12},
keywords = {Test Oracle, Test Automation, Software Testing, Deep Learning},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@article{10.1145/3654438,
author = {Guglielmi, Emanuela and Rosa, Giovanni and Scalabrino, Simone and Bavota, Gabriele and Oliveto, Rocco},
title = {Help Them Understand: Testing and Improving Voice User Interfaces},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3654438},
doi = {10.1145/3654438},
abstract = {Voice-based virtual assistants are becoming increasingly popular. Such systems provide frameworks to developers for building custom apps. End-users can interact with such apps through a Voice User Interface (VUI), which allows the user to use natural language commands to perform actions. Testing such apps is not trivial: The same command can be expressed in different semantically equivalent ways. In this article, we introduce VUI-UPSET, an approach that adapts chatbot-testing approaches to VUI-testing. We conducted an empirical study to understand how VUI-UPSET compares to two state-of-the-art approaches (i.e., a chatbot testing technique and ChatGPT) in terms of (i) correctness of the generated paraphrases, and (ii) capability of revealing bugs. To this aim, we analyzed 14,898 generated paraphrases for 40 Alexa Skills. Our results show that VUI-UPSET generates more bug-revealing paraphrases than the two baselines with, however, ChatGPT being the approach generating the highest percentage of correct paraphrases. We also tried to use the generated paraphrases to improve the skills. We tried to include in the voice interaction models of the skills (i) only the bug-revealing paraphrases, (ii) all the valid paraphrases. We observed that including only bug-revealing paraphrases is sometimes not sufficient to make all the tests pass.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {143},
numpages = {33},
keywords = {Voice user interfaces, software testing, NLP;}
}

@inproceedings{10.1145/3551349.3559505,
author = {Lv, Zhengwei and Peng, Chao and Zhang, Zhao and Su, Ting and Liu, Kai and Yang, Ping},
title = {Fastbot2: Reusable Automated Model-based GUI Testing for Android Enhanced by Reinforcement Learning},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3559505},
doi = {10.1145/3551349.3559505},
abstract = {We introduce a reusable automated model-based GUI testing technique for Android apps to accelerate the testing cycle. Our key insight is that the knowledge of event-activity transitions from the previous testing runs, i.e., executing which events can reach which activities, is valuable for guiding the follow-up testing runs to quickly cover major app functionalities. To this end, we propose (1) a probabilistic model to memorize and leverage this knowledge during testing, and (2) design a model-based guided testing strategy (enhanced by a reinforcement learning algorithm). We implemented our technique as an automated testing tool named Fastbot2. The evaluation on two popular industrial apps (with billions of user installations), Douyin and Toutiao, shows that Fastbot2 outperforms the state-of-the-art testing tools (Monkey, Ape and Stoat) in both activity coverage and fault detection in the context of continuous testing. To date, Fastbot2 has been deployed in the CI pipeline at ByteDance for nearly two years, and 50.8% of the developer-fixed crash bugs were reported by Fastbot2, which significantly improves app quality. Fastbot2 has been made publicly available to benefit the community at: https://github.com/bytedance/Fastbot_Android.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {135},
numpages = {5},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3566097.3567912,
author = {Moussa, Dina and Hefenbrock, Michael and M\"{u}nch, Christopher and Tahoori, Mehdi},
title = {Automatic Test Pattern Generation and Compaction for Deep Neural Networks},
year = {2023},
isbn = {9781450397834},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3566097.3567912},
doi = {10.1145/3566097.3567912},
abstract = {Deep Neural Networks (DNNs) have gained considerable attention lately due to their excellent performance on a wide range of recognition and classification tasks. Accordingly, fault detection in DNNs and their implementations plays a crucial role in the quality of DNN implementations to ensure that their post-mapping and infield accuracy matches with model accuracy. This paper proposes a functional-level automatic test pattern generation approach for DNNs. This is done by generating inputs which causes misclassification of the output class label in the presence of single or multiple faults. Furthermore, to obtain a smaller set of test patterns with full coverage, a heuristic algorithm as well as a test pattern clustering method using K-means were implemented. The experimental results showed that the proposed test patterns achieved the highest label misclassification and a high output deviation compared to state-of-the-art approaches.},
booktitle = {Proceedings of the 28th Asia and South Pacific Design Automation Conference},
pages = {436–441},
numpages = {6},
keywords = {test pattern generation, test compaction, functional faults, fault injection, deep neural networks},
location = {Tokyo, Japan},
series = {ASPDAC '23}
}

@inproceedings{10.1145/3531146.3533175,
author = {Lucchesi, Lydia R. and Kuhnert, Petra M. and Davis, Jenny L. and Xie, Lexing},
title = {Smallset Timelines: A Visual Representation of Data Preprocessing Decisions},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533175},
doi = {10.1145/3531146.3533175},
abstract = {Data preprocessing is a crucial stage in the data analysis pipeline, with both technical and social aspects to consider. Yet, the attention it receives is often lacking in research practice and dissemination. We present the Smallset Timeline, a visualisation to help reflect on and communicate data preprocessing decisions. A “Smallset” is a small selection of rows from the original dataset containing instances of dataset alterations. The Timeline is comprised of Smallset snapshots representing different points in the preprocessing stage and captions to describe the alterations visualised at each point. Edits, additions, and deletions to the dataset are highlighted with colour. We develop the R software package, smallsets, that can create Smallset Timelines from R and Python data preprocessing scripts. Constructing the figure asks practitioners to reflect on and revise decisions as necessary, while sharing it aims to make the process accessible to a diverse range of audiences. We present two case studies to illustrate use of the Smallset Timeline for visualising preprocessing decisions. Case studies include software defect data and income survey benchmark data, in which preprocessing affects levels of data loss and group fairness in prediction tasks, respectively. We envision Smallset Timelines as a go-to data provenance tool, enabling better documentation and communication of preprocessing tasks at large.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1136–1153},
numpages = {18},
keywords = {communication, data preprocessing, open-source software, reflexivity, visualization},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{10.1109/ICSE43902.2021.00042,
author = {Guerriero, Antonio and Pietrantuono, Roberto and Russo, Stefano},
title = {Operation is the hardest teacher: estimating DNN accuracy looking for mispredictions},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00042},
doi = {10.1109/ICSE43902.2021.00042},
abstract = {Deep Neural Networks (DNN) are typically tested for accuracy relying on a set of unlabelled real world data (operational dataset), from which a subset is selected, manually labelled and used as test suite. This subset is required to be small (due to manual labelling cost) yet to faithfully represent the operational context, with the resulting test suite containing roughly the same proportion of examples causing misprediction (i.e., failing test cases) as the operational dataset.However, while testing to estimate accuracy, it is desirable to also learn as much as possible from the failing tests in the operational dataset, since they inform about possible bugs of the DNN. A smart sampling strategy may allow to intentionally include in the test suite many examples causing misprediction, thus providing this way more valuable inputs for DNN improvement while preserving the ability to get trustworthy unbiased estimates.This paper presents a test selection technique (DeepEST) that actively looks for failing test cases in the operational dataset of a DNN, with the goal of assessing the DNN expected accuracy by a small and "informative" test suite (namely with a high number of mispredictions) for subsequent DNN improvement. Experiments with five subjects, combining four DNN models and three datasets, are described. The results show that DeepEST provides DNN accuracy estimates with precision close to (and often better than) those of existing sampling-based DNN testing techniques, while detecting from 5 to 30 times more mispredictions, with the same test suite size.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {348–358},
numpages = {11},
keywords = {Software testing, Artificial neural networks},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1145/3490489,
author = {Xie, Xiaofei and Li, Tianlin and Wang, Jian and Ma, Lei and Guo, Qing and Juefei-Xu, Felix and Liu, Yang},
title = {NPC: Neuron Path Coverage via Characterizing Decision Logic of Deep Neural Networks},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3490489},
doi = {10.1145/3490489},
abstract = {Deep learning has recently been widely applied to many applications across different domains, e.g., image classification and audio recognition. However, the quality of Deep Neural Networks (DNNs) still raises concerns in the practical operational environment, which calls for systematic testing, especially in safety-critical scenarios. Inspired by software testing, a number of structural coverage criteria are designed and proposed to measure the test adequacy of DNNs. However, due to the blackbox nature of DNN, the existing structural coverage criteria are difficult to interpret, making it hard to understand the underlying principles of these criteria. The relationship between the structural coverage and the decision logic of DNNs is unknown. Moreover, recent studies have further revealed the non-existence of correlation between the structural coverage and DNN defect detection, which further posts concerns on what a suitable DNN testing criterion should be.In this article, we propose the interpretable coverage criteria through constructing the decision structure of a DNN. Mirroring the control flow graph of the traditional program, we first extract a decision graph from a DNN based on its interpretation, where a path of the decision graph represents a decision logic of the DNN. Based on the control flow and data flow of the decision graph, we propose two variants of path coverage to measure the adequacy of the test cases in exercising the decision logic. The higher the path coverage, the more diverse decision logic the DNN is expected to be explored. Our large-scale evaluation results demonstrate that: The path in the decision graph is effective in characterizing the decision of the DNN, and the proposed coverage criteria are also sensitive with errors, including natural errors and adversarial examples, and strongly correlate with the output impartiality.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {47},
numpages = {27},
keywords = {model interpretation, testing coverage criteria, Deep learning testing}
}

@inproceedings{10.1145/3300115.3309516,
author = {Kangas, Vilma and Pirttinen, Nea and Nygren, Henrik and Leinonen, Juho and Hellas, Arto},
title = {Does Creating Programming Assignments with Tests Lead to Improved Performance in Writing Unit Tests?},
year = {2019},
isbn = {9781450362597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3300115.3309516},
doi = {10.1145/3300115.3309516},
abstract = {We have constructed a tool, CrowdSorcerer, in which students create programming assignments, their model solutions and associated test cases using a simple input-output format. We have used the tool as a part of an introductory programming course with normal course activities such as programming assignments and a final exam. In our work, we focus on whether creating programming assignments and associated tests correlate with students' performance in a testing-related exam question. We study this through an analysis of the quality of student-written tests within the tool, measured using the number of test cases, line coverage and mutation coverage, and students' performance in testing related exam question, measured using exam points. Finally, we study whether previous programming experience correlates with how students act within the tool and within the testing related exam question.},
booktitle = {Proceedings of the ACM Conference on Global Computing Education},
pages = {106–112},
numpages = {7},
keywords = {testing, educational data mining, crowdsourcing, assignment creation},
location = {Chengdu,Sichuan, China},
series = {CompEd '19}
}

@article{10.1145/3702974,
author = {Qian, Ruixiang and Zhang, Quanjun and Fang, Chunrong and Guo, Lihua and Chen, Zhenyu},
title = {FunFuzz: Greybox Fuzzing with Function Significance},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3702974},
doi = {10.1145/3702974},
abstract = {Greybox fuzzing is dedicated to revealing software bugs by maximizing code coverage. Concentrating on code coverage, greybox fuzzing effectively exposes bugs in real-world programs by continuously executing the program under test (PUT) with the test inputs generated from initial seeds, making it a popular software testing technique. Although powerful, the effectiveness of greybox fuzzing can be restricted in some cases. Ignoring the significant degrees of executed functions, traditional greybox fuzzing usually fails to identify significant seeds that execute more significant functions, and thus may assign similar energy to significant and trivial seeds when conducting power scheduling. As a result, the effectiveness of greybox fuzzing can be degraded due to wasting too much energy on trivial seeds.In this paper, we introduce function significance (FS) to measure the significant degrees of functions. Our key insight is that the influential functions that connect to many other functions are significant to greybox fuzzing as they provide more probabilities to reach previously unexplored code regions. To quantify FS, we conduct influence analysis upon the call graphs extracted from the PUTs to obtain the centrality values of function nodes. With FS as the significance measurement, we further propose FunFuzz, an FS-aware greybox fuzzing technique, to optimize significant seeds and tackle the aforementioned restriction. To this end, FunFuzz dynamically tracks the functions executed by a seed during fuzzing, and computes the significance score for the seed by accumulating the FS values of the functions executed by it. Based on the computed FS values, FunFuzz then takes an estimation-based power scheduling to assign more (or less) energy to seeds that achieve over-estimated (or under-estimated) significance scores. Specifically, the seed energy is adjusted by multiplying with a scale factor computed regarding the ratio of the actual significance score achieved by executing the seed and the estimated significance score predicted by a linear model constructed on-the-fly. To evaluate FunFuzz, we prototype it on top of AFL++ and conduct experiments with 15 programs, of which 10 are from common real-world projects and five are from Magma, and compare it to seven popular fuzzers. The experimental results obtained through fuzzing exceeding 40,800 CPU hours show that: (1) In terms of covering code, FunFuzz outperforms AFL++ by achieving 0.1% (sim) 18.4% more region coverage on 13 out of 15 targets. (2) In terms of finding bugs, FunFuzz unveils 114 unique crashes and 25 Magma bugs (which are derived from CVEs) in 20 trials of 24-hour fuzzing, which are the most compared to the competitor fuzzers and include 32 crashes and 1 Magma bug that the other fuzzers fail to discover. Besides the experiments focusing on code coverage and bug finding, we evaluate the key components of FunFuzz, namely the FS-centered estimation-based power scheduling and the lazy FS computation mechanism. The extensive evaluation not only suggests FunFuzz’s superiority in code coverage and bug finding, but also demonstrates the effectiveness of the two components.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
keywords = {Greybox Fuzzing, Function Significance, Influence Analysis}
}

@inproceedings{10.1145/1328279.1328287,
author = {Madhavan, Janaki T. and Whitehead, E. James},
title = {Predicting buggy changes inside an integrated development environment},
year = {2007},
isbn = {9781605580159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1328279.1328287},
doi = {10.1145/1328279.1328287},
abstract = {We present a tool that predicts whether the software under development inside an IDE has a bug. An IDE plugin performs this prediction, using the Change Classification technique to classify source code changes as buggy or clean during the editing session. Change Classification uses Support Vector Machines (SVM), a machine learning classifier algorithm, to classify changes to projects mined from their configuration management repository. This technique, besides being language independent and relatively accurate, can (a) classify a change immediately upon its completion and (b) use features extracted solely from the change delta (added, deleted) and the source code to predict buggy changes. Thus, integrating change classification within an IDE can predict potential bugs in the software as the developer edits the source code, ideally reducing the amount of time spent on fixing bugs later. To this end, we have developed a Change Classification plugin for Eclipse based on client-server architecture, described in this paper.},
booktitle = {Proceedings of the 2007 OOPSLA Workshop on Eclipse Technology EXchange},
pages = {36–40},
numpages = {5},
keywords = {integrated development environments, bug prediction},
location = {Montreal, Quebec, Canada},
series = {eclipse '07}
}

@inproceedings{10.1145/3679006.3685070,
author = {Sun, Chang-Ai and Xing, Jiayu and Li, Xiaobei and Zhang, Xiaoyi and Fu, An},
title = {Metamorphic Testing of Image Processing Applications: A General Framework and Optimization Strategies},
year = {2024},
isbn = {9798400711176},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3679006.3685070},
doi = {10.1145/3679006.3685070},
abstract = {Metamorphic testing (MT) is widely adopted for testing image processing applications. Although a variety of metamorphic relations (MRs) have been proposed, using all of them for testing will cost a large amount of computational resources. In addition, complex transformation operations are not well supported when generating follow-up test images based on MRs. To overcome these limitations, this study proposes a general MT framework for image processing applications, which employs CycleGAN to generate images that are very close to the realistic scenarios and leverages MRs for various categories of image processing applications. Two optimization strategies called EquivalentMR and SSampling are further proposed to reduce MRs and test images, respectively. A prototype tool called MT4I was developed. The experimental results showed that the proposed framework was capable of effectively testing various categories of image processing applications, while optimization strategies can reduce the amounts of MRs and test images without significantly jeopardizing the fault detection effectiveness.},
booktitle = {Proceedings of the 9th ACM International Workshop on Metamorphic Testing},
pages = {26–33},
numpages = {8},
keywords = {Fault Detection Effectiveness, Image Processing Applications, Metamorphic Testing, Software Testing},
location = {Vienna, Austria},
series = {MET 2024}
}

@inproceedings{10.1145/3387940.3391541,
author = {Zhang, Jie M.},
title = {Automatic Improvement of Machine Translation Using Mutamorphic Relation: Invited Talk Paper},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391541},
doi = {10.1145/3387940.3391541},
abstract = {This paper introduces Mutamorphic Relation for Machine Learning Testing. Mutamorphic Relation combines data mutation and metamorphic relations as test oracles for machine learning systems. These oracles can help achieve fully automatic testing as well as automatic repair of the machine learning models.The paper takes TransRepair as an example to show the effectiveness of Mutamorphic Relation in automatically testing and improving machine translators, TransRepair detects inconsistency bugs without access to human oracles. It then adopts probability-reference or cross-reference to post-process the translations, in a grey-box or black-box manner, to repair the inconsistencies. Manual inspection indicates that the translations repaired by TransRepair improve consistency in 87% of cases (degrading it in 2%), and that the repairs of have better translation acceptability in 27% of the cases (worse in 8%).},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {425–426},
numpages = {2},
keywords = {mutation testing, mutamorphic relation, metamorphic testing},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/2989238.2989242,
author = {Dehghan, Ali and Blincoe, Kelly and Damian, Daniela},
title = {A hybrid model for task completion effort estimation},
year = {2016},
isbn = {9781450343954},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2989238.2989242},
doi = {10.1145/2989238.2989242},
abstract = {Predicting time and effort of software task completion has been an active area of research for a long time. Previous studies have proposed predictive models based on either text data or metadata of software tasks to estimate either completion time or completion effort of software tasks, but there is a lack of focus in the literature on integrating all sets of attributes together to achieve better performing models. We first apply the previously proposed models on the datasets of two IBM commercial projects called RQM and RTC to find the best performing model in predicting task completion effort on each set of attributes. Then we propose an approach to create a hybrid model based on selected individual predictors to achieve more accurate and stable results in early prediction of task completion effort and to make sure the model is not bounded to some attributes and consequently is adoptable to a larger number of tasks. Categorizing task completion effort values into Low and High labels based on their measured median value, we show that our hybrid model provides 3-8% more accuracy in early prediction of task completion effort compared to the best individual predictors.},
booktitle = {Proceedings of the 2nd International Workshop on Software Analytics},
pages = {22–28},
numpages = {7},
keywords = {task completion effort, machine learning, ensemble learning, effort estimation, Mining software repositories},
location = {Seattle, WA, USA},
series = {SWAN 2016}
}

@inproceedings{10.1145/3624062.3624116,
author = {Schlueter, Benjamin and Calhoun, Jon and Poulos, Alexandra},
title = {Evaluating the Resiliency of Posits for Scientific Computing},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624116},
doi = {10.1145/3624062.3624116},
abstract = {IEEE-754 is the de-facto standard for the implementation of floating-point number systems in hardware. With the rise of machine-learning and mixed-precision computation, applications tolerate small inaccuracies in the data by using lower bit-width floating-point values to improve computational speed and memory bandwidth, but still rely on some variant of IEEE-754. Posits have been proposed as a drop-in replacement to the IEEE-754 floating-point standard. Recent work has suggested that posits can offer greater numerical accuracy and reproducibility than IEEE-754-compliant floating point numbers at a comparable architectural cost. There have been several studies which consider the use of posits and other floating-point implementations in hardware and software, but there is limited work examining this new number system from a reliability perspective. In this paper, we evaluate the resiliency of posits to inform hardware design for fault-tolerant systems. Using real-world scientific datasets, we conduct an extensive bit-flip fault injection campaign to explore the tradeoff between accuracy and sensitivity to silent data corruption (SDC) between posits and IEEE-754. Our analysis breaks down the impact of a bit flip on the various fields within both floating-point standards. Results show the presence of the regime reduces the number of bits that cause catastrophic error in posits compared to IEEE floats. This leads to overall less error in upper bit positions of posits, except in certain edge cases. We found flips in the sign bit usually cause more error in posits due to the effect sign has on posit magnitude, unlike in IEEE floats. We also found that unlike in IEEE floats, the posit exponent does not produce significant error due to its small, static two bit size. Finally, error in the posit fraction was found to be similar to that of IEEE floats.},
booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {477–487},
numpages = {11},
keywords = {IEEE-754, Posit, accuracy, floating-point, reliability, soft error},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@inproceedings{10.1145/3578527.3581748,
author = {Godboley, Sangharatna and Mohapatra, Durga Prasad},
title = {2nd Recent Advances in Program Analysis and Software Testing (RAPAST-2023)},
year = {2023},
isbn = {9798400700644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578527.3581748},
doi = {10.1145/3578527.3581748},
abstract = {The conference on Program Analysis and Software Testing is a special session under the workshop category which will be held in conjunction with ISEC-2023, Indian Institute of Information Technology, Allahabad Prayagraj, UP, India. It is in line with the scope of ISEC-2023 which will be specially focused on the theme of the event. The theme is based on the research work specific to the areas mentioned in the scope of this conference. RAPAST-2023 is expected to be a good avenue where the researchers from Academia and Industries will participate by presenting their most recent research findings and experimental results in the domain of the scope.},
booktitle = {Proceedings of the 16th Innovations in Software Engineering Conference},
articleno = {27},
numpages = {2},
keywords = {Software Testing, Program Analysis},
location = {Allahabad, India},
series = {ISEC '23}
}

@article{10.1145/3664604,
author = {Li, Yuechen and Pei, Hanyu and Huang, Linzhi and Yin, Beibei and Cai, Kai-Yuan},
title = {Automatic Repair of Quantum Programs via Unitary Operation},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3664604},
doi = {10.1145/3664604},
abstract = {With the continuous advancement of quantum computing (QC), the demand for high-quality quantum programs (QPs) is growing. To avoid program failure, in software engineering, the technology of automatic program repair (APR) employs appropriate patches to remove potential bugs without the intervention of a human. However, the method tailored for repairing defective QPs is still absent. This article proposes, to the best of our knowledge, a new APR method named UnitAR that can repair QPs via unitary operation automatically. Based on the characteristics of superposition and entanglement in QC, the article constructs an algebraic model and adopts a generate-and-validate approach for the repair procedure. Furthermore, the article presents two schemes that can respectively promote the efficiency of generating patches and guarantee the effectiveness of applying patches. For the purpose of evaluating the proposed method, the article selects 29 mutated versions as well as five real-world buggy programs as the objects and introduces two traditional APR approaches GenProg and TBar as baselines. According to the experiments, UnitAR can fix 23 buggy programs, and this method demonstrates the highest efficiency and effectiveness among three APR approaches. Besides, the experimental results further manifest the crucial roles of two constituents involved in the framework of UnitAR.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {154},
numpages = {43},
keywords = {Quantum computing, automatic program repair, quantum software engineering, unitary operation, software cybernetics, S-ADA}
}

@inproceedings{10.1109/ASE56229.2023.00126,
author = {Zheng, Haibin and Chen, Jinyin and Jin, Haibo},
title = {CertPri: Certifiable Prioritization for Deep Neural Networks via Movement Cost in Feature Space},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00126},
doi = {10.1109/ASE56229.2023.00126},
abstract = {Deep neural networks (DNNs) have demonstrated their outperformance in various software systems, but also exhibit misbehavior and even result in irreversible disasters. Therefore, it is crucial to identify the misbehavior of DNN-based software and improve DNNs' quality. Test input prioritization is one of the most appealing ways to guarantee DNNs' quality, which prioritizes test inputs so that more bug-revealing inputs can be identified earlier with limited time and manual labeling efforts. However, the existing prioritization methods are still limited from three aspects: certifiability, effectiveness, and generalizability. To overcome the challenges, we propose CertPri, a test input prioritization technique designed based on a movement cost perspective of test inputs in DNNs' feature space. CertPri differs from previous works in three key aspects: (1) certifiable - it provides a formal robustness guarantee for the movement cost; (2) effective - it leverages formally guaranteed movement costs to identify malicious bug-revealing inputs; and (3) generic - it can be applied to various tasks, data, models, and scenarios. Extensive evaluations across 2 tasks (i.e., classification and regression), 6 data forms, 4 model structures, and 2 scenarios (i.e., white-box and black-box) demonstrate CertPri's superior performance. For instance, it significantly improves 53.97% prioritization effectiveness on average compared with baselines. Its robustness and generalizability are 1.41~2.00 times and 1.33~3.39 times that of baselines on average, respectively. The code of CertPri is open-sourced at https://github.com/haibinzheng/CertPri.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1–13},
numpages = {13},
keywords = {deep neural network, test input prioritization, deep learning testing, movement cost, certifiable prioritization},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3641399.3641441,
author = {Yashavant, Chavhan Sujeet},
title = {SecSEC: Securing Smart Ethereum Contracts},
year = {2024},
isbn = {9798400717673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641399.3641441},
doi = {10.1145/3641399.3641441},
abstract = {Smart contracts are a driving force for the Ethereum blockchain. A smart contract is a code that resides on blockchain and executes when certain predetermined conditions are satisfied. Ethereum smart contracts handle ether (a cryptocurrency) equivalent to millions of dollars and other essential assets. A bug in the smart contract can cause financial loss and damage to essential assets. The Ethereum community has developed ample tools to detect bugs in smart contracts. However, the tools generate false reports. We plan to integrate existing tools and detect smart contract bugs by combining the best of each tool. We use the Logistic Regression model to combine the tools. We are working on a training dataset of around 40K real-world Ethereum smart contracts labelled with vulnerabilities for the Logistic Regression model. We also plan to improve individual tools by finding the drawbacks of each tool. Finally, we aim to develop a new tool that will be better than existing tools regarding precision and recall.},
booktitle = {Proceedings of the 17th Innovations in Software Engineering Conference},
articleno = {23},
numpages = {4},
keywords = {Annotated dataset, Ethereum, Smart contracts, Vulnerabilities},
location = {Bangalore, India},
series = {ISEC '24}
}

@inproceedings{10.1145/1150402.1150520,
author = {Forman, George and Kirshenbaum, Evan and Suermondt, Jaap},
title = {Pragmatic text mining: minimizing human effort to quantify many issues in call logs},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150520},
doi = {10.1145/1150402.1150520},
abstract = {We discuss our experiences in analyzing customer-support issues from the unstructured free-text fields of technical-support call logs. The identification of frequent issues and their accurate quantification is essential in order to track aggregate costs broken down by issue type, to appropriately target engineering resources, and to provide the best diagnosis, support and documentation for most common issues. We present a new set of techniques for doing this efficiently on an industrial scale, without requiring manual coding of calls in the call center. Our approach involves (1) a new text clustering method to identify common and emerging issues; (2) a method to rapidly train large numbers of categorizers in a practical, interactive manner; and (3) a method to accurately quantify categories, even in the face of inaccurate classifications and training sets that necessarily cannot match the class distribution of each new month's data. We present our methodology and a tool we developed and deployed that uses these methods for tracking ongoing support issues and discovering emerging issues at HP.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {852–861},
numpages = {10},
keywords = {text mining, text classification, supervised machine learning, quantification, log processing, applications},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@article{10.1145/3643767,
author = {Jiang, Weipeng and Zhai, Juan and Ma, Shiqing and Zhang, Xiaoyu and Shen, Chao},
title = {COSTELLO: Contrastive Testing for Embedding-Based Large Language Model as a Service Embeddings},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643767},
doi = {10.1145/3643767},
abstract = {Large language models have gained significant popularity and are often provided as a service (i.e., LLMaaS).  Companies like OpenAI and Google provide online APIs of LLMs to allow downstream users to create innovative applications.  Despite its popularity, LLM safety and quality assurance is a well-recognized concern in the real world, requiring extra efforts for testing these LLMs.  Unfortunately, while end-to-end services like ChatGPT have garnered rising attention in terms of testing, the LLMaaS embeddings have comparatively received less scrutiny.  We state the importance of testing and uncovering problematic individual embeddings without considering downstream applications.  The abstraction and non-interpretability of embedded vectors, combined with the black-box inaccessibility of LLMaaS, make testing a challenging puzzle.  This paper proposes COSTELLO, a black-box approach to reveal potential defects in abstract embedding vectors from LLMaaS by contrastive testing.  Our intuition is that high-quality LLMs can adequately capture the semantic relationships of the input texts and properly represent their relationships in the high-dimensional space.  For the given interface of LLMaaS and seed inputs, COSTELLO can automatically generate test suites and output words with potential problematic embeddings.  The idea is to synthesize contrastive samples with guidance, including positive and negative samples, by mutating seed inputs.  Our synthesis guide will leverage task-specific properties to control the mutation procedure and generate samples with known partial relationships in the high-dimensional space.  Thus, we can compare the expected relationship (oracle) and embedding distance (output of LLMs) to locate potential buggy cases.  We evaluate COSTELLO on 42 open-source (encoder-based) language models and two real-world commercial LLMaaS.  Experimental results show that COSTELLO can effectively detect semantic violations, where more than 62% of violations on average result in erroneous behaviors (e.g., unfairness) of downstream applications.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {41},
numpages = {23},
keywords = {Contrastive Testing, Embeddings, LLMaaS}
}

@article{10.1145/3617168,
author = {Li, Tianlin and Xie, Xiaofei and Wang, Jian and Guo, Qing and Liu, Aishan and Ma, Lei and Liu, Yang},
title = {Faire: Repairing Fairness of Neural Networks via Neuron Condition Synthesis},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3617168},
doi = {10.1145/3617168},
abstract = {Deep Neural Networks (DNNs) have achieved tremendous success in many applications, while it has been demonstrated that DNNs can exhibit some undesirable behaviors on concerns such as robustness, privacy, and other trustworthiness issues. Among them, fairness (i.e., non-discrimination) is one important property, especially when they are applied to some sensitive applications (e.g., finance and employment). However, DNNs easily learn spurious correlations between protected attributes (e.g., age, gender, race) and the classification task and develop discriminatory behaviors if the training data is imbalanced. Such discriminatory decisions in sensitive applications would introduce severe social impacts. To expose potential discrimination problems in DNNs before putting them in use, some testing techniques have been proposed to identify the discriminatory instances (i.e., instances that show defined discrimination1). However, how to repair DNNs after detecting such discrimination is still challenging. Existing techniques mainly rely on retraining on a large number of discriminatory instances generated by testing methods, which requires huge time overhead and makes the repairing inefficient.In this work, we propose the method Faire to effectively and efficiently repair the fairness issues of DNNs, without using additional data (e.g., discriminatory instances). Our basic idea is inspired by the traditional program repair method that synthesizes proper condition checking. To repair traditional programs, a typical method is to localize the program defects and repair the program logic by adding condition checking. Similarly, for DNNs, we try to understand the unfair logic and reformulate it with well-designed condition checking. In this article, we synthesize the condition that can reduce the effect of features relevant to the protected attributes in the DNN. Specifically, we first perform the neuron-based analysis and check the functionalities of neurons to identify neurons whose outputs could be regarded as features relevant to protected attributes and original tasks. Then a new condition layer is added after each hidden layer to penalize neurons that are accountable for the protected features (i.e., intermediate features relevant to protected attributes) and promote neurons that are accountable for the non-protected features (i.e., intermediate features relevant to original tasks). In sum, the repair rate2 of Faire reaches up to more than 99%, which outperforms other methods, and the whole repairing process only takes no more than 340 s. The evaluation results demonstrate that our approach can effectively and efficiently repair the individual discriminatory instances of the target model.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {21},
numpages = {24},
keywords = {model interpretation, individual discrimination, fairness, Deep learning repair}
}

@inproceedings{10.1145/3597503.3623311,
author = {Tappler, Martin and Pferscher, Andrea and Aichernig, Bernhard K. and K\"{o}nighofer, Bettina},
title = {Learning and Repair of Deep Reinforcement Learning Policies from Fuzz-Testing Data},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623311},
doi = {10.1145/3597503.3623311},
abstract = {Reinforcement learning from demonstrations (RLfD) is a promising approach to improve the exploration efficiency of reinforcement learning (RL) by learning from expert demonstrations in addition to interactions with the environment. In this paper, we propose a framework that combines techniques from search-based testing with RLfD with the goal to raise the level of dependability of RL policies and to reduce human engineering effort. Within our framework, we provide methods for efficiently training, evaluating, and repairing RL policies. Instead of relying on the costly collection of demonstrations from (human) experts, we automatically compute a diverse set of demonstrations via search-based fuzzing methods and use the fuzz demonstrations for RLfD. To evaluate the safety and robustness of the trained RL agent, we search for safety-critical scenarios in the black-box environment. Finally, when unsafe behavior is detected, we compute demonstrations through fuzz testing that represent safe behavior and use them to repair the policy. Our experiments show that our framework is able to efficiently learn high-performing and safe policies without requiring any expert knowledge.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {6},
numpages = {13},
keywords = {deep reinforcement learning, reinforcement learning from demonstrations, search-based software testing, policy repair},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@article{10.1145/3698810,
author = {Deng, Wenjing and Mang, Qiuyang and Zhang, Chengyu and Rigger, Manuel},
title = {Finding Logic Bugs in Spatial Database Engines via Affine Equivalent Inputs},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {6},
url = {https://doi.org/10.1145/3698810},
doi = {10.1145/3698810},
abstract = {Spatial Database Management Systems (SDBMSs) aim to store, manipulate, and retrieve spatial data. SDBMSs are employed in various modern applications, such as geographic information systems, computer-aided design tools, and location-based services. However, the presence of logic bugs in SDBMSs can lead to incorrect results, substantially undermining the reliability of these applications. Detecting logic bugs in SDBMSs is challenging due to the lack of ground truth for identifying incorrect results. In this paper, we propose an automated geometry-aware generator to generate high-quality SQL statements for SDBMSs and a novel concept named Affine Equivalent Inputs (AEI) to validate the results of SDBMSs. We implemented them as a tool named Spatter (Spatial DBMS Tester) for finding logic bugs in four popular SDBMSs: PostGIS, DuckDB Spatial, MySQL, and SQL Server. Our testing campaign detected 34 previously unknown and unique bugs in these SDBMSs, of which 30 have been confirmed, and 18 have already been fixed. Our testing efforts have been well appreciated by the developers. Experimental results demonstrate that the geometry-aware generator significantly outperforms a naive random-shape generator in detecting unique bugs, and AEI can identify 14 logic bugs in SDBMSs that were totally overlooked by previous methodologies.},
journal = {Proc. ACM Manag. Data},
month = dec,
articleno = {235},
numpages = {26},
keywords = {logic bug, spatial query processing}
}

@inproceedings{10.1145/3205651.3208262,
author = {Ebert, Samuel and Farhana, Effat and Heber, Steffen},
title = {A parallel island model for biogeography-based classification rule mining in julia},
year = {2018},
isbn = {9781450357647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205651.3208262},
doi = {10.1145/3205651.3208262},
abstract = {In this paper, we present a distributed island model implementation of biogeography-based optimization for classification rule mining (island BBO-RM). Island BBO-RM is an evolutionary algorithm for rule mining that uses Pittsburgh style classification rule encoding, which represents an entire ruleset (classifier) as a single chromosome. Our algorithm relies on biogeography-based optimization (BBO), an optimization technique that is inspired by species migration pattern between habitats. Biogeography-based optimization has been reported to perform well in various applications ranging from function optimization to image classification. A major limitation of evolutionary rule mining algorithms is their high computational cost and running time. To address this challenge, we have applied a distributed island model to parallelize the rule extraction phase via BBO. We have explored several different migration topologies and data windowing techniques. Our algorithm is implemented in Julia, a dynamic programming language designed for high-performance and parallel computation. Our results show that our distributed implementation is able to achieve considerable speedups when compared to a serial implementation. Without data windowing, we obtain speedups up to a factor of nine without a loss of classification accuracy. With data windowing, we obtain speedups up to a factor of 30 with a small loss of accuracy in some cases.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1284–1291},
numpages = {8},
keywords = {island model, genetics-based machine learning, evolutionary algorithms, biogeography-based optimization},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@inproceedings{10.1145/3387940.3392171,
author = {Nielebock, Sebastian and Heum\"{u}ller, Robert and Kr\"{u}ger, Jacob and Ortmeier, Frank},
title = {Using API-Embedding for API-Misuse Repair},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392171},
doi = {10.1145/3387940.3392171},
abstract = {Application Programming Interfaces (APIs) are a way to reuse existing functionalities of one application in another one. However, due to lacking knowledge on the correct usage of a particular API, developers sometimes commit misuses, causing unintended or faulty behavior. To detect and eventually repair such misuses automatically, inferring API usage patterns from real-world code is the state-of-the-art. A contradiction to an identified usage pattern denotes a misuse, while applying the pattern fixes the respective misuse. The success of this process heavily depends on the quality of the usage patterns and on the code from which these are inferred. Thus, a lack of code demonstrating the correct usage makes it impossible to detect and fix a misuse. In this paper, we discuss the potential of using machine-learning vector embeddings to improve automatic program repair and to extend it towards cross-API and cross-language repair. We illustrate our ideas using one particular technique for API-embedding (i.e., API2Vec) and describe the arising possibilities and challenges.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {1–2},
numpages = {2},
keywords = {Program Repair, API Misuse, API Embeddings},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@article{10.1145/3630011,
author = {Jiang, Jiajun and Yang, Junjie and Zhang, Yingyi and Wang, Zan and You, Hanmo and Chen, Junjie},
title = {A Post-training Framework for Improving the Performance of Deep Learning Models via Model Transformation},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3630011},
doi = {10.1145/3630011},
abstract = {Deep learning (DL) techniques have attracted much attention in recent years and have been applied to many application scenarios. To improve the performance of DL models regarding different properties, many approaches have been proposed in the past decades, such as improving the robustness and fairness of DL models to meet the requirements for practical use. Among existing approaches, post-training is an effective method that has been widely adopted in practice due to its high efficiency and good performance. Nevertheless, its performance is still limited due to the incompleteness of training data. Additionally, existing approaches are always specifically designed for certain tasks, such as improving model robustness, which cannot be used for other purposes.In this article, we aim to fill this gap and propose an effective and general post-training framework, which can be adapted to improve the model performance from different aspects. Specifically, it incorporates a novel model transformation technique that transforms a classification model into an isomorphic regression model for fine-tuning, which can effectively overcome the problem of incomplete training data by forcing the model to strengthen the memory of crucial input features and thus improve the model performance eventually. To evaluate the performance of our framework, we have adapted it to two emerging tasks for improving DL models, i.e., robustness and fairness improvement, and conducted extensive studies by comparing it with state-of-the-art approaches. The experimental results demonstrate that our framework is indeed general, as it is effective in both tasks. Specifically, in the task of robustness improvement, our approach Dare has achieved the best results on 61.1% cases (vs. 11.1% cases achieved by baselines). In the task of fairness improvement, our approach FMT can effectively improve the fairness without sacrificing the accuracy of the models.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {61},
numpages = {41},
keywords = {Deep neural network, delta debugging, model robustness, model fairness}
}

@proceedings{10.1145/3643690,
title = {IWSiB '24: Proceedings of the 7th ACM/IEEE International Workshop on Software-intensive Business},
year = {2024},
isbn = {9798400705717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The workshop brings together research communities working on softwareintensive business and software engineering. It aims to bridge the gap between the research in these areas. This year's theme, "Software Business in the Era of Generative Artificial Intelligence," reflects our focus on exploring how generative artificial intelligence (GenAI) and the related large language models (LLMs) impact the established practices of software engineering and software business.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1109/ASE56229.2023.00034,
author = {Humayun, Ahmad and Wu, Yaoxuan and Kim, Miryung and Gulzar, Muhammad Ali},
title = {NaturalFuzz: Natural Input Generation for Big Data Analytics},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00034},
doi = {10.1109/ASE56229.2023.00034},
abstract = {Fuzzing applies input mutations iteratively with the only goal of finding more bugs, resulting in synthetic tests that tend to lack realism. Big data analytics are expected to ingest real-world data as input. Therefore, when synthetic test data are not easily comprehensible, they are less likely to facilitate the downstream task of fixing errors. Our position is that fuzzing in this domain must achieve both high naturalness and high code coverage. We propose a new natural synthetic test generation tool for big data analytics, called NaturalFuzz. It generates both unstructured, semi-structured, and structured data with corresponding semantics such as 'zipcode' and 'age.' The key insights behind NaturalFuzz are two-fold. First, though existing test data may be small and lack coverage, we can grow this data to increase code coverage. Second, we can strategically mix constituent parts across different rows and columns to construct new realistic synthetic data by leveraging fine-grained data provenance.On commercial big data application benchmarks, NaturalFuzz achieves an additional 19.9% coverage and detects 1.9\texttimes{} more faults than a machine learning-based synthetic data generator (SDV) when generating comparably sized inputs. This is because an ML-based synthetic data generator does not consider which code branches are exercised by which input rows from which tables, while NaturalFuzz is able to select input rows that have a high potential to increase code coverage and mutate the selected data towards unseen, new program behavior. NaturalFuzz's test data is more realistic than the test data generated by two baseline fuzzers (BigFuzz and Jazzer), while increasing code coverage and fault detection potential. NaturalFuzz is the first fuzzing methodology with three benefits: (1) exclusively generate natural inputs, (2) fuzz multiple input sources simultaneously, and (3) find deeper semantics faults.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1592–1603},
numpages = {12},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3368089.3409696,
author = {Li, Zenan and Ma, Xiaoxing and Xu, Chang and Xu, Jingwei and Cao, Chun and L\"{u}, Jian},
title = {Operational calibration: debugging confidence errors for DNNs in the field},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409696},
doi = {10.1145/3368089.3409696},
abstract = {Trained DNN models are increasingly adopted as integral parts of software systems, but they often perform deficiently in the field. A particularly damaging problem is that DNN models often give false predictions with high confidence, due to the unavoidable slight divergences between operation data and training data. To minimize the loss caused by inaccurate confidence, operational calibration, i.e., calibrating the confidence function of a DNN classifier against its operation domain, becomes a necessary debugging step in the engineering of the whole system.  Operational calibration is difficult considering the limited budget of labeling operation data and the weak interpretability of DNN models. We propose a Bayesian approach to operational calibration that gradually corrects the confidence given by the model under calibration with a small number of labeled operation data deliberately selected from a larger set of unlabeled operation data. The approach is made effective and efficient by leveraging the locality of the learned representation of the DNN model and modeling the calibration as Gaussian Process Regression. Comprehensive experiments with various practical datasets and DNN models show that it significantly outperformed alternative methods, and in some difficult tasks it eliminated about 71% to 97% high-confidence (&gt;0.9) errors with only about 10% of the minimal amount of labeled operation data needed for practical learning techniques to barely work},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {901–913},
numpages = {13},
keywords = {Operational Calibration, Gaussian Process, Deep Neural Networks},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3540250.3569449,
author = {Santos, Joanna C. S. and Dolby, Julian},
title = {Program analysis using WALA (tutorial)},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3569449},
doi = {10.1145/3540250.3569449},
abstract = {Static analysis is widely used in research and practice for multiple purposes such as fault localization, vulnerability detection, code clone identification, code refactoring, optimization, etc. Since implementing static analyzers is a non-trivial task, engineers often rely on existing frameworks to implement their techniques. The IBM T.J. Watson Libraries for Analysis (WALA) is one of such frameworks that allows the analysis of multiple environments, such as Java bytecode (and related languages), JavaScript, Android, Python, etc. In this tutorial, we walk through the process of using WALA for program analysis. First, the tutorial will cover all the required background knowledge that is necessary to understand the technical implementation details of the explained algorithms and techniques. Subsequently, we provide a technical overview of the WALA framework and its support for analysis of multiple programming languages and frameworks code. Then, we will do several live demonstration of using WALA to implement client analyses. We will focus on two common uses of analysis: a form of security analysis, taint analysis, and on using analysis graphs for machine learning of code.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1819},
numpages = {1},
keywords = {WALA, Static Analysis, Program Analysis},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@article{10.1145/3624745,
author = {Formica, Federico and Fan, Tony and Menghi, Claudio},
title = {Search-Based Software Testing Driven by Automatically Generated and Manually Defined Fitness Functions},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3624745},
doi = {10.1145/3624745},
abstract = {Search-based software testing (SBST) typically relies on fitness functions to guide the search exploration toward software failures. There are two main techniques to define fitness functions: (a)&nbsp;automated fitness function computation from the specification of the system requirements, and (b)&nbsp;manual fitness function design. Both techniques have advantages. The former uses information from the system requirements to guide the search toward portions of the input domain more likely to contain failures. The latter uses the engineers’ domain knowledge.We propose ATheNA, a novel SBST framework that combines fitness functions automatically generated from requirements specifications and those manually defined by engineers. We design and implement ATheNA-S, an instance of ATheNA that targets Simulink® models. We evaluate ATheNA-S by considering a large set of models from different domains. Our results show that ATheNA-S generates more failure-revealing test cases than existing baseline tools and that the difference between the runtime performance of ATheNA-S and the baseline tools is not statistically significant. We also assess whether ATheNA-S could generate failure-revealing test cases when applied to two representative case studies: one from the automotive domain and one from the medical domain. Our results show that ATheNA-S successfully revealed a requirement violation in our case studies.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {40},
numpages = {37},
keywords = {CPS, fitness functions, falsification, Testing}
}

@inproceedings{10.1109/ICSE43902.2021.00034,
author = {Wardat, Mohammad and Le, Wei and Rajan, Hridesh},
title = {DeepLocalize: Fault Localization for Deep Neural Networks},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00034},
doi = {10.1109/ICSE43902.2021.00034},
abstract = {Deep neural networks (DNNs) are becoming an integral part of most software systems. Previous work has shown that DNNs have bugs. Unfortunately, existing debugging techniques don't support localizing DNN bugs because of the lack of understanding of model behaviors. The entire DNN model appears as a black box. To address these problems, we propose an approach and a tool that automatically determines whether the model is buggy or not, and identifies the root causes for DNN errors. Our key insight is that historic trends in values propagated between layers can be analyzed to identify faults, and also localize faults. To that end, we first enable dynamic analysis of deep learning applications: by converting it into an imperative representation and alternatively using a callback mechanism. Both mechanisms allows us to insert probes that enable dynamic analysis over the traces produced by the DNN while it is being trained on the training data. We then conduct dynamic analysis over the traces to identify the faulty layer or hyperparameter that causes the error. We propose an algorithm for identifying root causes by capturing any numerical error and monitoring the model during training and finding the relevance of every layer/parameter on the DNN outcome. We have collected a benchmark containing 40 buggy models and patches that contain real errors in deep learning applications from Stack Overflow and GitHub. Our benchmark can be used to evaluate automated debugging tools and repair techniques. We have evaluated our approach using this DNN bug-and-patch benchmark, and the results showed that our approach is much more effective than the existing debugging approach used in the state-of-the-practice Keras library. For 34/40 cases, our approach was able to detect faults whereas the best debugging approach provided by Keras detected 32/40 faults. Our approach was able to localize 21/40 bugs whereas Keras did not localize any faults.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {251–262},
numpages = {12},
keywords = {Program Analysis, Fault Location, Deep learning bugs, Deep Neural Networks, Debugging},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3510454.3516830,
author = {Johnson, Brittany and Brun, Yuriy},
title = {Fairkit-learn: a fairness evaluation and comparison toolkit},
year = {2022},
isbn = {9781450392235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510454.3516830},
doi = {10.1145/3510454.3516830},
abstract = {Advances in how we build and use software, specifically the integration of machine learning for decision making, have led to widespread concern around model and software fairness. We present fairkit-learn, an interactive Python toolkit designed to support data scientists' ability to reason about and understand model fairness. We outline how fairkit-learn can support model training, evaluation, and comparison and describe the potential benefit that comes with using fairkit-learn in comparison to the state-of-the-art. Fairkit-learn is open source at https://go.gmu.edu/fairkit-learn/.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings},
pages = {70–74},
numpages = {5},
keywords = {visualization, software fairness, bias-free software design},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@article{10.1145/3633311,
author = {Deokuliar, Harsh and Sangwan, Raghvinder S. and Badr, Yoaukim and Srinivasan, Satish M.},
title = {Improving Testing of Deep-Learning Systems},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {67},
number = {3},
issn = {0001-0782},
url = {https://doi.org/10.1145/3633311},
doi = {10.1145/3633311},
abstract = {A combination of differential and mutation testing results in better test data.},
journal = {Commun. ACM},
month = feb,
pages = {44–48},
numpages = {5}
}

@article{10.1145/3660825,
author = {Wu, Yaoxuan and Humayun, Ahmad and Gulzar, Muhammad Ali and Kim, Miryung},
title = {Natural Symbolic Execution-Based Testing for Big Data Analytics},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660825},
doi = {10.1145/3660825},
abstract = {Symbolic execution is an automated test input generation technique that models individual program paths as logical constraints. However, the realism of concrete test inputs generated by SMT solvers often comes into question. Existing symbolic execution tools only seek arbitrary solutions for given path constraints. These constraints do not incorporate the naturalness of inputs that observe statistical distributions, range constraints, or preferred string constants. This results in unnatural-looking inputs that fail to emulate real-world data.                In this paper, we extend symbolic execution with consideration for incorporating naturalness. Our key insight is that users typically understand the semantics of program inputs, such as the distribution of height or possible values of zipcode, which can be leveraged to advance the ability of symbolic execution to produce natural test inputs. We instantiate this idea in NaturalSym, a symbolic execution-based test generation tool for data-intensive scalable computing (DISC) applications. NaturalSym generates natural-looking data that mimics real-world distributions by utilizing user-provided input semantics to drastically enhance the naturalness of inputs, while preserving strong bug-finding potential.                On DISC applications and commercial big data test benchmarks, NaturalSym achieves a higher degree of realism —as evidenced by a perplexity score 35.1 points lower on median, and detects 1.29\texttimes{} injected faults compared to the state-of-the-art symbolic executor for DISC, BigTest. This is because BigTest draws inputs purely based on the satisfiability of path constraints constructed from branch predicates, while NaturalSym is able to draw natural concrete values based on user-specified semantics and prioritize using these values in input generation. Our empirical results demonstrate that NaturalSym finds injected faults 47.8\texttimes{} more than NaturalFuzz (a coverage-guided fuzzer) and 19.1\texttimes{} more than ChatGPT. Meanwhile, TestMiner (a mining-based approach) fails to detect any injected faults. NaturalSym is the first symbolic executor that combines the notion of input naturalness in symbolic path constraints during SMT-based input generation. We make our code available at https://github.com/UCLA-SEAL/NaturalSym.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {118},
numpages = {24},
keywords = {DISC Applications, Naturalness, Symbolic Execution}
}

@inproceedings{10.1145/3416507.3423190,
author = {Sonnekalb, Tim and Heinze, Thomas S. and Kurnatowski, Lynn von and Schreiber, Andreas and Gonzalez-Barahona, Jesus M. and Packer, Heather},
title = {Towards automated, provenance-driven security audit for git-based repositories: applied to germany's corona-warn-app: vision paper},
year = {2020},
isbn = {9781450381260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416507.3423190},
doi = {10.1145/3416507.3423190},
abstract = {Software repositories contain information about source code, software development processes, and team interactions. We combine provenance of the development process with code security analysis to automatically discover insights. This provides fast feedback on the software's design and security issues, which we evaluate on projects that are developed under time pressure, such as Germany's COVID-19 contact tracing app 'Corona-Warn-App'.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Software Security from Design to Deployment},
pages = {15–18},
numpages = {4},
keywords = {software security, repository mining, provenance, program analysis, open source software, covid-19},
location = {Virtual, USA},
series = {SEAD 2020}
}

@inproceedings{10.1145/3230467.3230477,
author = {Shatnawi, Raed},
title = {Identifying Threshold Values of Change-Prone Modules},
year = {2018},
isbn = {9781450364300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230467.3230477},
doi = {10.1145/3230467.3230477},
abstract = {Software changes frequently during the lifetime of a project. These changes increase the total cost of software development. Knowing where changes are more likely help in allocating appropriate resources in different activities of software lifecycle. The change-proneness of modules is defined as likelihood of change in a module. Change-proneness can be predicted using software metrics such as Chidamber and Kemerer metrics. However, the use of prediction models requires knowledge in advanced techniques such as data mining and regression analysis. Hence, there is a need to a more direct and simple technique to have more information about change-prone modules. Threshold values are proposed as such technique to identify the modules that are more change-prone. We propose the ROC analysis to identify thresholds. The ROC analysis suggests that large values of software metrics are indicators of change proneness. The research could identify practical thresholds for four metrics, CBO, RFC, WMC and the LCOM. The inheritance metrics do not have practical thresholds.In this paper, we describe the formatting guidelines for ACM SIG Proceedings.},
booktitle = {Proceedings of the 2018 International Conference on E-Business and Mobile Commerce},
pages = {39–43},
numpages = {5},
keywords = {product metrics, change-prone metrics, ROC curves, CK metrics},
location = {Chengdu, China},
series = {ICEMC '18}
}

@inproceedings{10.1145/1294948.1294954,
author = {Aversano, Lerina and Cerulo, Luigi and Del Grosso, Concettina},
title = {Learning from bug-introducing changes to prevent fault prone code},
year = {2007},
isbn = {9781595937223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1294948.1294954},
doi = {10.1145/1294948.1294954},
abstract = {A version control system, such as CVS/SVN, can provide the history of software changes performed during the evolution of a software project. Among all the changes performed there are some which cause the introduction of bugs, often resolved later with other changes.In this paper we use a technique to identify bug-introducing changes to train a model that can be used to predict if a new change may introduces or not a bug. We represent software changes as elements of a n-dimensional vector space of terms coordinates extracted from source code snapshots.The evaluation of various learning algorithms on a set of open source projects looks very promising, in particular for KNN (K-Nearest Neighbor algorithm) where a significant tradeoff between precision and recall has been obtained.},
booktitle = {Ninth International Workshop on Principles of Software Evolution: In Conjunction with the 6th ESEC/FSE Joint Meeting},
pages = {19–26},
numpages = {8},
keywords = {software evolution, mining software repositories, bug prediction},
location = {Dubrovnik, Croatia},
series = {IWPSE '07}
}

@inproceedings{10.1145/3512353.3512379,
author = {Chopra, Rahul and Roy, Shreoshi and Malhotra, Ruchika},
title = {Transductive Instance Transfer Learning for Cross-Language Defect Prediction},
year = {2022},
isbn = {9781450395571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512353.3512379},
doi = {10.1145/3512353.3512379},
abstract = {Predicting defects (bugs) is critical to increasing software quality. Many software defect prediction algorithms have been presented, and many of them have shown to be effective in practice. However, because existing works are largely limited to a single project, their effectiveness in predicting cross-project defects is usually poor. This is primarily due to the issue of class imbalance and discrepancies in feature distribution between the source and destination projects. However, because of the disparities in distribution amongst datasets from different studies, developing high-quality Cross Project Defect Prediction (CPDP) models remains a difficulty. In our study, instead of collecting data from a single project, we have collected source code from multiple code submissions on a programming contest website and employed Natural Language Processing (NLP) models to detect software defects in them.},
booktitle = {Proceedings of the 2022 4th Asia Pacific Information Technology Conference},
pages = {176–182},
numpages = {7},
keywords = {Transfer Learning, Natural Language Processing, Long Short Term Memory, Doc2Vec Embedding, Defect Prediction, Artificial Neural Network},
location = {Virtual Event, Thailand},
series = {APIT '22}
}

@inproceedings{10.5555/2486788.2486839,
author = {Nam, Jaechang and Pan, Sinno Jialin and Kim, Sunghun},
title = {Transfer defect learning},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Many software defect prediction approaches have been proposed and most are effective in within-project prediction settings. However, for new projects or projects with limited training data, it is desirable to learn a prediction model by using sufficient training data from existing source projects and then apply the model to some target projects (cross-project defect prediction). Unfortunately, the performance of cross-project defect prediction is generally poor, largely because of feature distribution differences between the source and target projects. In this paper, we apply a state-of-the-art transfer learning approach, TCA, to make feature distributions in source and target projects similar. In addition, we propose a novel transfer defect learning approach, TCA+, by extending TCA. Our experimental results for eight open-source projects show that TCA+ significantly improves cross-project prediction performance.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {382–391},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/3551349.3556895,
author = {Zhong, Hao},
title = {Which Exception Shall We Throw?},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556895},
doi = {10.1145/3551349.3556895},
abstract = {Although the exception handling mechanism is critical for resolving runtime errors, bugs inside this process can have far-reaching impacts. Therefore, researchers have proposed various approaches to assist catching and handling such thrown exceptions and to detect corresponding bugs. If the thrown exceptions themselves are incorrect, their errors will never be correctly caught and handled. Like bugs in catching and handling exceptions, wrong thrown exceptions have caused real critical bugs. However, to the best of our knowledge, no approach has been proposed to recommend which exceptions shall be thrown. Exceptions are widely adopted in programs, often poorly documented, and sometimes ambiguous, making the rules of throwing correct exceptions rather complicated. A project team can leverage exceptions in a way totally different from other teams. As a result, even experienced programmers can have difficulties in determining which exception shall be thrown, although they have the skills to implement its surrounding code. In this paper, we propose the first approach, ThEx, to predict which exception(s) shall be thrown under a given programming context. The basic idea is to learn a classification model from existing thrown exceptions in source files. Here, the learning features are extracted from various code information surrounding the thrown exceptions, such as the thrown locations and related variable names. Then, given a new context, ThEx can predict its best exception(s). We have evaluated ThEx on 12,012 thrown exceptions that were collected from nine popular open-source projects. Our results show that it can achieve high f-scores and mcc values (both around 0.8). On this benchmark, we also evaluated the impacts of our underlying technical details. Furthermore, we evaluated our approach in the wild, and used ThEx to detect anomalies from the latest versions of the nine projects. In this way, we found 20 anomalies, and reported them as bugs to their issue trackers. Among them, 18 were confirmed, and 13 have already been fixed.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {116},
numpages = {12},
keywords = {wrong thrown exception, text tagging, exception-related bug},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3573942.3574014,
author = {Pan, Xiaoying and Feng, Congcong and Liu, Chen and Mu, Yuanzhen},
title = {An Anti-Noise Hybrid Clustering Oversampling Technique for Imbalanced Data Classification},
year = {2023},
isbn = {9781450396899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573942.3574014},
doi = {10.1145/3573942.3574014},
abstract = {Oversampling techniques have always been favored in the field of imbalanced data classification. Noise processing plays an important role in the field of unbalanced data classification because the noise data directly affects the distribution of newly synthesized samples. We propose a new anti-noise hybrid clustering oversampling technique for imbalanced data classification to synthesize high-quality samples (ANCO). The algorithm is the first to propose a combination of noise filtering and optimization in an oversampling method. Firstly, based on the sample location information, the processing strategy of noise filtering and optimization is designed. Then, sample nearest neighbor interpolation is used to create a new sample. To compare the performance of our approach with representative oversampling, five datasets with variable imbalance rates in KEEL are chosen for testing. The results show that the ANCO algorithm improves the classifier's overall performance.},
booktitle = {Proceedings of the 2022 5th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {222–227},
numpages = {6},
keywords = {Classification, Evaluation metrics, Imbalanced data, Oversampling},
location = {Xiamen, China},
series = {AIPR '22}
}

@inproceedings{10.1145/2642937.2653469,
author = {Borg, Markus},
title = {Embrace your issues: compassing the software engineering landscape using bug reports},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2653469},
doi = {10.1145/2642937.2653469},
abstract = {Software developers in large projects work in complex information landscapes, and staying on top of all relevant software artifacts is challenging. As software systems often evolve for years, a high number of issue reports is typically managed during the lifetime of a system. Efficient management of incoming issue requires successful navigation of the information landscape. In our work, we address two important work tasks involved in issue management: Issue Assignment (IA) and Change Impact Analysis (CIA). IA is the early task of allocating an issue report to a development team. CIA deals with identifying how source code changes affect the software system, a fundamental activity in safety-critical development. Our solution approach is to support navigation, both among development teams and software artifacts, based on information available in historical issue reports. We present how we apply techniques from machine learning and information retrieval to develop recommendation systems. Finally, we report intermediate results from two controlled experiments and an industrial case study.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {891–894},
numpages = {4},
keywords = {recommendation systems, machine learning, issue management, information retrieval},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@article{10.1145/3715322,
author = {Japke, Nils and Grambow, Martin and Laaber, Christoph and Bermbach, David},
title = {µOpTime: Statically Reducing the Execution Time of Microbenchmark Suites Using Stability Metrics},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715322},
doi = {10.1145/3715322},
abstract = {Performance regressions have a tremendous impact on the quality of software. One way to catch regressions before they reach production is executing performance tests before deployment, e.g., using microbenchmarks, which measure performance at subroutine level. In projects with many microbenchmarks, this may take several hours due to repeated execution to get accurate results, disqualifying them from frequent use in CI/CD pipelines. We propose µOpTime, a static approach to reduce the execution time of microbenchmark suites by configuring the number of repetitions for each microbenchmark. Based on the results of a full, previous microbenchmark suite run, µOpTime determines the minimal number of (measurement) repetitions with statistical stability metrics that still lead to accurate results. We evaluate µOpTime with an experimental study on 14 open-source projects written in two programming languages and five stability metrics. Our results show that (i) µOpTime reduces the total suite execution time (measurement phase) by up to 95.83% (Go) and 94.17% (Java), (ii) the choice of stability metric depends on the project and programming language, (iii) microbenchmark warmup phases have to be considered for Java projects (potentially leading to higher reductions), and (iv) µOpTime can be used to reliably detect performance regressions in CI/CD pipelines.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {software microbenchmarking, software performance, microbenchmark configuration, JMH, Go}
}

@inproceedings{10.1145/3460945.3464954,
author = {Hasabnis, Niranjan and Gottschlich, Justin},
title = {ControlFlag: a self-supervised idiosyncratic pattern detection system for software control structures},
year = {2021},
isbn = {9781450384674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460945.3464954},
doi = {10.1145/3460945.3464954},
abstract = {Software debugging has been shown to utilize upwards of half of developers’ time. Yet, machine programming (MP), the field concerned with the automation of software (and hardware) development, has recently made strides in both research and production-quality automated debugging systems. In this paper we present ControlFlag, a self-supervised MP system that aims to improve debugging by attempting to detect idiosyncratic pattern violations in software control structures. ControlFlag also suggests possible corrections in the event an anomalous pattern is detected. We present ControlFlag’s design and provide an experimental evaluation and analysis of its efficacy in identifying potential programming errors in production-quality software. As a first concrete evidence towards improving software quality, ControlFlag has already found an anomaly in CURL that has been acknowledged and fixed by its developers. We also discuss future extensions of ControlFlag.},
booktitle = {Proceedings of the 5th ACM SIGPLAN International Symposium on Machine Programming},
pages = {32–42},
numpages = {11},
keywords = {self-supervised learning, Source-code mining},
location = {Virtual, Canada},
series = {MAPS 2021}
}

@inproceedings{10.1145/3338906.3338941,
author = {Jimenez, Matthieu and Rwemalika, Renaud and Papadakis, Mike and Sarro, Federica and Le Traon, Yves and Harman, Mark},
title = {The importance of accounting for real-world labelling when predicting software vulnerabilities},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338941},
doi = {10.1145/3338906.3338941},
abstract = {Previous work on vulnerability prediction assume that predictive models are trained with respect to perfect labelling information (includes labels from future, as yet undiscovered vulnerabilities). In this paper we present results from a comprehensive empirical study of 1,898 real-world vulnerabilities reported in 74 releases of three security-critical open source systems (Linux Kernel, OpenSSL and Wiresark). Our study investigates the effectiveness of three previously proposed vulnerability prediction approaches, in two settings: with and without the unrealistic labelling assumption. The results reveal that the unrealistic labelling assumption can profoundly mis- lead the scientific conclusions drawn; suggesting highly effective and deployable prediction results vanish when we fully account for realistically available labelling in the experimental methodology. More precisely, MCC mean values of predictive effectiveness drop from 0.77, 0.65 and 0.43 to 0.08, 0.22, 0.10 for Linux Kernel, OpenSSL and Wiresark, respectively. Similar results are also obtained for precision, recall and other assessments of predictive efficacy. The community therefore needs to upgrade experimental and empirical methodology for vulnerability prediction evaluation and development to ensure robust and actionable scientific findings.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {695–705},
numpages = {11},
keywords = {Software Vulnerabilities, Prediction Modelling, Machine Learning},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@article{10.1145/3678188,
author = {Wei, Qiping and Sikder, Fadul and Feng, Huadong and Lei, Yu and Kacker, Raghu and Kuhn, Richard},
title = {SmartExecutor: Coverage-Driven Symbolic Execution Guided via State Prioritization and Function Selection},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
url = {https://doi.org/10.1145/3678188},
doi = {10.1145/3678188},
abstract = {Symbolic execution of smart contracts suffers from sequence explosion. Some existing tools limit the sequence length, thus being unable to adequately evaluate some functions. In this article, we propose a symbolic execution approach without limiting the sequence length. In our approach, the symbolic execution process is a two-phase model that maximizes code coverage while reducing the number of sequences to be executed. The first phase executes all sequences up to a length limit to identify the not-fully covered functions, while the second attempts to cover these functions according to state evaluation and a function graph structure. We have developed a tool called SmartExecutor and conducted an experimental evaluation on the SGUARD dataset. The experimental results indicate that compared with state-of-the-art tools, SmartExecutor achieves higher code coverage with less time. It also detects more vulnerabilities than Mythril, a state-of-the-art symbolic execution tool.},
journal = {Distrib. Ledger Technol.},
month = feb,
articleno = {4},
numpages = {29},
keywords = {Ethereum smart contract, symbolic execution, vulnerability detection, sequence explosion, function dependency}
}

@inproceedings{10.1145/3701625.3701676,
author = {Virg\'{\i}nio, T\'{a}ssio and Bastos, Larissa and Bezerra, Carla and Ribeiro, M\'{a}rcio and Machado, Ivan},
title = {How Aware Are We of Test Smells in Quantum Software Systems? A Preliminary Empirical Evaluation},
year = {2024},
isbn = {9798400717772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701625.3701676},
doi = {10.1145/3701625.3701676},
abstract = {Context: With the rapid progress of quantum computing, Quantum Software Engineering (QSE) is establishing itself as an essential discipline to support developers throughout all stages of quantum software development. The area of testing in quantum systems has received greater attention in research on this topic to guarantee the quality and reliability of these technologies. Objective: This paper presents an empirical study focused on the testing of quantum software at its classical layer. Specifically, it aims to identify and analyze the unique characteristics of quantum software tests, particularly in terms of Test Smells, their distribution, recurrence, and differences compared to classical software tests. Method: We used two sets of software from previous studies, one comprising 12 quantum software and the other comprising 80 classical software. From these datasets, we conducted an analysis to detect 10 test smells, allowing us to map their dispersion in quantum software, identify their specific characteristics, and draw comparisons with classical software. Results: Our findings reveal a high dispersion of test smells of 51% in quantum software. Furthermore, quantum tests exhibit statistical differences from classical software tests, with the most outlier being Conditional Test Logic, which is 20% more frequent than in classical software. Conclusions: The insights gained from this study can contribute to enhancing the quality, maintainability, and readability of tests written for the classical layer of quantum software. Ultimately, this can improve the overall understanding and quality of quantum software.},
booktitle = {Proceedings of the XXIII Brazilian Symposium on Software Quality},
pages = {383–393},
numpages = {11},
keywords = {Quantum Software Systems, Software Testing, Test Smells, Empirical Evaluation.},
location = {
},
series = {SBQS '24}
}

@inproceedings{10.1145/3236024.3236050,
author = {Chen, Di and Fu, Wei and Krishna, Rahul and Menzies, Tim},
title = {Applications of psychological science for actionable analytics},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236050},
doi = {10.1145/3236024.3236050},
abstract = {According to psychological scientists, humans understand models that most match their own internal models, which they characterize as lists of "heuristic"s (i.e. lists of very succinct rules). One such heuristic rule generator is the Fast-and-Frugal Trees (FFT) preferred by psychological scientists. Despite their successful use in many applied domains, FFTs have not been applied in software analytics. Accordingly, this paper assesses FFTs for software analytics.  We find that FFTs are remarkably effective in that their models are very succinct (5 lines or less describing a binary decision tree) while also outperforming result from very recent, top-level, conference papers. Also, when we restrict training data to operational attributes (i.e., those attributes that are frequently changed by developers), the performance of FFTs are not effected (while the performance of other learners can vary wildly).  Our conclusions are two-fold. Firstly, there is much that software analytics community could learn from psychological science. Secondly, proponents of complex methods should always baseline those methods against simpler alternatives. For example, FFTs could be used as a standard baseline learner against which other software analytics tools are compared.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {456–467},
numpages = {12},
keywords = {software analytics, psychological science, heuristics, empirical studies, defect prediction, Decision trees},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3468264.3468539,
author = {Rabin, Md Rafiqul Islam and Hellendoorn, Vincent J. and Alipour, Mohammad Amin},
title = {Understanding neural code intelligence through program simplification},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468539},
doi = {10.1145/3468264.3468539},
abstract = {A wide range of code intelligence (CI) tools, powered by deep neural networks, have been developed recently to improve programming productivity and perform program analysis. To reliably use such tools, developers often need to reason about the behavior of the underlying models and the factors that affect them. This is especially challenging for tools backed by deep neural networks. Various methods have tried to reduce this opacity in the vein of "transparent/interpretable-AI". However, these approaches are often specific to a particular set of network architectures, even requiring access to the network's parameters. This makes them difficult to use for the average programmer, which hinders the reliable adoption of neural CI systems. In this paper, we propose a simple, model-agnostic approach to identify critical input features for models in CI systems, by drawing on software debugging research, specifically delta debugging. Our approach, SIVAND, uses simplification techniques that reduce the size of input programs of a CI model while preserving the predictions of the model. We show that this approach yields remarkably small outputs and is broadly applicable across many model architectures and problem domains. We find that the models in our experiments often rely heavily on just a few syntactic features in input programs. We believe that SIVAND's extracted features may help understand neural CI systems' predictions and learned behavior.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {441–452},
numpages = {12},
keywords = {Program Simplification, Models of Code, Interpretable AI},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3524842.3527996,
author = {Keshavarz, Hossein and Nagappan, Meiyappan},
title = {ApacheJIT: a large dataset for just-in-time defect prediction},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3527996},
doi = {10.1145/3524842.3527996},
abstract = {In this paper, we present ApacheJIT, a large dataset for Just-In-Time (JIT) defect prediction. ApacheJIT consists of clean and bug-inducing software changes in 14 popular Apache projects. ApacheJIT has a total of 106,674 commits (28,239 bug-inducing and 78,435 clean commits). Having a large number of commits makes ApacheJIT a suitable dataset for machine learning JIT models, especially deep learning models that require large training sets to effectively generalize the patterns present in the historical data to future data.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {191–195},
numpages = {5},
keywords = {software engineering, defect prediction, dataset},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1145/3551349.3556926,
author = {Ye, He and Martinez, Matias and Luo, Xiapu and Zhang, Tao and Monperrus, Martin},
title = {SelfAPR: Self-supervised Program Repair with Test Execution Diagnostics},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556926},
doi = {10.1145/3551349.3556926},
abstract = {Learning-based program repair has achieved good results in a recent series of papers. Yet, we observe that the related work fails to repair some bugs because of a lack of knowledge about 1) the application domain of the program being repaired, and 2) the fault type being repaired. In this paper, we solve both problems by changing the learning paradigm from supervised training to self-supervised training in an approach called SelfAPR. First, SelfAPR generates training samples on disk by perturbing a previous version of the program being repaired, enforcing the neural model to capture project-specific knowledge. This is different from the previous work based on mined past commits. Second, SelfAPR executes all training samples and extracts and encodes test execution diagnostics into the input representation, steering the neural model to fix the kind of fault. This is different from the existing studies that only consider static source code as input. We implement SelfAPR and evaluate it in a systematic manner. We generate 1&nbsp;039&nbsp;873 training samples obtained by perturbing 17 open-source projects. We evaluate SelfAPR on 818 bugs from Defects4J, SelfAPR correctly repairs 110 of them, outperforming all the supervised learning repair approaches.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {92},
numpages = {13},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@article{10.1145/3643676,
author = {Li, Yinghua and Dang, Xueqi and Ma, Lei and Klein, Jacques and Le Traon, Yves and Bissyand\'{e}, Tegawend\'{e} F.},
title = {Test Input Prioritization for 3D Point Clouds},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3643676},
doi = {10.1145/3643676},
abstract = {3D point cloud applications have become increasingly prevalent in diverse domains, showcasing their efficacy in various software systems. However, testing such applications presents unique challenges due to the high-dimensional nature of 3D point cloud data and the vast number of possible test cases. Test input prioritization has emerged as a promising approach to enhance testing efficiency by prioritizing potentially misclassified test cases during the early stages of the testing process. Consequently, this enables the early labeling of critical inputs, leading to a reduction in the overall labeling cost. However, applying existing prioritization methods to 3D point cloud data is constrained by several factors: (1) inadequate consideration of crucial spatial information, and (2) susceptibility to noises inherent in 3D point cloud data. In this article, we propose PCPrior, the first test prioritization approach specifically designed for 3D point cloud test cases. The fundamental concept behind PCPrior is that test inputs closer to the decision boundary of the model are more likely to be predicted incorrectly. To capture the spatial relationship between a point cloud test and the decision boundary, we propose transforming each test (a point cloud) into a low-dimensional feature vector, toward indirectly revealing the underlying proximity between a test and the decision boundary. To achieve this, we carefully design a group of feature generation strategies, and for each test input, we generate four distinct types of features, namely spatial features, mutation features, prediction features, and uncertainty features. Through a concatenation of the four feature types, PCPrior assembles a final feature vector for each test. Subsequently, a ranking model is employed to estimate the probability of misclassification for each test based on its feature vector. Finally, PCPrior ranks all tests based on their misclassification probabilities. We conducted an extensive study based on 165 subjects to evaluate the performance of PCPrior, encompassing both natural and noisy datasets. The results demonstrate that PCPrior outperforms all of the compared test prioritization approaches, with an average improvement of 10.99% to 66.94% on natural datasets and 16.62% to 53% on noisy datasets.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {132},
numpages = {44},
keywords = {Test input prioritization, deep neural network, learning to rank, labeling}
}

@inproceedings{10.1145/3377929.3398128,
author = {Rosenbauer, Lukas and Stein, Anthony and Maier, Roland and P\"{a}tzel, David and H\"{a}hner, J\"{o}rg},
title = {XCS as a reinforcement learning approach to automatic test case prioritization},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3398128},
doi = {10.1145/3377929.3398128},
abstract = {Testing is a crucial part in the development of new products. With the rise of test automation methods, companies start relying on an even higher number of tests. Sometimes it is not feasible to run all tests and the goal is to determine which tests are crucial and which are less important. This prioritization problem has just recently gotten into the focus of reinforcement learning. A neural network combined with prioritized experience replay (ER) was used to identify critical tests. We are the first to apply XCS classifier systems (XCS) for this use case and reveal that XCS is not only suitable for this problem, but can also be superior to the aforementioned neural network and leads to more stable results. In this work, we adapt XCS's learning mechanism to the task by introducing a batch update which is based on Monte Carlo control. Further, we investigate if prioritized ER has the same positive effects on XCS as on the neural network for this test prioritization problem. Our experiments show that in general this is not the case for XCS.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {1798–1806},
numpages = {9},
keywords = {test automation, reinforcement learning, experience replay, artificial intelligence, XCS classifier system},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@article{10.1145/3638245,
author = {Wang, Han and Yu, Sijia and Chen, Chunyang and Turhan, Burak and Zhu, Xiaodong},
title = {Beyond Accuracy: An Empirical Study on Unit Testing in Open-source Deep Learning Projects},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3638245},
doi = {10.1145/3638245},
abstract = {Deep Learning (DL) models have rapidly advanced, focusing on achieving high performance through testing model accuracy and robustness. However, it is unclear whether DL projects, as software systems, are tested thoroughly or functionally correct when there is a need to treat and test them like other software systems. Therefore, we empirically study the unit tests in open-source DL projects, analyzing 9,129 projects from GitHub. We find that: (1) unit tested DL projects have positive correlation with the open-source project metrics and have a higher acceptance rate of pull requests; (2) 68% of the sampled DL projects are not unit tested at all; (3) the layer and utilities (utils) of DL models have the most unit tests. Based on these findings and previous research outcomes, we built a mapping taxonomy between unit tests and faults in DL projects. We discuss the implications of our findings for developers and researchers and highlight the need for unit testing in open-source DL projects to ensure their reliability and stability. The study contributes to this community by raising awareness of the importance of unit testing in DL projects and encouraging further research in this area.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {104},
numpages = {22},
keywords = {Deep learning, unit testing}
}

@inproceedings{10.1145/3634737.3637650,
author = {Sharma, Shashank and Tanksalkar, Sai Ritvik and Cherupattamoolayil, Sourag and Machiry, Aravind},
title = {Fuzzing API Error Handling Behaviors using Coverage Guided Fault Injection},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634737.3637650},
doi = {10.1145/3634737.3637650},
abstract = {Incorrect handling of Software Application Programming Interfaces (APIs) errors results in bugs or security vulnerabilities that are hard to trigger during regular testing. Most of the existing techniques to detect such errors are based on static analysis and fail to identify certain cases where API return values are incorrectly handled. Furthermore, most of these techniques suffer from a very high false positive rate (≥50%), raising concerns regarding their practical use. We propose a dynamic analysis approach to detect API error handling bugs based on coverage-guided software fault injection. Specifically, we inject faults into APIs and observe how a program handles them. Our fault injection mechanism is generic and targeted to explore a given program's error handling behavior effectively. We avoid false positives by proactively filtering out crashes caused by infeasible faults. We implemented our technique in an automated pipeline called FuzzERR and applied it to 20 different programs spanning 444 APIs. Our evaluation shows that FuzzERR found 31 new and previously unknown bugs resulting from incorrect handling of API errors. Moreover, a comparative evaluation showed that FuzzERR significantly outperformed the state-of-the-art tools.},
booktitle = {Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
pages = {1495–1509},
numpages = {15},
keywords = {program analysis, fuzzing, LLVM, error handling, API error handling},
location = {Singapore, Singapore},
series = {ASIA CCS '24}
}

@inproceedings{10.1145/3503047.3503100,
author = {Ye, Huijia and Rao, Juwei and Shi, Yang and Li, Zhihua},
title = {ProExtor: Mining API Protocols for Program Vulnerability Detection},
year = {2022},
isbn = {9781450385862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503047.3503100},
doi = {10.1145/3503047.3503100},
abstract = {API protocols play an important role in program verification, testing, evolution and other phases of the software development process. Many approaches have been proposed to mine API protocols automatically from programs. However, few tools are available, especially dynamical API protocol mining tools. In this paper, we present a dynamical API protocol mining tool for Java programs: ProExtor. Our tool mines API protocols in an online mode based on the instrumentation technique of Java agent. For each class, it produces two models: a probabilistic model and a deterministic model. The probabilistic model will be evolved persistently when more application programs are fed for mining. The deterministic model is transformed from the latest probabilistic model, which can be used for program verification, testing, evolution, etc. Both models can be visualized with the software Graphviz. We elaborate design and implementation details of our tool and an application to a real-world program. We believe our work is a good reference for the development of similar tools.},
booktitle = {Proceedings of the 3rd International Conference on Advanced Information Science and System},
articleno = {49},
numpages = {7},
keywords = {Program Specification Mining, Program Instrumentation, Program Dynamic Analysis},
location = {Sanya, China},
series = {AISS '21}
}

@inproceedings{10.1145/3697090.3697105,
author = {Callou, Gustavo and Vieira, Marco},
title = {Availability and Performance Analysis of Cloud Services},
year = {2024},
isbn = {9798400717406},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3697090.3697105},
doi = {10.1145/3697090.3697105},
abstract = {Companies need to adjust their service infrastructure to the demand over time. Given its elasticity and scalability, cloud computing represents an interesting option for meeting such demands. In this context, availability and performance are two critical aspects of cloud services that can significantly impact the success of businesses and organizations. This work proposes models to analyze availability and performance metrics of a web service hosted in a private cloud environment. Models based on stochastic Petri net and reliability block diagrams are proposed and validated through a testbed cloud service. The case studies illustrate the applicability of the proposed models, and the results clearly show the relevance of adopting models to represent cloud service systems.},
booktitle = {Proceedings of the 13th Latin-American Symposium on Dependable and Secure Computing},
pages = {262–271},
numpages = {10},
keywords = {Analytical models, Availability, Performance evaluation, Cloud Computing},
location = {
},
series = {LADC '24}
}

@inproceedings{10.1109/CGO57630.2024.10444847,
author = {Seeker, Volker and Cummins, Chris and Cole, Murray and Franke, Bj\"{o}rn and Hazelwood, Kim and Leather, Hugh},
title = {Revealing Compiler Heuristics through Automated Discovery and Optimization},
year = {2024},
isbn = {9798350395099},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CGO57630.2024.10444847},
doi = {10.1109/CGO57630.2024.10444847},
abstract = {Tuning compiler heuristics and parameters is well known to improve optimization outcomes dramatically. Prior works have tuned command line flags and a few expert identified heuristics. However, there are an unknown number of heuristics buried, unmarked and unexposed inside the compiler as a consequence of decades of development without auto-tuning being foremost in the minds of developers. Many may not even have been considered heuristics by the developers who wrote them. The result is that auto-tuning search and machine learning can optimize only a tiny fraction of what could be possible if all heuristics were available to tune. Manually discovering all of these heuristics hidden among millions of lines of code and exposing them to auto-tuning tools is a Herculean task that is simply not practical. What is needed is a method of automatically finding these heuristics to extract every last drop of potential optimization.In this work, we propose Heureka, a framework that automatically identifies potential heuristics in the compiler that are highly profitable optimization targets and then automatically finds available tuning parameters for those heuristics with minimal human involvement. Our work is based on the following key insight: When modifying the output of a heuristic within an acceptable value range, the calling code using that output will still function correctly and produce semantically correct results. Building on that, we automatically manipulate the output of potential heuristic code in the compiler and decide using a Differential Testing approach if we found a heuristic or not. During output manipulation, we also explore acceptable value ranges of the targeted code. Heuristics identified in this way can then be tuned to optimize an objective function.We used Heureka to search for heuristics among eight thousand functions from the LLVM optimization passes, which is about 2% of all available functions. We then use identified heuristics to tune the compilation of 38 applications from the NAS and Polybench benchmark suites. Compared to an -Ozbaseline we reduce binary sizes by up to 11.6% considering single heuristics only and up to 19.5% when stacking the effects of multiple identified tuning targets and applying a random search with minimal search effort. Generalizing from existing analysis results, Heureka needs, on average, a little under an hour on a single machine to identify relevant heuristic targets for a previously unseen application.},
booktitle = {Proceedings of the 2024 IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {55–66},
numpages = {12},
keywords = {search methodologies, compiler optimization, differential testing},
location = {Edinburgh, United Kingdom},
series = {CGO '24}
}

@inproceedings{10.1145/2901739.2901760,
author = {Layman, Lucas and Nikora, Allen P. and Meek, Joshua and Menzies, Tim},
title = {Topic modeling of NASA space system problem reports: research in practice},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2901760},
doi = {10.1145/2901739.2901760},
abstract = {Problem reports at NASA are similar to bug reports: they capture defects found during test, post-launch operational anomalies, and document the investigation and corrective action of the issue. These artifacts are a rich source of lessons learned for NASA, but are expensive to analyze since problem reports are comprised primarily of natural language text. We apply topic modeling to a corpus of NASA problem reports to extract trends in testing and operational failures. We collected 16,669 problem reports from six NASA space flight missions and applied Latent Dirichlet Allocation topic modeling to the document corpus. We analyze the most popular topics within and across missions, and how popular topics changed over the lifetime of a mission. We find that hardware material and flight software issues are common during the integration and testing phase, while ground station software and equipment issues are more common during the operations phase. We identify a number of challenges in topic modeling for trend analysis: 1) that the process of selecting the topic modeling parameters lacks definitive guidance, 2) defining semantically-meaningful topic labels requires non-trivial effort and domain expertise, 3) topic models derived from the combined corpus of the six missions were biased toward the larger missions, and 4) topics must be semantically distinct as well as cohesive to be useful. Nonetheless, topic modeling can identify problem themes within missions and across mission lifetimes, providing useful feedback to engineers and project managers.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {303–314},
numpages = {12},
keywords = {topic modeling, natural language processing, defects, data mining, LDA},
location = {Austin, Texas},
series = {MSR '16}
}

@inproceedings{10.1145/3629479.3629489,
author = {Castro, Renata and Oliveira, Flavia and Rodrigues, Janderson and Tiago, Leonardo and Sousa, Cesar and Chaves, Lennon},
title = {Enhancing Issue Management through the Employment of Inspection Technique: An Experience Report in The Industry},
year = {2023},
isbn = {9798400707865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629479.3629489},
doi = {10.1145/3629479.3629489},
abstract = {The software industry has been adopting the concept of Global Software Development to release products quicker and with fewer costs, assuring quality by employing software testing techniques which are able to detect issues early. In this context, this paper presents an experience report performed at SIDIA Institute of Science and Technology between the years of 2020 and 2022, during which a process of issue management (IM) was developed to reduce the problems in the issue report, such as: lack of cohesion, absence of confirmation tests and long learning curve time of the testing team. In order to face these challenges, the IM process employs inspection techniques in order to improve the quality of reports, and then, increase its effectiveness, i.e., the number of valid issues. In addition, to support the IM process, it was created an onboarding phase, a template for issue reports and the monitoring of issues until the confirmation test. Through the implementation of this process, there was evaluated a total amount of 11002 issues report, and as a result, it was observed that the issues’ effectiveness rate was increased from to (raising by ) in the period in which this research was performed. In sequence, it was detected that around of issues had problems related to issue report writing. Furthermore, qualitative research revealed that of the interviewed testers consider the process effective. The authors hope that lessons learned described in this paper can contribute to the academic and industry communities by regarding the employment of inspection techniques to evaluate issue reports.},
booktitle = {Proceedings of the XXII Brazilian Symposium on Software Quality},
pages = {208–217},
numpages = {10},
keywords = {Inspection Techniques, Issue Effectiveness, Issue Report, Software Testing, V&amp;V Techniques},
location = {Bras\'{\i}lia, Brazil},
series = {SBQS '23}
}

@article{10.1145/3699596,
author = {Crespo-Rodriguez, Victor and Neelofar and Aleti, Aldeida and Turhan, Burak},
title = {Instance Space Analysis of Testing of Autonomous Vehicles in Critical Scenarios},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3699596},
doi = {10.1145/3699596},
abstract = {Before being deployed on roads, Autonomous Vehicles (AVs) must undergo comprehensive testing. Safety-critical situations, however, are infrequent in usual driving conditions, so simulated scenarios are used to create them. A test scenario comprises static and dynamic features related to the AV and the test environment; the representation of these features is complex and makes testing a heavy process. A test scenario is effective if it identifies incorrect behaviors of the AV. In this article, we present a technique for identifying key features of test scenarios associated with their effectiveness using Instance Space Analysis (ISA). ISA generates a ( (2D) ) representation of test scenarios and their features. This visualization helps to identify combinations of features that make a test scenario effective. We present a graphical representation of each feature that helps identify how well each testing technique explores the search space. While identifying key features is a primary goal, this study specifically seeks to determine the critical features that differentiate the performance of algorithms. Finally, we present metrics to assess the robustness of testing algorithms and the scenarios generated. Collecting essential features in combination with their values associated with effectiveness can be used for selection and prioritization of effective test cases.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {61},
numpages = {36},
keywords = {Instance Space Analysis, Autonomous Vehicles, Software Testing}
}

@inproceedings{10.1145/3624062.3624117,
author = {Chen, Zizhao and Verrecchia, Thomas and Sun, Hongyang and Booth, Joshua and Raghavan, Padma},
title = {Dynamic Selective Protection of Sparse Iterative Solvers via ML Prediction of Soft Error Impacts},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624117},
doi = {10.1145/3624062.3624117},
abstract = {Soft errors occur frequently on large computing platforms due to the increasing scale and complexity of HPC systems. Various resilience techniques (e.g., checkpointing, ABFT, and replication) have been proposed to protect scientific applications from soft errors at different levels. Among them, system-level replication often involves duplicating or even triplicating the entire computation, thus resulting in high resilience overhead. This paper proposes dynamic selective protection for sparse iterative solvers, in particular for the Preconditioned Conjugate Gradient (PCG) solver, at the system level to reduce the resilience overhead. For this method, we leverage machine learning (ML) to predict the impact of soft errors that strike different elements of a key computation (i.e., sparse matrix-vector multiplication) at different iterations of the solver. Based on the result of the prediction, we design a dynamic strategy to selectively protect those elements that would result in a large performance degradation if struck by soft errors. An experimental evaluation demonstrates that our dynamic protection strategy is able to reduce the resilience overhead compared to existing algorithms.},
booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {488–491},
numpages = {4},
keywords = {Fault tolerance, dynamic selective protection, iterative solvers, preconditioned conjugate gradient., soft errors},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@inproceedings{10.1109/ASE.2009.89,
author = {Ali, Shaimaa and Andrews, James H. and Dhandapani, Tamilselvi and Wang, Wantao},
title = {Evaluating the Accuracy of Fault Localization Techniques},
year = {2009},
isbn = {9780769538914},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2009.89},
doi = {10.1109/ASE.2009.89},
abstract = {We investigate claims and assumptions made in several recent papers about fault localization (FL) techniques. Most of these claims have to do with evaluating FL accuracy. Our investigation centers on a new subject program having properties useful for FL experiments. We find that Tarantula (Jones et al.) works well on the program, and we show weak support for the assertion that coverage-based test suites help Tarantula to localize faults. Baudry et al. used automatically-generated mutants to evaluate the accuracy of an FL technique that generates many distinct scores for program locations. We find no evidence to suggest that the use of mutants for this purpose is invalid. However, we find evidence that the standard method for evaluating FL accuracy is unfairly biased toward techniques that generate many distinct scores, and we propose a fairer method of accuracy evaluation. Finally, Denmat et al. suggest that data mining techniques may apply to FL. We investigate this suggestion with the data mining tool Weka, using standard techniques for evaluating the accuracy of data mining classifiers. We find that standard classifiers suffer from the class imbalance problem. However, we find that adding cost information improves accuracy.},
booktitle = {Proceedings of the 24th IEEE/ACM International Conference on Automated Software Engineering},
pages = {76–87},
numpages = {12},
keywords = {mutation analysis, data mining, Fault localization},
series = {ASE '09}
}

@inproceedings{10.1145/3611643.3616337,
author = {Liu, Jiawei and Peng, Jinjun and Wang, Yuyao and Zhang, Lingming},
title = {NeuRI: Diversifying DNN Generation via Inductive Rule Inference},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616337},
doi = {10.1145/3611643.3616337},
abstract = {Deep Learning (DL) is prevalently used in various industries to improve decision-making and automate processes, driven by the ever-evolving DL libraries and compilers. The correctness of DL systems is crucial for trust in DL applications.  
As such, the recent wave of research has been studying the automated synthesis of test-cases (i.e., DNN models and their inputs) for fuzzing DL systems. However, existing model generators only subsume a limited number of operators, lacking the ability to pervasively model operator constraints.  
To address this challenge, we propose NeuRI, a fully automated approach for generating valid and diverse DL models composed of hundreds of types of operators. NeuRI adopts a three-step process:  
(i) collecting valid and invalid API traces from various sources;  
(ii) applying inductive program synthesis over the traces to infer the constraints for constructing valid models; and  
(iii) using hybrid model generation which incorporates both symbolic and concrete operators.  
Our evaluation shows that NeuRI improves branch coverage of TensorFlow and PyTorch by 24% and 15% over the state-of-the-art model-level fuzzers. NeuRI finds 100 new bugs for PyTorch and TensorFlow in four months, with 81 already fixed or confirmed. Of these, 9 bugs are labelled as high priority or security vulnerability, constituting 10% of all high-priority bugs of the period.  
Open-source developers regard error-inducing tests reported by us as "high-quality" and "common in practice".},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {657–669},
numpages = {13},
keywords = {Compiler Testing, Deep Learning Compilers, Fuzzing},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1109/ASE56229.2023.00083,
author = {Ni, Chao and Yang, Kaiwen and Zhu, Yan and Chen, Xiang and Yang, Xiaohu},
title = {Unifying Defect Prediction, Categorization, and Repair by Multi-task Deep Learning},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00083},
doi = {10.1109/ASE56229.2023.00083},
abstract = {Just-In-Time defect prediction models can identify defect-inducing commits at check-in time and many approaches are proposed with remarkable performance. However, these approaches still have a few limitations which affect their effectiveness and practical usage: (1) partially using semantic information or structure information of code, (2) coarsely providing results to a commit (buggy or clean), and (3) independently investigating the defect prediction model and defect repair model.In this study, to handle the aforementioned limitations, we propose a unified defect prediction and repair framework named CompDefect, which can identify whether a changed function inside a commit is defect-prone, categorize the type of defect, and repair such a defect automatically if it falls into several scenarios, e.g., defects with single statement fixes, or those that match a small set of defect templates. Technically, the first two tasks in CompDefect are treated as a multiclass classification task, while the last task is treated as a sequence generation task.To verify the effectiveness of CompDefect, we first build a large-scale function-level dataset (i.e., 21,047) named Function-SStuBs4J and then compare CompDefect with tens of state-of-the-art (SOTA) approaches by considering five performance measures. The experimental results indicate that CompDefect outperforms all SOTAs with a substantial improvement in three tasks separately. Moreover, the pipeline experimental results also indicate the feasibility of CompDefect to unify three tasks in a model.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1980–1992},
numpages = {13},
keywords = {just-in-time defect prediction, defect categorization, defect repair},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3533767.3534373,
author = {Huang, Pei and Yang, Yuting and Liu, Minghao and Jia, Fuqi and Ma, Feifei and Zhang, Jian},
title = {𝜀-weakened robustness of deep neural networks},
year = {2022},
isbn = {9781450393799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533767.3534373},
doi = {10.1145/3533767.3534373},
abstract = {Deep neural networks have been widely adopted for many real-world applications and their reliability has been widely concerned. This paper introduces a notion of ε-weakened robustness (briefly as ε-robustness) for analyzing the reliability and some related quality issues of deep neural networks. Unlike the conventional robustness, which focuses on the “perfect” safe region in the absence of adversarial examples, ε-weakened robustness focuses on the region where the proportion of adversarial examples is bounded by user-specified ε. The smaller the value of ε is, the less vulnerable a neural network is to be fooled by a random perturbation. Under such a robustness definition, we can give conclusive results for the regions where conventional robustness ignores. We propose an efficient testing-based method with user-controllable error bounds to analyze it. The time complexity of our algorithms is polynomial in the dimension and size of the network. So, they are scalable to large networks. One of the important applications of our ε-robustness is to build a robustness enhanced classifier to resist adversarial attack. Based on this theory, we design a robustness enhancement method with good interpretability and rigorous robustness guarantee. The basic idea is to resist perturbation with perturbation. Experimental results show that our robustness enhancement method can significantly improve the ability of deep models to resist adversarial attacks while maintaining the prediction performance on the original clean data. Besides, we also show the other potential value of ε-robustness in neural networks analysis.},
booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {126–138},
numpages = {13},
keywords = {testing, robustness, neural networks, adversarial attack},
location = {Virtual, South Korea},
series = {ISSTA 2022}
}

@inproceedings{10.1145/3679006.3685068,
author = {Pedram, Saba and Labiche, Yvan},
title = {Using Category Partition to Detect Metamorphic Relations},
year = {2024},
isbn = {9798400711176},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3679006.3685068},
doi = {10.1145/3679006.3685068},
abstract = {The Category Partition (CP) functional testing method has proven to be useful in various contexts. It begins by identifying parameters and environment conditions on the basis of the function's behaviour. The characteristics/categories of these parameters/environment conditions are identified and partitioned into choices. The choices of a category are mutually exclusive and can be based on input partitioning and boundary value analysis. Thereafter, the choices are combined on the basis of a selection criterion to form test frames. Once input values satisfying the conditions of a test frame's choices are identified, one is equipped with a test case. This paper suggests and demonstrates that those test frames, once equipped with characterizations of output values, i.e., with categories and choices for outputs, can be considered Metamorphic Relations to be used in Metamorphic Testing.},
booktitle = {Proceedings of the 9th ACM International Workshop on Metamorphic Testing},
pages = {10–17},
numpages = {8},
keywords = {Automation, Category partition, Metamorphic testing},
location = {Vienna, Austria},
series = {MET 2024}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00045,
author = {Berend, David},
title = {Distribution awareness for AI system testing},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00045},
doi = {10.1109/ICSE-Companion52605.2021.00045},
abstract = {As Deep Learning (DL) is continuously adopted in many safety critical applications, its quality and reliability start to raise concerns. Similar to the traditional software development process, testing the DL software to uncover its defects at an early stage is an effective way to reduce risks after deployment. Although recent progress has been made in designing novel testing techniques for DL software, the distribution of generated test data is not taken into consideration. It is therefore hard to judge whether the identified errors are indeed meaningful errors to the DL application. Therefore, we propose a new distribution aware testing technique which aims to generate new unseen test cases relevant to the underlying DL system task. Our results show that this technique is able to filter up to 55.44% of error test case on CIFAR-10 and is 10.05% more effective in enhancing robustness.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {96–98},
numpages = {3},
keywords = {software testing, distribution awareness, deep learning},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3686614.3686616,
author = {Hallal, Hicham and Faizan, Alisha},
title = {Model Checking Based Test Adaptation in Changing Business Software},
year = {2024},
isbn = {9798400718052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686614.3686616},
doi = {10.1145/3686614.3686616},
abstract = {Testing large software applications poses a major challenge, especially in the presence of changes that are usually introduced after the initial deployment of said applications. This makes it rather difficult to anticipate the impact of the introduced changes throughout the initial development lifecycle. Any existing test suite that was used to validate the application before the changes needs to be modified accordingly to cover the updated or new functionality. Test adaptation techniques can be used to alleviate this issue, and to offer tools allowing the automated adaptation of the test suite to the changes. In this paper, we explore the problem of test adaptation in the case of changing business applications, where we focus on changes introduced at the level of the business logic. We propose a model checking based approach to test adaptation that uses behavioral models inferred from execution traces of application under test (AUT). Models of different versions of the changing application are compared to detect the introduced changes, to evaluate the validity of existing test cases, and to adapt the ones invalidated by the changes. We illustrate the applicability of the approach using the example of a travel management business process.},
booktitle = {Proceedings of the 2024 6th International Conference on Software Engineering and Development},
pages = {14–22},
numpages = {9},
keywords = {Change Management, Model Checking, Model Inference, Test Adaptation, Test Case validation},
location = {Hong Kong, Hong Kong},
series = {ICSED '24}
}

@article{10.1145/3544792,
author = {Zohdinasab, Tahereh and Riccio, Vincenzo and Gambi, Alessio and Tonella, Paolo},
title = {Efficient and Effective Feature Space Exploration for Testing Deep Learning Systems},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3544792},
doi = {10.1145/3544792},
abstract = {Assessing the quality of Deep Learning (DL) systems is crucial, as they are increasingly adopted in safety-critical domains. Researchers have proposed several input generation techniques for DL systems. While such techniques can expose failures, they do not explain which features of the test inputs influenced the system’s (mis-) behaviour. DeepHyperion was the first test generator to overcome this limitation by exploring the DL systems’ feature space at large. In this article, we propose DeepHyperion-CS, a test generator for DL systems that enhances DeepHyperion by promoting the inputs that contributed more to feature space exploration during the previous search iterations. We performed an empirical study involving two different test subjects (i.e., a digit classifier and a lane-keeping system for self-driving cars). Our results proved that the contribution-based guidance implemented within DeepHyperion-CS outperforms state-of-the-art tools and significantly improves the efficiency and the effectiveness of DeepHyperion. DeepHyperion-CS exposed significantly more misbehaviours for five out of six feature combinations and was up to 65% more efficient than DeepHyperion in finding misbehaviour-inducing inputs and exploring the feature space. DeepHyperion-CS was useful for expanding the datasets used to train the DL systems, populating up to 200% more feature map cells than the original training set.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {49},
numpages = {38},
keywords = {self-driving cars, search based software engineering, Deep Learning, Software testing}
}

@inproceedings{10.1145/2905055.2905123,
author = {Singh, Satwinder and Singla, Rozy},
title = {Comparative Performance of Fault-Prone Prediction Classes with K-means Clustering and MLP},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905123},
doi = {10.1145/2905055.2905123},
abstract = {Software defect in today's era is most important in the field of software engineering. Most of the organizations used various techniques to predict defects in their products before they are delivered. Defect prediction techniques help the organizations to use their resources effectively which results in lower cost and time requirements. There are various techniques that are used for predicting defects in software before it has to be delivered. For example clustering, neural networks, support vector machine (SVM) etc. In this paper two defect prediction techniques: - K-means Clustering and Multilayer Perceptron model (MLP), are compared. Both the techniques are implemented on different platforms. K-means clustering is implemented using WEKA tool and MLP is implemented using SPSS. The results are compared to find which algorithm produces better results. In this paper Object-Oriented metrics are used for predicting defects in the software.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {65},
numpages = {7},
keywords = {Weka, Object-Oriented Metrics, Neural Network, K-means Clustering, Defect prediction},
location = {Udaipur, India},
series = {ICTCS '16}
}

@inproceedings{10.1145/3603287.3656162,
author = {Zhang, Ziliang and Gray, Jeff},
title = {Enhanced Test Case Expression for End-User Developers},
year = {2024},
isbn = {9798400702372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603287.3656162},
doi = {10.1145/3603287.3656162},
abstract = {Low-Code Development Platforms (LCDPs) have significantly transformed the field of software development, offering streamlined solutions for application creation. The research summarized in this poster addresses the enhancement of test case expression in LCDPs, focusing on creating user-friendly tools for streamlined test case creation and reusability. Key innovations include a drag-and-drop interface facilitating test case generation, applicable to diverse domains such as banking and HR systems. This poster highlights notable advancements in LCDP testing, leading to the improved efficiency and adaptability of the developed methodologies. There are promising implications for the future of software testing in LCDPs, emphasizing the potential for broader application and continuous improvement for end-user developers.},
booktitle = {Proceedings of the 2024 ACM Southeast Conference},
pages = {317–318},
numpages = {2},
keywords = {Low-Code development, test case expression, testing adaptability},
location = {Marietta, GA, USA},
series = {ACMSE '24}
}

@article{10.1145/3563213,
author = {Ghorbani, Negar and Jabbarvand, Reyhaneh and Salehnamadi, Navid and Garcia, Joshua and Malek, Sam},
title = {DeltaDroid: Dynamic Delivery Testing in Android},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3563213},
doi = {10.1145/3563213},
abstract = {Android is a highly fragmented platform with a diverse set of devices and users. To support the deployment of apps in such a heterogeneous setting, Android has introduced dynamic delivery—a new model of software deployment in which optional, device- or user-specific functionalities of an app, called Dynamic Feature Modules (DFMs), can be installed, as needed, after the app’s initial installation. This model of app deployment, however, has exacerbated the challenges of properly testing Android apps. In this article, we first describe the results of an extensive study in which we formalized a defect model representing the various conditions under which DFM installations may fail. We then present DeltaDroid—a tool aimed at assisting the developers with validating dynamic delivery behavior in their apps by augmenting their existing test suite. Our experimental evaluation using real-world apps corroborates DeltaDroid’s ability to detect many crashes and unexpected behaviors that the existing automated testing tools cannot reveal.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {84},
numpages = {26},
keywords = {Android applications, test augmentation, dynamic delivery, Software testing}
}

@inproceedings{10.1145/3368089.3409750,
author = {Zhang, Fuyuan and Chowdhury, Sankalan Pal and Christakis, Maria},
title = {DeepSearch: a simple and effective blackbox attack for deep neural networks},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409750},
doi = {10.1145/3368089.3409750},
abstract = {Although deep neural networks have been very successful in image-classification tasks, they are prone to adversarial attacks. To generate adversarial inputs, there has emerged a wide variety of techniques, such as black- and whitebox attacks for neural networks. In this paper, we present DeepSearch, a novel fuzzing-based, query-efficient, blackbox attack for image classifiers. Despite its simplicity, DeepSearch is shown to be more effective in finding adversarial inputs than state-of-the-art blackbox approaches. DeepSearch is additionally able to generate the most subtle adversarial inputs in comparison to these approaches.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {800–812},
numpages = {13},
keywords = {Deep Neural Networks, Blackbox Fuzzing, Adversarial Attack},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1109/TASLP.2022.3153269,
author = {Jiao, Wenxiang and Wang, Xing and He, Shilin and Tu, Zhaopeng and King, Irwin and Lyu, Michael R.},
title = {Exploiting Inactive Examples for Natural Language Generation With Data Rejuvenation},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3153269},
doi = {10.1109/TASLP.2022.3153269},
abstract = {Recent years have witnessed the success of natural language generation (NLG) accomplished by deep neural networks, which require a large amount of training data for optimization. With the constant increase of data scale, the complex patterns and potential noises make training NLG models difficult. In order to fully utilize large-scale training data, we explore inactive examples in the training data and propose to rejuvenate the inactive examples for improving the performance of NLG models. Specifically, we define inactive examples as those sentence pairs that contribute less to the performance of NLG models, and show that their existence is independent of model variants but mainly determined by the data distribution. We further introduce &lt;italic&gt;data rejuvenation&lt;/italic&gt; to improve the training of NLG models by re-labeling the inactive examples. The rejuvenated examples and active examples are combined to train a final NLG model. We evaluate our approach by experiments on machine translation (MT) and text summarization (TS) tasks, and achieve significant improvements of performance. Extensive analyses reveal that inactive examples are more difficult to learn than active ones and rejuvenation can reduce the learning difficulty, which stabilizes and accelerates the training process of NLG models and results in models with better generalization capability.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {931–943},
numpages = {13}
}

@article{10.1145/762483.762485,
author = {Van Renesse, Robbert and Birman, Kenneth P. and Vogels, Werner},
title = {Astrolabe: A robust and scalable technology for distributed system monitoring, management, and data mining},
year = {2003},
issue_date = {May 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0734-2071},
url = {https://doi.org/10.1145/762483.762485},
doi = {10.1145/762483.762485},
abstract = {Scalable management and self-organizational capabilities are
emerging as central requirements for a generation of large-scale,
highly dynamic, distributed applications. We have developed an
entirely new distributed information management system called
Astrolabe. Astrolabe collects large-scale system state, permitting
rapid updates and providing on-the-fly attribute aggregation. This
latter capability permits an application to locate a resource, and
also offers a scalable way to track system state as it evolves over
time. The combination of features makes it possible to solve a wide
variety of management and self-configuration problems. This paper
describes the design of the system with a focus upon its
scalability. After describing the Astrolabe service, we present
examples of the use of Astrolabe for locating resources,
publish-subscribe, and distributed synchronization in large
systems. Astrolabe is implemented using a peer-to-peer protocol,
and uses a restricted form of mobile code based on the SQL query
language for aggregation. This protocol gives rise to a novel
consistency model. Astrolabe addresses several security
considerations using a built-in PKI. The scalability of the system
is evaluated using both simulation and experiments; these confirm
that Astrolabe could scale to thousands and perhaps millions of
nodes, with information propagation delays in the tens of seconds.},
journal = {ACM Trans. Comput. Syst.},
month = may,
pages = {164–206},
numpages = {43},
keywords = {scalability, publish-subscribe, membership, gossip, failure detection, epidemic protocols, Aggregation}
}

@inproceedings{10.1145/1390817.1390822,
author = {Jiang, Yue and Cukic, Bojan and Menzies, Tim},
title = {Can data transformation help in the detection of fault-prone modules?},
year = {2008},
isbn = {9781605580517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390817.1390822},
doi = {10.1145/1390817.1390822},
abstract = {Data preprocessing (transformation) plays an important role in data mining and machine learning. In this study, we investigate the effect of four different preprocessing methods to fault-proneness prediction using nine datasets from NASA Metrics Data Programs (MDP) and ten classification algorithms. Our experiments indicate that log transformation rarely improves classification performance, but discretization affects the performance of many different algorithms. The impact of different transformations differs. Random forest algorithm, for example, performs better with original and log transformed data set. Boosting and NaiveBayes perform significantly better with discretized data. We conclude that no general benefit can be expected from data transformations. Instead, selected transformation techniques are recommended to boost the performance of specific classification algorithms.},
booktitle = {Proceedings of the 2008 Workshop on Defects in Large Software Systems},
pages = {16–20},
numpages = {5},
location = {Seattle, Washington},
series = {DEFECTS '08}
}

@inproceedings{10.1145/3639631.3639674,
author = {Li, He and Hong, Danni and Zhou, Zhenwei and Li, Xiangning and Liu, Junbin},
title = {Fault Diagnosis of Avionics System Based on Improved CNN and BiLSTM},
year = {2024},
isbn = {9798400709203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639631.3639674},
doi = {10.1145/3639631.3639674},
abstract = {This paper presents a novel approach for diagnosing faults in aerospace systems by leveraging the power of Word2vec to transform text data into word vectors. Using improved Convolutional Neural Network(CNN) and bi-directional LSTM models to extract fault feature information from the input data, introducing an attention mechanism to focus more on key feature information in the data, and employing a dropout regularization method to prevent the model from being overfitted. Our model considers local features, as well as contextual information, which fully extracts the feature information of the text to classify the fault data. Compared with other methods, the proposed method outperforms other techniques and significantly reduces the manual dependence of fault classification, saves the troubleshooting time of maintenance personnel, and achieves the optimum in accuracy, recall and f1-score performance indicators, which demonstrates the effectiveness of our approach.},
booktitle = {Proceedings of the 2023 6th International Conference on Algorithms, Computing and Artificial Intelligence},
pages = {254–260},
numpages = {7},
keywords = {Avionics system, Convolutional network, Fault diagnosis, Text feature extraction},
location = {Sanya, China},
series = {ACAI '23}
}

@inproceedings{10.1109/ICSE43902.2021.00107,
author = {Jiang, Nan and Lutellier, Thibaud and Tan, Lin},
title = {CURE: Code-Aware Neural Machine Translation for Automatic Program Repair},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00107},
doi = {10.1109/ICSE43902.2021.00107},
abstract = {Automatic program repair (APR) is crucial to improve software reliability. Recently, neural machine translation (NMT) techniques have been used to fix software bugs automatically. While promising, these approaches have two major limitations. Their search space often does not contain the correct fix, and their search strategy ignores software knowledge such as strict code syntax. Due to these limitations, existing NMT-based techniques underperform the best template-based approaches.We propose CURE, a new NMT-based APR technique with three major novelties. First, CURE pre-trains a programming language (PL) model on a large software codebase to learn developer-like source code before the APR task. Second, CURE designs a new code-aware search strategy that finds more correct fixes by focusing on compilable patches and patches that are close in length to the buggy code. Finally, CURE uses a subword tokenization technique to generate a smaller search space that contains more correct fixes.Our evaluation on two widely-used benchmarks shows that CURE correctly fixes 57 Defects4J bugs and 26 QuixBugs bugs, outperforming all existing APR techniques on both benchmarks.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1161–1173},
numpages = {13},
keywords = {software reliability, automatic program repair},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3387940.3391456,
author = {Kang, Sungmin and Feldt, Robert and Yoo, Shin},
title = {SINVAD: Search-based Image Space Navigation for DNN Image Classifier Test Input Generation},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391456},
doi = {10.1145/3387940.3391456},
abstract = {The testing of Deep Neural Networks (DNNs) has become increasingly important as DNNs are widely adopted by safety critical systems. While many test adequacy criteria have been suggested, automated test input generation for many types of DNNs remains a challenge because the raw input space is too large to randomly sample or to navigate and search for plausible inputs. Consequently, current testing techniques for DNNs depend on small local perturbations to existing inputs, based on the metamorphic testing principle. We propose new ways to search not over the entire image space, but rather over a plausible input space that resembles the true training distribution. This space is constructed using Variational Autoencoders (VAEs), and navigated through their latent vector space. We show that this space helps efficiently produce test inputs that can reveal information about the robustness of DNNs when dealing with realistic tests, opening the field to meaningful exploration through the space of highly structured images.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {521–528},
numpages = {8},
keywords = {Test Data Generation, Search-based Software Engineering, Neural Network},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3377811.3380400,
author = {Tian, Yuchi and Zhong, Ziyuan and Ordonez, Vicente and Kaiser, Gail and Ray, Baishakhi},
title = {Testing DNN image classifiers for confusion &amp; bias errors},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380400},
doi = {10.1145/3377811.3380400},
abstract = {Image classifiers are an important component of today's software, from consumer and business applications to safety-critical domains. The advent of Deep Neural Networks (DNNs) is the key catalyst behind such wide-spread success. However, wide adoption comes with serious concerns about the robustness of software systems dependent on DNNs for image classification, as several severe erroneous behaviors have been reported under sensitive and critical circumstances. We argue that developers need to rigorously test their software's image classifiers and delay deployment until acceptable. We present an approach to testing image classifier robustness based on class property violations.We found that many of the reported erroneous cases in popular DNN image classifiers occur because the trained models confuse one class with another or show biases towards some classes over others. These bugs usually violate some class properties of one or more of those classes. Most DNN testing techniques focus on perimage violations, so fail to detect class-level confusions or biases.We developed a testing technique to automatically detect class-based confusion and bias errors in DNN-driven image classification software. We evaluated our implementation, DeepInspect, on several popular image classifiers with precision up to 100% (avg. 72.6%) for confusion errors, and up to 84.3% (avg. 66.8%) for bias errors. DeepInspect found hundreds of classification mistakes in widely-used models, many exposing errors indicating confusion or bias.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1122–1134},
numpages = {13},
keywords = {DNNs, bias, deep learning, image classifiers, whitebox testing},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3533767.3534408,
author = {Li, Yu and Chen, Muxi and Xu, Qiang},
title = {HybridRepair: towards annotation-efficient repair for deep learning models},
year = {2022},
isbn = {9781450393799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533767.3534408},
doi = {10.1145/3533767.3534408},
abstract = {A well-trained deep learning (DL) model often cannot achieve expected performance after deployment due to the mismatch between the distributions of the training data and the field data in the operational environment. Therefore, repairing DL models is critical, especially when deployed on increasingly larger tasks with shifted distributions. Generally speaking, it is easy to obtain a large amount of field data. Existing solutions develop various techniques to select a subset for annotation and then fine-tune the model for repair. While effective, achieving a higher repair rate is inevitably associated with more expensive labeling costs. To mitigate this problem, we propose a novel annotation-efficient repair solution for DL models, namely HybridRepair, wherein we take a holistic approach that coordinates the use of a small amount of annotated data and a large amount of unlabeled data for repair. Our key insight is that accurate yet sufficient training data is needed to repair the corresponding failure region in the data distribution. Under a given labeling budget, we selectively annotate some data in failure regions and propagate their labels to the neighboring data on the one hand. On the other hand, we take advantage of the semi-supervised learning (SSL) techniques to further boost the training data density. However, different from existing SSL solutions that try to use all the unlabeled data, we only use a selected part of them considering the impact of distribution shift on SSL solutions. Experimental results show that HybridRepair outperforms both state-of-the-art DL model repair solutions and semi-supervised techniques for model improvements, especially when there is a distribution shift between the training data and the field data. Our code is available at: https://github.com/cure-lab/HybridRepair.},
booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {227–238},
numpages = {12},
keywords = {semi-supervised learning, model repairing, deep neural networks},
location = {Virtual, South Korea},
series = {ISSTA 2022}
}

@article{10.1145/3591227,
author = {Gao, Fengjuan and Wang, Yu and Wang, Ke},
title = {Discrete Adversarial Attack to Models of Code},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591227},
doi = {10.1145/3591227},
abstract = {The pervasive brittleness of deep neural networks has attracted significant attention in recent years. A particularly interesting finding is the existence of adversarial examples, imperceptibly perturbed natural inputs that induce erroneous predictions in state-of-the-art neural models. In this paper, we study a different type of adversarial examples specific to code models, called discrete adversarial examples, which are created through program transformations that preserve the semantics of original inputs.In particular, we propose a novel, general method that is highly effective in attacking a broad range of code models. From the defense perspective, our primary contribution is a theoretical foundation for the application of adversarial training — the most successful algorithm for training robust classifiers — to defending code models against discrete adversarial attack. Motivated by the theoretical results, we present a simple realization of adversarial training that substantially improves the robustness of code models against adversarial attacks in practice. We extensively evaluate both our attack and defense methods. Results show that our discrete attack is significantly more effective than state-of-the-art whether or not defense mechanisms are in place to aid models in resisting attacks. In addition, our realization of adversarial training improves the robustness of all evaluated models by the widest margin against state-of-the-art adversarial attacks as well as our own.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {113},
numpages = {24},
keywords = {Models of code, Discrete Adversarial Attack, Adversarial Training}
}

@article{10.1145/3617385,
author = {Cerf, Vinton G.},
title = {The Dilemma of Scale},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/3617385},
doi = {10.1145/3617385},
journal = {Commun. ACM},
month = sep,
pages = {5},
numpages = {1}
}

@inproceedings{10.1145/3551349.3559528,
author = {Wang, Sen and Sheng, Zhuheng and Xu, Jingwei and Chen, Taolue and Zhu, Junjun and Zhang, Shuhui and Yao, Yuan and Ma, Xiaoxing},
title = {ADEPT: A Testing Platform for Simulated Autonomous Driving},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3559528},
doi = {10.1145/3551349.3559528},
abstract = {Effective quality assurance methods for autonomous driving systems ADS have attracted growing interests recently. In this paper, we report a new testing platform ADEPT, aiming to provide practically realistic and comprehensive testing facilities for DNN-based ADS. ADEPT is based on the virtual simulator CARLA and provides numerous testing facilities such as scene construction, ADS importation, test execution and recording, etc. In particular, ADEPT features two distinguished test scenario generation strategies designed for autonomous driving. First, we make use of real-life accident reports from which we leverage natural language processing to fabricate abundant driving scenarios. Second, we synthesize physically-robust adversarial attacks by taking the feedback of ADS into consideration and thus are able to generate closed-loop test scenarios. The experiments confirm the efficacy of the platform.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {150},
numpages = {4},
keywords = {Testing platform, Test case generation, Software testing, Deep neural networks, Autonomous driving},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3366424.3383117,
author = {Gupta, Somit and Shi, Xiaolin and Dmitriev, Pavel and Fu, Xin},
title = {Challenges, Best Practices and Pitfalls in Evaluating Results of Online Controlled Experiments},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383117},
doi = {10.1145/3366424.3383117},
abstract = {A/B Testing is the gold standard to estimate the causal relationship between a change in a product and its impact on key outcome measures. It is widely used in the industry to test changes ranging from simple copy change or UI change to more complex changes like using machine learning models to personalize user experience. The key aspect of A/B testing is evaluation of experiment results. Designing the right set of metrics - correct outcome measures, data quality indicators, guardrails that prevent harm to business, and a comprehensive set of supporting metrics to understand the “why” behind the key movements is the #1 challenge practitioners face when trying to scale their experimentation program 11, 14. On the technical side, improving sensitivity of experiment metrics is a hard problem and an active research area, with large practical implications as more and more small and medium size businesses are trying to adopt A/B testing and suffer from insufficient power. In this tutorial we will discuss challenges, best practices, and pitfalls in evaluating experiment results, focusing on both lessons learned and practical guidelines as well as open research questions. A version of this tutorial was also present at KDD 2019 23. It was attended by around 150 participants. This tutorial has also been accepted for the WSDM 2020 conference.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {317–319},
numpages = {3},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/2591062.2591099,
author = {Zhang, Sai and Zhang, Congle},
title = {Software bug localization with markov logic},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591099},
doi = {10.1145/2591062.2591099},
abstract = {Software bug localization is the problem of determining buggy statements in a software system. It is a crucial and expensive step in the software debugging process. Interest in it has grown rapidly in recent years, and many approaches have been proposed. However, existing approaches tend to use isolated information to address the problem, and are often ad hoc. In particular, most existing approaches predict the likelihood of a statement being buggy sequentially and separately.  This paper proposes a well-founded, integrated solution to the software bug localization problem based on Markov logic. Markov logic combines first-order logic and probabilistic graphical models by attaching weights to first-order formulas, and views them as templates for features of Markov networks. We show how a number of salient program features can be seamlessly combined in Markov logic, and how the resulting joint inference can be solved.  We implemented our approach in a debugging system, called MLNDebugger, and evaluated it on 4 small programs. Our initial results demonstrated that our approach achieved higher accuracy than a previous approach.},
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {424–427},
numpages = {4},
keywords = {Machine learning, Automated debugging},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@inproceedings{10.1145/3324884.3415292,
author = {Yu, Runze and Zhang, Youzhe and Xuan, Jifeng},
title = {MetPurity: a learning-based tool of pure method identification for automatic test generation},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3415292},
doi = {10.1145/3324884.3415292},
abstract = {In object-oriented programming, a method is pure if calling the method does not change object states that exist in the pre-states of the method call. Pure methods are widely-used in automatic techniques, including test generation, compiler optimization, and program repair. Due to the source code dependency, it is infeasible to completely and accurately identify all pure methods. Instead, existing techniques such as ReImInfer are designed to identify a subset of accurate results of pure method and mark the other methods as unknown ones. In this paper, we designed and implemented MetPurity, a learning-based tool of pure method identification. Given all methods in a project, MetPurity labels a training set via automatic program analysis and builds a binary classifier (implemented with the random forest classifier) based on the training set. This classifier is used to predict the purity of all the other methods (i.e., unknown ones) in the same project. Preliminary evaluation on four open-source Java projects shows that MetPurity can provide a list of identified pure methods with a low error rate. Applying MetPurity to EvoSuite can increase the number of generated assertions for regression testing in test generation by EvoSuite.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1326–1330},
numpages = {5},
keywords = {debugging, machine learning, method purity, regression testing, static analysis, test generation},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3639478.3639802,
author = {Qiu, Ketai},
title = {Autonomic Testing: Testing with Scenarios from Production},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639802},
doi = {10.1145/3639478.3639802},
abstract = {My PhD addresses the problem of detecting field failures with a new approach to test software systems under conditions that emerge only in production. Ex-vivo approaches detect field failures by executing the software system in the testbed with data extracted from the production environment. In-vivo approaches execute the available test suites in the production environment. We will define autonomic testing that detects conditions that emerge only in production scenarios, generates test cases for the new conditions, and executes the generated test cases in the new scenarios, to detect failures before they occur in production.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {156–158},
numpages = {3},
keywords = {autonomic testing, failure detection, test generation},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3632366.3632389,
author = {Adeyemo, Hayatullahi B and Bahsoon, Rami and Ti\v{n}o, Peter},
title = {An Approach for Dynamic Behavioural Prediction and Fault Injection in Cyber-Physical Systems},
year = {2024},
isbn = {9798400704734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632366.3632389},
doi = {10.1145/3632366.3632389},
abstract = {Modern technology integrates Cyber-Physical Systems (CPS), merging computational and physical processes. Ensuring CPS dependability is vital in averting adverse effects on critical applications due to unforeseen behaviour. To fortify CPS resilience, a novel technique for dynamic behavioural prediction and fault injection is introduced. It predicts dynamic CPS behaviour through system modelling under diverse operational scenarios, employing a fault model with diverse fault classes. Unlike the single model tenet, this approach engages multiple expert models to simulate both faultless and faulty behaviours. By adopting this approach, we can inject specialised faults and scale the analysis of the faults together or separately. Injecting faults assesses system reactions and reveals vulnerabilities. Tested on a water tank system, the approach proves effective in behaviour prediction and proactive fault handling, enhancing CPS design for robust, secure, and fault-tolerant systems.},
booktitle = {Proceedings of the IEEE/ACM 10th International Conference on Big Data Computing, Applications and Technologies},
articleno = {08},
numpages = {6},
keywords = {cyber-physical system, fault, modelling, dynamic behavioural prediction, fault injection},
location = {Taormina (Messina), Italy},
series = {BDCAT '23}
}

@inproceedings{10.1145/3533767.3534364,
author = {Wu, Zhiyong and Liang, Jie and Wang, Mingzhe and Zhou, Chijin and Jiang, Yu},
title = {Unicorn: detect runtime errors in time-series databases with hybrid input synthesis},
year = {2022},
isbn = {9781450393799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533767.3534364},
doi = {10.1145/3533767.3534364},
abstract = {The ubiquitous use of time-series databases in the safety-critical Internet of Things domain demands strict security and correctness. One successful approach in database bug detection is fuzzing, where  
hundreds of bugs have been detected automatically in relational databases. However, it cannot be easily applied to time-series databases: the bulk of time-series logic is unreachable because of mismatched query specifications, and serious bugs are undetectable because of implicitly handled exceptions.  
In this paper, we propose Unicorn to secure time-series databases with automated fuzzing. First, we design hybrid input synthesis to generate high-quality queries which not only cover time-series features but also ensure grammar correctness. Then, Unicorn uses proactive exception detection to discover minuscule-symptom bugs which hide behind implicit exception handling. With the specialized  
design oriented to time-series databases, Unicorn outperforms the state-of-the-art database fuzzers in terms of coverage and bugs. Specifically, Unicorn outperforms SQLsmith and SQLancer on  
widely used time-series databases IoTDB, KairosDB, TimescaleDB, TDEngine, QuestDB, and GridDB in the number of basic blocks by 21%-199% and 34%-693%, respectively. More importantly, Unicorn  
has discovered 42 previously unknown bugs.},
booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {251–262},
numpages = {12},
keywords = {Time-series Databases, Runtime Error, Hybrid Input Synthesis},
location = {Virtual, South Korea},
series = {ISSTA 2022}
}

@article{10.1145/3565479,
author = {Moss, Emanuel},
title = {Trust Is Not Enough: Accuracy, Error, Randomness, and Accountability in an Algorithmic Society},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/3565479},
doi = {10.1145/3565479},
abstract = {Searching for improved algorithmic accountability.},
journal = {Commun. ACM},
month = may,
pages = {42–44},
numpages = {3}
}

@inproceedings{10.1145/3526072.3527537,
author = {Diller, Abigail C. and Fredericks, Erik M.},
title = {Towards run-time search for real-world multi-agent systems},
year = {2023},
isbn = {9781450393188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526072.3527537},
doi = {10.1145/3526072.3527537},
abstract = {Multi-agent systems (MAS) may encounter uncertainties in the form of unexpected environmental conditions, sub-optimal system configurations, and unplanned interactions between autonomous agents. The number of combinations of such uncertainties may be innumerable, however run-time testing may reduce the issues impacting such a system. We posit that search heuristics can augment a run-time testing process, in-situ, for a MAS. To support our position we discuss our in-progress experimental testbed to realize this goal and highlight challenges we anticipate for this domain.},
booktitle = {Proceedings of the 15th Workshop on Search-Based Software Testing},
pages = {14–15},
numpages = {2},
keywords = {search-based software testing, multi-agent systems, cyber-physical systems},
location = {Pittsburgh, Pennsylvania},
series = {SBST '22}
}

@inproceedings{10.1145/3611643.3616266,
author = {Wang, Longtian and Xie, Xiaofei and Du, Xiaoning and Tian, Meng and Guo, Qing and Yang, Zheng and Shen, Chao},
title = {DistXplore: Distribution-Guided Testing for Evaluating and Enhancing Deep Learning Systems},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616266},
doi = {10.1145/3611643.3616266},
abstract = {Deep learning (DL) models are trained on sampled data, where the distribution of training data differs from that of real-world data (i.e., the distribution shift), which reduces the model's robustness. Various testing techniques have been proposed, including distribution-unaware and distribution-aware methods. However, distribution-unaware testing lacks effectiveness by not explicitly considering the distribution of test cases and may generate redundant errors (within same distribution). Distribution-aware testing techniques primarily focus on generating test cases that follow the training distribution, missing out-of-distribution data that may also be valid and should be considered in the testing process.  
In this paper, we propose a novel distribution-guided approach for generating valid test cases with diverse distributions, which can better evaluate the model's robustness (i.e., generating hard-to-detect errors) and enhance the model's robustness (i.e., enriching training data). Unlike existing testing techniques that optimize individual test cases, DistXplore optimizes test suites that represent specific distributions. To evaluate and enhance the model's robustness, we design two metrics: distribution difference, which maximizes the similarity in distribution between two different classes of data to generate hard-to-detect errors, and distribution diversity, which increase the distribution diversity of generated test cases for enhancing the model's robustness. To evaluate the effectiveness of DistXplore in model evaluation and enhancement, we compare DistXplore with 14 state-of-the-art baselines on 10 models across 4 datasets. The evaluation results show that DisXplore not only detects a larger number of errors (e.g., 2\texttimes{}+ on average). Furthermore, DistXplore achieves a higher improvement in empirical robustness (e.g., 5.2% more accuracy improvement than the baselines on average).},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {68–80},
numpages = {13},
keywords = {Deep learning, distribution diversity, model enhancement, software testing},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@article{10.1145/3302542.3302544,
author = {Danglot, Benjamin and An, Gabin},
title = {Genetic improvement events in 2018},
year = {2019},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
url = {https://doi.org/10.1145/3302542.3302544},
doi = {10.1145/3302542.3302544},
abstract = {Genetic improvement uses optimisation and machine learning techniques, particularly heuristic search and evolutionary algorithms, to improve existing software. The main application is automatic bug fixing, where reducing or eliminating buggy behaviour improves a program. Other applications involve automatically producing a better program that runs faster, uses less memory, uses less energy, or runs on a different type of computer.This article summarise two international events exclusively devoted to this topic in 2018.},
journal = {SIGEVOlution},
month = jan,
pages = {9–13},
numpages = {5}
}

@inproceedings{10.1145/3456126.3456135,
author = {Li, Longbo and Zhou, Yanhui and Yuan, Yuan and Wu, Shenghua},
title = {An Extensive Study on Multi-Priority Algorithm in Test Case Prioritization and Reduction},
year = {2021},
isbn = {9781450389082},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3456126.3456135},
doi = {10.1145/3456126.3456135},
abstract = {Although test case prioritization and reduction are two different problems in regression testing, they are essentially interrelated. To improve the effectiveness of regression testing, we need to perform fewer test cases, and hope to detect program faults as early as possible. However, most existing techniques haven't proposed a better solution to solve these two issues at the same time. In this paper, we present a multi-priority algorithm combine mutation testing and clustering techniques, and use clustering techniques to put test cases with similar fault-detection ability into a cluster, the multi-priority algorithm selects a high-priority test in each cluster. The results show that the multi-priority algorithm not only reduces a large number of test cases, but also obtains the results of test case prioritization better than greedy algorithm and random reduction method. Especially, the average reduction size of test cases is 40.23% in total 556018 test cases, and removing test cases that trigger real program faults only accounts for 0.7435% of all tests. Our method achieves a greater reduction in the number of test cases at the expense of a mini loss of in fault-detection ability. The effectiveness of test case prioritization is 2.63% higher than other methods. In addition, we find that the different number of clusters affect the effectiveness of test case prioritization and reduction in regression testing.1},
booktitle = {2021 2nd Asia Service Sciences and Software Engineering Conference},
pages = {48–57},
numpages = {10},
keywords = {test case prioritization and reduction, regression testing, mutation testing, clustering},
location = {Macau, Macao},
series = {ASSE '21}
}

@inproceedings{10.5555/3237383.3237831,
author = {de Jong, Michiel and Zhang, Kevin and Roth, Aaron M. and Rhodes, Travers and Schmucker, Robin and Zhou, Chenghui and Ferreira, Sofia and Cartucho, Jo\~{a}o and Veloso, Manuela},
title = {Towards a Robust Interactive and Learning Social Robot},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Pepper is a humanoid robot, specifically designed for social interaction, that has been deployed in a variety of public environments. A programmable version of Pepper is also available, enabling our focused research on perception and behavior robustness and capabilities of an interactive social robot. We address Pepper perception by integrating state-of-the-art vision and speech recognition systems and experimentally analyzing their effectiveness. As we recognize limitations of the individual perceptual modalities, we introduce a multi-modality approach to increase the robustness of human social interaction with the robot. We combine vision, gesture, speech, and input from an onboard tablet, a remote mobile phone, and external microphones. Our approach includes the proactive seeking of input from a different modality, adding robustness to the failures of the separate components. We also introduce a learning algorithm to improve communication capabilities over time, updating speech recognition through social interactions. Finally, we realize the rich robot body-sensory data and introduce both a nearest-neighbor and a deep learning approach to enable Pepper to classify and speak up a variety of its own body motions. We view the contributions of our work to be relevant both to Pepper specifically and to other general social robots.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {883–891},
numpages = {9},
keywords = {social robot, service robot, robot machine learning, robot autonomy, pepper, human-robot interaction},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{10.1145/3377811.3380415,
author = {Gao, Xiang and Saha, Ripon K. and Prasad, Mukul R. and Roychoudhury, Abhik},
title = {Fuzz testing based data augmentation to improve robustness of deep neural networks},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380415},
doi = {10.1145/3377811.3380415},
abstract = {Deep neural networks (DNN) have been shown to be notoriously brittle to small perturbations in their input data. This problem is analogous to the over-fitting problem in test-based program synthesis and automatic program repair, which is a consequence of the incomplete specification, i.e., the limited tests or training examples, that the program synthesis or repair algorithm has to learn from. Recently, test generation techniques have been successfully employed to augment existing specifications of intended program behavior, to improve the generalizability of program synthesis and repair. Inspired by these approaches, in this paper, we propose a technique that re-purposes software testing methods, specifically mutation-based fuzzing, to augment the training data of DNNs, with the objective of enhancing their robustness. Our technique casts the DNN data augmentation problem as an optimization problem. It uses genetic search to generate the most suitable variant of an input data to use for training the DNN, while simultaneously identifying opportunities to accelerate training by skipping augmentation in many instances. We instantiate this technique in two tools, Sensei and Sensei-SA, and evaluate them on 15 DNN models spanning 5 popular image data-sets. Our evaluation shows that Sensei can improve the robust accuracy of the DNN, compared to the state of the art, on each of the 15 models, by upto 11.9% and 5.5% on average. Further, Sensei-SA can reduce the average DNN training time by 25%, while still improving robust accuracy.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1147–1158},
numpages = {12},
keywords = {DNN, data augmentation, genetic algorithm, robustness},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1145/3563332,
author = {Muduli, Sujit Kumar and Roy, Subhajit},
title = {Satisfiability modulo fuzzing: a synergistic combination of SMT solving and fuzzing},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3563332},
doi = {10.1145/3563332},
abstract = {Programming languages and software engineering tools routinely encounter components that are difficult to reason on via formal techniques or whose formal semantics are not even available—third-party libraries, inline assembly code, SIMD instructions, system calls, calls to machine learning models, etc. However, often access to these components is available as input-output oracles—interfaces are available to query these components on certain inputs to receive the respective outputs. We refer to such functions as closed-box functions. Regular SMT solvers are unable to handle such closed-box functions. We propose Sundefineddhak, a solver for SMT theories modulo closed-box functions. Our core idea is to use a synergistic combination of a fuzzer to reason on closed-box functions and an SMT engine to solve the constraints pertaining to the SMT theories. The fuzz and the SMT engines attempt to converge to a model by exchanging a rich set of interface constraints that are relevant and interpretable by them. Our implementation, Sundefineddhak, demonstrates a significant advantage over the only other solver that is capable of handling such closed-box constraints: Sundefineddhak solves 36.45% more benchmarks than the best-performing mode of this state-of-the-art solver and has 5.72x better PAR-2 score; on the benchmarks that are solved by both tools, Sundefineddhak is (on an average) 14.62x faster.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {169},
numpages = {28},
keywords = {SMT, Fuzzing, Conflict-Driven Fuzz Loop, Closed-Box Function}
}

@inproceedings{10.1145/1774088.1774612,
author = {Sami, Ashkan and Fakhrahmad, Seyed Mostafa},
title = {Design-level metrics estimation based on code metrics},
year = {2010},
isbn = {9781605586397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1774088.1774612},
doi = {10.1145/1774088.1774612},
abstract = {Fault detection based on mining code and design metrics has been an active research area for many years. Basically "module"-based metrics for source code and design level are calculated or obtained and data mining is used to build predictor models. However, in many projects due to organizational or software process models, design level metrics are not available and/or accurate. It has been shown that performance of these classifiers or predictors decline if only source code features are used for training them. Based on best of our know knowledge no set of rule to estimate design level metrics based on code level metrics has been presented since it is believed that design level metrics have additional information and cannot be estimated without access to design artifacts. In this study we present a fuzzy modeling system to find and present these relationships for projects presented in NASA Metrics Data Repository (MDP) datasets. Interestingly, we could find a set of empirical rules that govern all the projects regardless of size, programming language and software development methodology. Comparison of fault detectors built based on estimated design metrics with actual design metrics on various projects showed a very small difference in accuracy of classifiers and validated our hypothesis that estimation of design metrics based on source code attributes can become a practical exercise.},
booktitle = {Proceedings of the 2010 ACM Symposium on Applied Computing},
pages = {2531–2535},
numpages = {5},
keywords = {software metrics, software defect prediction, parameter estimation, fuzzy classification, approximate dependencies},
location = {Sierre, Switzerland},
series = {SAC '10}
}

@inproceedings{10.1145/2875913.2875922,
author = {Tang, Hao and Lan, Tian and Hao, Dan and Zhang, Lu},
title = {Enhancing Defect Prediction with Static Defect Analysis},
year = {2015},
isbn = {9781450336413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2875913.2875922},
doi = {10.1145/2875913.2875922},
abstract = {In the software development process, how to develop better software at lower cost has been a major issue of concern. One way that helps is to find more defects as early as possible, on which defect prediction can provide effective guidance. The most popular defect prediction technique is to build defect prediction models based on machine learning. To improve the performance of defect prediction model, selecting appropriate features is critical. On the other hand, static analysis is usually used in defect detection. As static defect analyzers detects defects by matching some well-defined "defect patterns", its result is useful for locating defects. However, defect prediction and static defect analysis are supposed to be two parallel areas due to the differences in research motivation, solution and granularity.In this paper, we present a possible approach to improve the performance of defect prediction with the help of static analysis techniques. Specifically, we present to extract features based on defect patterns from static defect analyzers to improve the performance of defect prediction models. Based on this approach, we implemented a defect prediction tool and set up experiments to measure the effect of the features.},
booktitle = {Proceedings of the 7th Asia-Pacific Symposium on Internetware},
pages = {43–51},
numpages = {9},
keywords = {static defect analyzer, predictive model, machine learning, defect pattern, code feature, Defect},
location = {Wuhan, China},
series = {Internetware '15}
}

@article{10.1145/3696117.3696123,
author = {Melegati, Jorge and Petrik, Dimitri and Saltan, Andrey},
title = {7th International Workshop on Software-intensive Business (IWSiB 2024): Software-intensive Business in the Era of Generative Artificial Intelligence},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/3696117.3696123},
doi = {10.1145/3696117.3696123},
abstract = {The 7th International Workshop on Software-intensive Business (IWSiB'24), held on April 16, 2024, in Lisbon, Portugal, as part of the 46th International Conference on Software Engineering (ICSE 2024), brought together diverse research communities to address the pressing challenges faced by software-producing organizations. These challenges, driven by changing demands, evolving technologies, and dynamic environments, require interdisciplinary collaboration across software engineering and business research. The workshop focused on the theme "Software Business in the Era of Generative Artificial Intelligence," highlighting the transformative impact of GenAI and large language models on software businesses. The event featured a keynote by Jan Bosch and showcased 11 accepted papers on a diverse range of topics. Participants engaged in interactive discussions, group activities, and ideation sessions aimed at identifying future challenges and solutions for software-intensive businesses, fostering ongoing community development and cross-disciplinary synergy.},
journal = {SIGSOFT Softw. Eng. Notes},
month = oct,
pages = {22–25},
numpages = {4},
keywords = {agile, software ecosystem, artificial intelligence, ethics., software-intensive business}
}

@inproceedings{10.1145/3551349.3561156,
author = {Richter, Cedric and Haltermann, Jan and Jakobs, Marie-Christine and Pauck, Felix and Schott, Stefan and Wehrheim, Heike},
title = {Are Neural Bug Detectors Comparable to Software Developers on Variable Misuse Bugs?},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3561156},
doi = {10.1145/3551349.3561156},
abstract = {Debugging, that is, identifying and fixing bugs in software, is a central part of software development. Developers are therefore often confronted with the task of deciding whether a given code snippet contains a bug, and if yes, where. Recently, data-driven methods have been employed to learn this task of bug detection, resulting (amongst others) in so called neural bug detectors. Neural bug detectors are trained on millions of buggy and correct code snippets. Given the “neural learning” procedure, it seems likely that neural bug detectors – on the specific task of finding bugs – have a performance similar to human software developers. For this work, we set out to substantiate or refute such a hypothesis. We report on the results of an empirical study with over 100&nbsp;software developers, targeting the comparison of humans and neural bug detectors. As detection task, we chose a specific form of bugs (variable misuse bugs) for which neural bug detectors have recently made significant progress. Our study shows that despite the fact that neural bug detectors see millions of such misuse bugs during training, software developers – when conducting bug detection as a majority decision – are slightly better than neural bug detectors on this class of bugs. Altogether, we find a large overlap in the performance, both for classifying code as buggy and for localizing the buggy line in the code. In comparison to developers, one of the two evaluated neural bug detectors, however, raises a higher number of false alarms in our study.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {9},
numpages = {12},
keywords = {variable misuse bugs, empirical study., Bug detection},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3597926.3598135,
author = {Wu, Yi and Jiang, Nan and Pham, Hung Viet and Lutellier, Thibaud and Davis, Jordan and Tan, Lin and Babkin, Petr and Shah, Sameena},
title = {How Effective Are Neural Networks for Fixing Security Vulnerabilities},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598135},
doi = {10.1145/3597926.3598135},
abstract = {Security vulnerability repair is a difficult task that is in dire need of automation. Two groups of techniques have shown promise: (1) large code language models (LLMs) that have been pre-trained on source code for tasks such as code completion, and (2) automated program repair (APR) techniques that use deep learning (DL) models to automatically fix software bugs. This paper is the first to study and compare Java vulnerability repair capabilities of LLMs and DL-based APR models. The contributions include that we (1) apply and evaluate five LLMs (Codex, CodeGen, CodeT5, PLBART and InCoder), four fine-tuned LLMs, and four DL-based APR techniques on two real-world Java vulnerability benchmarks (Vul4J and VJBench), (2) design code transformations to address the training and test data overlapping threat to Codex, (3) create a new Java vulnerability repair benchmark VJBench, and its transformed version VJBench-trans, to better evaluate LLMs and APR techniques, and (4) evaluate LLMs and APR techniques on the transformed vulnerabilities in VJBench-trans. Our findings include that (1) existing LLMs and APR models fix very few Java vulnerabilities. Codex fixes 10.2 (20.4%), the most number of vulnerabilities. Many of the generated patches are uncompilable patches. (2) Fine-tuning with general APR data improves LLMs’ vulnerability-fixing capabilities. (3) Our new VJBench reveals that LLMs and APR models fail to fix many Common Weakness Enumeration (CWE) types, such as CWE-325 Missing cryptographic step and CWE-444 HTTP request smuggling. (4) Codex still fixes 8.7 transformed vulnerabilities, outperforming all the other LLMs and APR models on transformed vulnerabilities. The results call for innovations to enhance automated Java vulnerability repair such as creating larger vulnerability repair training data, tuning LLMs with such data, and applying code simplification transformation to facilitate vulnerability repair.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1282–1294},
numpages = {13},
keywords = {Vulnerability, Language Model, Automated Program Repair, AI and Software Engineering},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3078155.3078158,
author = {McAllister, Jeff and Levy, Uri},
title = {Challenges and Opportunities in Native GPU Debugging},
year = {2017},
isbn = {9781450352147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078155.3078158},
doi = {10.1145/3078155.3078158},
abstract = {In this technical session we present the open architectural design of the debugger and how it fits into the OpenCL JIT compilation flow. We demonstrate a show case on how to natively work with the debugger to solve functional bugs, as-well-as low-level debugging techniques on SIMD thread level which help to solve complex issues such as misaligned or out of range accesses to local/global memory, stack overflows, Illegal instructions, etc. Finally, we cover the challenges in debugging.},
booktitle = {Proceedings of the 5th International Workshop on OpenCL},
articleno = {7},
numpages = {1},
keywords = {Artificial Intelligence, Autonomous Driving, Computer Vision, Deep Learning, Machine Learning, OpenCL, Performance, Smart Camera},
location = {Toronto, Canada},
series = {IWOCL '17}
}

@article{10.1145/3583564,
author = {Tian, Yongqiang and Zhang, Wuqi and Wen, Ming and Cheung, Shing-Chi and Sun, Chengnian and Ma, Shiqing and Jiang, Yu},
title = {Finding Deviated Behaviors of the Compressed DNN Models for Image Classifications},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3583564},
doi = {10.1145/3583564},
abstract = {Model compression can significantly reduce the sizes of deep neural network (DNN) models and thus facilitate the dissemination of sophisticated, sizable DNN models, especially for deployment on mobile or embedded devices. However, the prediction results of compressed models may deviate from those of their original models. To help developers thoroughly understand the impact of model compression, it is essential to test these models to find those deviated behaviors before dissemination. However, this is a non-trivial task, because the architectures and gradients of compressed models are usually not available.To this end, we propose Dflare, a novel, search-based, black-box testing technique to automatically find triggering inputs that result in deviated behaviors in image classification tasks. Dflare iteratively applies a series of mutation operations to a given seed image until a triggering input is found. For better efficacy and efficiency, Dflare models the search problem as Markov Chains and leverages the Metropolis-Hasting algorithm to guide the selection of mutation operators in each iteration. Further, Dflare utilizes a novel fitness function to prioritize the mutated inputs that either cause large differences between two models’ outputs or trigger previously unobserved models’ probability vectors. We evaluated Dflare on 21 compressed models for image classification tasks with three datasets. The results show that Dflare not only constantly outperforms the baseline in terms of efficacy but also significantly improves the efficiency: Dflare is 17.84\texttimes{}–446.06\texttimes{} as fast as the baseline in terms of time; the number of queries required by Dflare to find one triggering input is only 0.186–1.937% of those issued by the baseline. We also demonstrated that the triggering inputs found by Dflare can be used to repair up to 48.48% deviated behaviors in image classification tasks and further decrease the effectiveness of Dflare on the repaired models.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {128},
numpages = {32},
keywords = {image classification models, neural networks, model compression, Model dissemination}
}

@article{10.1145/3597173,
author = {Acquisti, Alessandro and Steed, Ryan},
title = {Learning to Live with Privacy-Preserving Analytics},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/3597173},
doi = {10.1145/3597173},
abstract = {Seeking to close the gap between research and real-world applications of PPAs.},
journal = {Commun. ACM},
month = jun,
pages = {24–27},
numpages = {4}
}

@inproceedings{10.1145/3468264.3468586,
author = {Chen, Ke and Li, Yufei and Chen, Yingfeng and Fan, Changjie and Hu, Zhipeng and Yang, Wei},
title = {GLIB: towards automated test oracle for graphically-rich applications},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468586},
doi = {10.1145/3468264.3468586},
abstract = {Graphically-rich applications such as games are ubiquitous with attractive visual effects of Graphical User Interface (GUI) that offers a bridge between software applications and end-users. However, various types of graphical glitches may arise from such GUI complexity and have become one of the main component of software compatibility issues. Our study on bug reports from game development teams in NetEase Inc. indicates that graphical glitches frequently occur during the GUI rendering and severely degrade the quality of graphically-rich applications such as video games. Existing automated testing techniques for such applications focus mainly on generating various GUI test sequences and check whether the test sequences can cause crashes. These techniques require constant human attention to captures non-crashing bugs such as bugs causing graphical glitches. In this paper, we present the first step in automating the test oracle for detecting non-crashing bugs in graphically-rich applications. Specifically, we propose GLIB based on a code-based data augmentation technique to detect game GUI glitches. We perform an evaluation of GLIB on 20 real-world game apps (with bug reports available) and the result shows that GLIB can achieve 100% precision and 99.5% recall in detecting non-crashing bugs such as game GUI glitches. Practical application of GLIB on another 14 real-world games (without bug reports) further demonstrates that GLIB can effectively uncover GUI glitches, with 48 of 53 bugs reported by GLIB having been confirmed and fixed so far.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1093–1104},
numpages = {12},
keywords = {Game Testing, GUI Testing, Deep Learning, Automated Test Oracle},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3644032.3644463,
author = {Masserini, Elena and Ginelli, Davide and Micucci, Daniela and Briola, Daniela and Mariani, Leonardo},
title = {Anonymizing Test Data in Android: Does It Hurt?},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644463},
doi = {10.1145/3644032.3644463},
abstract = {Failure data collected from the field (e.g., failure traces, bug reports, and memory dumps) represent an invaluable source of information for developers who need to reproduce and analyze failures. Unfortunately, field data may include sensitive information and thus cannot be collected indiscriminately. Privacy-preserving techniques can address this problem anonymizing data and reducing the risk of disclosing personal information. However, collecting anonymized information may harm reproducibility, that is, the anonymized data may not allow the reproduction of a failure observed in the field. In this paper, we present an empirical investigation about the impact of privacy-preserving techniques on the reproducibility of failures. In particular, we study how five privacy-preserving techniques may impact reproducibilty for 19 bugs in 17 Android applications. Results provide insights on how to select and configure privacy-preserving techniques.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {88–98},
numpages = {11},
keywords = {data anonymization, privacy-preserving, privacy, bug reproduction, mobile applications, debugging, testing},
location = {Lisbon, Portugal},
series = {AST '24}
}

@inproceedings{10.1145/3611643.3616296,
author = {Eberlein, Martin and Smytzek, Marius and Steinh\"{o}fel, Dominic and Grunske, Lars and Zeller, Andreas},
title = {Semantic Debugging},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616296},
doi = {10.1145/3611643.3616296},
abstract = {Why does my program fail? We present a novel and general technique to automatically determine failure causes and conditions, using logical properties over input elements: “The program fails if and only if int(&lt;length&gt;) &gt; len(&lt;payload&gt;) holds—that is, the given &lt;length&gt; is larger than the &lt;payload&gt; length.” Our AVICENNA prototype uses modern techniques for inferring properties of passing and failing inputs and validating and refining hypotheses by having a constraint solver generate supporting test cases to obtain such diagnoses. As a result, AVICENNA produces crisp and expressive diagnoses even for complex failure conditions, considerably improving over the state of the art with diagnoses close to those of human experts.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {438–449},
numpages = {12},
keywords = {behavior explanation, debugging, program behavior, testing},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@article{10.1145/3720526,
author = {Kim, Donguk and Jeon, Minseok and Hwang, Doha and Oh, Hakjoo},
title = {PAFL: Enhancing Fault Localizers by Leveraging Project-Specific Fault Patterns},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3720526},
doi = {10.1145/3720526},
abstract = {We present PAFL, a new technique for enhancing existing fault localization methods by leveraging project-specific fault patterns.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
We observed that each software project has its own challenges and suffers from recurring fault patterns associated with those challenges. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
However, existing fault localization techniques use a universal localization strategy without considering those repetitive faults.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
To address this limitation, our technique, called project-aware fault localization (PAFL), enables existing fault localizers to leverage project-specific fault patterns.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Given a buggy version of a project and a baseline fault localizer, PAFL first mines the fault patterns from past buggy versions of the project. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Then, it uses the mined fault patterns to update the suspiciousness scores of statements computed by the baseline fault localizer.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
To this end, we use two novel ideas.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
First, we design a domain-specific fault pattern-description language to represent various fault patterns.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
An instance, called crossword, in our language describes a project-specific fault pattern and how it affects the suspiciousness scores of statements.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Second, we develop an algorithm that synthesizes crosswords (i.e., fault patterns) from past buggy versions of the project.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Evaluation using seven baseline fault localizers and 12 real-world C/C++ and Python projects demonstrates that PAFL effectively, robustly, and efficiently improves the performance of the baseline fault localization techniques.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {129},
numpages = {28},
keywords = {Domain-Specific Language, Fault Localization, Program Synthesis}
}

@article{10.1145/3417330,
author = {Ma, Wei and Papadakis, Mike and Tsakmalis, Anestis and Cordy, Maxime and Traon, Yves Le},
title = {Test Selection for Deep Learning Systems},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3417330},
doi = {10.1145/3417330},
abstract = {Testing of deep learning models is challenging due to the excessive number and complexity of the computations involved. As a result, test data selection is performed manually and in an ad hoc way. This raises the question of how we can automatically select candidate data to test deep learning models. Recent research has focused on defining metrics to measure the thoroughness of a test suite and to rely on such metrics to guide the generation of new tests. However, the problem of selecting/prioritising test inputs (e.g., to be labelled manually by humans) remains open. In this article, we perform an in-depth empirical comparison of a set of test selection metrics based on the notion of model uncertainty (model confidence on specific inputs). Intuitively, the more uncertain we are about a candidate sample, the more likely it is that this sample triggers a misclassification. Similarly, we hypothesise that the samples for which we are the most uncertain are the most informative and should be used in priority to improve the model by retraining. We evaluate these metrics on five models and three widely used image classification problems involving real and artificial (adversarial) data produced by five generation algorithms. We show that uncertainty-based metrics have a strong ability to identify misclassified inputs, being three times stronger than surprise adequacy and outperforming coverage-related metrics. We also show that these metrics lead to faster improvement in classification accuracy during retraining: up to two times faster than random selection and other state-of-the-art metrics on all models we considered.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {13},
numpages = {22},
keywords = {software testing, software engineering, Deep learning testing}
}

@article{10.1145/3707455,
author = {Jiang, Yingjie and Mo, Ran and Zhan, Wenjing and Wang, Dongyu and Li, Zengyang and Ma, Yutao},
title = {Leveraging Modular Architecture for Bug Characterization and Analysis in Automated Driving Software},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3707455},
doi = {10.1145/3707455},
abstract = {With the rapid advancement of automated driving technology, numerous manufacturers deploy vehicles with auto-driving features. This highlights the importance of ensuring the quality of automated driving software. To achieve this, characterizing bugs in automated driving software is important, as it can facilitate bug detection and bug fixes, thereby ensuring software quality. Automated driving software typically has a modular architecture, where software is divided into multiple modules, each designed for its own functionality for automated driving. This may lead to varying bug characteristics. Additionally, our recent study has shown a correlation between bugs caused by code clones and the functionalities of modules in automated driving software. Hence, we consider the modular structure when analyzing bug characteristics. In this paper, we analyze 3,078 bugs from two representative open-source Level-4 automated driving systems, Apollo and Autoware. By analyzing the bug report description, title, and developers’ discussions, we have identified 20 bug symptoms and 17 bug-fixing strategies, and analyzed their relationships with the respective modules. Our analysis achieves 12 main findings offering a comprehensive view of bug characteristics in automated driving software. We believe our findings can help developers better understand and manage bugs in automated driving software, thereby improving software quality and reliability.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {Automated Driving Software, Software Modules, Bug Characterization, Bug Analysis}
}

@article{10.1145/3533818,
author = {Birchler, Christian and Khatiri, Sajad and Derakhshanfar, Pouria and Panichella, Sebastiano and Panichella, Annibale},
title = {Single and Multi-objective Test Cases Prioritization for Self-driving Cars in Virtual Environments},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3533818},
doi = {10.1145/3533818},
abstract = {Testing with simulation environments helps to identify critical failing scenarios for self-driving cars (SDCs). Simulation-based tests are safer than in-field operational tests and allow detecting software defects before deployment. However, these tests are very expensive and are too many to be run frequently within limited time constraints.In this article, we investigate test case prioritization techniques to increase the ability to detect SDC regression faults with virtual tests earlier. Our approach, called SDC-Prioritizer, prioritizes virtual tests for SDCs according to static features of the roads we designed to be used within the driving scenarios. These features can be collected without running the tests, which means that they do not require past execution results. We introduce two evolutionary approaches to prioritize the test cases using diversity metrics (black-box heuristics) computed on these static features. These two approaches, called SO-SDC-Prioritizer and MO-SDC-Prioritizer, use single-objective and multi-objective genetic algorithms (GA), respectively, to find trade-offs between executing the less expensive tests and the most diverse test cases earlier.Our empirical study conducted in the SDC domain shows that MO-SDC-Prioritizer significantly (P- value &lt;=0.1e-10) improves the ability to detect safety-critical failures at the same level of execution time compared to baselines: random and greedy-based test case orderings. Besides, our study indicates that multi-objective meta-heuristics outperform single-objective approaches when prioritizing simulation-based tests for SDCs.MO-SDC-Prioritizer prioritizes test cases with a large improvement in fault detection while its overhead (up to 0.45% of the test execution cost) is negligible.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {28},
numpages = {30},
keywords = {test case prioritization, software simulation, Autonomous systems}
}

@inproceedings{10.1145/3533767.3534392,
author = {Eniser, Hasan Ferit and Gros, Timo P. and W\"{u}stholz, Valentin and Hoffmann, J\"{o}rg and Christakis, Maria},
title = {Metamorphic relations via relaxations: an approach to obtain oracles for action-policy testing},
year = {2022},
isbn = {9781450393799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533767.3534392},
doi = {10.1145/3533767.3534392},
abstract = {Testing is a promising way to gain trust in a learned action policy π, in particular if π is a neural network. A “bug” in this context constitutes undesirable or fatal policy behavior, e.g., satisfying a failure condition. But how do we distinguish whether such behavior is due to bad policy decisions, or whether it is actually unavoidable under the given circumstances? This requires knowledge about optimal solutions, which defeats the scalability of testing. Related problems occur in software testing when the correct program output is not known. Metamorphic testing addresses this issue through metamorphic relations, specifying how a given change to the input should affect the output, thus providing an oracle for the correct output. Yet, how do we obtain such metamorphic relations for action policies? Here, we show that the well explored concept of relaxations in the Artificial Intelligence community can serve this purpose. In particular, if state s′ is a relaxation of state s, i.e., s′ is easier to solve than s, and π fails on easier s′ but does not fail on harder s, then we know that π contains a bug manifested on s′. We contribute the first exploration of this idea in the context of failure testing of neural network policies π learned by reinforcement learning in simulated environments. We design fuzzing strategies for test-case generation as well as metamorphic oracles leveraging simple, manually designed relaxations. In experiments on three single-agent games, our technology is able to effectively identify true bugs, i.e., avoidable failures of π, which has not been possible until now.},
booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {52–63},
numpages = {12},
keywords = {metamorphic testing, fuzzing, action policies},
location = {Virtual, South Korea},
series = {ISSTA 2022}
}

@inproceedings{10.1145/3650212.3680332,
author = {Alian, Parsa and Nashid, Noor and Shahbandeh, Mobina and Mesbah, Ali},
title = {Semantic Constraint Inference for Web Form Test Generation},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680332},
doi = {10.1145/3650212.3680332},
abstract = {Automated test generation for web forms has been a longstanding challenge, exacerbated by the intrinsic human-centric design of forms and their complex, device-agnostic structures. We introduce an innovative approach, called FormNexus, for automated web form test generation, which emphasizes deriving semantic insights from individual form elements and relations among them, utilizing textual content, DOM tree structures, and visual proximity. The insights gathered are transformed into a new conceptual graph, the Form Entity Relation Graph (FERG), which offers machine-friendly semantic information extraction. Leveraging LLMs, FormNexus adopts a feedback-driven mechanism for generating and refining input constraints based on real-time form submission responses. The culmination of this approach is a robust set of test cases, each produced by methodically invalidating constraints, ensuring comprehensive testing scenarios for web forms. This work bridges the existing gap in automated web form testing by intertwining the capabilities of LLMs with advanced semantic inference methods. Our evaluation demonstrates that FormNexus combined with GPT-4 achieves 89% coverage in form submission states. This outcome significantly outstrips the performance of the best baseline model by a margin of 25%.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {932–944},
numpages = {13},
keywords = {Large Language Models, Test Input Generation, Web Forms},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@article{10.1145/3607191,
author = {Dang, Xueqi and Li, Yinghua and Papadakis, Mike and Klein, Jacques and Bissyand\'{e}, Tegawend\'{e} F. and Le Traon, Yves},
title = {GraphPrior: Mutation-based Test Input Prioritization for Graph Neural Networks},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3607191},
doi = {10.1145/3607191},
abstract = {Graph Neural Networks (GNNs) have achieved promising performance in a variety of practical applications. Similar to traditional DNNs, GNNs could exhibit incorrect behavior that may lead to severe consequences, and thus testing is necessary and crucial. However, labeling all the test inputs for GNNs can be costly and time-consuming, especially when dealing with large and complex graphs, which seriously affects the efficiency of GNN testing. Existing studies have focused on test prioritization for DNNs, which aims to identify and prioritize fault-revealing tests (i.e., test inputs that are more likely to be misclassified) to detect system bugs earlier in a limited time. Although some DNN prioritization approaches have been demonstrated effective, there is a significant problem when applying them to GNNs: They do not take into account the connections (edges) between GNN test inputs (nodes), which play a significant role in GNN inference. In general, DNN test inputs are independent of each other, while GNN test inputs are usually represented as a graph with complex relationships between each test. In this article, we propose GraphPrior (GNN-oriented Test Prioritization), a set of approaches to prioritize test inputs specifically for GNNs via mutation analysis. Inspired by mutation testing in traditional software engineering, in which test suites are evaluated based on the mutants they kill, GraphPrior generates mutated models for GNNs and regards test inputs that kill many mutated models as more likely to be misclassified. Then, GraphPrior leverages the mutation results in two ways, killing-based and feature-based methods. When scoring a test input, the killing-based method considers each mutated model equally important, while feature-based methods learn different importance for each mutated model through ranking models. Finally, GraphPrior ranks all the test inputs based on their scores. We conducted an extensive study based on 604 subjects to evaluate GraphPrior on both natural and adversarial test inputs. The results demonstrate that KMGP, the killing-based GraphPrior approach, outperforms the compared approaches in a majority of cases, with an average improvement of 4.76% ~49.60% in terms of APFD. Furthermore, the feature-based GraphPrior approach, RFGP, performs the best among all the GraphPrior approaches. On adversarial test inputs, RFGP outperforms the compared approaches across different adversarial attacks, with the average improvement of 2.95% ~46.69%.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {22},
numpages = {40},
keywords = {Labelling, Mutation, Graph Neural Networks, Test Input Prioritization}
}

@inproceedings{10.1109/ASE56229.2023.00010,
author = {Das, Debeshee and Mathews, Noble Saji and Mathai, Alex and Tamilselvam, Srikanth and Sedamaki, Kranthi and Chimalakonda, Sridhar and Kumar, Atul},
title = {COMEX: A Tool for Generating Customized Source Code Representations},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00010},
doi = {10.1109/ASE56229.2023.00010},
abstract = {Learning effective representations of source code is critical for any Machine Learning for Software Engineering (ML4SE) system. Inspired by natural language processing, large language models (LLMs) like Codex and CodeGen treat code as generic sequences of text and are trained on huge corpora of code data, achieving state of the art performance on several software engineering (SE) tasks. However, valid source code, unlike natural language, follows a strict structure and pattern governed by the underlying grammar of the programming language. Current LLMs do not exploit this property of the source code as they treat code like a sequence of tokens and overlook key structural and semantic properties of code that can be extracted from code-views like the Control Flow Graph (CFG), Data Flow Graph (DFG), Abstract Syntax Tree (AST), etc. Unfortunately, the process of generating and integrating code-views for every programming language is cumbersome and time consuming. To overcome this barrier, we propose our tool COMEX - a framework that allows researchers and developers to create and combine multiple code-views which can be used by machine learning (ML) models for various SE tasks. Some salient features of our tool are: (i) it works directly on source code (which need not be compilable), (ii) it currently supports Java and C#, (iii) it can analyze both method-level snippets and program-level snippets by using both intra-procedural and inter-procedural analysis, and (iv) it is easily extendable to other languages as it is built on tree-sitter - a widely used incremental parser that supports over 40 languages. We believe this easy-to-use code-view generation and customization tool will give impetus to research in source code representation learning methods and ML4SE. The source code and demonstration of our tool can be found at https://github.com/IBM/tree-sitter-codeviews and https://youtu.be/GER6U87FVbU, respectively.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {2054–2057},
numpages = {4},
keywords = {representation learning, static analysis},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3183440.3183456,
author = {Chen, Junjie},
title = {Learning to accelerate compiler testing},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3183456},
doi = {10.1145/3183440.3183456},
abstract = {Compilers are one of the most important software infrastructures. Compiler testing is an effective and widely-used way to assure the quality of compilers. While many compiler testing techniques have been proposed to detect compiler bugs, these techniques still suffer from the serious efficiency problem. This is because these techniques need to run a large number of randomly generated test programs on the fly through automated test-generation tools (e.g., Csmith). To accelerate compiler testing, it is desirable to schedule the execution order of the generated test programs so that the test programs that are more likely to trigger compiler bugs are executed earlier. Since different test programs tend to trigger the same compiler bug, the ideal goal of accelerating compiler testing is to execute the test programs triggering different compiler bugs in the beginning. However, such perfect goal is hard to achieve, and thus in this work, we design four steps to approach the ideal goal through learning, in order to largely accelerate compiler testing.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {472–475},
numpages = {4},
keywords = {compiler testing, machine learning, test prioritization},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@proceedings{10.1145/3643991,
title = {MSR '24: Proceedings of the 21st International Conference on Mining Software Repositories},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {MSR is a thriving research community that organizes a yearly conference with a solid reputation amongst software engineering researchers.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3611643.3616362,
author = {Moradi Moghadam, Mohsen and Bagherzadeh, Mehdi and Khatchadourian, Raffi and Bagheri, Hamid},
title = {𝜇Akka: Mutation Testing for Actor Concurrency in Akka using Real-World Bugs},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616362},
doi = {10.1145/3611643.3616362},
abstract = {Actor concurrency is becoming increasingly important in the real world and mission-critical software. This requires these applications to be free from actor bugs, that occur in the real world, and have tests that are effective in finding these bugs. Mutation testing is a well-established technique that transforms an application to induce its likely bugs and evaluate the effectiveness of its tests in finding these bugs. Mutation testing is available for a broad spectrum of applications and their bugs, ranging from web to mobile to machine learning, and is used at scale in companies like Google and Facebook. However, there still is no mutation testing for actor concurrency that uses real-world actor bugs. In this paper, we propose 𝜇Akka, a framework for mutation testing of Akka actor concurrency using real actor bugs. Akka is a popular industrial-strength implementation of actor concurrency. To design, implement, and evaluate 𝜇Akka, we take the following major steps: (1) manually analyze a recent set of 186 real Akka bugs from Stack Overflow and GitHub to understand their causes; (2) design a set of 32 mutation operators, with 138 source code changes in Akka API, to emulate these causes and induce their bugs; (3) implement these operators in an Eclipse plugin for Java Akka; (4) use the plugin to generate 11.7k mutants of 10 real GitHub applications, with 446.4k lines of code and 7.9k tests; (5) run these tests on these mutants to measure the quality of mutants and effectiveness of tests; (6) use PIT to generate 26.2k mutants to compare 𝜇Akka and PIT mutant quality and test effectiveness. PIT is a popular mutation testing tool with traditional operators; (7) manually analyze the bug coverage and overlap of 𝜇Akka, PIT, and actor operators in a previous work; and (8) discuss a few implications of our findings. Among others, we find that 𝜇Akka mutants are higher quality, cover more bugs, and tests are less effective in detecting them.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {262–274},
numpages = {13},
keywords = {Actor concurrency, Akka, Mutant quality, Mutation opertors, Mutation testing, Test effectiveness, 𝜇Akka},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3641399.3644114,
author = {He, Shilin},
title = {SPINE: A Scalable Log Parser with Feedback Guidance},
year = {2024},
isbn = {9798400717673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641399.3644114},
doi = {10.1145/3641399.3644114},
abstract = {Log parsing, which extracts log templates and parameters, is a critical prerequisite step for automated log analysis techniques. Though existing log parsers have achieved promising accuracy on public log datasets, they still face many challenges when applied in the industry. Through studying the characteristics of real-world log data and analyzing the limitations of existing log parsers, we identify two problems. Firstly, it is non-trivial to scale a log parser to a vast number of logs, especially in real-world scenarios where the log data is extremely imbalanced. Secondly, existing log parsers overlook the importance of user feedback, which is imperative for parser fine-tuning under the continuous evolution of log data. To overcome the challenges, we propose SPINE, which is a highly scalable log parser with user feedback guidance. Based on our log parser equipped with initial grouping and progressive clustering, we propose a novel log data scheduling algorithm to improve the efficiency of parallelization under the large-scale imbalanced log data. Besides, we introduce user feedback to make the parser fast adapt to the evolving logs. We evaluated SPINE on 16 public log datasets. SPINE achieves more than 0.90 parsing accuracy on average with the highest parsing efficiency, which outperforms the state-of-the-art log parsers. We also evaluated SPINE in the production environment of Microsoft, in which SPINE can parse 30 million logs in less than 8 minutes under 16 executors, achieving near real-time performance. In addition, our evaluations show that SPINE can consistently achieve good accuracy under log evolution with a moderate number of user feedback.},
booktitle = {Proceedings of the 17th Innovations in Software Engineering Conference},
articleno = {4},
numpages = {1},
keywords = {Program Analysis, Programming Language},
location = {Bangalore, India},
series = {ISEC '24}
}

@inproceedings{10.1109/ICSE43902.2021.00047,
author = {He, Pinjia and Meister, Clara and Su, Zhendong},
title = {Testing Machine Translation via Referential Transparency},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00047},
doi = {10.1109/ICSE43902.2021.00047},
abstract = {Machine translation software has seen rapid progress in recent years due to the advancement of deep neural networks. People routinely use machine translation software in their daily lives for tasks such as ordering food in a foreign restaurant, receiving medical diagnosis and treatment from foreign doctors, and reading international political news online. However, due to the complexity and intractability of the underlying neural networks, modern machine translation software is still far from robust and can produce poor or incorrect translations; this can lead to misunderstanding, financial loss, threats to personal safety and health, and political conflicts. To address this problem, we introduce referentially transparent inputs (RTIs), a simple, widely applicable methodology for validating machine translation software. A referentially transparent input is a piece of text that should have similar translations when used in different contexts. Our practical implementation, Purity, detects when this property is broken by a translation. To evaluate RTI, we use Purity to test Google Translate and Bing Microsoft Translator with 200 unlabeled sentences, which detected 123 and 142 erroneous translations with high precision (79.3% and 78.3%). The translation errors are diverse, including examples of under-translation, over-translation, word/phrase mistranslation, incorrect modification, and unclear logic.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {410–422},
numpages = {13},
keywords = {Testing, Referential transparency, Metamorphic testing, Machine translation},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3643991.3644934,
author = {Begoug, Mahi and Chouchen, Moataz and Ouni, Ali and Abdullah Alomar, Eman and Mkaouer, Mohamed Wiem},
title = {Fine-Grained Just-In-Time Defect Prediction at the Block Level in Infrastructure-as-Code (IaC)},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644934},
doi = {10.1145/3643991.3644934},
abstract = {Infrastructure-as-Code (IaC) is an emerging software engineering practice that leverages source code to facilitate automated configuration of software systems' infrastructure. IaC files are typically complex, containing hundreds of lines of code and dependencies, making them prone to defects, which can result in breaking online services at scale. To help developers early identify and fix IaC defects, research efforts have introduced IaC defect prediction models at the file level. However, the granularity of the proposed approaches remains coarse-grained, requiring developers to inspect hundreds of lines of code in a file, while only a small fragment of code is defective. To alleviate this issue, we introduce a machine-learning-based approach to predict IaC defects at a fine-grained level, focusing on IaC blocks, i.e., small code units that encapsulate specific behaviours within an IaC file. We trained various machine learning algorithms based on a mixture of code, process, and change-level metrics. We evaluated our approach on 19 open-source projects that use Terraform, a widely used IaC tool. The results indicated that there is no single algorithm that consistently outperforms the others in 19 projects. Overall, among the six algorithms, we observed that the LightGBM model achieved a higher average of 0.21 in terms of MCC and 0.71 in terms of AUC. Models analysis reveals that the developer's experience and the relative number of added lines tend to be the most important features. Additionally, we found that blocks belonging to the most frequent types are more prone to defects. Our defect prediction models have also shown sensitivity to concept drift, indicating that IaC practitioners should regularly retrain their models.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {100–112},
numpages = {13},
keywords = {defect prediction, infrastructure-as-code, IaC, terraform},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@article{10.1145/3622826,
author = {Wang, Yu and Wang, Ke and Wang, Linzhang},
title = {An Explanation Method for Models of Code},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622826},
doi = {10.1145/3622826},
abstract = {This paper introduces a novel method, called WheaCha, for explaining the predictions of code models. Similar to attribution methods, WheaCha seeks to identify input features that are responsible for a particular prediction that models make. On the other hand, it differs from attribution methods in crucial ways. Specifically, WheaCha separates an input program into "wheat" (i.e., defining features that are the reason for which models predict the label that they predict) and the rest "chaff" for any given prediction. We realize WheaCha in a tool, HuoYan, and use it to explain four prominent code models: code2vec, seq-GNN, GGNN, and CodeBERT. Results show that (1) HuoYan is efficient — taking on average under twenty seconds to compute wheat for an input program in an end-to-end fashion (i.e., including model prediction time); (2) the wheat that all models use to make predictions is predominantly comprised of simple syntactic or even lexical properties (i.e., identifier names); (3) neither the latest explainability methods for code models (i.e., SIVAND and CounterFactual Explanations) nor the most noteworthy attribution methods (i.e., Integrated Gradients and SHAP) can precisely capture wheat. Finally, we set out to demonstrate the usefulness of WheaCha, in particular, we assess if WheaCha’s explanations can help end users to identify defective code models (e.g., trained on mislabeled data or learned spurious correlations from biased data). We find that, with WheaCha, users achieve far higher accuracy in identifying faulty models than SIVAND, CounterFactual Explanations, Integrated Gradients and SHAP.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {250},
numpages = {27},
keywords = {Models of Code, Explainability Method, Defining Features}
}

@inproceedings{10.1145/3540250.3558958,
author = {Shetty, Manish and Bansal, Chetan and Upadhyayula, Sai Pramod and Radhakrishna, Arjun and Gupta, Anurag},
title = {AutoTSG: learning and synthesis for incident troubleshooting},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558958},
doi = {10.1145/3540250.3558958},
abstract = {Incident management is a key aspect of operating large-scale cloud services. To aid with faster and efficient resolution of incidents, engineering teams document frequent troubleshooting steps in the form of Troubleshooting Guides (TSGs), to be used by on-call engineers (OCEs). However, TSGs are siloed, unstructured, and often incomplete, requiring developers to manually understand and execute necessary steps. This results in a plethora of issues such as on-call fatigue, reduced productivity, and human errors. In this work, we conduct a large-scale empirical study of over 4K+ TSGs mapped to incidents and find that TSGs are widely used and help significantly reduce mitigation efforts. We then analyze feedback on TSGs provided by 400+ OCEs and propose a taxonomy of issues that highlights significant gaps in TSG quality. To alleviate these gaps, we investigate the automation of TSGs and propose AutoTSG -- a novel framework for automation of TSGs to executable workflows by combining machine learning and program synthesis. Our evaluation of AutoTSG on 50 TSGs shows the effectiveness in both identifying TSG statements (accuracy 0.89) and parsing them for execution (precision 0.94 and recall 0.91). Lastly, we survey ten Microsoft engineers and show the importance of TSG automation and the usefulness of AutoTSG.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1477–1488},
numpages = {12},
keywords = {Troubleshooting, Program Synthesis, Meta Learning, Incident Management, Cloud Reliability},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.5555/3545946.3598816,
author = {Gangopadhyay, Briti and Dasgupta, Pallab and Dey, Soumyajit},
title = {Counterexample-Guided Policy Refinement in Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-Agent Reinforcement Learning (MARL) policies are being incorporated into a wide range of safety-critical applications. It is important for these policies to be free of counterexamples and adhere to safety requirements. We present a methodology for the counterexample-guided refinement of an optimized MARL policy with respect to given safety specifications. The proposed algorithm refines a calibrated MARL policy to become safer by eliminating counterexamples found during testing, using targeted gradient updates. We empirically validate our method on different cooperative multi-agent tasks and demonstrate that targeted gradient updates induce safety in MARL policies.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1606–1614},
numpages = {9},
keywords = {counterexample-guided refinement, multi-agent proximal policy optimization, multi-agent reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3412841.3442027,
author = {Tsimpourlas, Foivos and Rajan, Ajitha and Allamanis, Miltiadis},
title = {Supervised learning over test executions as a test oracle},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442027},
doi = {10.1145/3412841.3442027},
abstract = {The challenge of automatically determining the correctness of test executions is referred to as the test oracle problem and is a key remaining issue for automated testing. The paper aims at solving the test oracle problem in a scalable and accurate way. To achieve this, we use supervised learning over test execution traces. We label a small fraction of the execution traces with their verdict of pass or fail. We use the labelled traces to train a neural network (NN) model to learn to distinguish runtime patterns for passing versus failing executions for a given program.We evaluate our approach using case studies from different application domains - 1. Module from Ethereum Blockchain, 2. Module from PyTorch deep learning framework, 3. Microsoft SEAL encryption library components and 4. Sed stream editor. We found the classification models for all subject programs resulted in high precision, recall and specificity, averaging to 89%, 88% and 92% respectively, while only training with an average 15% of the total traces. Our experiments show that the proposed NN model is promising as a test oracle and is able to learn runtime patterns to distinguish test executions for systems and tests from different application domains.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1521–1531},
numpages = {11},
keywords = {execution trace, neural networks, software testing, test oracle},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1109/ICSE43902.2021.00032,
author = {Dola, Swaroopa and Dwyer, Matthew B. and Soffa, Mary Lou},
title = {Distribution-Aware Testing of Neural Networks Using Generative Models},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00032},
doi = {10.1109/ICSE43902.2021.00032},
abstract = {The reliability of software that has a Deep Neural Network (DNN) as a component is urgently important today given the increasing number of critical applications being deployed with DNNs. The need for reliability raises a need for rigorous testing of the safety and trustworthiness of these systems. In the last few years, there have been a number of research efforts focused on testing DNNs. However the test generation techniques proposed so far lack a check to determine whether the test inputs they are generating are valid, and thus invalid inputs are produced. To illustrate this situation, we explored three recent DNN testing techniques. Using deep generative model based input validation, we show that all the three techniques generate significant number of invalid test inputs. We further analyzed the test coverage achieved by the test inputs generated by the DNN testing techniques and showed how invalid test inputs can falsely inflate test coverage metrics.To overcome the inclusion of invalid inputs in testing, we propose a technique to incorporate the valid input space of the DNN model under test in the test generation process. Our technique uses a deep generative model-based algorithm to generate only valid inputs. Results of our empirical studies show that our technique is effective in eliminating invalid tests and boosting the number of valid test inputs generated.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {226–237},
numpages = {12},
keywords = {test generation, test coverage, input validation, deep neural networks, deep learning},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1145/3721133,
author = {Lu, Yuntao and Bai, Chen and Zhao, Yuxuan and Zheng, Ziyue and Lyu, Yangdi and Liu, Mingyu and Yu, Bei},
title = {DeepVerifier: Learning to Update Test Sequences for Coverage-Guided Verification},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1084-4309},
url = {https://doi.org/10.1145/3721133},
doi = {10.1145/3721133},
abstract = {Verification is critical in ensuring the reliable operation of modern, complex computing systems. However, as processor designs become increasingly sophisticated, conventional static verification techniques struggle to generate high-quality test sequences that achieve comprehensive coverage. Dynamic simulation-based approaches, which leverage coverage-driven objectives, can increase confidence in correct processor functionality but often suffer from low verification efficiency due to the generation of redundant test sequences and significant computational overhead. To address these challenges, this paper presents DeepVerifier, a novel coverage-guided test generation framework that leverages data-driven learning of existing test sequences and their associated coverage feedback. DeepVerifier uses a language model to learn the semantic representations of test sequences, ensure adherence to syntax constraints, and estimate the relationship between test sequences and coverage scores. By updating test sequences with higher coverage, DeepVerifier can significantly improve the efficiency and effectiveness of the verification process. Experimental results of verifying an out-of-order RISC-V microprocessor demonstrate that the framework accurately estimates the coverage scores of test sequences and updates high-quality sequences that contribute to higher coverage. This coverage-guided test generation technique holds promise for enhancing the reliability of modern processor designs.},
note = {Just Accepted},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = mar
}

@inproceedings{10.1145/3338906.3338930,
author = {Li, Zenan and Ma, Xiaoxing and Xu, Chang and Cao, Chun and Xu, Jingwei and L\"{u}, Jian},
title = {Boosting operational DNN testing efficiency through conditioning},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338930},
doi = {10.1145/3338906.3338930},
abstract = {With the increasing adoption of Deep Neural Network (DNN) models as integral parts of software systems, efficient operational testing of DNNs is much in demand to ensure these models' actual performance in field conditions. A challenge is that the testing often needs to produce precise results with a very limited budget for labeling data collected in field.  Viewing software testing as a practice of reliability estimation through statistical sampling, we re-interpret the idea behind conventional structural coverages as conditioning for variance reduction. With this insight we propose an efficient DNN testing method based on the conditioning on the representation learned by the DNN model under testing. The representation is defined by the probability distribution of the output of neurons in the last hidden layer of the model. To sample from this high dimensional distribution in which the operational data are sparsely distributed, we design an algorithm leveraging cross entropy minimization.  Experiments with various DNN models and datasets were conducted to evaluate the general efficiency of the approach. The results show that, compared with simple random sampling, this approach requires only about a half of labeled inputs to achieve the same level of precision.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {499–509},
numpages = {11},
keywords = {Software testing, Neural networks, Coverage criteria},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@article{10.1145/3550270,
author = {Laurent, Thomas and Klikovits, Stefan and Arcaini, Paolo and Ishikawa, Fuyuki and Ventresque, Anthony},
title = {Parameter Coverage for Testing of Autonomous Driving Systems under Uncertainty},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3550270},
doi = {10.1145/3550270},
abstract = {Autonomous Driving Systems (ADSs) are promising, but must show they are secure and trustworthy before adoption. Simulation-based testing is a widely adopted approach, where the ADS is run in a simulated environment over specific scenarios. Coverage criteria specify what needs to be covered to consider the ADS sufficiently tested. However, existing criteria do not guarantee to exercise the different decisions that the ADS can make, which is essential to assess its correctness. ADSs usually compute their decisions using parameterised rule-based systems and cost functions, such as cost components or decision thresholds. In this article, we argue that the parameters characterise the decision process, as their values affect the ADS’s final decisions. Therefore, we propose parameter coverage, a criterion requiring to cover the ADS’s parameters. A scenario covers a parameter if changing its value leads to different simulation results, meaning it is relevant for the driving decisions made in the scenario. Since ADS simulators are slightly uncertain, we employ statistical methods to assess multiple simulation runs for execution difference and coverage. Experiments using the Autonomoose ADS show that the criterion discriminates between different scenarios and that the cost of computing coverage can be managed with suitable heuristics.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {58},
numpages = {31},
keywords = {mutation analysis, coverage criteria, autonomous driving, Software testing}
}

@article{10.1145/3643771,
author = {Sun, Gengyi and Habchi, Sarra and McIntosh, Shane},
title = {RavenBuild: Context, Relevance, and Dependency Aware Build Outcome Prediction},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643771},
doi = {10.1145/3643771},
abstract = {Continuous Integration (CI) is a common practice adopted by modern software organizations. It plays an especially important role for large corporations like Ubisoft, where thousands of build jobs are submitted daily. Indeed, the cadence of development progress is constrained by the pace at which CI services process build jobs. To provide faster CI feedback, recent work explores how build outcomes can be anticipated. Although early results show plenty of promise, the distinct characteristics of Project X—a AAA video game project at Ubisoft, present new challenges for build outcome prediction. In the Project X setting, changes that do not modify source code also incur build failures. Moreover, we find that the code changes that have an impact that crosses the source-data boundary are more prone to build failures than code changes that do not impact data files. Since such changes are not fully characterized by the existing set of build outcome prediction features, state-of-the art models tend to underperform. 
 
Therefore, to accommodate the data context into build outcome prediction, we propose RavenBuild, a novel approach that leverages context, relevance, and dependency-aware features. We apply the state of-the-art BuildFast model and RavenBuild to Project X, and observe that RavenBuild improves the F1 score of the failing class by 50%, the recall of the failing class by 105%, and AUC by 11%. To ease adoption in settings with heterogeneous project sets, we also provide a simplified alternative RavenBuild-CR, which excludes dependency-aware features. We apply RavenBuild-CR on 22 open-source projects and Project X, and observe across-the-board improvements as well. On the other hand, we find that a na\"{\i}ve Parrot approach, which simply echoes the previous build outcome as its prediction, is surprisingly competitive with BuildFast and RavenBuild. Though Parrot fails to predict when the build outcome differs from their immediate predecessor, Parrot serves well as a tendency indicator of the sequences in build outcome datasets. Therefore, future studies should also consider comparing to the Parrot approach as a baseline when evaluating build outcome prediction models.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {45},
numpages = {23},
keywords = {build outcome prediction, continuous integration, maintenance cost, mining software repositories}
}

@inproceedings{10.1145/3540250.3549137,
author = {Li, Yi and Wang, Shaohua and Nguyen, Tien N.},
title = {Fault localization to detect co-change fixing locations},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549137},
doi = {10.1145/3540250.3549137},
abstract = {Fault Localization (FL) is a precursor step to most Automated Program Repair (APR) approaches, which fix the faulty statements identified by the FL tools. We present FixLocator, a Deep Learning (DL)-based fault localization approach supporting the detection of faulty statements in one or multiple methods that need to be modified accordingly in the same fix. Let us call them co-change (CC) fixing locations for a fault. We treat this FL problem as dual-task learning with two models. The method-level FL model, MethFL, learns the methods to be fixed together. The statement-level FL model, StmtFL, learns the statements to be co-fixed. Correct learning in one model can benefit the other and vice versa. Thus, we simultaneously train them with soft-sharing the models' parameters via cross-stitch units to enable the propagation of the impact of MethFL and StmtFL onto each other. Moreover, we explore a novel feature for FL: the co-changed statements. We also use Graph-based Convolution Network to integrate different types of program dependencies.  

Our empirical results show that FixLocator relatively improves over the state-of-the-art statement-level FL baselines by locating 26.5%–155.6% more CC fixing statements. To evaluate its usefulness in APR, we used FixLocator in combination with the state-of-the-art APR tools. The results show that FixLocator+DEAR (the original FL in DEAR replaced by FixLocator) and FixLocator+CURE improve relatively over the original DEAR and Ochiai+CURE by 10.5% and 42.9% in terms of the number of fixed bugs.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {659–671},
numpages = {13},
keywords = {Fault Localization, Deep Learning, Co-Change Fixing Locations},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@article{10.1145/3720428,
author = {Ramesh, Arjun and Huang, Tianshu and Riar, Jaspreet and Titzer, Ben L. and Rowe, Anthony},
title = {Unveiling Heisenbugs with Diversified Execution},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3720428},
doi = {10.1145/3720428},
abstract = {Heisenbugs, notorious for their ability to change behavior and elude reproducibility under observation, are among the toughest challenges in debugging programs. They often evade static detection tools, making them especially prevalent in cyber-physical edge systems characterized by complex dynamics and unpredictable interactions with physical environments. Although dynamic detection tools work much better, most still struggle to meet low enough jitter and overhead performance requirements, impeding their adoption. More importantly however, dynamic tools currently lack metrics to determine an observed bug's "difficulty" or "heisen-ness" undermining their ability to make any claims regarding their effectiveness against heisenbugs.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
This paper proposes a methodology for detecting and identifying heisenbugs with low overheads at scale, actualized through the lens of dynamic data-race detection. In particular, we establish the critical impact of execution diversity across both instrumentation density and hardware platforms for detecting heisenbugs; the benefits of which outweigh any reduction in efficiency from limited instrumentation or weaker devices. We develop an experimental WebAssembly-backed dynamic data-race detection framework, Beanstalk, which exploits this diversity to show superior bug detection capability compared to any homogeneous instrumentation strategy on a fixed compute budget. Beanstalk's approach also gains power with scale, making it suitable for low-overhead deployments across numerous compute nodes. Finally, based on a rigorous statistical treatment of bugs observed by Beanstalk, we propose a novel metric, the heisen factor, that similar detectors can utilize to categorize heisenbugs and measure effectiveness. We reflect on our analysis of Beanstalk to provide insight on effective debugging strategies for both in-house and in deployment settings.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {94},
numpages = {28},
keywords = {WebAssembly, cyber-physical systems, data-races, heisen factor, heisenbugs, statistical bug analysis}
}

@inproceedings{10.1145/3540250.3558910,
author = {Eberlein, Martin},
title = {Explaining and debugging pathological program behavior},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558910},
doi = {10.1145/3540250.3558910},
abstract = {Programs fail. But which part of the input is responsible for the failure? To resolve the issue, developers must first understand how and why the program behaves as it does, notably when it deviates from the expected outcome. A program’s behavior is essentially the set of all its executions. This set is usually diverse, unpredictable, and generally unbounded. A pathological program behavior occurs once the actual outcome does not match the expected behavior. Consequently, developers must fix these issues to ensure the built system is the desired software. In our upcoming research, we want to focus on providing developers with a detailed description of the root causes that resulted in the program’s unwanted behavior. Thus, we aim to automatically produce explanations that capture the circumstances of arbitrary program behavior by correlating individual input elements (features) and their corresponding execution outcome. To this end, we use the scientific method and combine generative and predictive models, allowing us (i) to learn the statistical relations between the features of the inputs and the program behavior and (ii) to generate new inputs to refine or refute our current explanatory prediction model.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1795–1799},
numpages = {5},
keywords = {testing, program behavior, debugging, behavior explanation},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@article{10.1145/3635714,
author = {Pan, Weifeng and Kessentini, Marouane and Ming, Hua and Yang, Zijiang},
title = {EASE: An Effort-aware Extension of Unsupervised Key Class Identification Approaches},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3635714},
doi = {10.1145/3635714},
abstract = {Key class identification approaches aim at identifying the most important classes to help developers, especially newcomers, start the software comprehension process. So far, many supervised and unsupervised approaches have been proposed; however, they have not considered the effort to comprehend classes. In this article, we identify the challenge of “effort-aware key class identification”; to partially tackle it, we propose an approach, EASE, which is implemented through a modification to existing unsupervised key class identification approaches to take into consideration the effort to comprehend classes. First, EASE chooses a set of network metrics that has a wide range of applications in the existing unsupervised approaches and also possesses good discriminatory power. Second, EASE normalizes the network metric values of classes to quantify the probability of any class to be a key class and utilizes Cognitive Complexity to estimate the effort required to comprehend classes. Third, EASE proposes a metric, RKCP, to measure the relative key-class proneness of classes and further uses it to sort classes in descending order. Finally, an effort threshold is utilized, and the top-ranked classes within the threshold are identified as the cost-effective key classes. Empirical results on a set of 18 software systems show that (i) the proposed effort-aware variants perform significantly better in almost all (≈98.33%) the cases, (ii) they are superior to most of the baseline approaches with only several exceptions, and (iii) they are scalable to large-scale software systems. Based on these findings, we suggest that (i) we should resort to effort-aware key class identification techniques in budget-limited scenarios; and (ii) when using different techniques, we should carefully choose the weighting mechanism to obtain the best performance.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {84},
numpages = {43},
keywords = {Key classes, network metrics, complex networks, static analysis, program comprehension}
}

@inproceedings{10.1145/3691620.3695257,
author = {Schaef, Martin and Cirisci, Berk and Luo, Linghui and Mansur, Muhammad Numair and Tripp, Omer and Sanchez, Daniel and Zhou, Qiang and Zafar, Muhammad Bilal},
title = {Understanding Developer-Analyzer Interactions in Code Reviews},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695257},
doi = {10.1145/3691620.3695257},
abstract = {Static code analyzers are now a common part of the codereview process. These automated tools integrate into the code review process by commenting on code changes and suggesting improvements, in the same way as human reviewers. The comments made by static analyzers often trigger a conversation between developers to align on if and how the issue should be fixed. Because developers rarely give feedback directly to the tool, understanding the sentiment and intent in the conversation triggered by the tool comments can be used to measure the usefulness of the static analyzer.In this paper, we report on an experiment where we use large language models to automatically label and categorize the sentiment and intent of such conversations triggered by static analyzer comments. Our experiment demonstrates that LLMs not only classify and interpret complex developer-analyzer conversations, but can be more accurate than human experts.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1945–1955},
numpages = {11},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3663529.3663815,
author = {Wu, Yonghao and Li, Zheng and Zhang, Jie M. and Liu, Yong},
title = {ConDefects: A Complementary Dataset to Address the Data Leakage Concern for LLM-Based Fault Localization and Program Repair},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663815},
doi = {10.1145/3663529.3663815},
abstract = {With the growing interest on Large Language Models (LLMs) for fault localization and program repair, ensuring the integrity and generalizability of the LLM-based methods becomes paramount. The code in existing widely-adopted benchmarks for these tasks was written before the bloom of LLMs and may be included in the training data of existing popular LLMs, thereby suffering from the threat of data leakage, leading to misleadingly optimistic performance metrics.  To address this issue, we introduce ConDefects, a dataset developed as a complement to existing datasets, meticulously curated with real faults to eliminate such overlap. ConDefects contains 1,254 Java faulty programs and 1,625 Python faulty programs. All these programs are sourced from the online competition platform AtCoder and were produced between October 2021 and September 2023. We pair each fault with fault locations and the corresponding repaired code versions, making it tailored for fault localization and program repair related research. We also provide interfaces for selecting subsets based on different time windows and coding task difficulties. While inspired by LLM-based tasks, ConDefects can be adopted for benchmarking ALL types of fault localization and program repair methods. The dataset is publicly available, and a demo video can be found at https://www.youtube.com/watch?v=22j15Hj5ONk.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {642–646},
numpages = {5},
keywords = {Dataset, Fault Localization, Large Language Model, Program Repair},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/3701625.3701642,
author = {de Oliveira, Icaro Santos and Alves, Jo\~{a}o Matheus and Alc\^{a}ntara, Samuel and Santos, Ismayle Sousa and Andrade, Rossana Maria de Castro},
title = {Quality of Big Data Systems: a Systematic Review of Practices Methods and Tools},
year = {2024},
isbn = {9798400717772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701625.3701642},
doi = {10.1145/3701625.3701642},
abstract = {As the complexity and scale of big data applications increase, traditional testing methodologies often fall short in ensuring system reliability and performance. This paper provides a literature review focused on testing and quality assurance in big data systems. The objective of this study is to identify the key challenges encountered in ensuring the quality of big data systems, as well as the practices, methods, and tools available for this purpose. Through an analysis of academic sources, we identify research gaps and solutions aiming to improve testing processes and enhance the quality of big data systems. Our findings offer valuable insights for researchers and practitioners aiming to develop testing strategies for big data environments, thereby contributing to the advancement of software engineering within this domain.},
booktitle = {Proceedings of the XXIII Brazilian Symposium on Software Quality},
pages = {22–31},
numpages = {10},
keywords = {Big Data, Software Testing, Tests, Systematic review, Software Quality, Literature review},
location = {
},
series = {SBQS '24}
}

@inproceedings{10.1109/ICSE43902.2021.00041,
author = {Mastropaolo, Antonio and Scalabrino, Simone and Cooper, Nathan and Palacio, David Nader and Poshyvanyk, Denys and Oliveto, Rocco and Bavota, Gabriele},
title = {Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00041},
doi = {10.1109/ICSE43902.2021.00041},
abstract = {Deep learning (DL) techniques are gaining more and more attention in the software engineering community. They have been used to support several code-related tasks, such as automatic bug fixing and code comments generation. Recent studies in the Natural Language Processing (NLP) field have shown that the Text-To-Text Transfer Transformer (T5) architecture can achieve state-of-the-art performance for a variety of NLP tasks. The basic idea behind T5 is to first pre-train a model on a large and generic dataset using a self-supervised task (e.g., filling masked words in sentences). Once the model is pre-trained, it is fine-tuned on smaller and specialized datasets, each one related to a specific task (e.g., language translation, sentence classification). In this paper, we empirically investigate how the T5 model performs when pre-trained and fine-tuned to support code-related tasks. We pre-train a T5 model on a dataset composed of natural language English text and source code. Then, we fine-tune such a model by reusing datasets used in four previous works that used DL techniques to: (i) fix bugs, (ii) inject code mutants, (iii) generate assert statements, and (iv) generate code comments. We compared the performance of this single model with the results reported in the four original papers proposing DL-based solutions for those four tasks. We show that our T5 model, exploiting additional data for the self-supervised pre-training phase, can achieve performance improvements over the four baselines.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {336–347},
numpages = {12},
keywords = {Empirical software engineering, Deep Learning},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1145/3591109,
author = {Michiels, Lien and Verachtert, Robin and Ferraro, Andres and Falk, Kim and Goethals, Bart},
title = {A Framework and Toolkit for Testing the Correctness of Recommendation Algorithms},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3591109},
doi = {10.1145/3591109},
abstract = {Evaluating recommender systems adequately and thoroughly is an important task. Significant efforts are dedicated to proposing metrics, methods, and protocols for doing so. However, there has been little discussion in the recommender systems’ literature on the topic of testing. In this work, we adopt and adapt concepts from the software testing domain, e.g., code coverage, metamorphic testing, or property-based testing, to help researchers to detect and correct faults in recommendation algorithms. We propose a test suite that can be used to validate the correctness of a recommendation algorithm, and thus identify and correct issues that can affect the performance and behavior of these algorithms. Our test suite contains both black box and white box tests at every level of abstraction, i.e., system, integration, and unit. To facilitate adoption, we release RecPack Tests, an open-source Python package containing template test implementations. We use it to test four popular Python packages for recommender systems: RecPack, PyLensKit, Surprise, and Cornac. Despite the high test coverage of each of these packages, we find that we are still able to uncover undocumented functional requirements and even some bugs. This validates our thesis that testing the correctness of recommendation algorithms can complement traditional methods for evaluating recommendation algorithms.},
journal = {ACM Trans. Recomm. Syst.},
month = mar,
articleno = {4},
numpages = {45},
keywords = {Recommender systems evaluation, automated testing, correctness, toolkit, open-source}
}

@article{10.1145/3655022,
author = {Perera, Anjana and Turhan, Burak and Aleti, Aldeida and B\"{o}hme, Marcel},
title = {On the Impact of Lower Recall and Precision in Defect Prediction for Guiding Search-based Software Testing},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3655022},
doi = {10.1145/3655022},
abstract = {Defect predictors, static bug detectors, and humans inspecting the code can propose locations in the program that are more likely to be buggy before they are discovered through testing. Automated test generators such as search-based software testing (SBST) techniques can use this information to direct their search for test cases to likely buggy code, thus speeding up the process of detecting existing bugs in those locations. Often the predictions given by these tools or humans are imprecise, which can misguide the SBST technique and may deteriorate its performance. In this article, we study the impact of imprecision in defect prediction on the bug detection effectiveness of SBST.Our study finds that the recall of the defect predictor, i.e., the proportion of correctly identified buggy code, has a significant impact on bug detection effectiveness of SBST with a large effect size. More precisely, the SBST technique detects 7.5 fewer bugs on average (out of 420 bugs) for every 5% decrements of the recall. However, the effect of precision, a measure for false alarms, is not of meaningful practical significance, as indicated by a very small effect size.In the context of combining defect prediction and SBST, our recommendation is to increase the recall of defect predictors as a primary objective and precision as a secondary objective. In our experiments, we find that 75% precision is as good as 100% precision. To account for the imprecision of defect predictors, in particular low recall values, SBST techniques should be designed to search for test cases that also cover the predicted non-buggy parts of the program, while prioritising the parts that have been predicted as buggy.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {144},
numpages = {27},
keywords = {Search-based software testing, automated test generation, defect prediction}
}

@inproceedings{10.1145/1062455.1062490,
author = {Michail, Amir and Xie, Tao},
title = {Helping users avoid bugs in GUI applications},
year = {2005},
isbn = {1581139632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062455.1062490},
doi = {10.1145/1062455.1062490},
abstract = {In this paper, we propose a method to help users avoid bugs in GUI applications. In particular, users would use the application normally and report bugs that they encounter to prevent anyone -- including themselves -- from encountering those bugs again. When a user attempts an action that has led to problems in the past, he/she will receive a warning and will be given the opportunity to abort the action -- thus avoiding the bug altogether and keeping the application stable. Of course, bugs should be fixed eventually by the application developers, but our approach allows application users to collaboratively help each other avoid bugs -- thus making the application more usable in the meantime. We demonstrate this approach using our "Stabilizer" prototype. We also include a preliminary evaluation of the Stabilizer's bug prediction.},
booktitle = {Proceedings of the 27th International Conference on Software Engineering},
pages = {107–116},
numpages = {10},
keywords = {software tools, software testing, gui applications, fault evasion, bug tracking system, bug prediction},
location = {St. Louis, MO, USA},
series = {ICSE '05}
}

@inproceedings{10.1145/3106237.3106257,
author = {Fu, Wei and Menzies, Tim},
title = {Revisiting unsupervised learning for defect prediction},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106257},
doi = {10.1145/3106237.3106257},
abstract = {Collecting quality data from software projects can be time-consuming and expensive. Hence, some researchers explore "unsupervised" approaches to quality prediction that does not require labelled data. An alternate technique is to use "supervised" approaches that learn models from project data labelled with, say, "defective" or "not-defective". Most researchers use these supervised models since, it is argued, they can exploit more knowledge of the projects. At FSE-16, Yang et al. reported startling results where unsupervised defect predictors outperformed supervised predictors for effort-aware just-in-time defect prediction. If confirmed, these results would lead to a dramatic simplification of a seemingly complex task (data mining) that is widely explored in the software engineering literature. This paper repeats and refutes those results as follows. (1) There is much variability in the efficacy of the Yang et al. predictors so even with their approach, some supervised data is required to prune weaker predictors away. (2) Their findings were grouped across N projects. When we repeat their analysis on a project-by-project basis, supervised predictors are seen to work better. Even though this paper rejects the specific conclusions of Yang et al., we still endorse their general goal. In our our experiments, supervised predictors did not perform outstandingly better than unsupervised ones for effort-aware just-in-time defect prediction. Hence, they may indeed be some combination of unsupervised learners to achieve comparable performance to supervised ones. We therefore encourage others to work in this promising area.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {72–83},
numpages = {12},
keywords = {software repository mining, empirical studies, defect prediction, data analytics for software engineering},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/3691620.3695539,
author = {Zhao, Yu and Gong, Lina and Huang, Zhiqiu and Wang, Yongwei and Wei, Mingqiang and Wu, Fei},
title = {Coding-PTMs: How to Find Optimal Code Pre-trained Models for Code Embedding in Vulnerability Detection?},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695539},
doi = {10.1145/3691620.3695539},
abstract = {Vulnerability detection is garnering increasing attention in software engineering, since code vulnerabilities possibly pose significant security. Recently, reusing various code pre-trained models (e.g., CodeBERT, CodeT5, and CodeGen) has become common for code embedding without providing reasonable justifications in vulnerability detection. The premise for casually utilizing pre-trained models (PTMs) is that the code embeddings generated by different PTMs would generate a similar impact on the performance. Is that TRUE? To answer this important question, we systematically investigate the effects of code embedding generated by ten different code PTMs on the performance of vulnerability detection, and get the answer, i.e., that is NOT true. We observe that code embedding generated by various code PTMs can indeed influence the performance and selecting an embedding technique based on parameter scales and embedding dimension is not reliable. Our findings highlight the necessity of quantifying and evaluating the characteristics of code embedding generated by various code PTMs to understand the effects. To achieve this goal, we analyze the numerical representation and data distribution of code embedding generated by different PTMs to evaluate differences and characteristics. Based on these insights, we propose Coding-PTMs, a recommendation framework to assist engineers in selecting optimal code PTMs for their specific vulnerability detection tasks. Specifically, we define thirteen code embedding metrics across three dimensions (i.e., statistics, norm, and distribution) for constructing a specialized code PTM recommendation dataset. We then employ a Random Forest classifier to train a recommendation model and identify the optimal code PTMs from the candidate model zoo. We encourage engineers to use our Coding-PTMs to evaluate the characteristics of code embeddings generated by candidate code PTMs on the performance and recommend optimal code PTMs for code embedding in their vulnerability detection tasks.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1732–1744},
numpages = {13},
keywords = {coding-PTMs, code embedding, pre-trained models, vulnerability detection, embedding metrics, recommendation framework},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3510003.3510071,
author = {Wardat, Mohammad and Cruz, Breno Dantas and Le, Wei and Rajan, Hridesh},
title = {DeepDiagnosis: automatically diagnosing faults and recommending actionable fixes in deep learning programs},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510071},
doi = {10.1145/3510003.3510071},
abstract = {Deep Neural Networks (DNNs) are used in a wide variety of applications. However, as in any software application, DNN-based apps are afflicted with bugs. Previous work observed that DNN bug fix patterns are different from traditional bug fix patterns. Furthermore, those buggy models are non-trivial to diagnose and fix due to inexplicit errors with several options to fix them. To support developers in locating and fixing bugs, we propose DeepDiagnosis, a novel debugging approach that localizes the faults, reports error symptoms and suggests fixes for DNN programs. In the first phase, our technique monitors a training model, periodically checking for eight types of error conditions. Then, in case of problems, it reports messages containing sufficient information to perform actionable repairs to the model. In the evaluation, we thoroughly examine 444 models - 53 real-world from GitHub and Stack Overflow, and 391 curated by AUTOTRAINER. DeepDiagnosis provides superior accuracy when compared to UMLUAT and DeepLocalize. Our technique is faster than AUTOTRAINER for fault localization. The results show that our approach can support additional types of models, while state-of-the-art was only able to handle classification ones. Our technique was able to report bugs that do not manifest as numerical errors during training. Also, it can provide actionable insights for fix whereas DeepLocalize can only report faults that lead to numerical errors during training. DeepDiagnosis manifests the best capabilities of fault detection, bug localization, and symptoms identification when compared to other approaches.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {561–572},
numpages = {12},
keywords = {debugging, deep learning bugs, deep neural networks, fault location, program analysis},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3242887.3242889,
author = {Zaman, Tarannum Shaila and Yu, Tingting},
title = {Extracting implicit programming rules: comparing static and dynamic approaches},
year = {2018},
isbn = {9781450359757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242887.3242889},
doi = {10.1145/3242887.3242889},
abstract = {Programs often follow implicit programming rules, such as, function call A must be followed by function call B. Rules of such kinds are rarely documented by developers. Nevertheless, programming rules play an important role in software testing and maintenance. For example, the rules can be used as test oracles to detect violations. If a programmer can be notified of these rules before updating the source code, the chances of generating defects due to rule violations might be minimized. Prior works have used static and dynamic analysis techniques to extract implicit programming rules, but none compares the effectiveness of the two techniques. In this paper, we have undertaken an empirical study to compare the two techniques when they are being used for extracting programming rules. Our results indicate that the performance of the dynamic analysis technique depends on the number and the diversity of the traces. Moreover, the dynamic analysis technique generates more precise rules than the static analysis technique if a diverse and sufficient number of test cases are provided.},
booktitle = {Proceedings of the 7th International Workshop on Software Mining},
pages = {1–7},
numpages = {7},
keywords = {Static Analysis, Implicit Programming Rules, Empirical study, Dynamic analysis},
location = {Montpellier, France},
series = {SoftwareMining 2018}
}

@inproceedings{10.1145/2897695.2897696,
author = {Ortu, Marco and Destefanis, Giuseppe and Swift, Stephen and Marchesi, Michele},
title = {Measuring high and low priority defects on traditional and mobile open source software},
year = {2016},
isbn = {9781450341776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897695.2897696},
doi = {10.1145/2897695.2897696},
abstract = {Software defects are the major cause for system failures. To effectively design tools and provide support for detecting and recovering from software failures, requires a deep understanding of defect features. In this paper we present an analysis of defect characteristics in two different open source software development domains: Mobile and Traditional. Our attention is focused on measuring the differences between High-Priority and Low-Priority defects. High or Low priority of a given defect is decided by a developer when creating a bug report for an issue tracking system. We sampled hundreds of real world bugs in hundreds of large and representative open-source projects. We used natural language text classification techniques to automatically analyse roughly 700,000 bug reports from the Bugzilla, Jira and Google Issues issue tracking systems. Results show that there are differences between High-Priority and Low-Priority defects classification in Mobile and Traditional development domains.},
booktitle = {Proceedings of the 7th International Workshop on Emerging Trends in Software Metrics},
pages = {1–7},
numpages = {7},
keywords = {data mining, bug reports, bug categorisation},
location = {Austin, Texas},
series = {WETSoM '16}
}

@inproceedings{10.1145/3092703.3092709,
author = {Spieker, Helge and Gotlieb, Arnaud and Marijan, Dusica and Mossige, Morten},
title = {Reinforcement learning for automatic test case prioritization and selection in continuous integration},
year = {2017},
isbn = {9781450350761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092703.3092709},
doi = {10.1145/3092703.3092709},
abstract = {Testing in Continuous Integration (CI) involves test case prioritization, selection, and execution at each cycle. Selecting the most promising test cases to detect bugs is hard if there are uncertainties on the impact of committed code changes or, if traceability links between code and tests are not available. This paper introduces Retecs, a new method for automatically learning test case selection and prioritization in CI with the goal to minimize the round-trip time between code commits and developer feedback on failed test cases. The Retecs method uses reinforcement learning to select and prioritize test cases according to their duration, previous last execution and failure history. In a constantly changing environment, where new test cases are created and obsolete test cases are deleted, the Retecs method learns to prioritize error-prone test cases higher under guidance of a reward function and by observing previous CI cycles. By applying Retecs on data extracted from three industrial case studies, we show for the first time that reinforcement learning enables fruitful automatic adaptive test case selection and prioritization in CI and regression testing.},
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {12–22},
numpages = {11},
keywords = {Test case selection, Test case prioritization, Reinforcement Learning, Regression testing, Machine Learning, Continuous Integration},
location = {Santa Barbara, CA, USA},
series = {ISSTA 2017}
}

@inproceedings{10.1145/3236024.3275428,
author = {Zhou, Cheng},
title = {Intelligent bug fixing with software bug knowledge graph},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3275428},
doi = {10.1145/3236024.3275428},
abstract = {Software bugs continuously emerge during the process of software evolution. With the increasing size and complexity of software, bug fixing becomes increasingly more difficult. Bug and commit data of open source projects, Q&amp;A documents and other software resources contain a sea of bug knowledge which can be utilized to help developers understand and fix bugs. Existing work focuses on data mining from a certain software resource in isolation to assist in bug fixing, which may reduce the efficiency of bug fixing. How to obtain, organize and understand bug knowledge from multi-source software data is an urgent problem to be solved. In order to solve this problem, we utilize knowledge graph (KG) technology to explore the deep semantic and structural relationships in the multi-source software data, propose effective search and recommendation techniques based on the knowledge graph, and design a bug-fix knowledge question &amp; answering system to assist developers in intelligent software bug fixing. At present, we have designed a bug knowledge graph construction framework, proposed the identification principles and methods for bug knowledge entities and relationships, constructed a preliminary knowledge graph based on the bug repository. In the following work, we will further improve the knowledge graph, complete the knowledge graph fusion of multi-source database, comprehend bug knowledge through knowledge reasoning, utilize the collaborative search and recommendation technology for bug-fixing knowledge question and answering.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {944–947},
numpages = {4},
keywords = {software bug knowledge graph, collaborative search and recommendation, bug-fixing knowledge question and answering, Intelligent bug fixing},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3597503.3608137,
author = {Feng, Sidong and Chen, Chunyang},
title = {Prompting Is All You Need: Automated Android Bug Replay with Large Language Models},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3608137},
doi = {10.1145/3597503.3608137},
abstract = {Bug reports are vital for software maintenance that allow users to inform developers of the problems encountered while using the software. As such, researchers have committed considerable resources toward automating bug replay to expedite the process of software maintenance. Nonetheless, the success of current automated approaches is largely dictated by the characteristics and quality of bug reports, as they are constrained by the limitations of manually-crafted patterns and pre-defined vocabulary lists. Inspired by the success of Large Language Models (LLMs) in natural language understanding, we propose AdbGPT, a new lightweight approach to automatically reproduce the bugs from bug reports through prompt engineering, without any training and hard-coding effort. AdbGPT leverages few-shot learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning from LLMs to accomplish the bug replay in a manner similar to a developer. Our evaluations demonstrate the effectiveness and efficiency of our AdbGPT to reproduce 81.3% of bug reports in 253.6 seconds, outperforming the state-of-the-art baselines and ablation studies. We also conduct a small-scale user study to confirm the usefulness of AdbGPT in enhancing developers' bug replay capabilities.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {67},
numpages = {13},
keywords = {automated bug replay, large language model, prompt engineering},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3679006.3685071,
author = {Spieker, Helge and Belmecheri, Nassim and Gotlieb, Arnaud and Lazaar, Nadjib},
title = {Evaluating Human Trajectory Prediction with Metamorphic Testing},
year = {2024},
isbn = {9798400711176},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3679006.3685071},
doi = {10.1145/3679006.3685071},
abstract = {The prediction of human trajectories is important for planning in autonomous systems that act in the real world, e.g. automated driving or mobile robots. Human trajectory prediction is a noisy process, and no prediction does precisely match any future trajectory. It is therefore approached as a stochastic problem, where the goal is to minimise the error between the true and the predicted trajectory. In this work, we explore the application of metamorphic testing for human trajectory prediction. Metamorphic testing is designed to handle unclear or missing test oracles. It is well-designed for human trajectory prediction, where there is no clear criterion of correct or incorrect human behaviour. Metamorphic relations rely on transformations over source test cases and exploit invariants. A setting well-designed for human trajectory prediction where there are many symmetries of expected human behaviour under variations of the input, e.g. mirroring and rescaling of the input data. We discuss how metamorphic testing can be applied to stochastic human trajectory prediction and introduce the Wasserstein Violation Criterion to statistically assess whether a follow-up test case violates a label-preserving metamorphic relation.},
booktitle = {Proceedings of the 9th ACM International Workshop on Metamorphic Testing},
pages = {34–40},
numpages = {7},
keywords = {human trajectory prediction, metamorphic testing, software testing},
location = {Vienna, Austria},
series = {MET 2024}
}

@inproceedings{10.1145/3589250.3596148,
author = {Liblit, Ben and Lyu, Yingjun and Mukherjee, Rajdeep and Tripp, Omer and Wang, Yanjun},
title = {User-Assisted Code Query Optimization},
year = {2023},
isbn = {9798400701702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589250.3596148},
doi = {10.1145/3589250.3596148},
abstract = {Running static analysis rules in the wild, as part of a commercial service, demands special consideration of time limits and scalability given the large and diverse real-world workloads that the rules are evaluated on. Furthermore, these rules do not run in isolation, which exposes opportunities for reuse of partial evaluation results across rules. In our work on Amazon CodeGuru Reviewer, and its underlying rule-authoring toolkit known as the Guru Query Language (GQL), we have encountered performance and scalability challenges, and identified corresponding optimization opportunities such as, caching, indexing, and customization of analysis scope, which rule authors can take advantage of as built-in GQL constructs. Our experimental evaluation on a dataset of open-source GitHub repositories shows 3X speedup and perfect recall using indexing-based configurations, and 2X speedup and 51% increase on the number of findings for caching-based optimization.},
booktitle = {Proceedings of the 12th ACM SIGPLAN International Workshop on the State Of the Art in Program Analysis},
pages = {40–46},
numpages = {7},
keywords = {static analysis, performance optimization, caching, Guru Query Language (GQL), GitHub, AWS},
location = {Orlando, FL, USA},
series = {SOAP 2023}
}

@inproceedings{10.1145/3697090.3699868,
author = {Banjar, Carlos Eduardo de Schuller and Bicudo, Miguel Angelo Santos and Miranda, Lucas and Pereira, Cain\~{a} Figueiredo and Coutinho, Lucas Senos and Menasche, Daniel Sadoc and Srivastava, Gaurav Kumar and Lovat, Enrico and Kocheturov, Anton and Martins, Matheus and de Aguiar, Leandro Pfleger},
title = {Automated Severity Driven Patch Management},
year = {2024},
isbn = {9798400717406},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3697090.3699868},
doi = {10.1145/3697090.3699868},
abstract = {We present a method for assessing the temporal severity associated with software vulnerabilities by analyzing reported vulnerability data. Data from various platforms is collected and curated to define specific vulnerability features and historical vulnerability event data. When a vulnerability is specified, the system identifies its vulnerability class using a classifier based on the predefined features. Historical event data is then processed to generate a predictive severity curve, which estimates the evolution of a temporal severity score, parameterized by the occurrence of key vulnerability events. This curve predicts the time of weaponization and/or exploitation events, along with the corresponding severity score for the specified vulnerability. Our approach aims to support automated decision-making in software patch management by enabling accurate tracking and prediction of vulnerability severity over time.},
booktitle = {Proceedings of the 13th Latin-American Symposium on Dependable and Secure Computing},
pages = {179–183},
numpages = {5},
keywords = {Severity assessment, vulnerabilities, exploits, CVSS},
location = {
},
series = {LADC '24}
}

@inproceedings{10.1145/3650212.3680353,
author = {Chen, Jiachi and Chen, Chong and Hu, Jiang and Grundy, John and Wang, Yanlin and Chen, Ting and Zheng, Zibin},
title = {Identifying Smart Contract Security Issues in Code Snippets from Stack Overflow},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680353},
doi = {10.1145/3650212.3680353},
abstract = {Smart contract developers frequently seek solutions to developmental challenges on Q&amp;A platforms such as Stack Overflow (SO). Although community responses often provide viable solutions, the embedded code snippets can also contain hidden vulnerabilities. Integrating such code directly into smart contracts may make them susceptible to malicious attacks. We conducted an online survey and received 74 responses from smart contract developers. The results of this survey indicate that the majority (86.4%) of participants do not sufficiently consider security when reusing SO code snippets. Despite the existence of various tools designed to detect vulnerabilities in smart contracts, these tools are typically developed for analyzing fully-completed smart contracts and thus are ineffective for analyzing typical code snippets as found on SO. We introduce SOChecker, the first tool designed to identify potential vulnerabilities in incomplete SO smart contract code snippets. SOChecker first leverages a fine-tuned Llama2 model for code completion, followed by the application of symbolic execution methods for vulnerability detection. Our experimental results, derived from a dataset comprising 897 code snippets collected from smart contract-related SO posts, demonstrate that SOChecker achieves an F1 score of 68.2%, greatly surpassing GPT-3.5 and GPT-4 (20.9% and 33.2% F1 Scores respectively). Our findings underscore the need to improve the security of code snippets from Q&amp;A websites.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1198–1210},
numpages = {13},
keywords = {large language models, program analysis, smart contracts},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3661167.3661220,
author = {G\'{o}mez-Abajo, Pablo and P\'{e}rez-Soler, Sara and Ca\~{n}izares, Pablo C. and Guerra, Esther and de Lara, Juan},
title = {Mutation Testing for Task-Oriented Chatbots},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661220},
doi = {10.1145/3661167.3661220},
abstract = {Conversational agents, or chatbots, are increasingly used to access all sorts of services using natural language. While open-domain chatbots – like ChatGPT – can converse on any topic, task-oriented chatbots – the focus of this paper – are designed for specific tasks, like booking a flight, obtaining customer support, or setting an appointment. Like any other software, task-oriented chatbots need to be properly tested, usually by defining and executing test scenarios (i.e., sequences of user-chatbot interactions). However, there is currently a lack of methods to quantify the completeness and strength of such test scenarios, which can lead to low-quality tests, and hence to buggy chatbots. To fill this gap, we propose adapting mutation testing (MuT) for task-oriented chatbots. To this end, we introduce a set of mutation operators that emulate faults in chatbot designs, an architecture that enables MuT on chatbots built using heterogeneous technologies, and a practical realisation as an Eclipse plugin. Moreover, we evaluate the applicability, effectiveness and efficiency of our approach on open-source chatbots, with promising results.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {232–241},
numpages = {10},
keywords = {Botium, Dialogflow, Mutation testing, Rasa, Task-oriented chatbots},
location = {Salerno, Italy},
series = {EASE '24}
}

@article{10.1145/3483424,
author = {Notaro, Paolo and Cardoso, Jorge and Gerndt, Michael},
title = {A Survey of AIOps Methods for Failure Management},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3483424},
doi = {10.1145/3483424},
abstract = {Modern society is increasingly moving toward complex and distributed computing systems. The increase in scale and complexity of these systems challenges O&amp;M teams that perform daily monitoring and repair operations, in contrast with the increasing demand for reliability and scalability of modern applications. For this reason, the study of automated and intelligent monitoring systems has recently sparked much interest across applied IT industry and academia. Artificial Intelligence for IT Operations (AIOps) has been proposed to tackle modern IT administration challenges thanks to Machine Learning, AI, and Big Data. However, AIOps as a research topic is still largely unstructured and unexplored, due to missing conventions in categorizing contributions for their data requirements, target goals, and components. In this work, we focus on AIOps for Failure Management (FM), characterizing and describing 5 different categories and 14 subcategories of contributions, based on their time intervention window and the target problem being solved. We review 100 FM solutions, focusing on applicability requirements and the quantitative results achieved, to facilitate an effective application of AIOps solutions. Finally, we discuss current development problems in the areas covered by AIOps and delineate possible future trends for AI-based failure management.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {81},
numpages = {45},
keywords = {artificial intelligence, failure management, IT operations and maintenance, AIOps}
}

@inproceedings{10.1145/3522664.3528602,
author = {Friesel, Birte and Spinczyk, Olaf},
title = {Black-box models for non-functional properties of AI software systems},
year = {2022},
isbn = {9781450392754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3522664.3528602},
doi = {10.1145/3522664.3528602},
abstract = {Non-functional properties (NFPs) such as latency, memory requirements, or hardware cost are an important characteristic of AI software systems, especially in the domain of resource-constrained embedded devices. Embedded AI products require sufficient resources for satisfactory latency and accuracy, but should also be cost-efficient and therefore not use more powerful hardware than strictly necessary. Traditionally, modeling and optimization efforts focus on the AI architecture, utilizing methods such as neural architecture search (NAS). However, before developers can start optimizing, they need to know which architectures are suitable candidates for their use case. To this end, architectures must be viewed in context: model post-processing (e.g. quantization), hardware platform, and run-time configuration such as batching all have significant effects on NFPs and therefore on AI architecture performance. Moreover, scalar parameters such as batch size cannot be benchmarked exhaustively. We argue that it is worthwhile to address this issue by means of black-box models before deciding on AI architectures for optimization and hardware/software platforms for inference. To support our claim, we present an AI product line with variable hardware and software components, perform benchmarks, and present notable results. Additionally, we evaluate both compactness and generalization capabilities of regression tree-based modeling approaches from the machine learning and product line engineering communities. We find that linear model trees perform best: they can capture NFPs of known AI configurations with a mean error of up to 13 %, and can predict unseen configurations with a mean error of 10 to 26 %. We find linear model trees to be more compact and interpretable than other tree-based approaches.},
booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
pages = {170–180},
numpages = {11},
keywords = {AI, performance prediction, product lines, regression trees},
location = {Pittsburgh, Pennsylvania},
series = {CAIN '22}
}

@inproceedings{10.1145/3475960.3475985,
author = {Bhandari, Guru and Naseer, Amara and Moonen, Leon},
title = {CVEfixes: automated collection of vulnerabilities and their fixes from open-source software},
year = {2021},
isbn = {9781450386807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475960.3475985},
doi = {10.1145/3475960.3475985},
abstract = {Data-driven research on the automated discovery and repair of security vulnerabilities in source code requires comprehensive datasets of real-life vulnerable code and their fixes. To assist in such research, we propose a method to automatically collect and curate a comprehensive vulnerability dataset from Common Vulnerabilities and Exposures (CVE) records in the National Vulnerability Database (NVD). We implement our approach in a fully automated dataset collection tool and share an initial release of the resulting vulnerability dataset named CVEfixes. The CVEfixes collection tool automatically fetches all available CVE records from the NVD, gathers the vulnerable code and corresponding fixes from associated open-source repositories, and organizes the collected information in a relational database. Moreover, the dataset is enriched with meta-data such as programming language, and detailed code and security metrics at five levels of abstraction. The collection can easily be repeated to keep up-to-date with newly discovered or patched vulnerabilities. The initial release of CVEfixes spans all published CVEs up to 9 June 2021, covering 5365 CVE records for 1754 open-source projects that were addressed in a total of 5495 vulnerability fixing commits. CVEfixes supports various types of data-driven software security research, such as vulnerability prediction, vulnerability classification, vulnerability severity prediction, analysis of vulnerability-related code changes, and automated vulnerability repair.},
booktitle = {Proceedings of the 17th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {30–39},
numpages = {10},
keywords = {Security vulnerabilities, dataset, software repository mining, source code repair, vulnerability classification, vulnerability prediction},
location = {Athens, Greece},
series = {PROMISE 2021}
}

@inproceedings{10.1145/3379597.3387482,
author = {Pinto, Gustavo and Miranda, Breno and Dissanayake, Supun and d'Amorim, Marcelo and Treude, Christoph and Bertolino, Antonia},
title = {What is the Vocabulary of Flaky Tests?},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387482},
doi = {10.1145/3379597.3387482},
abstract = {Flaky tests are tests whose outcomes are non-deterministic. Despite the recent research activity on this topic, no effort has been made on understanding the vocabulary of flaky tests. This work proposes to automatically classify tests as flaky or not based on their vocabulary. Static classification of flaky tests is important, for example, to detect the introduction of flaky tests and to search for flaky tests after they are introduced in regression test suites.We evaluated performance of various machine learning algorithms to solve this problem. We constructed a data set of flaky and non-flaky tests by running every test case, in a set of 64k tests, 100 times (6.4 million test executions). We then used machine learning techniques on the resulting data set to predict which tests are flaky from their source code. Based on features, such as counting stemmed tokens extracted from source code identifiers, we achieved an F-measure of 0.95 for the identification of flaky tests. The best prediction performance was obtained when using Random Forest and Support Vector Machines. In terms of the code identifiers that are most strongly associated with test flakiness, we noted that job, action, and services are commonly associated with flaky tests. Overall, our results provides initial yet strong evidence that static detection of flaky tests is effective.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {492–502},
numpages = {11},
keywords = {Text classification, Test flakiness, Regression testing},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3650212.3680349,
author = {Huynh, Hieu and Pham, Nhu and Nguyen, Tien N. and Nguyen, Vu},
title = {Segment-Based Test Case Prioritization: A Multi-objective Approach},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680349},
doi = {10.1145/3650212.3680349},
abstract = {Regression testing of software is a crucial but time-consuming task, especially in the context of user interface (UI) testing where multiple microservices must be validated simultaneously. Test case prioritization (TCP) is a cost-efficient solution to address this by scheduling test cases in an execution order that maximizes an objective function, generally aimed at increasing the fault detection rate. While several techniques have been proposed for TCP, most rely on source code information which is usually not available for UI testing. In this paper, we introduce a multi-objective optimization approach to prioritize UI test cases, using evolutionary search algorithms and four coverage criteria focusing on web page elements as objectives for the optimization problem. Our method, which does not require source code information, is evaluated using two evolutionary algorithms (AGE-MOEA and NSGA-II) and compared with other TCP methods on a self-collected dataset of 11 test suites. The results show that our approach significantly outperforms other methods in terms of Average Percentage of Faults Detected (APFD) and APFD with Cost (APFDc), achieving the highest scores of 87.8% and 79.2%, respectively. We also introduce a new dataset and demonstrate the significant improvement of our approach over existing ones via empirical experiments. The paper’s contributions include the application of web page segmentation in TCP, the construction of a new dataset for UI TCP, and empirical comparisons that demonstrate the improvement of our approach.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1149–1160},
numpages = {12},
keywords = {Test case prioritization, multi-objective optimization},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@article{10.1145/3705310,
author = {Zhang, Yuntong and Costea, Andreea and Shariffdeen, Ridwan and McCall, Davin and Roychoudhury, Abhik},
title = {EffFix: Efficient and Effective Repair of Pointer Manipulating Programs},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3705310},
doi = {10.1145/3705310},
abstract = {This work introduces EffFix, a tool that applies a novel static analysis-driven automated program repair (APR) technique for fixing memory errors. APR tools typically rely on a given test-suite to guide the repair process. Apart from the need to provide test oracles, this reliance is also one of the main contributors to the over-fitting problem. Static analysis based APR techniques bypass these issues only to introduce new ones, such as soundness, scalability, and generalizability. This work demonstrates how we can overcome these challenges and achieve sound memory bug repair at scale by leveraging static analysis (specifically incorrectness separation logic (ISL)) to guide repair. This is the first repair approach to use ISL. Our key insight is that the abstract domain used by static analysis to detect the bugs also contains key information to derive correct patches. Our proposed approach learns what a desirable patch is by inspecting how close a patch is to fixing the bug based on the feedback from ISL based static analysis (specifically the Pulse analyzer), and turning this information into a distribution of probabilities over context free grammars. This approach to repair is generic in that its learning strategy allows for finding patches without relying on the commonly used patch templates. Furthermore, to achieve efficient program repair, instead of focusing on heuristics for reducing the search space of patches, we make repair scalable by creating classes of equivalent patches according to the effect they have on the symbolic heap. We then conduct candidate patch validation only once per patch equivalence class. This allows EffFix to efficiently discover quality repairs even in the presence of a large pool of patch candidates. Experimental evaluation of fixing real world memory errors in medium to large scale subjects like OpenSSL, Linux Kernel, swoole, shows the efficiency and effectiveness of EffFix— in terms of automatically producing repairs from large search spaces. In particular, EffFix has a fix ratio of 66% for memory leak bugs and 83% for Null Pointer Dereferences for the considered dataset.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {69},
numpages = {27},
keywords = {Automated Program Repair, Incorrectness Separation Logic, Probabilistic Context Free Grammars}
}

@article{10.1145/3533313,
author = {Wu, Jianwei and Clause, James},
title = {Automated Identification of Uniqueness in JUnit Tests},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3533313},
doi = {10.1145/3533313},
abstract = {In the context of testing, descriptive test names are desirable because they document the purpose of tests and facilitate comprehension tasks during maintenance. Unfortunately, prior work has shown that tests often do not have descriptive names. To address this limitation, techniques have been developed to automatically generate descriptive names. However, they often generated names that are invalid or do not meet developer approval. To help address these limitations, we present a novel approach to extract the attributes of a given test that make it unique among its siblings. Because such attributes often serve as the basis for descriptive names, identifying them is an important first step towards improving test name generation approaches. To evaluate the approach, we created a prototype implementation for JUnit tests and compared its output with human judgment. The results of the evaluation demonstrate that the attributes identified by the approach are consistent with human judgment and are likely to be useful for future name generation techniques.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {18},
numpages = {32},
keywords = {formal concept analysis, Unit testing}
}

@article{10.1145/3530786,
author = {Ojdanic, Milos and Soremekun, Ezekiel and Degiovanni, Renzo and Papadakis, Mike and Le Traon, Yves},
title = {Mutation Testing in Evolving Systems: Studying the Relevance of Mutants to Code Evolution},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3530786},
doi = {10.1145/3530786},
abstract = {Context:
When software evolves, opportunities for introducing faults appear. Therefore, it is important to test the evolved program behaviors during each evolution cycle. However, while software evolves, its complexity is also evolving, introducing challenges to the testing process. To deal with this issue, testing techniques should be adapted to target the effect of the program changes instead of the entire program functionality. To this end, commit-aware mutation testing, a powerful testing technique, has been proposed. Unfortunately, commit-aware mutation testing is challenging due to the complex program semantics involved. Hence, it is pertinent to understand the characteristics, predictability, and potential of the technique.Objective: We conduct an exploratory study to investigate the properties of commit-relevant mutants, i.e., the test elements of commit-aware mutation testing, by proposing a general definition and an experimental approach to identify them. We thus aim at investigating the prevalence, location, and comparative advantages of commit-aware mutation testing over time (i.e., the program evolution). We also investigate the predictive power of several commit-related features in identifying and selecting commit-relevant mutants to understand the essential properties for its best-effort application case.Method: Our commit-relevant definition relies on the notion of observational slicing, approximated by higher-order mutation. Specifically, our approach utilizes the impact of mutants, effects of one mutant on another in capturing and analyzing the implicit interactions between the changed and unchanged code parts. The study analyses millions of mutants (over 10 million), 288 commits, five (5) different open-source software projects involving over 68,213 CPU days of computation and sets a ground truth where we perform our analysis.Results: Our analysis shows that commit-relevant mutants are located mainly outside of program commit change (81%), suggesting a limitation in previous work. We also note that effective selection of commit-relevant mutants has the potential of reducing the number of mutants by up to 93%. In addition, we demonstrate that commit relevant mutation testing is significantly more effective and efficient than state-of-the-art baselines, i.e., random mutant selection and analysis of only mutants within the program change. In our analysis of the predictive power of mutants and commit-related features (e.g., number of mutants within a change, mutant type, and commit size) in predicting commit-relevant mutants, we found that most proxy features do not reliably predict commit-relevant mutants.Conclusion: This empirical study highlights the properties of commit-relevant mutants and demonstrates the importance of identifying and selecting commit-relevant mutants when testing evolving software systems.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {14},
numpages = {39},
keywords = {evolving-systems, continuous integration, mutation testing, Software testing}
}

@inproceedings{10.1145/3377049.3377076,
author = {Intisar, Arik and Islam, Md Khaled Ben and Rahman, Julia},
title = {A Deep Convolutional Neural Network Based Small Scale Test-bed for Autonomous Car},
year = {2020},
isbn = {9781450377782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377049.3377076},
doi = {10.1145/3377049.3377076},
abstract = {In deep end-to-end learning based autonomous car design, inferencing the signal by trained model is one of the critical issues, particularly, in case of embedded component. Researchers from both academia and industry have been putting their enormous efforts in making this critical autonomous driving more reliable and safer. As research on the real car is costly and poses safety issue, we have developed a small scale, low-cost, deep convolutional neural network powered self-driving car model. Its learning model adopted from NVIDIA's DAVE-2 which is a real autonomous car and University of Kansas' small scale DeepPicar. Similar to DAVE-2, its neural architecture uses 5 convolution layer and 3 fully connected layers with 250,000 parameters. We have considered Raspberry Pi 3B+ as the processing platform with Quad-core 1.4 GHz CPU based on A53 architecture which is capable to support CNN learning model.},
booktitle = {Proceedings of the International Conference on Computing Advancements},
articleno = {20},
numpages = {5},
keywords = {Autonomous Car, CNN, End-to-End Learning, Low-cost, Raspberry Pie},
location = {Dhaka, Bangladesh},
series = {ICCA 2020}
}

@article{10.1145/3718737,
author = {Zhang, Yi and Jiang, He and Guo, Shikai and Li, Xiaochen and Liu, Hui and Shi, Chongyang},
title = {Toward Understanding FPGA Synthesis Tool Bugs},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3718737},
doi = {10.1145/3718737},
abstract = {FPGA (Field Programmable Gate Array) synthesis tools are crucial for hardware development and AI acceleration, and their bugs could compromise hardware reliability and risk downstream applications. However, it remains unknown in understanding the characteristics of these bugs. What are the root causes that trigger bugs in FPGA synthesis tools? What are the characteristics of these bugs? What are the challenges in detecting and addressing them? This paper takes the first step towards answering these questions by conducting a comprehensive study of FPGA synthesis tool bugs. We analyze 551 confirmed bugs in both commercial and open source FPGA synthesis tools, i.e., Vivado, Quartus Prime, and Yosys, covering root causes, symptoms, bug-prone components, fix characteristics, and achieve 17 valuable findings. We find that, on average, around 46.2% of bugs result from HDL (Hardware Description Language) standard noncompliance across the three tools. However, it is hard for current formal validations to fully test HDL standards compliance. Additionally, on average over 25.8% bugs show domain-specific optimization traits due to inappropriate optimization and mapping. Meanwhile, beyond 28% of bugs trigger unexpected behavior without clear signs, making the formulation of effective test oracles challenging. These findings help addressing FPGA synthesis tool bugs and guide further research.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
keywords = {FPGA Logic Synthesis Tool, Logic Synthesis, Bug Characteristics, Empirical Study}
}

@inproceedings{10.1145/3650212.3680329,
author = {Yin, Yining and Feng, Yang and Weng, Shihao and Yao, Yuan and Liu, Jia and Zhao, Zhihong},
title = {Datactive: Data Fault Localization for Object Detection Systems},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680329},
doi = {10.1145/3650212.3680329},
abstract = {Object detection (OD) models are seamlessly integrated into numerous intelligent software systems, playing a crucial role in various tasks. These models are typically constructed upon humanannotated datasets, whose quality can greatly affect their performance and reliability. Erroneous and inadequate annotated datasets can induce classification/localization inaccuracies during deployment, precipitating security breaches or traffic accidents that inflict property damage or even loss of life. Therefore, ensuring and improving data quality is a crucial issue for the reliability of the object detection system. This paper introduces Datactive, a data fault localization technique for object detection systems. Datactive is designed to locate various types of data faults including mislocalization and missing objects, without utilizing the prediction of object detection models trained on dirty datasets. To achieve this, we first construct foreground-only and background-included datasets via data disassembling strategies, and then employ a robust learning method to train classifiers using disassembled datasets. Based on the classifier predictions, Datactive produces a unified suspiciousness score for both foreground annotations and image backgrounds. It allows testers to easily identify and correct faulty or missing annotations with minimal effort. To validate the effectiveness, we conducted experiments on three datasets with 6 baselines, and demonstrated the superiority of Datactive from various aspects. We also explored Datactive's ability to find natural data faults and its application in both training and evaluation scenarios.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {895–907},
numpages = {13},
keywords = {Data Quality, Deep Learning Testing, Fault Localization},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@article{10.1109/TCBB.2022.3174091,
author = {Garg, Shankey and Singh, Pradeep},
title = {Transfer Learning Based Lightweight Ensemble Model for Imbalanced Breast Cancer Classification},
year = {2022},
issue_date = {March-April 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2022.3174091},
doi = {10.1109/TCBB.2022.3174091},
abstract = {Automated classification of breast cancer can often save lives, as manual detection is usually time-consuming &amp; expensive. Since the last decade, deep learning techniques have been most widely used for the automatic classification of breast cancer using histopathology images. This paper has performed the binary and multi-class classification of breast cancer using a transfer learning-based ensemble model. To analyze the correctness and reliability of the proposed model, we have used an imbalance IDC dataset, an imbalance BreakHis dataset in the binary class scenario, and a balanced BACH dataset for the multi-class classification. A lightweight shallow CNN model with batch normalization technology to accelerate convergence is aggregated with lightweight MobileNetV2 to improve learning and adaptability. The aggregation output is fed into a multilayer perceptron to complete the final classification task. The experimental study on all three datasets was performed and compared with the recent works. We have fine-tuned three different pre-trained models (ResNet50, InceptionV4, and MobilNetV2) and compared it with the proposed lightweight ensemble model in terms of execution time, number of parameters, model size, etc. In both the evaluation phases, it is seen that our model outperforms in all three datasets.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = may,
pages = {1529–1539},
numpages = {11}
}

@inproceedings{10.1145/3650212.3680373,
author = {Guo, An and Gao, Xinyu and Chen, Zhenyu and Xiao, Yuan and Liu, Jiakai and Ge, Xiuting and Sun, Weisong and Fang, Chunrong},
title = {CooTest: An Automated Testing Approach for V2X Communication Systems},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680373},
doi = {10.1145/3650212.3680373},
abstract = {Perceiving the complex driving environment precisely is crucial to the safe operation of autonomous vehicles. With the tremendous advancement of deep learning and communication technology, Vehicle-to-Everything (V2X) collaboration has the potential to address limitations in sensing distant objects and occlusion for a single-agent perception system. However, despite spectacular progress, several communication challenges can undermine the effectiveness of multi-vehicle cooperative perception. The low interpretability of Deep Neural Networks (DNNs) and the high complexity of communication mechanisms make conventional testing techniques inapplicable for the cooperative perception of autonomous driving systems (ADS). Besides, the existing testing techniques, depending on manual data collection and labeling, become time-consuming and prohibitively expensive.
 
 
 
 
 
 
 
In this paper, we design and implement CooTest, the first automated testing tool of the V2X-oriented cooperative perception module. CooTest devises the V2X-specific metamorphic relation and equips communication and weather transformation operators that can reflect the impact of the various cooperative driving factors to produce transformed scenes. Furthermore, we adopt a V2X-oriented guidance strategy for the transformed scene generation process and improve testing efficiency. We experiment CooTest with multiple cooperative perception models with different fusion schemes to evaluate its performance on different tasks. The experiment results show that CooTest can effectively detect erroneous behaviors under various V2X-oriented driving conditions. Also, the results confirm that CooTest can improve detection average precision and decrease misleading cooperation errors by retraining with the generated scenes.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1453–1465},
numpages = {13},
keywords = {Autonomous driving system, Cooperative perception, Metamorphic testing, Software testing},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@article{10.1145/3631974,
author = {Zhang, Quanjun and Fang, Chunrong and Ma, Yuxiang and Sun, Weisong and Chen, Zhenyu},
title = {A Survey of Learning-based Automated Program Repair},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3631974},
doi = {10.1145/3631974},
abstract = {Automated program repair (APR) aims to fix software bugs automatically and plays a crucial role in software development and maintenance. With the recent advances in deep learning (DL), an increasing number of APR techniques have been proposed to leverage neural networks to learn bug-fixing patterns from massive open-source code repositories. Such learning-based techniques usually treat APR as a neural machine translation (NMT) task, where buggy code snippets (i.e., source language) are translated into fixed code snippets (i.e., target language) automatically. Benefiting from the powerful capability of DL to learn hidden relationships from previous bug-fixing datasets, learning-based APR techniques have achieved remarkable performance.In this article, we provide a systematic survey to summarize the current state-of-the-art research in the learning-based APR community. We illustrate the general workflow of learning-based APR techniques and detail the crucial components, including fault localization, patch generation, patch ranking, patch validation, and patch correctness phases. We then discuss the widely adopted datasets and evaluation metrics and outline existing empirical studies. We discuss several critical aspects of learning-based APR techniques, such as repair domains, industrial deployment, and the open science issue. We highlight several practical guidelines on applying DL techniques for future APR studies, such as exploring explainable patch generation and utilizing code features. Overall, our article can help researchers gain a comprehensive understanding about the achievements of the existing learning-based APR techniques and promote the practical application of these techniques. Our artifacts are publicly available at the repository: .},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {55},
numpages = {69},
keywords = {AI and software engineering, neural machine translation, deep learning, Automatic program repair}
}

@inproceedings{10.1145/3540250.3549092,
author = {Li, Zeyan and Zhao, Nengwen and Li, Mingjie and Lu, Xianglin and Wang, Lixin and Chang, Dongdong and Nie, Xiaohui and Cao, Li and Zhang, Wenchi and Sui, Kaixin and Wang, Yanhua and Du, Xu and Duan, Guoqiang and Pei, Dan},
title = {Actionable and interpretable fault localization for recurring failures in online service systems},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549092},
doi = {10.1145/3540250.3549092},
abstract = {Fault localization is challenging in an online service system due to its monitoring data's large volume and variety and complex dependencies across/within its components (e.g., services or databases). Furthermore, engineers require fault localization solutions to be actionable and interpretable, which existing research approaches cannot satisfy. Therefore, the common industry practice is that, for a specific online service system, its experienced engineers focus on localization for recurring failures based on the knowledge accumulated about the system and historical failures. More specifically, 1) they can identify the underlying root causes and take mitigation actions when pinpointing a group of indicative metrics on the faulty component; 2) their diagnosis knowledge is roughly based on how one failure might affect the components in the whole system.  

Although the above common practice is actionable and interpretable, it is largely manual, thus slow and sometimes inaccurate. In this paper, we aim to automate this practice through machine learning. That is, we propose an actionable and interpretable fault localization approach, DejaVu, for recurring failures in online service systems. For a specific online service system, DejaVu takes historical failures and dependencies in the system as input and trains a localization model offline; for an incoming failure, the trained model online recommends where the failure occurs (i.e., the faulty components) and which kind of failure occurs (i.e., the indicative group of metrics) (thus actionable), which are further interpreted both globally and locally (thus interpretable). Based on the evaluation on 601 failures from three production systems and one open-source benchmark, in less than one second, DejaVu can rank the ground truths at 1.66∼5.03-th among a long candidate list on average, outperforming baselines by 54.52%.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {996–1008},
numpages = {13},
keywords = {Recurring Failures, Online Service Systems, Fault Localization},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/1774088.1774300,
author = {Fernandes, Paulo and Lopes, Lucelene and Ruiz, Duncan D. A.},
title = {The impact of random samples in ensemble classifiers},
year = {2010},
isbn = {9781605586397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1774088.1774300},
doi = {10.1145/1774088.1774300},
abstract = {The use of ensemble classifiers, e.g., Bagging and Boosting, is wide spread to machine learning. However, most of studies in this area are based on empirical comparisons that suffer from a lack of care to the randomness of these methods. This paper describes the dangers of experiments with ensemble classifiers by analyzing the efficiency of Bagging and Boosting methods over 32 different data sets. The experiments show that variations due to randomness are often more relevant than the advantages among methods encountered in the literature. This paper main contribution is the claim, supported by statistical analysis, that no empirical comparison of ensemble classifiers can be scientifically done without paying attention to the random choices taken.},
booktitle = {Proceedings of the 2010 ACM Symposium on Applied Computing},
pages = {1002–1009},
numpages = {8},
keywords = {random samples, machine learning, ensemble classifiers, boosting, bagging, accuracy comparison},
location = {Sierre, Switzerland},
series = {SAC '10}
}

@inproceedings{10.1145/3159450.3162337,
author = {Kazerouni, Ayaan M.},
title = {Toward Continuous Assessment of the Programming Process: (Abstract Only)},
year = {2018},
isbn = {9781450351034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159450.3162337},
doi = {10.1145/3159450.3162337},
abstract = {Assessment of software tends to focus on postmortem evaluation of metrics like correctness, mergeability, and code coverage. This is evidenced in the current practices of continuous integration and deployment that focus on software's ability to pass unit tests before it can be merged into a deployment pipeline. However, little attention or tooling is given to the assessment of the software development process itself. Good process becomes both more challenging and more critical as software complexity increases. Real-time evaluation and feedback about a software developer's skills, such as incremental development, testing, and time management, could greatly increase productivity and improve the ability to write tested andcorrect code. My work focuses on the collection and analysis of fine-grained programming process data to help quantitatively model the programming process in terms of these metrics. I report on my research problem, presenting past work involving the collection and analysis of IDE event data from junior level students working on large and complex projects. The goal is to quantify the programming process in terms of incremental development and procrastination. I also present a long-term vision for my research and present work planned in the short term as a step toward that vision.},
booktitle = {Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
pages = {272},
numpages = {1},
keywords = {testing behavior, repository mining, programming process, educational data mining},
location = {Baltimore, Maryland, USA},
series = {SIGCSE '18}
}

@proceedings{10.1145/3611643,
title = {ESEC/FSE 2023: Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to ESEC/FSE 2023, the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ESEC/FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. ESEC/FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {San Francisco, CA, USA}
}

@inproceedings{10.1145/3551349.3556934,
author = {Guglielmi, Emanuela and Rosa, Giovanni and Scalabrino, Simone and Bavota, Gabriele and Oliveto, Rocco},
title = {Sorry, I don’t Understand: Improving Voice User Interface Testing},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556934},
doi = {10.1145/3551349.3556934},
abstract = {Voice-based virtual assistants are becoming increasingly popular. Such systems provide frameworks to developers on which they can build their own apps. End-users can interact with such apps through a Voice User Interface (VUI), which allows to use natural language commands to perform actions. Testing such apps is far from trivial: The same command can be expressed in different ways. To support developers in testing VUIs, Deep Learning (DL)-based tools have been integrated in the development environments (e.g., the Alexa Developer Console, or ADC) to generate paraphrases for the commands (seed utterances) specified by the developers. Such tools, however, generate few paraphrases that do not always cover corner cases. In this paper, we introduce VUI-UPSET, a novel approach that aims at adapting chatbot-testing approaches to VUI-testing. Both systems, indeed, provide a similar natural-language-based interface to users. We conducted an empirical study to understand how VUI-UPSET compares to existing approaches in terms of (i) correctness of the generated paraphrases, and (ii) capability of revealing bugs. Multiple authors analyzed 5,872 generated paraphrases, with a total of 13,310 manual evaluations required for such a process. Our results show that, while the DL-based tool integrated in the ADC generates a higher percentage of meaningful paraphrases compared to VUI-UPSET, VUI-UPSET generates more bug-revealing paraphrases. This allows developers to test more thoroughly their apps at the cost of discarding a higher number of irrelevant paraphrases.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {96},
numpages = {12},
keywords = {voice user interfaces, software testing, NLP},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3236024.3236053,
author = {Chen, Junjie and Lou, Yiling and Zhang, Lingming and Zhou, Jianyi and Wang, Xiaoleng and Hao, Dan and Zhang, Lu},
title = {Optimizing test prioritization via test distribution analysis},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236053},
doi = {10.1145/3236024.3236053},
abstract = {Test prioritization aims to detect regression faults faster via reordering test executions, and a large number of test prioritization techniques have been proposed accordingly. However, test prioritization effectiveness is usually measured in terms of the average percentage of faults detected concerned with the number of test executions, rather than the actual regression testing time, making it unclear which technique is optimal in actual regression testing time. To answer this question, this paper first conducts an empirical study to investigate the actual regression testing time of various prioritization techniques. The results reveal a number of practical guidelines. In particular, no prioritization technique can always perform optimal in practice.  To achieve the optimal prioritization effectiveness for any given project in practice, based on the findings of this study, we design learning-based Predictive Test Prioritization (PTP). PTP predicts the optimal prioritization technique for a given project based on the test distribution analysis (i.e., the distribution of test coverage, testing time, and coverage per unit time). The results show that PTP correctly predicts the optimal prioritization technique for 46 out of 50 open-source projects from GitHub, outperforming state-of-the-art techniques significantly in regression testing time, e.g., 43.16% to 94.92% improvement in detecting the first regression fault. Furthermore, PTP has been successfully integrated into the practical testing infrastructure of Baidu (a search service provider with over 600M monthly active users), and received positive feedbacks from the testing team of this company, e.g., saving beyond 2X testing costs with negligible overheads.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {656–667},
numpages = {12},
keywords = {Test Prioritization, Regression Testing, Machine Learning},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1109/ASE.2019.00127,
author = {Xie, Xiaofei and Chen, Hongxu and Li, Yi and Ma, Lei and Liu, Yang and Zhao, Jianjun},
title = {Coverage-guided fuzzing for feedforward neural networks},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00127},
doi = {10.1109/ASE.2019.00127},
abstract = {Deep neural network (DNN) has been widely applied to safety-critical scenarios such as autonomous vehicle, security surveillance, and cyber-physical control systems. Yet, the incorrect behaviors of DNNs can lead to severe accidents and tremendous losses due to hidden defects. In this paper, we present DeepHunter, a general-purpose fuzzing framework for detecting defects of DNNs. DeepHunter is inspired by traditional grey-box fuzzing and aims to increase the overall test coverage by applying adaptive heuristics according to runtime feedback. Specifically, DeepHunter provides a series of seed selection strategies, metamorphic mutation strategies, and testing criteria customized to DNN testing; all these components support multiple built-in configurations which are easy to extend. We evaluated DeepHunter on two popular datasets and the results demonstrate the effectiveness of DeepHunter in achieving coverage increase and detecting real defects. A video demonstration which showcases the main features of DeepHunter can be found at https://youtu.be/s5DfLErcgrc.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1162–1165},
numpages = {4},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/3663529.3663863,
author = {Sun, Jingling and Su, Ting and Sun, Jun and Li, Jianwen and Wang, Mengfei and Pu, Geguang},
title = {Property-Based Testing for Validating User Privacy-Related Functionalities in Social Media Apps},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663863},
doi = {10.1145/3663529.3663863},
abstract = {There are various privacy-related functionalities in social media apps. For example, users of TikTok can upload videos that record their daily activities and specify which users can view these videos. Ensuring the correctness of these functionalities is crucial. Otherwise, it may threaten the users' privacy or frustrate users. Due to the absence of appropriate automated testing techniques, manual testing remains the primary approach for validating these functionalities in the industrial setting, which is cumbersome, error-prone, and inadequate due to its small-scale validation. To this end, we adapt property-based testing to validate app behaviors against the properties described by the given privacy specifications. Our key idea is that privacy specifications maintained by testers written in natural language can be transformed into the B\"{u}chi automata, which can be used to determine whether the app has reached unexpected states as well as guide the test case generation. To support the application of our approach, we implemented an automated GUI testing tool, PDTDroid, which can detect the app behavior that is inconsistent with the checked privacy specifications. Our evaluation on TikTok, involving 125 real privacy specifications, shows that PDTDroid can efficiently validate privacy-related functionality and reduce manual effort by an average of 95.2% before each app release.Our further experiments on six popular social media apps show the generability and applicability of PDTDroid. During the evaluation, PDTDroid also found 22 previously unknown inconsistencies between the specification and implementation in these extensively tested apps (including four privacy leakage bugs, nine privacy-related functional bugs, and nine specification issues).},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {440–451},
numpages = {12},
keywords = {Android app testing, Non-crashing bugs, Property-based testing},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/2365324.2365338,
author = {Bowes, David and Hall, Tracy and Gray, David},
title = {Comparing the performance of fault prediction models which report multiple performance measures: recomputing the confusion matrix},
year = {2012},
isbn = {9781450312417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2365324.2365338},
doi = {10.1145/2365324.2365338},
abstract = {There are many hundreds of fault prediction models published in the literature. The predictive performance of these models is often reported using a variety of different measures. Most performance measures are not directly comparable. This lack of comparability means that it is often difficult to evaluate the performance of one model against another. Our aim is to present an approach that allows other researchers and practitioners to transform many performance measures of categorical studies back into a confusion matrix. Once performance is expressed in a confusion matrix alternative preferred performance measures can then be derived. Our approach has enabled us to compare the performance of 600 models published in 42 studies. We demonstrate the application of our approach on several case studies, and discuss the advantages and implications of doing this.},
booktitle = {Proceedings of the 8th International Conference on Predictive Models in Software Engineering},
pages = {109–118},
numpages = {10},
keywords = {confusion matrix, fault, machine learning},
location = {Lund, Sweden},
series = {PROMISE '12}
}

@inproceedings{10.1145/3651781.3651794,
author = {\"{O}ZER, Elif G\"{u}\c{s}ta and BUZLUCA, Feza},
title = {Test Case Prioritization For Embedded Software},
year = {2024},
isbn = {9798400708329},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3651781.3651794},
doi = {10.1145/3651781.3651794},
abstract = {Electronic devices used daily contain software, which may have errors due to human factors during coding. Testing is essential before release, especially as software complexity increases with diverse user needs. Testing new features separately and then in combination multiplies test cases. Rerunning all tests after each change is costly. The aim of this study is to develop a test case prioritization method to decrease the time to find software errors in embedded software systems. For this purpose, we extracted the basic features that characterize embedded software systems and tests that run on them. The proposed method calculates prioritization scores for test cases utilizing these characteristics. The test cases will then be arranged in a systematic manner according to their respective scores. This prioritization strategy is designed to minimize error detection time by promptly finding and resolving errors throughout the initial stages of the testing process. The proposed prioritization strategy was tested on an embedded software system, and it was evaluated using the metrics APFD (average percentage of faults detected) and APFDc (APFD with cost). The results indicate that the proposed method based on the attributes of software systems and related tests reduces the time required to find the majority of the errors.},
booktitle = {Proceedings of the 2024 13th International Conference on Software and Computer Applications},
pages = {81–89},
numpages = {9},
keywords = {APFD, APFDc, Embedded Software, Test Case Prioritization, Test Features},
location = {Bali Island, Indonesia},
series = {ICSCA '24}
}

@inproceedings{10.1145/3412841.3442019,
author = {Zhao, Kunsong and Xu, Zhou and Yan, Meng and Tang, Yutian and Fan, Ming and Catolino, Gemma},
title = {Just-in-time defect prediction for Android apps via imbalanced deep learning model},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442019},
doi = {10.1145/3412841.3442019},
abstract = {Android mobile apps have played important roles in our daily life and work. To meet new requirements from users, the mobile apps encounter frequent updates, which involves in a large quantity of code commits. Previous studies proposed to apply Just-in-Time (JIT) defect prediction for mobile apps to timely identify whether new code commits can introduce defects into apps, aiming to assure the quality of mobile apps. In general, the number of defective commit instances is much fewer than that of clean ones, in other words, the defect data is class imbalanced. In this work, we propose a novel Imbalanced Deep Learning model, called IDL, to conduct JIT defect prediction task for Android mobile apps. More specifically, we introduce a state-of-the-art cost-sensitive cross-entropy loss function into the deep neural network to learn the high-level feature representation, in which the loss function alleviates the class imbalance issue by taking the prior probability of the two types of classes into account. We conduct experiments on a benchmark defect data consisting of 12 Android mobile apps. The results of rigorous experiments show that our proposed IDL model performs significantly better than 23 comparative imbalanced learning methods in terms of Matthews correlation coefficient performance indicator.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1447–1454},
numpages = {8},
keywords = {JIT defect prediction, imbalanced learning, mobile apps},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@article{10.1145/3502287,
author = {Bansal, Ms. Aayushi and Sharma, Dr. Rewa and Kathuria, Dr. Mamta},
title = {A Systematic Review on Data Scarcity Problem in Deep Learning: Solution and Applications},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {10s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3502287},
doi = {10.1145/3502287},
abstract = {Recent advancements in deep learning architecture have increased its utility in real-life applications. Deep learning models require a large amount of data to train the model. In many application domains, there is a limited set of data available for training neural networks as collecting new data is either not feasible or requires more resources such as in marketing, computer vision, and medical science. These models require a large amount of data to avoid the problem of overfitting. One of the data space solutions to the problem of limited data is data augmentation. The purpose of this study focuses on various data augmentation techniques that can be used to further improve the accuracy of a neural network. This saves the cost and time consumption required to collect new data for the training of deep neural networks by augmenting available data. This also regularizes the model and improves its capability of generalization. The need for large datasets in different fields such as computer vision, natural language processing, security, and healthcare is also covered in this survey paper. The goal of this paper is to provide a comprehensive survey of recent advancements in data augmentation techniques and their application in various domains.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {208},
numpages = {29},
keywords = {overfitting, generalization, cost sensitive learning, transfer learning, data augmentation, Deep learning}
}

@inproceedings{10.1145/3324884.3416584,
author = {Wang, Shuai and Su, Zhendong},
title = {Metamorphic object insertion for testing object detection systems},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416584},
doi = {10.1145/3324884.3416584},
abstract = {Recent advances in deep neural networks (DNNs) have led to object detectors (ODs) that can rapidly process pictures or videos, and recognize the objects that they contain. Despite the promising progress by industrial manufacturers such as Amazon and Google in commercializing deep learning-based ODs as a standard computer vision service, ODs --- similar to traditional software --- may still produce incorrect results. These errors, in turn, can lead to severe negative outcomes for the users. For instance, an autonomous driving system that fails to detect pedestrians can cause accidents or even fatalities. However, despite their importance, principled, systematic methods for testing ODs do not yet exist.To fill this critical gap, we introduce the design and realization of MetaOD, a metamorphic testing system specifically designed for ODs to effectively uncover erroneous detection results. To this end, we (1) synthesize natural-looking images by inserting extra object instances into background images, and (2) design metamorphic conditions asserting the equivalence of OD results between the original and synthetic images after excluding the prediction results on the inserted objects. MetaOD is designed as a streamlined workflow that performs object extraction, selection, and insertion. We develop a set of practical techniques to realize an effective workflow, and generate diverse, natural-looking images for testing. Evaluated on four commercial OD services and four pretrained models provided by the TensorFlow API, MetaOD found tens of thousands of detection failures. To further demonstrate the practical usage of MetaOD, we use the synthetic images that cause erroneous detection results to retrain the model. Our results show that the model performance is significantly increased, from an mAP score of 9.3 to an mAP score of 10.5.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1053–1065},
numpages = {13},
keywords = {computer vision, deep neural networks, object detection, testing},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3540250.3549169,
author = {Tao, Guanhong and Sun, Weisong and Han, Tingxu and Fang, Chunrong and Zhang, Xiangyu},
title = {RULER: discriminative and iterative adversarial training for deep neural network fairness},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549169},
doi = {10.1145/3540250.3549169},
abstract = {Deep Neural Networks (DNNs) are becoming an integral part of many real-world applications, such as autonomous driving and financial management. While these models enable autonomy, there are however concerns regarding their ethics in decision making. For instance, fairness is an aspect that requires particular attention. A number of fairness testing techniques have been proposed to address this issue, e.g., by generating test cases called individual discriminatory instances for repairing DNNs. Although they have demonstrated great potential, they tend to generate many test cases that are not directly effective in improving fairness and incur substantial computation overhead. We propose a new model repair technique, RULER, by discriminating sensitive and non-sensitive attributes during test case generation for model repair. The generated cases are then used in training to improve DNN fairness. RULER balances the trade-off between accuracy and fairness by decomposing the training procedure into two phases and introducing a novel iterative adversarial training method for fairness. Compared to the state-of-the-art techniques on four datasets, RULER has 7-28 times more effective repair test cases generated, is 10-15 times faster in test generation, and has 26-43% more fairness improvement on ‍average.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1173–1184},
numpages = {12},
keywords = {Fairness, Deep Neural Network, Adversarial Training},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3650212.3652138,
author = {Gerten, Michael C. and Lathrop, James I. and Cohen, Myra B.},
title = {Traceback: A Fault Localization Technique for Molecular Programs},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3652138},
doi = {10.1145/3650212.3652138},
abstract = {Fault localization is essential to software maintenance tasks such  
as testing and automated program repair. Many fault localization  
techniques have been developed, the most common of which are  
spectrum-based. Most techniques have been designed for traditional programming paradigms that map passing and failing test  
cases to lines or branches of code, hence specialized programming  
paradigms which utilize different code abstractions may fail to localize well. In this paper, we study fault localization in the context  
of a class of programs, molecular programs. Recent research has  
designed automated testing and repair frameworks for these pro-  
grams but has ignored the importance of fault localization. As we  
demonstrate, using existing spectrum-based approaches may not  
provide much information. Instead we propose a novel approach,  
Traceback, that leverages temporal trace data. In an empirical study  
on a set of 89 faulty program variants, we demonstrate that Trace-  
back provides between a 32-90% improvement in localization over  
reaction-based mapping, a direct translation of spectrum-based  
localization. We see little difference in parameter tuning of Trace-  
back when all tests, or only code-based (invariant) tests are used,  
however the best depth and weight parameters vary when using  
specification based tests, which can be either functional or meta-  
morphic. Overall, invariant-based tests provide the best localization  
results (either alone or in combination with others), followed by  
metamorphic and then functional tests.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {415–427},
numpages = {13},
keywords = {chemical reaction networks, fault localization, molecular programs, software debugging},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3368089.3409671,
author = {Yan, Shenao and Tao, Guanhong and Liu, Xuwei and Zhai, Juan and Ma, Shiqing and Xu, Lei and Zhang, Xiangyu},
title = {Correlations between deep neural network model coverage criteria and model quality},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409671},
doi = {10.1145/3368089.3409671},
abstract = {Inspired by the great success of using code coverage as guidance in software testing, a lot of neural network coverage criteria have been proposed to guide testing of neural network models (e.g., model accuracy under adversarial attacks). However, while the monotonic relation between code coverage and software quality has been supported by many seminal studies in software engineering, it remains largely unclear whether similar monotonicity exists between neural network model coverage and model quality. This paper sets out to answer this question. Specifically, this paper studies the correlation between DNN model quality and coverage criteria, effects of coverage guided adversarial example generation compared with gradient decent based methods, effectiveness of coverage based retraining compared with existing adversarial training, and the internal relationships among coverage criteria.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {775–787},
numpages = {13},
keywords = {Software Testing, Deep Neural Networks},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3236024.3236082,
author = {Ma, Shiqing and Liu, Yingqi and Lee, Wen-Chuan and Zhang, Xiangyu and Grama, Ananth},
title = {MODE: automated neural network model debugging via state differential analysis and input selection},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236082},
doi = {10.1145/3236024.3236082},
abstract = {Artificial intelligence models are becoming an integral part of modern computing systems. Just like software inevitably has bugs, models have bugs too, leading to poor classification/prediction accuracy. Unlike software bugs, model bugs cannot be easily fixed by directly modifying models. Existing solutions work by providing additional training inputs. However, they have limited effectiveness due to the lack of understanding of model misbehaviors and hence the incapability of selecting proper inputs. Inspired by software debugging, we propose a novel model debugging technique that works by first conducting model state differential analysis to identify the internal features of the model that are responsible for model bugs and then performing training input selection that is similar to program input selection in regression testing. Our evaluation results on 29 different models for 6 different applications show that our technique can fix model bugs effectively and efficiently without introducing new bugs. For simple applications (e.g., digit recognition), MODE improves the test accuracy from 75% to 93% on average whereas the state-of-the-art can only improve to 85% with 11 times more training time. For complex applications and models (e.g., object recognition), MODE is able to improve the accuracy from 75% to over 91% in minutes to a few hours, whereas state-of-the-art fails to fix the bug or even degrades the test accuracy.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {175–186},
numpages = {12},
keywords = {Differential Analysis, Deep Neural Network, Debugging},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@article{10.1145/3715107,
author = {Molina, Facundo and Gorla, Alessandra and d’Amorim, Marcelo},
title = {Test Oracle Automation in the Era of LLMs},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715107},
doi = {10.1145/3715107},
abstract = {The effectiveness of a test suite in detecting faults highly depends on the quality of its test oracles. Large Language Models (LLMs) have demonstrated remarkable proficiency in tackling diverse software testing tasks. This paper aims to present a roadmap for future research on the use of LLMs for test oracle automation. We discuss the progress made in the field of test oracle automation before the introduction of LLMs, identifying the main limitations and weaknesses of existing techniques. Additionally, we discuss recent studies on the use of LLMs for this task, highlighting the main challenges that arise from their use, e.g., how to assess quality and usefulness of the generated oracles. We conclude with a discussion about the directions and opportunities for future research on LLM-based oracle automation.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Test Oracle Problem, Large Language Models}
}

@article{10.1145/3695990,
author = {Zheng, Zheng and Ren, Daixu and Liu, Huai and Chen, Tsong Yueh and Li, Tiancheng},
title = {Identifying the Failure-Revealing Test Cases in Metamorphic Testing: A Statistical Approach},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3695990},
doi = {10.1145/3695990},
abstract = {Metamorphic testing, thanks to its high failure-detection effectiveness especially in the absence of test oracle, has been widely applied in both the traditional context of software testing and other relevant fields such as fault localization and program repair. Its core element is a set of metamorphic relations, which are the necessary properties of the target algorithm in the form of the relationships among multiple inputs and corresponding expected outputs. When a relation is violated by the outputs of a group of test cases, namely metamorphic group of test cases, that are constructed based on the relation, a failure is said to be revealed. Traditionally, the primary task of software testing is to reveal failures. Therefore, from the perspective of software testing, it may not need to know which test case(s) in the metamorphic group cause the violation and thus the failure. However, such information is definitely helpful for other software engineering activities, such as software debugging. The current literature of metamorphic testing lacks a systematic mechanism of identifying the actual failure-revealing test cases, which hinders its applicability and effectiveness in other relevant fields. In this article, we propose a new technique for the FAILure-revealing Test case Identification in Metamorphic testing, namely FAILTIM. The approach is based on a novel application of statistical methods. More specifically, we leverage and adapt the basic ideas of spectrum-based techniques, which are originally used in fault localization, and propose the utilization of a set of risk formulas to estimate the suspiciousness of each individual test case in metamorphic groups. Failure-revealing test cases are then suggested according to their suspiciousness. A series of experiments have been conducted to evaluate the effectiveness and efficiency of FAILTIM using 9 subject programs and 30 risk formulas. The experimental results showed that the new approach can achieve a high accuracy in identifying the actual failure-revealing test cases in metamorphic testing. Consequently, our study will help boost the applicability and performance of metamorphic testing beyond testing to other software engineering areas. The present work also unfolds a number of research directions for further advancing the theory of metamorphic testing and more broadly, software testing.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {41},
numpages = {26},
keywords = {Metamorphic testing, metamorphic groups, failure-revealing test cases, spectrum-based approach}
}

@inproceedings{10.1145/3377812.3382146,
author = {Tufano, Michele and Kimko, Jason and Wang, Shiya and Watson, Cody and Bavota, Gabriele and Di Penta, Massimiliano and Poshyvanyk, Denys},
title = {DeepMutation: a neural mutation tool},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3382146},
doi = {10.1145/3377812.3382146},
abstract = {Mutation testing can be used to assess the fault-detection capabilities of a given test suite. To this aim, two characteristics of mutation testing frameworks are of paramount importance: (i) they should generate mutants that are representative of real faults; and (ii) they should provide a complete tool chain able to automatically generate, inject, and test the mutants. To address the first point, we recently proposed an approach using a Recurrent Neural Network Encoder-Decoder architecture to learn mutants from ~787k faults mined from real programs. The empirical evaluation of this approach confirmed its ability to generate mutants representative of real faults. In this paper, we address the second point, presenting DeepMutation, a tool wrapping our deep learning model into a fully automated tool chain able to generate, inject, and test mutants learned from real faults.Video: https://sites.google.com/view/learning-mutation/deepmutation},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {29–32},
numpages = {4},
keywords = {software testing, neural networks, mutation testing},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3663529.3663792,
author = {Hora, Andre},
title = {Monitoring the Execution of 14K Tests: Methods Tend to Have One Path That Is Significantly More Executed},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663792},
doi = {10.1145/3663529.3663792},
abstract = {The literature has provided evidence that developers are likely to test some behaviors of the program and avoid other ones. Despite this observation, we still lack empirical evidence from real-world systems. In this paper, we propose to automatically identify the tested paths of a method as a way to detect the method's behaviors. Then, we provide an empirical study to assess the tested paths quantitatively. We monitor the execution of 14,177 tests from 25 real-world Python systems and assess 11,425 tested paths from 2,357 methods. Overall, our empirical study shows that one tested path is prevalent and receives most of the calls, while others are significantly less executed. We find that the most frequently executed tested path of a method has 4x more calls than the second one. Based on these findings, we discuss practical implications for practitioners and researchers and future research directions.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {532–536},
numpages = {5},
keywords = {dynamic analysis, python, runtime monitoring, software testing},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/3691620.3695551,
author = {Wang, Jiale Amber and Wang, Kaiyuan and Nie, Pengyu},
title = {Efficient Incremental Code Coverage Analysis for Regression Test Suites},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695551},
doi = {10.1145/3691620.3695551},
abstract = {Code coverage analysis has been widely adopted in the continuous integration of open-source and industry software repositories to monitor the adequacy of regression test suites. However, computing code coverage can be costly, introducing significant overhead during test execution. Plus, re-collecting code coverage for the entire test suite is usually unnecessary when only a part of the coverage data is affected by code changes. While regression test selection (RTS) techniques exist to select a subset of tests whose behaviors may be affected by code changes, they are not compatible with code coverage analysis techniques---that is, simply executing RTS-selected tests leads to incorrect code coverage results.In this paper, we present the first incremental code coverage analysis technique, which speeds up code coverage analysis by executing a minimal subset of tests to update the coverage data affected by code changes. We implement our technique in a tool dubbed iJaCoCo, which builds on Ekstazi and JaCoCo---the state-of-the-art RTS and code coverage analysis tools for Java. We evaluate iJaCoCo on 1,122 versions from 22 open-source repositories and show that iJaCoCo can speed up code coverage analysis time by an average of 1.86\texttimes{} and up to 8.20\texttimes{} compared to JaCoCo.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1882–1894},
numpages = {13},
keywords = {regression testing, code coverage analysis, regression test selection},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3597503.3639078,
author = {Sun, Yue and Yang, Guowei and Lv, Shichao and Li, Zhi and Sun, Limin},
title = {Concrete Constraint Guided Symbolic Execution},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639078},
doi = {10.1145/3597503.3639078},
abstract = {Symbolic execution is a popular program analysis technique. It systematically explores all feasible paths of a program but its scalability is largely limited by the path explosion problem, which causes the number of paths proliferates at runtime. A key idea in existing methods to mitigate this problem is to guide the selection of states for path exploration, which primarily relies on the features to represent program states. In this paper, we propose concrete constraint guided symbolic execution, which aims to cover more concrete branches and ultimately improve the overall code coverage during symbolic execution. Our key insight is based on the fact that symbolic execution strives to cover all symbolic branches while concrete branches are neglected, and directing symbolic execution toward uncovered concrete branches has a great potential to improve the overall code coverage. The experimental results demonstrate that our approach can improve the ability of KLEE to both increase code coverage and find more security violations on 10 open-source C programs.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {122},
numpages = {12},
keywords = {symbolic execution, data dependency analysis},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3545258.3545266,
author = {Li, Zhengmin and Tang, Enyi and Chen, Xin and Wang, Linzhang and Li, Xuandong},
title = {Graph Neural Network based Two-Phase Fault Localization Approach},
year = {2022},
isbn = {9781450397803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545258.3545266},
doi = {10.1145/3545258.3545266},
abstract = {Spectrum-based fault localization(SBFL) has become one of the most widely studied localization techniques by its effectiveness and lightweightness. However, existing simple SBFL techniques are still not accurate enough for they are not able to distinguish specific locations in the same basic block. To address this problem, techniques that combine SBFL and MBFL(Mutation-based fault localization) have been proposed with the cost of introducing huge overhead from mutants. This paper proposes a graph neural network (GNN) based two-phase localization approach that localizes statements in blocks accurately and efficiently. The graph neural network introduced from our approach extracts the information from both the control flow graph and data flow graph, which includes the dependencies that distinguish the specific locations and further increase the localization accuracy. Our localization process is divided into two phases: Phase-I computes the suspiciousness score of each method and generates a ranking list, and phase-II further highlights the potential faulty locations inside a method by a fine-grained GNN with graphs in the method. We conduct experiments on 357 real bugs of 5 projects in the Defects4j benchmark. The results show that with a small overhead in our approach, the number of our successfully localized faults within the top-1, top-3, and top-5 positions is obviously higher than other SBFL techniques.},
booktitle = {Proceedings of the 13th Asia-Pacific Symposium on Internetware},
pages = {85–95},
numpages = {11},
keywords = {Spectrum-based fault localization, Graph Neural Network, Fault Localization},
location = {Hohhot, China},
series = {Internetware '22}
}

@article{10.14778/3476249.3476266,
author = {Chen, Zhiwei and Song, Shaoxu and Wei, Ziheng and Fang, Jingyun and Long, Jiang},
title = {Approximating median absolute deviation with bounded error},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476249.3476266},
doi = {10.14778/3476249.3476266},
abstract = {The median absolute deviation (MAD) is a statistic measuring the variability of a set of quantitative elements. It is known to be more robust to outliers than the standard deviation (SD), and thereby widely used in outlier detection. Computing the exact MAD however is costly, e.g., by calling an algorithm of finding median twice, with space cost O (n) over n elements in a set. In this paper, we propose the first fully mergeable approximate MAD algorithm, OP-MAD, with one-pass scan of the data. Remarkably, by calling the proposed algorithm at most twice, namely TP-MAD, it guarantees to return an (ϵ, 1)-accurate MAD, i.e., the error relative to the exact MAD is bounded by the desired ϵ or 1. The space complexity is reduced to O(m) while the time complexity is O(n + m log m), where m is the size of the sketch used to compress data, related to the desired error bound ϵ. To get a more accurate MAD, i.e., with smaller ϵ, the sketch size m will be larger, a trade-off between effectiveness and efficiency. In practice, we often have the sketch size m ≪ n, leading to constant space cost O(1) and linear time cost O(n). The extensive experiments over various datasets demonstrate the superiority of our solution, e.g., 160000\texttimes{} less memory and 18x faster than the aforesaid exact method in datasets pareto and norm. Finally, we further implement and evaluate the parallelizable TP-MAD in Apache Spark, and the fully mergeable OP-MAD in Structured Streaming.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {2114–2126},
numpages = {13}
}

@inproceedings{10.1145/3689493.3689981,
author = {Van Praet, Lucas and Hoobergs, Jesse and Schrijvers, Tom},
title = {ASSIST: Automated Feedback Generation for Syntax and Logical Errors in Programming Exercises},
year = {2024},
isbn = {9798400712166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689493.3689981},
doi = {10.1145/3689493.3689981},
abstract = {Introductory programming courses often rely on numerous exercises to help students practice and reinforce their skills. Commonly used automated tests fall short by merely identifying the issues without offering guidance on how to resolve them and manual reviews are too resource-intensive to use in large classes. To address these challenges, we present ASSIST—a tool designed to provide automated, detailed feedback on how to resolve issues in programming exercise submissions with both syntactic and logical errors. ASSIST combines fault-tolerant parsing with fixes based on the context of error nodes to resolve syntactic errors and give feedback. ASSIST feeds this valid program to the Sketch program synthesis tool to determine the needed changes from a set of potential changes induced by rewrite rules, and generates feedback on logic errors based on the needed changes. This dual approach allows ASSIST to offer actionable feedback on both syntax and logic issues in student submissions. We evaluated ASSIST on submissions from an online platform for secondary education. Our findings reveal that, for submissions with syntax errors, ASSIST delivers feedback on all syntax errors in 71% of cases and extends its feedback to cover logical errors in 34% of these submissions. When evaluating all incorrect submissions, ASSIST is able to give feedback on logical errors in 64% of cases. These results indicate that ASSIST can significantly enhance the feedback process in large-scale programming courses, offering a feasible and efficient alternative to current methods.},
booktitle = {Proceedings of the 2024 ACM SIGPLAN International Symposium on SPLASH-E},
pages = {66–76},
numpages = {11},
keywords = {Automated Feedback, Computer Science Education, Program Repair},
location = {Pasadena, CA, USA},
series = {SPLASH-E '24}
}

@inproceedings{10.5555/2663370.2663378,
author = {Kanewala, Upulee and Bieman, James M.},
title = {Techniques for testing scientific programs without an Oracle},
year = {2013},
isbn = {9781467362610},
publisher = {IEEE Press},
abstract = {The existence of an oracle is often assumed in software testing. But in many situations, especially for scientific programs, oracles do not exist or they are too hard to implement. This paper examines three techniques that are used to test programs without oracles: (1) Metamorphic testing, (2) Run-time Assertions and (3) Developing test oracles using machine learning. We examine these methods in terms of their (1) fault finding ability, (2) automation, and (3) required domain knowledge. Several case studies apply these three techniques to effectively test scientific programs that do not have oracles. Certain techniques have reported a better fault finding ability than the others when testing specific programs. Finally, there is potential to increase the level of automation of these techniques, thereby reducing the required level of domain knowledge. Techniques that can potentially be automated include (1) detection of likely metamorphic relations, (2) static analyses to eliminate spurious invariants and (3) structural analyses to develop machine learning generated oracles.},
booktitle = {Proceedings of the 5th International Workshop on Software Engineering for Computational Science and Engineering},
pages = {48–57},
numpages = {10},
keywords = {test oracles, scientific software testing, mutation analysis, metamorphic testing, metamorphic relation, machine learning, assertion checking},
location = {San Francisco, California},
series = {SE-CSE '13}
}

@inproceedings{10.1145/3549206.3549319,
author = {Rizwan, Syed and Ali Sobuj, Md. Shazzad and Akhond, Mostafijur Rahman},
title = {A Survey on Software Test Case Minimization},
year = {2022},
isbn = {9781450396752},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549206.3549319},
doi = {10.1145/3549206.3549319},
abstract = {Software testing consumes the most time and resource-intensive phase of software testing. Test case minimization techniques are used to reduce the test suite which in turn saves time and resources. The main purpose of test case minimization is to eliminate the ineffective test cases in a way that the remaining test cases satisfy all the requirements as before. Many techniques are used for test case minimization. The use of a convoluted neural net, a nature-inspired algorithm like fire algorithm, data mining technique, and coverage-based and hybrid techniques has been reviewed in this paper. In this study, a few of the most recent takes on test case minimization has been summarized and sorted depending on their approach. This is a survey paper that reviews the way each paper handled their problem, which dataset they used, their accuracy on the respective dataset, and the shortcomings.},
booktitle = {Proceedings of the 2022 Fourteenth International Conference on Contemporary Computing},
pages = {679–684},
numpages = {6},
keywords = {Test Case Reduction Technique, Test Case Minimization, Survey, Software Testing, Regression Testing},
location = {Noida, India},
series = {IC3-2022}
}

@inproceedings{10.1145/3650212.3680395,
author = {Tian, Zhao and Shu, Honglin and Wang, Dong and Cao, Xuejie and Kamei, Yasutaka and Chen, Junjie},
title = {Large Language Models for Equivalent Mutant Detection: How Far Are We?},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680395},
doi = {10.1145/3650212.3680395},
abstract = {Mutation testing is vital for ensuring software quality. However, the presence of equivalent mutants is known to introduce redundant cost and bias issues, hindering the effectiveness of mutation testing in practical use. Although numerous equivalent mutant detection (EMD) techniques have been proposed, they exhibit limitations due to the scarcity of training data and challenges in generalizing to unseen mutants. Recently, large language models (LLMs) have been extensively adopted in various code-related tasks and have shown superior performance by more accurately capturing program semantics. Yet the performance of LLMs in equivalent mutant detection remains largely unclear. In this paper, we conduct an empirical study on 3,302 method-level Java mutant pairs to comprehensively investigate the effectiveness and efficiency of LLMs for equivalent mutant detection. Specifically, we assess the performance of LLMs compared to existing EMD techniques, examine the various strategies of LLMs, evaluate the orthogonality between EMD techniques, and measure the time overhead of training and inference. Our findings demonstrate that LLM-based techniques significantly outperform existing techniques (i.e., the average improvement of 35.69% in terms of F1-score), with the fine-tuned code embedding strategy being the most effective. Moreover, LLM-based techniques offer an excellent balance between cost (relatively low training and inference time) and effectiveness. Based on our findings, we further discuss the impact of model size and embedding quality, and provide several promising directions for future research. This work is the first to examine LLMs in equivalent mutant detection, affirming their effectiveness and efficiency.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1733–1745},
numpages = {13},
keywords = {Empirical Study, Equivalent Mutant Detection, Large Language Model, Mutation Testing},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1109/ICSE-NIER.2019.00030,
author = {Sekhon, Jasmine and Fleming, Cody},
title = {Towards improved testing for deep learning},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2019.00030},
doi = {10.1109/ICSE-NIER.2019.00030},
abstract = {The growing use of deep neural networks in safety-critical applications makes it necessary to carry out adequate testing to detect and correct any incorrect behavior for corner case inputs before they can be actually used. Deep neural networks lack an explicit control-flow structure, making it impossible to apply to them traditional software testing criteria such as code coverage. In this paper, we examine existing testing methods for deep neural networks, the opportunities for improvement and the need for a fast, scalable, generalizable end-to-end testing method. We also propose a coverage criterion for deep neural networks that tries to capture all possible parts of the deep neural network's logic.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {85–88},
numpages = {4},
keywords = {whitebox testing, deep neural networks, coverage criterion},
location = {Montreal, Quebec, Canada},
series = {ICSE-NIER '19}
}

@inproceedings{10.1145/3650212.3680400,
author = {Guan, Kevin and Legunsen, Owolabi},
title = {An In-Depth Study of Runtime Verification Overheads during Software Testing},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680400},
doi = {10.1145/3650212.3680400},
abstract = {Runtime verification (RV) monitors program executions against formal specifications (specs). Researchers showed that RV during software testing amplifies the bug-finding ability of tests, and found hundreds of new bugs by using RV to monitor passing tests in open-source projects. But, RV’s runtime overhead is widely seen as a hindrance to its broad adoption, especially during continuous integration. Yet, there is no in-depth study of the prevalence, usefulness for bug finding, and components of these overheads during testing, so that researchers can better understand how to speed up RV.
 
 
 
We study RV overhead during testing, monitoring developer-written unit tests in 1,544 open-source projects against 160 specs of correct JDK API usage. We make four main findings. (1) RV overhead is below 12.48 seconds, which others considered acceptable, in 40.9% of projects, but up to 5,002.9x (or, 28.7 hours) in the other projects. (2) 99.87% of monitors that RV generates to dynamically check program traces are wasted; they can only find bugs that the other 0.13% find. (3) Contrary to conventional wisdom, RV overhead in most projects is dominated by instrumentation, not monitoring. (4) 36.74% of monitoring time is spent in test code or libraries.
 
 
 
As evidence that our study provides a new basis that future work can exploit, we perform two more experiments. First, we show that offline instrumentation (when possible) greatly reduces RV runtime overhead for single versions of many projects. Second, we show that simply amortizing high instrumentation costs across multiple program versions can outperform, by up to 4.53x, a recent evolution-aware RV technique that uses complex program analysis.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1798–1810},
numpages = {13},
keywords = {Runtime Verification, software evolution, software testing},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3691620.3695513,
author = {Jiang, Zongze and Wen, Ming and Cao, Jialun and Shi, Xuanhua and Jin, Hai},
title = {Towards Understanding the Effectiveness of Large Language Models on Directed Test Input Generation},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695513},
doi = {10.1145/3691620.3695513},
abstract = {Automatic testing has garnered significant attention and success over the past few decades. Techniques such as unit testing and coverage-guided fuzzing have revealed numerous critical software bugs and vulnerabilities. However, a long-standing, formidable challenge for existing techniques is how to achieve higher testing coverage. Constraint-based techniques, such as symbolic execution and concolic testing, have been well-explored and integrated into the existing approaches. With the popularity of Large Language Models (LLMs), recent research efforts to design tailored prompts to generate inputs that can reach more uncovered target branches. However, the effectiveness of using LLMs for generating such directed inputs and the comparison with the proven constraint-based solutions has not been systematically explored.To bridge this gap, we conduct the first systematic study on the mainstream LLMs and constraint-based tools for directed input generation with a comparative perspective. We find that LLMs such as ChatGPT are comparable to or even better than the constraint-based tools, succeeding in 43.40%-58.57% samples in our dataset. Meanwhile, there are also limitations for LLMs in specific scenarios such as sequential calculation, where constraint-based tools are in a position of strength. Based on these findings, we propose a simple yet effective method to combine these two types of tools and implement a prototype based on ChatGPT and constraint-based tools. Our evaluation shows that our approach can outperform the baselines by 1.4x to 2.3x relatively. We believe our study can provide novel insights into directed input generation using LLMs, and our findings are essential for future testing research.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1408–1420},
numpages = {13},
keywords = {LLM, symbolic execution, directed input generation},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1109/ICSE43902.2021.00043,
author = {Zhang, Xiaoyu and Zhai, Juan and Ma, Shiqing and Shen, Chao},
title = {AutoTrainer: An Automatic DNN Training Problem Detection and Repair System},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00043},
doi = {10.1109/ICSE43902.2021.00043},
abstract = {With machine learning models especially Deep Neural Network (DNN) models becoming an integral part of the new intelligent software, new tools to support their engineering process are in high demand. Existing DNN debugging tools are either post-training which wastes a lot of time training a buggy model and requires expertises, or limited on collecting training logs without analyzing the problem not even fixing them. In this paper, we propose AutoTrainer, a DNN training monitoring and automatic repairing tool which supports detecting and auto-repairing five commonly seen training problems. During training, it periodically checks the training status and detects potential problems. Once a problem is found, AutoTrainer tries to fix it by using built-in state-of-the-art solutions. It supports various model structures and input data types, such as Convolutional Neural Networks (CNNs) for image and Recurrent Neural Networks (RNNs) for texts. Our evaluation on 6 datasets, 495 models show that AutoTrainer can effectively detect all potential problems with 100% detection rate and no false positives. Among all models with problems, it can fix 97.33% of them, increasing the accuracy by 47.08% on average.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {359–371},
numpages = {13},
keywords = {software tools, software engineering, deep learning training},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1145/3433928,
author = {Vandehei, Bailey and Costa, Daniel Alencar Da and Falessi, Davide},
title = {Leveraging the Defects Life Cycle to Label Affected Versions and Defective Classes},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3433928},
doi = {10.1145/3433928},
abstract = {Two recent studies explicitly recommend labeling defective classes in releases using the affected versions (AV) available in issue trackers (e.g., Jira). This practice is coined as the realistic approach. However, no study has investigated whether it is feasible to rely on AVs. For example, how available and consistent is the AV information on existing issue trackers? Additionally, no study has attempted to retrieve AVs when they are unavailable. The aim of our study is threefold: (1) to measure the proportion of defects for which the realistic method is usable, (2) to propose a method for retrieving the AVs of a defect, thus making the realistic approach usable when AVs are unavailable, (3) to compare the accuracy of the proposed method versus three SZZ implementations. The assumption of our proposed method is that defects have a stable life cycle in terms of the proportion of the number of versions affected by the defects before discovering and fixing these defects. Results related to 212 open-source projects from the Apache ecosystem, featuring a total of about 125,000 defects, reveal that the realistic method cannot be used in the majority (51%) of defects. Therefore, it is important to develop automated methods to retrieve AVs. Results related to 76 open-source projects from the Apache ecosystem, featuring a total of about 6,250,000 classes, affected by 60,000 defects, and spread over 4,000 versions and 760,000 commits, reveal that the proportion of the number of versions between defect discovery and fix is pretty stable (standard deviation &lt;2)—across the defects of the same project. Moreover, the proposed method resulted significantly more accurate than all three SZZ implementations in (i) retrieving AVs, (ii) labeling classes as defective, and (iii) in developing defects repositories to perform feature selection. Thus, when the realistic method is unusable, the proposed method is a valid automated alternative to SZZ for retrieving the origin of a defect. Finally, given the low accuracy of SZZ, researchers should consider re-executing the studies that have used SZZ as an oracle and, in general, should prefer selecting projects with a high proportion of available and consistent AVs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {24},
numpages = {35},
keywords = {developing defects repository, defect origin, SZZ, Affected version}
}

@inproceedings{10.1145/3691620.3695530,
author = {Feng, Shiwei and Ye, Yapeng and Shi, Qingkai and Cheng, Zhiyuan and Xu, Xiangzhe and Cheng, Siyuan and Choi, Hongjun and Zhang, Xiangyu},
title = {ROCAS: Root Cause Analysis of Autonomous Driving Accidents via Cyber-Physical Co-mutation},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695530},
doi = {10.1145/3691620.3695530},
abstract = {As Autonomous driving systems (ADS) have transformed our daily life, safety of ADS is of growing significance. While various testing approaches have emerged to enhance the ADS reliability, a crucial gap remains in understanding the accidents causes. Such post-accident analysis is paramount and beneficial for enhancing ADS safety and reliability. Existing cyber-physical system (CPS) root cause analysis techniques are mainly designed for drones and cannot handle the unique challenges introduced by more complex physical environments and deep learning models deployed in ADS. In this paper, we address the gap by offering a formal definition of ADS root cause analysis problem and introducing Rocas, a novel ADS root cause analysis framework featuring cyber-physical co-mutation. Our technique uniquely leverages both physical and cyber mutation that can precisely identify the accident-trigger entity and pinpoint the misconfiguration of the target ADS responsible for an accident. We further design a differential analysis to identify the responsible module to reduce search space for the misconfiguration. We study 12 categories of ADS accidents and demonstrate the effectiveness and efficiency of Rocas in narrowing down search space and pinpointing the misconfiguration. We also show detailed case studies on how the identified misconfiguration helps understand rationale behind accidents.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1620–1632},
numpages = {13},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3544548.3580852,
author = {Wang, Zhijie and Huang, Yuheng and Song, Da and Ma, Lei and Zhang, Tianyi},
title = {DeepSeer: Interactive RNN Explanation and Debugging via State Abstraction},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580852},
doi = {10.1145/3544548.3580852},
abstract = {Recurrent Neural Networks (RNNs) have been widely used in Natural Language Processing (NLP) tasks given its superior performance on processing sequential data. However, it is challenging to interpret and debug RNNs due to the inherent complexity and the lack of transparency of RNNs. While many explainable AI (XAI) techniques have been proposed for RNNs, most of them only support local explanations rather than global explanations. In this paper, we present DeepSeer, an interactive system that provides both global and local explanations of RNN behavior in multiple tightly-coordinated views for model understanding and debugging. The core of DeepSeer is a state abstraction method that bundles semantically similar hidden states in an RNN model and abstracts the model as a finite state machine. Users can explore the global model behavior by inspecting text patterns associated with each state and the transitions between states. Users can also dive into individual predictions by inspecting the state trace and intermediate prediction results of a given input. A between-subjects user study with 28 participants shows that, compared with a popular XAI technique, LIME, participants using DeepSeer made a deeper and more comprehensive assessment of RNN model behavior, identified the root causes of incorrect predictions more accurately, and came up with more actionable plans to improve the model performance.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {740},
numpages = {20},
keywords = {Explainable AI, Model Debugging, Recurrent Neural Networks, Visualization},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3377811.3380378,
author = {Islam, Md Johirul and Pan, Rangeet and Nguyen, Giang and Rajan, Hridesh},
title = {Repairing deep neural networks: fix patterns and challenges},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380378},
doi = {10.1145/3377811.3380378},
abstract = {Significant interest in applying Deep Neural Network (DNN) has fueled the need to support engineering of software that uses DNNs. Repairing software that uses DNNs is one such unmistakable SE need where automated tools could be beneficial; however, we do not fully understand challenges to repairing and patterns that are utilized when manually repairing DNNs. What challenges should automated repair tools address? What are the repair patterns whose automation could help developers? Which repair patterns should be assigned a higher priority for building automated bug repair tools? This work presents a comprehensive study of bug fix patterns to address these questions. We have studied 415 repairs from Stack Overflow and 555 repairs from GitHub for five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand challenges in repairs and bug repair patterns. Our key findings reveal that DNN bug fix patterns are distinctive compared to traditional bug fix patterns; the most common bug fix patterns are fixing data dimension and neural network connectivity; DNN bug fixes have the potential to introduce adversarial vulnerabilities; DNN bug fixes frequently introduce new bugs; and DNN bug localization, reuse of trained model, and coping with frequent releases are major challenges faced by developers when fixing bugs. We also contribute a benchmark of 667 DNN (bug, repair) instances.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1135–1146},
numpages = {12},
keywords = {bug fix, bug fix patterns, bugs, deep neural networks},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1145/3182659,
author = {Rodrigues, Davi Silva and Delamaro, M\'{a}rcio Eduardo and Corr\^{e}a, Cl\'{e}ber Gimenez and Nunes, F\'{a}tima L. S.},
title = {Using Genetic Algorithms in Test Data Generation: A Critical Systematic Mapping},
year = {2018},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3182659},
doi = {10.1145/3182659},
abstract = {Software testing activities account for a considerable portion of systems development cost and, for this reason, many studies have sought to automate these activities. Test data generation has a high cost reduction potential (especially for complex domain systems), since it can decrease human effort. Although several studies have been published about this subject, articles of reviews covering this topic usually focus only on specific domains. This article presents a systematic mapping aiming at providing a broad, albeit critical, overview of the literature in the topic of test data generation using genetic algorithms. The selected studies were categorized by software testing technique (structural, functional, or mutation testing) for which test data were generated and according to the most significantly adapted genetic algorithms aspects. The most used evaluation metrics and software testing techniques were identified. The results showed that genetic algorithms have been successfully applied to simple test data generation, but are rarely used to generate complex test data such as images, videos, sounds, and 3D (three-dimensional) models. From these results, we discuss some challenges and opportunities for research in this area.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {41},
numpages = {23},
keywords = {test case generation, systematic mapping, software testing, scoping study, genetic algorithms, evolutionary test, Test data generation}
}

@proceedings{10.1145/3558489,
title = {PROMISE 2022: Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering},
year = {2022},
isbn = {9781450398602},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our pleasure to welcome you to the 18th ACM International Conference on Predictive Models and Data Analytics in Software Engineering (PROMISE 2022), to be held in hybrid mode (physically and virtually) on November 18th, 2022, co-located with the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2022). PROMISE is an annual forum for researchers and practitioners to present, discuss and exchange ideas, results, expertise and experiences in the construction and/or application of predictive models and data analytics in software engineering. Such models and analyses could be targeted at planning, design, implementation, testing, maintenance, quality assurance, evaluation, process improvement, management, decision making, and risk assessment in software and systems development. This year PROMISE received a total of 18 paper submissions. The review process was double blind and each paper was reviewed by at least three members of the program committee. An online discussion was also held for 8 days. Based on this procedure, we accepted a total of 10 full papers, which will be presented in 3 technical sessions. The acceptance criteria were entirely based on the quality of the papers, without imposing any constraint on the number of papers to be accepted.  

We are delighted to announce an outstanding keynote: Release Engineering in the AI World: How can Analytics Help? By Prof. Bram Adams, Queen’s University, Canada  

We would like to thank all authors for submitting high quality papers, and program committee members for their timely and accurate reviewing activity. Last, but not least, we would like to thank the FSE 2022 organizers for hosting PROMISE 2022 as a co-located event and for their logistic support in the organization of the conference.  

We hope you will enjoy PROMISE 2022.  
We certainly will!  

Many thanks from  
Shane McIntosh (General Chair),  
Gema Rodriguez-Perez and Weiyi Shang (Program Chairs).},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3597926.3598060,
author = {Liu, Kaibo and Han, Yudong and Zhang, Jie M. and Chen, Zhenpeng and Sarro, Federica and Harman, Mark and Huang, Gang and Ma, Yun},
title = {Who Judges the Judge: An Empirical Study on Online Judge Tests},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598060},
doi = {10.1145/3597926.3598060},
abstract = {Online Judge platforms play a pivotal role in education, competitive programming, recruitment, career training, and large language model training. They rely on predefined test suites to judge the correctness of submitted solutions. It is therefore important that the solution judgement is reliable and free from potentially misleading false positives (i.e., incorrect solutions that are judged as correct). In this paper, we conduct an empirical study of 939 coding problems with 541,552 solutions, all of which are judged to be correct according to the test suites used by the platform, finding that 43.4% of the problems include false positive solutions (3,440 bugs are revealed in total). We also find that test suites are, nevertheless, of high quality according to widely-studied test effectiveness measurements: 88.2% of false positives have perfect (100%) line coverage, 78.9% have perfect branch coverage, and 32.5% have a perfect mutation score. Our findings indicate that more work is required to weed out false positive solutions and to further improve test suite effectiveness. We have released the detected false positive solutions and the generated test inputs to facilitate future research.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {334–346},
numpages = {13},
keywords = {test assessment, software testing, Online judge platform},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@inproceedings{10.1145/2851613.2851973,
author = {Hussain, Shahid and Khan, Arif Ali and Bennin, Kwabena Ebo},
title = {Empirical investigation of fault predictors in context of class membership probability estimation},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851973},
doi = {10.1145/2851613.2851973},
abstract = {In the domain of software fault prediction, class membership probability of a selected classifier and the factors related to its estimation can be considered as necessary information for tester to take informed decisions about software quality issues. The objective of this study is to empirically investigate the class membership probability estimation capability of 15 classifiers/fault predictors on 12 datasets of open source projects retrieved from PROMISE repository. We empirically validate the effect of dataset characteristics and set of metrics on the performance of classifiers in estimating the class membership probability. We used Receiver Operating Characteristics-Area under Curve (ROC-AUC) value and overall accuracy as benchmarks to evaluate and compare the performance of classifiers. We apply Friedman's, post-hoc Nemenyi and Analysis of Means (ANOM) test to compare the significant performance of classifiers. We conclude that ADTree and RandomForest outperform, while ZeroR classifier cannot show significant performance for estimation of class membership probability.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1550–1553},
numpages = {4},
keywords = {accuracy, chidamber and kemerer metrics, classifiers, estimation, performance, probability, weka},
location = {Pisa, Italy},
series = {SAC '16}
}

@article{10.1145/3571855,
author = {Nass, Michel and Al\'{e}groth, Emil and Feldt, Robert and Leotta, Maurizio and Ricca, Filippo},
title = {Similarity-based Web Element Localization for Robust Test Automation},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3571855},
doi = {10.1145/3571855},
abstract = {Non-robust (fragile) test execution is a commonly reported challenge in GUI-based test automation, despite much research and several proposed solutions. A test script needs to be resilient to (minor) changes in the tested application but, at the same time, fail when detecting potential issues that require investigation. Test script fragility is a multi-faceted problem. However, one crucial challenge is how to reliably identify and locate the correct target web elements when the website evolves between releases or otherwise fail and report an issue. This article proposes and evaluates a novel approach called similarity-based web element localization (Similo), which leverages information from multiple web element locator parameters to identify a target element using a weighted similarity score. This experimental study compares Similo to a baseline approach for web element localization. To get an extensive empirical basis, we target 48 of the most popular websites on the Internet in our evaluation. Robustness is considered by counting the number of web elements found in a recent website version compared to how many of these existed in an older version. Results of the experiment show that Similo outperforms the baseline; it failed to locate the correct target web element in 91 out of 801 considered cases (i.e., 11%) compared to 214 failed cases (i.e., 27%) for the baseline approach. The time efficiency of Similo was also considered, where the average time to locate a web element was determined to be 4 milliseconds. However, since the cost of web interactions (e.g., a click) is typically on the order of hundreds of milliseconds, the additional computational demands of Similo can be considered negligible. This study presents evidence that quantifying the similarity between multiple attributes of web elements when trying to locate them, as in our proposed Similo approach, is beneficial. With acceptable efficiency, Similo gives significantly higher effectiveness (i.e., robustness) than the baseline web element localization approach.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {75},
numpages = {30},
keywords = {XPath locators, web element locators, test case robustness, test automation, GUI testing}
}

@inproceedings{10.1145/3691620.3695261,
author = {Haben, Guillaume and Habchi, Sarra and Micco, John and Harman, Mark and Papadakis, Mike and Cordy, Maxime and Le Traon, Yves},
title = {The Importance of Accounting for Execution Failures when Predicting Test Flakiness},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695261},
doi = {10.1145/3691620.3695261},
abstract = {Flaky tests are tests that pass and fail on different executions of the same version of a program under test. They waste valuable developer time by making developers investigate false alerts (flaky test failures). To deal with this issue, many prediction methods have been proposed. However, the utility of these methods remains unclear since they are typically evaluated based on single-release data, ignoring that in many cases tests that fail flakily in one release also correctly fail (indicating the presence of bugs) in some other, meaning that it is possible for subsequent correctly-failing cases to pass unnoticed. In this paper, we show that this situation is prevalent and can raise significant concerns for both researchers and practitioners. In particular, we show that flaky tests, tests that exhibit flaky behaviour at some point in time, have a strong fault-revealing capability, i.e., they reveal more than 1/3 of all encountered regression faults. We also show that 76.2%, of all test executions that reveal faults in the codebase under test are made by tests that are classified as flaky by existing prediction methods. Overall, our findings motivate the need for future research to focus on predicting flaky test executions instead of flaky tests.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1979–1989},
numpages = {11},
keywords = {software testing, flaky tests, ML, continuous integration},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@article{10.1145/3607184,
author = {Clark, Andrew G. and Foster, Michael and Prifling, Benedikt and Walkinshaw, Neil and Hierons, Robert M. and Schmidt, Volker and Turner, Robert D.},
title = {Testing Causality in Scientific Modelling Software},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3607184},
doi = {10.1145/3607184},
abstract = {From simulating galaxy formation to viral transmission in a pandemic, scientific models play a pivotal role in developing scientific theories and supporting government policy decisions that affect us all. Given these critical applications, a poor modelling assumption or bug could have far-reaching consequences. However, scientific models possess several properties that make them notoriously difficult to test, including a complex input space, long execution times, and non-determinism, rendering existing testing techniques impractical. In fields such as epidemiology, where researchers seek answers to challenging causal questions, a statistical methodology known as Causal inference has addressed similar problems, enabling the inference of causal conclusions from noisy, biased, and sparse data instead of costly experiments. This article introduces the causal testing framework: a framework that uses causal inference techniques to establish causal effects from existing data, enabling users to conduct software testing activities concerning the effect of a change, such as metamorphic testing, a posteriori. We present three case studies covering real-world scientific models, demonstrating how the causal testing framework can infer metamorphic test outcomes from reused, confounded test data to provide an efficient solution for testing scientific modelling software.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {10},
numpages = {42},
keywords = {causal testing, causal inference, Software testing}
}

@inproceedings{10.1145/3143434.3143463,
author = {Melleg\r{a}rd, Niklas},
title = {Using weekly open defect reports as an indicator for software process efficiency: theoretical framework and a longitudinal automotive industrial case study},
year = {2017},
isbn = {9781450348539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3143434.3143463},
doi = {10.1145/3143434.3143463},
abstract = {Well-defined, informative and cheap indicators are important in any software development organization that needs to evaluate aspects of its development processes and product quality. This is especially true for large organizations and for organizations developing complex products; for example automotive safety functions where mechanical, electronic and software systems need to interact. In this paper we describe defect backlog profiles as a well-defined, cheap and informative indicator. We define defect backlog profiles in terms of ISO/IEC 15939, provide a theoretical framework for interpretation, and finally present an evaluation in which we applied the indicator in a longitudinal case study at an automotive manufacturer. In the case study, we compare the software integration defect backlog profile for the active safety component released in 2010 to the profile for the following generation of the same component released in 2015. The results are then linked to a number of process and product changes that occurred between the two product generations. We conclude that defect backlog profiles are cheap in terms of data collection and analysis, and can provide valuable process and product quality information although with limitations.},
booktitle = {Proceedings of the 27th International Workshop on Software Measurement and 12th International Conference on Software Process and Product Measurement},
pages = {170–175},
numpages = {6},
keywords = {time-to-market, indicators, defect backlog profile, continuous integration, agile development process, ISO/IEC15939},
location = {Gothenburg, Sweden},
series = {IWSM Mensura '17}
}

@article{10.1145/3643781,
author = {Li, Yichen and Xiao, Dongwei and Liu, Zhibo and Pang, Qi and Wang, Shuai},
title = {Metamorphic Testing of Secure Multi-party Computation (MPC) Compilers},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643781},
doi = {10.1145/3643781},
abstract = {The demanding need to perform privacy-preserving computations among multiple 
 
 
 
 
 
 
 
data owners has led to the prosperous development of secure multi-party 
 
 
 
 
 
 
 
computation (MPC) protocols. MPC offers protocols for parties to jointly compute 
 
 
 
 
 
 
 
a function over their inputs while keeping those inputs private. To date, MPC 
 
 
 
 
 
 
 
has been widely adopted in various real-world, privacy-sensitive sectors, such 
 
 
 
 
 
 
 
as healthcare and finance. Moreover, to ease the adoption of MPC, industrial and academic 
 
 
 
 
 
 
 
MPC compilers have been developed to automatically translate 
 
 
 
 
 
 
 
programs describing arbitrary MPC procedures into low-level MPC executables. 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
Compiling high-level descriptions into high-efficiency MPC executables is 
 
 
 
 
 
 
 
challenging: the compilation often involves converting high-level languages into 
 
 
 
 
 
 
 
several intermediate representations (IR), e.g., arithmetic or boolean circuits, 
 
 
 
 
 
 
 
optimizing the computation/communication cost, and picking proper MPC protocols (and 
 
 
 
 
 
 
 
underlying virtual machines) for a particular task and threat model. Various 
 
 
 
 
 
 
 
optimizations and heuristics are employed during the compilation procedure to 
 
 
 
 
 
 
 
improve the efficiency of the generated MPC executables. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Despite the prosperous adoption of MPC compilers by industrial vendors and 
 
 
 
 
 
 
 
academia, a principled and systematic understanding of the correctness of 
 
 
 
 
 
 
 
MPC compilers does not yet exist. To fill this critical gap, this paper 
 
 
 
 
 
 
 
introduces MT-MPC, a metamorphic testing (MT) framework specifically designed for 
 
 
 
 
 
 
 
MPC compilers to effectively uncover erroneous compilations. Our approach 
 
 
 
 
 
 
 
proposes three metamorphic relations (MRs) that are tailored for MPC programs to 
 
 
 
 
 
 
 
mutate high-level MPC programs (compiler inputs). We then examine if MPC 
 
 
 
 
 
 
 
compilers yield semantics-equivalent MPC executables regarding the original and 
 
 
 
 
 
 
 
mutated MPC programs by comparing their execution results. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Real-world MPC compilers exhibit a high level of engineering quality. 
 
 
 
 
 
 
 
Nevertheless, we detected 4,772 inputs that can result in erroneous 
 
 
 
 
 
 
 
compilations in three popular MPC compilers available on the market. While the 
 
 
 
 
 
 
 
discovered error-triggering inputs do not cause the MPC compilers to crash 
 
 
 
 
 
 
 
directly, they can lead to the generation of incorrect MPC executables, 
 
 
 
 
 
 
 
jeopardizing the underlying dependability of the computation. 
 
 
 
 
 
 
 
With substantial manual effort and help from the MPC compiler developers, we 
 
 
 
 
 
 
 
uncovered thirteen bugs in these MPC compilers by debugging them using the 
 
 
 
 
 
 
 
error-triggering inputs. Our proposed testing frameworks and findings can be 
 
 
 
 
 
 
 
used to guide developers in their efforts to improve MPC compilers.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {55},
numpages = {22},
keywords = {Compiler, Metamorphic Testing, Secure Multi-party Computation}
}

@proceedings{10.1145/3632366,
title = {BDCAT '23: Proceedings of the IEEE/ACM 10th International Conference on Big Data Computing, Applications and Technologies},
year = {2023},
isbn = {9798400704734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The IEEE/ACM International Conference on Big Data Computing, Applications, and Technologies (BDCAT) is a premier annual conference series aiming to provide a platform for researchers from both academia and industry to present new discoveries in the broad area of big data computing and applications.},
location = {Taormina (Messina), Italy}
}

@inproceedings{10.1145/2020390.2020403,
author = {Duc Anh, Nguyen and Cruzes, Daniela S. and Conradi, Reidar and Ayala, Claudia},
title = {Empirical validation of human factors in predicting issue lead time in open source projects},
year = {2011},
isbn = {9781450307093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020390.2020403},
doi = {10.1145/2020390.2020403},
abstract = {[Context] Software developers often spend a significant portion of their resources resolving submitted evolution issue reports. Classification or prediction of issue lead time is useful for prioritizing evolution issues and supporting human resources allocation in software maintenance. However, the predictability of issue lead time is still a research gap that calls for more empirical investigation. [Aim] In this paper, we empirically assess different types of issue lead time prediction models using human factor measures collected from issue tracking systems. [Method] We conduct an empirical investigation of three active open source projects. A machine learning based classification and statistical univariate and multivariate analyses are performed. [Results] The accuracy of classification models in ten-fold cross-validation varies from 75.56% to 91%. The R2 value of linear multivariate regression models ranges from 0.29 to 0.60. Correlation analysis confirms the effectiveness of collaboration measures, such as the number of stakeholders and number of comments, in prediction models. The measures of assignee past performance are also an effective indicator of issue lead time. [Conclusions] The results indicate that the number of stakeholders and average past issue lead time are important variables in constructing prediction models of issue lead time. However, more variables should be explored to achieve better prediction performance.},
booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
articleno = {13},
numpages = {10},
keywords = {bug lead time, bug prediction, bug triage, classification model, empirical study, issue lead time, issue resolution time, regression model},
location = {Banff, Alberta, Canada},
series = {Promise '11}
}

@inproceedings{10.1145/2593801.2593808,
author = {Langer, Falk and Oswald, Erik},
title = {A self-learning approach for validation of communication in embedded systems},
year = {2014},
isbn = {9781450328463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593801.2593808},
doi = {10.1145/2593801.2593808},
abstract = {This paper demonstrates a new approach that addresses the problem of evaluating the communication behavior of embedded systems by applying algorithms from the area of artificial intelligence. An important problem for the validation for the interaction in the distributed system is missing, wrong or incomplete specification. This paper demonstrates the application of a new self-learning approach for assessing the communication behavior based on reference traces. The benefit of the approach is that it works automatically, with low additional effort and without using any specification. The investigated methodology uses algorithms from the field of machine learning and data mining to extract behavior models out of a reference trace. For showing the application, this paper provides a use case and the basic setup for the proposed method. The applicability of this self-learning methodology is evaluated based on real vehicle network data.},
booktitle = {Proceedings of the 3rd International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
pages = {38–44},
numpages = {7},
keywords = {testing procedures, self-learning test methods, network trace analysis, embedded system validation},
location = {Hyderabad, India},
series = {RAISE 2014}
}

@inproceedings{10.1145/3629527.3651432,
author = {Belkhiri, Adel and Ben Attia, Maroua and Gohring De Magalhaes, Felipe and Nicolescu, Gabriela},
title = {Towards Efficient Diagnosis of Performance Bottlenecks in Microservice-Based Applications (Work In Progress paper)},
year = {2024},
isbn = {9798400704451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629527.3651432},
doi = {10.1145/3629527.3651432},
abstract = {Microservices have been a cornerstone for building scalable, flexible, and robust applications, thereby enabling service providers to enhance their systems' resilience and fault tolerance. However, adopting this architecture has often led to many challenges, particularly when pinpointing performance bottlenecks and diagnosing their underlying causes. Various tools have been developed to bridge this gap and facilitate comprehensive observability in microservice ecosystems. While these tools are effective at detecting latency-related anomalies, they often fall short of isolating the root causes of these problems. In this paper, we present a novel method for identifying and analyzing performance anomalies in microservice-based applications by leveraging cross-layer tracing techniques. Our method uniquely integrates system resource metrics-such as CPU, disk, and network consumption-with each user request, providing a multi-dimensional view for diagnosing performance issues. Through the use of sequential pattern mining, this method effectively isolates aberrant execution behaviors and helps identify their root causes. Our experimental evaluations demonstrate its efficiency in diagnosing a wide range of performance anomalies.},
booktitle = {Companion of the 15th ACM/SPEC International Conference on Performance Engineering},
pages = {40–46},
numpages = {7},
keywords = {distributed systems, microservices, performance analysis, software tracing},
location = {London, United Kingdom},
series = {ICPE '24 Companion}
}

@article{10.1145/3007787.3001175,
author = {Alam, Mohammad Mejbah ul and Muzahid, Abdullah},
title = {Production-run software failure diagnosis via Adaptive Communication Tracking},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/3007787.3001175},
doi = {10.1145/3007787.3001175},
abstract = {Software failure diagnosis techniques work either by sampling some events at production-run time or by using some bug detection algorithms. Some of the techniques require the failure to be reproduced multiple times. The ones that do not require such, are not adaptive enough when the execution platform, environment or code changes. We propose ACT, a diagnosis technique for production-run failures, that uses the machine intelligence of neural hardware. ACT learns some invariants (e.g., data communication invariants) on-the-fly using the neural hardware and records any potential violation of them. Since ACT can learn invariants on-the-fly, it can adapt to any change in execution setting or code. Since it records only the potentially violated invariants, the postprocessing phase can pinpoint the root cause fairly accurately without requiring to observe the failure again. ACT works seamlessly for many sequential and concurrency bugs. The paper provides a detailed design and implementation of ACT in a typical multiprocessor system. It uses a three stage pipeline for partially configurable one hidden layer neural networks. We have evaluated ACT on a variety of programs from popular benchmarks as well as open source programs. ACT diagnoses failures caused by 16 bugs from these programs with accurate ranking. Compared to existing learning and sampling based approaches, ACT has better diagnostic ability. For the default configuration, ACT has an average execution overhead of 8.2%.},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {354–366},
numpages = {13},
keywords = {sequential bugs, neural hardware, failures, dependence, concurrency bugs}
}

@inproceedings{10.5555/2820282.2820292,
author = {Thung, Ferdian and Le, Xuan-Bach D. and Lo, David},
title = {Active semi-supervised defect categorization},
year = {2015},
publisher = {IEEE Press},
abstract = {Defects are inseparable part of software development and evolution. To better comprehend problems affecting a software system, developers often store historical defects and these defects can be categorized into families. IBM proposes Orthogonal Defect Categorization (ODC) which include various classifications of defects based on a number of orthogonal dimensions (e.g., symptoms and semantics of defects, root causes of defects, etc.). To help developers categorize defects, several approaches that employ machine learning have been proposed in the literature. Unfortunately, these approaches often require developers to manually label a large number of defect examples. In practice, manually labelling a large number of examples is both time-consuming and labor-intensive. Thus, reducing the onerous burden of manual labelling while still being able to achieve good performance is crucial towards the adoption of such approaches. To deal with this challenge, in this work, we propose an active semi-supervised defect prediction approach. It is performed by actively selecting a small subset of diverse and informative defect examples to label (i.e., active learning), and by making use of both labeled and unlabeled defect examples in the prediction model learning process (i.e., semi-supervised learning). Using this principle, our approach is able to learn a good model while minimizing the manual labeling effort.To evaluate the effectiveness of our approach, we make use of a benchmark dataset that contains 500 defects from three software systems that have been manually labelled into several families based on ODC. We investigate our approach's ability in achieving good classification performance, measured in terms of weighted precision, recall, F-measure, and AUC, when only a small number of manually labelled defect examples are available. Our experiment results show that our active semi-supervised defect categorization approach is able to achieve a weighted precision, recall, F-measure, and AUC of 0.651, 0.669, 0.623, and 0.710, respectively, when only 50 defects are manually labelled. Furthermore, it outperforms an existing active multi-class classification algorithm, proposed in the machine learning community, by a substantial margin.},
booktitle = {Proceedings of the 2015 IEEE 23rd International Conference on Program Comprehension},
pages = {60–70},
numpages = {11},
location = {Florence, Italy},
series = {ICPC '15}
}

@inproceedings{10.1145/2931037.2931038,
author = {Zhang, Jie and Wang, Ziyi and Zhang, Lingming and Hao, Dan and Zang, Lei and Cheng, Shiyang and Zhang, Lu},
title = {Predictive mutation testing},
year = {2016},
isbn = {9781450343909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2931037.2931038},
doi = {10.1145/2931037.2931038},
abstract = {Mutation testing is a powerful methodology for evaluating test suite quality. In mutation testing, a large number of mutants are generated and executed against the test suite to check the ratio of killed mutants. Therefore, mutation testing is widely believed to be a computationally expensive technique. To alleviate the efficiency concern of mutation testing, in this paper, we propose predictive mutation testing (PMT), the first approach to predicting mutation testing results without mutant execution. In particular, the proposed approach constructs a classification model based on a series of features related to mutants and tests, and uses the classification model to predict whether a mutant is killed or survived without executing it. PMT has been evaluated on 163 real-world projects under two application scenarios (i.e., cross-version and cross-project). The experimental results demonstrate that PMT improves the efficiency of mutation testing by up to 151.4X while incurring only a small accuracy loss when predicting mutant execution results, indicating a good tradeoff between efficiency and effectiveness of mutation testing.},
booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
pages = {342–353},
numpages = {12},
keywords = {software testing, mutation testing, machine learning},
location = {Saarbr\"{u}cken, Germany},
series = {ISSTA 2016}
}

@inproceedings{10.1145/2393596.2393619,
author = {Caglayan, Bora and Misirli, Ayse Tosun and Calikli, Gul and Bener, Ayse and Aytac, Turgay and Turhan, Burak},
title = {Dione: an integrated measurement and defect prediction solution},
year = {2012},
isbn = {9781450316149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393596.2393619},
doi = {10.1145/2393596.2393619},
abstract = {We present an integrated measurement and defect prediction tool: Dione. Our tool enables organizations to measure, monitor, and control product quality through learning based defect prediction. Similar existing tools either provide data collection and analytics, or work just as a prediction engine. Therefore, companies need to deal with multiple tools with incompatible interfaces in order to deploy a complete measurement and prediction solution. Dione provides a fully integrated solution where data extraction, defect prediction and reporting steps fit seamlessly. In this paper, we present the major functionality and architectural elements of Dione followed by an overview of our demonstration.},
booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
articleno = {20},
numpages = {2},
keywords = {software tool, software defect prediction, measurement},
location = {Cary, North Carolina},
series = {FSE '12}
}

@inproceedings{10.1109/ASE.2019.00055,
author = {Zaman, Tarannum Shaila and Han, Xue and Yu, Tingting},
title = {SCMiner: localizing system-level concurrency faults from large system call traces},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00055},
doi = {10.1109/ASE.2019.00055},
abstract = {Localizing concurrency faults that occur in production is hard because, (1) detailed field data, such as user input, file content and interleaving schedule, may not be available to developers to reproduce the failure; (2) it is often impractical to assume the availability of multiple failing executions to localize the faults using existing techniques; (3) it is challenging to search for buggy locations in an application given limited runtime data; and, (4) concurrency failures at the system level often involve multiple processes or event handlers (e.g., software signals), which cannot be handled by existing tools for diagnosing intra-process (thread-level) failures. To address these problems, we present SCMiner, a practical online bug diagnosis tool to help developers understand how a system-level concurrency fault happens based on the logs collected by the default system audit tools. SCMiner achieves online bug diagnosis to obviate the need for offline bug reproduction. SCMiner does not require code instrumentation on the production system or rely on the assumption of the availability of multiple failing executions. Specifically, after the system call traces are collected, SCMiner uses data mining and statistical anomaly detection techniques to identify the failure-inducing system call sequences. It then maps each abnormal sequence to specific application functions. We have conducted an empirical study on 19 real-world benchmarks. The results show that SCMiner is both effective and efficient at localizing system-level concurrency faults.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {515–526},
numpages = {12},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/3650212.3680320,
author = {Hayet, Ishrak and Scott, Adam and d'Amorim, Marcelo},
title = {Feedback-Directed Partial Execution},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680320},
doi = {10.1145/3650212.3680320},
abstract = {Partial code execution is the problem of executing code with missing definitions. The problem has gained recent traction as solutions to the problem could enable various downstream analyses. We propose feedback-directed partial execution, a technique supported by a tool, named Incompleter, that uses the error feedback from executions to enable partial code execution. Incompleter builds on the observation that errors observed during the execution of incomplete snippets often follow similar error patterns. Incompleter takes an incomplete snippet as input and applies rules (e.g., add class, add field, add file, etc.) to resolve the successive dynamic errors it encounters during execution of the snippet. Incompleter stops when the snippet successfully executes or when it reaches certain bounds. Our results indicate that Incompleter outperforms LExecutor, the state-of-the-art in partial execution. For example, considering a dataset of 4.7K incomplete StackOverflow snippets, Incompleter enables the execution of 10% more code snippets compared to LExecutor and covers 23% more statements. We also show that Incompleter’s type inference significantly improves over LExecutor’s type inference, with a 37% higher F1 score.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {781–793},
numpages = {13},
keywords = {Code completion, Debugging, Rule mining},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@article{10.1145/3652150,
author = {Zhao, Yu and Harrison, Brent and Yu, Tingting},
title = {DinoDroid: Testing Android Apps Using Deep Q-Networks},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3652150},
doi = {10.1145/3652150},
abstract = {The large demand of mobile devices creates significant concerns about the quality of mobile applications (apps). Developers need to guarantee the quality of mobile apps before it is released to the market. There have been many approaches using different strategies to test the GUI of mobile apps. However, they still need improvement due to their limited effectiveness. In this article, we propose DinoDroid, an approach based on deep Q-networks to automate testing of Android apps. DinoDroid learns a behavior model from a set of existing apps and the learned model can be used to explore and generate tests for new apps. DinoDroid is able to capture the fine-grained details of GUI events (e.g., the content of GUI widgets) and use them as features that are fed into deep neural network, which acts as the agent to guide app exploration. DinoDroid automatically adapts the learned model during the exploration without the need of any modeling strategies or pre-defined rules. We conduct experiments on 64 open-source Android apps. The results showed that DinoDroid outperforms existing Android testing tools in terms of code coverage and bug detection.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {122},
numpages = {24},
keywords = {Mobile testing, deep q-networks, reinforcement learning}
}

@inproceedings{10.1145/3377812.3390793,
author = {Gerasimou, Simos and Eniser, Hasan Ferit and Sen, Alper and Cakan, Alper},
title = {Importance-driven deep learning system testing},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3390793},
doi = {10.1145/3377812.3390793},
abstract = {Deep Learning (DL) systems are key enablers for engineering intelligent applications. Nevertheless, using DL systems in safety- and security-critical applications requires to provide testing evidence for their dependable operation. We introduce DeepImportance, a systematic testing methodology accompanied by an Importance-Driven (IDC) test adequacy criterion for DL systems. Applying IDC enables to establish a layer-wise functional understanding of the importance of DL system components and use this information to assess the semantic diversity of a test set. Our empirical evaluation on several DL systems and across multiple DL datasets demonstrates the usefulness and effectiveness of DeepImportance.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {322–323},
numpages = {2},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3643788.3648007,
author = {Meyer, Bertrand and Kananchuk, Viktoryia and Huang, Li},
title = {BUGFIX: towards a common language and framework for the Automatic Program Repair community},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648007},
doi = {10.1145/3643788.3648007},
abstract = {Techniques of Automatic Program Repair (APR) have the potential of thoroughly facilitating the task of producing quality software. After a promising start, however, progress in making APR practical has been hindered by the lack of a common framework to support the multiplicity of APR ideas and tools, and of target programming languages and environments. In this position paper we outline a general framework to enable the APR community to benefit from each other's advances, in particular through a standard language for describing bugs and their fixes. Such a common framework --- which is also applicable to work on fault seeding --- could be a tremendous benefit to researchers and developers of Interactive Development Environments (IDEs) who are working to make APR an effective part of the software developer's practical experience.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {9–13},
numpages = {5},
keywords = {automatic program repair, debugging, integrated development environments, software tools, program transformation, bug seeding, software quality},
location = {Lisbon, Portugal},
series = {APR '24}
}

@inproceedings{10.1145/3659677.3659749,
author = {Boukhlif, Mohamed and Kharmoum, Nassim and Hanine, Mohamed},
title = {LLMs for Intelligent Software Testing: A Comparative Study},
year = {2024},
isbn = {9798400709296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3659677.3659749},
doi = {10.1145/3659677.3659749},
abstract = {The need for effective and timely testing processes has become critical in the constantly changing field of software development. Large Language Models (LLMs) have demonstrated promise in automating test case creation, defect detection, and other software testing tasks through the use of the capabilities of machine/deep learning and natural language processing. This work explores the field of intelligent software testing, with a focus on the use of LLMs in this context. The purpose of this comparative study is to assess the corpus of research in the field in terms of used LLMs, how to interact with them, the use of fine-tuning, and prompt engineering, and explore the different technologies and testing types automated using LLMs. The findings of this study not only contribute to the growing body of knowledge on intelligent software testing but also guide fellow researchers and industry engineers in selecting the most suitable LLM for their specific testing needs.},
booktitle = {Proceedings of the 7th International Conference on Networking, Intelligent Systems and Security},
articleno = {42},
numpages = {8},
keywords = {Comparative Study, Large Language Models, Natural Language Processing, Software Testing, Test Case Generation},
location = {Meknes, AA, Morocco},
series = {NISS '24}
}

@inproceedings{10.1145/3597503.3608138,
author = {Gruber, Martin and Roslan, Muhammad Firhard and Parry, Owain and Scharnb\"{o}ck, Fabian and McMinn, Phil and Fraser, Gordon},
title = {Do Automatic Test Generation Tools Generate Flaky Tests?},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3608138},
doi = {10.1145/3597503.3608138},
abstract = {Non-deterministic test behavior, or flakiness, is common and dreaded among developers. Researchers have studied the issue and proposed approaches to mitigate it. However, the vast majority of previous work has only considered developer-written tests. The prevalence and nature of flaky tests produced by test generation tools remain largely unknown. We ask whether such tools also produce flaky tests and how these differ from developer-written ones. Furthermore, we evaluate mechanisms that suppress flaky test generation. We sample 6 356 projects written in Java or Python. For each project, we generate tests using EvoSuite (Java) and Pynguin (Python), and execute each test 200 times, looking for inconsistent outcomes. Our results show that flakiness is at least as common in generated tests as in developer-written tests. Nevertheless, existing flakiness suppression mechanisms implemented in EvoSuite are effective in alleviating this issue (71.7 % fewer flaky tests). Compared to developer-written flaky tests, the causes of generated flaky tests are distributed differently. Their non-deterministic behavior is more frequently caused by randomness, rather than by networking and concurrency. Using flakiness suppression, the remaining flaky tests differ significantly from any flakiness previously reported, where most are attributable to runtime optimizations and EvoSuite-internal resource thresholds. These insights, with the accompanying dataset, can help maintainers to improve test generation tools, give recommendations for developers using these tools, and serve as a foundation for future research in test flakiness or test generation.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {47},
numpages = {12},
keywords = {test generation, flaky tests, empirical study},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3022227.3022314,
author = {Hsu, Kuo-Wei},
title = {Integrating adaptive boosting and support vector machines with varying kernels},
year = {2017},
isbn = {9781450348881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3022227.3022314},
doi = {10.1145/3022227.3022314},
abstract = {Adaptive Boosting, or AdaBoost , is a meta-learning algorithm that employs a classification algorithm as a base learner to train classification models and uses these models to perform collective classification. One of its main features is that iteratively it forces the base learner to work more on difficult samples. Usually it can achieve better overall classification performance, when compared to a single classification model trained by the classification algorithm used as the base learner. SVM, short for Support Vector Machine, is a learning algorithm that employs a kernel to project the original data space to a data space where a hyperplane that can linearly separate as many samples of classes as possible can be found. Because both are top algorithms, researchers have been exploring the use of AdaBoost with SVM. Unlike others simply using SVM with a single kernel as the base learner in AdaBoost, we propose an approach that uses SVM with multiple kernels as the base learners in a variant of AdaBoost. Its main feature is that it not only considers difficulties of samples but also classification performance of kernels, and accordingly it selects as well as switches between kernels in the boosting process. The experiment results show that we can obtain better classification performance by using the proposed approach.},
booktitle = {Proceedings of the 11th International Conference on Ubiquitous Information Management and Communication},
articleno = {88},
numpages = {8},
keywords = {multi-class, classification, SVM, AdaBoost},
location = {Beppu, Japan},
series = {IMCOM '17}
}

@inproceedings{10.1145/3663529.3663806,
author = {Samhi, Jordan and Zeller, Andreas},
title = {AndroLog: Android Instrumentation and Code Coverage Analysis},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663806},
doi = {10.1145/3663529.3663806},
abstract = {Dynamic analysis has emerged as a pivotal technique for testing Android apps, enabling the detection of bugs, malicious code, and vulnerabilities. A key metric in evaluating the efficacy of tools employed by both research and practitioner communities for this purpose is code coverage. Obtaining code coverage typically requires planting probes within apps to gather coverage data during runtime. Due to the general unavailability of source code to analysts, there is a necessity for instrumenting apps to insert these probes in black-box environments. However, the tools available for such instrumentation are limited in their reliability and require intrusive changes interfering with apps’ functionalities.
 
 
 
This paper introduces AndroLog, a novel tool developed on top of the Soot framework, designed to provide fine-grained coverage information at multiple levels, including class, methods, statements, and Android components. In contrast to existing tools, AndroLog leaves the responsibility to test apps to analysts, and its motto is simplicity. As demonstrated in this paper, AndroLog can instrument up to 98% of recent Android apps compared to existing tools with 79% and 48% respectively for COSMO and ACVTool. AndroLog also stands out for its potential for future enhancements to increase granularity on demand. We make AndroLog available to the community and provide a video demonstration of AndroLog.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {597–601},
numpages = {5},
keywords = {Android Instrumentation, Code Coverage, Dynamic Analysis},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/3691620.3695520,
author = {Tang, Shuncheng and Zhang, Zhenya and Zhou, Jixiang and Lei, Lei and Zhou, Yuan and Xue, Yinxing},
title = {LeGEND: A Top-Down Approach to Scenario Generation of Autonomous Driving Systems Assisted by Large Language Models},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695520},
doi = {10.1145/3691620.3695520},
abstract = {Autonomous driving systems (ADS) are safety-critical and require comprehensive testing before their deployment on public roads. While existing testing approaches primarily aim at the criticality of scenarios, they often overlook the diversity of the generated scenarios that is also important to reflect system defects in different aspects. To bridge the gap, we propose LeGEND, that features a top-down fashion of scenario generation: it starts with abstract functional scenarios, and then steps downwards to logical and concrete scenarios, such that scenario diversity can be controlled at the functional level. However, unlike logical scenarios that can be formally described, functional scenarios are often documented in natural languages (e.g., accident reports) and thus cannot be precisely parsed and processed by computers. To tackle that issue, LeGEND leverages the recent advances of large language models (LLMs) to transform textual functional scenarios to formal logical scenarios. To mitigate the distraction of useless information in functional scenario description, we devise a two-phase transformation that features the use of an intermediate language; consequently, we adopt two LLMs in LeGEND, one for extracting information from functional scenarios, the other for converting the extracted information to formal logical scenarios. We experimentally evaluate LeGEND on Apollo, an industry-grade ADS from Baidu. Evaluation results show that LeGEND can effectively identify critical scenarios, and compared to baseline approaches, LeGEND exhibits evident superiority in diversity of generated scenarios. Moreover, we also demonstrate the advantages of our two-phase transformation framework, and the accuracy of the adopted LLMs.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1497–1508},
numpages = {12},
keywords = {autonomous driving systems, critical scenario generation, large language models},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3639477.3639726,
author = {Sun, Gengyi and Meidani, Mehran and Habchi, Sarra and Nayrolles, Mathieu and Mcintosh, Shane},
title = {Code Impact Beyond Disciplinary Boundaries: Constructing a Multidisciplinary Dependency Graph and Analyzing Cross-Boundary Impact},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639726},
doi = {10.1145/3639477.3639726},
abstract = {To produce a video game, engineers and artists must iterate on the same project simultaneously. In such projects, a change to the work products of any of the teams can impact the work of other teams. As a result, any analytics tasks should consider intra- and inter-dependencies within and between artifacts produced by different teams. For instance, the focus of quality assurance teams on changes that are local to a team differs from one that impacts others. To extract and analyze such cross-disciplinary dependencies, we propose the multidisciplinary dependency graph. We instantiate our idea by developing tools that extract dependencies and construct the graph at Ubisoft---a multinational video game organization with more than 18,000 employees.Our analysis of a recently launched video game project reveals that code files only make up 2.8% of the dependency graph, and code-to-code dependencies only make up 4.3% of all dependencies. We also observe that 44% of the studied source code changes impact the artifacts that are developed by other teams, highlighting the importance of analyzing inter-artifact dependencies. A comparative analysis of cross-boundary changes with changes that do not cross boundaries indicates that cross-boundary changes are: (1) impacting a median of 120,368 files; (2) with a 51% probability of causing build failures; and (3) a 67% likelihood of introducing defects. All three measurements are larger than changes that do not cross boundaries to statistically significant degrees.We also find that cross-boundary changes are: (4) more commonly associated with gameplay functionality and feature additions that directly impact the game experience than changes that do not cross boundaries, and (5) disproportionately produced by the same team (74% of the contributors are associated with that team).},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {122–133},
numpages = {12},
keywords = {interdisciplinary dependencies, build systems, impact analysis},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@article{10.1145/3476105,
author = {Parry, Owain and Kapfhammer, Gregory M. and Hilton, Michael and McMinn, Phil},
title = {A Survey of Flaky Tests},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3476105},
doi = {10.1145/3476105},
abstract = {Tests that fail inconsistently, without changes to the code under test, are described as flaky. Flaky tests do not give a clear indication of the presence of software bugs and thus limit the reliability of the test suites that contain them. A recent survey of software developers found that 59% claimed to deal with flaky tests on a monthly, weekly, or daily basis. As well as being detrimental to developers, flaky tests have also been shown to limit the applicability of useful techniques in software testing research. In general, one can think of flaky tests as being a threat to the validity of any methodology that assumes the outcome of a test only depends on the source code it covers. In this article, we systematically survey the body of literature relevant to flaky test research, amounting to 76 papers. We split our analysis into four parts: addressing the causes of flaky tests, their costs and consequences, detection strategies, and approaches for their mitigation and repair. Our findings and their implications have consequences for how the software-testing community deals with test flakiness, pertinent to practitioners and of interest to those wanting to familiarize themselves with the research area.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = oct,
articleno = {17},
numpages = {74},
keywords = {software testing, Flaky tests}
}

@inproceedings{10.1145/3597503.3639181,
author = {Wang, Zhaohui and Zhang, Min and Yang, Jingran and Shao, Bojie and Zhang, Min},
title = {MAFT: Efficient Model-Agnostic Fairness Testing for Deep Neural Networks via Zero-Order Gradient Search},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639181},
doi = {10.1145/3597503.3639181},
abstract = {Deep neural networks (DNNs) have shown powerful performance in various applications and are increasingly being used in decisionmaking systems. However, concerns about fairness in DNNs always persist. Some efficient white-box fairness testing methods about individual fairness have been proposed. Nevertheless, the development of black-box methods has stagnated, and the performance of existing methods is far behind that of white-box methods. In this paper, we propose a novel black-box individual fairness testing method called Model-Agnostic Fairness Testing (MAFT). By leveraging MAFT, practitioners can effectively identify and address discrimination in DL models, regardless of the specific algorithm or architecture employed. Our approach adopts lightweight procedures such as gradient estimation and attribute perturbation rather than non-trivial procedures like symbol execution, rendering it significantly more scalable and applicable than existing methods. We demonstrate that MAFT achieves the same effectiveness as state-of-the-art white-box methods whilst improving the applicability to large-scale networks. Compared to existing black-box approaches, our approach demonstrates distinguished performance in discovering fairness violations w.r.t effectiveness (~ 14.69\texttimes{}) and efficiency (~ 32.58\texttimes{}).},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {121},
numpages = {12},
keywords = {software bias, fairness testing, test case generation, deep neural network},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@proceedings{10.1145/3650212,
title = {ISSTA 2024: Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 33rd edition of the International Symposium on Software Testing and Analysis, ISSTA 2024, held on September 16--20, 2024 in Vienna, Austria. ISSTA 2024 is co-located with ECOOP and MPLR 2024. ISSTA brings together academics, industrial researchers, and practitioners from all over the world working on testing and analyzing software systems.},
location = {Vienna, Austria}
}

@inproceedings{10.1145/3610579.3611087,
author = {Qin, Xin and Arechiga, Nikos and Deshmukh, Jyotirmoy and Best, Andrew},
title = {Robust Testing for Cyber-Physical Systems using Reinforcement Learning},
year = {2023},
isbn = {9798400703188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610579.3611087},
doi = {10.1145/3610579.3611087},
abstract = {In this paper, we propose a testing framework for cyber-physical systems (CPS) that operate in uncertain environments. Testing such CPS applications requires carefully defining the environment to include all possible realistic operating scenarios that the CPS may encounter. Simultaneously, the process of testing hopes to identify operating scenarios in which the system-under-test (SUT) violates its specifications. We present a novel approach of testing based on the use of deep reinforcement learning for robust testing of a given SUT. In a robust testing framework, the test generation tool can provide meaningful and challenging tests even when there are small changes to the SUT. Such a method can be quite valuable in incremental design methods where small changes to the design does not necessitate expensive test generation from scratch. We demonstrate the efficacy of our method on three example systems in autonomous driving implemented within a photo-realistic autonomous driving simulator.},
booktitle = {Proceedings of the 21st ACM-IEEE International Conference on Formal Methods and Models for System Design},
pages = {36–46},
numpages = {11},
location = {Hamburg, Germany},
series = {MEMOCODE '23}
}

@inproceedings{10.1145/3524842.3528438,
author = {Taesiri, Mohammad Reza and Macklon, Finlay and Bezemer, Cor-Paul},
title = {CLIP meets GamePhysics: towards bug identification in gameplay videos using zero-shot transfer learning},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528438},
doi = {10.1145/3524842.3528438},
abstract = {Gameplay videos contain rich information about how players interact with the game and how the game responds. Sharing gameplay videos on social media platforms, such as Reddit, has become a common practice for many players. Often, players will share game-play videos that showcase video game bugs. Such gameplay videos are software artifacts that can be utilized for game testing, as they provide insight for bug analysis. Although large repositories of gameplay videos exist, parsing and mining them in an effective and structured fashion has still remained a big challenge. In this paper, we propose a search method that accepts any English text query as input to retrieve relevant videos from large repositories of gameplay videos. Our approach does not rely on any external information (such as video metadata); it works solely based on the content of the video. By leveraging the zero-shot transfer capabilities of the Contrastive Language-Image Pre-Training (CLIP) model, our approach does not require any data labeling or training. To evaluate our approach, we present the GamePhysics dataset consisting of 26,954 videos from 1,873 games, that were collected from the GamePhysics section on the Reddit website. Our approach shows promising results in our extensive analysis of simple queries, compound queries, and bug queries, indicating that our approach is useful for object and event detection in gameplay videos. An example application of our approach is as a gameplay video search engine to aid in reproducing video game bugs. Please visit the following link for the code and the data: https://asgaardlab.github.io/CLIPxGamePhysics/},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {270–281},
numpages = {12},
keywords = {video mining, video games, software testing, bug reports},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1145/3709026.3709057,
author = {Jiang, Chao and He, Dan},
title = {Hard disk drive failure prediction model based on Temporal Convolutional Network combined with Auto-Encoder},
year = {2025},
isbn = {9798400718182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3709026.3709057},
doi = {10.1145/3709026.3709057},
abstract = {Hard disk drive failures pose a significant risk to the normal operation of data centers, making proactive failure prediction crucial for timely issue identification and minimizing potential losses. To effectively predict disk failures, a hard disk drive failure prediction model named AE-TCN is proposed, which combines Auto-Encoder (AE) with Temporal Convolutional Network(TCN) to achieve outstanding predictive performance. First, an improved feature normalization method is employed to select appropriate input features. Then, the Auto-Encoder is used to reduce noise in the input data, while the Temporal Convolutional Network leverages its long-term memory capability for failure prediction. Experimental results demonstrate that, compared to using TCN alone, AE-TCN improves the average Failure Detection Rate (FDR) by 3.31% and the average F1-Score by 2.55% across different time input windows. Moreover, it exhibits better predictive capability compared to other state-of-the-art models.},
booktitle = {Proceedings of the 2024 8th International Conference on Computer Science and Artificial Intelligence},
pages = {513–518},
numpages = {6},
keywords = {Autoencoder, Hard disk drives, Key words: Failure prediction, Time Convolutional Network},
location = {
},
series = {CSAI '24}
}

@inproceedings{10.1145/3468264.3468610,
author = {Zhang, Qian and Wang, Jiyuan and Kim, Miryung},
title = {HeteroFuzz: fuzz testing to detect platform dependent divergence for heterogeneous applications},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468610},
doi = {10.1145/3468264.3468610},
abstract = {As specialized hardware accelerators like FPGAs become a prominent part of the current computing landscape, software applications are increasingly constructed to leverage heterogeneous architectures. Such a trend is already happening in the domain of machine learning and Internet-of-Things (IoT) systems built on edge devices. Yet, debugging and testing methods for heterogeneous applications are currently lacking. These applications may look similar to regular C/C++ code but include hardware synthesis details in terms of preprocessor directives. Therefore, their behavior under heterogeneous architectures may diverge significantly from CPU due to hardware synthesis details. Further, the compilation and hardware simulation cycle takes an enormous amount of time, prohibiting frequent invocations required for fuzz testing.  We propose a novel fuzz testing technique, called HeteroFuzz, designed to specifically target heterogeneous applications and to detect platform-dependent divergence. The key essence of HeteroFuzz is that it uses a three-pronged approach to reduce the long latency of repetitively invoking a hardware simulator on a heterogeneous application. First, in addition to monitoring code coverage as a fuzzing guidance mechanism, we analyze synthesis pragmas in kernel code and monitor accelerator-relevant value spectra. Second, we design dynamic probabilistic mutations to increase the chance of hitting divergent behavior under different platforms. Third, we memorize the boundaries of seen kernel inputs and skip HLS simulator invocation if it can expose only redundant divergent behavior. We evaluate HeteroFuzz on seven real-world heterogeneous applications with FPGA kernels. HeteroFuzz is 754X faster in exposing the same set of distinct divergence symptoms than naive fuzzing. Probabilistic mutations contribute to 17.5X speed up than the one without. Selective invocation of HLS simulation contributes to 8.8X speed up than the one without.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {242–254},
numpages = {13},
keywords = {platform-dependent divergence, heterogeneous applications, Fuzz testing},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3650212.3680383,
author = {Guan, Hao and Bai, Guangdong and Liu, Yepang},
title = {Large Language Models Can Connect the Dots: Exploring Model Optimization Bugs with Domain Knowledge-Aware Prompts},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680383},
doi = {10.1145/3650212.3680383},
abstract = {Model optimization, such as pruning and quantization, has become the de facto pre-deployment phase when deploying deep learning&nbsp;(DL) models on resource-constrained platforms.         However, the complexity of DL models often leads to non-trivial bugs in model optimizers, known as model optimization bugs&nbsp;(MOBs).         These MOBs are characterized by involving complex data types and layer structures inherent to DL models, causing significant hurdles in detecting them through traditional static analysis and dynamic testing techniques.        In this work, we leverage Large Language Models (LLMs) with prompting techniques to generate test cases for MOB detection.        We explore how LLMs can draw an understanding of the MOB domain from scattered bug instances and generalize to detect new ones, a paradigm we term as concentration and diffusion.        We extract MOB domain knowledge from the artifacts of known MOBs, such as their issue reports and fixes, and design knowledge-aware prompts to guide LLMs in generating effective test cases.         The domain knowledge of code structure and error description provides precise in-depth depictions of the problem domain, i.e., the concentration, and heuristic directions to generate innovative test cases, i.e., the diffusion.         Our approach is implemented as a tool named YanHui and benchmarked against existing few-shot LLM-based fuzzing techniques.         Test cases generated by YanHui demonstrate enhanced capability to find relevant API and data combinations for exposing MOBs, leading to an 11.4% increase in generating syntactically valid code and a 22.3% increase in generating on-target code specific to model optimization.         YanHui detects 17 MOBs, and among them, five are deep MOBs that are difficult to reveal without our prompting technique.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1579–1591},
numpages = {13},
keywords = {Large Language Model, Library Testing, Model Optimization},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/1982185.1982498,
author = {Debroy, Vidroha and Wong, W. Eric},
title = {On the equivalence of certain fault localization techniques},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982498},
doi = {10.1145/1982185.1982498},
abstract = {Software fault localization is an expensive component of program debugging, and thus, many different types of fault localization techniques have been proposed over the recent years. Such techniques aim to rank program components (such as statements, blocks, functions, etc.) in decreasing order of their likelihood of being faulty, such that programmers may then examine the ranking starting from the top, until a fault is found. However, comparisons between fault localization techniques (to see which one is more effective) have generally been based on case studies and empirical data. In this paper we propose an equivalence relation by virtue of which two or more fault localization techniques may be considered equivalent if they produce identical rankings of program components, and are therefore, equally as effective. We then make use of the proposed equivalence relation to prove that several similarity coefficient-based fault localization techniques are in fact equivalent to one another. Furthermore, no case studies and/or data were required for any of the proofs of equivalency provided in this paper.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {1457–1463},
numpages = {7},
keywords = {statement suspiciousness, software fault Localization, similarity coefficients, ranking of statements, order-preserving operations, equivalence relation},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@inproceedings{10.1145/3395363.3397369,
author = {Lutellier, Thibaud and Pham, Hung Viet and Pang, Lawrence and Li, Yitong and Wei, Moshi and Tan, Lin},
title = {CoCoNuT: combining context-aware neural translation models using ensemble for program repair},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397369},
doi = {10.1145/3395363.3397369},
abstract = {Automated generate-and-validate (GV) program repair techniques (APR) typically rely on hard-coded rules, thus only fixing bugs following specific fix patterns. These rules require a significant amount of manual effort to discover and it is hard to adapt these rules to different programming languages. To address these challenges, we propose a new G&amp;V technique—CoCoNuT, which uses ensemble learning on the combination of convolutional neural networks (CNNs) and a new context-aware neural machine translation (NMT) architecture to automatically fix bugs in multiple programming languages. To better represent the context of a bug, we introduce a new context-aware NMT architecture that represents the buggy source code and its surrounding context separately. CoCoNuT uses CNNs instead of recurrent neural networks (RNNs), since CNN layers can be stacked to extract hierarchical features and better model source code at different granularity levels (e.g., statements and functions). In addition, CoCoNuT takes advantage of the randomness in hyperparameter tuning to build multiple models that fix different bugs and combines these models using ensemble learning to fix more bugs. Our evaluation on six popular benchmarks for four programming languages (Java, C, Python, and JavaScript) shows that CoCoNuT correctly fixes (i.e., the first generated patch is semantically equivalent to the developer’s patch) 509 bugs, including 309 bugs that are fixed by none of the 27 techniques with which we compare.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {101–114},
numpages = {14},
keywords = {Neural Machine Translation, Deep Learning, Automated program repair, AI and Software Engineering},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@proceedings{10.1145/3624032,
title = {SAST '23: Proceedings of the 8th Brazilian Symposium on Systematic and Automated Software Testing},
year = {2023},
isbn = {9798400716294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Campo Grande, MS, Brazil}
}

@inproceedings{10.1145/2639490.2639504,
author = {Russo, Barbara},
title = {A proposed method to evaluate and compare fault predictions across studies},
year = {2014},
isbn = {9781450328982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2639490.2639504},
doi = {10.1145/2639490.2639504},
abstract = {Studies on fault prediction often pay little attention to empirical rigor and presentation. Researchers might not have full command over the statistical method they use, full understanding of the data they have, or tend not to report key details about their work. What does it happen when we want to compare such studies for building a theory on fault prediction? There are two issues that if not addressed, we believe, prevent building such theory. The first concerns how to compare and report prediction performance across studies on different data sets. The second regards fitting performance of prediction models. Studies tend not to control and report the performance of predictors on historical data underestimating the risk that good predictors may poorly perform on past data. The degree of both fitting and prediction performance determines the risk managers are requested to take when they use such predictors. In this work, we propose a framework to compare studies on categorical fault prediction that aims at addressing the two issues. We propose three algorithms that automate our framework. We finally review baseline studies on fault prediction to discuss the application of the framework.},
booktitle = {Proceedings of the 10th International Conference on Predictive Models in Software Engineering},
pages = {2–11},
numpages = {10},
keywords = {confusion matrix, fault, machine learning, model comparison},
location = {Turin, Italy},
series = {PROMISE '14}
}

@inproceedings{10.1145/1985793.1986028,
author = {Kidwell, Billy},
title = {A decision support system for the classification of software coding faults: a research abstract},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1986028},
doi = {10.1145/1985793.1986028},
abstract = {A decision support system for fault classification is presented. The fault classification scheme is developed to provide guidance in process improvement and fault-based testing. The research integrates results in fault classification, source code analysis, and fault-based testing research. Initial results indicate that existing change type and fault classification schemes are insufficient for this purpose. Development of sufficient schemes and their evaluation are discussed.},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {1158–1160},
numpages = {3},
keywords = {software fault taxonomy, software evolution, object-oriented software, fault-based testing, fault model, decision support system},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@inproceedings{10.1109/ICSE43902.2021.00140,
author = {Alshammari, Abdulrahman and Morris, Christopher and Hilton, Michael and Bell, Jonathan},
title = {FlakeFlagger: Predicting Flakiness Without Rerunning Tests},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00140},
doi = {10.1109/ICSE43902.2021.00140},
abstract = {When developers make changes to their code, they typically run regression tests to detect if their recent changes (re)introduce any bugs. However, many tests are flaky, and their outcomes can change non-deterministically, failing without apparent cause. Flaky tests are a significant nuisance in the development process, since they make it more difficult for developers to trust the outcome of their tests, and hence, it is important to know which tests are flaky. The traditional approach to identify flaky tests is to rerun them multiple times: if a test is observed both passing and failing on the same code, it is definitely flaky. We conducted a very large empirical study looking for flaky tests by rerunning the test suites of 24 projects 10,000 times each, and found that even with this many reruns, some previously identified flaky tests were still not detected. We propose FlakeFlagger, a novel approach that collects a set of features describing the behavior of each test, and then predicts tests that are likely to be flaky based on similar behavioral features. We found that FlakeFlagger correctly labeled as flaky at least as many tests as a state-of-the-art flaky test classifier, but that FlakeFlagger reported far fewer false positives. This lower false positive rate translates directly to saved time for researchers and developers who use the classification result to guide more expensive flaky test detection processes. Evaluated on our dataset of 23 projects with flaky tests, FlakeFlagger outperformed the prior approach (by F1 score) on 16 projects and tied on 4 projects. Our results indicate that this approach can be effective for identifying likely flaky tests prior to running time-consuming flaky test detectors.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1572–1584},
numpages = {13},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1145/3582573,
author = {Chen, Jialuo and Wang, Jingyi and Ma, Xingjun and Sun, Youcheng and Sun, Jun and Zhang, Peixin and Cheng, Peng},
title = {QuoTe: Quality-oriented Testing for Deep Learning Systems},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3582573},
doi = {10.1145/3582573},
abstract = {Recently, there has been significant growth of interest in applying software engineering techniques for the quality assurance of deep learning (DL) systems. One popular direction is DL testing—that is, given a property of test, defects of DL systems are found either by fuzzing or guided search with the help of certain testing metrics. However, recent studies have revealed that the neuron coverage metrics, which are commonly used by most existing DL testing approaches, are not necessarily correlated with model quality (e.g., robustness, the most studied model property), and are also not an effective measurement on the confidence of the model quality after testing. In this work, we address this gap by proposing a novel testing framework called QuoTe (i.e., Quality-oriented Testing). A key part of QuoTe is a quantitative measurement on (1) the value of each test case in enhancing the model property of interest (often via retraining) and (2) the convergence quality of the model property improvement. QuoTe utilizes the proposed metric to automatically select or generate valuable test cases for improving model quality. The proposed metric is also a lightweight yet strong indicator of how well the improvement converged. Extensive experiments on both image and tabular datasets with a variety of model architectures confirm the effectiveness and efficiency of QuoTe in improving DL model quality—that is, robustness and fairness. As a generic quality-oriented testing framework, future adaptations can be made to other domains (e.g., text) as well as other model properties.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {125},
numpages = {33},
keywords = {fairness, robustness, testing, Deep learning}
}

@inproceedings{10.1145/3663529.3663787,
author = {Houdaille, Phil\'{e}mon and Khelladi, Djamel Eddine and Combemale, Benoit and Mussbacher, Gunter},
title = {On Polyglot Program Testing},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663787},
doi = {10.1145/3663529.3663787},
abstract = {In modern applications, it has become increasingly necessary to use multiple languages in a coordinated way to deal with the complexity and diversity of concerns encountered during development. This practice is known as polyglot programming. However, while execution platforms for polyglot programs are increasingly mature, there is a lack of support in how to test polyglot programs. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
This paper is a first step to increase awareness about polyglot testing efforts. It provides an overview of how polyglot programs are constructed, and an analysis of the impact on test writing at its different steps. More specifically, we focus on dynamic white box testing, and how polyglot programming impacts selection of input data, scenario specification and execution, and oracle expression.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
We discuss the related challenges in particular with regards to the current state of the practice. We envision in this paper to raise interest in polyglot program testing within the software engineering community, and help in defining directions for future work.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {507–511},
numpages = {5},
keywords = {polyglot programming, white box testing},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@article{10.1145/3643747,
author = {Nolasco, Agust\'{\i}n and Molina, Facundo and Degiovanni, Renzo and Gorla, Alessandra and Garbervetsky, Diego and Papadakis, Mike and Uchitel, Sebastian and Aguirre, Nazareno and Frias, Marcelo F.},
title = {Abstraction-Aware Inference of Metamorphic Relations},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643747},
doi = {10.1145/3643747},
abstract = {Metamorphic testing is a valuable technique that helps in dealing with the oracle problem. It involves testing software against specifications of its intended behavior given in terms of so called metamorphic relations, statements that express properties relating different software elements (e.g., different inputs, methods, etc). The effective application of metamorphic testing strongly depends on identifying suitable domain-specific metamorphic relations, a challenging task, that is typically manually performed.     This paper introduces MemoRIA, a novel approach that aims at automatically identifying metamorphic relations. The technique focuses on a particular kind of metamorphic relation, which asserts equivalences between methods and method sequences. MemoRIA works by first generating an object-protocol abstraction of the software being tested, then using fuzzing to produce candidate relations from the abstraction, and finally validating the candidate relations through run-time analysis. A SAT-based analysis is used to eliminate redundant relations, resulting in a concise set of metamorphic relations for the software under test. We evaluate our technique on a benchmark consisting of 22 Java subjects taken from the literature, and compare MemoRIA with the metamorphic relation inference technique SBES. Our results show that by incorporating the object protocol abstraction information, MemoRIA is able to more effectively infer meaningful metamorphic relations, that are also more precise, compared to SBES, measured in terms of mutation analysis. Also, the SAT-based reduction allows us to significantly reduce the number of reported metamorphic relations, while in general having a small impact in the bug finding ability of the corresponding obtained relations.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {21},
numpages = {23},
keywords = {Metamorphic testing, grammar-based fuzzing, oracle problem}
}

@proceedings{10.1145/3652588,
title = {SOAP 2024: Proceedings of the 13th ACM SIGPLAN International Workshop on the State Of the Art in Program Analysis},
year = {2024},
isbn = {9798400706219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 13th ACM SIGPLAN International Workshop on the State Of the Art in Pro-
 
gram Analysis (SOAP’24) is co-located with the 45th ACM SIGPLAN International
 
Conference on Programming Language Design and Implementation (PLDI’24). In
 
line with past workshops, SOAP’24 aims to bring together members of the program
 
analysis community to share new developments and shape innovations in program
 
analysis.},
location = {Copenhagen, Denmark}
}

@inproceedings{10.1145/3551349.3559519,
author = {Ghanbari, Ali and Marcus, Andrian (Andi)},
title = {Shibboleth: Hybrid Patch Correctness Assessment in Automated Program Repair},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3559519},
doi = {10.1145/3551349.3559519},
abstract = {Test-based generate-and-validate automated program repair (APR) systems generate many patches that pass the test suite without fixing the bug. The generated patches must be manually inspected by the developers, a task that tends to be time-consuming, thereby diminishing the role of APR in reducing debugging costs. We present the design and implementation of a novel tool, named Shibboleth, for automatic assessment of the patches generated by test-based generate-and-validate APR systems. Shibboleth leverages lightweight static and dynamic heuristics from both test and production code to rank and classify the patches. Shibboleth is based on the idea that the buggy program is almost correct and the bugs are small mistakes that require small changes to fix and specifically the fix does not remove the code implementing correct functionality of the program. Thus, the tool measures the impact of patches on both production code (via syntactic and semantic similarity) and test code (via code coverage) to separate the patches that result in similar programs and that do not remove desired program elements. We have evaluated Shibboleth on 1,871 patches, generated by 29 Java-based APR systems for Defects4J programs. The technique outperforms state-of-the-art raking and classification techniques. Specifically, in our ranking data set, in 66% of the cases, Shibboleth ranks the correct patch in top-1 or top-2 positions and, in our classification data set, it achieves an accuracy and F1-score of 0.887 and 0.852, respectively, in classification mode. A demo video of the tool is available at https://bit.ly/3NvYJN8.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {166},
numpages = {4},
keywords = {Similarity, Patch Correctness Assessment, Branch Coverage, Automated Program Repair},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/2222444.2222446,
author = {Abuasad, Arwa and Alsmadi, Izzat M.},
title = {The correlation between source code analysis change recommendations and software metrics},
year = {2012},
isbn = {9781450313278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2222444.2222446},
doi = {10.1145/2222444.2222446},
abstract = {Developing a software with high quality is a challenge that is continuously evolving. There are several methods to evaluate the quality in developed software products. In this paper, we evaluated the correlation between two software quality tools: Source Code Analysis (SCA) and software metrics. While both approaches are used to assess one or more attributes of the software, each one has its own goals and characteristics. In this paper, we evaluated the relation and correlation between SCA and software metrics. We used tools to fix warning detected by SCA tools. Software metrics are measured before and after the modifications of SCA recommended changes. Results showed that some specific metrics related to structure, complexity and maintainability metrics can be significantly impacted by SCA modifications. This is expected given the nature of proposed modifications from SCA tools.},
booktitle = {Proceedings of the 3rd International Conference on Information and Communication Systems},
articleno = {2},
numpages = {5},
keywords = {source code analysis, software mining, software faults, object oriented designs, design metrics, data mining, coupling and complexity metrics},
location = {Irbid, Jordan},
series = {ICICS '12}
}

@inproceedings{10.1145/3324884.3418913,
author = {Xu, Xiangzhe},
title = {The classification and propagation of program comments},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3418913},
doi = {10.1145/3324884.3418913},
abstract = {Natural language comments are like bridges between human logic and software semantics. Developers use comments to describe the function, implementation, and property of code snippets. This kind of connections contains rich information, like the potential types of a variable and the pre-condition of a method, among other things. In this paper, we categorize comments and use natural language processing techniques to extract information from them. Based on the semantics of programming languages, different rules are built for each comment category to systematically propagate comments among code entities. Then we use the propagated comments to check the code usage and comments consistency. Our demo system finds 37 bugs in real-world projects, 30 of which have been confirmed by the developers. Except for bugs in the code, we also find 304 pieces of defected comments. The 12 of them are misleading and 292 of them are not correct. Moreover, among the 41573 pieces of comments we propagate, 87 comments are for private native methods which had neither code nor comments. We also conduct a user study where we find that propagated comments are as good as human-written comments in three dimensions of consistency, naturalness, and meaningfulness.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1394–1396},
numpages = {3},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3275219.3275239,
author = {Qin, Hanmin and Sun, Xin},
title = {Classifying Bug Reports into Bugs and Non-bugs Using LSTM},
year = {2018},
isbn = {9781450365901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3275219.3275239},
doi = {10.1145/3275219.3275239},
abstract = {Studies have found that significant amount of bug reports are misclassified between bugs and non-bugs, which inevitably affects relevant studies, e.g., bug prediction. Manually classifying bug reports helps reduce the noise but is often time-consuming. To ease the problem, we propose a bug classification method based on Long Short-Term Memory (LSTM), a typical recurrent neural network which is widely used in text classification tasks. Our method outperforms existing topic-based method and n-gram IDF-based method on four datasets from three popular JAVA open source projects. We believe our work can assist developers and researches to classify bug reports and identify misclassified bug reports. Datasets and scripts used in this work are provided on GitHub1 for others to reproduce and further improve our study.},
booktitle = {Proceedings of the 10th Asia-Pacific Symposium on Internetware},
articleno = {20},
numpages = {4},
keywords = {bug classification, LSTM},
location = {Beijing, China},
series = {Internetware '18}
}

@inproceedings{10.1145/3551349.3559558,
author = {Vu, Anh Duc and Kehrer, Timo and Tsigkanos, Christos},
title = {Outcome-Preserving Input Reduction for Scientific Data Analysis Workflows},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3559558},
doi = {10.1145/3551349.3559558},
abstract = {Analysis of data is the foundation of multiple scientific disciplines, manifesting in complex and diverse scientific data analysis workflows often involving exploratory analyses. Such analyses represent a particular case for traditional data engineering workflows, as results may be hard to interpret and judge whether they are correct or not, and where experimentation is a central theme. Oftentimes, there are certain aspects of a result which are suspicious and which should be further investigated to increase the trustworthiness of the workflow’s outcome. To this end, we advocate a semi-automated approach to reducing a workflow’s input data while preserving a specified outcome of interest, facilitating irregularity localization by narrowing down the search space for spotting corrupted input data or wrong assumptions made about it. We outline our vision on building engineering support for outcome-preserving input reduction within data analysis workflows, and report on preliminary results obtained from applying an early research prototype on a computational notebook taken from an online community of data scientists and machine learning practitioners.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {182},
numpages = {5},
keywords = {notebooks, data science, data analysis workflows, causality, automated debugging},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3540250.3558967,
author = {Kim, Misoo and Kim, Youngkyoung and Jeong, Hohyeon and Heo, Jinseok and Kim, Sungoh and Chung, Hyunhee and Lee, Eunseok},
title = {An empirical study of deep transfer learning-based program repair for Kotlin projects},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558967},
doi = {10.1145/3540250.3558967},
abstract = {Deep learning-based automated program repair (DL-APR) can automatically fix software bugs and has received significant attention in the industry because of its potential to significantly reduce software development and maintenance costs. The Samsung mobile experience (MX) team is currently switching from Java to Kotlin projects. This study reviews the application of DL-APR, which automatically fixes defects that arise during this switching process; however, the shortage of Kotlin defect-fixing datasets in Samsung MX team precludes us from fully utilizing the power of deep learning. Therefore, strategies are needed to effectively reuse the pretrained DL-APR model. This demand can be met using the Kotlin defect-fixing datasets constructed from industrial and open-source repositories, and transfer learning.  
This study aims to validate the performance of the pretrained DL-APR model in fixing defects in the Samsung Kotlin projects, then improve its performance by applying transfer learning. We show that transfer learning with open source and industrial Kotlin defect-fixing datasets can improve the defect-fixing performance of the existing DL-APR by 307%. Furthermore, we confirmed that the performance was improved by 532% compared with the baseline DL-APR model as a result of transferring the knowledge of an industrial (non-defect) bug-fixing dataset. We also discovered that the embedded vectors and overlapping code tokens of the code-change pairs are valuable features for selecting useful knowledge transfer instances by improving the performance of APR models by up to 696%. Our study demonstrates the possibility of applying transfer learning to practitioners who review the application of DL-APR to industrial software.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1441–1452},
numpages = {12},
keywords = {Transfer learning, SonarQube defects, Industrial Kotlin project, Empirical study, Deep learning-based program repair},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3379597.3387458,
author = {Mattis, Toni and Rein, Patrick and D\"{u}rsch, Falco and Hirschfeld, Robert},
title = {RTPTorrent: An Open-source Dataset for Evaluating Regression Test Prioritization},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387458},
doi = {10.1145/3379597.3387458},
abstract = {The software engineering practice of automated testing helps programmers find defects earlier during development. With growing software projects and longer-running test suites, frequency and immediacy of feedback decline, thereby making defects harder to repair. Regression test prioritization (RTP) is concerned with running relevant tests earlier to lower the costs of defect localization and to improve feedback.Finding representative data to evaluate RTP techniques is non-trivial, as most software is published without failing tests. In this work, we systematically survey a wide range of RTP literature regarding whether their dataset uses real or synthetic defects or tests, whether they are publicly available, and whether datasets are reused. We observed that some datasets are reused, however, many projects study only few projects and these rarely resemble real-world development activity.In light of these threats to ecological validity, we describe the construction and characteristics of a new dataset, named RTPTorrent, based on 20 open-source Java programs.Our dataset allows researchers to evaluate prioritization heuristics based on version control meta-data, source code, and test results from fine-grained, automated builds over 9 years of development history. We provide reproducible baselines for initial comparisons and make all data publicly available.We see this as a step towards better reproducibility, ecological validity, and long-term availability of studied software in the field of test prioritization.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {385–396},
numpages = {12},
keywords = {TravisCI, Regression Test Prioritization, Java, GitHub, Dataset},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3338906.3338954,
author = {Du, Xiaoning and Xie, Xiaofei and Li, Yi and Ma, Lei and Liu, Yang and Zhao, Jianjun},
title = {DeepStellar: model-based quantitative analysis of stateful deep learning systems},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338954},
doi = {10.1145/3338906.3338954},
abstract = {Deep Learning (DL) has achieved tremendous success in many cutting-edge applications. However, the state-of-the-art DL systems still suffer from quality issues. While some recent progress has been made on the analysis of feed-forward DL systems, little study has been done on the Recurrent Neural Network (RNN)-based stateful DL systems, which are widely used in audio, natural languages and video processing, etc. In this paper, we initiate the very first step towards the quantitative analysis of RNN-based DL systems. We model RNN as an abstract state transition system to characterize its internal behaviors. Based on the abstract model, we design two trace similarity metrics and five coverage criteria which enable the quantitative analysis of RNNs. We further propose two algorithms powered by the quantitative measures for adversarial sample detection and coverage-guided test generation. We evaluate DeepStellar on four RNN-based systems covering image classification and automated speech recognition. The results demonstrate that the abstract model is useful in capturing the internal behaviors of RNNs, and confirm that (1) the similarity metrics could effectively capture the differences between samples even with very small perturbations (achieving 97% accuracy for detecting adversarial samples) and (2) the coverage criteria are useful in revealing erroneous behaviors (generating three times more adversarial samples than random testing and hundreds times more than the unrolling approach).},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {477–487},
numpages = {11},
keywords = {testing, recurrent neural network, model-based analysis, adversarial sample, Deep learning},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3597503.3639115,
author = {Rahman, Shanto and Shi, August},
title = {FlakeSync: Automatically Repairing Async Flaky Tests},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639115},
doi = {10.1145/3597503.3639115},
abstract = {Regression testing is an important part of the development process but suffers from the presence of flaky tests. Flaky tests nondeterministically pass or fail when run on the same code, misleading developers about the correctness of their changes. A common type of flaky tests are async flaky tests that flakily fail due to timing-related issues such as asynchronous waits that do not return in time or different thread interleavings during execution. Developers commonly try to repair async flaky tests by inserting or increasing some wait time, but such repairs are unreliable.We propose FlakeSync, a technique for automatically repairing async flaky tests by introducing synchronization for a specific test execution. FlakeSync works by identifying a critical point, representing some key part of code that must be executed early w.r.t. other concurrently executing code, and a barrier point, representing the part of code that should wait until the critical point has been executed. FlakeSync can modify code to check when the critical point is executed and have the barrier point keep waiting until the critical point has been executed, essentially synchronizing these two parts of code for the specific test execution. Our evaluation of FlakeSync on known flaky tests from prior work shows that FlakeSync can automatically repair 83.75% of async flaky tests, and the resulting changes add a median overhead of only 1.00X the original test runtime. We submitted 10 pull requests with our changes to developers, with 3 already accepted and none rejected.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {136},
numpages = {12},
keywords = {flaky test repair, async flaky tests},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3687997.3695648,
author = {Hu, Yuefeng and Ishibe, Hiromu and Dai, Feng and Yamazaki, Tetsuro and Chiba, Shigeru},
title = {Bugfox: A Trace-Based Analyzer for Localizing the Cause of Software Regression in JavaScript},
year = {2024},
isbn = {9798400711800},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3687997.3695648},
doi = {10.1145/3687997.3695648},
abstract = {Software regression has been a persistent issue in software development. Although numerous techniques have been proposed to prevent regression from being introduced before release, few are available to address regression as it occurs post-release. Therefore, identifying the root cause of regression has always been a time-consuming and labor-intensive task. We aim to deliver automated solutions for solving regressions based on tracing. We present Bugfox, a trace-based analyzer that reports functions as the possible cause of regression in JavaScript. The idea is to generate runtime trace with instrumented programs, then extract the differences between clean and regression traces, and apply two heuristic strategies based on invocation order and frequency to identify the suspicious functions among differences. We evaluate our approach on 12 real-world regressions taken from the benchmark BugsJS. First strategy solves 6 regressions, and second strategy solves other 4 regressions, resulting in an overall accuracy of 83% on test cases. Notably, Bugfox solves each regression in under 1 minute with minimal memory overhead (&lt;200 Megabytes). Our findings suggest Bugfox could help developers solve regression in real development.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {224–233},
numpages = {10},
keywords = {Code Transformation, Debugging, Regression, Runtime Tracing},
location = {Pasadena, CA, USA},
series = {SLE '24}
}

@inproceedings{10.1145/2590748.2590755,
author = {Rathore, Santosh Singh and Gupta, Atul},
title = {A comparative study of feature-ranking and feature-subset selection techniques for improved fault prediction},
year = {2014},
isbn = {9781450327763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2590748.2590755},
doi = {10.1145/2590748.2590755},
abstract = {The quality of a fault prediction model depends on the software metrics that are used to build the prediction model. Feature selection represents a process of selecting a subset of relevant features that may lead to build improved prediction models. Feature selection techniques can be broadly categorized into two subcategories: feature-ranking and feature-subset selection. In this paper, we present a comparative investigation of seven feature-ranking techniques and eight feature-subset selection techniques for improved fault prediction. The performance of these feature selection techniques is evaluated using two popular machine-learning classifiers: Naive Bayes and Random Forest, over fourteen software project's fault-datasets obtained from the PROMISE data repository. The performances were measured using F-measure and AUC values. Our results demonstrated that feature-ranking techniques produced better results compared to feature-subset selection techniques. Among, the feature-ranking techniques used in the study, InfoGain and PCA techniques provided the best performance over all the datasets, while for feature-subset selection techniques ClassifierSubsetEval and Logistic Regression produced better results against their peers.},
booktitle = {Proceedings of the 7th India Software Engineering Conference},
articleno = {7},
numpages = {10},
keywords = {wrappers, software metrics, filters, feature-ranking, feature selection, fault prediction},
location = {Chennai, India},
series = {ISEC '14}
}

@inproceedings{10.1145/3377930.3390167,
author = {Anjum, Muhammad Sheraz and Ryan, Conor},
title = {Scalability analysis of grammatical evolution based test data generation},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3390167},
doi = {10.1145/3377930.3390167},
abstract = {Heuristic-based search techniques have been increasingly used to automate different aspects of software testing. Several studies suggest that variable interdependencies may exist in branching conditions of real-life programs, and these dependencies result in the need for highly precise data values (such as of the form i=j=k) for code coverage analysis. This requirement makes it very difficult for Genetic Algorithm (GA)-based approach to successfully search for the required test data from vast search spaces of real-life programs.Ariadne is the only Grammatical Evolution (GE)-based test data generation system, proposed to date, that uses grammars to exploit variable interdependencies to improve code coverage. Ariadne has been compared favourably to other well-known test data generation techniques in the literature; however, its scalability has not yet been tested for increasingly complex programs.This paper presents the results of a rigorous analysis performed to examine Ariadne's scalability. We also designed and employed a large set of highly scalable 18 benchmark programs for our experiments. Our results suggest that Ariadne is highly scalable as it exhibited 100% coverage across all the programs of increasing complexity with significantly smaller search costs than GA-based approaches, which failed even with huge search budgets.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {1213–1221},
numpages = {9},
keywords = {variable interdependencies, software testing, search based software testing, scalability, grammatical evolution, evolutionary testing, code coverage analysis, automatic test data generation},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.5555/3615924.3615951,
author = {Rivera, Luis F. and Villegas, Norha M. and Tamura, Gabriel and Muller, Hausi A. and Watts, Ian and Erpenbach, Eric and Shwartz, Laura and Liu, Xiaotong},
title = {Using Digital Twins for Software Change Risk Assessment Toward Proactive AIOps},
year = {2023},
publisher = {IBM Corp.},
address = {USA},
abstract = {The increasing structural and behavioural complexity of modern IT systems and environments (IT-Sys|Envs) calls for adopting automated fault anticipation and forecasting mechanisms to mitigate risks, limit system disturbances and damage, and improve problem resolution readiness. Preventing unexpected or undesired ITSys| Envs behaviour is crucial not only to maximize customer value and satisfaction but also to fulfill strict service-level agreements. Given the constant need for operational changes in today’s dynamic IT-Sys|Envs as a response to evolving user requirements and expectations, DevOps teams face several challenges and uncertainties to limit the potential introduction of faults, foresee their occurrence, and comprehend their associated risks. While the accomplishments attained in the application of artificial intelligence to the operation of IT-Sys|Envs (i.e., AIOps) show promise, further research is necessary to facilitate the transition from reactive AIOps to its anticipated proactive, and risk-focused, manifestation. This involves advancing AIOps conceptualization and tooling for enabling improved software fault prognosis and remediation, augmented risk management, and enhanced explainability. We describe our vision for proactive AIOps and report our progress toward its viable realization through the application of relevant notions from the revolutionary concept of Digital Twins. Moreover, we discuss challenges and opportunities regarding this promising integration and its impact on the future of software development and operation life cycles.},
booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering},
pages = {211–216},
numpages = {6},
keywords = {Digital Twins, AIOps, Reference Models, Runtime Experimentation},
location = {Las Vegas, NV, USA},
series = {CASCON '23}
}

@inproceedings{10.1145/3551349.3563241,
author = {Pham, Khang and Nguyen, Vu and Nguyen, Tien},
title = {Application of Natural Language Processing Towards Autonomous Software Testing},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3563241},
doi = {10.1145/3551349.3563241},
abstract = {The process of creating test cases from requirements written in natural language (NL) requires intensive human efforts and can be tedious, repetitive, and error-prone. Thus, many studies have attempted to automate that process by utilizing Natural Language Processing (NLP) approaches. Furthermore, with the advent of massive language models and transfer learning techniques, people have introduced various advancements in NLP-assisted software testing with promising results. More notably, in recent years, not only have researchers been engrossed in solving the above task, but many companies have also embedded the feature to translate from human language to test cases their products. This paper presents an overview of NLP-assisted solutions being used in both the literature and the software testing industry.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {216},
numpages = {4},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3387940.3392218,
author = {Zid, Cyrine and Humeniuk, Dmytro and Khomh, Foutse and Antoniol, Giuliano},
title = {Double Cycle Hybrid Testing of Hybrid Distributed IoT System},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392218},
doi = {10.1145/3387940.3392218},
abstract = {Testing heterogeneous IoT applications such as a home automation systems integrating a variety of devices poses serious challenges. Oftentimes requirements are vaguely defined. Consumer grade cyber-physical devices and software may not meet the reliability and quality standard needed. Plus, system behavior may partially depend on various environmental conditions. For example, WI-FI congestion may cause packet delay; meanwhile cold weather may cause an unexpected drop of inside temperature.We surmise that generating and executing failure exposing scenarios is especially challenging. Modeling phenomenons such as network traffic or weather conditions is complex. One possible solution is to rely on machine learning models approximating the reality. These models, integrated in a system model, can be used to define surrogate models and fitness functions to steer the search in the direction of failure inducing scenarios.However, these models also should be validated. Therefore, there should be a double loop co-evolution between machine learned surrogate models functions and fitness functions.Overall, we argue that in such complex cyber-physical systems, co-evolution and multi-hybrid approaches are needed.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {529–532},
numpages = {4},
keywords = {test data Generation, scenario generation, cyber-physical systems, IoT},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3340531.3412157,
author = {Baril, Xavier and Cousti\'{e}, Oihana and Mothe, Josiane and Teste, Olivier},
title = {Application Performance Anomaly Detection with LSTM on Temporal Irregularities in Logs},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412157},
doi = {10.1145/3340531.3412157},
abstract = {Performance anomalies are a core problem in modern information systems, that affects the execution of the hosted applications. The detection of these anomalies often relies on the analysis of the application execution logs. The current most effective approach is to detect samples that differ from a learnt nominal model. However, current methods often focus on detecting sequential anomalies in logs, neglecting the time elapsed between logs, which is a core component of the performance anomaly detection. In this paper, we develop a new model for performance anomaly detection that captures temporal deviations from the nominal model, by means of a sliding window data representation. This nominal model is trained by a Long Short-Term Memory neural network, which is appropriate to represent complex sequential dependencies. We assess the effectiveness of our model on both simulated and real datasets. We show that it is more robust to temporal variations than current state-of-the-art approaches, while remaining as effective.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {1961–1964},
numpages = {4},
keywords = {information system, event logs, anomaly detection},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3524842.3528492,
author = {Sousa, Bruno L. and Bigonha, Mariza A. S. and Ferreira, Kecia A. M. and Franco, Glaura C.},
title = {A time series-based dataset of open-source software evolution},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528492},
doi = {10.1145/3524842.3528492},
abstract = {Software evolution is the process of developing, maintaining, and updating software systems. It is known that the software systems tend to increase their complexity and size over their evolution to meet the demands required by the users. Due to this fact, researchers have increasingly carried out studies on software evolution to understand the systems' evolution pattern and propose techniques to overcome inherent problems in software evolution. Many of these works collect data but do not make them publicly available. Many datasets on software evolution are outdated, and/or are small, and some of them do not provide time series from software metrics. We propose an extensive software evolution dataset with temporal information about open-source Java systems. To build this dataset, we proposed a methodology of four steps: selecting the systems using a criterion, extracting and measuring their releases, and generating their time series. Our dataset contains time series of 46 software metrics extracted from 46 open-source Java systems, and we make it publicly available.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {702–706},
numpages = {5},
keywords = {time series, software metrics, software evolution, open-source software, dataset},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1145/3597926.3598144,
author = {Ounjai, Jiradet and W\"{u}stholz, Valentin and Christakis, Maria},
title = {Green Fuzzer Benchmarking},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598144},
doi = {10.1145/3597926.3598144},
abstract = {Over the last decade, fuzzing has been increasingly gaining  
 traction due to its effectiveness in finding  
 bugs. Nevertheless, fuzzer evaluations have been challenging  
 during this time, mainly due to lack of standardized  
 benchmarking. Aiming to alleviate this issue, in 2020, Google  
 released FuzzBench, an open-source benchmarking platform, that  
 is widely used for accurate fuzzer benchmarking.  

 However, a typical FuzzBench experiment takes CPU years to  
 run. If we additionally consider that fuzzers under active  
 development evaluate any changes empirically, benchmarking  
 becomes prohibitive both in terms of computational resources  
 and time. In this paper, we propose GreenBench, a greener  
 benchmarking platform that, compared to FuzzBench,  
 significantly speeds up fuzzer evaluations while maintaining  
 very high accuracy.  

 In contrast to FuzzBench, GreenBench drastically increases the  
 number of benchmarks while drastically decreasing the duration  
 of fuzzing campaigns. As a result, the fuzzer rankings  
 generated by GreenBench are almost as accurate as those by  
 FuzzBench (with very high correlation), but GreenBench is from  
 18 to 61 times faster. We discuss the implications of these  
 findings for the fuzzing community.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1396–1406},
numpages = {11},
keywords = {testing, fuzzing, benchmarking},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3652032.3657568,
author = {Feng, Zhongwen and Ma, Junyan},
title = {TWFuzz: Fuzzing Embedded Systems with Three Wires},
year = {2024},
isbn = {9798400706165},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652032.3657568},
doi = {10.1145/3652032.3657568},
abstract = {Fuzzing is a highly effective means of discovering vulnerabilities in traditional software. However, when it comes to fuzzing an embedded system, especially for a smaller microcontroller device with monolithic firmware, the challenges are considerable. The highly constrained resources of the system and the diverse behaviors of peripherals often render software instrumentation or emulation-based fuzzing impractical in many cases.
 
 In this work, we introduce TWFuzz, a fuzzing tool designed for effective coverage collection in embedded systems. Specifically, TWFuzz relies on two hardware interfaces (tracing and debugging) and three types of probes (program counter samples, instruction address matchings, hardware breakpoints) as feedback channels for coverage-guided fuzzing. The ARM Single Wire Output (SWO) hardware tracing interface is used for capturing periodic samples of the program counter. The ARM Serial Wire Debug (SWD) debugging interface is used for setting and checking a limited number of instruction address matchings and hardware breakpoints. With these three types of probes, TWFuzz can extract coverage information without costly code instrumentation and hardware emulation, which enables effective fuzzing on embedded systems. To optimize the fuzzing flow, in particular the tracing analysis part, we implement TWFuzz on PYNQ-Z1 FPGA board. We evaluate TWFuzz on two development boards. Compared to the state-of-the-art open-source fuzzer GDBFuzz, TWFuzz's average code coverage rate is 1.24 times that of GDBFuzz.},
booktitle = {Proceedings of the 25th ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems},
pages = {107–118},
numpages = {12},
keywords = {ARM CoreSight, embedded systems, firmware security, fuzzing},
location = {Copenhagen, Denmark},
series = {LCTES 2024}
}

@proceedings{10.1145/3664646,
title = {AIware 2024: Proceedings of the 1st ACM International Conference on AI-Powered Software},
year = {2024},
isbn = {9798400706851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 1st ACM International Conference on AI-Powered Software (AIware), held on 15th and 16th July 2024 in Porto de Galinhas, Brazil co-located with the ACM International Conference on the Foundations of Software Engineering (FSE 2024). AIware aims to be an annual conference that brings the software engineering community together in anticipation of the upcoming changes driven by Foundation Models (FMs) and looks at them from the perspective of AI-powered software and their evolution.
 
 
 

 
 
 
AIware 2024 prioritizes fostering discussions about the latest developments in the interdisciplinary field of AIware rather than solely focusing on the presentation of papers. The emphasis is on engaging conversations from diverse backgrounds to identify emerging research challenges and establish a new research agenda for the community in the Foundation Model era. To present papers and for discussions, the two-day conference will have five sessions themed around AIware Vision, SE for AIware, Human - AI Conversation, Security &amp; Safety and AIware for Software Lifecycle Activities. Furthermore, the conference program will include two keynotes and five industry talks. The final session in the conference program will be dedicated to presenting accepted papers of the AIware challenge track.},
location = {Porto de Galinhas, Brazil}
}

@inproceedings{10.1145/3540250.3549171,
author = {Li, Yi and Wang, Shaohua and Nguyen, Tien N.},
title = {UTANGO: untangling commits with context-aware, graph-based, code change clustering learning model},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549171},
doi = {10.1145/3540250.3549171},
abstract = {During software evolution, developers make several changes and commit them into the repositories. Unfortunately, many of them tangle different purposes, both hampering program comprehension and reducing separation of concerns. Automated approaches with deterministic solutions have been proposed to untangle commits. However, specifying an effective clustering criteria on the changes in a commit for untangling is challenging for those approaches. In this work, we present UTango, a machine learning (ML)-based approach that learns to untangle the changes in a commit. We develop a novel code change clustering learning model that learns to cluster the code changes, represented by the embeddings, into different groups with different concerns. We adapt the agglomerative clustering algorithm into a supervised-learning clustering model operating on the learned code change embeddings via trainable parameters and a loss function in comparing the predicted clusters and the correct ones during training. To facilitate our clustering learning model, we develop a context-aware, graph-based, code change representation learning model, leveraging Label, Graph-based Convolution Network to produce the contextualized embeddings for code changes, that integrates program dependencies and the surrounding contexts of the changes. The contexts and cloned code are also explicitly represented, helping UTango distinguish the concerns. Our empirical evaluation on C# and Java datasets with 1,612 and 14k tangled commits show that it achieves the accuracy of 28.6%– 462.5% and 13.3%–100.0% relatively higher than the state-of-the-art commit-untangling approaches for C# and Java, respectively.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {221–232},
numpages = {12},
keywords = {Deep Learning, Commit Untangling, Code Change Embeddings},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3578527.3581766,
author = {Ray, Baishakhi},
title = {Programming Language Processing : How AI can Revolutionize Software Development?},
year = {2023},
isbn = {9798400700644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578527.3581766},
doi = {10.1145/3578527.3581766},
abstract = {The past decade has seen unprecedented growth in Software Engineering— developers spend enormous time and effort to create new products. With such enormous growth comes the responsibility of producing and maintaining quality and robust software. However, developing such software is non-trivial— 50% of software developers’ valuable time is wasted on finding and fixing bugs, costing the global economy around USD$1.1 trillion. Today, I will discuss how AI can help in different stages of the software development life cycle for developing quality products. In particular, I will talk about Programming Language Processing (PLP), an emerging research field that can model different aspects of code (source, binary, execution, etc.) to automate diverse Software Engineering tasks, including code generation, bug finding, security analysis, etc.},
booktitle = {Proceedings of the 16th Innovations in Software Engineering Conference},
articleno = {1},
numpages = {1},
keywords = {Programming Language, Program Analysis},
location = {Allahabad, India},
series = {ISEC '23}
}

@inproceedings{10.1145/3319008.3319716,
author = {Alsolai, Hadeel and Roper, Marc},
title = {Application of Ensemble Techniques in Predicting Object-Oriented Software Maintainability},
year = {2019},
isbn = {9781450371452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319008.3319716},
doi = {10.1145/3319008.3319716},
abstract = {While prior object-oriented software maintainability literature acknowledges the role of machine learning techniques as valuable predictors of potential change, the most suitable technique that achieves consistently high accuracy remains undetermined. With the objective of obtaining more consistent results, an ensemble technique is investigated to advance the performance of the individual models and increase their accuracy in predicting software maintainability of the object-oriented system. This paper describes the research plan for predicting object-oriented software maintainability using ensemble techniques. First, we present a brief overview of the main research background and its different components. Second, we explain the research methodology. Third, we provide expected results. Finally, we conclude summary of the current status.},
booktitle = {Proceedings of the 23rd International Conference on Evaluation and Assessment in Software Engineering},
pages = {370–373},
numpages = {4},
keywords = {software maintainability, prediction, individual model, ensemble model, Object-oriented system},
location = {Copenhagen, Denmark},
series = {EASE '19}
}

@article{10.1145/3447332.3447334,
author = {Islam, Md Rakibul and Zibran, Minhaz F.},
title = {What changes in where? an empirical study of bug-fixing change patterns},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1559-6915},
url = {https://doi.org/10.1145/3447332.3447334},
doi = {10.1145/3447332.3447334},
abstract = {A deep understanding of the common patterns of bug-fixing changes is useful in several ways: (a) such knowledge can help developers in proactively avoiding coding patterns that lead to bugs and (b) bug-fixing patterns are exploited in devising techniques for automatic bug localization and program repair.This work includes an in-depth quantitative and qualitative analysis over 4,653 buggy revisions of five software systems. Our study identifies 38 bug-fixing edit patterns and discovers 37 new patterns of nested code structures, which frequently host the bug-fixing edits. While some of the edit patterns were reported in earlier studies, these nesting patterns are new and were never targeted before.},
journal = {SIGAPP Appl. Comput. Rev.},
month = jan,
pages = {18–34},
numpages = {17},
keywords = {vulnerability, source code, software, pattern, nesting, fault, error, empirical study, edits, defect, bug, analysis}
}

@inproceedings{10.1145/1540438.1540466,
author = {Jiang, Yue and Cukic, Bojan},
title = {Misclassification cost-sensitive fault prediction models},
year = {2009},
isbn = {9781605586342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1540438.1540466},
doi = {10.1145/1540438.1540466},
abstract = {Traditionally, software fault prediction models are built by assuming a uniform misclassification cost. In other words, cost implications of misclassifying a faulty module as fault free are assumed to be the same as the cost implications of misclassifying a fault free module as faulty. In reality, these two types of misclassification costs are rarely equal. They are project-specific, reflecting the characteristics of the domain in which the program operates. In this paper, using project information from a public repository, we analyze the benefits of techniques which incorporate misclassification costs in the development of software fault prediction models. We find that cost-sensitive learning does not provide operational points which outperform cost-insensitive classifiers. However, an advantage of cost-sensitive modeling is the explicit choice of the operational threshold appropriate for the cost differential.},
booktitle = {Proceedings of the 5th International Conference on Predictor Models in Software Engineering},
articleno = {20},
numpages = {10},
keywords = {cost-sensitive, fault prediction, machine learning, misclassification cost},
location = {Vancouver, British Columbia, Canada},
series = {PROMISE '09}
}

@article{10.1145/3664603,
author = {Xu, Hang and Chen, Liheng and Gan, Shuitao and Zhang, Chao and Li, Zheming and Ji, Jiangan and Chen, Baojian and Hu, Fan},
title = {Graphuzz: Data-driven Seed Scheduling for Coverage-guided Greybox Fuzzing},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3664603},
doi = {10.1145/3664603},
abstract = {Seed scheduling is a critical step of greybox fuzzing, which assigns different weights to seed test cases during seed selection, and significantly impacts the efficiency of fuzzing. Existing seed scheduling strategies rely on manually designed models to estimate the potentials of seeds and determine their weights, which fails to capture the rich information of a seed and its execution and thus the estimation of seeds’ potentials is not optimal. In this article, we introduce a new seed scheduling solution, Graphuzz, for coverage-guided greybox fuzzing, which utilizes deep learning models to estimate the potentials of seeds and works in a data-driven way. Specifically, we propose an extended control flow graph called e-CFG to represent the control-flow and data-flow features of a seed's execution, which is suitable for graph neural networks (GNN) to process and estimate seeds’ potential. We evaluate each seed's code coverage increment and use it as the label to train the GNN model. Further, we propose a self-attention mechanism to enhance the GNN model so that it can capture overlooked features. We have implemented a prototype of Graphuzz based on the baseline fuzzer AFLplusplus. The evaluation results show that our model can estimate the potential of seeds and has the robust capability to generalize to different targets. Furthermore, the evaluation using 12 benchmarks from FuzzBench shows that Graphuzz outperforms AFLplusplus and the state-of-the-art seed scheduling solution K-Scheduler and other coverage-guided fuzzers in terms of code coverage, and the evaluation using 8 benchmarks from Magma shows that Graphuzz outperforms the baseline fuzzer AFLplusplus and SOTA solutions in terms of bug detection.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {185},
numpages = {36},
keywords = {Fuzzing, seed scheduling, graph neural network}
}

@inproceedings{10.5555/3643142.3643150,
author = {Aros, Susan K. and McDonald, Mary L.},
title = {Squashing Bugs and Improving Design: Using Data Farming to Support Verification and Validation of Military Agent-Based Simulations},
year = {2024},
isbn = {9798350369663},
publisher = {IEEE Press},
abstract = {Verification and validation of complex agent-based human behavior simulation models is a challenging endeavor, particularly since a dearth of real-world data makes it impossible to use most traditional validation methods. Data farming techniques have stepped up to the challenge, proving to be a valuable tool for verification and validation of complex models. In this paper we demonstrate how data farming and analysis aids in the verification and validation of complex models by presenting specific examples pertaining to WRENCH, an agent-based simulation model that represents complex interactions between security forces and civilians during civil security stability operations. We first provide an overview of data farming and its relevance for verification and validation of military agent-based simulation models, then give an overview of WRENCH, and finally demonstrate with examples how we have used data farming to aid in the verification and validation of WRENCH.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {106–117},
numpages = {12},
location = {San Antonio, Texas, USA},
series = {WSC '23}
}

@inproceedings{10.1145/3597503.3639076,
author = {Ba, Jinsheng and Rigger, Manuel},
title = {CERT: Finding Performance Issues in Database Systems Through the Lens of Cardinality Estimation},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639076},
doi = {10.1145/3597503.3639076},
abstract = {Database Management Systems (DBMSs) process a given query by creating a query plan, which is subsequently executed, to compute the query's result. Deriving an efficient query plan is challenging, and both academia and industry have invested decades into researching query optimization. Despite this, DBMSs are prone to performance issues, where a DBMS produces an unexpectedly inefficient query plan that might lead to the slow execution of a query. Finding such issues is a longstanding problem and inherently difficult, because no ground truth information on an expected execution time exists. In this work, we propose Cardinality Estimation Restriction Testing (CERT), a novel technique that finds performance issues through the lens of cardinality estimation. Given a query on a database, CERT derives a more restrictive query (e.g., by replacing a LEFT JOIN with an INNER JOIN), whose estimated number of rows should not exceed the estimated number of rows for the original query. CERT tests cardinality estimation specifically, because it was shown to be the most important part for query optimization; thus, we expect that finding and fixing cardinality-estimation issues might result in the highest performance gains. In addition, we found that other kinds of query optimization issues can be exposed by unexpected estimated cardinalities, which can also be found by CERT. CERT is a black-box technique that does not require access to the source code; DBMSs expose query plans via the EXPLAIN statement. CERT eschews executing queries, which is costly and prone to performance fluctuations. We evaluated CERT on three widely used and mature DBMSs, MySQL, TiDB, and CockroachDB. CERT found 13 unique issues, of which 2 issues were fixed and 9 confirmed by the developers. We expect that this new angle on finding performance bugs will help DBMS developers in improving DMBSs' performance.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {133},
numpages = {13},
keywords = {database, performance issue, cardinality estimation},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3686614.3686622,
author = {Sanusi, Bashir and Ogunshile, Emmanuel and Aydin, Mehmet and Olabiyisi, Stephen},
title = {Assuring Correctness, Testing, and Verification of X-Compiler by Integrating Communicating Stream X-Machine},
year = {2024},
isbn = {9798400718052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686614.3686622},
doi = {10.1145/3686614.3686622},
abstract = {Compiler design plays an important role in ensuring that the translation of the programs written in high-level language into executable code is correct. However, in todays’ safety-critical environments, security gaps, visible and hidden defects in compiler models are liability factors that needed to be addressed in the process of examining that a system meets specifications and requirements of its intended objectives. Nevertheless, defects in this model are errors in coding which bring about incorrect or unexpected results from a compiler design that does not meet the actual requirements. Hence, this paper developed a novel approach by integrating the computational power of Communicating Stream X-Machine (CSXM) to address the problem associated with compiler correctness. In addition, this paper outlines the development of a novel compiler design called X-compiler, emphasizing its important role in software development. CSXM technique was implemented in Visual Studio (2022) to develop the X-compiler. The X-compiler utilize state-of-the-art techniques to translate program source code written in high-level programming language which is the blend of C# and Python programming language into executable code. The development process includes the lexical analysis, syntax parsing, semantic analysis, and code generation ensuring the correctness of the X-compiler. The results of the compiled code were run to produce output in command Windows environment for user interactions. Also, highlights the potential for enhanced language interoperability and the development of efficient compiler that leverage the strengths of multiple programming paradigms. Overall, the study on CSXM contributed to a deeper understanding of concurrent systems, while the design and implementation of the compiler showcased the feasibility of creating a synergistic blend of C# and Python programming languages.},
booktitle = {Proceedings of the 2024 6th International Conference on Software Engineering and Development},
pages = {64–72},
numpages = {9},
keywords = {Code Generation, Communicating Stream X-Machine, Compiler, Software Development, X-Compiler},
location = {Hong Kong, Hong Kong},
series = {ICSED '24}
}

@inproceedings{10.1145/3643796.3648448,
author = {Szatm\'{a}ri, Attila and Sarhan, Qusay Idrees and Soha, Peter Attila and Balogh, Gergo and Beszedes, Arpad},
title = {On the Integration of Spectrum-Based Fault Localization Tools into IDEs},
year = {2024},
isbn = {9798400705809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643796.3648448},
doi = {10.1145/3643796.3648448},
abstract = {Spectrum-Based Fault Localization (SBFL) is a technique to be used during debugging, the premise of which is that, based on the test case outcomes and code coverage, faulty code elements can be automatically detected. SBFL is popular among researchers because it is lightweight and easy to implement, and there is a lot of potential in it when it comes to research that aims to improve its effectiveness. Despite this, the technique cannot be found in contemporary development and debugging tools, only a handful of research prototypes are available. Reasons for this can be multiple, including the algortihms' sub-optimal effectiveness and other technical weaknesses. But, also the lack of clear functional and non-functional requirements for such a tool, either standalone or integrated into IDEs. In this paper, we attempt to provide such a list in form of recommendations, based on surveying the most popular SBFL tools and on our own researchers' and tool builders' experience.},
booktitle = {Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments},
pages = {24–29},
numpages = {6},
keywords = {spectrum-based fault localization, SBFL, IDE, debugging},
location = {Lisbon, Portugal},
series = {IDE '24}
}

@inproceedings{10.5555/2819261.2819282,
author = {Ding, Sun and Beng Kuan Tan, Hee and Shar, Lwin Khin},
title = {Mining patterns of unsatisfiable constraints to detect infeasible paths},
year = {2015},
publisher = {IEEE Press},
abstract = {Detection of infeasible paths is required in many areas including test coverage analysis, test case generation, security vulnerability analysis, etc. Existing approaches typically use static analysis coupled with symbolic evaluation, heuristics, or path-pattern analysis. This paper is related to these approaches but with a different objective. It is to analyze code of real systems to build patterns of unsatisfiable constraints in infeasible paths. The resulting patterns can be used to detect infeasible paths without the use of constraint solver and evaluation of function calls involved, thus improving scalability. The patterns can be built gradually. Evaluation of the proposed approach shows promising results.},
booktitle = {Proceedings of the 10th International Workshop on Automation of Software Test},
pages = {65–69},
numpages = {5},
keywords = {symbolic evaluation, structural testing, static analysis, pattern mining, infeasible paths},
location = {Florence, Italy},
series = {AST '15}
}

@inproceedings{10.1145/3643991.3644889,
author = {Beyer, Dirk and Grunske, Lars and Kettl, Matthias and Lingsch-Rosenfeld, Marian and Raselimo, Moeketsi},
title = {P3: A Dataset of Partial Program Fixes},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644889},
doi = {10.1145/3643991.3644889},
abstract = {Identifying and fixing bugs in programs remains a challenge and is one of the most time-consuming tasks in software development. But even after a bug is identified, and a fix has been proposed by a developer or tool, it is not uncommon that the fix is incomplete and does not cover all possible inputs that trigger the bug. This can happen quite often and leads to re-opened issues and inefficiencies. In this paper, we introduce P3, a curated dataset composed of incomplete fixes. Each entry in the set contains a series of commits fixing the same underlying issue, where multiple of the intermediate commits are incomplete fixes. These are sourced from real-world open-source C projects. The selection process involves both automated and manual stages. Initially, we employ heuristics to identify potential partial fixes from repositories, subsequently we validate them through meticulous manual inspection. This process ensures the accuracy and reliability of our curated dataset. We envision that the dataset will support researchers while investigating partial fixes in more detail, allowing them to develop new techniques to detect and fix them. We make our dataset publicly available at https://gitlab.com/sosy-lab/research/data/partial-fix-dataset.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {123–127},
numpages = {5},
keywords = {partial program fixes, supplementary bug fixes, recurring bugs},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3196321.3196331,
author = {Xu, Zhou and Li, Shuai and Tang, Yutian and Luo, Xiapu and Zhang, Tao and Liu, Jin and Xu, Jun},
title = {Cross version defect prediction with representative data via sparse subset selection},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196331},
doi = {10.1145/3196321.3196331},
abstract = {Software defect prediction aims at detecting the defect-prone software modules by mining historical development data from software repositories. If such modules are identified at the early stage of the development, it can save large amounts of resources. Cross Version Defect Prediction (CVDP) is a practical scenario by training the classification model on the historical data of the prior version and then predicting the defect labels of modules of the current version. However, software development is a constantly-evolving process which leads to the data distribution differences across versions within the same project. The distribution differences will degrade the performance of the classification model. In this paper, we approach this issue by leveraging a state-of-the-art Dissimilarity-based Sparse Subset Selection (DS3) method. This method selects a representative module subset from the prior version based on the pairwise dissimilarities between the modules of two versions and assigns each module of the current version to one of the representative modules. These selected modules can well represent the modules of the current version, thus mitigating the distribution differences. We evaluate the effectiveness of DS3 for CVDP performance on total 40 cross-version pairs from 56 versions of 15 projects with three traditional and two effort-aware indicators. The extensive experiments show that DS3 outperforms three baseline methods, especially in terms of two effort-aware indicators.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {132–143},
numpages = {12},
keywords = {cross version defect prediction, pairwise dissimilarities, representative data, sparse subset selection},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@inproceedings{10.1145/3533767.3534381,
author = {Lahiri, Sumit and Roy, Subhajit},
title = {Almost correct invariants: synthesizing inductive invariants by fuzzing proofs},
year = {2022},
isbn = {9781450393799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533767.3534381},
doi = {10.1145/3533767.3534381},
abstract = {Real-life programs contain multiple operations whose semantics are unavailable to verification engines, like third-party library calls, inline assembly and SIMD instructions, special compiler-provided primitives, and queries to uninterpretable machine learning models. Even with the exceptional success story of program verification, synthesis of inductive invariants for such "open" programs has remained a challenge. Currently, this problem is handled by manually "closing" the program---by providing hand-written stubs that attempt to capture the behavior of the unmodelled operations; writing stubs is not only difficult and tedious, but the stubs are often incorrect---raising serious questions on the whole endeavor.  
	  
	In this work, we propose Almost Correct Invariants as an automated strategy for synthesizing inductive invariants for such "open" programs. We adopt an active learning strategy where a data-driven learner proposes candidate invariants. In deviation from prior work that attempt to verify invariants, we attempt to falsify the invariants: we reduce the falsification problem to a set of reachability checks on non-deterministic programs; we ride on the success of modern fuzzers to answer these reachability queries. Our tool, Achar, automatically synthesizes inductive invariants that are sufficient to prove the correctness of the target programs. We compare Achar with a state-of-the-art invariant synthesis tool that employs theorem proving on formulae built over the program source. Though Achar is without strong soundness guarantees, our experiments show that even when we provide almost no access to the program source, Achar outperforms the state-of-the-art invariant generator that has complete access to the source. We also evaluate Achar on programs that current invariant synthesis engines cannot handle---programs that invoke external library calls, inline assembly, and queries to convolution neural networks; Achar successfully infers the necessary inductive invariants within a reasonable time.},
booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {352–364},
numpages = {13},
keywords = {verification, testing, inductive invariant synthesis, fuzzing},
location = {Virtual, South Korea},
series = {ISSTA 2022}
}

@inproceedings{10.1145/3639478.3640032,
author = {Urrico, Michael Ferdinando and Clerissi, Diego and Mariani, Leonardo},
title = {MutaBot: A Mutation Testing Approach for Chatbots},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3640032},
doi = {10.1145/3639478.3640032},
abstract = {Mutation testing is a technique aimed at assessing the effectiveness of test suites by seeding artificial faults into programs. Although available for many platforms and languages, no mutation testing tool is currently available for conversational chatbots, which represent an increasingly popular solution to design systems that can interact with users through a natural language interface. Note that since conversations must be explicitly engineered by the developers of conversational chatbots, these systems are exposed to specific types of faults not supported by existing mutation testing tools.In this paper, we present MutaBot, a mutation testing tool for conversational chatbots. MutaBot addresses mutations at multiple levels, including conversational flows, intents, and contexts. We designed the tool to potentially target multiple platforms, while we implemented initial support for Google Dialogflow chatbots. We assessed the tool with three Dialogflow chatbots and test cases generated with Botium, revealing weaknesses in the test suites.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {79–83},
numpages = {5},
keywords = {chatbot testing, mutation testing, botium, dialogflow},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3319619.3322004,
author = {Lo, Fang-Yi and Chen, Chao-Hong and Chen, Ying-ping},
title = {Genetic algorithms as shrinkers in property-based testing},
year = {2019},
isbn = {9781450367486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319619.3322004},
doi = {10.1145/3319619.3322004},
abstract = {This paper proposes the use of genetic algorithms as shrinkers for shrinking the counterexamples generated by QuickChick, a property-based testing framework for Coq. The present study incorporates the flexibility and versatility of evolutionary algorithms into the realm of rigorous software development, in particular, making the results of property-based testing observable and comprehensible for human. The program code for merge sort is investigated as a showcase in the study. Due to the lack of similar proposals in the literature, random sample is used to compete with the proposal for comparison. The experimental results indicate that the proposed genetic algorithm outperforms random sample. Moreover, the minimal counterexamples, through which programmers are able to pinpoint the program mistakes with ease, can be successfully obtained by using genetic algorithms as shrinkers.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {291–292},
numpages = {2},
keywords = {shrinker, quickchick, property-based testing, genetic algorithms, Coq},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@article{10.1145/3689596,
author = {Neumann, Peter G. and Lindqvist, Ulf},
title = {The Future of Misuse Detection},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {67},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/3689596},
doi = {10.1145/3689596},
abstract = {From lessons learned to new directions.},
journal = {Commun. ACM},
month = oct,
pages = {27–28},
numpages = {2}
}

@inproceedings{10.5555/2486788.2486808,
author = {Cotroneo, Domenico and Pietrantuono, Roberto and Russo, Stefano},
title = {A learning-based method for combining testing techniques},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {This work presents a method to combine testing techniques adaptively during the testing process. It intends to mitigate the sources of uncertainty of software testing processes, by learning from past experience and, at the same time, adapting the technique selection to the current testing session. The method is based on machine learning strategies. It uses offline strategies to take historical information into account about the techniques performance collected in past testing sessions; then, online strategies are used to adapt the selection of test cases to the data observed as the testing proceeds. Experimental results show that techniques performance can be accurately characterized from features of the past testing sessions, by means of machine learning algorithms, and that integrating this result into the online algorithm allows improving the fault detection effectiveness with respect to single testing techniques, as well as to their random combination.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {142–151},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/3613904.3642111,
author = {Kabir, Samia and Li, Lixiang and Zhang, Tianyi},
title = {STILE: Exploring and Debugging Social Biases in Pre-trained Text Representations},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642111},
doi = {10.1145/3613904.3642111},
abstract = {The recent success of Natural Language Processing (NLP) relies heavily on pre-trained text representations such as word embeddings. However, pre-trained text representations may exhibit social biases and stereotypes, e.g., disproportionately associating gender with occupations. Though prior work presented various bias detection algorithms, they are limited to pre-defined biases and lack effective interaction support. In this work, we propose Stile, an interactive system that supports mixed-initiative bias discovery and debugging in pre-trained text representations. Stile provides users the flexibility to interactively define and customize biases to detect based on their interests. Furthermore, it provides a bird’s-eye view of detected biases in a Chord diagram and allows users to dive into the training data to investigate how a bias was developed. Our lab study and expert review confirm the usefulness and usability of Stile as an effective aid in identifying and understanding biases in pre-trained text representations.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {293},
numpages = {20},
keywords = {AI Fairness, Natural Language Processing, Word Embedding},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3180155.3180160,
author = {Abdessalem, Raja Ben and Nejati, Shiva and Briand, Lionel C. and Stifter, Thomas},
title = {Testing vision-based control systems using learnable evolutionary algorithms},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180160},
doi = {10.1145/3180155.3180160},
abstract = {Vision-based control systems are key enablers of many autonomous vehicular systems, including self-driving cars. Testing such systems is complicated by complex and multidimensional input spaces. We propose an automated testing algorithm that builds on learnable evolutionary algorithms. These algorithms rely on machine learning or a combination of machine learning and Darwinian genetic operators to guide the generation of new solutions (test scenarios in our context). Our approach combines multiobjective population-based search algorithms and decision tree classification models to achieve the following goals: First, classification models guide the search-based generation of tests faster towards critical test scenarios (i.e., test scenarios leading to failures). Second, search algorithms refine classification models so that the models can accurately characterize critical regions (i.e., the regions of a test input space that are likely to contain most critical test scenarios). Our evaluation performed on an industrial automotive automotive system shows that: (1) Our algorithm outperforms a baseline evolutionary search algorithm and generates 78% more distinct, critical test scenarios compared to the baseline algorithm. (2) Our algorithm accurately characterizes critical regions of the system under test, thus identifying the conditions that are likely to lead to system failures.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1016–1026},
numpages = {11},
keywords = {automotive software systems, evolutionary algorithms, search-based software engineering, software testing},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3664476.3670871,
author = {Swierzy, Ben and Boes, Felix and Pohl, Timo and Bungartz, Christian and Meier, Michael},
title = {SoK: Automated Software Testing for TLS Libraries},
year = {2024},
isbn = {9798400717185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664476.3670871},
doi = {10.1145/3664476.3670871},
abstract = {Reusable software components, typically integrated as libraries, are a central paradigm of modern software development. By incorporating a library into their software, developers trust in its quality and its correct and complete implementation. Since errors in a library affect all applications using it, there is a need for quality assurance tools such as automated testing that can be used by library and application developers to verify the functionality. In the past decade, many different systems have been published that focus on the automated analysis of TLS implementations for finding bugs and security vulnerabilities. However, all of these systems focus only on few TLS components and lack a common analysis scenario and inter-approach comparisons. Especially, the amount of manual effort required across the whole analysis process to obtain the root cause of an error is often ignored. In this paper, we survey and categorize literature on automated testing approaches for TLS libraries. The results reveal a heterogeneous landscape of approaches with a trade-off between the manual effort required for setup and for result interpretation, along with major deficits in the considered performance metrics. These imply important future directions to advance the current state of protocol test automation.},
booktitle = {Proceedings of the 19th International Conference on Availability, Reliability and Security},
articleno = {54},
numpages = {12},
keywords = {TLS, automatic testing, secondary study, security protocols},
location = {Vienna, Austria},
series = {ARES '24}
}

@inproceedings{10.1145/3650212.3680334,
author = {Ran, Dezhi and Wang, Hao and Song, Zihe and Wu, Mengzhou and Cao, Yuan and Zhang, Ying and Yang, Wei and Xie, Tao},
title = {Guardian: A Runtime Framework for LLM-Based UI Exploration},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680334},
doi = {10.1145/3650212.3680334},
abstract = {Tests for feature-based UI testing have been indispensable for ensuring the quality of mobile applications (apps for short).        The high manual labor costs to create such tests have led to a strong interest in automated feature-based UI testing, where an approach automatically explores the App under Test (AUT) to find correct sequences of UI events achieving the target test objective, given only a high-level test objective description.        Given that the task of automated feature-based UI testing resembles conventional AI planning problems, large language models (LLMs), known for their effectiveness in AI planning, could be ideal for this task.        However, our study reveals that LLMs struggle with following specific instructions for UI testing and replanning based on new information. This limitation results in reduced effectiveness of LLM-driven solutions for automated feature-based UI testing, despite the use of advanced prompting techniques.                Toward addressing the preceding limitation, we propose Guardian, a runtime system framework to improve the effectiveness of automated feature-based UI testing by offloading computational tasks from LLMs with two major strategies.        First, Guardian refines UI action space that the LLM can plan over, enforcing the instruction following of the LLM by construction.        Second, Guardian deliberately checks whether the gradually enriched information invalidates previous planning by the LLM.        Guardian removes the invalidated UI actions from the UI action space that the LLM can plan over, restores the state of the AUT to the state before the execution of the invalidated UI actions, and prompts the LLM to re-plan with the new UI action space.        We instantiate Guardian with ChatGPT and construct a benchmark named FestiVal with 58 tasks from 23 highly popular apps.        Evaluation results on FestiVal show that Guardian achieves 48.3},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {958–970},
numpages = {13},
keywords = {Android Testing, Large Language Models, Mobile Testing, Runtime System, Sequential Planning, UI Testing},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@article{10.1109/TCBB.2020.2980831,
author = {Singh, Rishav and Ahmed, Tanveer and Kumar, Abhinav and Singh, Amit Kumar and Pandey, Anil Kumar and Singh, Sanjay Kumar},
title = {Imbalanced Breast Cancer Classification Using Transfer Learning},
year = {2021},
issue_date = {Jan.-Feb. 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2020.2980831},
doi = {10.1109/TCBB.2020.2980831},
abstract = {Accurate breast cancer detection using automated algorithms remains a problem within the literature. Although a plethora of work has tried to address this issue, an exact solution is yet to be found. This problem is further exacerbated by the fact that most of the existing datasets are imbalanced, i.e., the number of instances of a particular class far exceeds that of the others. In this paper, we propose a framework based on the notion of transfer learning to address this issue and focus our efforts on histopathological and imbalanced image classification. We use the popular VGG-19 as the base model and complement it with several state-of-the-art techniques to improve the overall performance of the system. With the ImageNet dataset taken as the source domain, we apply the learned knowledge in the target domain consisting of histopathological images. With experimentation performed on a large-scale dataset consisting of 277,524 images, we show that the framework proposed in this paper gives superior performance than those available in the existing literature. Through numerical simulations conducted on a supercomputer, we also present guidelines for work in transfer learning and imbalanced image classification.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = feb,
pages = {83–93},
numpages = {11}
}

@inproceedings{10.1145/3524843.3528097,
author = {Albuquerque, Danyllo and Guimaraes, Everton and Tonin, Graziela and Perkusich, Mirko and Almeida, Hyggo and Perkusich, Angelo},
title = {Comprehending the use of intelligent techniques to support technical debt management},
year = {2022},
isbn = {9781450393041},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524843.3528097},
doi = {10.1145/3524843.3528097},
abstract = {Technical Debt (TD) refers to the consequences of taking shortcuts when developing software. Technical Debt Management (TDM) becomes complex since it relies on a decision process based on multiple and heterogeneous data, which are not straightforward to be synthesized. In this context, there is a promising opportunity to use Intelligent Techniques to support TDM activities since these techniques explore data for knowledge discovery, reasoning, learning, or supporting decision-making. Although these techniques can be used for improving TDM activities, there is no empirical study exploring this research area. This study aims to identify and analyze solutions based on Intelligent Techniques employed to support TDM activities. A Systematic Mapping Study was performed, covering publications between 2010 and 2020. From 2276 extracted studies, we selected 111 unique studies. We found a positive trend in applying Intelligent Techniques to support TDM activities, being Machine Learning, Reasoning Under Uncertainty, and Natural Language Processing the most recurrent ones. Identification, measurement, and monitoring were the more recurrent TDM activities, whereas Design, Code, and Architectural were the most frequently investigated TD types. Although the research area is up-and-coming, it is still in its infancy, and this study provides a baseline for future research.},
booktitle = {Proceedings of the International Conference on Technical Debt},
pages = {21–30},
numpages = {10},
keywords = {technical debt management, technical debt, systematic mapping study, intelligent techniques},
location = {Pittsburgh, Pennsylvania},
series = {TechDebt '22}
}

@inproceedings{10.1145/3524842.3527949,
author = {Hin, David and Kan, Andrey and Chen, Huaming and Babar, M. Ali},
title = {LineVD: statement-level vulnerability detection using graph neural networks},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3527949},
doi = {10.1145/3524842.3527949},
abstract = {Current machine-learning based software vulnerability detection methods are primarily conducted at the function-level. However, a key limitation of these methods is that they do not indicate the specific lines of code contributing to vulnerabilities. This limits the ability of developers to efficiently inspect and interpret the predictions from a learnt model, which is crucial for integrating machine-learning based tools into the software development work-flow. Graph-based models have shown promising performance in function-level vulnerability detection, but their capability for statement-level vulnerability detection has not been extensively explored. While interpreting function-level predictions through explainable AI is one promising direction, we herein consider the statement-level software vulnerability detection task from a fully supervised learning perspective. We propose a novel deep learning framework, LineVD, which formulates statement-level vulnerability detection as a node classification task. LineVD leverages control and data dependencies between statements using graph neural networks, and a transformer-based model to encode the raw source code tokens. In particular, by addressing the conflicting outputs between function-level and statement-level information, LineVD significantly improve the prediction performance without vulnerability status for function code. We have conducted extensive experiments against a large-scale collection of real-world C/C++ vulnerabilities obtained from multiple real-world projects, and demonstrate an increase of 105% in F1-score over the current state-of-the-art.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {596–607},
numpages = {12},
keywords = {software vulnerability detection, program representation, deep learning},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1145/3551349.3563242,
author = {Ngo, Kiet and Nguyen, Vu and Nguyen, Tien},
title = {Research on Test Flakiness: from Unit to System Testing},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3563242},
doi = {10.1145/3551349.3563242},
abstract = {Test flakiness has been a common problem in the software automation testing, affecting the effectiveness and productivity of test automation. Many studies have been published to tackle test flakiness in the software research community, and existing test automation tools provide capabilities to address issues related to test flakiness. This paper describes our review of recent approaches in the research community and popular industrial tools with capabilities to tackle test flakiness. Our review indicates that while many studies focus on test flakiness in unit testing, few address the problem in the end-to-end system testing. On the contrary, industrial test automation tools tend to be more concerned with test flakiness at the system level.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {218},
numpages = {4},
keywords = {unit testing, system testing, software testing, flaky test, end-to-end testing},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3678720.3685317,
author = {Rojas Castillo, Carlos and Marra, Matteo and Gonzalez Boix, Elisa},
title = {Language-Agnostic Debugging for Microcontrollers},
year = {2024},
isbn = {9798400711107},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678720.3685317},
doi = {10.1145/3678720.3685317},
abstract = {With the advent of WebAssembly (Wasm), programming microcontrollers (MCUs) has become possible by leveraging on a wide range of languages (e.g., Rust, AssemblyScript, C, C#, Go, C++) that compile to Wasm. However, current WebAssembly debugging support is still in early development and is designed for applications running on desktop machines, making it too resource-intensive for MCUs. While DWARF and OpenOCD have facilitated language-agnsotic debugging for languages like Rust, Go, and C, these solutions are limited to languages that compile to native machine code and fail to target IoT systems. Consequently, IoT systems often undergo only partial debugging, increasing the likelihood of severe and frequent concurrency and communication bugs.    In this position paper, we explore the challenges and issues associated with language-agnostic debugging. We identify several key requirements for effective language-agnostic debugging, such as the need for over-the-air debugging and the ability to perform distributed debugging operations. Additionally, we present an envisioned language-agnostic debugging approach based on WebAssembly, designed to support the debugging of large-scale distributed IoT systems.},
booktitle = {Proceedings of the 2nd ACM International Workshop on Future Debugging Techniques},
pages = {22–27},
numpages = {6},
keywords = {Distributed, Embedded Devices, IoT, MCU, VM, WebAssembly},
location = {Vienna, Austria},
series = {DEBT 2024}
}

@article{10.1145/3648610,
author = {Elder, Sarah and Rahman, Md Rayhanur and Fringer, Gage and Kapoor, Kunal and Williams, Laurie},
title = {A Survey on Software Vulnerability Exploitability Assessment},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3648610},
doi = {10.1145/3648610},
abstract = {Knowing the exploitability and severity of software vulnerabilities helps practitioners prioritize vulnerability mitigation efforts. Researchers have proposed and evaluated many different exploitability assessment methods. The goal of this research is to assist practitioners and researchers in understanding existing methods for assessing vulnerability exploitability through a survey of exploitability assessment literature. We identify three exploitability assessment approaches: assessments based on original, manual Common Vulnerability Scoring System, automated Deterministic assessments, and automated Probabilistic assessments. Other than the original Common Vulnerability Scoring System, the two most common sub-categories are Deterministic, Program State based, and Probabilistic learning model assessments.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {205},
numpages = {41},
keywords = {Exploitability, software vulnerability}
}

@article{10.1145/3689780,
author = {Wang, Qian and Jung, Ralf},
title = {Rustlantis: Randomized Differential Testing of the Rust Compiler},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689780},
doi = {10.1145/3689780},
abstract = {Compilers are at the core of all computer architecture.  Their middle-end and back-end are full of subtle code that is easy to get wrong.  At the same time, the consequences of compiler bugs can be severe.  Therefore, it is important that we develop techniques to increase our confidence in compiler correctness, and to help find the bugs that inevitably happen.  One promising such technique that has successfully found many compiler bugs in the past is randomized differential testing, a fuzzing approach whereby the same program is executed with different compilers or different compiler settings to detect any unexpected differences in behavior.    We present Rustlantis, the first fuzzer for the Rust programming language that is able to find new correctness bugs in the official Rust compiler.  To avoid having to deal with Rust’s strict type and borrow checker, Rustlantis directly generates MIR, the central IR of the Rust compiler for optimizations.  The program generation strategy of Rustlantis is a combination of statically tracking the state of the program, obscuring the program state for the compiler, and decoy blocks to lead optimizations astray.  This has allowed us to identify 22 previously unknown bugs in the Rust compiler, most of which have been fixed.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {340},
numpages = {27},
keywords = {Compiler testing, Differential fuzzing, Rust}
}

@inproceedings{10.1145/2365324.2365326,
author = {Shepperd, Martin},
title = {The scientific basis for prediction research},
year = {2012},
isbn = {9781450312417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2365324.2365326},
doi = {10.1145/2365324.2365326},
abstract = {In recent years there has been a huge growth in using statistical and machine learning methods to find useful prediction systems for software engineers. Of particular interest is predicting project effort and duration and defect behaviour. Unfortunately though results are often promising no single technique dominates and there are clearly complex interactions between technique, training methods and the problem domain. Since we lack deep theory our research is of necessity experimental. Minimally, as scientists, we need reproducible studies. We also need comparable studies. I will show through a meta-analysis of many primary studies that we are not presently in that situation and so the scientific basis for our collective research remains in doubt. By way of remedy I will argue that we need to address these issues of reporting protocols and expertise plus ensure blind analysis is routine.},
booktitle = {Proceedings of the 8th International Conference on Predictive Models in Software Engineering},
pages = {1–2},
numpages = {2},
keywords = {defect prediction, empirical research, machine learning, software metrics},
location = {Lund, Sweden},
series = {PROMISE '12}
}

@inproceedings{10.1145/3611643.3613868,
author = {Wang, Chaozheng and Lu, Haochuan and Gao, Cuiyun and Li, Zongjie and Xiong, Ting and Deng, Yuetang},
title = {A Unified Framework for Mini-game Testing: Experience on WeChat},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613868},
doi = {10.1145/3611643.3613868},
abstract = {Mobile games play an increasingly important role in our daily life. The quality of mobile games can substantially affect the user experience and game revenue. Different from traditional mobile games, the mini-games provided by our partner, Tencent, are embedded in the mobile app WeChat, so users do not need to install specific game apps and can directly play the games in the app. Due to the convenient installation, WeChat has attracted large numbers of developers to design and publish on the mini-game platform in the app. Until now, the platform has more than one hundred thousand published mini-games. Manually testing all the mini-games requires enormous effort and is impractical. There exist automated game testing methods; however, they are difficult to be applied for testing mini-games for the following reasons: 1) Effective game testing heavily relies on prior knowledge about game operations and extraction of GUI widget trees. However, this knowledge is specific and not always applicable when testing a large number of mini-games with complex game engines (e.g., Unity). 2) The highly diverse GUI widget design of mini-games deviates significantly from that of mobile apps. Such issue prevents the existing image-based GUI widget detection techniques from effectively detecting widgets in mini-games. To address the aforementioned issues, we propose a unified framework for black-box mini-game testing named iExplorer. iExplorer involves a mixed GUI widget detection approach incorporating both deep learning-based object detection and edge aggregation-based segmentation for detecting GUI widgets in mini-games. A category-aware testing strategy is then proposed for testing mini-games, with different categories of widgets (e.g., sliding and clicking widgets) considered. iExplorer has been deployed in for more than six months. In the past 30 days, iExplorer has tested large-scale mini-games (i.e., 76,000) and successfully found 22,144 real bugs.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1623–1634},
numpages = {12},
keywords = {GUI widget detection, game testing},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/1595696.1595748,
author = {Vangala, Vipindeep and Czerwonka, Jacek and Talluri, Phani},
title = {Test case comparison and clustering using program profiles and static execution},
year = {2009},
isbn = {9781605580012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1595696.1595748},
doi = {10.1145/1595696.1595748},
abstract = {Selection of diverse test cases and elimination of duplicates are two major problems in product testing life cycle, especially in sustained engineering environment. In order to solve these, we introduce a framework of test case comparison metrics which will quantitatively describe the distance between any arbitrary test case pair of an existing test suite, allowing various test case analysis applications. We combine program profiles from test execution, static analysis and statistical techniques to capture various aspects of test execution and compute a specialized test case distance measurement. Using these distance metrics, we drive a customized hierarchical test suite clustering algorithm that groups similar test cases together. We present an industrial strength framework called SPIRiT that works at binary level, implementing different metrics in the form of coverage, control, data, def-use, temporal variances and does test case clustering. This is step towards integrating runtime analysis, static analysis, statistical techniques and machine learning to drive new generation of test suite analysis algorithms.},
booktitle = {Proceedings of the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
pages = {293–294},
numpages = {2},
keywords = {testing, sustained engineering, static analysis, machine learning},
location = {Amsterdam, The Netherlands},
series = {ESEC/FSE '09}
}

@inproceedings{10.1145/3597926.3598105,
author = {Wang, Zihan and Nie, Pengbo and Miao, Xinyuan and Chen, Yuting and Wan, Chengcheng and Bu, Lei and Zhao, Jianjun},
title = {GenCoG: A DSL-Based Approach to Generating Computation Graphs for TVM Testing},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598105},
doi = {10.1145/3597926.3598105},
abstract = {TVM is a popular deep learning (DL) compiler. It is designed for compiling DL models, which are naturally computation graphs, and as well promoting the efficiency of DL computation. State-of-the-art methods, such as Muffin and NNSmith, allow developers to generate computation graphs for testing DL compilers. However, these techniques are inefficient — their generated computation graphs are either type-invalid or inexpressive, and hence not able to test the core functionalities of a DL compiler.  

To tackle this problem, we propose GenCoG, a DSL-based approach to generating computation graphs for TVM testing. GenCoG is composed of (1) GenCoGL, a domain-specific language for specifying type constraints of operators, and (2) an approach that concolically solves type constraints and incrementally generates computation graphs of high expressivity. We implement and evaluate GenCoG on TVM releases. Our results show that GenCoG is effective in generating valid and expressive computation graphs — all of the GenCoG-generated graphs pass type checking, a critical graph validation stage; letting the graphs’ expressivity be measured by their vertex and edge diversities, GenCoG outperforms state-of-the-arts by achieving 1.65~6.93\texttimes{} in vertex diversity and 1.06~7.08\texttimes{} in edge diversity, respectively. Furthermore, GenCoG has detected 16 bugs in TVM v0.8 and v0.9, with 14 confirmed and 12 fixed.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {904–916},
numpages = {13},
keywords = {Deep Learning Compiler, Constraint Solving, Computation Graph Generation},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@article{10.1145/3643737,
author = {Song, Yahui and Gao, Xiang and Li, Wenhua and Chin, Wei-Ngan and Roychoudhury, Abhik},
title = {ProveNFix: Temporal Property-Guided Program Repair},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643737},
doi = {10.1145/3643737},
abstract = {Model checking has been used traditionally for finding violations of temporal properties. Recently, testing or fuzzing approaches have also been applied to software systems to find temporal property violations. However, model checking suffers from state explosion, while fuzzing can only partially cover program paths. Moreover, once a violation is found, the fix for the temporal error is usually manual. In this work, we develop the first compositional static analyzer for temporal properties, and the analyzer supports a proof-based repair strategy to fix temporal bugs automatically. To enable a more flexible specification style for temporal properties, on top of the classic pre/post-conditions, we allow users to write a future-condition to modularly express the expected behaviors after the function call. Instead of requiring users to write specifications for each procedure, our approach automatically infers the procedure’s specification according to user-supplied specifications for a small number of primitive APIs. We further devise a term rewriting system to check the actual behaviors against its inferred specification. Our method supports the analysis of 1) memory usage bugs, 2) unchecked return values, 3) resource leaks, etc., with annotated specifications for 17 primitive APIs, and detects 515 vulnerabilities from over 1 million lines of code ranging from ten real-world C projects. Intuitively, the benefit of our approach is that a small set of properties can be specified once and used to analyze/repair a large number of programs. Experimental results show that our tool, ProveNFix, detects 72.2% more true alarms than the latest release of the Infer static analyzer. Moreover, we show the effectiveness of our repair strategy when compared to other state-of-the-art systems — fixing 5% more memory leaks than SAVER, 40% more resource leaks than FootPatch, and with a 90% fix rate for null pointer dereferences.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {11},
numpages = {23},
keywords = {Automated Program Repair, Future-Conditions, Program Analysis}
}

@inproceedings{10.1145/3650212.3680308,
author = {Fan, Zhiyu and Ruan, Haifeng and Mechtaev, Sergey and Roychoudhury, Abhik},
title = {Oracle-Guided Program Selection from Large Language Models},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680308},
doi = {10.1145/3650212.3680308},
abstract = {While large language models (LLMs) have shown significant advancements in code generation, their susceptibility to producing incorrect code poses a significant challenge to the adoption of LLM-generated programs. This issue largely stems from the reliance on natural language descriptions as informal oracles in code generation. Current strategies to mitigate this involve selecting the best program from multiple LLM-generated alternatives, judged by criteria like the consistency of their execution results on an LLM-generated test suite. However, this approach has crucial limitations: (1) LLMs often generate redundant tests or tests that cannot distinguish between correct and incorrect solutions, (2) the used consistency criteria, such as the majority vote, fail to foster developer trust due to the absence of transparent rationale behind the made choices. In this work, we propose a new perspective on increasing the quality of LLM-generated code via program selection using the LLM as a test oracle. Our method is based on our experimentally confirmed observation that LLMs serve more effectively as oracles when tasked with selecting the correct output from multiple choices. Leveraging this insight, we first generate distinguishing inputs that capture semantic discrepancies of programs sampled from an LLM, and record outputs produced by the programs on these inputs. An LLM then selects the most likely to be correct output from these, guided by the natural language problem description. We implemented this idea in a tool LLMCodeChoice and evaluated its accuracy in generating and selecting standalone programs. Our experiments demonstrated its effectiveness in improving pass@1 by 3.6-7% on HumanEval and MBPP benchmarks compared to the state-of-art CodeT. Most interestingly, the selected input-output specifications helped us to uncover incompleteness and ambiguities in task descriptions and also identify incorrect ground-truth implementations in the benchmarks.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {628–640},
numpages = {13},
keywords = {code generation, differential testing, large language model, oracle inference},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3674805.3690751,
author = {Deng, Yang and Wang, Bangchao and Zou, Zhiyuan and Ye, Luyao},
title = {PromptLink: Multi-template prompt learning with adversarial training for issue-commit link recovery},
year = {2024},
isbn = {9798400710476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674805.3690751},
doi = {10.1145/3674805.3690751},
abstract = {In recent years, Prompt Learning, based on pre-training, prompting, and prediction, has achieved significant success in natural language processing (NLP). The current issue-commit link recovery (ILR) method converts the ILR into a classification task using pre-trained language models (PLMs) and dedicated neural networks. However, due to inconsistencies between the ILR task and PLMs, these methods not fully leverage the semantic information in PLMs. To imitate the above problem, we make the first trial of the new paradigm to propose a Multi-template prompt learning method with adversarial training for issue-commit link recovery (PromptLink), which transforms the ILR task into a cloze task through the template. Specifically, a Multi-template PromptLink is designed to enhance the generalisation capability by integrating various templates and adopting adversarial training to mitigate the model overfitting. Experiments are conducted on six open-source projects and comprehensively evaluated across six commonly measures. The results show that PromptLink achieves an average F1 of 96.10%, Precision of 96.49%, Recall of 95.92%, MCC of 94.04%, AUC of 96.05%, and ACC of 98.15%, significantly outperforming existing state-of-the-art methods on all measures. Overall, PromptLink not only enhances performance and generalisation but also emerges new ideas and methods for future research. The source code of PromptLink is available at https://figshare.com/s/6130d42ff464c579cdec.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {461–467},
numpages = {7},
keywords = {Issue-commit link recovery, Natural language processing, Pre-trained language model, Prompt learning},
location = {Barcelona, Spain},
series = {ESEM '24}
}

@inproceedings{10.1145/3650212.3680364,
author = {Go, Gwihwan and Zhou, Chijin and Zhang, Quan and Zou, Xiazijian and Shi, Heyuan and Jiang, Yu},
title = {Towards More Complete Constraints for Deep Learning Library Testing via Complementary Set Guided Refinement},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680364},
doi = {10.1145/3650212.3680364},
abstract = {Deep learning library is important in AI systems. Recently, many works have been proposed to ensure its reliability.         They often model inputs of tensor operations as constraints to guide the generation of test cases.        However, these constraints may narrow the search space, resulting in incomplete testing.        This paper introduces a complementary set-guided refinement that can enhance the completeness of constraints.        The basic idea is to see if the complementary set of constraints yields valid test cases. If so, the original constraint is incomplete and needs refinement.         Based on this idea, we design an automatic constraint refinement tool, DeepConstr, which adopts a genetic algorithm to refine constraints for better completeness.        We evaluated it on two DL libraries, PyTorch and TensorFlow.        DeepConstr discovered 84 unknown bugs, out of which 72 were confirmed, with 51 fixed.         Compared to state-of-the-art fuzzers, DeepConstr increased coverage for 43.44% of operators supported by NNSmith, and 59.16% of operators supported by NeuRI.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1338–1350},
numpages = {13},
keywords = {DL library, Fuzzing, Large Language Model},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3555776.3577762,
author = {Kim, Jisung and Lee, Byungjeong},
title = {MCRepair: Multi-Chunk Program Repair via Patch Optimization with Buggy Block},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577762},
doi = {10.1145/3555776.3577762},
abstract = {Automated program repair (APR) is a technology that identifies and repairs bugs automatically. However, repairing multi-chunk bugs remains a long-standing and challenging problem because an APR technique must consider dependencies and then reduce the large patch space. In addition, little is known about how to combine individual candidate patches even though multi-chunk bugs require combinations. Therefore, we propose a novel APR technique called multi-code repair (MCRepair), which applies a buggy block, patch optimization, and CodeBERT to target multi-chunk bugs. A buggy block is a novel method that binds buggy chunks into a multi-buggy chunk and preprocesses the chunk with its buggy contexts for patch space reduction and dependency problems. Patch optimization is a novel strategy that effectively combines the generated candidate patches with patch space reduction. In addition, CodeBERT, a BERT for source code datasets, is fine-tuned to address the lack of datasets and out-of-vocabulary problems. We conducted several experiments to evaluate our approach on six project modules of Defects4J. In the experiments using Defects4J, MCRepair repaired 66 bugs, including 22 multi-chunk bugs. Moreover, it fixed 19 unique bugs, including nine multi-chunk bugs, and improved 45--266% performance than the baselines.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1508–1515},
numpages = {8},
keywords = {deep learning, patch optimization, buggy block, automated program repair},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1109/ICSE.2017.65,
author = {Liu, Peng and Zhang, Xiangyu and Pistoia, Marco and Zheng, Yunhui and Marques, Manoel and Zeng, Lingfei},
title = {Automatic text input generation for mobile testing},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.65},
doi = {10.1109/ICSE.2017.65},
abstract = {Many designs have been proposed to improve the automated mobile testing. Despite these improvements, providing appropriate text inputs remains a prominent obstacle, which hinders the large-scale adoption of automated testing approaches. The key challenge is how to automatically produce the most relevant text in a use case context. For example, a valid website address should be entered in the address bar of a mobile browser app to continue the testing of the app; a singer's name should be entered in the search bar of a music recommendation app. Without the proper text inputs, the testing would get stuck. We propose a novel deep learning based approach to address the challenge, which reduces the problem to a minimization problem. Another challenge is how to make the approach generally applicable to both the trained apps and the untrained apps. We leverage the Word2Vec model to address the challenge. We have built our approaches as a tool and evaluated it with 50 iOS mobile apps including Firefox and Wikipedia. The results show that our approach significantly outperforms existing automatic text input generation methods.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {643–653},
numpages = {11},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@inproceedings{10.1145/3485832.3488018,
author = {Koo, Hyungjoon and Park, Soyeon and Kim, Taesoo},
title = {A Look Back on a Function Identification Problem},
year = {2021},
isbn = {9781450385794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485832.3488018},
doi = {10.1145/3485832.3488018},
abstract = {A function recognition problem serves as a basis for further binary analysis and many applications. Although common challenges for function detection are well known, prior works have repeatedly claimed a noticeable result with a high precision and recall. In this paper, we aim to fill the void of what has been overlooked or misinterpreted by closely looking into the previous datasets, metrics, and evaluations with varying case studies. Our major findings are that i)&nbsp;a common corpus like GNU utilities is insufficient to represent the effectiveness of function identification, ii)&nbsp;it is difficult to claim, at least in the current form, that an ML-oriented approach is scientifically superior to deterministic ones like IDA or Ghidra, iii)&nbsp;the current metrics may not be reasonable enough to measure varying function detection cases, and iv)&nbsp;the capability of recognizing functions depends on each tool’s strategic or peculiar choice. We perform re-evaluation of existing approaches on our own dataset, demonstrating that not a single state-of-the-art tool dominates all the others. In conclusion, a function detection problem has not yet been fully addressed, and we need a better methodology and metric to make advances in the field of function identification.},
booktitle = {Proceedings of the 37th Annual Computer Security Applications Conference},
pages = {158–168},
numpages = {11},
keywords = {Binary, Function Identification, Function Recognition, Lookback, ML-oriented},
location = {Virtual Event, USA},
series = {ACSAC '21}
}

@inproceedings{10.1145/3689031.3717477,
author = {Oliveira, Jo\~{a}o and Gon\c{c}alves, Jo\~{a}o and Matos, Miguel},
title = {HawkSet: Automatic, Application-Agnostic, and Efficient Concurrent PM Bug Detection},
year = {2025},
isbn = {9798400711961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689031.3717477},
doi = {10.1145/3689031.3717477},
abstract = {Persistent Memory (PM) enables the development of fast, persistent applications without employing costly HDD/SSD-based I/O operations. Since caches are volatile and CPUs may reorder and stall memory accesses for performance, developers must use low-level instructions to ensure a consistent state in case of a crash. Failure to do so can result in data corruption, data loss, or undefined behavior. In concurrent executions, this exposes a new class of bugs.HawkSet is an automatic, application-agnostic, and efficient tool to detect concurrent PM bugs. HawkSet uses lockset analysis, and automatic binary instrumentation to find all the bugs detected by the state-of-the-art tools and 7 previously unknown bugs. This is achieved without requiring application-specific knowledge or models, nor specialized debugging artifacts or guided executions. Compared to the state-of-the-art, HawkSet offers up to a 159x speedup, and consistently detects harder-to-reach bugs, where a rare interleaving is required.},
booktitle = {Proceedings of the Twentieth European Conference on Computer Systems},
pages = {1092–1108},
numpages = {17},
keywords = {Bug Detection, Concurrency, Persistent Memory},
location = {Rotterdam, Netherlands},
series = {EuroSys '25}
}

@inproceedings{10.1145/1315245.1315307,
author = {Brickell, Justin and Porter, Donald E. and Shmatikov, Vitaly and Witchel, Emmett},
title = {Privacy-preserving remote diagnostics},
year = {2007},
isbn = {9781595937032},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1315245.1315307},
doi = {10.1145/1315245.1315307},
abstract = {We present an efficient protocol for privacy-preserving evaluation of diagnostic programs, represented as binary decision trees or branching programs. The protocol applies a branching diagnostic program with classification labels in the leaves to the user's attribute vector. The user learns only the label assigned by the program to his vector; the diagnostic program itself remains secret. The program's owner does not learn anything. Our construction is significantly more efficient than those obtained by direct application of generic secure multi-party computation techniques.We use our protocol to implement a privacy-preserving version of the Clarify system for software fault diagnosis, and demonstrate that its performance is acceptable for many practical scenarios.},
booktitle = {Proceedings of the 14th ACM Conference on Computer and Communications Security},
pages = {498–507},
numpages = {10},
keywords = {privacy, diagnostics, data mining, branching programs},
location = {Alexandria, Virginia, USA},
series = {CCS '07}
}

@inproceedings{10.1145/3382494.3422172,
author = {Codabux, Zadia and Dutchyn, Christopher},
title = {Profiling Developers Through the Lens of Technical Debt},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3422172},
doi = {10.1145/3382494.3422172},
abstract = {Context: Technical Debt needs to be managed to avoid disastrous consequences, and investigating developers' habits concerning technical debt management is invaluable information in software development. Objective: This study aims to characterize how developers manage technical debt based on the code smells they induce and the refactorings they apply. Method: We mined a publicly-available Technical Debt dataset for Git commit information, code smells, coding violations, and refactoring activities for each developer of a selected project. Results: By combining this information, we profile developers to recognize prolific coders, highlight activities that discriminate among developer roles (reviewer, lead, architect), and estimate coding maturity and technical debt tolerance.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {38},
numpages = {6},
keywords = {Technical Debt, Refactoring, Open Source Software, Mining Software Repositories, Developer Characterization, Code Smell},
location = {Bari, Italy},
series = {ESEM '20}
}

@proceedings{10.1145/3540250,
title = {ESEC/FSE 2022: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of all members of the organizing committee, we are delighted to welcome everyone to the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2022. The event continues the long, distinguished ESEC/FSE tradition of presenting the most innovative research, and facilitating interactions between scientists and engineers who are passionate about advancing the theory and practice of software engineering.},
location = {Singapore, Singapore}
}

@article{10.1145/3647994,
author = {Xia, Xinmeng and Feng, Yang and Shi, Qingkai and Jones, James A. and Zhang, Xiangyu and Xu, Baowen},
title = {Enumerating Valid Non-Alpha-Equivalent Programs for Interpreter Testing},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3647994},
doi = {10.1145/3647994},
abstract = {Skeletal program enumeration (SPE) can generate a great number of test programs for validating the correctness of compilers or interpreters. The classic SPE generates programs by exhaustively enumerating all possible variable usage patterns into a given syntactic structure. Even though it is capable of producing many test programs, the exhaustive enumeration strategy generates a large number of invalid programs, which may waste plenty of testing time and resources. To address the problem, this article proposes a tree-based SPE technique. Compared to the state-of-the-art, the key merit of the tree-based approach is that it allows us to take the dependency information into consideration when producing test programs and, thus, make it possible to (1) directly generate non-equivalent programs and (2) apply dominance relations to eliminate invalid test programs that have undefined variables. Hence, our approach significantly saves the cost of the na\"{\i}ve SPE approach. We have implemented our approach into an automated testing tool, IFuzzer, and applied it to test eight different implementations of Python interpreters, including CPython, PyPy, IronPython, Jython, RustPython, GPython, Pyston, and Codon. In three months of fuzzing, IFuzzer detected 142 bugs, of which 87 have been confirmed to be previously unknown bugs, of which 34 have been fixed. Compared to the state-of-the-art SPE techniques, IFuzzer takes only 61.0% of the time cost given the same number of testing seeds and improves 5.3% source code function coverage in the same time budget of testing.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {118},
numpages = {31},
keywords = {Interpreter testing, fuzz testing, program enumeration}
}

@inproceedings{10.1145/3540250.3549165,
author = {Ni, Chao and Wang, Wei and Yang, Kaiwen and Xia, Xin and Liu, Kui and Lo, David},
title = {The best of both worlds: integrating semantic features with expert features for defect prediction and localization},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549165},
doi = {10.1145/3540250.3549165},
abstract = {To improve software quality, just-in-time defect prediction (JIT-DP) (identifying defect-inducing commits) and just-in-time defect localization (JIT-DL) (identifying defect-inducing code lines in commits) have been widely studied by learning semantic features or expert features respectively, and indeed achieved promising performance. Semantic features and expert features describe code change commits from different aspects, however, the best of the two features have not been fully explored together to boost the just-in-time  
defect prediction and localization in the literature yet. Additional, JIT-DP identifies defects at the coarse commit level, while as the  
consequent task of JIT-DP, JIT-DL cannot achieve the accurate localization of defect-inducing code lines in a commit without JIT-DP.  
We hypothesize that the two JIT tasks can be combined together to boost the accurate prediction and localization of defect-inducing  
commits by integrating semantic features with expert features. Therefore, we propose to build a unified model, JIT-Fine, for the  
just-in-time defect prediction and localization by leveraging the best of semantic features and expert features. To assess the feasibility  
of JIT-Fine, we first build a large-scale line-level manually labeled dataset, JIT-Defects4J. Then, we make a comprehensive comparison  
with six state-of-the-art baselines under various settings using ten performance measures grouped into two types: effort-agnostic  
and effort-aware. The experimental results indicate that JIT-Fine can outperform all state-of-the-art baselines on both JIT-DP and JITDL  
tasks in terms of ten performance measures with a substantial improvement (i.e., 10%-629% in terms of effort-agnostic measures on JIT-DP, 5%-54% in terms of effort-aware measures on JIT-DP, and 4%-117% in terms of effort-aware measures on JIT-DL).},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {672–683},
numpages = {12},
keywords = {Just-In-Time, Defect Prediction, Defect Localization, Deep Learning},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3475716.3475791,
author = {Gesi, Jiri and Li, Jiawei and Ahmed, Iftekhar},
title = {An Empirical Examination of the Impact of Bias on Just-in-time Defect Prediction},
year = {2021},
isbn = {9781450386654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475716.3475791},
doi = {10.1145/3475716.3475791},
abstract = {Background: Just-In-Time (JIT) defect prediction models predict if a commit will introduce defects in the future. DeepJIT and CC2Vec are two state-of-the-art JIT Deep Learning (DL) techniques. Usually, defect prediction techniques are evaluated, treating all training data equally. However, data is usually imbalanced not only in terms of the overall class label (e.g., defect and non-defect) but also in terms of characteristics such as File Count, Edit Count, Multiline Comments, Inward Dependency Sum etc. Prior research has investigated the impact of class imbalance on prediction technique's performance but not the impact of imbalance of other characteristics. Aims: We aim to explore the impact of different commit related characteristic's imbalance on DL defect prediction. Method: We investigated different characteristic's impact on the overall performance of DeepJIT and CC2Vec. We also propose a Siamese network based few-shot learning framework for JIT defect prediction (SifterJIT) combining Siamese network and DeepJIT. Results: Our results show that DeepJIT and CC2Vec lose out on the performance by around 20% when trained and tested on imbalanced data. However, SifterJIT can outperform state-of-the-art DL techniques with an average of 8.65% AUC score, 11% precision, and 6% F1-score improvement. Conclusions: Our results highlight that dataset imbalanced in terms of commit characteristics can significantly impact prediction performance, and few-shot learning based techniques can help alleviate the situation.},
booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {7},
numpages = {12},
keywords = {software engineering, few-shot learning, defect prediction, Deep learning},
location = {Bari, Italy},
series = {ESEM '21}
}

@article{10.1145/3417986,
author = {C., Marimuthu and Chandrasekaran, K. and Chimalakonda, Sridhar},
title = {Energy Diagnosis of Android Applications: A Thematic Taxonomy and Survey},
year = {2020},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3417986},
doi = {10.1145/3417986},
abstract = {The abnormal energy consumption of Android applications is a significant problem faced by developers and users. In recent years, researchers have invested their efforts to develop energy diagnosis tools that pinpoint and fix the energy bugs from source code automatically. These tools use traditional software engineering methods such as program analysis, refactoring, software repair, and bug localization to diagnose energy inefficiencies. Existing surveys focus only on energy measurement techniques and profiling tools and do not consider automated energy diagnosis tools. Therefore, this article organizes state of the art by surveying 25 relevant studies on Android applications’ automatic energy diagnosis. Further, this survey presents a systematic thematic taxonomy of existing approaches from a software engineering perspective. The taxonomy presented in this article would serve as a body of knowledge and help researchers and developers to understand the state of the field better. The future research directions discussed in this article might help prospective researchers to identify suitable topics to improve the current research work in this field.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {117},
numpages = {36},
keywords = {thematic taxonomy, state of the art, android application, Automated energy diagnosis}
}

@inproceedings{10.1145/3611643.3613867,
author = {Liang, Xiaoyun and Qi, Jiayi and Gao, Yongqiang and Peng, Chao and Yang, Ping},
title = {AG3: Automated Game GUI Text Glitch Detection Based on Computer Vision},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613867},
doi = {10.1145/3611643.3613867},
abstract = {With the advancement of device software and hardware performance, and the evolution of game engines, an increasing number of emerging high-quality games are captivating game players from all around the world who speak different languages. However, due to the vast fragmentation of the device and platform market, a well-tested game may still experience text glitches when installed on a new device with an unseen screen resolution and system version, which can significantly impact the user experience. In our testing pipeline, current testing techniques for identifying multilingual text glitches are laborious and inefficient. In this paper, we present AG3, which offers intelligent game traversal, precise visual text glitch detection, and integrated quality report generation capabilities. Our empirical evaluation and internal industrial deployment demonstrate that AG3 can detect various real-world multilingual text glitches with minimal human involvement.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1879–1890},
numpages = {12},
keywords = {Deep Learning, Software Testing, Visual Test Oracle},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3625549.3658685,
author = {Maurya, Avinash and Underwood, Robert and Rafique, M. Mustafa and Cappello, Franck and Nicolae, Bogdan},
title = {DataStates-LLM: Lazy Asynchronous Checkpointing for Large Language Models},
year = {2024},
isbn = {9798400704130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625549.3658685},
doi = {10.1145/3625549.3658685},
abstract = {LLMs have seen rapid adoption in all domains. They need to be trained on high-end high-performance computing (HPC) infrastructures and ingest massive amounts of input data. Unsurprisingly, at such a large scale, unexpected events (e.g., failures of components, instability of the software, undesirable learning patterns, etc.), are frequent and typically impact the training in a negative fashion. Thus, LLMs need to be checkpointed frequently so that they can be rolled back to a stable state and subsequently fine-tuned. However, given the large sizes of LLMs, a straightforward checkpointing solution that directly writes the model parameters and optimizer state to persistent storage (e.g., a parallel file system), incurs significant I/O overheads. To address this challenge, in this paper we study how to reduce the I/O overheads for enabling fast and scalable checkpointing for LLMs that can be applied at high frequency (up to the granularity of individual iterations) without significant impact on the training process. Specifically, we introduce a lazy asynchronous multi-level approach that takes advantage of the fact that the tensors making up the model and optimizer state shards remain immutable for extended periods of time, which makes it possible to copy their content in the background with minimal interference during the training process. We evaluate our approach at scales of up to 180 GPUs using different model sizes, parallelism settings, and checkpointing frequencies. The results show up to 48\texttimes{} faster checkpointing and 2.2\texttimes{} faster end-to-end training runtime compared with the state-of-art checkpointing approaches.},
booktitle = {Proceedings of the 33rd International Symposium on High-Performance Parallel and Distributed Computing},
pages = {227–239},
numpages = {13},
keywords = {LLMs and transformers, scalable checkpointing, asynchronous multilevel checkpointing},
location = {Pisa, Italy},
series = {HPDC '24}
}

@article{10.1145/3448977,
author = {Laranjeiro, Nuno and Agnelo, Jo\~{a}o and Bernardino, Jorge},
title = {A Systematic Review on Software Robustness Assessment},
year = {2021},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3448977},
doi = {10.1145/3448977},
abstract = {Robustness is the degree to which a certain system or component can operate correctly in the presence of invalid inputs or stressful environmental conditions. With the increasing complexity and widespread use of computer systems, obtaining assurances regarding their robustness has become of vital importance. This survey discusses the state of the art on software robustness assessment, with emphasis on key aspects like types of systems being evaluated, assessment techniques used, the target of the techniques, the types of faults used, and how system behavior is classified. The survey concludes with the identification of gaps and open challenges related with robustness assessment.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {89},
numpages = {65},
keywords = {robustness testing, robustness evaluation, Software robustness}
}

@inproceedings{10.1145/3377811.3380409,
author = {Rahman, Akond and Farhana, Effat and Parnin, Chris and Williams, Laurie},
title = {Gang of eight: a defect taxonomy for infrastructure as code scripts},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380409},
doi = {10.1145/3377811.3380409},
abstract = {Defects in infrastructure as code (IaC) scripts can have serious consequences, for example, creating large-scale system outages. A taxonomy of IaC defects can be useful for understanding the nature of defects, and identifying activities needed to fix and prevent defects in IaC scripts. The goal of this paper is to help practitioners improve the quality of infrastructure as code (IaC) scripts by developing a defect taxonomy for IaC scripts through qualitative analysis. We develop a taxonomy of IaC defects by applying qualitative analysis on 1,448 defect-related commits collected from open source software (OSS) repositories of the Openstack organization. We conduct a survey with 66 practitioners to assess if they agree with the identified defect categories included in our taxonomy. We quantify the frequency of identified defect categories by analyzing 80,425 commits collected from 291 OSS repositories spanning across 2005 to 2019.Our defect taxonomy for IaC consists of eight categories, including a category specific to IaC called idempotency (i.e., defects that lead to incorrect system provisioning when the same IaC script is executed multiple times). We observe the surveyed 66 practitioners to agree most with idempotency. The most frequent defect category is configuration data i.e., providing erroneous configuration data in IaC scripts. Our taxonomy and the quantified frequency of the defect categories may help in advancing the science of IaC script quality.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {752–764},
numpages = {13},
keywords = {bug, category, configuration as code, configuration scripts, defect, devops, infrastructure as code, puppet, software quality, taxonomy},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3377812.3382162,
author = {Gupta, Shashij},
title = {Machine translation testing via pathological invariance},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3382162},
doi = {10.1145/3377812.3382162},
abstract = {Due to the rapid development of deep neural networks, in recent years, machine translation software has been widely adopted in people's daily lives, such as communicating with foreigners or understanding political news from the neighbouring countries. However, machine translation software could return incorrect translations because of the complexity of the underlying network. To address this problem, we introduce a novel methodology called PaInv for validating machine translation software. Our key insight is that sentences of different meanings should not have the same translation (i.e., pathological invariance). Specifically, PaInv generates syntactically similar but semantically different sentences by replacing one word in the sentence and filter out unsuitable sentences based on both syntactic and semantic information. We have applied PaInv to Google Translate using 200 English sentences as input with three language settings: English→Hindi, English→Chinese, and English→German. PaInv can accurately find 331 pathological invariants in total, revealing more than 100 translation errors.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {107–109},
numpages = {3},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@proceedings{10.1145/3678720,
title = {DEBT 2024: Proceedings of the 2nd ACM International Workshop on Future Debugging Techniques},
year = {2024},
isbn = {9798400711107},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to DEBT'24: the second Workshop on Future Debugging Techniques colocated with ECOOP 2024, which took place on September 19th, 2024, in Vienna, Austria. The goal of the DEBT workshop series is to gather researchers from all areas in the field of programming languages to discuss novel ideas to define the debugging techniques and tools of the future.},
location = {Vienna, Austria}
}

@article{10.1109/TASLP.2024.3485555,
author = {Wang, Wenxuan and Jiao, Wenxiang and Wang, Shuo and Tu, Zhaopeng and Lyu, Michael R.},
title = {Understanding and Mitigating the Uncertainty in Zero-Shot Translation},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3485555},
doi = {10.1109/TASLP.2024.3485555},
abstract = {Zero-shottranslation is a promising direction for building a comprehensive multilingual neural machine translation (MNMT) system. However, its quality is still not satisfactory due to off-target issues. In this paper, we aim to understand and alleviate the off-target issues from the perspective of uncertainty in zero-shot translation. By carefully examining the translation output and model confidence, we identify two uncertainties that are responsible for the off-target issues, namely, extrinsic data uncertainty and intrinsic model uncertainty. Based on the observations, we propose two lightweight and complementary approaches to denoise the training data for model training and explicitly penalize the off-target translations by unlikelihood training during model training. Extensive experiments on both balanced and imbalanced datasets show that our approaches significantly improve the performance of zero-shot translation over strong MNMT baselines.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {4894–4904},
numpages = {11}
}

@inproceedings{10.1145/3613904.3642517,
author = {Yang, Jackie (Junrui) and Shi, Yingtian and Zhang, Yuhan and Li, Karina and Rosli, Daniel Wan and Jain, Anisha and Zhang, Shuning and Li, Tianshi and Landay, James A. and Lam, Monica S.},
title = {ReactGenie: A Development Framework for Complex Multimodal Interactions Using Large Language Models},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642517},
doi = {10.1145/3613904.3642517},
abstract = {By combining voice and touch interactions, multimodal interfaces can surpass the efficiency of either modality alone. Traditional multimodal frameworks require laborious developer work to support rich multimodal commands where the user’s multimodal command involves possibly exponential combinations of actions/function invocations. This paper presents ReactGenie, a programming framework that better separates multimodal input from the computational model to enable developers to create efficient and capable multimodal interfaces with ease. ReactGenie translates multimodal user commands into NLPL (Natural Language Programming Language), a programming language we created, using a neural semantic parser based on large-language models. The ReactGenie runtime interprets the parsed NLPL and composes primitives in the computational model to implement complex user commands. As a result, ReactGenie allows easy implementation and unprecedented richness in commands for end-users of multimodal apps. Our evaluation showed that 12 developers can learn and build a non-trivial ReactGenie application in under 2.5 hours on average. In addition, compared with a traditional GUI, end-users can complete tasks faster and with less task load using ReactGenie apps.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {483},
numpages = {23},
keywords = {development frameworks, large-language model, multimodal interactions, natural language processing, programming framework},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3597926.3598108,
author = {Zhang, Xiaodong and Zhao, Wei and Sun, Yang and Sun, Jun and Shen, Yulong and Dong, Xuewen and Yang, Zijiang},
title = {Testing Automated Driving Systems by Breaking Many Laws Efficiently},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598108},
doi = {10.1145/3597926.3598108},
abstract = {An automated driving system (ADS), as the brain of an autonomous vehicle (AV), should be tested thoroughly ahead of deployment.  
ADS must satisfy a complex set of rules to ensure road safety, e.g., the existing traffic laws and possibly future laws that are dedicated to AVs.  
To comprehensively test an ADS, we would like to systematically discover diverse scenarios in which certain traffic law is violated. The challenge is that (1) there are many traffic laws (e.g., 13 testable articles in Chinese traffic laws and 16 testable articles in Singapore traffic laws, with 81 and 43 violation situations respectively); and (2) many of traffic laws are only relevant in complicated specific scenarios.  

Existing approaches to testing ADS either focus on simple oracles such as no-collision or have limited capacity in generating diverse law-violating scenarios.  
In this work, we propose ABLE, a new ADS testing method inspired by the success of GFlowNet, which Aims to Break many Laws Efficiently by generating diverse scenarios.  
Different from vanilla GFlowNet, ABLE drives the testing process with dynamically updated testing objectives (based on a robustness semantics of signal temporal logic) as well as active learning, so as to effectively explore the vast search space.  
We evaluate ABLE based on Apollo and LGSVL, and the results show that ABLE outperforms the state-of-the-art by violating 17% and 25% more laws when testing Apollo 6.0 and Apollo 7.0, most of which are hard-to-violate laws, respectively.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {942–953},
numpages = {12},
keywords = {Traffic Laws, Testing Scenario Generation, Generative Flow Network, Baidu Apollo, Automated Driving System},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3545258.3545286,
author = {Wang, Bo and Liu, Guizhuang and Lin, Youfang and Ren, Shuang and Li, Honghui and Zhang, Dalin},
title = {Enhanced Evolutionary Automated Program Repair by Finer-Granularity Ingredients and Better Search Algorithms},
year = {2022},
isbn = {9781450397803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545258.3545286},
doi = {10.1145/3545258.3545286},
abstract = {Bug repair is time-consuming and tedious, which hampers software maintenance. To alleviate the burden, automated program repair (APR) is proposed and has been fruitful in the last decade. Evolutionary repair is the seminal work of this field and proliferated a family of approaches. The performance of evolutionary repair approaches is affected by two main factors: (1) search space, which defines all possible patches, and (2) search algorithms, which navigates the space. Although recent approaches have achieved remarkable progress, the main challenges of the two factors still remain. On one hand, the different kinds of search space are very coarse for containing correct patches. On the other hand, the search process guided by genetic algorithms is inefficient to find the correct patches in an appropriate time budget. In this paper, we propose MicroRepair, a new evolutionary repair approach to address the two challenges. Rather than finding statement-level patches like existing genetic repair approaches, MicroRepair enlarges the search space by breaking the statements into finer-granularity ingredients that are consist of AST leaves. As the search space grows exponentially, the former search algorithms may become inefficient to navigate in the larger space. We utilize the best multi-objective search algorithm selected from our empirical comparison on a set of search algorithms. We evaluated MicroRepair on 224 bugs of real-world from the benchmark Defects4J and compare it with several state-of-the-art repair approaches. The evaluation results show that MicroRepair correctly repaired 26 bugs with a precision of 62%, which significantly outperforms the state-of-the-art evolutionary APR approaches in terms of precision.},
booktitle = {Proceedings of the 13th Asia-Pacific Symposium on Internetware},
pages = {107–116},
numpages = {10},
keywords = {Genetic Programming, Genetic Improvement, Evolutionary Program Repair, Automated Program Repair},
location = {Hohhot, China},
series = {Internetware '22}
}

@proceedings{10.1145/3589250,
title = {SOAP 2023: Proceedings of the 12th ACM SIGPLAN International Workshop on the State Of the Art in Program Analysis},
year = {2023},
isbn = {9798400701702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 12th ACM SIGPLAN International Workshop on the State Of the Art in Program Analysis (SOAP ’23) is co-located with the 44th ACM SIGPLAN International Conference on Programming Language Design and Implementation (PLDI ’23). In line with past workshops, SOAP ’23 aims to bring together the members of the program analysis community to share new developments and shape innovations in program analysis.},
location = {Orlando, FL, USA}
}

@inproceedings{10.1145/3131151.3131157,
author = {Villanes, Isabel K. and Ascate, Silvia M. and Gomes, Josias and Dias-Neto, Arilo Claudio},
title = {What are Software Engineers asking about Android Testing on Stack Overflow?},
year = {2017},
isbn = {9781450353267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131151.3131157},
doi = {10.1145/3131151.3131157},
abstract = {Software testing represents an important activity to achieve quality during mobile application development. The constant evolution of mobile applications in previous years relating to size and complexity entails the need to improve testing techniques and tools. In this context, developers/testers often resort to specialized communities or Question &amp; Answer repositories to clarity doubts regarding testing in a practical and efficient way. Thus, these repositories become a popular source of data to understand the current context of software testing practices. In this paper, we present a study using the Stack Overflow repository for analyzing and clustering the main topics on Android testing. We employed the LDA algorithm to summarize the mobile testing related questions. Our findings show that topics such as testing tools, functional testing, and unit testing are often discussed when compared to other topics. We also analyzed the evolution of the interest of Android testing tools. Results show that developers are more interested in Appium, Espresso, Monkey, and Robotium tools.},
booktitle = {Proceedings of the XXXI Brazilian Symposium on Software Engineering},
pages = {104–113},
numpages = {10},
keywords = {Stack Overflow, Model Topics, Mobile Testing},
location = {Fortaleza, CE, Brazil},
series = {SBES '17}
}

@inproceedings{10.1145/3597926.3598126,
author = {Lau, Julia Kaiwen and Kong, Kelvin Kai Wen and Yong, Julian Hao and Tan, Per Hoong and Yang, Zhou and Yong, Zi Qian and Low, Joshua Chern Wey and Chong, Chun Yong and Lim, Mei Kuan and Lo, David},
title = {Synthesizing Speech Test Cases with Text-to-Speech? An Empirical Study on the False Alarms in Automated Speech Recognition Testing},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598126},
doi = {10.1145/3597926.3598126},
abstract = {Recent studies have proposed the use of Text-To-Speech (TTS) systems to automatically synthesise speech test cases on a scale and uncover a large number of failures in ASR systems. However, the failures uncovered by synthetic test cases may not reflect the actual performance of an ASR system when it transcribes human audio, which we refer to as false alarms. Given a failed test case synthesised from TTS systems, which consists of TTS-generated audio and the corresponding ground truth text, we feed the human audio stating the same text to an ASR system. If human audio can be correctly transcribed, an instance of a false alarm is detected. In this study, we investigate false alarm occurrences in five popular ASR systems using synthetic audio generated from four TTS systems and human audio obtained from two commonly used datasets. Our results show that the least number of false alarms is identified when testing Deepspeech, and the number of false alarms is the highest when testing Wav2vec2. On average, false alarm rates range from 21% to 34% in all five ASR systems. Among the TTS systems used, Google TTS produces the least number of false alarms (17%), and Espeak TTS produces the highest number of false alarms (32%) among the four TTS systems. Additionally, we build a false alarm estimator that flags potential false alarms, which achieves promising results: a precision of 98.3%, a recall of 96.4%, an accuracy of 98.5%, and an F1 score of 97.3%. Our study provides insight into the appropriate selection of TTS systems to generate high-quality speech to test ASR systems. Additionally, a false alarm estimator can be a way to minimise the impact of false alarms and help developers choose suitable test inputs when evaluating ASR systems. The source code used in this paper is publicly available on GitHub at https://github.com/julianyonghao/FAinASRtest.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1169–1181},
numpages = {13},
keywords = {Software Testing, False Alarms, Automated Speech Recognition},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@inproceedings{10.1109/ASE51524.2021.9678672,
author = {Hu, Qiang and Guo, Yuejun and Cordy, Maxime and Xie, Xiaofei and Ma, Wei and Papadakis, Mike and Traon, Yves Le},
title = {Towards exploring the limitations of active learning: an empirical study},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678672},
doi = {10.1109/ASE51524.2021.9678672},
abstract = {Deep neural networks (DNNs) are increasingly deployed as integral parts of software systems. However, due to the complex interconnections among hidden layers and massive hyperparameters, DNNs must be trained using a large number of labeled inputs, which calls for extensive human effort for collecting and labeling data. Spontaneously, to alleviate this growing demand, multiple state-of-the-art studies have developed different metrics to select a small yet informative dataset for the model training. These research works have demonstrated that DNN models can achieve competitive performance using a carefully selected small set of data. However, the literature lacks proper investigation of the limitations of data selection metrics, which is crucial to apply them in practice. In this paper, we fill this gap and conduct an extensive empirical study to explore the limits of data selection metrics. Our study involves 15 data selection metrics evaluated over 5 datasets (2 image classification tasks and 3 text classification tasks), 10 DNN architectures, and 20 labeling budgets (ratio of training data being labeled). Our findings reveal that, while data selection metrics are usually effective in producing accurate models, they may induce a loss of model robustness (against adversarial examples) and resilience to compression. Overall, we demonstrate the existence of a trade-off between labeling effort and different model qualities. This paves the way for future research in devising data selection metrics considering multiple quality criteria.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {917–929},
numpages = {13},
keywords = {empirical study, deep learning, data selection, active learning},
location = {Melbourne, Australia},
series = {ASE '21}
}

@article{10.1145/3637226,
author = {Guo, Shikai and Li, Dongmin and Huang, Lin and Lv, Sijia and Chen, Rong and Li, Hui and Li, Xiaochen and Jiang, He},
title = {Estimating Uncertainty in Labeled Changes by SZZ Tools on Just-In-Time Defect Prediction},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3637226},
doi = {10.1145/3637226},
abstract = {The aim of Just-In-Time (JIT) defect prediction is to predict software changes that are prone to defects in a project in a timely manner, thereby improving the efficiency of software development and ensuring software quality. Identifying changes that introduce bugs is a critical task in just-in-time defect prediction, and researchers have introduced the SZZ approach and its variants to label these changes. However, it has been shown that different SZZ algorithms introduce noise to the dataset to a certain extent, which may reduce the predictive performance of the model. To address this limitation, we propose the Confident Learning Imbalance (CLI) model. The model identifies and excludes samples whose labels may be corrupted by estimating the joint distribution of noisy labels and true labels, and mitigates the impact of noisy data on the performance of the prediction model. The CLI consists of two components: identifying noisy data (Confident Learning Component) and generating a predicted probability matrix for imbalanced data (Imbalanced Data Probabilistic Prediction Component). The IDPP component generates precise predicted probabilities for each instance in the training set, while the CL component uses the generated predicted probability matrix and noise labels to clean up the noise and build a classification model. We evaluate the performance of our model through extensive experiments on a total of 126,526 changes from ten Apache open source projects, and the results show that our model outperforms the baseline methods.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {105},
numpages = {25},
keywords = {Just-in-time defect prediction, SZZ tools, confident learning, imbalance}
}

@inproceedings{10.1145/3650212.3680307,
author = {Cheng, Runxiang and Wang, Shuai and Jabbarvand, Reyhaneh and Marinov, Darko},
title = {Revisiting Test-Case Prioritization on Long-Running Test Suites},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680307},
doi = {10.1145/3650212.3680307},
abstract = {The prolonged continuous integration (CI) runs are affecting timely feedback to software developers.
 
Test-case prioritization (TCP) aims to expose faults sooner by reordering tests such that the ones more likely to fail are run earlier.
 
TCP is thus especially important for long-running test suites.
 
While many studies have explored TCP, they are based on outdated CI builds from over 10 years ago with test suites that last several minutes, or builds from inaccessible, proprietary projects.
 
In this paper, we present LRTS, the first dataset of long-running test suites, with 21,255 CI builds and 57,437 test-suite runs from 10 large-scale, open-source projects that use Jenkins CI.
 
LRTS spans from 2020 to 2023, with an average test-suite run duration of 6.5 hours.
 
On LRTS, we study the effectiveness of 59 leading TCP techniques, the impact of confounding test failures on TCP, and TCP for failing tests with no prior failures.
 
We revisit prior key findings (9 confirmed, 2 refuted) and establish 3 new findings.
 
Our results show that prioritizing faster tests that recently failed performs the best, outperforming the sophisticated techniques.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {615–627},
numpages = {13},
keywords = {Software testing, regression testing, reliability, test prioritization},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3650212.3652120,
author = {Hu, Tianmin and Ye, Guixin and Tang, Zhanyong and Tan, Shin Hwei and Wang, Huanting and Li, Meng and Wang, Zheng},
title = {UPBEAT: Test Input Checks of Q# Quantum Libraries},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3652120},
doi = {10.1145/3650212.3652120},
abstract = {High-level programming models like Q# significantly simplify the complexity of programming for quantum computing. These models are supported by a set of foundation libraries for code development. However, errors can occur in the library implementation, and one common root cause is the lack of or incomplete checks on properties like values, length, and quantum states of inputs passed to user-facing subroutines. This paper presents Upbeat, a fuzzing tool to generate random test cases for bugs related to input checking in Q# libraries. Upbeat develops an automated process to extract constraints from the API documentation and the developer implemented input-checking statements. It leverages open-source Q# code samples to synthesize test programs. It frames the test case generation as a constraint satisfaction problem for classical computing and a quantum state model for quantum computing to produce carefully generated subroutine inputs to test if the input-checking mechanism is appropriately implemented. Under 100 hours of automated test runs, Upbeat has successfully identified 16 bugs in API implementations and 4 documentation errors. Of these, 14 have been confirmed, and 12 have been fixed by the library developers.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {186–198},
numpages = {13},
keywords = {Fuzzing, Quantum computing, Software testing},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3597926.3598079,
author = {Wang, Wenxuan and Huang, Jingyuan and Chen, Chang and Gu, Jiazhen and Zhang, Jianping and Wu, Weibin and He, Pinjia and Lyu, Michael},
title = {Validating Multimedia Content Moderation Software via Semantic Fusion},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598079},
doi = {10.1145/3597926.3598079},
abstract = {The exponential growth of social media platforms, such as Facebook, Instagram, Youtube, and TikTok, has revolutionized communication and content publication in human society. Users on these platforms can publish multimedia content that delivers information via the combination of text, audio, images, and video. Meanwhile, the multimedia content release facility has been increasingly exploited to propagate toxic content, such as hate speech, malicious advertisement, and pornography. To this end, content moderation software has been widely deployed on these platforms to detect and blocks toxic content. However, due to the complexity of content moderation models and the difficulty of understanding information across multiple modalities, existing content moderation software can fail to detect toxic content, which often leads to extremely negative impacts (e.g., harmful effects on teen mental health).  
We introduce Semantic Fusion, a general, effective methodology for validating multimedia content moderation software. Our key idea is to fuse two or more existing single-modal inputs (e.g., a textual sentence and an image) into a new input that combines the semantics of its ancestors in a novel manner and has toxic nature by construction. This fused input is then used for validating multimedia content moderation software. We realized Semantic Fusion as DUO, a practical content moderation software testing tool. In our evaluation, we employ DUO to test five commercial content moderation software and two state-of-the-art models against three kinds of toxic contents. The results show that DUO achieves up to 100% error finding rate (EFR) when testing moderation software and it obtains up to 94.1% EFR when testing the state-of-the-art models. In addition, we leverage the test cases generated by DUO to retrain the two models we explored, which largely improves model robustness (2.5%∼5.7% EFR) while maintaining the accuracy on the original test set.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {576–588},
numpages = {13},
keywords = {semantic fusion, multimedia content moderation, metamorphic testing, Software testing},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@inproceedings{10.1109/ASP-DAC58780.2024.10473799,
author = {Hazott, Christoph and St\"{o}gm\"{u}ller, Florian and Gro\ss{}e, Daniel},
title = {Verifying Embedded Graphics Libraries leveraging Virtual Prototypes and Metamorphic Testing},
year = {2024},
isbn = {9798350393545},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASP-DAC58780.2024.10473799},
doi = {10.1109/ASP-DAC58780.2024.10473799},
abstract = {Embedded graphics libraries are part of the firmware of embedded systems and provide complex functionalities optimized for specific hardware. After unit testing of embedded graphics libraries, integration testing is a significant challenge, in particular since the hardware is needed to obtain the output image as well as the inherent difficulty in defining the reference result.In this paper, we present a novel approach focusing on integration testing of embedded graphic libraries. We leverage Virtual Prototypes (VPs) and integrate them with Metamorphic Testing (MT). MT is a software testing technique that uncovers faults or issues in a system by exploring how its outputs change under predefined input transformations, without relying on explicit oracles or predetermined results. In combination with virtualizing the displays in VPs, we even eliminate the need for physical hardware. This allows us to develop a MT framework automating the verification process. In our evaluation, we demonstrate the effectiveness of our MT framework. On an extended RISC-V VP for the GD32V platform we found 15 distinct bugs for the widely used TFT eSPI embedded graphics library, confirming the strength our approach.},
booktitle = {Proceedings of the 29th Asia and South Pacific Design Automation Conference},
pages = {275–281},
numpages = {7},
location = {Incheon, Republic of Korea},
series = {ASPDAC '24}
}

@inproceedings{10.1109/ICSE43902.2021.00131,
author = {Li, Zhenhao and Li, Heng and Chen, Tse-Hsun Peter and Shang, Weiyi},
title = {DeepLV: Suggesting Log Levels Using Ordinal Based Neural Networks},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00131},
doi = {10.1109/ICSE43902.2021.00131},
abstract = {Developers write logging statements to generate logs that provide valuable runtime information for debugging and maintenance of software systems. Log level is an important component of a logging statement, which enables developers to control the information to be generated at system runtime. However, due to the complexity of software systems and their runtime behaviors, deciding a proper log level for a logging statement is a challenging task. For example, choosing a higher level (e.g., error) for a trivial event may confuse end users and increase system maintenance overhead, while choosing a lower level (e.g., trace) for a critical event may prevent the important execution information to be conveyed opportunely. In this paper, we tackle the challenge by first conducting a preliminary manual study on the characteristics of log levels. We find that the syntactic context of the logging statement and the message to be logged might be related to the decision of log levels, and log levels that are further apart in order (e.g., trace and error) tend to have more differences in their characteristics. Based on this, we then propose a deep-learning based approach that can leverage the ordinal nature of log levels to make suggestions on choosing log levels, by using the syntactic context and message features of the logging statements extracted from the source code. Through an evaluation on nine large-scale open source projects, we find that: 1) our approach outperforms the state-of-the-art baseline approaches; 2) we can further improve the performance of our approach by enlarging the training data obtained from other systems; 3) our approach also achieves promising results on cross-system suggestions that are even better than the baseline approaches on within-system suggestions. Our study highlights the potentials in suggesting log levels to help developers make informed logging decisions.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1461–1472},
numpages = {12},
keywords = {logs, log level, empirical study, deep learning},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3306618.3314244,
author = {Raji, Inioluwa Deborah and Buolamwini, Joy},
title = {Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314244},
doi = {10.1145/3306618.3314244},
abstract = {Although algorithmic auditing has emerged as a key strategy to expose systematic biases embedded in software platforms, we struggle to understand the real-world impact of these audits, as scholarship on the impact of algorithmic audits on increasing algorithmic fairness and transparency in commercial systems is nascent. To analyze the impact of publicly naming and disclosing performance results of biased AI systems, we investigate the commercial impact of Gender Shades, the first algorithmic audit of gender and skin type performance disparities in commercial facial analysis models. This paper 1) outlines the audit design and structured disclosure procedure used in the Gender Shades study, 2) presents new performance metrics from targeted companies IBM, Microsoft and Megvii (Face++) on the Pilot Parliaments Benchmark (PPB) as of August 2018, 3) provides performance results on PPB by non-target companies Amazon and Kairos and, 4) explores differences in company responses as shared through corporate communications that contextualize differences in performance on PPB. Within 7 months of the original audit, we find that all three targets released new API versions. All targets reduced accuracy disparities between males and females and darker and lighter-skinned subgroups, with the most significant update occurring for the darker-skinned female subgroup, that underwent a 17.7% - 30.4% reduction in error between audit periods. Minimizing these disparities led to a 5.72% to 8.3% reduction in overall error on the Pilot Parliaments Benchmark (PPB) for target corporation APIs. The overall performance of non-targets Amazon and Kairos lags significantly behind that of the targets, with error rates of 8.66% and 6.60% overall, and error rates of 31.37% and 22.50% for the darker female subgroup, respectively.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {429–435},
numpages = {7},
keywords = {machine learning, fairness, facial recognition, ethics, computer vision, commercial applications, artificial intelligence},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1109/ICSE43902.2021.00037,
author = {Luo, Weisi and Chai, Dong and Run, Xiaoyue and Wang, Jiang and Fang, Chunrong and Chen, Zhenyu},
title = {Graph-based Fuzz Testing for Deep Learning Inference Engines},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00037},
doi = {10.1109/ICSE43902.2021.00037},
abstract = {With the wide use of Deep Learning (DL) systems, academy and industry begin to pay attention to their quality. Testing is one of the major methods of quality assurance. However, existing testing techniques focus on the quality of DL models but lacks attention to the core underlying inference engines (i.e., frameworks and libraries). Inspired by the success stories of fuzz testing, we design a graph-based fuzz testing method to improve the quality of DL inference engines. This method is naturally followed by the graph structure of DL models. A novel operator-level coverage criterion based on graph theory is introduced and six different mutations are implemented to generate diversified DL models by exploring combinations of model structures, parameters, and data inputs. The Monte Carlo Tree Search (MCTS) is used to drive DL model generation without a training process. The experimental results show that the MCTS outperforms the random method in boosting operator-level coverage and detecting exceptions. Our method has discovered more than 40 different exceptions in three types of undesired behaviors: model conversion failure, inference failure, output comparison failure. The mutation strategies are useful to generate new valid test inputs, by up to an 8.2% more operator-level coverage on average and 8.6 more exceptions captured.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {288–299},
numpages = {12},
keywords = {Operator-Level Coverage, Monte Carlo Tree Search, Graph Theory, Deep Learning Models, Deep Learning Inference Engine},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3387940.3392251,
author = {Yang, Yingzhuo and Xu, Chang},
title = {M.R. Hunter: Hunting for Metamorphic Relations by Puzzle Solving},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392251},
doi = {10.1145/3387940.3392251},
abstract = {Metamorphic testing (MT) is getting increasingly popular by exhibiting test effectiveness for a wide range of subjects, from compilers to machine learning programs. However, the central part of MT, i.e., the derivation of useful metamorphic relations (MRs), still falls behind MT's rapid applications. In this paper, we propose M.R. Hunter, an interactive online game for attracting users, especially those who are not familiar with, or even reluctant to, learning intrinsic complexities behind MT, to participate into the MR derivation process in a puzzle-solving way. The game design carefully considers how to guide users to participate actively, how to present conjectured MRs intuitively, and how to validate MRs effectively. So far, we have built and deployed a preliminary version of the game, and received active feedbacks, suggesting both promising results and useful advices for future improvement.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {404–409},
numpages = {6},
keywords = {Metamorphic Testing, Metamorphic Relation},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3377811.3380362,
author = {Zhang, Ru and Xiao, Wencong and Zhang, Hongyu and Liu, Yu and Lin, Haoxiang and Yang, Mao},
title = {An empirical study on program failures of deep learning jobs},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380362},
doi = {10.1145/3377811.3380362},
abstract = {Deep learning has made significant achievements in many application areas. To train and test models more efficiently, enterprise developers submit and run their deep learning programs on a shared, multi-tenant platform. However, some of the programs fail after a long execution time due to code/script defects, which reduces the development productivity and wastes expensive resources such as GPU, storage, and network I/O.This paper presents the first comprehensive empirical study on program failures of deep learning jobs. 4960 real failures are collected from a deep learning platform in Microsoft. We manually examine their failure messages and classify them into 20 categories. In addition, we identify the common root causes and bug-fix solutions on a sample of 400 failures. To better understand the current testing and debugging practices for deep learning, we also conduct developer interviews. Our major findings include: (1) 48.0% of the failures occur in the interaction with the platform rather than in the execution of code logic, mostly due to the discrepancies between local and platform execution environments; (2) Deep learning specific failures (13.5%) are mainly caused by inappropriate model parameters/structures and framework API misunderstanding; (3) Current debugging practices are not efficient for fault localization in many cases, and developers need more deep learning specific tools. Based on our findings, we further suggest possible research topics and tooling support that could facilitate future deep learning development.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1159–1170},
numpages = {12},
keywords = {deep learning jobs, empirical study, program failures},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1145/3643746,
author = {Y\i{}ld\i{}ran, Necip Faz\i{}l and Oh, Jeho and Lawall, Julia and Gazzillo, Paul},
title = {Maximizing Patch Coverage for Testing of Highly-Configurable Software without Exploding Build Times},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643746},
doi = {10.1145/3643746},
abstract = {The Linux kernel is highly-configurable, with a build system that takes a configuration file as input and automatically tailors the source code accordingly. Configurability, however, complicates testing, because different configuration options lead to the inclusion of different code fragments. With thousands of patches received per month, Linux kernel maintainers employ extensive automated continuous integration testing. To attempt patch coverage, i.e., taking all changed lines into account, current approaches either use configuration files that maximize total statement coverage or use multiple randomly-generated configuration files, both of which incur high build times without guaranteeing patch coverage. To achieve patch coverage without exploding build times, we propose krepair, which automatically repairs configuration files that are fast-building but have poor patch coverage to achieve high patch coverage with little effect on build times. krepair works by discovering a small set of changes to a configuration file that will ensure patch coverage, preserving most of the original configuration file's settings. Our evaluation shows that, when applied to configuration files with poor patch coverage on a statistically-significant sample of recent Linux kernel patches, krepair achieves nearly complete patch coverage, 98.5% on average, while changing less than 1.53% of the original default configuration file in 99% of patches, which keeps build times 10.5x faster than maximal configuration files.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {20},
numpages = {23},
keywords = {build systems, software configuration, static analysis}
}

@article{10.1145/3468744.3468756,
author = {Yi, Pu and Wei, Anjiang and Lam, Wing and Xie, Tao and Marinov, Darko},
title = {Finding Polluter Tests Using Java PathFinder},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3468744.3468756},
doi = {10.1145/3468744.3468756},
abstract = {Tests that modify (i.e., "pollute") the state shared among tests in a test suite are called polluter tests". Finding these tests is im- portant because they could result in di erent test outcomes based on the order of the tests in the test suite. Prior work has proposed the PolDet technique for nding polluter tests in runs of JUnit tests on a regular Java Virtual Machine (JVM). Given that Java PathFinder (JPF) provides desirable infrastructure support, such as systematically exploring thread schedules, it is a worthwhile attempt to re-implement techniques such as PolDet in JPF. We present a new implementation of PolDet for nding polluter tests in runs of JUnit tests in JPF. We customize the existing state comparison in JPF to support the so-called common-root iso- morphism" required by PolDet. We find that our implementation is simple, requiring only -200 lines of code, demonstrating that JPF is a sophisticated infrastructure for rapid exploration of re-search ideas on software testing. We evaluate our implementation on 187 test classes from 13 Java projects and nd 26 polluter tests. Our results show that the runtime overhead of PolDet@JPF com- pared to base JPF is relatively low, on average 1.43x. However, our experiments also show some potential challenges with JPF.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jul,
pages = {37–41},
numpages = {5},
keywords = {polluter tests, poldet, java pathfinder, aky tests}
}

@inproceedings{10.1145/3579895.3579925,
author = {Sun, Jingchun and Deng, Fei and Du, Boya},
title = {Research on Whole-Link Risk Situational Awareness Index System and Dynamic Risk Pool Supervision},
year = {2023},
isbn = {9781450398039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579895.3579925},
doi = {10.1145/3579895.3579925},
abstract = {With the development of society, network security becomes more and more important. It is inevitable to establish the risk index system to effectively monitor the security of network. In the previous study, due to absence of risk indicators, the risk supervising system is not perfect. However, there are few researches on the whole-link risk index system, and the determination of risk indicators lacks basis. Therefore, this paper firstly constructs a new risk index system of situational awareness, which can avoid the omission of risk indicators. Then the sliding window is used to observe this system and the optimal window width is found to determine the dynamic risk pool. Finally, the indicators of dynamic risk pool are used to dynamically monitor the whole link risk index system. The whole technical framework not only proves advantages of the mature risk index system of situational awareness, but also overcomes the disadvantage that the risk index of the whole link cannot be determined, and the dynamic risk pool has the sharing characteristics in the whole process.},
booktitle = {Proceedings of the 2022 11th International Conference on Networks, Communication and Computing},
pages = {190–197},
numpages = {8},
keywords = {The whole-link network, Sliding window, Situational awareness, Dynamic risk pooling},
location = {Beijing, China},
series = {ICNCC '22}
}

@inproceedings{10.1145/3639478.3643063,
author = {Arcaini, Paolo and Ishikawa, Fuyuki and Ma, Lei and Maezawa, Yuta and Yoshioka, Nobukazu and Zhang, Fuyuan},
title = {Technical Briefing on Deep Neural Network Repair},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643063},
doi = {10.1145/3639478.3643063},
abstract = {Deep Neural Networks (DNNs) are used for different tasks in many domains, some safety critical like autonomous driving. When in operation, the DNN could misbehave on some inputs unseen during training. DNN repair is a new emerging technique that tries to improve the DNN to fix these misbehaviours, without affecting the correct behaviours. The technical briefing will give an overview of the different DNN repair techniques that have been proposed in the literature, that differ in the faults they target, the way to localise them, and the technical approach they employ to modify the network. Moreover, the technical briefing will also demonstrate the usage of some of these DNN repair techniques.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {428–430},
numpages = {3},
keywords = {deep neural networks, DNN repair, fault localisation},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00041,
author = {Aggarwal, Aniya and Shaikh, Samiulla and Hans, Sandeep and Haldar, Swastik and Ananthanarayanan, Rema and Saha, Diptikalyan},
title = {Testing framework for black-box AI models},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00041},
doi = {10.1109/ICSE-Companion52605.2021.00041},
abstract = {With widespread adoption of AI models for important decision making, ensuring reliability of such models remains an important challenge. In this paper, we present an end-to-end generic framework for testing AI Models which performs automated test generation for different modalities such as text, tabular, and time-series data and across various properties such as accuracy, fairness, and robustness. Our tool has been used for testing industrial AI models and was very effective to uncover issues present in those models.Demo video link- https://youtu.be/984UCU17YZI},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {81–84},
numpages = {4},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3600006.3613148,
author = {Gong, Sishuai and Peng, Dinglan and Alt\i{}nb\"{u}ken, Deniz and Fonseca, Pedro and Maniatis, Petros},
title = {Snowcat: Efficient Kernel Concurrency Testing using a Learned Coverage Predictor},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600006.3613148},
doi = {10.1145/3600006.3613148},
abstract = {Random-based approaches and heuristics are commonly used in kernel concurrency testing due to the massive scale of modern kernels and corresponding interleaving space. The lack of accurate and scalable approaches to analyze concurrent kernel executions makes existing testing approaches heavily rely on expensive dynamic executions to measure the effectiveness of a new test. Unfortunately, the high cost incurred by dynamic executions limits the breadth of the exploration and puts latency pressure on finding effective concurrent test inputs and schedules, hindering the overall testing effectiveness.This paper proposes Snowcat, a kernel concurrency testing framework that generates effective test inputs and schedules using a learned kernel block-coverage predictor. Using a graph neural network, the coverage predictor takes a concurrent test input and scheduling hints and outputs a prediction on whether certain important code blocks will be executed. Using this predictor, Snowcat can skip concurrent tests that are likely to be fruitless and prioritize the promising ones for actual dynamic execution.After testing the Linux kernel for over a week, Snowcat finds ~17% more potential data races, by prioritizing tests of more fruitful schedules than existing work would have chosen. Snowcat can also find effective test inputs that expose new concurrency bugs with higher probability (1.4\texttimes{}~2.6\texttimes{}), or reproduce known bugs more quickly (15\texttimes{}) than state-of-art testing tools. More importantly, Snowcat is shown to be more efficient at reaching a desirable level of race coverage in the continuous setting, as the Linux kernel evolves from version to version. In total, Snowcat discovered 17 new concurrency bugs in Linux kernel 6.1, of which 13 are confirmed and 6 are fixed.},
booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
pages = {35–51},
numpages = {17},
keywords = {concurrency programming, software testing and debugging, operating systems security, kernel concurrency bugs},
location = {Koblenz, Germany},
series = {SOSP '23}
}

@inproceedings{10.1145/3510455.3512796,
author = {B\"{o}hme, Marcel},
title = {Statistical reasoning about programs},
year = {2022},
isbn = {9781450392242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510455.3512796},
doi = {10.1145/3510455.3512796},
abstract = {We discuss the advent of a new program analysis paradigm that allows anyone to make precise statements about the behavior of programs as they run in production across hundreds and millions of machines or devices. The scale-oblivious, in vivo program analysis leverages an almost inconceivable rate of user-generated program executions across large fleets to analyze programs of arbitrary size and composition with negligible performance overhead.In this paper, we reflect on the program analysis problem, the prevalent paradigm, and the practical reality of program analysis at large software companies. We illustrate the new paradigm using several success stories and suggest a number of exciting new research directions.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {76–80},
numpages = {5},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-NIER '22}
}

@inproceedings{10.1145/3691620.3695356,
author = {Eshghie, Mojtaba and Artho, Cyrille and Stammler, Hans and Ahrendt, Wolfgang and Hildebrandt, Thomas and Schneider, Gerardo},
title = {HighGuard: Cross-Chain Business Logic Monitoring of Smart Contracts},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695356},
doi = {10.1145/3691620.3695356},
abstract = {Logical flaws in smart contracts are often exploited, leading to significant financial losses. Our tool, HighGuard, detects transactions that violate business logic specifications of smart contracts. HighGuard employs dynamic condition response (DCR) graph models as formal specifications to verify contract execution against these models. It is capable of operating in a cross-chain environment for detecting business logic flaws across different blockchain platforms. We demonstrate HighGuard's effectiveness in identifying deviations from specified behaviors in smart contracts without requiring code instrumentation or incurring additional gas costs. By using precise specifications in the monitor, HighGuard achieves detection without false positives. Our evaluation, involving 54 exploits, confirms HighGuard's effectiveness in detecting business logic vulnerabilities.Our open-source implementation of HighGuard and a screencast of its usage are available at: https://github.com/mojtaba-eshghie/HighGuardhttps://www.youtube.com/watch?v=sZYVV-slDaY},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {2378–2381},
numpages = {4},
keywords = {smart contracts, DCR graphs, runtime monitoring, blockchain security},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3551349.3556966,
author = {Qian, Ju and Ma, Yingwei and Lin, Chenghao and Chen, Lin},
title = {Accelerating OCR-Based Widget Localization for Test Automation of GUI Applications},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556966},
doi = {10.1145/3551349.3556966},
abstract = {Optical character recognition (OCR) algorithms often run slow. They may take several seconds to recognize the texts on a GUI screen, which makes OCR-based widget localization in test automation unfriendly for use, especially on GPU-free computers. This paper first concludes a common type of widget text to be located in GUI testing: label text, which are short texts in widgets like buttons, menu items, and window titles. We then investigate the characteristics of texts on a GUI screen and introduce a fast GPU-independent Label Text Screening (LTS) technique to accelerate the OCR process for label text localization. The technique opens the black box of OCR engines and uses a combination of simple methods to avoid excessive text analysis on a screen as much as possible. Experiments show that, on the subject datasets, LTS reduces the average OCR-based label text localization time to a large extent. On 4k resolution GUI screens, it keeps the localization time below 0.5 seconds for over about 60% of cases without GPU support on a normal laptop computer. In contrast, the existing CPU-based approaches built on popular OCR engines Tesseract, PaddleOCR, and EasyOCR usually need over 2 seconds to achieve the same goal on the same platform. Even with GPU acceleration, they can hardly keep the analysis time in 1 second. We believe the proposed approach would be helpful for implementing OCR-based test automation tools.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {6},
numpages = {13},
keywords = {test automation, computer vision, OCR, GUI testing},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3540250.3549126,
author = {Eghbali, Aryaz and Pradel, Michael},
title = {DynaPyt: a dynamic analysis framework for Python},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549126},
doi = {10.1145/3540250.3549126},
abstract = {Python is a widely used programming language that powers important application domains such as machine learning, data analysis, and web applications. For many programs in these domains it is consequential to analyze aspects like security and performance, and with Python’s dynamic nature, it is crucial to be able to dynamically analyze Python programs. However, existing tools and frameworks  
do not provide the means to implement dynamic analyses easily and practitioners resort to implementing an ad-hoc dynamic analysis for their own use case. This work presents DynaPyt, the first general-purpose framework for heavy-weight dynamic analysis of Python programs. Compared to existing tools for other programming languages, our framework provides a wider range of analysis hooks arranged in a hierarchical structure, which allows developers to concisely implement analyses. DynaPyt features selective instrumentation and execution modification as well. We evaluate our framework on test suites of 9 popular open-source Python projects, 1,268,545 lines of code in total, and show that it, by and large, preserves the semantics of the original execution. The running time of DynaPyt is between 1.2x and 16x times the original execution time, which is in line with similar frameworks designed for other languages, and 5.6%–88.6% faster than analyses using a built-in tracing API offered by Python. We also implement multiple analyses, show the simplicity of implementing them and some potential use cases of DynaPyt. Among the analyses implemented are: an analysis to detect a memory blow up in Pytorch programs, a taint analysis to detect SQL injections, and an analysis to warn about a runtime performance anti-pattern.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {760–771},
numpages = {12},
keywords = {python, dynamic analysis},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3634713.3634720,
author = {Klikovits, Stefan and Gambi, Alessio and Dhungana, Deepak and Rabiser, Rick},
title = {Leveraging Software Product Lines for Testing Autonomous Vehicles},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634720},
doi = {10.1145/3634713.3634720},
abstract = {Extensive testing of Automated Driving Systems (ADS), such as Advanced Driver Assistance Systems and Autonomous Vehicles, is commonly conducted using simulators programmed to implement various driving scenarios, a technique known as scenario-based testing. ADS scenario-based testing using simulations is challenging because it requires identifying scenarios that can effectively test ADS functionalities while ensuring that driving simulators’ features match the driving scenarios’ requirements. This short paper discusses the main challenges of systematically conducting simulation-based testing and proposes leveraging Software Product Line techniques to address them. Specifically, we argue that variability models can be used to support testers in generating test scenarios by effectively capturing and relating the variability in driving simulators, testing scenarios, and ADS implementations. We conclude by outlining an agenda for future research in this important area.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {56–60},
numpages = {5},
keywords = {Autonomous Vehicles, Scenario- and Simulation-based Testing, Software Product Lines},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.5555/3408352.3408474,
author = {Pfeifer, N\'{\i}colas and Zimpel, Bruno V. and Andrade, Gabriel A. G. and Santos, Luiz C. V. dos},
title = {A reinforcement learning approach to directed test generation for shared memory verification},
year = {2020},
isbn = {9783981926347},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Multicore chips are expected to rely on coherent shared memory. Albeit the coherence hardware can scale gracefully, the protocol state space grows exponentially with core count. That is why design verification requires directed test generation (DTG) for dynamic coverage control under the tight time constraints resulting from slow simulation and short verification budgets. Next generation EDA tools are expected to exploit Machine Learning for reaching high coverage in less time. We propose a technique that addresses DTG as a decision process and tries to find a decision-making policy for maximizing the cumulative coverage, as a result of successive actions taken by an agent. Instead of simply relying on learning, our technique builds upon the legacy from constrained random test generation (RTG). It casts DTG as coverage-driven RTG, and it explores distinct RTG engines subject to progressively tighter constraints. We compared three Reinforcement Learning generators with a state-of-the-art generator based on Genetic Programming. The experimental results show that the proper enforcement of constraints is more efficient for guiding learning towards higher coverage than simply letting the generator learn how to select the most promising memory events for increasing coverage. For a 3-level MESI 32-core design, the proposed approach led to the highest observed coverage (95.81%), and it was 2.4 times faster than the baseline generator to reach the latter's maximal coverage.},
booktitle = {Proceedings of the 23rd Conference on Design, Automation and Test in Europe},
pages = {538–543},
numpages = {6},
keywords = {decision process, design verification, multicore chips, reinforcement learning, shared memory},
location = {Grenoble, France},
series = {DATE '20}
}

@proceedings{10.1145/3590837,
title = {ICIMMI '22: Proceedings of the 4th International Conference on Information Management &amp; Machine Intelligence},
year = {2022},
isbn = {9781450399937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Jaipur, India}
}

@inproceedings{10.1109/ASE56229.2023.00139,
author = {Zhang, Zuoyan and Zhou, Bei and Hao, Jiangwei and Yang, Hongru and Cui, Mengqi and Zhou, Yuchang and Song, Guanghui and Li, Fei and Xu, Jinchen and Zhao, Jie},
title = {Eiffel: Inferring Input Ranges of Significant Floating-Point Errors via Polynomial Extrapolation},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00139},
doi = {10.1109/ASE56229.2023.00139},
abstract = {Existing search heuristics used to find input values that result in significant floating-point (FP) errors or small ranges that cover them are accompanied by severe constraints, complicating their implementation and restricting their general applicability. This paper introduces an error analysis tool called Eiffel to infer error-inducing input ranges instead of searching them. Given an FP expression with its domain D, Eiffel first constructs an error data set by sampling values across a smaller domain ℛ and assembles these data into clusters. If more than two clusters are formed, Eiffel derives polynomial curves that best fit the bound coordinates of the error-inducing ranges in ℛ, extrapolating them to infer all target ranges of D and reporting the maximal error. Otherwise, Eiffel simply returns the largest error across ℛ. Experimental results show that Eiffel exhibits a broader applicability than Atomu and S3FP by successfully detecting the errors of all 70 considered benchmarks while the two baselines only report errors for part of them. By taking as input the inferred ranges of Eiffel, Herbie obtains an average accuracy improvement of 3.35 bits and up to 53.3 bits.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1441–1453},
numpages = {13},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.5555/3398761.3398884,
author = {Phan, Thomy and Gabor, Thomas and Sedlmeier, Andreas and Ritz, Fabian and Kempter, Bernhard and Klein, Cornel and Sauer, Horst and Schmid, Reiner and Wieghardt, Jan and Zeller, Marc and Linnhoff-Popien, Claudia},
title = {Learning and Testing Resilience in Cooperative Multi-Agent Systems},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {State-of-the-art multi-agent reinforcement learning has achieved remarkable success in recent years. The success has been mainly based on the assumption that all teammates perfectly cooperate to optimize a global objective in order to achieve a common goal. While this may be true in the ideal case, these approaches could fail in practice, since in multi-agent systems (MAS), all agents may be a potential source of failure. In this paper, we focus on resilience in cooperative MAS and propose an Antagonist-Ratio Training Scheme (ARTS) by reformulating the original target MAS as a mixed cooperative-competitive game between a group of protagonists which represent agents of the target MAS and a group of antagonists which represent failures in the MAS. While the protagonists can learn robust policies to ensure resilience against failures, the antagonists can learn malicious behavior to provide an adequate test suite for other MAS. We empirically evaluate ARTS in a cyber physical production domain and show the effectiveness of ARTS w.r.t. resilience and testing capabilities.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1055–1063},
numpages = {9},
keywords = {multi-agent learning, learning and testing, adversarial learning},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1145/3611643.3613892,
author = {Jin, Matthew and Shahriar, Syed and Tufano, Michele and Shi, Xin and Lu, Shuai and Sundaresan, Neel and Svyatkovskiy, Alexey},
title = {InferFix: End-to-End Program Repair with LLMs},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613892},
doi = {10.1145/3611643.3613892},
abstract = {Software development life cycle is profoundly influenced by bugs; their introduction, identification, and eventual resolution account for a significant portion of software development cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. Large Language Models (LLMs) have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose : a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs.  combines a Retriever – transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator – an LLM (12 billion parameter Codex Cushman model) finetuned on supervised bug-fix data with prompts augmented via adding bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. To train and evaluate our approach, we curated , a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories. Our evaluation demonstrates that  outperforms strong LLM baselines, with a top-1 accuracy of 65.6% for generating fixes in C# and 76.8% in Java. We discuss the deployment of  alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration (CI) pipeline to automate the software development workflow.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1646–1656},
numpages = {11},
keywords = {Program repair, finetuning, prompt augmentation, static analyses},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@article{10.1145/3720540,
author = {Shen, Zongwen and Li, Yuning and Ge, Jidong and Chen, Xiang and Li, Chuanyi and Huang, Liguo and Luo, Bin},
title = {An Empirical Study of Code Simplification Methods in Code Intelligence Tasks},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3720540},
doi = {10.1145/3720540},
abstract = {In recent years, pre-trained language models have seen significant success in natural language processing and have been increasingly applied to code-related tasks. Code intelligence tasks have shown promising performance with the support of code pre-trained language models. Pre-processing code simplification methods have been introduced to prune code tokens from the model’s input while maintaining task effectiveness. These methods improve the efficiency of code intelligence tasks while reducing computational costs. Post-prediction code simplification methods provide explanations for code intelligence task outcomes, enhancing the reliability and interpretability of model predictions. However, comprehensive evaluations of these methods across diverse code pre-trained model architectures and code intelligence tasks are lacking. To assess the effectiveness of code simplification methods, we conduct an empirical study integrating these code simplification methods with various pre-trained code models across multiple code intelligence tasks.Our empirical findings suggest that developing task-specific code simplification methods would be beneficial. Then, we recommend leveraging post-prediction methods to summarize prior knowledge, which can pre-process code simplification strategies. Moreover, establishing more evaluation mechanisms for code simplification is crucial. Finally, we propose incorporating code simplification methods into the pre-training phase of code pre-trained models to enhance their program comprehension and code representation capabilities.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
keywords = {Code Simplifications, Code Pre-train Models, Code intelligence tasks, Program representations}
}

@inproceedings{10.1145/3524842.3528480,
author = {Csuvik, Viktor and Vid\'{a}cs, L\'{a}szl\'{o}},
title = {FixJS: a dataset of bug-fixing JavaScript commits},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528480},
doi = {10.1145/3524842.3528480},
abstract = {The field of Automated Program Repair (APR) has received increasing attention in recent years both from the academic world and from leading IT companies. Its main goal is to repair software bugs automatically, thus reducing the cost of development and maintenance significantly. Recent works use state-of-the-art deep learning models to predict correct patches, for these teaching on a large amount of data is inevitable almost in every scenarios. Despite this, readily accessible data on the field is very scarce. To contribute to related research, we present FixJS, a dataset containing bug-fixing information of ~2 million commits. The commits were gathered from GitHub and processed locally to have both the buggy (before bug fixing commit) and fixed (after fix) version of the same program. We focused on JavaScript functions, as it is one of the most popular programming language globally and functions are first class objects there. The data includes more than 300,000 samples of such functions, including commit information, before/after states and 3 source code representations.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {712–716},
numpages = {5},
keywords = {software engineering, bug-fixing commits, automated program repair},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1145/3646548.3672597,
author = {Purandare, Salil and Cohen, Myra B.},
title = {Exploration of Failures in an sUAS Controller Software Product Line},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672597},
doi = {10.1145/3646548.3672597},
abstract = {Small uncrewed aerial systems (sUAS) are growing in their use for commercial, scientific, recreational, and emergency management purposes. A critical part of a successful flight is a correctly tuned controller which manages the physics of the vehicle. If improperly configured, it can lead to flight instability, deviation, or crashes. These types of misconfigurations are often within the valid ranges specified in the documentation; hence, they are hard to identify. Recent research has used fuzzing or explored only a small part of the parameter space, providing little understanding of the configuration landscape itself. In this work we leverage software product line engineering to model a subset of the parameter space of a widely used flight control software, using it to guide a systematic exploration of the controller space. Via simulation, we test over 20,000 configurations from a feature model with 50 features and 8.88 \texttimes{} 1034 products, covering all single parameter value changes and all pairs of changes from their default values. Our results show that only a small number of single configuration changes fail (15%), however almost 40% fail when we evaluate changes to two-parameters at a time. We explore the interactions between parameters in more detail, finding what appear to be many dependencies and interactions between parameters which are not well documented. We then explore a smaller, exhaustive product line model, with eight of the most important features (and 6,561 configurations) and uncover a complex set of interactions; over 48% of all configurations fail.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {125–135},
numpages = {11},
keywords = {Configurability, Software Product Lines, sUAS},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3551349.3561157,
author = {Yuan, Yuanyuan and Pang, Qi and Wang, Shuai},
title = {Unveiling Hidden DNN Defects with Decision-Based Metamorphic Testing},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3561157},
doi = {10.1145/3551349.3561157},
abstract = {Contemporary DNN testing works are frequently conducted using metamorphic testing (MT). In general, de facto MT frameworks mutate DNN input images using semantics-preserving mutations and determine if DNNs can yield consistent predictions. Nevertheless, we find that DNNs may rely on erroneous decisions (certain components on the DNN inputs) to make predictions, which may still retain the outputs by chance. Such DNN defects would be neglected by existing MT frameworks. Erroneous decisions, however, would likely result in successive mis-predictions over diverse images that may exist in real-life scenarios. This research aims to unveil the pervasiveness of hidden DNN defects caused by incorrect DNN decisions (but retaining consistent DNN predictions). To do so, we tailor and optimize modern eXplainable AI (XAI) techniques to identify visual concepts that represent regions in an input image upon which the DNN makes predictions. Then, we extend existing MT-based DNN testing frameworks to check the consistency of DNN decisions made over a test input and its mutated inputs. Our evaluation shows that existing MT frameworks are oblivious to a considerable number of DNN defects caused by erroneous decisions. We conduct human evaluations to justify the validity of our findings and to elucidate their characteristics. Through the lens of DNN decision-based metamorphic relations, we re-examine the effectiveness of metamorphic transformations proposed by existing MT frameworks. We summarize lessons from this study, which can provide insights and guidelines for future DNN testing.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {113},
numpages = {13},
keywords = {Deep learning testing},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3524613.3527809,
author = {Li, Chun},
title = {Mobile GUI test script generation from natural language descriptions using pre-trained model},
year = {2022},
isbn = {9781450393010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524613.3527809},
doi = {10.1145/3524613.3527809},
abstract = {GUI test scripts are valuable assets to guarantee the quality of mobile apps; however, manually writing executable GUI test scripts can incur huge cost. In this paper, we propose an approach to the generation of test scripts from the natural language descriptions, with the help of descriptions to locate elements and use the attributes of elements to select actions to construct the corresponding events. The construction of test scripts with the help of natural language descriptions can greatly reduce the burden of testers and is robust to changes in the position of GUI elements.},
booktitle = {Proceedings of the 9th IEEE/ACM International Conference on Mobile Software Engineering and Systems},
pages = {112–113},
numpages = {2},
location = {Pittsburgh, Pennsylvania},
series = {MOBILESoft '22}
}

@inproceedings{10.1145/3597926.3598033,
author = {Wu, Shuohan and Li, Jianfeng and Zhou, Hao and Fang, Yongsheng and Zhao, Kaifa and Wang, Haoyu and Qian, Chenxiong and Luo, Xiapu},
title = {CydiOS: A Model-Based Testing Framework for iOS Apps},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598033},
doi = {10.1145/3597926.3598033},
abstract = {To make an app stand out in an increasingly competitive market, developers must ensure its quality to deliver a better user experience. UI testing is a popular technique for quality assurance, which can thoroughly test the app from the users’ perspective. However, while considerable research has already studied UI testing on the Android platform, there is no research on iOS. This paper introduces CydiOS, a novel approach to performing model-based testing for iOS apps. CydiOS enhances the existing static analysis to build a more complete static model for the app under test. We propose an approach to retrieve runtime information to obtain real-time app context that can be mapped in the model. To improve the effectiveness of UI testing, we also introduce a potential-aware search algorithm to guide testing execution. We compare CydiOS with four representative algorithms(i.e., random, depth-first, stoat, and ape). We have evaluated CydiOS on 50 popular apps from App Store, and the results show that CydiOS outperforms other tools, achieving both higher code coverage and screen coverage. We open source CydiOS at https://github.com/SoftWare2022Testing/CydiOS, and a demo video can be found there.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1–13},
numpages = {13},
keywords = {iOS, Dynamic Testing, App Analysis},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

