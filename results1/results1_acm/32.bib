@proceedings{10.1145/3718491,
title = {AIBDF '24: Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum},
year = {2024},
isbn = {9798400710865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3656766,
title = {ICBAR '23: Proceedings of the 2023 3rd International Conference on Big Data, Artificial Intelligence and Risk Management},
year = {2023},
isbn = {9798400716478},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chengdu, China}
}

@article{10.1145/3572905,
author = {Kotti, Zoe and Galanopoulou, Rafaila and Spinellis, Diomidis},
title = {Machine Learning for Software Engineering: A Tertiary Study},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3572905},
doi = {10.1145/3572905},
abstract = {Machine learning (ML) techniques increase the effectiveness of software engineering (SE) lifecycle activities. We systematically collected, quality-assessed, summarized, and categorized 83 reviews in ML for SE published between 2009 and 2022, covering 6,117 primary studies. The SE areas most tackled with ML are software quality and testing, while human-centered areas appear more challenging for ML. We propose a number of ML for SE research challenges and actions, including conducting further empirical validation and industrial studies on ML, reconsidering deficient SE methods, documenting and automating data collection and pipeline processes, reexamining how industrial practitioners distribute their proprietary data, and implementing incremental ML approaches.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {256},
numpages = {39},
keywords = {systematic literature review, software engineering, machine learning, Tertiary study}
}

@inproceedings{10.1145/2499393.2499398,
author = {Calikli, Gul and Bener, Ayse},
title = {An algorithmic approach to missing data problem in modeling human aspects in software development},
year = {2013},
isbn = {9781450320160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499393.2499398},
doi = {10.1145/2499393.2499398},
abstract = {Background: In our previous research, we built defect prediction models by using confirmation bias metrics. Due to confirmation bias developers tend to perform unit tests to make their programs run rather than breaking their code. This, in turn, leads to an increase in defect density. The performance of prediction model that is built using confirmation bias was as good as the models that were built with static code or churn metrics.Aims: Collection of confirmation bias metrics may result in partially "missing data" due to developers' tight schedules, evaluation apprehension and lack of motivation as well as staff turnover. In this paper, we employ Expectation-Maximization (EM) algorithm to impute missing confirmation bias data.Method: We used four datasets from two large-scale companies. For each dataset, we generated all possible missing data configurations and then employed Roweis' EM algorithm to impute missing data. We built defect prediction models using the imputed data. We compared the performances of our proposed models with the ones that used complete data.Results: In all datasets, when missing data percentage is less than or equal to 50% on average, our proposed model that used imputed data yielded performance results that are comparable with the performance results of the models that used complete data.Conclusions: We may encounter the "missing data" problem in building defect prediction models. Our results in this study showed that instead of discarding missing or noisy data, in our case confirmation bias metrics, we can use effective techniques such as EM based imputation to overcome this problem.},
booktitle = {Proceedings of the 9th International Conference on Predictive Models in Software Engineering},
articleno = {10},
numpages = {10},
keywords = {confirmation bias, expectation maximisation (EM) algorithm, handling missing data, software defect prediction},
location = {Baltimore, Maryland, USA},
series = {PROMISE '13}
}

@article{10.1145/3343440,
author = {Kaur, Harsurinder and Pannu, Husanbir Singh and Malhi, Avleen Kaur},
title = {A Systematic Review on Imbalanced Data Challenges in Machine Learning: Applications and Solutions},
year = {2019},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3343440},
doi = {10.1145/3343440},
abstract = {In machine learning, the data imbalance imposes challenges to perform data analytics in almost all areas of real-world research. The raw primary data often suffers from the skewed perspective of data distribution of one class over the other as in the case of computer vision, information security, marketing, and medical science. The goal of this article is to present a comparative analysis of the approaches from the reference of data pre-processing, algorithmic and hybrid paradigms for contemporary imbalance data analysis techniques, and their comparative study in lieu of different data distribution and their application areas.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {79},
numpages = {36},
keywords = {sampling, machine learning, data analysis, Data imbalance}
}

@article{10.1145/3483424,
author = {Notaro, Paolo and Cardoso, Jorge and Gerndt, Michael},
title = {A Survey of AIOps Methods for Failure Management},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3483424},
doi = {10.1145/3483424},
abstract = {Modern society is increasingly moving toward complex and distributed computing systems. The increase in scale and complexity of these systems challenges O&amp;M teams that perform daily monitoring and repair operations, in contrast with the increasing demand for reliability and scalability of modern applications. For this reason, the study of automated and intelligent monitoring systems has recently sparked much interest across applied IT industry and academia. Artificial Intelligence for IT Operations (AIOps) has been proposed to tackle modern IT administration challenges thanks to Machine Learning, AI, and Big Data. However, AIOps as a research topic is still largely unstructured and unexplored, due to missing conventions in categorizing contributions for their data requirements, target goals, and components. In this work, we focus on AIOps for Failure Management (FM), characterizing and describing 5 different categories and 14 subcategories of contributions, based on their time intervention window and the target problem being solved. We review 100 FM solutions, focusing on applicability requirements and the quantitative results achieved, to facilitate an effective application of AIOps solutions. Finally, we discuss current development problems in the areas covered by AIOps and delineate possible future trends for AI-based failure management.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {81},
numpages = {45},
keywords = {artificial intelligence, failure management, IT operations and maintenance, AIOps}
}

@article{10.1145/3678259,
author = {Balasubramaniam, Balaji and Ahmed, Iftekhar and Bagheri, Hamid and Bradley, Justin},
title = {Carving Out Control Code: Automated Identification of Control Software in Autopilot Systems},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2378-962X},
url = {https://doi.org/10.1145/3678259},
doi = {10.1145/3678259},
abstract = {Cyber-physical systems interact with the world through software controlling physical effectors. Carefully designed controllers, implemented as safety-critical control software, also interact with other parts of the software suite, and may be difficult to separate, verify, or maintain. Moreover, some software changes, not intended to impact control system performance, do change controller response through a variety of means including interaction with external libraries or unmodeled changes only existing in the cyber system (e.g., exception handling). As a result, identifying safety-critical control software, its boundaries with other embedded software in the system, and the way in which control software evolves could help developers isolate, test, and verify control implementation, and improve control software development. In this work we present an automated technique, based on a novel application of machine learning, to detect commits related to control software, its changes, and how the control software evolves. We leverage messages from developers (e.g., commit comments), and code changes themselves to understand how control software is refined, extended, and adapted over time. We examine three distinct, popular, real-world, safety-critical autopilots—ArduPilot, Paparazzi UAV, and LibrePilot to test our method demonstrating an effective detection rate of 0.95 for control-related code changes.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = nov,
articleno = {39},
numpages = {20},
keywords = {Autopilot Software, Control Software, Small Uncrewed Aerial Vehicle, and Software code changes}
}

@inproceedings{10.1145/3617572.3617879,
author = {Pei, Yulong and Alamir, Salwa and Dolga, Rares and Shah, Sameena},
title = {Code Revert Prediction with Graph Neural Networks: A Case Study at J.P. Morgan Chase},
year = {2023},
isbn = {9798400703775},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617572.3617879},
doi = {10.1145/3617572.3617879},
abstract = {Code revert prediction, a specialized form of software defect detection, aims to forecast or predict the likelihood of code changes being reverted or rolled back in software development. This task is very important in practice because by identifying code changes that are more prone to being reverted, developers and project managers can proactively take measures to prevent issues, improve code quality, and optimize development processes. However, compared to code defect detection, code revert prediction has been rarely studied in previous research. Additionally, many previous methods for code defect detection relied on independent features but ignored relationships between code scripts. Moreover, new challenges are introduced due to constraints in an industry setting such as company regulation, limited features and large-scale codebase. To overcome these limitations, this paper presents a systematic empirical study for code revert prediction that integrates the code import graph with code features. Different strategies to address anomalies and data imbalance have been implemented including graph neural networks with imbalance classification and anomaly detection. We conduct the experiments on real-world code commit data within J.P. Morgan Chase which is extremely imbalanced in order to make a comprehensive comparison of these different approaches for the code revert prediction problem.},
booktitle = {Proceedings of the 1st International Workshop on Software Defect Datasets},
pages = {1–5},
numpages = {5},
keywords = {imbalanced classification, graph neural networks, anomaly detection, Code revert prediction},
location = {San Francisco, CA, USA},
series = {SDD 2023}
}

@inproceedings{10.1145/3273934.3273936,
author = {Ferenc, Rudolf and T\'{o}th, Zolt\'{a}n and Lad\'{a}nyi, Gergely and Siket, Istv\'{a}n and Gyim\'{o}thy, Tibor},
title = {A Public Unified Bug Dataset for Java},
year = {2018},
isbn = {9781450365932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3273934.3273936},
doi = {10.1145/3273934.3273936},
abstract = {Background: Bug datasets have been created and used by many researchers to build bug prediction models.Aims: In this work we collected existing public bug datasets and unified their contents.Method: We considered 5 public datasets which adhered to all of our criteria. We also downloaded the corresponding source code for each system in the datasets and performed their source code analysis to obtain a common set of source code metrics. This way we produced a unified bug dataset at class and file level that is suitable for further research (e.g. to be used in the building of new bug prediction models). Furthermore, we compared the metric definitions and values of the different bug datasets.Results: We found that (i) the same metric abbreviation can have different definitions or metrics calculated in the same way can have different names, (ii) in some cases different tools give different values even if the metric definitions coincide because (iii) one tool works on source code while the other calculates metrics on bytecode, or (iv) in several cases the downloaded source code contained more files which influenced the afferent metric values significantly.Conclusions: Apart from all these imprecisions, we think that having a common metric set can help in building better bug prediction models and deducing more general conclusions. We made the unified dataset publicly available for everyone. By using a public dataset as an input for different bug prediction related investigations, researchers can make their studies reproducible, thus able to be validated and verified.},
booktitle = {Proceedings of the 14th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {12–21},
numpages = {10},
keywords = {Bug dataset, code metrics, static code analysis},
location = {Oulu, Finland},
series = {PROMISE'18}
}

@inproceedings{10.1145/3475716.3475781,
author = {Croft, Roland and Newlands, Dominic and Chen, Ziyu and Babar, M. Ali},
title = {An Empirical Study of Rule-Based and Learning-Based Approaches for Static Application Security Testing},
year = {2021},
isbn = {9781450386654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475716.3475781},
doi = {10.1145/3475716.3475781},
abstract = {Background: Static Application Security Testing (SAST) tools purport to assist developers in detecting security issues in source code. These tools typically use rule-based approaches to scan source code for security vulnerabilities. However, due to the significant shortcomings of these tools (i.e., high false positive rates), learning-based approaches for Software Vulnerability Prediction (SVP) are becoming a popular approach. Aims: Despite the similar objectives of these two approaches, their comparative value is unexplored. We provide an empirical analysis of SAST tools and SVP models, to identify their relative capabilities for source code security analysis. Method: We evaluate the detection and assessment performance of several common SAST tools and SVP models on a variety of vulnerability datasets. We further assess the viability and potential benefits of combining the two approaches. Results: SAST tools and SVP models provide similar detection capabilities, but SVP models exhibit better overall performance for both detection and assessment. Unification of the two approaches is difficult due to lacking synergies. Conclusions: Our study generates 12 main findings which provide insights into the capabilities and synergy of these two approaches. Through these observations we provide recommendations for use and improvement.},
booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {8},
numpages = {12},
keywords = {Static Application Security Testing, Security, Machine Learning},
location = {Bari, Italy},
series = {ESEM '21}
}

@inproceedings{10.1145/3643991.3644920,
author = {Zhao, Guoliang and Georgiou, Stefanos and Hassan, Safwat and Zou, Ying and Truong, Derek and Corbin, Toby},
title = {Enhancing Performance Bug Prediction Using Performance Code Metrics},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644920},
doi = {10.1145/3643991.3644920},
abstract = {Performance bugs are non-functional defects that can significantly reduce the performance of an application (e.g., software hanging or freezing) and lead to poor user experience. Prior studies found that each type of performance bugs follows a unique code-based performance anti-pattern and proposed different approaches to detect such anti-patterns by analyzing the source code of a program. However, each approach can only recognize one performance anti-pattern. Different approaches need to be applied separately to identify different performance anti-patterns. To predict a large variety of performance bug types using a unified approach, we propose an approach that predicts performance bugs by leveraging various historical data (e.g., source code and code change history). We collect performance bugs from 80 popular Java projects. Next, we propose performance code metrics to capture the code characteristics of performance bugs. We build performance bug predictors using machine learning models, such as Random Forest, eXtreme Gradient Boosting, and Linear Regressions. We observe that: (1) Random Forest and eXtreme Gradient Boosting are the best algorithms for predicting performance bugs at a file level with a median of 0.84 AUC, 0.21 PR-AUC, and 0.38 MCC; (2) The proposed performance code metrics have the most significant impact on the performance of our models compared to code and process metrics. In particular, the median AUC, PR-AUC, and MCC of the studied machine learning models drop by 7.7%, 25.4%, and 20.2% without using the proposed performance code metrics; and (3) Our approach can predict additional performance bugs that are not covered by the anti-patterns proposed in the prior studies.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {50–62},
numpages = {13},
keywords = {performance bugs, performance anti-patterns, performance code metrics, performance bug prediction},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@proceedings{10.1145/3643665,
title = {FinanSE '24: Proceedings of the 1st IEEE/ACM Workshop on Software Engineering Challenges in Financial Firms},
year = {2024},
isbn = {9798400705687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Software development has an integral role in every financial organisation; indeed, almost every service provided by a bank utilizes some form of software solution. While SE research has led to solutions and innovations for many popular SE problems, there remain unresolved challenges, particularly, those challenges faced in software development in financial firms. An example of such a challenge is defect prediction, where defects are not equal as some may lead to larger reputational and financial damage than others. Consequently, testing and verification is burdened with a further set of restraints for finance-based SE teams. Financial firms began automating processes as early as the 1960s, and as such, must maintain large legacy systems which may host critical operations. This problem is further exacerbated by the numerous mergers and acquisitions common in the financial sector, which leaves firms with a set of heterogeneous legacy systems that need to communicate with one another effectively and efficiently. Therefore, maintaining these systems while modernizing them leads to intriguing challenges, spanning from model extraction and process optimisation to code translation. Moreover, highly regulated institutions like financial firms require a high degree of transparency and accountability. This requirement facilitates the need for model fairness and explainability for any SE solution, in particular those that rely on AI.The 1st International Workshop on Software Engineering Challenges in Financial Firms (FinanSE 2024) is a forum to bring together academia and industry to share new ideas and results in tackling these challenges.},
location = {Lisbon, Portugal}
}

@article{10.1145/3694782,
author = {Lin, Ruyan and Fu, Yulong and Yi, Wei and Yang, Jincheng and Cao, Jin and Dong, Zhiqiang and Xie, Fei and Li, Hui},
title = {Vulnerabilities and Security Patches Detection in OSS: A Survey},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3694782},
doi = {10.1145/3694782},
abstract = {Over the past decade, Open Source Software (OSS) has experienced rapid growth and widespread adoption, attributed to its openness and editability. However, this expansion has also brought significant security challenges, particularly introducing and propagating software vulnerabilities. Despite the use of machine learning and formal methods to tackle these issues, there remains a notable gap in comprehensive surveys that summarize and analyze both Vulnerability Detection (VD) and Security Patch Detection (SPD) in OSS. This article seeks to bridge this gap through an extensive survey that evaluates 127 technical studies published between 2014 and 2023, structured around the Vulnerability-Patch lifecycle. We begin by delineating the six critical events that constitute the Vulnerability-Patch lifecycle, leading to an in-depth exploration of the Vulnerability-Patch ecosystem. We then systematically review the databases commonly used in VD and SPD, and analyze their characteristics. Subsequently, we examine existing VD methods, focusing on traditional and deep learning based approaches. Additionally, we organize current security patch identification methods by kernel type and discuss techniques for detecting the presence of security patches. Based on our comprehensive review, we identify open research questions and propose future research directions that merit further exploration.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {23},
numpages = {37},
keywords = {Open source software, vulnerability detection, security patch detection, software security, AI security}
}

@inproceedings{10.1145/3377812.3381396,
author = {Halepmollasi, Ru\c{s}en},
title = {A composed technical debt identification methodology to predict software vulnerabilities},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3381396},
doi = {10.1145/3377812.3381396},
abstract = {Technical debt (TD), its impact on development and its consequences such as defects and vulnerabilities, are of common interest and great importance to software researchers and practitioners. Although there exist many studies investigating TD, the majority of them focuses on identifying and detecting TD from a single stage of development. There are also studies that analyze vulnerabilities focusing on some phases of the life cycle. Moreover, several approaches have investigated the relationship between TD and vulnerabilities, however, the generalizability and validity of findings are limited due to small dataset. In this study, we aim to identify TD through multiple phases of development, and to automatically measure it through data and text mining techniques to form a comprehensive feature model. We plan to utilize neural network based classifiers that will incorporate evolutionary changes on TD measures into predicting vulnerabilities. Our approach will be empirically assessed on open source and industrial projects.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {186–189},
numpages = {4},
keywords = {technical debt, software security, machine learning, feature engineering},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/2597073.2597080,
author = {Tulsian, Varun and Kanade, Aditya and Kumar, Rahul and Lal, Akash and Nori, Aditya V.},
title = {MUX: algorithm selection for software model checkers},
year = {2014},
isbn = {9781450328630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597073.2597080},
doi = {10.1145/2597073.2597080},
abstract = {With the growing complexity of modern day software, software model checking has become a critical technology for ensuring correctness of software. As is true with any promising technology, there are a number of tools for software model checking. However, their respective performance trade-offs are difficult to characterize accurately – making it difficult for practitioners to select a suitable tool for the task at hand. This paper proposes a technique called MUX that addresses the problem of selecting the most suitable software model checker for a given input instance. MUX performs machine learning on a repository of software verification instances. The algorithm selector, synthesized through machine learning, uses structural features from an input instance, comprising a program-property pair, at runtime and determines which tool to use.  We have implemented MUX for Windows device drivers and evaluated it on a number of drivers and model checkers. Our results are promising in that the algorithm selector not only avoids a significant number of timeouts but also improves the total runtime by a large margin, compared to any individual model checker. It also outperforms a portfolio-based algorithm selector being used in Microsoft at present. Besides, MUX identifies structural features of programs that are key factors in determining performance of model checkers.},
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
pages = {132–141},
numpages = {10},
keywords = {software model checking, machine learning, Algorithm selection},
location = {Hyderabad, India},
series = {MSR 2014}
}

@article{10.1145/3127360.3127368,
author = {Kumar, Lov and Behera, Ranjan Kumar and Rath, Santanu and Sureka, Ashish},
title = {Transfer Learning for Cross-Project Change-Proneness Prediction in Object-Oriented Software Systems: A Feasibility Analysis},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3127360.3127368},
doi = {10.1145/3127360.3127368},
abstract = {Change-prone classes or modules are defined as regions of the source code which are more likely to change as a result of a software development of maintenance activity. Automatic identification of change-prone classes are useful for the software development team as they can focus their testing efforts on areas within the source code which are more likely to change. Several machine learning techniques have been proposed for predicting change-prone classes based on the application of source code metrics as indicators. However, most of the work has focused on within-project training and model building. There are several real word scenario in which sufficient training dataset is not available for model building such as in the case of a new project. Cross-project prediction is an approach which consists of training a model from dataset belonging to one project and testing it on dataset belonging to a different project. Cross-project change-proneness prediction is relatively unexplored.We propose a machine learning based approach for cross-project change-proneness prediction. We conduct experiments on 10 open-source Eclipse plug-ins and demonstrate the effectiveness of our approach. We frame several research questions comparing the performance of within project and cross project prediction and also propose a Genetic Algorithm (GA) based approach for identifying the best set of source code metrics. We conclude that for within project experimental setting, Random Forest (RF) technique results in the best precision. In case of cross-project change-proneness prediction, our analysis reveals that the NDTF ensemble method performs higher than other individual classifiers (such as decision tree and logistic regression) and ensemble methods in the experimental dataset. We conduct a comparison of within-project, cross-project without GA and cross-project with GA and our analysis reveals that cross-project with GA performs best followed by within-project and then cross-project without GA.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {1–11},
numpages = {11}
}

@inproceedings{10.1145/3691621.3694950,
author = {Liang, Chen and Wei, Qiang and Jiang, Zirui and Wang, Yisen and Du, Jiang},
title = {A Source Code Vulnerability Detection Method Based on Adaptive Graph Neural Networks},
year = {2024},
isbn = {9798400712494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691621.3694950},
doi = {10.1145/3691621.3694950},
abstract = {This paper proposes a mobile application vulnerability detection method based on Code Property Graphs (CPG) and adaptive graph neural networks. The method first converts source code into CPGs, then uses CodeBERT to vectorize CPG nodes. Subsequently, high-level graph features are extracted through graph centrality analysis, and an adaptive graph neural network model combining Transformer's adaptive attention mechanism and Graph Convolutional Networks (GCN) is designed for feature learning and vulnerability detection. Experimental results show that this method achieves an F1 score of 82.9% on real vulnerability datasets, an improvement of 13.6%-49.9% compared to existing methods. Ablation experiments further validate the effectiveness of each key component. This research provides new insights and effective methods based on deep learning for mobile application security, demonstrating high application value and practical significance.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering Workshops},
pages = {187–196},
numpages = {10},
keywords = {vulnerability detection, code property graph, graph neural network, centrality analysis, adaptive attention mechanism},
location = {Sacramento, CA, USA},
series = {ASEW '24}
}

@proceedings{10.1145/3663529,
title = {FSE 2024: Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to FSE 2024, the ACM International Conference on the Foundations of Software Engineering (FSE) 2024. The conference now has a shorter name! FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {Porto de Galinhas, Brazil}
}

@inproceedings{10.1109/ESEM.2017.12,
author = {Ahmed, Iftekhar and Brindescu, Caius and Mannan, Umme Ayda and Jensen, Carlos and Sarma, Anita},
title = {An empirical examination of the relationship between code smells and merge conflicts},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.12},
doi = {10.1109/ESEM.2017.12},
abstract = {Background: Merge conflicts are a common occurrence in software development. Researchers have shown the negative impact of conflicts on the resulting code quality and the development workflow. Thus far, no one has investigated the effect of bad design (code smells) on merge conflicts. Aims: We posit that entities that exhibit certain types of code smells are more likely to be involved in a merge conflict. We also postulate that code elements that are both "smelly" and involved in a merge conflict are associated with other undesirable effects (more likely to be buggy). Method: We mined 143 repositories from GitHub and recreated 6,979 merge conflicts to obtain metrics about code changes and conflicts. We categorized conflicts into semantic or non-semantic, based on whether changes affected the Abstract Syntax Tree. For each conflicting change, we calculate the number of code smells and the number of future bug-fixes associated with the affected lines of code. Results: We found that entities that are smelly are three times more likely to be involved in merge conflicts. Method-level code smells (Blob Operation and Internal Duplication) are highly correlated with semantic conflicts. We also found that code that is smelly and experiences merge conflicts is more likely to be buggy. Conclusion: Bad code design not only impacts maintainability, it also impacts the day to day operations of a project, such as merging contributions, and negatively impacts the quality of the resulting code. Our findings indicate that research is needed to identify better ways to support merge conflict resolution to minimize its effect on code quality.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {58–67},
numpages = {10},
keywords = {merge conflict, machine learning, empirical analysis, code smell},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@inproceedings{10.1145/3238147.3238193,
author = {He, Pinjia and Chen, Zhuangbin and He, Shilin and Lyu, Michael R.},
title = {Characterizing the natural language descriptions in software logging statements},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238193},
doi = {10.1145/3238147.3238193},
abstract = {Logging is a common programming practice of great importance in modern software development, because software logs have been widely used in various software maintenance tasks. To provide high-quality logs, developers need to design the description text in logging statements carefully. Inappropriate descriptions will slow down or even mislead the maintenance process, such as postmortem analysis. However, there is currently a lack of rigorous guide and specifications on developer logging behaviors, which makes the construction of description text in logging statements a challenging problem. To fill this significant gap, in this paper, we systematically study what developers log, with focus on the usage of natural language descriptions in logging statements. We obtain 6 valuable findings by conducting source code analysis on 10 Java projects and 7 C# projects, which contain 28,532,975 LOC and 115,159 logging statements in total. Furthermore, our study demonstrates the potential of automated description text generation for logging statements by obtaining up to 49.04 BLEU-4 score and 62.1 ROUGE-L score using a simple information retrieval method. To facilitate future research in this field, the datasets have been publicly released.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {178–189},
numpages = {12},
keywords = {natural language processing, empirical study, Logging},
location = {Montpellier, France},
series = {ASE '18}
}

@proceedings{10.1145/3583133,
title = {GECCO '23 Companion: Proceedings of the Companion Conference on Genetic and Evolutionary Computation},
year = {2023},
isbn = {9798400701207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {GECCO is the largest peer-reviewed conference in the field of Evolutionary Computation, and the main conference of the Special Interest Group on Genetic and Evolutionary Computation (SIGEVO) of the Association for Computing Machinery (ACM).},
location = {Lisbon, Portugal}
}

@article{10.1145/3678167,
author = {Liu, Yue and Tantithamthavorn, Chakkrit and Liu, Yonghui and Thongtanunam, Patanamon and Li, Li},
title = {Automatically Recommend Code Updates: Are We There Yet?},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {8},
issn = {1049-331X},
url = {https://doi.org/10.1145/3678167},
doi = {10.1145/3678167},
abstract = {In recent years, large pre-trained Language Models of Code (CodeLMs) have shown promising results on various software engineering tasks. One such task is automatic code update recommendation, which transforms outdated code snippets into their approved and revised counterparts. Although many CodeLM-based approaches have been proposed, claiming high accuracy, their effectiveness and reliability on real-world code update tasks remain questionable. In this article, we present the first extensive evaluation of state-of-the-art CodeLMs for automatically recommending code updates. We assess their performance on two diverse datasets of paired updated methods, considering factors such as temporal evolution, project specificity, method size, and update complexity. Our results reveal that while CodeLMs exhibit higher performance in settings that ignore temporal information, they struggle in more realistic time-wise scenarios and generalize poorly to new projects. Furthermore, CodeLM performance decreases significantly for larger methods and more complex updates. Furthermore, we observe that many CodeLM-generated “updates” are actually null, especially in time-wise settings, and meaningful edits remain challenging. Our findings highlight the significant gap between the perceived and actual effectiveness of CodeLMs for real-world code update recommendation and emphasize the need for more research on improving their practicality, robustness, and generalizability.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {217},
numpages = {27},
keywords = {Code updates, neural machine translation}
}

@inproceedings{10.1145/3106237.3106256,
author = {Fu, Wei and Menzies, Tim},
title = {Easy over hard: a case study on deep learning},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106256},
doi = {10.1145/3106237.3106256},
abstract = {While deep learning is an exciting new technique, the benefits of this method need to be assessed with respect to its computational cost. This is particularly important for deep learning since these learners need hours (to weeks) to train the model. Such long training time limits the ability of (a)~a researcher to test the stability of their conclusion via repeated runs with different random seeds; and (b)~other researchers to repeat, improve, or even refute that original work.  For example, recently, deep learning was used to find which questions in the Stack Overflow programmer discussion forum can be linked together. That deep learning system took 14 hours to execute. We show here that applying a very simple optimizer called DE to fine tune SVM, it can achieve similar (and sometimes better) results. The DE approach terminated in 10 minutes; i.e. 84 times faster hours than deep learning method.  We offer these results as a cautionary tale to the software analytics community and suggest that not every new innovation should be applied without critical analysis. If researchers deploy some new and expensive process, that work should be baselined against some simpler and faster alternatives.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {49–60},
numpages = {12},
keywords = {software analytic, parameter tuning, differential evolution, deep learning, data analytics for software engineering, Search based software engineering, SVM},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@proceedings{10.1145/3617572,
title = {SDD 2023: Proceedings of the 1st International Workshop on Software Defect Datasets},
year = {2023},
isbn = {9798400703775},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the First International Workshop on Software Defect Datasets (SDD), co-located with ESEC/FSE 2023 and to take place in San Francisco, CA on December 8th, 2023.},
location = {San Francisco, CA, USA}
}

@proceedings{10.1145/3661167,
title = {EASE '24: Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Salerno, Italy}
}

@proceedings{10.5555/3606013,
title = {ICSE '23: Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Victoria, Australia}
}

@inproceedings{10.1145/3540250.3558950,
author = {Kim, Hyungjin and Kwon, Yonghwi and Joh, Sangwoo and Kwon, Hyukin and Ryou, Yeonhee and Kim, Taeksu},
title = {Understanding automated code review process and developer experience in industry},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558950},
doi = {10.1145/3540250.3558950},
abstract = {Code Review Automation can reduce human efforts during code review by automatically providing valuable information to reviewers. Nevertheless, it is a challenge to automate the process for large-scale companies, such as Samsung Electronics, due to their complexity: various development environments, frequent review requests, huge size of software, and diverse process among the teams. In this work, we show how we automated the code review process for those intricate environments, and share some lessons learned during two years of operation. Our unified code review automation system, Code Review Bot, is designed to process review requests holistically regardless of such environments, and checks various quality-assurance items such as potential defects in the code, coding style, test coverage, and open source license violations. Some key findings include: 1) about 60% of issues found by Code Review Bot were reviewed and fixed in advance of product releases, 2) more than 70% of developers gave positive feedback about the system, 3) developers rapidly and actively responded to reviews, and 4) the automation did not much affect the amount or the frequency of human code reviews compared to the internal policy to encourage code review activities. Our findings provide practical evidence that automating code review helps assure software quality.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1398–1407},
numpages = {10},
keywords = {static analysis, review bot, code review automation, code review},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@proceedings{10.1145/3643991,
title = {MSR '24: Proceedings of the 21st International Conference on Mining Software Repositories},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {MSR is a thriving research community that organizes a yearly conference with a solid reputation amongst software engineering researchers.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3196398.3196431,
author = {Tufano, Michele and Watson, Cody and Bavota, Gabriele and Di Penta, Massimiliano and White, Martin and Poshyvanyk, Denys},
title = {Deep learning similarities from different representations of source code},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196431},
doi = {10.1145/3196398.3196431},
abstract = {Assessing the similarity between code components plays a pivotal role in a number of Software Engineering (SE) tasks, such as clone detection, impact analysis, refactoring, etc. Code similarity is generally measured by relying on manually defined or hand-crafted features, e.g., by analyzing the overlap among identifiers or comparing the Abstract Syntax Trees of two code components. These features represent a best guess at what SE researchers can utilize to exploit and reliably assess code similarity for a given task. Recent work has shown, when using a stream of identifiers to represent the code, that Deep Learning (DL) can effectively replace manual feature engineering for the task of clone detection. However, source code can be represented at different levels of abstraction: identifiers, Abstract Syntax Trees, Control Flow Graphs, and Bytecode. We conjecture that each code representation can provide a different, yet orthogonal view of the same code fragment, thus, enabling a more reliable detection of similarities in code. In this paper, we demonstrate how SE tasks can benefit from a DL-based approach, which can automatically learn code similarities from different representations.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {542–553},
numpages = {12},
keywords = {code similarities, deep learning, neural networks},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1145/3238147.3240467,
author = {Mo, Ran and Snipes, Will and Cai, Yuanfang and Ramaswamy, Srini and Kazman, Rick and Naedele, Martin},
title = {Experiences applying automated architecture analysis tool suites},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240467},
doi = {10.1145/3238147.3240467},
abstract = {In this paper, we report our experiences of applying three complementary automated software architecture analysis techniques, supported by a tool suite, called DV8, to 8 industrial projects within a large company. DV8 includes two state-of-the-art architecture-level maintainability metrics—Decoupling Level and Propagation Cost, an architecture flaw detection tool, and an architecture root detection tool. We collected development process data from the project teams as input to these tools, reported the results back to the practitioners, and followed up with telephone conferences and interviews. Our experiences revealed that the metrics scores, quantitative debt analysis, and architecture flaw visualization can effectively bridge the gap between management and development, help them decide if, when, and where to refactor. In particular, the metrics scores, compared against industrial benchmarks, faithfully reflected the practitioners’ intuitions about the maintainability of their projects, and enabled them to better understand the maintainability relative to other projects internal to their company, and to other industrial products. The automatically detected architecture flaws and roots enabled the practitioners to precisely pinpoint, visualize, and quantify the “hotspots" within the systems that are responsible for high maintenance costs. Except for the two smallest projects for which both architecture metrics indicated high maintainability, all other projects are planning or have already begun refactorings to address the problems detected by our analyses. We are working on further automating the tool chain, and transforming the analysis suite into deployable services accessible by all projects within the company.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {779–789},
numpages = {11},
keywords = {Software Quality, Software Maintenance, Software Architecture},
location = {Montpellier, France},
series = {ASE '18}
}

@proceedings{10.1145/3544902,
title = {ESEM '22: Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement},
year = {2022},
isbn = {9781450394277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Helsinki, Finland}
}

@proceedings{10.1145/3644384,
title = {TechDebt '24: Proceedings of the 7th ACM/IEEE International Conference on Technical Debt},
year = {2024},
isbn = {9798400705908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {TechDebt 2024 brings together leading software engineering researchers, practitioners and educators to explore the theoretical and practical techniques for managing Technical Debt and to share experiences, challenges, and best practices to address open issues in both the industry and research and bridge the gap between them. In addition, a unique aspect of the Technical Debt community is its emphasis on using existing tools to manage Technical Debt as well as developing new ones.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3689236,
title = {ICCSIE '24: Proceedings of the 2024 9th International Conference on Cyber Security and Information Engineering},
year = {2024},
isbn = {9798400718137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3609437,
title = {Internetware '23: Proceedings of the 14th Asia-Pacific Symposium on Internetware},
year = {2023},
isbn = {9798400708947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hangzhou, China}
}

@inproceedings{10.1109/ICSE48619.2023.00143,
author = {Poozhithara, Jeffy Jahfar and Asuncion, Hazeline U. and Lagesse, Brent},
title = {Keyword Extraction from Specification Documents for Planning Security Mechanisms},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00143},
doi = {10.1109/ICSE48619.2023.00143},
abstract = {Software development companies heavily invest both time and money to provide post-production support to fix security vulnerabilities in their products. Current techniques identify vulnerabilities from source code using static and dynamic analyses. However, this does not help integrate security mechanisms early in the architectural design phase. We develop VDocScan, a technique for predicting vulnerabilities based on specification documents, even before the development stage. We evaluate VDocScan using an extensive dataset of CVE vulnerability reports mapped to over 3600 product documentations. An evaluation of 8 CWE vulnerability pillars shows that even interpretable whitebox classifiers predict vulnerabilities with up to 61.1% precision and 78% recall. Further, using strategies to improve the relevance of extracted keywords, addressing class imbalance, segregating products into categories such as Operating Systems, Web applications, and Hardware, and using blackbox ensemble models such as the random forest classifier improves the performance to 96% precision and 91.1% recall. The high precision and recall shows that VDocScan can anticipate vulnerabilities detected in a product's lifetime ahead of time during the Design phase to incorporate necessary security mechanisms. The performance is consistently high for vulnerabilities with the mode of introduction: architecture and design.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1661–1673},
numpages = {13},
keywords = {keyword extraction, CWE, CVE, vulnerability prediction},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3238147.3238201,
author = {Mukelabai, Mukelabai and Ne\v{s}i\'{c}, Damir and Maro, Salome and Berger, Thorsten and Stegh\"{o}fer, Jan-Philipp},
title = {Tackling combinatorial explosion: a study of industrial needs and practices for analyzing highly configurable systems},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238201},
doi = {10.1145/3238147.3238201},
abstract = {Highly configurable systems are complex pieces of software. To tackle this complexity, hundreds of dedicated analysis techniques have been conceived, many of which able to analyze system properties for all possible system configurations, as opposed to traditional, single-system analyses. Unfortunately, it is largely unknown whether these techniques are adopted in practice, whether they address actual needs, or what strategies practitioners actually apply to analyze highly configurable systems. We present a study of analysis practices and needs in industry. It relied on a survey with 27 practitioners engineering highly configurable systems and follow-up interviews with 15 of them, covering 18 different companies from eight countries. We confirm that typical properties considered in the literature (e.g., reliability) are relevant, that consistency between variability models and artifacts is critical, but that the majority of analyses for specifications of configuration options (a.k.a., variability model analysis) is not perceived as needed. We identified rather pragmatic analysis strategies, including practices to avoid the need for analysis. For instance, testing with experience-based sampling is the most commonly applied strategy, while systematic sampling is rarely applicable. We discuss analyses that are missing and synthesize our insights into suggestions for future research.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {155–166},
numpages = {12},
keywords = {Product Lines, Highly Configurable Systems, Analysis},
location = {Montpellier, France},
series = {ASE '18}
}

@proceedings{10.1145/3643916,
title = {ICPC '24: Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ICPC is the premier (CORE A) venue for research on program comprehension. Research on program comprehension encompasses both human activities for comprehending the software and technologies for supporting such comprehension.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3607947,
title = {IC3-2023: Proceedings of the 2023 Fifteenth International Conference on Contemporary Computing},
year = {2023},
isbn = {9798400700224},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Noida, India}
}

@inproceedings{10.5555/3291291.3291310,
author = {Barrak, Amine and Laverdi\`{e}re, Marc-Andr\'{e} and Khomh, Foutse and An, Le and Merlo, Ettore},
title = {Just-in-time detection of protection-impacting changes on WordPress and MediaWiki},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {Access control mechanisms based on roles and privileges restrict the access of users to security sensitive resources in a multi-user software system. Unintentional privilege protection changes may occur during the evolution of a system, which may introduce security vulnerabilities; threatening user's confidential data, and causing other severe problems. In this paper, we use the Pattern Traversal Flow Analysis technique to identify definite protection differences in WordPress and MediaWiki systems. We analyse the evolution of privilege protections across 211 and 193 releases from respectively WordPress and Mediawiki, and observe that around 60% of commits affect privileges protections in both projects. We refer to these commits as protection-impacting change (PIC) commits. To help developers identify PIC commits just-in-time, we extract a series of metrics from commit logs and source code, and build statistical models. The evaluation of these models revealed that they can achieve a precision up to 73.8% and a recall up to 98.8% in WordPress and for MediaWiki, a precision up to 77.2% and recall up to 97.8%. Among the metrics examined, commit churn, bug fixing, author experiences and code complexity between two releases are the most important predictors in the models. We performed a qualitative analysis of false positives and false negatives and observe that PIC commits detectors should ignore documentation-only commits and process code changes without the comments.Software organizations can use our proposed approach and models, to identify unintentional privilege protection changes as soon as they are introduced, in order to prevent the introduction of vulnerabilities in their systems.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {178–188},
numpages = {11},
keywords = {security vulnerabilities, reliability, protection impacting changes, privilege protection changes},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@proceedings{10.1145/3639478,
title = {ICSE-Companion '24: Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ICSE is the leading and, by far, the largest conference in Software Engineering, attracting researchers, practitioners, and students worldwide. ICSE2024 is co-located with 11 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3641399,
title = {ISEC '24: Proceedings of the 17th Innovations in Software Engineering Conference},
year = {2024},
isbn = {9798400717673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bangalore, India}
}

@proceedings{10.1145/2970276,
title = {ASE '16: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/2810146.2810152,
author = {An, Le and Khomh, Foutse},
title = {An Empirical Study of Crash-inducing Commits in Mozilla Firefox},
year = {2015},
isbn = {9781450337151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810146.2810152},
doi = {10.1145/2810146.2810152},
abstract = {Software crashes are feared by software organisations and end users. Many software organisations have embedded automatic crash reporting tools in their software systems to help development teams track and fix crash-related bugs. Previous techniques, which focus on the triaging of crash-types and crash-related bugs, can help software practitioners increase their debugging efficiency on crashes. But, these techniques can only be applied after the crashes occurred and already affected a large population of users. To help software organisations detect and address crash-prone code early, we conduct a case study of commits that would lead to crashes, called "crash-inducing commits", in Mozilla Firefox. We found that crash-inducing commits are often submitted by developers with less experience. Developers perform more addition and deletion of lines of code in crash-inducing commits. We built predictive models to help software practitioners detect and fix crash-prone bugs early on. Our predictive models achieve a precision of 61.4% and a recall of 95.0%. Software organisations can use our proposed predictive models to track and fix crash-prone commits early on before they negatively impact users; increasing bug fixing efficiency and user-perceived quality.},
booktitle = {Proceedings of the 11th International Conference on Predictive Models and Data Analytics in Software Engineering},
articleno = {5},
numpages = {10},
keywords = {Crash analysis, bug triaging, mining software repositories, prediction model},
location = {Beijing, China},
series = {PROMISE '15}
}

@proceedings{10.1145/3555228,
title = {SBES '22: Proceedings of the XXXVI Brazilian Symposium on Software Engineering},
year = {2022},
isbn = {9781450397353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Brazil}
}

@article{10.1145/3585004,
author = {Badampudi, Deepika and Unterkalmsteiner, Michael and Britto, Ricardo},
title = {Modern Code Reviews—Survey of Literature and Practice},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3585004},
doi = {10.1145/3585004},
abstract = {Background: Modern Code Review (MCR) is a lightweight alternative to traditional code inspections. While secondary studies on MCR exist, it is uanknown whether the research community has targeted themes that practitioners consider important.Objectives: The objectives are to provide an overview of MCR research, analyze the practitioners’ opinions on the importance of MCR research, investigate the alignment between research and practice, and propose future MCR research avenues.Method: We conducted a systematic mapping study to survey state of the art until and including 2021, employed the Q-Methodology to analyze the practitioners’ perception of the relevance of MCR research, and analyzed the primary studies’ research impact.Results: We analyzed 244 primary studies, resulting in five themes. As a result of the 1,300 survey data points, we found that the respondents are positive about research investigating the impact of MCR on product quality and MCR process properties. In contrast, they are negative about human factor– and support systems–related research.Conclusion: These results indicate a misalignment between the state of the art and the themes deemed important by most survey respondents. Researchers should focus on solutions that can improve the state of MCR practice. We provide an MCR research agenda that can potentially increase the impact of MCR research.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {107},
numpages = {61},
keywords = {practitioner survey, literature survey, Modern code review}
}

@proceedings{10.1145/3593434,
title = {EASE '23: Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Oulu, Finland}
}

@proceedings{10.1145/3555776,
title = {SAC '23: Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tallinn, Estonia}
}

@proceedings{10.1145/3540250,
title = {ESEC/FSE 2022: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of all members of the organizing committee, we are delighted to welcome everyone to the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2022. The event continues the long, distinguished ESEC/FSE tradition of presenting the most innovative research, and facilitating interactions between scientists and engineers who are passionate about advancing the theory and practice of software engineering.},
location = {Singapore, Singapore}
}

@proceedings{10.1145/3671016,
title = {Internetware '24: Proceedings of the 15th Asia-Pacific Symposium on Internetware},
year = {2024},
isbn = {9798400707056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Macau, China}
}

@proceedings{10.1145/3674805,
title = {ESEM '24: Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
year = {2024},
isbn = {9798400710476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Barcelona, Spain}
}

@proceedings{10.1145/3611643,
title = {ESEC/FSE 2023: Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to ESEC/FSE 2023, the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ESEC/FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. ESEC/FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {San Francisco, CA, USA}
}

@proceedings{10.1145/3597503,
title = {ICSE '24: Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3551349,
title = {ASE '22: Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
year = {2022},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rochester, MI, USA}
}

@inproceedings{10.1145/1370750.1370762,
author = {Layman, Lucas and Nagappan, Nachiappan and Guckenheimer, Sam and Beehler, Jeff and Begel, Andrew},
title = {Mining software effort data: preliminary analysis of visual studio team system data},
year = {2008},
isbn = {9781605580241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370750.1370762},
doi = {10.1145/1370750.1370762},
abstract = {In the software development process, scheduling and predictability are important components to delivering a product on time and within budget. Effort estimation artifacts offer a rich data set for improving scheduling accuracy and for understanding the development process. Effort estimation data for 55 features in the latest release of Visual Studio Team System (VSTS) were collected and analyzed for trends, patterns, and differences. Statistical analysis shows that actual estimation error was positively correlated with feature size, and that in-process metrics of estimation error were also correlated with the final estimation error. These findings suggest that smaller features can be estimated more accurately, and that in-process estimation error metrics can be provide a quantitative supplement to developer intuition regarding high-risk features during the development process.},
booktitle = {Proceedings of the 2008 International Working Conference on Mining Software Repositories},
pages = {43–46},
numpages = {4},
keywords = {prediction, effort estimation},
location = {Leipzig, Germany},
series = {MSR '08}
}

@proceedings{10.5555/3606010,
title = {ICSE '23: Proceedings of the 45th International Conference on Software Engineering},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Victoria, Australia}
}

@proceedings{10.1145/3597926,
title = {ISSTA 2023: Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to ISSTA 2023, the 32nd edition of the International Symposium on Software Testing and Analysis, to be held on July 18–20, 2023 in Seattle, USA. The symposium has become a premier scientific event in the expanding area of software testing and analysis, with a strong appeal to researchers from all continents.},
location = {Seattle, WA, USA}
}

@proceedings{10.1145/2950290,
title = {FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@proceedings{10.1145/3634737,
title = {ASIA CCS '24: Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to ACM AsiaCCS 2024, the 19th ACM Asia Conference on Computer and Communications Security. AsiaCCS 2024 takes place in Singapore from 1 July to 5 July.},
location = {Singapore, Singapore}
}

@proceedings{10.1145/3650212,
title = {ISSTA 2024: Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 33rd edition of the International Symposium on Software Testing and Analysis, ISSTA 2024, held on September 16--20, 2024 in Vienna, Austria. ISSTA 2024 is co-located with ECOOP and MPLR 2024. ISSTA brings together academics, industrial researchers, and practitioners from all over the world working on testing and analyzing software systems.},
location = {Vienna, Austria}
}

@article{10.1145/1082983.1085124,
title = {Frontmatter (TOC, Letters, Election results, Software Reliability Resources!, Computing Curricula 2004 and the Software Engineering Volume SE2004, Software Reuse Research, ICSE 2005 Forward)},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1082983.1085124},
doi = {10.1145/1082983.1085124},
journal = {SIGSOFT Softw. Eng. Notes},
month = jul,
pages = {0},
numpages = {63}
}

@proceedings{10.5555/3623295,
title = {ICSE-SEET '23: Proceedings of the 45th International Conference on Software Engineering: Software Engineering Education and Training},
year = {2023},
isbn = {9798350322590},
publisher = {IEEE Press},
location = {Melbourne, Australia}
}

@proceedings{10.1145/2997364,
title = {SLE 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Software Language Engineering},
year = {2016},
isbn = {9781450344470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

