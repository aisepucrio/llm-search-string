@article{10.1145/3695987,
author = {Oliveria de Souza, Leandro and Santana de Almeida, Eduardo and Silveira Neto, Paulo Anselmo da Mota and Barr, Earl T. and Petke, Justyna},
title = {Software Product Line Engineering via Software Transplantation},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3695987},
doi = {10.1145/3695987},
abstract = {Software Product Lines (SPLs) improve time-to-market, enhance software quality, and reduce maintenance costs. Current SPL reengineering practices are largely manual and require domain knowledge. Thus, adopting and, to a lesser extent, maintaining SPLs are expensive tasks, preventing many companies from enjoying their benefits. To address these challenges, we introduce Foundry, an approach utilising software transplantation to reduce the manual effort of SPL adoption and maintenance. Foundry enables integrating features across different codebases, even codebases that are unaware that they are contributing features to a software product line. Each product produced by Foundry is pure code, without variability annotation, unlike feature flags, which eases variability management and reduces code bloat.We realise Foundry in prodScalpel, a tool that transplants multiple organs (i.e., a set of interesting features) from donor systems into an emergent product line for codebases written in C. Given tests and lightweight annotations identifying features and implantation points, prodScalpel automates feature extraction and integration. To evaluate its effectiveness, our evaluation compares feature transplantation using prodScalpel to the current state of practice: on our dataset, prodScalpel’s use speeds up feature migration by an average of 4.8 times when compared to current practice.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {31},
numpages = {27},
keywords = {Software Product Lines, Software Transplantation, Genetic Improvement}
}

@inproceedings{10.1145/3646548.3672587,
author = {Becker, Martin and Rabiser, Rick and Botterweck, Goetz},
title = {Not Quite There Yet: Remaining Challenges in Systems and Software Product Line Engineering as Perceived by Industry Practitioners},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672587},
doi = {10.1145/3646548.3672587},
abstract = {Research on system and software product line engineering (SPLE) and the community around it have been inspired by industrial applications. However, despite decades of research, industry is still struggling with adopting product line approaches and more generally with managing system variability. We argue that it is essential to better understand why this is the case. Particularly, we need to understand the current challenges industry is facing wrt. adopting SPLE practices, how far existing research helps industry practitioners to cope with their challenges, and where additional research would be required. We conducted a hybrid workshop at the 2023 Systems and Software Product Line Conference (SPLC) with over 30 participants from industry and academia. 9 companies from diverse domains and in different phases of SPLE adoption presented their context and perceived challenges. We grouped, discussed, and rated the relevance of the articulated challenges. We then formed clusters of relevant research topics to discuss existing literature as well as research opportunities. In this paper, we report the industry cases, the identified challenges and clusters of research topics, provide pointers to existing work, and discuss research opportunities. With this, we want to enable industry practitioners to become aware of typical challenges and find their way into the existing body of knowledge and to relevant fields of research.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {179–190},
numpages = {12},
keywords = {Software product line engineering, industry challenges},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3652620.3687798,
author = {Sousa, Tiago and Ries, Beno\^{\i}t and Guelfi, Nicolas},
title = {Model-Driven Software Product Line Engineering of AI-Based Applications for Achieving Sustainable Development Goals: Vision Paper},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687798},
doi = {10.1145/3652620.3687798},
abstract = {Achieving the Sustainable Development Goals (SDGs) set by the United Nations requires innovative solutions to address the related complex and interconnected challenges. The application of AI has demonstrated the potential to significantly contribute to these efforts by providing advanced analytics and decision-making capabilities. However, integrating AI into sustainability initiatives faces several challenges, including the need for flexible and reusable solutions that can be adapted to diverse and evolving SDG contexts, as well as the challenge of making these technologies accessible to nonexpert stakeholders. This paper proposes an integrated approach that combines Model-Driven Engineering (MDE) with Software Product Line Engineering (SPLE) to address these challenges. The proposed process includes key activities such as domain analysis, metamodel-driven requirements specification, product derivation, and AI model training. This approach aims to automate the derivation of flexible and reusable AI architectures tailored to specific SDG contexts, thus reducing the development time of AI-based software solutions for sustainability efforts.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {523–527},
numpages = {5},
keywords = {model-driven engineering, software product line, artificial intelligence, sustainable development goals, vision paper},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3336294.3336304,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Software Product Line Engineering: A Practical Experience},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336304},
doi = {10.1145/3336294.3336304},
abstract = {The lack of mature tool support is one of the main reasons that make the industry to be reluctant to adopt Software Product Line (SPL) approaches. A number of systematic literature reviews exist that identify the main characteristics offered by existing tools and the SPL phases in which they can be applied. However, these reviews do not really help to understand if those tools are offering what is really needed to apply SPLs to complex projects. These studies are mainly based on information extracted from the tool documentation or published papers. In this paper, we follow a different approach, in which we firstly identify those characteristics that are currently essential for the development of an SPL, and secondly analyze whether the tools provide or not support for those characteristics. We focus on those tools that satisfy certain selection criteria (e.g., they can be downloaded and are ready to be used). The paper presents a state of practice with the availability and usability of the existing tools for SPL, and defines different roadmaps that allow carrying out a complete SPL process with the existing tool support.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {164–176},
numpages = {13},
keywords = {spl in practice, state of practice, tool support, tooling roadmap},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3483899.3483909,
author = {Furtado, Viviane and OliveiraJr, Edson and Kalinowski, Marcos},
title = {Guidelines for Promoting Software Product Line Experiments},
year = {2021},
isbn = {9781450384193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483899.3483909},
doi = {10.1145/3483899.3483909},
abstract = {The importance of experimentation for Software Engineering research has been notably established in the last years. The software engineering community has discussed how to proper report and evaluate experiments using different approaches, such as quality criteria, scales, and checklists. Nevertheless, there are no guidelines to support researchers and practitioners active in straightforward software engineering research areas, as in Software Product Lines (SPL), at conducting experiments. We hypothesize that experimentation guidelines may aid such a specific area by providing advice and actual excerpts reflecting good practices of SPL experimentation, thus experimentally evolving this area. Therefore, the goal of this paper is to provide guidelines for properly reporting and promoting SPL experiments. We defined such guidelines based on well-known software engineering experiment reports, quality evaluation checklists, and data extracted from 211 SPL experiments identified in a systematic mapping study. We evaluated the guidelines with a qualitative study with SPL and experimentation experts applying open and axial coding procedures. The evaluation enabled us to improve the guidelines. The resulting guidelines contain specific advice to researchers active in SPL and provide examples taken from published SPL experiments. The experts’ positive points indicate that the proposed guidelines can aid SPL researchers and practitioners. Sharing the resulting guidelines could support conducting SPL experiments and allow further area evolution based on prospective experiment replications and reproductions from well-designed and reported experiments.},
booktitle = {Proceedings of the 15th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {31–40},
numpages = {10},
keywords = {Experiment Reporting and Sharing, Guidelines, Qualitative Study, SPL Experiments},
location = {Joinville, Brazil},
series = {SBCARS '21}
}

@article{10.1145/3442389,
author = {Castro, Thiago and Teixeira, Leopoldo and Alves, Vander and Apel, Sven and Cordy, Maxime and Gheyi, Rohit},
title = {A Formal Framework of Software Product Line Analyses},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3442389},
doi = {10.1145/3442389},
abstract = {A number of product-line analysis approaches lift analyses such as type checking, model checking, and theorem proving from the level of single programs to the level of product lines. These approaches share concepts and mechanisms that suggest an unexplored potential for reuse of key analysis steps and properties, implementation, and verification efforts. Despite the availability of taxonomies synthesizing such approaches, there still remains the underlying problem of not being able to describe product-line analyses and their properties precisely and uniformly. We propose a formal framework that models product-line analyses in a compositional manner, providing an overall understanding of the space of family-based, feature-based, and product-based analysis strategies. It defines precisely how the different types of product-line analyses compose and inter-relate. To ensure soundness, we formalize the framework, providing mechanized specification and proofs of key concepts and properties of the individual analyses. The formalization provides unambiguous definitions of domain terminology and assumptions as well as solid evidence of key properties based on rigorous formal proofs. To qualitatively assess the generality of the framework, we discuss to what extent it describes five representative product-line analyses targeting the following properties: safety, performance, dataflow facts, security, and functional program properties.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {34},
numpages = {37},
keywords = {Software product lines, product-line analysis}
}

@inproceedings{10.1145/3461001.3473060,
author = {Sch\"{a}fer, Andreas and Becker, Martin and Andres, Markus and Kistenfeger, Tim and Rohlf, Florian},
title = {Variability realization in model-based system engineering using software product line techniques: an industrial perspective},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3473060},
doi = {10.1145/3461001.3473060},
abstract = {Efficiently handling system variants is rising of importance in industry and challenges the application of model-based systems engineering.This paper reveals the increasing industrial demand of guidance and decision support on how to handle variants and variability within SysML and UML models. While a substantial amount of variability realization approaches has already been published on source code level, there is little guidance for practitioners on system model level. Hence, there is major uncertainty in dealing with system changes or concurrent system modeling of related system. Due to a poor modularization and variability realization these model variants are ending up in interwoven and complex system models.In this paper, we aim to raise awareness of the need for appropriate guidance and decision support, identify important contextual factors of MBSE that influence variability realization, and derive well known variability mechanisms used in software coding for their applicability in system modeling.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {25–34},
numpages = {10},
keywords = {SysML, UML, decision support, model-based systems engineering, system and software product line engineering, variability mechanism, variability realization, variant management},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3425269.3425271,
author = {Nicolodi, Luciane Baldo and Colanzi, Thelma Elita and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Architectural Feature Re-Modularization for Software Product Line Evolution},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425271},
doi = {10.1145/3425269.3425271},
abstract = {Extensive maintenance leads to the Software Product Line Architecture (PLA) degradation over time. When there is the need of evolving the Software Product Line (SPL) to include new features, or move to a new platform, a degraded PLA requires considerable effort to understand and modify, demanding expensive refactoring activity. In the state of the art, search-based algorithms are used to improve PLA at package level. However, recent studies have shown that the most variability and implementation details of an SPL are described in the level of classes. There is a gap between existing approaches and existing practical needs. In this work, we extend the current state of the art to deal with feature modularization in the level of classes by introducing a new search operator and a set of objective functions to deal with feature modularization in a finer granularity of the architectural elements, namely at class level. We evaluated the proposal in an exploratory study with a PLA widely investigated and a real-world PLA. The results of quantitative and qualitative analysis point out that our proposal provides solutions to properly re-modularize features in a PLA, being preferred by practitioners, in order to support the evolution of SPLs.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {31–40},
numpages = {10},
keywords = {Architectural Degradation, Feature Modularization, Search-based Software Engineering, Software Evolution},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1145/3233027.3233038,
author = {Martinez, Jabier and T\"{e}rnava, Xhevahire and Ziadi, Tewfik},
title = {Software product line extraction from variability-rich systems: the robocode case study},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233038},
doi = {10.1145/3233027.3233038},
abstract = {The engineering of a Software Product Line (SPL), either by creating it from scratch or through the re-engineering of existing variants, it uses to be a project that spans several years with a high investment. It is often hard to analyse and quantify this investment, especially in the context of extractive SPL adoption when the related software variants are independently created by different developers following different system architectures and implementation conventions. This paper reports an experience on the creation of an SPL by re-engineering system variants implemented around an educational game called Robocode. The objective of this game is to program a bot (a battle tank) that battles against the bots of other developers. The world-wide Robocode community creates and maintains a large base of knowledge and implementations that are mainly organized in terms of features, although not presented as an SPL. Therefore, a group of master students analysed this variability-rich domain and extracted a Robocode SPL. We present the results of such extraction augmented with an analysis and a quantification regarding the spent time and effort. We believe that the results and the a-posteriori analysis can provide insights on global challenges on SPL adoption. We also provide all the elements to SPL educators to reproduce the teaching activity, and we make available this SPL to be used for any research purpose.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {132–142},
numpages = {11},
keywords = {education, extractive software product line adoption, reverse-engineering, robocode, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3109729.3109744,
author = {Munoz, Daniel-Jesus},
title = {Achieving energy efficiency using a Software Product Line Approach},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109744},
doi = {10.1145/3109729.3109744},
abstract = {Green computing and energy-aware software engineering are trend approaches that try to address the development of applications respectful with the environment. To reduce the energy consumption of an application the developer needs: (i) to identify what are the concerns that will impact more in the energy consumption; (ii) to model the variability of alternative designs and implementations of each concern; (iii) to store and compare the experimentation results related with the energy and time consumption of concerns; (iv) to find out what is the most eco-efficient solution for each concern. HADAS addresses these issues by modelling the variability of energy consuming concerns for different energy contexts. It connects the variability model with a repository that stores energy measurements, providing a Software Product Line (SPL) service, helping developers to reason and find out what are the most eco-friendly configurations. We have an initial implementation of the HADAS toolkit using Clafer. We have tested our implementation with several case studies.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {131–138},
numpages = {8},
keywords = {Clafer, Energy Efficiency, Metrics, Optimisation, Repository, Software Product Line, Variability},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3233027.3233032,
author = {Kr\"{o}her, Christian and Gerling, Lea and Schmid, Klaus},
title = {Identifying the intensity of variability changes in software product line evolution},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233032},
doi = {10.1145/3233027.3233032},
abstract = {The evolution of a Software Product Line (SPL) typically affects a variety of artifact types. The intensity (the frequency and the amount) in which developers change variability information in these different types of artifacts is currently unknown. In this paper, we present a fine-grained approach for the variability-centric extraction and analysis of changes to code, build, and variability model artifacts introduced by commits. This approach complements existing work that is typically based on a feature-perspective and, thus, abstracts from this level of detail. Further, it provides a detailed understanding of the intensity of changes affecting variability information in these types of artifacts. We apply our approach to the Linux kernel revealing that changes to variability information occur infrequently and only affect small parts of the analyzed artifacts. Further, we outline how these results may improve certain analysis and verification tasks during SPL evolution.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {54–64},
numpages = {11},
keywords = {evolution analysis, intensity, software product line evolution, variability changes},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3289402.3289504,
author = {Sebbaq, Hanane and Retbi, Asmaa and Idrissi, Mohammed Khalidi and Bennani, Samir},
title = {Software Product Line to overcome the variability issue in E-Learning: Systematic literature review},
year = {2018},
isbn = {9781450364621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289402.3289504},
doi = {10.1145/3289402.3289504},
abstract = {The disparity of educational technologies, pedagogies and learning styles implies a problem of variability when modeling E-learning systems. Furthermore, the current learning context, which has become very open and heterogeneous, raises the problem of automating the modeling, development and maintenance of personalized E-learning systems based on various pedagogies. For its part, the "Software Product Line" is a paradigm that aims to produce product families based on the principles of reuse, configuration and derivation. The main purpose of this literature review is to explore the different potential applications of "SPL" in the E-learning domain to figure out the problem of variability. We will adopt a protocol for a systematic review of literature, after which we will draw up an analysis report.},
booktitle = {Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications},
articleno = {4},
numpages = {8},
keywords = {E-learning, Software Product line, Variability, heterogeneity, scale, systematic literature review, variety},
location = {Rabat, Morocco},
series = {SITA'18}
}

@inproceedings{10.1145/2934466.2934481,
author = {Sion, Laurens and Van Landuyt, Dimitri and Joosen, Wouter and de Jong, Gjalt},
title = {Systematic quality trade-off support in the software product-line configuration process},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934481},
doi = {10.1145/2934466.2934481},
abstract = {Software product line engineering is a compelling methodology that accomplishes systematic reuse in families of systems by relying on two key principles: (i) the decomposition of complex systems into composable and reusable building blocks (often logical units called features), and (ii) on-demand construction of products and product variants by composing these building blocks.However, unless the stakeholder responsible for product configuration has detailed knowledge of the technical ins and outs of the software product line (e.g., the architectural impact of a specific feature, or potential feature interactions), he is in many cases flying in the dark. Although many initial approaches and techniques have been proposed that take into account quality considerations and involve trade-off decisions during product configuration, no systematic support exists.In this paper, we present a reference architecture for product configuration tooling, providing support for (i) up-front generation of variants, and (ii) quality analysis of these variants. This allows pro-actively assessing and predicting architectural quality properties for each product variant and in turn, product configuration tools can take into account architectural considerations. In addition, we provide an in-depth discussion of techniques and tactics for dealing with the problem of variant explosion, and as such to maintain practical feasibility of such approaches.We validated and implemented our reference architecture in the context of a real-world industrial application, a product-line for the firmware of an automotive sensor. Our prototype, based on FeatureIDE, is open for extension and readily available.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {164–173},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3106195.3106224,
author = {Tizzei, Leonardo P. and Nery, Marcelo and Segura, Vin\'{\i}cius C. V. B. and Cerqueira, Renato F. G.},
title = {Using Microservices and Software Product Line Engineering to Support Reuse of Evolving Multi-tenant SaaS},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106224},
doi = {10.1145/3106195.3106224},
abstract = {In order to achieve economies of scale, a Software as a Service (SaaS) should be configurable, multi-tenant efficient, and scalable. But building SaaS with these characteristics comes at a price of having more complex services. Some works in the literature integrate software product line engineering and service-oriented architecture to tackle the complexity of building multi-tenant SaaS. Most of these works focused on centralized approaches that rely on middleware or platforms, but they do not investigate the use of decentralized architectural style. Microservices architecture is an architectural style that relies on small, decentralized, and autonomous services that work together. Thus, this paper investigates the integrated use of microservices architecture and software produt line techniques to develop multi-tenant SaaS. We conducted an empirical study that analyzes the behavior of software reuse during the evolution of a multi-tenant SaaS. This empirical study showed an average software reuse of 62% of lines of code among tenants. We also provide lessons we learned during the the re-engineering and maintenance of such multi-tenant SaaS.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {205–214},
numpages = {10},
keywords = {Microservices, Multi-tenancy, Service-oriented Architectures, Software Evolution, Software Reuse},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2648511.2648537,
author = {Colanzi, Thelma Elita and Vergilio, Silvia Regina and Gimenes, Itana M. S. and Oizumi, Willian Nalepa},
title = {A search-based approach for software product line design},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648537},
doi = {10.1145/2648511.2648537},
abstract = {The Product Line Architecture (PLA) can be improved by taking into account key factors such as feature modularization, and by continuously evaluating its design according to metrics. Search-Based Software Engineering (SBSE) principles can be used to support an informed-design of PLAs. However, existing search-based design works address only traditional software design not considering intrinsic Software Product Line aspects. This paper presents MOA4PLA, a search-based approach to support the PLA design. It gives a multi-objective treatment to the design problem based on specific PLA metrics. A metamodel to represent the PLA and a novel search operator to improve feature modularization are proposed. Results point out that the application of MOA4PLA leads to PLA designs with well modularized features, contributing to improve features reusability and extensibility. It raises a set of solutions with different design trade-offs that can be used to improve the PLA design.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {237–241},
numpages = {5},
keywords = {multi-objective algorithms, searchbased PLA design, software product lines},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2362536.2362545,
author = {Lee, Jihyun and Kang, Sungwon and Lee, Danhyung},
title = {A survey on software product line testing},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362545},
doi = {10.1145/2362536.2362545},
abstract = {Software product line (SPL) testing consists of two separate but closely related test engineering activities: domain testing and application testing. Various software product line testing approaches have been developed over the last decade, and surveys have been conducted on them. However, thus far none of them deeply addressed the questions of what researches have been conducted in order to overcome the challenges posed by the two separate testing activities and their relationships. Thus, this paper surveys the current software product line testing approaches by defining a reference SPL testing processes and identifying, based on them, key research perspectives that are important in SPL testing. Through this survey, we identify the researches that addressed the challenges and also derive open research opportunities from each perspective.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {31–40},
numpages = {10},
keywords = {software product line engineering, software product line testing, software testing},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3266237.3266275,
author = {Filho, Helson Luiz Jakubovski and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Multiple objective test set selection for software product line testing: evaluating different preference-based algorithms},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266275},
doi = {10.1145/3266237.3266275},
abstract = {The selection of optimal test sets for Software Product Lines (SPLs) is a complex task impacted by many factors and that needs to consider the tester's preferences. To help in this task, Preference-based Evolutionary Multi-objective Algorithms (PEMOAs) have been explored. They use a Reference Point (RP), which represents the user preference and guides the search, resulting in a greater number of solutions in the ROI (Region of Interest). This region contains solutions that are more interesting from the tester's point of view. However, the explored PEMOAs have not been compared yet and the results reported in the literature do not consider many-objective formulations. Such an evaluation is important because in the presence of more than three objectives the performance of the algorithms may change and the number of solutions increases. Considering this fact, this work presents evaluation results of four PEMOAs for selection of products in the SPL testing considering cost, testing criteria coverage, products similarity, and the number of revealed faults, given by the mutation score. The PEMOAs present better performance than traditional algorithms, avoiding uninteresting solutions. We introduce a hyper-heuristic version of the PEMOA R-NSGA-II that presents the best results in a general case.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {162–171},
numpages = {10},
keywords = {preference-based multi-objective algorithms, search-based software engineering, software product line testing},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@inproceedings{10.1145/3056662.3056663,
author = {Kang, Sungwon and Kim, Jungmin and Baek, Haeun and Ahn, Hwi and Jung, Pilsu and Lee, Jihyun},
title = {Comparison of software product line test derivation methods from the reuse viewpoint},
year = {2017},
isbn = {9781450348577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3056662.3056663},
doi = {10.1145/3056662.3056663},
abstract = {Product line test development is more complicated than test development for a single application, as the former has to deal with variability among assets (artifacts) and is carried out in two separate but related development phases, i.e. domain engineering and application engineering. Different software product test development methods provide different opportunities for reuse depending on how variability is represented in the domain test artifacts, when binding is formed and applied and also when test data are determined. This paper compares and analyzes the six major methods for the software product line test development in the literature. Through the comparison, we find out that existing software product line testing methods did not fully consider the aspects of software product line that are essential for reuse in software product line development such as variability representation, binding formation and application time and test data determination time. As the conclusion of this literature review, this paper suggests future research opportunities for software product line testing to explore.},
booktitle = {Proceedings of the 6th International Conference on Software and Computer Applications},
pages = {1–8},
numpages = {8},
keywords = {software product line development, software testing, systematic product line testing},
location = {Bangkok, Thailand},
series = {ICSCA '17}
}

@inproceedings{10.1145/2993236.2993251,
author = {Steindorfer, Michael J. and Vinju, Jurgen J.},
title = {Towards a software product line of trie-based collections},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993251},
doi = {10.1145/2993236.2993251},
abstract = {Collection data structures in standard libraries of programming languages are designed to excel for the average case by carefully balancing memory footprint and runtime performance. These implicit design decisions and hard-coded trade-offs do constrain users from using an optimal variant for a given problem. Although a wide range of specialized collections is available for the Java Virtual Machine (JVM), they introduce yet another dependency and complicate user adoption by requiring specific Application Program Interfaces (APIs) incompatible with the standard library.  A product line for collection data structures would relieve library designers from optimizing for the general case. Furthermore, a product line allows evolving the potentially large code base of a collection family efficiently. The challenge is to find a small core framework for collection data structures which covers all variations without exhaustively listing them, while supporting good performance at the same time.  We claim that the concept of Array Mapped Tries (AMTs) embodies a high degree of commonality in the sub-domain of immutable collection data structures. AMTs are flexible enough to cover most of the variability, while minimizing code bloat in the generator and the generated code. We implemented a Data Structure Code Generator (DSCG) that emits immutable collections based on an AMT skeleton foundation. The generated data structures outperform competitive hand-optimized implementations, and the generator still allows for customization towards specific workloads.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {168–172},
numpages = {5},
keywords = {Code generation, Hash trie, Immutability, Performance, Persistent data structure, Software product line},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, M\'{a}rcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Configuration, Feature Model, Non-Functional Properties, Recommender Systems, Software Product Lines},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/2648511.2648513,
author = {Harman, M. and Jia, Y. and Krinke, J. and Langdon, W. B. and Petke, J. and Zhang, Y.},
title = {Search based software engineering for software product line engineering: a survey and directions for future work},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648513},
doi = {10.1145/2648511.2648513},
abstract = {This paper presents a survey of work on Search Based Software Engineering (SBSE) for Software Product Lines (SPLs). We have attempted to be comprehensive, in the sense that we have sought to include all papers that apply computational search techniques to problems in software product line engineering. Having surveyed the recent explosion in SBSE for SPL research activity, we highlight some directions for future work. We focus on suggestions for the development of recent advances in genetic improvement, showing how these might be exploited by SPL researchers and practitioners: Genetic improvement may grow new products with new functional and non-functional features and graft these into SPLs. It may also merge and parameterise multiple branches to cope with SPL branchmania.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {5–18},
numpages = {14},
keywords = {program synthesis, genetic programming, SPL, SBSE},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3167132.3167350,
author = {Fischer, Stefan and Lopez-Herrejon, Roberto Erick and Egyed, Alexander},
title = {Towards a fault-detection benchmark for evaluating software product line testing approaches},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167350},
doi = {10.1145/3167132.3167350},
abstract = {Software Product Lines (SPLs) are families of related software systems distinguished by the set of features each one provides. The commonly large number of variants that can be derived from an SPL poses a unique set of challenges, because it is not feasible to test all the individual variants. Over the last few years many approaches for SPL testing have been devised. They usually select a set of variants to test based on some covering criterion. A problem when evaluating these testing approaches is properly comparing them to one another. Even though some benchmarks have been proposed, they focus on covering criteria and do not consider fault data in their analysis. Considering the dire lack of publicly available fault data, in this paper we present the first results of our ongoing project to introduce simulated faults into SPLs along with using evolutionary techniques for synthesizing unit test cases for SPL examples.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {2034–2041},
numpages = {8},
keywords = {software product lines, mutation testing},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/2695664.2695797,
author = {Tizzei, Leonardo P. and Azevedo, Leonardo G. and de Bayser, Maximilien and Cerqueira, Renato F. G.},
title = {Architecting cloud tools using software product line techniques: an exploratory study},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695797},
doi = {10.1145/2695664.2695797},
abstract = {Multitenant cloud computing tools are usually complex and have to manage variabilities to support customization. Software Product Line (SPL) techniques have been successfully applied in the industry to manage variability in complex systems. However, few works in the literature discuss the application of SPL techniques to architect industry cloud computing tools, resulting in a lack of support to cloud architects on how to apply such techniques. This work presents how software product line techniques can be applied for architecting cloud tools, and discusses the benefits, drawbacks, and some challenges of applying such techniques to develop a real industry cloud tool, named as Installation Service.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1441–1448},
numpages = {8},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/2648511.2648515,
author = {Wang, Shuai and Buchmann, David and Ali, Shaukat and Gotlieb, Arnaud and Pradhan, Dipesh and Liaaen, Marius},
title = {Multi-objective test prioritization in software product line testing: an industrial case study},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648515},
doi = {10.1145/2648511.2648515},
abstract = {Test prioritization is crucial for testing products in a product line considering limited budget in terms of available time and resources. In general, it is not practically feasible to execute all the possible test cases and so, ordering test case execution permits test engineers to discover faults earlier in the testing process. An efficient prioritization of test cases for one or more products requires a clear consideration of the tradeoff among various costs (e.g., time, required resources) and effectiveness (e.g., feature coverage) objectives. As an integral part of the future Cisco's test scheduling system for validating video conferencing products, we introduce a search-based multi-objective test prioritization technique, considering multiple cost and effectiveness measures. In particular, our multi-objective optimization setup includes the minimization of execution cost (e.g., time), and the maximization of number of prioritized test cases, feature pairwise coverage and fault detection capability. Based on cost-effectiveness measures, a novel fitness function is defined for such test prioritization problem. The fitness function is empirically evaluated together with three commonly used search algorithms (e.g., (1+1) Evolutionary algorithm (EA)) and Random Search as a comparison baseline based on the Cisco's industrial case study and 500 artificial designed problems. The results show that (1+1) EA achieves the best performance for solving the test prioritization problem and it scales up to solve the problems of varying complexity.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {32–41},
numpages = {10},
keywords = {test prioritization, software product lines, search algorithms, multi-objective optimization},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2593882.2593888,
author = {Metzger, Andreas and Pohl, Klaus},
title = {Software product line engineering and variability management: achievements and challenges},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593888},
doi = {10.1145/2593882.2593888},
abstract = {Software product line engineering has proven to empower organizations to develop a diversity of similar software-intensive systems (applications) at lower cost, in shorter time, and with higher quality when compared with the development of single systems. Over the last decade the software product line engineering research community has grown significantly. It has produced impressive research results both in terms of quality as well as quantity. We identified over 600 relevant research and experience papers published within the last seven years in established conferences and journals. We briefly summarize the major research achievements of these past seven years. We structure this research summary along a standardized software product line framework. Further, we outline current and future research challenges anticipated from major trends in software engineering and technology.},
booktitle = {Future of Software Engineering Proceedings},
pages = {70–84},
numpages = {15},
keywords = {variability modeling, variability management, requirements engineering, quality assurance, design, Software product lines},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@inproceedings{10.5555/2814058.2814112,
author = {Lobato, Luanna Lopes and Bittar, Thiago Jabur},
title = {A Risk Management Approach for Software Product Line Engineering},
year = {2015},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {TSoftware Product Line (SPL) Engineering is a software development paradigm that fosters systematic reuse. It is focused on improving software practices, leading companies to experience benefits, such as reduced time-to-market and effort, and higher quality for the products delivered to customers. However, establishing a SPL is neither a simple nor a cheap task, and may affect several aspects of a software company. Besides, it involves a range of risks that may hinder project success. These have to be managed accordingly, so as to minimize the likelihood of project failure. Despite the importance of Risk Management (RM) for SPL Engineering, little has been published in terms of suitable and structured practices to cope with that. This present paper reports an approach for RM in SPL Engineering, named RiPLERM (Rise Product Line Engineering and Risk Management). The approach presents activities to structure RM in SPL projects, The design of the RiPLE-RM approach elaborated on results from empirical investigations, and was proposed to facilitate the management and provide significant insights that can be used to avoid and solve risks.},
booktitle = {Proceedings of the Annual Conference on Brazilian Symposium on Information Systems: Information Systems: A Computer Socio-Technical Perspective - Volume 1},
pages = {331–338},
numpages = {8},
keywords = {Software Product Line Engineering, Software Process, Risk Management, Project management},
location = {Goiania, Goias, Brazil},
series = {SBSI '15}
}

@inproceedings{10.1145/2739482.2764650,
author = {Karimpour, Reza and Ruhe, Guenther},
title = {A Search Based Approach Towards Robust Optimization in Software Product Line Scoping},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2764650},
doi = {10.1145/2739482.2764650},
abstract = {Software product line (SPL) scoping is important for planning upfront investment. One challenge with scoping comes from inaccuracies in estimated parameters and uncertainty in environment. In this paper, a method to incorporate uncertainty in SPL scoping optimization and its application to generate robust solutions is proposed. We model scoping optimization as a multi-objective problem with profit and stability as heuristics. To evaluate our proposal, a number of experiments are conducted. Analysis of results show that both performance stability and feasibility stability were improved providing the product line manager enhanced decision-making support.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1415–1416},
numpages = {2},
keywords = {uncertainty, software product line portfolio scoping, robust optimization, multi-objective},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@inproceedings{10.1145/2996890.3007893,
author = {Ruiz, Carlos and Duran-Limon, Hector A. and Parlavantzas, Nikos},
title = {Towards a software product line-based approach to adapt IaaS cloud configurations},
year = {2016},
isbn = {9781450346160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2996890.3007893},
doi = {10.1145/2996890.3007893},
abstract = {Cloud computing is nowadays one of the most promising IT technologies, since it provides seemingly unlimited resources on demand at low costs. Hence, different types of applications have been migrated to IaaS environments, e.g. multi-tier (distributed) applications. However, in order to benefit from such characteristics, cloud configurations (i.e. virtual resource configurations) should be designed accordingly to the necessities of the applications. Furthermore, such configurations have to provide the required resources not only at the application deployment-time, but also during the whole application execution time. Hence, adaptive paradigms are required when designing solutions to cloud applications with dynamic resource requirements. Software Product Lines (SPLs) provide great flexibility and a high level of abstraction to describe complete system configurations. Even though SPLs are not commonly used to describe changes after an initial product (configuration) has been created, their inherent characteristics can enable producing the required virtual resource configuration to adapt applications after their initial deployment, i.e., at runtime. In this paper, we present an approach to create and adapt cloud configurations at the IaaS level by using SPLs. We focus on the architectural design of our solution as well as on the possible implementation challenges we could face.},
booktitle = {Proceedings of the 9th International Conference on Utility and Cloud Computing},
pages = {398–403},
numpages = {6},
keywords = {software product lines, self-adaptation, cloud computing},
location = {Shanghai, China},
series = {UCC '16}
}

@inproceedings{10.5555/2662572.2662582,
author = {Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {Representation of software product line architectures for search-based design},
year = {2013},
isbn = {9781467362849},
publisher = {IEEE Press},
abstract = {The Product-Line Architecture (PLA) is the main artifact of a Software Product Line (SPL). Search-based approaches can provide automated discovery of near-optimal PLAs and make its design less dependent on human architects. To do this, it is necessary to adopt a suitable PLA representation to apply the search operators. In this sense, we review existing architecture representations proposed by related work, but all of them need to be extended to encompass specific characteristics of SPL. Then, the use of such representations for PLA is discussed and, based on the performed analysis, we introduce a novel direct PLA representation for search-based optimization. Some implementation aspects are discussed involving implementation details about the proposed PLA representation, constraints and impact on specific search operators. Ongoing work addresses the application of specific search operators for the proposed representation and the definition of a fitness function to be applied in a multi-objective search-based approach for the PLA design.},
booktitle = {Proceedings of the 1st International Workshop on Combining Modelling and Search-Based Software Engineering},
pages = {28–33},
numpages = {6},
keywords = {software product line, multi-objective search-based approach, architecture modelling},
location = {San Francisco, California},
series = {CMSBSE '13}
}

@inproceedings{10.1145/1985484.1985489,
author = {Michalik, Bartosz and Weyns, Danny and Van Betsbrugge, Wim},
title = {On the problems with evolving Egemin's software product line},
year = {2011},
isbn = {9781450305846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985484.1985489},
doi = {10.1145/1985484.1985489},
abstract = {Egemin, an industrial manufacturer of logistic systems is adopting a Software Product Line (SPL) approach to manage the development of their product portfolio. However, due to the intrinsic complexity of the logistic systems and lack of explicitly documented architectural knowledge evolution of the products is error-prone. Faulty updates increase maintenance costs and harm the company's reputation. Therefore, Egemin searches for a systematic solution that can improve their SPL evolution strategy.},
booktitle = {Proceedings of the 2nd International Workshop on Product Line Approaches in Software Engineering},
pages = {15–19},
numpages = {5},
keywords = {spl, software product line, evolution},
location = {Waikiki, Honolulu, HI, USA},
series = {PLEASE '11}
}

@inproceedings{10.1145/1982185.1982336,
author = {Asadi, Mohsen and Bagheri, Ebrahim and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Mohabbati, Bardia},
title = {Goal-driven software product line engineering},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982336},
doi = {10.1145/1982185.1982336},
abstract = {Feature Models encapsulate functionalities and quality properties of a product family. The employment of feature models for managing variability and commonality of large-scale product families raises an important question: on what basis should the features of a product family be selected for a target software application, which is going to be derived from the product family. Thus, the selection of the most suitable features for a specific application requires the understanding of its stakeholders' intentions and also the relationship between their intentions and the available software features. To address this important issue, we adopt a standard goal-oriented requirements engineering framework, i.e., the i* framework, for identifying stakeholders' intentions and propose an approach for explicitly mapping and bridging between the features of a product family and the goals and objectives of the stakeholders. We propose a novel approach to automatically preconfigure a given feature model based on the objectives of the target product stakeholders. Also, our approach is able to elucidate the rationale behind the selection of the most important features of a family for a target application.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {691–698},
numpages = {8},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@inproceedings{10.1145/2420942.2420948,
author = {Gonz\'{a}lez-Huerta, Javier and Insfran, Emilio and Abrah\~{a}o, Silvia and McGregor, John D.},
title = {Non-functional requirements in model-driven software product line engineering},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420948},
doi = {10.1145/2420942.2420948},
abstract = {Developing variant-rich software systems through the application of the software product line approach requires the management of a wide set of requirements. However, in most cases, the focus of those requirements is limited to the functional requirements. The non-functional requirements are often informally defined and their management does not provide traceability mechanisms for their validation. In this paper, we present a multimodel approach that allows the explicit representation of non-functional requirements for software product lines both at domain engineering, and application engineering levels. The multimodel allows the representation of different viewpoints of a software product line, including the non-functional requirements and the relationships that these non-functional requirements might have with features and functionalities. The feasibility of this approach is illustrated through a specific example from the automotive domain.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {6},
numpages = {6},
keywords = {software product lines, non-functional requirements, model driven engineering},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@inproceedings{10.5555/2337223.2337302,
author = {Cordy, Maxime and Classen, Andreas and Perrouin, Gilles and Schobbens, Pierre-Yves and Heymans, Patrick and Legay, Axel},
title = {Simulation-based abstractions for software product-line model checking},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Software Product Line (SPL) engineering is a software engineering paradigm that exploits the commonality between similar software products to reduce life cycle costs and time-to-market. Many SPLs are critical and would benefit from efficient verification through model checking. Model checking SPLs is more difficult than for single systems, since the number of different products is potentially huge. In previous work, we introduced Featured Transition Systems (FTS), a formal, compact representation of SPL behaviour, and provided efficient algorithms to verify FTS. Yet, we still face the state explosion problem, like any model checking-based verification. Model abstraction is the most relevant answer to state explosion. In this paper, we define a novel simulation relation for FTS and provide an algorithm to compute it. We extend well-known simulation preservation properties to FTS and thus lay the theoretical foundations for abstraction-based model checking of SPLs. We evaluate our approach by comparing the cost of FTS-based simulation and abstraction with respect to product-by-product methods. Our results show that FTS are a solid foundation for simulation-based model checking of SPL.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {672–682},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.5555/1753235.1753272,
author = {Dordowsky, Frank and Hipp, Walter},
title = {Adopting software product line principles to manage software variants in a complex avionics system},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Eurocopter is the majority partner in NH Industries, the international consortium that develops and produces the medium weight multi-role helicopter NH90. AgustaWestland and Stork Fokker are additional partners. The NH90 has been successfully sold to 14 nations and their armed forces. The software division at Eurocopter Germany develops the on-board software for three computers of the NH90 avionics CORE and MISSION Systems. The growing number of customers and their specific application domains for the NH90 has led to an increasing number of functionally different helicopter variants. Moreover, during the long development time that is typical for complex military avionics projects, the computing technology has changed considerably over time so that the current operational software has to fit to several processor architectures. In order to cope with the high number of software variants and technology variations, the NH90 software team developed concepts and strategies for SW architecture and tool modifications based on Software Product Line (SPL) principles.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {265–274},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/1842752.1842809,
author = {Helleboogh, Alexander and Avgeriou, Paris and Bouck\'{e}, Nelis and Heymans, Patrick},
title = {Workshop on Variability in Software Product Line Architectures (VARI-ARCH 2010)},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842809},
doi = {10.1145/1842752.1842809},
abstract = {The objective of this workshop is to bring together researchers from the software product line community and software architecture community to identify critical challenges and progress the state-of-the-art on variability in software product line architectures.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {309–311},
numpages = {3},
keywords = {viewpoint, view, variability, software architecture, product lines, product line architecture, model, concern, assets},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@inproceedings{10.1145/2480362.2480694,
author = {Diwan, Piyush and Carey, Patricia and Franz, Eric and Li, Yixue and Bitterman, Thomas and Hudak, David E. and Ramnath, Rajiv},
title = {Applying software product line engineering in building web portals for supercomputing services},
year = {2013},
isbn = {9781450316569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2480362.2480694},
doi = {10.1145/2480362.2480694},
abstract = {Supercomputing centers, typically non-profit, government or university-based organizations with scarce resources, are increasingly being requested to provide customized web portals for user-centered access to their services in order to support a demanding customer base. These portals often have very similar architectures and meet similar requirements, with the variations primarily being in the specialized analysis applications, and in the input and output of these applications. Given these characteristics, Software Production Line Engineering (SPLE) approaches will be valuable in enabling development teams to cost-effectively meet demands. In this paper, we demonstrate a suite of web portals developed at The Ohio Supercomputer Center (OSC) by applying SPLE methodologies. We show how we applied feature modeling on these applications to identify commonalities in their application level features despite differences in their problem domains. We describe a common framework (we term it Per User DrupaL, or PUDL), which serves as the common foundation for these portals. We demonstrate the effectiveness of SPLE in terms of reduced development time and effort, and discuss the technical challenges faced in this process. Finally we propose, as an extension to our work, an automation framework for portal generation, which users could build their own customized portals.},
booktitle = {Proceedings of the 28th Annual ACM Symposium on Applied Computing},
pages = {1765–1771},
numpages = {7},
keywords = {supercomputing, software-as-a-service, software product line engineering, portals, high performance computing, feature modeling, end-user computing, drupal},
location = {Coimbra, Portugal},
series = {SAC '13}
}

@inproceedings{10.1145/3646548.3676541,
author = {Jadoon, Gullelala},
title = {Preserving Non-Functional Requirements in Goal Models Using Meta-models of the Software Product Lines},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676541},
doi = {10.1145/3646548.3676541},
abstract = {Non-functional requirements (NFRs) play a critical role in software product line (SPL) engineering, ensuring products meet essential criteria beyond mere functionality. However, preserving NFRs across product variants induces considerable challenges, particularly in goal-oriented SPLE where goals guide product derivation. This research proposes a novel framework to preserve NFRs in goal models using meta-models of SPLs and manage inconsistent NFRs. The framework utilizes product and domain meta-models to accurately capture and represent NFRs, addressing construct validity concerns. This research aims to enhance the credibility and generalizability of findings in SPL engineering, contributing to the advancement of goal-oriented modeling and NFR preservation practices.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {1–5},
numpages = {5},
keywords = {Goal models, Meta-models, Non-functional requirements, Property preservation, Software Product Line Engineering},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.5555/1753235.1753277,
author = {Botterweck, Goetz and Groher, Iris and Polzer, Andreas and Schwanninger, Christa and Thiel, Steffen and V\"{o}lter, Markus},
title = {1st International Workshop on Model-driven Approaches in Software Product Line Engineering: (MAPLE 2009)},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {The MAPLE workshop focuses on the combination of Model-driven Software Engineering and Software Product Lines (SPL). It explores how model-driven approaches can help to achieve the goals of product lines in terms of reducing cost and time to market and increasing quality and productivity. In particular the workshop revolves around three themes: Efficient product derivation, the link between SPL research and industry practice, and SPL models with a meaning.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {297–298},
numpages = {2},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/3646548.3672582,
author = {Corti\~{n}as, Alejandro and Lamas, Victor and R. Luaces, Miguel},
title = {SensorPublisher: Applying Software Product Lines to the development of IoT dashboards},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672582},
doi = {10.1145/3646548.3672582},
abstract = {Geosciences have witnessed a revolution in data collection thanks to the Internet of Things (IoT), which has made it possible to monitor complex phenomena using sensor networks. However, developing web-centric, sensor-based, data warehousing information systems presents challenges because of their complexity and cost. This paper presents an intuitive low-code development system (called SensorPublisher), based on a software product line (SPL) and a domain-specific language (DSL), that speeds up the creation of data warehousing applications for geographic sensor data. SensorPublisher allows the geoscientist to define the sensor network, to generate a software product, and to deploy the product to a local or a remote server. Our tool seeks to encourage scientists to share the outcomes of their sensor data analysis projects with their communities by means of a simple, user-friendly and cost-effective approach. We showcase the system in different geoscientific domains, such as meteorological monitoring services, traffic data and air quality monitoring in urban areas, and marine area monitoring systems.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {153–163},
numpages = {11},
keywords = {Domain Specific Language (DSL), Internet of Things (IoT), Software Product Line (SPL)},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3579027.3608975,
author = {Burgue\~{n}o, Lola and Horcas, Jose-Miguel and Kienzle, J\"{o}rg},
title = {Development and Evolution of Software Product Lines Driven by Stakeholder Beliefs},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608975},
doi = {10.1145/3579027.3608975},
abstract = {The planning, realization, and release of a Software Product Line (SPL) are driven by features. Therefore, many high-level decisions about the evolution of an SPL are made at the feature level. However, a feature can involve many stakeholders with different expertise, and taking their opinions into account to make the right decisions is not trivial. In this paper, we propose using belief uncertainty in conjunction with feature models to assist in the evolution of SPLs by explicitly quantifying opinions. We outline three evolution scenarios in which subjective logic can be used to represent the opinions of stakeholders and explain in detail how to use subjective logic to make decisions in the context of the next release problem. We illustrate our ideas with a Smartwatch SPL. Finally, we discuss different ways of combining the opinions of stakeholders depending on the situation, the goals and the risks that can be assumed.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {34–40},
numpages = {7},
keywords = {uncertainty, subjective logic, software product line, feature model, Decision making support},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/1244002.1244266,
author = {Inoki, Mari and Fukazawa, Yoshiaki},
title = {Software product line evolution method based on kaizen approach},
year = {2007},
isbn = {1595934804},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1244002.1244266},
doi = {10.1145/1244002.1244266},
abstract = {Continuing optimal product line development needs to evolve core assets in response to market, technology or organization changes. In this paper, we propose a product line evolution method based on the kaizen approach. Kaizen is a continuous improvement method that is adopted in Japanese industry. The important points of the kaizen are to prepare a work standard and continue to improve processes by correcting the differences between the standard and actual results. Our core asset kaizen method provides a standard that includes core asset types based on simple metrics, kaizen patterns representing expertise, and kaizen processes for continuous improvement.},
booktitle = {Proceedings of the 2007 ACM Symposium on Applied Computing},
pages = {1207–1214},
numpages = {8},
keywords = {software product line, pattern, evolution, core asset, kaizen},
location = {Seoul, Korea},
series = {SAC '07}
}

@inproceedings{10.1145/3503229.3547026,
author = {Friesel, Birte and Elmenhorst, Kathrin and Kaiser, Lennart and M\"{u}ller, Michael and Spinczyk, Olaf},
title = {kconfig-webconf: retrofitting performance models onto kconfig-based software product lines},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547026},
doi = {10.1145/3503229.3547026},
abstract = {Despite decades of research and clear advantages, performance-aware configuration of real-world software product lines is still an exception rather than the norm. One reason for this may be tooling: configuration software with support for non-functional property models is generally not compatible with the configuration and build process of existing product lines. Specifically, the Kconfig language is popular in open source software projects, but neither language nor configuration frontends support performance models. To address this, we present kconfig-webconf: a performance-aware, Kconfig-compatible software product line configuration frontend. It is part of a toolchain that can automatically generate performance models with a minimal amount of changes to a software product line's build process. With such a performance model, kconfig-webconf can serve as a performance-aware drop-in replacement for existing Kconfig frontends. We evaluate its usage in five examples, including the busybox multi-call binary and the resKIL agricultural AI product line.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {58–61},
numpages = {4},
keywords = {kconfig, performance prediction, product lines, regression trees},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3233027.3233045,
author = {Becker, Martin and Zhang, Bo},
title = {How do our neighbours do product line engineering? a comparison of hardware and software product line engineering approaches from an industrial perspective},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233045},
doi = {10.1145/3233027.3233045},
abstract = {Product line engineering (PLE) approaches have been followed in industry for hardware and software solutions for more than three decades now. However, the different engineering disciplines (e.g. mechanics, electrics, software) have developed and evolved their approaches within their own realms, which is fine as long as there is no need for integrated approaches. Driven by the increasing complexity of systems, there is a rising need for interdisciplinary systems engineering these days. Companies engineering cyber-physical systems and their components have to integrate product line engineering approaches across the involved engineering disciplines to enable a global optimization of portfolio, solution structures, and assets along their lifecycle. From a bird's-eye view, there is noticeable commonality but also variety in the approaches followed for PLE in the different engineering disciplines, which renders the integration of approaches a non-trivial endeavour. In order to foster the development of integrated PLE approaches, this paper explores, maps, and compares PLE approaches in the field of hardware and software engineering. Furthermore, the paper identifies integration opportunities and challenges. As the paper targets industrial practitioners, it mainly provides references to respective industrial events and material and does not fully cover related work in the respective research communities.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {190–195},
numpages = {6},
keywords = {software product lines, industry, academia, SPLC},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3646548.3676548,
author = {Romero-Organvidez, David and Neira, Pablo and Galindo, Jos\'{e} A. and Benavides, David},
title = {Kconfig metamodel: a first approach},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676548},
doi = {10.1145/3646548.3676548},
abstract = {Kconfig is the de facto configuration language for describing and configuring the variability of the Linux kernel. Nonetheless, it has been used since the early stages of kernel development. Moreover, Kconfig is also used as a niche configuration languages, such as microkernel compilation for air navigation systems, proprietary routers or embedded systems. In the last decade, the software product line (SPL) community worked intensively on observing Linux Kernel and Kconfig. However, the official documentation is difficult to understand, and the examples are long and challenging to synthesize for non-Kconfig experts, such as SPL engineers, and researchers. In this paper, we propose a Kconfig metamodel based on the documentation and the feedback of a kernel developer expert. Thanks to this metamodel, the design of transformations from Kconfig to other variability models such as UVL (Universal Variability Language) can be facilitated. To our knowledge, this is the first proposal for a metamodel of the Kconfig language. This opens the door to further research, such as Kconfig analysis and transformations, and leverage interoperability among the Kconfig toolchain and SPL tools.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {55–60},
numpages = {6},
keywords = {Kconfig, linux kernel, metamodel, variability},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3546932.3547001,
author = {Six, Nicolas and Herbaut, Nicolas and Lopez-Herrejon, Roberto Erick and Salinesi, Camille},
title = {Using software product lines to create blockchain products: application to supply chain traceability},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547001},
doi = {10.1145/3546932.3547001},
abstract = {In recent years, blockchain has been growing rapidly from a niche technology to a promising solution for many sectors, due to its unique properties that empower the design of innovative applications. Nevertheless, the development of blockchain applications is still a challenge. Due to the technological novelty, only a few developers are familiar with blockchain technologies and smart contracts. Others might face a steep learning curve or difficulties to reuse existing code to build blockchain applications. This study proposes a novel approach to tackle these issues, through software product line engineering. To support the approach, a web platform to configure and generate a blockchain application for on-chain traceability is introduced. First, a feature model has been designed to model core features of the chosen domain, based on the existing literature. Then, a configurator has been implemented to support the feature selection phase. Finally, a generator is able to ingest such configurations to generate on-the-shelf blockchain products. The generalizability of the contribution is validated by reproducing on-chain traceability applications proposed in the literature by using the platform. This work provides the first evidence that the implementation of blockchain applications using software product lines enhances the quality of produced applications and reduces the time to market.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {97–107},
numpages = {11},
keywords = {software product line, code generation, blockchain},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/1217935.1217955,
author = {Krishna, Arvind S. and Gokhale, Aniruddha S. and Schmidt, Douglas C.},
title = {Context-specific middleware specialization techniques for optimizing software product-line architectures},
year = {2006},
isbn = {1595933220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1217935.1217955},
doi = {10.1145/1217935.1217955},
abstract = {Product-line architectures (PLAs) are an emerging paradigm for developing software families for distributed real-time and embedded (DRE) systems by customizing reusable artifacts, rather than hand-crafting software from scratch. To reduce the effort of developing software PLAs and product variants for DRE systems, developers are applying general-purpose -- ideally standard -- middleware platforms whose reusable services and mechanisms support a range of application quality of service (QoS) requirements, such as low latency and jitter. The generality and flexibility of standard middleware, however, often results in excessive time/space overhead for DRE systems, due to lack of optimizations tailored to meet the specific QoS requirements of different product variants in a PLA.This paper provides the following contributions to the study of middleware specialization techniques for PLA-based DRE systems. First, we identify key dimensions of generality in standard middleware stemming from framework implementations, deployment platforms, and middleware standards. Second, we illustrate how context-specific specialization techniques can be automated and used to tailor standard middleware to better meet the QoS needs of different PLA product variants. Third, we quantify the benefits of applying automated tools to specialize a standard Realtime CORBA middleware implementation. When applied together, these middleware specializations improved our application product variant throughput by ~65%, average- and worst-case end-to-end latency measures by ~43% and ~45%, respectively, and predictability by a factor of two over an already optimized middleware implementation, with little or no effect on portability, standard middleware APIs, or application software implementations, and interoperability.},
booktitle = {Proceedings of the 1st ACM SIGOPS/EuroSys European Conference on Computer Systems 2006},
pages = {205–218},
numpages = {14},
keywords = {specializations, product lines, middleware},
location = {Leuven, Belgium},
series = {EuroSys '06}
}

@inproceedings{10.1145/3646548.3672584,
author = {Kogler, Philipp and Chen, Wei and Falkner, Andreas and Haselb\"{o}ck, Alois and Wallner, Stefan},
title = {Modelling Engineering Processes in Natural Language: A Case Study},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672584},
doi = {10.1145/3646548.3672584},
abstract = {Engineering process management aims to formally specify processes which are executable, measurable, and controllable. Common representations include text-based domain-specific languages (DSLs) or graphical notations such as the Business Process Modelling Notation (BPMN). The specification itself can be seen as a Software Product Line (SPL), building upon concepts such as tasks, UI forms, fields and actions. Domain experts provide requirements for processes but often lack the technical programming skills to formalize them in a process specification language. We present an interactive SPL application prototype that allows domain experts to model simple processes in natural language. Our framework for the reliable generation of formal specifications with Large Language Models (LLMs) supports the machine-translation from natural language to a JSON-based process DSL. In this case study, five domain experts were asked to model any process of their choice through natural-language interactions. As a result, the user interface corresponding to the process DSL was shown as immediate feedback. We documented their perceived translation quality and interviewed them on their impressions of this methodology. An average user-assessed performance rating of 68% was achieved. Even though the modelling strategies differed greatly between individuals, the tool was able to adequately capture the majority of instructions, leaving an overall positive impression on the participants. More context awareness and additional conventional interaction elements were the main aspects found to be improved for a productive implementation.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {170–178},
numpages = {9},
keywords = {Domain-specific Languages, Generative Artificial Intelligence, Large Language Models, Process Management, Process Modelling, Reliable Code Generation},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@article{10.1145/1279711.1279715,
author = {Krishna, Arvind S. and Gokhale, Aniruddha and Schmidt, Douglas C. and Ranganath, Venkatesh Prasad and Hatcliff, John},
title = {Towards highly optimized real-time middleware for software product-line architectures},
year = {2006},
issue_date = {January 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/1279711.1279715},
doi = {10.1145/1279711.1279715},
abstract = {This paper provides the following contributions to the study of middleware optimization techniques for product line architectures in real-time systems. First, we identify different dimensions of generality in standards based middleware implementations. Second, we describe how specialization approaches used in other domains including OS, compiler and programming languages can be applied to address middleware generality challenges. Third, we present preliminary results from the application of our specialization techniques. Our results illustrate that specialization techniques represent a promising approach for minimizing time/space overheads in middleware.},
journal = {SIGBED Rev.},
month = jan,
pages = {13–16},
numpages = {4}
}

@inproceedings{10.1145/1094855.1094946,
author = {Liu, Shih-Hsi},
title = {A software product line architecture for distributed real-time and embedded systems: a separation of concerns approach},
year = {2005},
isbn = {1595931937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1094855.1094946},
doi = {10.1145/1094855.1094946},
abstract = {This paper presents a separation of concerns approach to solve the tangling problem of functional and Quality of Service (QoS) concerns in traditional Component-based Software Engineering (CBSE) and Software Product Line (SPL) technologies applied to Distributed Real-time and Embedded (DRE) systems. This problem originates from the interchangeability for fulfilling functional and QoS concerns during composition. The approach utilizes the perspective of QoS to design and analyze a set of software systems represented by a collection of QoS systemic paths, which determine how well functional tasks perform in terms of flows of application-specific and functionality-determined information between components. Our approach not only reserves the virtues of reusability, changeability, productivity and expeditiousness that traditional CBSE and SPL technologies possess, but also dedicates the contributions in terms of separation of concerns, design space exploration, fine-grained commonality and reusability evaluation and less subjective feasibility analyses for a component-based SPL.},
booktitle = {Companion to the 20th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {224–225},
numpages = {2},
keywords = {two-level grammar++, software product line architecture, real-time, quality of service, UniFrame},
location = {San Diego, CA, USA},
series = {OOPSLA '05}
}

@inproceedings{10.1145/3546932.3547008,
author = {Amraoui, Yassine El and Blay-Fornarino, Mireille and Collet, Philippe and Precioso, Fr\'{e}d\'{e}ric and Muller, Julien},
title = {Evolvable SPL management with partial knowledge: an application to anomaly detection in time series},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547008},
doi = {10.1145/3546932.3547008},
abstract = {In Machine Learning (ML), the resolution of anomaly detection problems in time series presents a great diversity of practices as it can correspond to many different contexts. These practices cover both grasping the business problem and designing the solution itself. By practice, we designate explicit and implicit steps toward resolving a problem, while a solution corresponds to a combination of algorithms selected for their performance on a given problem. Two related issues arise. The first one is that the practices are individual and not explicitly mutualized. The second one is that choosing one solution over another is all the more difficult to justify because the space of solutions and the evaluation criteria are vast and evolve rapidly with the advances in ML. To solve these issues and tame the evolving diversity in ML, a Software Product Line (SPL) approach can be envisaged to represent the variable set of solutions. However, this requires characterizing an ML business problem through an explicit set of criteria and justifying one ML solution over all others. The resolution of anomaly detection problems is thus different from finding the best configuration workflow from past configurations but lies more in guiding the configuration towards a solution that may never have been studied before. This paper proposes an SPL approach that capitalizes on past practices by exploiting a variability-aware representation to detect new criteria and constraints when practices adopt different solutions to seemingly similar problems. We report on the evaluation of our approach using a set of applications from the literature and an ML software company. We show how the analysis of practices makes it possible to consolidate the knowledge contained in the SPL.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {222–233},
numpages = {12},
keywords = {software product line, metrics, machine learning, evolution},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3646548.3676546,
author = {G\"{u}thing, Lukas and Pett, Tobias and Schaefer, Ina},
title = {Out-of-the-Box Prediction of Non-Functional Variant Properties Using Automated Machine Learning},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676546},
doi = {10.1145/3646548.3676546},
abstract = {A configurable system is characterized by the configuration options present or absent in its variants. Selecting and deselecting those configuration options directly influences the functional properties of the system. Apart from functional properties, there are system characteristics that influence the performance (e.g., power demand), safety (e.g., fault probabilities), and security (e.g., susceptibility to attacks) of the system, called Non-Functional Properties (NFPs). Knowledge of NFPs is crucial for evaluating a system’s feasibility, usability, and resource demands. Although variability influences these characteristics, NFPs do not compose linearly for every selected feature. Feature interactions can increase the overall NFP values through (potentially exponential) amplification or decrease them through mitigation effects. In this paper, we propose an automated machine learning (AutoML) approach to predict NFP values for new configurations based on previously measured configuration values. Using AutoML, we leverage the advantages of machine learning for predicting NFPs without having to parameterize and fine-tune machine learning models. This approach and the resulting pipeline aim to reduce the complexity of performance prediction for configurable systems. We test the feasibility of our pipeline in a first evaluation on 4 real-world subject systems and discuss cases where AutoML may improve the prediction of NFPs.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {82–87},
numpages = {6},
keywords = {AutoML, Cyber-physical systems, Machine learning, Software product lines},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3646548.3672585,
author = {Landsberg, Tobias and Dietrich, Christian and Lohmann, Daniel},
title = {Should I Bother? Fast Patch Filtering for Statically-Configured Software Variants},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672585},
doi = {10.1145/3646548.3672585},
abstract = {In the face of critical security vulnerabilities, patch and update management are a crucial and challenging part of the software life cycle. In software product families, patching becomes even more challenging as we have to support different variants, which are not equally affected by critical patches. While the naive “better-patched-than-sorry” approach will apply all necessary updates, it provokes avoidable costs for developers and customers. In this paper we introduce SiB (Should I Bother?), a heuristic patch-filtering method for statically-configurable software that efficiently identifies irrelevant patches for specific variants. To solve the variability-aware patch-filtering problem, SiB compares modified line ranges from patches with those source-code ranges included in variants currently deployed. We apply our prototype for CPP-managed variability to four open-source projects (Linux, OpenSSL, SQLite, Bochs), demonstrating that SiB is both effective and efficient in reducing the number of to-be-considered patches for unaffected software variants. It correctly classifies up to 68 percent of variants as unaffected, with a recall of 100 percent, thus reducing deployments significantly, without missing any relevant patches.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {12–23},
numpages = {12},
keywords = {Patch Filtering, Software Evolution, Software Product Lines},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3503229.3547057,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Gurov, Dilian and Fuentes, Lidia},
title = {Defining categorical reasoning of numerical feature models with feature-wise and variant-wise quality attributes},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547057},
doi = {10.1145/3503229.3547057},
abstract = {Automatic analysis of variability is an important stage of Software Product Line (SPL) engineering. Incorporating quality information into this stage poses a significant challenge. However, quality-aware automated analysis tools are rare, mainly because in existing solutions variability and quality information are not unified under the same model.In this paper, we make use of the Quality Variability Model (QVM), based on Category Theory (CT), to redefine reasoning operations. We start defining and composing the six most common operations in SPL, but now as quality-based queries, which tend to be unavailable in other approaches. Consequently, QVM supports interactions between variant-wise and feature-wise quality attributes. As a proof of concept, we present, implement and execute the operations as lambda reasoning for CQL IDE - the state-of-the-art CT tool.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {132–139},
numpages = {8},
keywords = {automated reasoning, category theory, extended feature model, numerical features, quality attribute},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3461002.3473947,
author = {Pinnecke, Marcus},
title = {Product-lining the elinvar wealthtech microservice platform},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473947},
doi = {10.1145/3461002.3473947},
abstract = {Software product lining is the act of providing different but related software products under the same brand, known as a software product line (SPL). As engineering, management and validation of SPLs is far from trivial, special solutions for software product line engineering (SPLE) have a continuous momentum in both academic and industry. In general, it is hard to judge when to reasonably favor SPLE over alternative solutions that are more common in the industry. In this paper, we illustrate how we as Elinvar manage variability within our WealthTech Platform as a Service (PaaS) at different granularity levels, and discuss methods for SPLE in this context. More in detail, we share our techniques and concepts to address configuration management, and show how we manage a single microservice SPL including inter-service communication. Finally, we provide insights into platform solutions by means of packages for our clients. We end with a discussion on SPLE techniques in context of service SPLs and our packaging strategy. We conclude that while we are good to go with industry-standard approaches for microservice SPLs, the variability modeling and analysis advantages within SPLE is promising for our packaging strategy.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {60–68},
numpages = {9},
keywords = {variability management, technologies and concepts, product families, microservice platforms, configuration management},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/2815782.2815799,
author = {Schaefer, Ina and Seidl, Christoph and Cleophas, Loek and Watson, Bruce W.},
title = {SPLicing TABASCO: Custom-Tailored Software Product Line Variants from Taxonomy-Based Toolkits},
year = {2015},
isbn = {9781450336833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815782.2815799},
doi = {10.1145/2815782.2815799},
abstract = {Taxonomy-Based Software Construction (TABASCO) applies extensive domain analyses to create conceptual hierarchies of algorithmic domains. Those are used as basis for the implementation of software toolkits. The monolithic structure of TABASCO-based toolkits restricts their adoption on resource-constrained or special-purpose devices. In this paper, we address this problem by applying Software Product Line (SPL) techniques to TABASCO-based toolkits: We use software taxonomies as input to creating a conceptual representation of variability as feature models of an SPL. We apply the variability realization mechanism delta modeling to transform realization artifacts, such as source code, to only contain elements for a particular selection of features. Our method is suitable for proactive, reactive and extractive SPL development so that it supports a seamless adoption and evolution of an SPL approach for TABASCO-based toolkits. We demonstrate the feasibility of the method with three case studies by proactively, reactively and extractively transforming TABASCO-based toolkits to SPLs, which allow derivation of variants with custom-tailored functionality.},
booktitle = {Proceedings of the 2015 Annual Research Conference on South African Institute of Computer Scientists and Information Technologists},
articleno = {34},
numpages = {10},
keywords = {Taxonomy-Based Software Construction (TABASCO) toolkit, Software Product Line (SPL) adoption},
location = {Stellenbosch, South Africa},
series = {SAICSIT '15}
}

@inproceedings{10.1145/3461001.3473061,
author = {Tomashchuk, Oleksandr and Van Landuyt, Dimitri and Joosen, Wouter},
title = {The architectural divergence problem in security and privacy of eHealth IoT product lines},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3473061},
doi = {10.1145/3461001.3473061},
abstract = {The Internet of Things (IoT) seamlessly becomes integrated into many aspects of daily life, and in the case of healthcare, it arises in the shape of eHealth IoT systems. Evidently, the design of such systems must apply best practices when it comes to security and privacy, in addition to ensuring compliance with various national and international regulations. When it comes to the required functionality, commonalities and variations can effectively be managed in a product line approach that involves deriving specific application architecture variants from a common reference architecture.This paper illustrates and discusses a specific problem encountered in the establishment of a software product-line in this specific context: the adoption of systematic security and privacy threat modeling and risk assessment approaches introduces a variation space that is very difficult to capture in a proactive product-line approach. One of the main causes for this is that threat assessment itself suffers from the problem of threat explosion, i.e. combinatorial explosions of threats that have to be investigated and systematically mitigated. The highlighted divergence of the security and privacy threats across architectural variants is illustrated in the specific case of an industry IoT-based e-health software product line.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {114–119},
numpages = {6},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3579028.3609018,
author = {Nienaber, S\"{o}ren and Soorati, Mohammad D. and Ghasemzadeh, Arash and Ghofrani, Javad},
title = {Software Product Lines for Development of Evolutionary Robots},
year = {2023},
isbn = {9798400700927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579028.3609018},
doi = {10.1145/3579028.3609018},
abstract = {Evolutionary Robotics utilizes evolutionary algorithms for training robot controllers (e.g., neural networks) and adapting robot morphologies for different environments in design and runtime. One of the main challenges in robotics is the lack of reusability as AI-based robot controllers have to be trained from scratch for any change in the environment or a new task specification that a robot should adapt to. Training Artificial Neural Networks can be computationally heavy, time-consuming, and hard to reuse due to their monolithic black-box nature. The building blocks of emerging behaviors from Artificial Neural Networks cannot be fully separated or reused. We address the issue of reusability and propose an incremental approach for applying the reusability of behaviors. We implemented an Evolutionary Robotics framework to form a product family of robots. This product family is used to show the feasibility of our method for handling variability in a domain. Our results can be used to demonstrate a sample binding between the software product lines and machine learning domains.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
pages = {77–84},
numpages = {8},
keywords = {Software Product Lines, Primitive Behaviors, Mobile Robots, Evolutionary Robotics, Configuration},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3646548.3672588,
author = {Burgstaller, Tamim and Garber, Damian and Le, Viet-Man and Felfernig, Alexander},
title = {Optimization Space Learning: A Lightweight, Noniterative Technique for Compiler Autotuning},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672588},
doi = {10.1145/3646548.3672588},
abstract = {Compilers are highly configurable systems. One can influence the performance of a compiled program by activating and deactivating selected compiler optimizations. However, automatically finding well-performing configurations is a challenging task. We consider expensive iteration, paired with recompilation of the program to optimize, as one of the main shortcomings of state-of-the-art approaches. Therefore, we propose Optimization Space Learning, a lightweight and noniterative technique. It exploits concepts known from configuration space learning and recommender systems to discover well-performing compiler configurations. This reduces the overhead induced by the approach significantly, compared to existing approaches. The process of finding a well-performing configuration is 800k times faster than with the state-of-the-art techniques.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {36–46},
numpages = {11},
keywords = {Collaborative Filtering, Compiler, Compiler Autotuning, Configuration, Configuration Space Learning, Performance Optimization},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3579027.3608978,
author = {Wang, Alan and Feng, Nick and Chechik, Marsha},
title = {Code-Level Functional Equivalence Checking of Annotative Software Product Lines},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608978},
doi = {10.1145/3579027.3608978},
abstract = {Software functional equivalence checking is a technique for analyzing the impact of change of a portion of code on the rest of the system. The existing functional equivalence checking approaches are applicable only at the individual software product level. In this paper, we propose a lifted functional equivalence checking approach, CLEVER-V, that can efficiently handle annotative software product lines. Instead of checking functional equivalence of every product separately, CLEVER-V analyzes all products together to iteratively identify groups of non-equivalent products with common causes. We report on the implementation of the lifted functional equivalence checking approach and demonstrate its effectiveness and scalability on a suite of 288 realistic software updates from BusyBox.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {64–75},
numpages = {12},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3646548.3672594,
author = {Bombarda, Andrea and Gargantini, Angelo},
title = {On the Use of Multi-valued Decision Diagrams to Count Valid Configurations of Feature Models},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672594},
doi = {10.1145/3646548.3672594},
abstract = {This paper addresses the challenge of efficiently counting valid configurations in Software Product Lines (SPLs). We propose a novel approach leveraging Multi-Valued Decision Diagrams (MDDs) for building the set of products. Building upon the MDD structure, we introduce several algorithmic optimizations to achieve a more compact and efficient representation of the product set compared to existing methods based on Binary Decision Diagrams. The effectiveness of our approach is evaluated through experimentation on two datasets: a set of synthetic benchmarks and large-scale industrial feature models. The results demonstrate significant improvements in scalability for models of medium complexity, particularly those rich in alternative groups. However, challenges remain for other model types, highlighting areas for future research.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {96–106},
numpages = {11},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3579027.3608994,
author = {Fadhlillah, Hafiyyan Sayyid and Fern\'{a}ndez, Antonio M. Guti\'{e}rrez and Rabiser, Rick and Zoitl, Alois},
title = {Managing Cyber-Physical Production Systems Variability using V4rdiac: Industrial Experiences},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608994},
doi = {10.1145/3579027.3608994},
abstract = {Cyber-Physical Production Systems (CPPSs) are highly robust and versatile production systems that utilize diverse hardware components through control software. Employing a systematic variability management approach for developing variants of control software can reduce cost and time-to-market to build such complex systems. However, employing this approach in the CPPS domain is challenging. Engineering CPPSs require multidisciplinary engineering knowledge (e.g., process, signal, mechanical). Knowledge about CPPS variability is thus typically scattered across diverse engineering artifacts. Also, variability knowledge is usually not documented explicitly but rather tacit knowledge of mostly senior engineers. Furthermore, control software is commonly implemented using a graphical Domain-Specific Modeling Language (DSML) which only provides minimal support to express variability. This paper describes our experiences dealing with these challenges in an industrial context using a multidisciplinary variability management approach called Variability for 4diac (V4rdiac). V4rdiac is an integrated approach that allows CPPS engineers to conduct stepwise product configuration based on heterogeneous variability models from multiple engineering disciplines. V4rdiac also provides a mechanism to automatically generate control software based on a set of selected configuration options. We evaluate how V4rdiac implements and manages CPPS control software variants in the metallurgical production plant domain. We describe the benefits and lessons learned from using V4rdiac in this domain based on feedback from industrial practitioners.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {223–233},
numpages = {11},
keywords = {Variability Modeling, Software Product Line, Software Configuration, Cyber-Physical Production System},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3646548.3672590,
author = {Sundermann, Chico and Brancaccio, Vincenzo Francesco and Kuiter, Elias and Krieter, Sebastian and He\ss{}, Tobias and Th\"{u}m, Thomas},
title = {Collecting Feature Models from the Literature: A Comprehensive Dataset for Benchmarking},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672590},
doi = {10.1145/3646548.3672590},
abstract = {Feature models are widely used for specifying the valid configurations of product lines. Many automated analyses on feature models have been considered, but they often depend on computationally complex algorithms (e.g., solving satisfiability problems). To identify and develop efficient reasoning engines, it is necessary to compare their performance on practically relevant feature models. However, empirical evaluations on feature-model analysis often suffer from the limitations of available feature-model datasets in terms of transferability. A major problem is the accessibility of relevant feature models as they are scattered over numerous publications. In this work, we perform a literature survey on empirical evaluations that target the performance of feature-model analyses to examine common evaluation practices and collect feature models for future evaluations. Furthermore, we examine the suitability of the derived collection for benchmarking performance. To improve accessibility, we provide a repository including all 2,518 identified feature models from 13 application domains, such as system software.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {54–65},
numpages = {12},
keywords = {benchmark, evaluation, feature model, product line, survey},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3579027.3608983,
author = {Murphy, Logan and Di Sandro, Alessio and Shahin, Ramy and Chechik, Marsha},
title = {Reusing Your Favourite Analysis Framework to Handle Workflows of Product Line Models},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608983},
doi = {10.1145/3579027.3608983},
abstract = {Model management frameworks support a wide array of analyses, transformations, and workflows, but lack native support for handling product lines of models. Yet the ubiquity of domains that heavily use model-driven techniques and are built using product lines, such as automotive, require adaptation, or lifting, of model management frameworks to be variability-aware. Lifting might introduce new implementation and validation costs, especially in safety-critical contexts. To facilitate the implementation and validation of variability-aware model management workflows, this paper provides a novel taxonomy of lifting methods. We compare the lifting methods in their capacity to reuse existing components and validation results. We then define a general framework for lifting and validating model management workflows, and report on an experience of lifting and validating modeling tasks and workflows in an existing Eclipse-based model management framework.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {117–128},
numpages = {12},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3646548.3676552,
author = {Malenfant, Jacques and Ziadi, Tewfik},
title = {Can Conditional Preferences and *CP-net Concepts Enhance Feature Models?},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676552},
doi = {10.1145/3646548.3676552},
abstract = {Variability in software systems is a key concept in mastering complexity. Most software systems exhibit functionalities that can either be implemented in several different ways or may be options that can be chosen late in their design, depending on the deployment conditions. Expressing these forms of variability attracted a lot of attention since several years, leading to a predominant expression media: Feature models. Feature models can express the mandatory/optional status of a feature, as well as alternative realizations of a feature, exclusive of each others or not. Though extensions of feature models have been proposed to express more properties, they are still limited in their ability to capture complex configuration problems, especially those involving extra functional properties. Lines of research have proposed extended feature models with non-functional attributes and constraint-solving approach to reason about them. Yet, these approaches appear too restrictive and inflexible to cater for really complex SPL. In this paper, we rather propose to extend feature models with conditional preferences concepts from the multi-criteria decision-making field, and more specifically on relationships introduced in the graphical modeling approach of the * CP-net family of models. We show how these new relationships enable the expression of complex configuration constraints, while ensuring that the feature models remain intuitive and user-friendly across various feature model analysis-based activities.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {66–74},
numpages = {9},
keywords = {conditional preferences, feature models, variability management},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3646548.3672586,
author = {Fernandez-Amoros, David and Heradio, Ruben and Horcas Aguilera, Jose Miguel and Galindo, Jos\'{e} A. and Benavides, David and Fuentes, Lidia},
title = {Pragmatic Random Sampling of the Linux Kernel: Enhancing the Randomness and Correctness of the conf Tool},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672586},
doi = {10.1145/3646548.3672586},
abstract = {The configuration space of some systems is so large that it cannot be computed. This is the case with the Linux Kernel, which provides almost 19,000 configurable options described across more than 1,600 files in the Kconfig language. As a result, many analyses of the Kernel rely on sampling its configuration space (e.g., debugging compilation errors, predicting configuration performance, finding the configuration that optimizes specific performance metrics, etc.). The Kernel can be sampled pragmatically, with its built-in tool conf, or idealistically, translating the Kconfig files into logic formulas. The pros of the idealistic approach are that it provides statistical guarantees for the sampled configurations, but the cons are that it sets out many challenging problems that have not been solved yet, such as scalability issues. This paper introduces a new version of conf called randconfig+, which incorporates a series of improvements that increase the randomness and correctness of pragmatic sampling and also help validate the Boolean translation required for the idealistic approach. randconfig+ has been tested on 20,000 configurations generated for 10 different Kernel versions from 2003 to the present day. The experimental results show that randconfig+ is compatible with all tested Kernel versions, guarantees the correctness of the generated configurations, and increases conf’s randomness for numeric and string options.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {24–35},
numpages = {12},
keywords = {Kconfig, SAT, configurable systems, randconfig, random sampling, software product lines, variability modeling},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3646548.3676538,
author = {Dubslaff, Clemens and Husung, Nils and K\"{a}fer, Nikolai},
title = {Configuring BDD Compilation Techniques for Feature Models},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676538},
doi = {10.1145/3646548.3676538},
abstract = {The compilation of feature models into binary decision diagrams (BDDs) is a major challenge in the area of configurable systems analysis. Many large-scale feature models have been reported to exceed state-of-the-art compilation capabilities, e.g., for variants of the Linux kernel product line. However, experiments have been mainly conducted on standard settings of the BDD compilers themselves, not taking advanced configurations into account. In this paper, we investigate the impact of various BDD compilation techniques for compiling feature models in conjunctive normal form. Specifically, we evaluate preprocessing techniques from satisfiability (SAT) solving, variable and clause ordering heuristics, non-incremental construction schemes, as well as parallelization of BDD construction. Our experiments on current feature models show that BDD compilation of feature models greatly benefits from these techniques, enabling to construct many previously not constructible large-scale feature models within seconds.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {209–216},
numpages = {8},
keywords = {Binary Decision Diagrams, Configurable Systems, Feature Models, Knowledge Compilation},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3646548.3672598,
author = {He\ss{}, Tobias and Semmler, Sean Niklas and Sundermann, Chico and Tor\'{a}n, Jacobo and Th\"{u}m, Thomas},
title = {Towards Deterministic Compilation of Binary Decision Diagrams From Feature Models},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672598},
doi = {10.1145/3646548.3672598},
abstract = {Compiling binary decision diagrams (BDD) from feature models is a challenging task whose outcome and performance depends on many interconnected factors. As almost all of these factors trace back to the BDD’s variable order, finding suitable orders is paramount for compilation success. In addition to cross-tree constraints, Alternative groups in feature models pose a challenge for variable ordering as separating group variables in the order can lead to duplication of large parts of the BDD. Previous approaches only scale to under-constrained models and require knowledge of the feature hierarchy. We address both shortcomings with a novel deterministic variable-ordering heuristic that detects Alternative groups in Boolean formulas and exploits them for variable ordering using the well-known FORCE heuristic in a divide-and-conquer approach. Our evaluation shows that this heuristic, together with our compilation strategy, scales to many models for which BDDs could not be compiled previously. Thereby, this work resolves SPLC’s knowledge compilation challenge for an important subset of real-world models.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {136–147},
numpages = {12},
keywords = {Binary Decision Diagrams, Feature-Model Analysis, Variable Ordering},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3646548.3676601,
author = {Marinho, Euler},
title = {Characterizing Resource Interaction Failures in Mobile Applications},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676601},
doi = {10.1145/3646548.3676601},
abstract = {Resource interaction failures can compromise the mobile application quality and harm the user experience. In this thesis proposal, we first investigated resource interaction failures using exhaustive testing. After increasing the number of resources, we used sampling strategies for the investigation of these failures. In addition, we examined the feasibility of Spectrum-based Fault Localization for locating faults in Android applications and the sensitivity to resource interaction failures. We plan to extend the previous study by considering manually injected faults based on bug fix patterns. Moreover, we plan to identify guidelines for developers, testers, and researchers to handle resource interaction failures and faults based on the lessons learned in this thesis proposal.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {11–16},
numpages = {6},
keywords = {Resource Interaction Failures, Software Quality, Software Testing},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3546932.3546997,
author = {Acher, Mathieu and Martin, Hugo and Lesoil, Luc and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc and Khelladi, Djamel Eddine and Barais, Olivier and Pereira, Juliana Alves},
title = {Feature subset selection for learning huge configuration spaces: the case of linux kernel size},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3546997},
doi = {10.1145/3546932.3546997},
abstract = {Linux kernels are used in a wide variety of appliances, many of them having strong requirements on the kernel size due to constraints such as limited memory or instant boot. With more than nine thousands of configuration options to choose from, developers and users of Linux actually spend significant effort to document, understand, and eventually tune (combinations of) options for meeting a kernel size. In this paper, we describe a large-scale endeavour automating this task and predicting a given Linux kernel binary size out of unmeasured configurations. We first experiment that state-of-the-art solutions specifically made for configurable systems such as performance-influence models cannot cope with that number of options, suggesting that software product line techniques may need to be adapted to such huge configuration spaces. We then show that tree-based feature selection can learn a model achieving low prediction errors over a reduced set of options. The resulting model, trained on 95 854 kernel configurations, is fast to compute, simple to interpret and even outperforms the accuracy of learning without feature selection.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {85–96},
numpages = {12},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3503229.3547055,
author = {Friesel, Birte and M\"{u}ller, Michael and Ferraz, Matheus and Spinczyk, Olaf},
title = {On the relation of variability modeling languages and non-functional properties},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547055},
doi = {10.1145/3503229.3547055},
abstract = {Non-functional properties (NFPs) such as code size (RAM, ROM), performance, and energy consumption are at least as important as functional properties in many software development domains. When configuring a software product line - especially in the area of resource-constrained embedded systems - developers must be aware of the NFPs of the configured product instance. Several NFP-aware variability modeling languages have been proposed to address this in the past. However, it is not clear whether a variability modeling language is the best place for handling NFP-related concerns, or whether separate NFP prediction models should be preferred. We shine light onto this question by discussing limitations of state-of-the-art NFP-aware variability modeling languages, and find that both in terms of the development process and model accuracy a separate NFP model is favorable. Our quantitative analysis is based on six different software product lines, including the widely used busybox multi-call binary and the x264 video encoder. We use classification and regression trees (CART) and our recently proposed Regression Model Trees [8] as separate NFP models. These tree-based models can cover the effects of arbitrary feature interactions and thus easily outperform variability models with static, feature-wise NFP annotations. For example, when estimating the throughput of an embedded AI product line, static annotations come with a mean generalization error of 114.5% while the error of CART is only 9.4 %.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {140–144},
numpages = {5},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3646548.3676550,
author = {He\ss{}, Tobias and Karrer, Simon and Ostheimer, Lukas},
title = {Multi-Version Decision Propagation for Configuring Feature Models in Space and Time},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676550},
doi = {10.1145/3646548.3676550},
abstract = {Real-world feature models are typically too large and complex to be configured manually. In practice, configuration tasks are, therefore, accomplished by employing interactive configurators. After each explicit feature selection or deselection by the user, these configurators use decision propagation to detect features that are implied by the current partial configuration and, consequently, select or deselect them accordingly. This way, the configuration remains valid throughout the configuration process. However, valid configurations may become invalid when then underlying model changes due to model evolution. As one is potentially interested in retaining a configuration for multiple versions, for instance, in testing or certification applications, this prompts the question on how to configure for multiple versions at once. In this work, we introduce multi-version decision propagation which allows to interactively configure on multiple model versions at once. Our prototype adapts the set of possible versions to the current configuration but also allows users to configure for a fixed set of versions.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {88–92},
numpages = {5},
keywords = {Configuration, Decision Propagation, Feature Models, Feature-Model Evolution},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3579027.3608981,
author = {Horcas, Jose-Miguel and Ballesteros, Joaquin and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Elimination of constraints for parallel analysis of feature models},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608981},
doi = {10.1145/3579027.3608981},
abstract = {Cross-tree constraints give feature models maximal expressive power since any interdependency between features can be captured through arbitrary propositional logic formulas. However, the existence of these constraints increases the complexity of reasoning about feature models, both for using SAT solvers or compiling the model to a binary decision diagram for efficient analyses. Although some works have tried to refactor constraints to eliminate them, they deal only with simple constraints (i.e., requires and excludes) or require the introduction of an additional set of features, increasing the complexity of the resulting feature model. This paper presents an approach that eliminates all the cross-tree constraints present in regular boolean feature models, including arbitrary constraints, in propositional logic formulas. Our approach for removing constraints consists of splitting the semantics of feature models into orthogonal disjoint feature subtrees, which are then analyzed in parallel to alleviate the exponential blow-up in memory of the resulting feature tree.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {99–110},
numpages = {12},
keywords = {software product line, parallelization, feature tree, feature model, constraint, Automated analysis},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3646548.3672595,
author = {Bounouas, Nassim and Blay-Fornarino, Mireille and Collet, Philippe},
title = {Tracing and Fixing Inconsistencies in Clone-and-Own Tabular Data Models},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672595},
doi = {10.1145/3646548.3672595},
abstract = {Many data-intensive applications handle tabular data with more advanced structuring and processes than spreadsheets, enabling end-users to copy and adapt tabular data and processes to create new templates or datasets anytime. Recent research advances demonstrated that, in such clone-and-own scenarios, actions performed on the data structure, together with cloning and adaptation actions, can be captured within an operation-based model to prevent the drift of the internal tabular data model. However, this approach is limited by the assumption that each operation must maintain consistency regarding dependencies generated by the domain-specific languages that connect the observed and computed data. To address this challenge, this paper first introduces an evolved operation-based model that is designed to capture inconsistent tabular data while keeping a fine-grained trace of what part of the model is inconsistent. We then define specific trace operations to either fix a dependency in a model or remove one if its creating process is no longer relevant to the user. These operations support high-level editing scenarios on the tabular data, which enables easily fixing the equivalent of a spreadsheet formula or a process statement, or making the user aware that some part of the model is inconsistent while it is cloned. Additionally, we report on a positive scalability experiment on the tracing of large tabular data models with inconsistencies.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {191–202},
numpages = {12},
keywords = {Tabular data, agronomy, clone-and-own, model-driven engineering, operation-based modeling, variability management},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3503229.3547025,
author = {Horcas, Jose M. and Galindo, Jose A. and Pinto, M\'{o}nica and Fuentes, Lidia and Benavides, David},
title = {FM fact label: a configurable and interactive visualization of feature model characterizations},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547025},
doi = {10.1145/3503229.3547025},
abstract = {Recognizing specific characteristics of feature models (FM) can be challenging due to the different nature and domains of the models. There are several metrics to characterize FMs. However, there is no standard way to visualize and identify the properties that make an FM unique and distinguishable. We propose FM Fact Label as a tool to visualize an FM characterization based on its metadata, structural measures, and analytical metrics. Although existing tools can provide a visualization of the FM and report some metrics, the feature diagram of large-scale FMs becomes ineffective to take an overall shape of the FM properties. Moreover, the reported metrics are often embedded in the tool user interface, preventing further analysis. FM Fact Label is a standalone web-based tool that provides a configurable and interactive visualization of FM characterizations that can be exported to several formats. Our contribution becomes important because the Universal Variability Language (UVL) is starting to gain attraction in the software product line community as a unified textual language to specify FMs and share knowledge. With this contribution, we help to advance the UVL ecosystem one step forward while providing a common representation for the results of existing analysis tools.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {42–45},
numpages = {4},
keywords = {characterization, feature model, metrics, variability, visualization},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3336294.3342376,
author = {Lanna, Andre and Castro, Thiago and Alves, Vander and Rodrigues, Genaina and Schobbens, Pierre-Yves and Apel, Sven},
title = {Feature-Family-Based Reliability Analysis of Software Product Lines},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342376},
doi = {10.1145/3336294.3342376},
abstract = {Context: Verification techniques such as model checking are being applied to ensure that software systems achieve desired quality levels and fulfill their functional and non-functional specification. However, applying these techniques to software product lines is a twofold challenge, given the exponential blowup of the number of products and the state-explosion problem inherent to model checking. Current product-line verification techniques leverage symbolic model checking and variability information to optimize the analysis but still face limitations that make them costly or infeasible. In particular, state-of-the-art verification techniques for product-line reliability analysis are enumerative which hinders their applicability, given the latent blowup of the configuration space.Objective: Our objectives are the following: (a) we present a method to efficiently compute the reliability of all configurations of a compositional or annotation-based software product line from its UML behavioral models, (b) we provide a tool that implements the proposed method, and (c) we report on an empirical study comparing the performance of different reliability analysis strategies for software product lines.Method: We present a novel feature-family-based analysis strategy to compute the reliability of all products of a (compositional or annotation-based) software product line. The strategy employs a divide-and-conquer approach over UML behavioral models endowed with probabilistic and variability information. The feature-based step of our strategy divides the behavioral models into smaller feature-dependent fragments that can be analyzed more efficiently. Such analysis consists of creating a probabilistic model for each behavioral fragment and analyzing such model using a parametric model checker that returns an expression denoting its reliability. Parameters in such expression represent the reliabilities of fragments on which it depends at runtime. The family-based step performs the reliability computation for all configurations at once (conquer) by evaluating reliability expressions in terms of a suitable variational data structure. This step solves the expression computed for each behavioral fragment taking into account (a) the fragment's variability information and (b) the reliability values already computed for the fragments on which it depends. The result is an Algebraic Decision Diagram (ADD) whose terminals different than zero represent the reliability value of valid (partial) configurations for the fragment. Therefore, the ADD computed for the last evaluated fragment contains the reliability values for all valid configurations of the software product line.Results: We performed an experiment to compare our feature-family-based and other four state-of-the-art evaluation strategies (product-based, family-based, feature-product-based and family-product-based). The subjects were variations of six publicly available product lines, whose configuration spaces were progressively increased. The empirical results show that our feature-family-based strategy outperforms, in terms of time and space, the other four state-of-the-art strategies. In addition, it is the only one that could be scaled to a 220-fold increase in the size of the configuration space.Conclusion: Our feature-family-based strategy leverages both feature-based and family-based strategies by taming the size of the models to be analyzed (due to the decomposition of behavioral models into fragments) and by avoiding the products enumeration inherent to some state-of-the-art analysis methods by using ADDs to represent both variability and reliability values.Journal paper: This paper was published at the Information and Software Technology Journal. It is available at https://doi.org/10.1016/j.infsof.2017.10.001.Supplementary material: Additional material to the IST submission is available at https://splmc.github.io/scalabilityAnalysis/. This material comprises experiments data, the tool implementing the feature-family-based reliability analysis strategy and the environment for experiment replication.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {64},
numpages = {1},
keywords = {software reliability analysis, software product lines, parametric verification},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3579027.3608985,
author = {Bittner, Paul Maximilian and Schulthei\ss{}, Alexander and Greiner, Sandra and Moosherr, Benjamin and Krieter, Sebastian and Tinnes, Christof and Kehrer, Timo and Th\"{u}m, Thomas},
title = {Views on Edits to Variational Software},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608985},
doi = {10.1145/3579027.3608985},
abstract = {Software systems are subject to frequent changes, for example to fix bugs or meet new customer requirements. In variational software systems, developers are confronted with the complexity of evolution and configurability on a daily basis; essentially handling changes to many distinct software variants simultaneously. To reduce the complexity of configurability for developers, filtered or projectional editing was introduced: By providing a partial or complete configuration, developers can interact with a simpler view of the variational system that shows only artifacts belonging to that configuration. Yet, such views are available for individual revisions only but not for edits performed across revisions. To reduce the complexity of evolution in variational software for developers, we extend the concept of views to edits. We formulate a correctness criterion for views on edits and introduce two correct operators for view generation, one operator suitable for formal reasoning, and a runtime optimized operator. In an empirical study, we demonstrate the feasibility of our operators by applying them to the change histories of 44 open-source software systems.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {141–152},
numpages = {12},
keywords = {variation control, software variability, software product lines, software evolution, projectional editing},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3579027.3608984,
author = {Krieter, Sebastian and Kr\"{u}ger, Jacob and Leich, Thomas and Saake, Gunter},
title = {VariantInc: Automatically Pruning and Integrating Versioned Software Variants},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608984},
doi = {10.1145/3579027.3608984},
abstract = {Developers use version-control systems and software-hosting platforms to manage their software systems. They rely on the provided branching and forking mechanisms to implement new features, fix bugs, and develop customized system variants. A particular problem arises when forked variants are not re-integrated (i.e., merged), but kept and co-evolved as individual systems. This can cause maintenance overheads, due to change propagation and limitations in simultaneously managing variations in space (variants) and time (revisions). Thus, most organizations decide to integrate their set of variants into a single platform at some point, and several techniques have been proposed to semi-automate such an integration. However, existing techniques usually consider only a single revision of each variant and do not merge the revision histories, disregarding that not only variants (i.e., configuring the features of the system) but also revisions (i.e., checking out specific versions of the features) are important. We propose an automated technique, VariantInc, for analyzing, pruning, and integrating variants of a system that also merges the revision history of each variant into the resulting platform (i.e., using presence conditions). To validate VariantInc, we employed it on 160 open-source C systems of various sizes (i.e., number of forks, revisions, source code). The results show that VariantInc works as intended, and allows developers or researchers to automatically integrate variants into a platform as well as to perform software analyses.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {129–140},
numpages = {12},
keywords = {Version control, Variant-rich systems, Variant integration, Forks},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3461001.3471142,
author = {Gu\'{e}gain, \'{E}douard and Quinton, Cl\'{e}ment and Rouvoy, Romain},
title = {On reducing the energy consumption of software product lines},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471142},
doi = {10.1145/3461001.3471142},
abstract = {Along the last decade, several studies considered green software design as a key development concern to improve the energy efficiency of software. Yet, few techniques address this concern for Software Product Lines (SPL). In this paper, we therefore introduce two approaches to measure and reduce the energy consumption of a SPL by analyzing a limited set of products sampled from this SPL. While the first approach relies on the analysis of individual feature consumptions, the second one takes feature interactions into account to better mitigate energy consumption of resulting products.Our experimental results on a real-world SPL indicate that both approaches succeed to produce significant energy improvements on a large number of products, while consumption data was modeled from a small set of sampled products. Furthermore, we show that taking feature interactions into account leads to more products improved with higher energy savings per product.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {89–99},
numpages = {11},
keywords = {software product lines, mitigation, measurement, energy, consumption},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3546932.3547007,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Quality-aware analysis and optimisation of virtual network functions},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547007},
doi = {10.1145/3546932.3547007},
abstract = {The softwarisation and virtualisation of network functionality is the last milestone in the networking industry. Software-Defined Networks (SDN) and Network Function Virtualization (NFV) offer the possibility of using software to manage computer and mobile networks and build novel Virtual Network Functions (VNFs) deployed in heterogeneous devices. To reason about the variability of network functions and especially about the quality of a software product defined as a set of VNFs instantiated as part of a service (i.e., Service Function Chaining), a variability model along with a quality model is required.However, this domain imposes certain challenges to quality-aware reasoning of service function chains, such as numerical features or configuration-level Quality Attributes (QAs) (e.g., energy consumption). Incorporating numerical reasoning with quality data into SPL analyses is challenging and tool support is rare. In this work, we present 3 groups of operations: model report, aggregate functions to dynamically convert QAs at the feature-level into the configuration-level, and quality-aware optimisation. Our objective is to test the most complete reasoning tools to exploit the extended variability with quality attributes needed for VNFs.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {210–221},
numpages = {12},
keywords = {virtual network function, variability, reasoning, quality attribute, optimization, numerical feature},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3106195.3106223,
author = {Iglesias, Aitziber and Lu, Hong and Arellano, Crist\'{o}bal and Yue, Tao and Ali, Shaukat and Sagardui, Goiuria},
title = {Product Line Engineering of Monitoring Functionality in Industrial Cyber-Physical Systems: A Domain Analysis},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106223},
doi = {10.1145/3106195.3106223},
abstract = {In recent years, manufacturing technology is evolving and progressively becoming more dynamic and complex. This means that manufacturing technology (e.g., based on Industry 4.0) should be able to control the production process at runtime by monitoring physical elements and adapting itself. Such functionality is aimed at increasing production effectiveness and reducing the production cost. We argue that monitoring process can be viewed as a software product line having commonalities and variability. To support our argument, we analyzed and conducted domain analysis of two monitoring systems of Industrial Cyber-Physical Systems (ICPSs) from two industrial domains including automated warehouses and press machines. Based on the domain analysis, we present a common solution for monitoring including a software product line. With such product line, a user can configure, monitor, and visualize data of an ICPS at runtime. However, such solution could not handle the dynamic functionality related to monitoring of ICPS. Thus, we propose the use of dynamic product line and present a set of research questions that must be addressed for such solution.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {195–204},
numpages = {10},
keywords = {Software Product Line, Industrial domains, Dynamic Software Product Line, Cyber Physical System},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3461002.3473944,
author = {Ballesteros, Joaqu\'{\i}n and Fuentes, Lidia},
title = {Transfer learning for multiobjective optimization algorithms supporting dynamic software product lines},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473944},
doi = {10.1145/3461002.3473944},
abstract = {Dynamic Software Product Lines (DSPLs) are a well-accepted approach for self-adapting Cyber-Physical Systems (CPSs) at run-time. The DSPL approaches make decisions supported by performance models, which capture system features' contribution to one or more optimization goals. Combining performance models with Multi-Objectives Evolutionary Algorithms (MOEAs) as decision-making mechanisms is common in DSPLs. However, MOEAs algorithms start solving the optimization problem from a randomly selected population, not finding good configurations fast enough after a context change, requiring too many resources so scarce in CPSs. Also, the DSPL engineer must deal with the hardware and software particularities of the target platform in each CPS deployment. And although each system instantiation has to solve a similar optimization problem of the DSPL, it does not take advantage of experiences gained in similar CPS. Transfer learning aims at improving the efficiency of systems by sharing the previously acquired knowledge and applying it to similar systems. In this work, we analyze the benefits of transfer learning in the context of DSPL and MOEAs testing on 8 feature models with synthetic performance models. Results are good enough, showing that transfer learning solutions dominate up to 71% of the non-transfer learning ones for similar DSPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {51–59},
numpages = {9},
keywords = {transfer learning, self-adaptation, multiobjective optimization algorithms, dynamic software product lines, cyber-physical systems},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3382025.3414942,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Kr\"{u}ger, Jacob and Mendon\c{c}a, Willian D. F.},
title = {Variability management meets microservices: six challenges of re-engineering microservice-based webshops},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414942},
doi = {10.1145/3382025.3414942},
abstract = {A microservice implements a small unit of functionality that it provides through a network using lightweight protocols. So, microservices can be combined to fulfill tasks and implement features of a larger software system---resembling a variability mechanism in the context of a software product line (SPL). Microservices and SPLs have similar goals, namely facilitating reuse and customizing, but they are usually employed in different contexts. Any developer who has access to the network can provide a microservice for any task, while SPLs are usually intended to implement features of a specific domain. Due to their different concepts, using microservices to implement an SPL or adopting SPL practices (e.g., variability management) for microservices is a challenging cross-area research problem. However, both techniques can complement each other, and thus tackling this problem promises benefits for organizations that employ either technique. In this paper, we reason on the importance of advancing in this direction, and sketch six concrete challenges to initiate research, namely (1) feature identification, (2) variability modeling, (3) variable microservice architectures, (4) interchangeability, (5) deep customization, and (6) re-engineering an SPL. We intend these challenges to serve as a starting point for future research in this cross-area research direction---avoiding that the concepts of one area are reinvented in the other.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {22},
numpages = {6},
keywords = {variability management, software product line, re-engineering, microservices, cloud computing},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1145/1183236.1183261,
author = {Clements, Paul C. and Jones, Lawrence G. and McGregor, John D. and Northrop, Linda M.},
title = {Getting there from here: a roadmap for software product line adoption},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183261},
doi = {10.1145/1183236.1183261},
abstract = {Mapping the technical and business activities and steps required for successful organizational adoption.},
journal = {Commun. ACM},
month = dec,
pages = {33–36},
numpages = {4}
}

@inproceedings{10.1145/3382026.3431247,
author = {Meixner, Kristof},
title = {Integrating Variability Modeling of Products, Processes, and Resources in Cyber-Physical Production Systems Engineering},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431247},
doi = {10.1145/3382026.3431247},
abstract = {The Industry 4.0 initiative envisions the flexible and optimized production of customized products on Cyber-Physical Production Systems (CPPSs) that consist of subsystems coordinated to conduct complex production processes. Hence, accurate CPPS modeling requires integrating the modeling of variability for Product-Process-Resource (PPR) aspects. Yet, current variability modeling approaches treat structural and behavioral variability separately, leading to inaccurate CPPS production models that impede CPPS engineering and optimization. This paper proposes a PhD project for integrated variability modeling of PPR aspects to improve the accuracy of production models with variability for CPPS engineers and production optimizers. The research project follows the Design Science approach aiming for the iterative design and evaluation of (a) a framework to categorize currently incomplete and scattered models and methods for PPR variability modeling as a foundation for an integrated model; and (b) a modeling approach for more accurate integrated PPR variability modeling. The planned research will provide the Software Product Line (SPL) and CPPS engineering research communities with (a) novel models, methods, and insights on integrated PPR variability modeling, (b) open data from CPPS engineering use cases for common modeling, and (c) empirical data from field studies for shared analysis and evaluation.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {96–103},
numpages = {8},
keywords = {Variability Modelling, Product-Process-Resource, Cyber-Physical Production System},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3579028.3609007,
author = {Fortz, Sophie},
title = {Variability-aware Behavioural Learning},
year = {2023},
isbn = {9798400700927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579028.3609007},
doi = {10.1145/3579028.3609007},
abstract = {Addressing variability proactively during software engineering activities means shifting from reasoning on individual systems to reasoning on families of systems. Adopting appropriate variability management techniques can yield important economies of scale and quality improvements. Conversely, variability can also be a curse, especially for Quality Assurance (QA), i.e., verification and testing of such systems, due to the combinatorial explosion of the number of software variants. Featured Transition Systems (FTSs) were introduced as a way to represent and reason about the behaviour of Variaility-intensive Systems (VISs). By labelling a transition system with feature expressions, FTSs capture multiple variants of a system in a single model, enabling reasoning at the family level. They have shown significant improvements in automated QA activities such as model-checking and model-based testing, as well as guiding design exploration activities. Yet, as most model-based approaches, FTS modelling requires both strong human expertise and significant effort that would be unaffordable in many cases, in particular for large legacy systems with outdated specifications and/or systems that evolve continuously.Therefore, this PhD project aims to automatically learn FTSs from existing artefacts, to ease the burden of modelling FTS and support continuous QA activities. To answer this research challenge, we propose a two-phase approach. First, we rely on deep learning techniques to locate variability from execution traces. For this purpose, we implemented a tool called VaryMinions. Then, we use these annotated traces to learn an FTS. In this second part, we adapt the seminal L algorithm to learn behavioural variability. Both frameworks are open-source and we evaluated them separately on several datasets of different sizes and origins (e.g., software product lines and configurable business processes).},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
pages = {11–15},
numpages = {5},
keywords = {Variability Mining, Software Product Lines, Reverse Engineering, Featured Transition Systems, Active Automata Learning},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3579027.3608976,
author = {Dimovski, Aleksandar S. and Lazreg, Sami and Cordy, Maxime and Legay, Axel},
title = {Family-based model checking of fMultiLTL properties},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608976},
doi = {10.1145/3579027.3608976},
abstract = {We introduce a new logic for expressing multi-properties of system families (Software Product Lines - SPLs). While the standard LTL logic refers only to a single trace at a time, fMultiLTL logic proposed here refers to multiple traces originating from different sets of variants of the SPL. This is achieved by allowing so-called featured quantification over traces, ∀ψ and ∃ψ, where the feature expression ψ describes a set of variants (sub-family) the quantified trace comes from. A specialized family-based model checking algorithm for verifying some fragments of fMultiLTL is given. A prototype family-based model checker, called D\k{a}dalux, has been implemented. We illustrate the practicality of this approach on several interesting SPL models.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {41–51},
numpages = {11},
keywords = {Temporal Multi-Properties, Software Product Lines, Model Checking, LTL},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3579027.3608998,
author = {Luo, Chuan and Song, Jianping and Zhao, Qiyuan and Li, Yibei and Cai, Shaowei and Hu, Chunming},
title = {Generating Pairwise Covering Arrays for Highly Configurable Software Systems},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608998},
doi = {10.1145/3579027.3608998},
abstract = {Highly configurable software systems play crucial roles in real-world applications, which urgently calls for useful testing methods. Combinatorial interaction testing (CIT) is an effective methodology for detecting those faults that are triggered by the interaction of any t options, where t is the testing strength. Pairwise testing, i.e., CIT with t = 2, is known to be the most practical and popular CIT technique, and the pairwise covering array generation (PCAG) problem is the most critical problem in pairwise testing. Due to the practical importance of PCAG, many PCAG algorithms have been proposed. Unfortunately, existing PCAG algorithms suffer from the severe scalability problem. To this end, the SPLC Scalability Challenge (i.e., Product Sampling for Product Lines: The Scalability Challenge) has been proposed since 2019, in order to motivate researchers to develop practical PCAG algorithms for overcoming this scalability problem. In this work, we present a practical PCAG algorithm dubbed SamplingCA-ASF. To the best of our knowledge, our experiments show that SamplingCA-ASF is the first algorithm that can generate PCAs for Automotive02 and Linux, the two hardest and largest-scale instances in the SPLC Scalability Challenge, within reasonable time. Our experimental results indicate that SamplingCA-ASF can effectively alleviate the scalability problem in pairwise testing.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {261–267},
numpages = {7},
keywords = {scalability problem, pairwise testing, covering array generation},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3579027.3608972,
author = {Acher, Mathieu and Duarte, Jos\'{e} Galindo and J\'{e}z\'{e}quel, Jean-Marc},
title = {On Programming Variability with Large Language Model-based Assistant},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608972},
doi = {10.1145/3579027.3608972},
abstract = {Programming variability is central to the design and implementation of software systems that can adapt to a variety of contexts and requirements, providing increased flexibility and customization. Managing the complexity that arises from having multiple features, variations, and possible configurations is known to be highly challenging for software developers. In this paper, we explore how large language model (LLM)-based assistants can support the programming of variability.We report on new approaches made possible with LLM-based assistants, like: features and variations can be implemented as prompts; augmentation of variability out of LLM-based domain knowledge; seamless implementation of variability in different kinds of artefacts, programming languages, and frameworks, at different binding times (compile-time or run-time). We are sharing our data (prompts, sessions, generated code, etc.) to support the assessment of the effectiveness and robustness of LLMs for variability-related tasks.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {8–14},
numpages = {7},
keywords = {variability, software product lines, programming, large language model, generative AI},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3336294.3336317,
author = {Duszynski, Slawomir and Dhar, Saura Jyoti and Beichter, Tobias},
title = {Using Relation Graphs for Improved Understanding of Feature Models in Software Product Lines},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336317},
doi = {10.1145/3336294.3336317},
abstract = {Feature models are widely used for describing the variability of a software product line. A feature model contains a tree of features and a set of constraints over these features, which define valid feature combinations. In the industrial practice, large feature models containing hundreds of features and constraints are common. Furthermore, in a hierarchical product line a feature model can be related to other feature models through inter-model constraints. Due to the model size and complexity, understanding industrial feature models is a challenging task.In this paper, we describe the feature model understanding challenges reported by feature model developers at Robert Bosch GmbH. To support the developers in model understanding, we extend the idea of a feature implication graph to feature relation graph by abstracting groups of implications to feature relations. A transitively closed relation graph shows all modeled and implicit feature relations and spans all related feature models. The graph is also used to identify modeling problems, such as false optional or dead features, and to show the derivation of any implicit relation or problem from the modeled constraints. In a case study at Bosch, we evaluate the use of feature relation graph for model understanding. We propose further use cases of the graph, supporting model maintenance, evolution and configuration.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {309–319},
numpages = {11},
keywords = {model understanding, implication graph, feature model},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3579027.3608980,
author = {Pett, Tobias and He\ss{}, Tobias and Krieter, Sebastian and Th\"{u}m, Thomas and Schaefer, Ina},
title = {Continuous T-Wise Coverage},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608980},
doi = {10.1145/3579027.3608980},
abstract = {Quality assurance for highly configurable systems uses t-wise feature interaction coverage as a metric to measure the quality of selected samples for testing. Achieving t-wise feature interaction coverage requires testing many configurations, often exceeding the available testing time for frequently evolving systems. As testing time is a limiting factor, current testing procedures face the challenge of finding a reasonable trade-off between achieving t-wise feature interaction coverage and reducing the time required for testing. To address this challenge, we can consider t-wise feature interactions covered in previous test executions when calculating the achieved t-wise feature interaction coverage. However, the current definition of t-wise feature interaction coverage does not consider previously tested configurations. Therefore, we propose continuous t-wise coverage as a new customizable metric for tracking the ratio of achieved t-wise feature interaction coverage over time. Our metric allows customizing the tradeoff between test effort per system version and the time to achieve t-wise coverage. We evaluate various parameterizations for our metric on four real-world evolution histories and investigate how they impact the calculated t-wise feature interaction coverage. Our results show that a high t-wise feature interaction coverage can be achieved by testing significant (up to 50%) smaller samples per commit, when the evolution of the system is considered.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {87–98},
numpages = {12},
keywords = {t-wise coverage, spl testing, spl evolution, software-product lines, sampling},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3382025.3414961,
author = {Favalli, Luca and K\"{u}hn, Thomas and Cazzola, Walter},
title = {Neverlang and FeatureIDE just married: integrated language product line development environment},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414961},
doi = {10.1145/3382025.3414961},
abstract = {Language development is inherently complex. With the support of a suitable language development environment most computer scientists could develop their own domain-specific language (DSL) with relative ease. Yet, when the DSL is the result of a configuration over a language product line (LPL)---a special software product line (SPL) of compilers/interpreters and corresponding IDE services---they fail to provide adequate support. An environment for LPL engineering should facilitate the underlying process involving three distinct roles: a language engineer developing the LPL, a language deployer configuring a language product, and a language user using the language product. Neither IDEs nor SPLE environments can cater all three roles and fully support the LPL engineering process with distributed, incremental development, configuration, and deployment of language variants. In this paper, we present an LPL engineering process for the distributed, incremental development of LPLs and an integrated language product line development environment supporting this process, catering the three roles, and ensuring the consistency among all artifacts of the LPL: language components implementing a language feature, the feature model, language configurations and the resulting language products. To create such an environment, we married the Neverlang language workbench and AiDE its LPL engineering environment with the FeatureIDE SPL engineering environment. While Neverlang supports the development of LPLs and deployment of language products, AiDE generates the feature model for the LPL under development, whereas FeatureIDE handles the feature configuration. We illustrate the applicability of the LPL engineering process and the suitability of our development environment for the three roles by showcasing its application for teaching programming with a growable language. In there, an LPL for Javascript was developed/refactored, 15 increasingly complex language products were configured/updated and finally deployed.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {33},
numpages = {11},
keywords = {neverlang, language product lines, domain specific languages},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3461001.3461660,
author = {Michelon, Gabriela Karoline and Obermann, David and Assun\c{c}\~{a}o, Wesley K. G. and Linsbauer, Lukas and Gr\"{u}nbacher, Paul and Egyed, Alexander},
title = {Managing systems evolving in space and time: four challenges for maintenance, evolution and composition of variants},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3461660},
doi = {10.1145/3461001.3461660},
abstract = {Software companies need to provide a large set of features satisfying functional and non-functional requirements of diverse customers, thereby leading to variability in space. Feature location techniques have been proposed to support software maintenance and evolution in space. However, so far only one feature location technique also analyses the evolution in time of system variants, which is required for feature enhancements and bug fixing. Specifically, existing tools for managing a set of systems over time do not offer proper support for keeping track of feature revisions, updating existing variants, and creating new product configurations based on feature revisions. This paper presents four challenges concerning such capabilities for feature (revision) location and composition of new product configurations based on feature/s (revisions). We also provide a benchmark containing a ground truth and support for computing metrics. We hope that this will motivate researchers to provide and evaluate tool-supported approaches aiming at managing systems evolving in space and time. Further, we do not limit the evaluation of techniques to only this benchmark: we introduce and provide instructions on how to use a benchmark extractor for generating ground truth data for other systems. We expect that the feature (revision) location techniques maximize information retrieval in terms of precision, recall, and F-score, while keeping execution time and memory consumption low.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {75–80},
numpages = {6},
keywords = {software product line, repository mining, feature revision, feature location, benchmark extractor},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.1145/3628158,
author = {Xiang, Yi and Huang, Han and Li, Sizhe and Li, Miqing and Luo, Chuan and Yang, Xiaowei},
title = {Automated Test Suite Generation for Software Product Lines Based on Quality-Diversity Optimization},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3628158},
doi = {10.1145/3628158},
abstract = {A Software Product Line (SPL) is a set of software products that are built from a variability model. Real-world SPLs typically involve a vast number of valid products, making it impossible to individually test each of them. This arises the need for automated test suite generation, which was previously modeled as either a single-objective or a multi-objective optimization problem considering only objective functions. This article provides a completely different mathematical model by exploiting the benefits of Quality-Diversity (QD) optimization that is composed of not only an objective function (e.g., t-wise coverage or test suite diversity) but also a user-defined behavior space (e.g., the space with test suite size as its dimension). We argue that the new model is more suitable and generic than the two alternatives because it provides at a time a large set of diverse (measured in the behavior space) and high-performing solutions that can ease the decision-making process. We apply MAP-Elites, one of the most popular QD algorithms, to solve the model. The results of the evaluation, on both realistic and artificial SPLs, are promising, with MAP-Elites significantly and substantially outperforming both single- and multi-objective approaches, and also several state-of-the-art SPL testing tools. In summary, this article provides a new and promising perspective on the test suite generation for SPLs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {46},
numpages = {52},
keywords = {Quality-Diversity (QD) optimization, automated test suite generation, Software Product Line}
}

@inproceedings{10.1145/3503229.3547038,
author = {Workalemahu, Robel Negussie and Forza, Cipriano and Suzic, Nikola},
title = {Product configurators for additively manufactured products: exploring their peculiar characteristics},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547038},
doi = {10.1145/3503229.3547038},
abstract = {The capability of realizing individually customized products with complex geometries makes additive manufacturing (AM) ever more considered by companies engaged in mass customized manufacturing. In order to be exploited in the market, the AM allowed geometry freedom has to be transferred to the customers for the customer-specific customization. Notably, this is a new request posed to product configurators (PC). So, in this research we ask: How is this request being answered by pioneers who engage in this challenge? Are there other new requests that AM poses to configurators? The present paper aims at answering these exploratory questions by looking at how these issues have been considered in existing literature and by providing some examples. We hope that considerations derived from this investigation will open a discussion on this topic in the product configuration research community with the goal to identify peculiar PC capabilities needed to customize additively manufactured products using PCs.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {201–208},
numpages = {8},
keywords = {additive manufacturing, mass customization, personalization, product configuration},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3503229.3547069,
author = {Comploi-Taupe, Richard and Francescutto, Giulia and Schenner, Gottfried},
title = {Applying incremental answer set solving to product configuration},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547069},
doi = {10.1145/3503229.3547069},
abstract = {In this paper, we apply incremental answer set solving to product configuration. Incremental answer set solving is a step-wise incremental approach to Answer Set Programming (ASP). We demonstrate how to use this technique to solve product configurations problems incrementally. Every step of the incremental solving process corresponds to a predefined configuration action. Using complex domain-specific configuration actions makes it possible to tightly control the level of non-determinism and performance of the solving process. We show applications of this technique for reasoning about product configuration, like simulating the behavior of a deterministic configuration algorithm and describing user actions.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {150–155},
numpages = {6},
keywords = {answer set programming, incremental solving, product configuration},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3579028.3609017,
author = {Bombarda, Andrea and Bonfanti, Silvia and Gargantini, Angelo},
title = {On the Reuse of Existing Configurations for Testing Evolving Feature Models},
year = {2023},
isbn = {9798400700927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579028.3609017},
doi = {10.1145/3579028.3609017},
abstract = {Software Product Lines (SPLs) are used for representing a variety of highly configurable systems or families of systems. They are commonly represented by feature models (FMs). Starting from FMs, configurations, used as test cases, can be generated to identify the products of interest for further activities. As the other types of software, SPLs and their FMs may evolve due to changing requirements or bug-fixing. However, no guidance is usually given on what to do with derived configurations when an FM evolves. The common approach is based on generating all configurations from scratch, which is not optimal since a greater effort is required for concretizing the new tests, and some of the old ones may be still applicable.In this paper, we present the use of a technique for generating combinatorial tests for evolving feature models: this technique incrementally builds the new combinatorial configuration set starting from the one generated from the previous model. Furthermore, we present a novel definition of dissimilarity among configuration sets that can be used to evaluate how much an evolved test suite differs from the previous one and thus allows evaluating the effort required for adapting old test cases to the new ones.Our experiments confirm that using the proposed technique, in general, leads to lower dissimilarity and test suite size w.r.t. the generation of tests from scratch.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
pages = {67–76},
numpages = {10},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3106195.3106219,
author = {Gregg, Susan P. and Albert, Denise M. and Clements, Paul},
title = {Product Line Engineering on the Right Side of the "V"},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106219},
doi = {10.1145/3106195.3106219},
abstract = {Product line engineering (PLE) is well-known for the savings it brings to organizations. This paper shows how a very large, in-service systems and software product line is achieving PLE-based savings in their verification and validation phase of development. The paper addresses how to achieve the sharing across product variants while the products being tested are evolving over time. Additionally, we will give a pragmatic set of decision criteria to help answer the longstanding issue in PLE-based testing of whether to test on the domain side or the application (product) side of the product derivation process.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {165–174},
numpages = {10},
keywords = {variation points, software product lines, second generation product line engineering, product portfolio, product configurator, feature profiles, feature modeling, bill-of-features, Product line engineering, PLE factory, AEGIS Combat System},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3503229.3547041,
author = {Kucher, Maximilian and Balyo, Tom\'{a}\v{s} and Christensen, Noemi},
title = {Black-box optimization in a configuration system},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547041},
doi = {10.1145/3503229.3547041},
abstract = {The product configurator Merlin is a CPQ solution (Configure, Price, Quote) that enables fast, error-free configuration and quotation generation for products with many variants. In the context of this paper an optimization module was developed and integrated into Merlin. Previously, Merlin could only minimize the number of changes that must be made when a user changes a configuration. With this work, the optimization capability in Merlin was extended in a way, that a user can define a custom target function. Specific features and variables then can be selected for optimization. The optimization module can optimize the values of these attributes and variables with respect to the defined target function. The optimization process has no limited runtime and does not stop automatically when reaching certain predefined values, since in the field of optimization often no promises can be made on finding global extrema. Instead, the optimization process is monitored live by the user and can be terminated at any time as soon as the user is satisfied with the current solution. In addition to the adaptation of the Merlin frontend, two black-box and derivative-free optimization algorithms are implemented and tested for performance to solve the optimization problem.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {229–236},
numpages = {8},
keywords = {black-box, configuration, optimization},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3461001.3471152,
author = {Silva, Publio and Bezerra, Carla I. M. and Machado, Ivan},
title = {A machine learning model to classify the feature model maintainability},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471152},
doi = {10.1145/3461001.3471152},
abstract = {Software Product Lines (SPL) are generally specified using a Feature Model (FM), an artifact designed in the early stages of the SPL development life cycle. This artifact can quickly become too complex, which makes it challenging to maintain an SPL. Therefore, it is essential to evaluate the artifact's maintainability continuously. The literature brings some approaches that evaluate FM maintainability through the aggregation of maintainability measures. Machine Learning (ML) models can be used to create these approaches. They can aggregate the values of independent variables into a single target data, also called a dependent variable. Besides, when using white-box ML models, it is possible to interpret and explain the ML model results. This work proposes white-box ML models intending to classify the FM maintainability based on 15 measures. To build the models, we performed the following steps: (i) we compared two approaches to evaluate the FM maintainability through a human-based oracle of FM maintainability classifications; (ii) we used the best approach to pre-classify the ML training dataset; (iii) we generated three ML models and compared them against classification accuracy, precision, recall, F1 and AUC-ROC; and, (iv) we used the best model to create a mechanism capable of providing improvement indicators to domain engineers. The best model used the decision tree algorithm that obtained accuracy, precision, and recall of 0.81, F1-Score of 0.79, and AUC-ROC of 0.91. Using this model, we could reduce the number of measures needed to evaluate the FM maintainability from 15 to 9 measures.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {35–45},
numpages = {11},
keywords = {software product line, quality evaluation, machine learning, feature model},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3579027.3608971,
author = {Eichhorn, Domenik and Pett, Tobias and Osborne, Tobias and Schaefer, Ina},
title = {Quantum Computing for Feature Model Analysis: Potentials and Challenges},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608971},
doi = {10.1145/3579027.3608971},
abstract = {Feature modeling is a technique to model the variability of configurable systems. When working with feature models, it is possible to analyze them, for instance, by counting the number of valid configurations, searching feature model anomalies, or creating samples of configurations for testing. Classical feature model analysis techniques are based on solving algorithmic problems such as boolean satisfiability, satisfiability modulo theories, or integer linear programming. Existing analysis approaches provide satisfactory solutions for small and medium-sized problem instances, but scaling issues are observed for large-sized feature models. Quantum computers provide up to superpolynomial speedups for specific algorithmic problems and have the potential to solve those scaling issues. This paper analyzes the algorithmic techniques used in classical product line analysis and identifies potentials and challenges for quantum speedups. Our findings show that quantum algorithms like QAOA and Grover have the potential to speed up SAT and ILP-based feature model analysis techniques, but only after additional improvements in quantum hardware have been made.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {1–7},
numpages = {7},
keywords = {quantum computing, quantum algorithms, feature model analysis},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3579027.3608973,
author = {Galindo, Jos\'{e} A. and Dominguez, Antonio J. and White, Jules and Benavides, David},
title = {Large Language Models to generate meaningful feature model instances},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608973},
doi = {10.1145/3579027.3608973},
abstract = {Feature models are the "de facto" standard for representing variability in software-intensive systems. Automated analysis of feature models is the computer-aided extraction of information of feature models and is used in testing, maintenance, configuration, and derivation, among other tasks. Testing the analyses of feature models often requires relying on a large number of models that are as realistic as possible. There exist different proposals to generate synthetic feature models using random techniques or metamorphic relations; however, the existing methods do not take into account the semantics of the concepts of the domain that are being represented and the interrelations between them, leading to less realistic feature models. In this paper, we propose a novel approach that uses Large Language Models (LLMs), such as Codex or GPT-3, to generate realistic feature models that preserve semantic coherence while maintaining syntactic validity. The approach automatically generates instances of feature models from a given domain. Concretely, two language models were used, first OpenAI's Codex to generate new instances of feature models using the Universal Variability Language (UVL) syntax and then Cohere's semantic analysis to verify if the newly introduced concepts are from the same domain. This approach enabled the generation of 90% of valid instances according to the UVL syntax. In addition, the valid models score well on model complexity metrics, and the generated features mirror the domain of the original UVL instance used as prompts. With this work, we envision a new thread of research where variability is generated and analyzed using LLMs. This opens the door for a new generation of techniques and tools for variability management.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {15–26},
numpages = {12},
keywords = {universal variability language, synthetic models, large language models, deep learning},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3503229.3547059,
author = {Wittler, Jan Willem and K\"{u}hn, Thomas and Reussner, Ralf},
title = {Towards an integrated approach for managing the variability and evolution of both software and hardware components},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547059},
doi = {10.1145/3503229.3547059},
abstract = {Although the development of mass-customized products has been successfully applied to both hardware and software, companies struggle managing the variability and evolution of software-intensive products within a coherent product engineering approach. While the variability and evolution of software alone is manageable, managing both software and hardware within one product line is a complex task and requires an integrated approach. Moreover, as the release cycle for hardware is longer than for software, a product line of hardware and software is usually developed in generations. While one generation is validated and produced, the next generation is already planned and designed, reusing both software and hardware of previous generations. Thus, the different generations and artifacts shared between them must be managed together. Finally, when approaches directly assign software to hardware, managing their evolution becomes increasingly complex. Evolved resource demands may be missed, exhausting the resources provided by the hardware, possibly leading to degraded or faulty functionality. To remedy this, we refine the Unified Conceptual Model to our Variability Model for both Software and Hardware capturing the notion of product line generations, versions and variants of both software and hardware components, as well as resource demands of software on hardware. This is the first step towards the development of an integrated product engineering approach for managing the variability and evolution of software-intensive products.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {94–98},
numpages = {5},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3546932.3546989,
author = {Bertolotti, Francesco and Cazzola, Walter and Favalli, Luca},
title = {Features, believe it or not! a design pattern for first-class citizen features on stock JVM},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3546989},
doi = {10.1145/3546932.3546989},
abstract = {Modern software systems must fulfill the needs of an ever-growing customer base. Due to the innate diversity of human needs, software should be highly customizable and reconfigurable. Researchers and practitioners gained interest in software product lines (SPL), mimicking aspects of product lines in industrial production for the engineering of highly-variable systems. There are two main approaches towards the engineering of SPLs. The first uses macros---such as the #ifdef macro in C. The second---called feature-oriented programming (FOP)---uses variability-aware preprocessors called composers to generate a program variant from a set of features and a configuration. Both approaches have disadvantages. Most notably, these approaches are usually not supported by the base language; for instance Java is one of the most commonly used FOP languages among researchers, but it does not support macros rather it relies on the C preprocessor or a custom one to translate macros into actual Java code. As a result, developers must struggle to keep up with the evolution of the base language, hindering the general applicability of SPL engineering. Moreover, to effectively evolve a software configuration and its features, their location must be known. The problem of recording and maintaining traceability information is considered expensive and error-prone and it is once again handled externally through dedicated modeling languages and tools. Instead, to properly convey the FOP paradigm, software features should be treated as first-class citizens using concepts that are proper to the host language, so that the variability can be expressed and analyzed with the same tools used to develop any other software in the same language. In this paper, we present a simple and flexible design pattern for JVM-based languages---dubbed devise pattern---that can be used to express feature dependencies and behaviors with a light-weight syntax both at domain analysis and at domain implementation level. To showcase the qualities and feasibility of our approach, we present several variability-aware implementations of a MNIST-encoder---including one using the devise pattern---and compare strengths and weaknesses of each approach.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {32–42},
numpages = {11},
keywords = {variability modeling, software product lines, design patterns},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3236405.3236426,
author = {Belarbi, Maouaheb},
title = {A methodological framework to enable the generation of code from DSML in SPL},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236426},
doi = {10.1145/3236405.3236426},
abstract = {Software Product Line has acquired a significant momentum at the end of the 1990ies since it allows the production of variable software systems corresponding to the same domain portfolio. The effectiveness of the derivation process depends on how well variability is defined and implemented which is a crucial topic area that was addressed among two essential trends: On the one hand, starting from Domain Specific Modelling Language to express domain requirements and automate the code generation with Model-Driven Engineering techniques and on the second hand, exploiting the soar of variability mechanisms.In this context, the current research presents a method that unifies the two aforementioned approaches to cover the overall strategies by defining a framework that allows a better code generation in terms of documentation, maintainability, rapidity,etc. The starting point is the usage of the Domain Specific Modelling Language to represent the stakeholders requirements. Then, the resulting meta-model will be converted into one our several Feature Diagrams on which variability mechanisms can be applied to generate all the family products.A preliminary experiment has been undertaken to design the methodology of the proposed software factory in a meta-model. The validation task was evaluated with an academic use case called HandiWeb developed to facilitate handicap persons access to the internet. The first results allow us to put the hand on the key challenges that must be resolved by the proposed methodology.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {64–71},
numpages = {8},
keywords = {variability, software factory, methodology, SPL, DSML},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3461001.3473064,
author = {Ayala, Inmaculada and Papadopoulos, Alessandro V. and Amor, Mercedes and Fuentes, Lidia},
title = {ProDSPL: proactive self-adaptation based on dynamic software product lines},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3473064},
doi = {10.1145/3461001.3473064},
abstract = {This is an extended abstract of the article: Inmaculada Ayala, Alessandro V. Papadopoulos, Mercedes Amor, Lidia Fuentes, ProDSPL: Proactive self-adaptation based on Dynamic Software Product Lines, Journal of Systems and Software, Volume 175, 2021, 110909, ISSN 0164-1212, https://doi.org/10.1016/j.jss.2021.110909.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {81},
numpages = {1},
keywords = {self-adaptation, proactive control, optimization, linear constraint, dynamic software product lines},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461001.3471148,
author = {Krieter, Sebastian and Arens, Rahel and Nieke, Michael and Sundermann, Chico and He\ss{}, Tobias and Th\"{u}m, Thomas and Seidl, Christoph},
title = {Incremental construction of modal implication graphs for evolving feature models},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471148},
doi = {10.1145/3461001.3471148},
abstract = {A feature model represents a set of variants as configurable features and dependencies between them. During variant configuration, (de)selection of a feature may entail that other features must or cannot be selected. A Modal Implication Graph (MIG) enables efficient decision propagation to perform automatic (de)selection of subsequent features. In addition, it facilitates other configuration-related activities such as t-wise sampling. Evolution of a feature model may change its configuration logic, thereby invalidating an existing MIG and forcing a full recomputation. However, repeated recomputation of a MIG is expensive, and thus hampers the overall usefulness of MIGs for frequently evolving feature models. In this paper, we devise a method to incrementally compute updated MIGs after feature model evolution. We identify expensive steps in the MIG construction algorithm, enable them for incremental computation, and measure performance compared to a full rebuild of a complete MIG within the evolution histories of four real-world feature models. Results show that our incremental method can increase the speed of MIG construction by orders of magnitude, depending on the given scenario and extent of evolutionary changes.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {64–74},
numpages = {11},
keywords = {software product line, evolution, configurable system},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2791060.2791106,
author = {Smiley, Karen and Schmidt, Werner and Dagnino, Aldo},
title = {Evolving an industrial analytics product line architecture},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791106},
doi = {10.1145/2791060.2791106},
abstract = {This paper focuses on an industrial experience with software product lines of analytics-enabled solutions, specifically the evolution of the software product line architecture for a Subject Matter Expert Workbench toolset which supports analytic plugins for multiple software product lines. As context, the toolset product line was intended for integration of expert knowledge into a family of industrial asset health applications at runtime. The toolset architecture is now being evolved to build and manage plugins for multiple Industrial Analytics solutions (software systems and services) beyond asset health. This evolution is driving changes in the desired architecture qualities of the toolset; widening the stakeholder pool and influencing priorities; affecting the architecture tradeoffs and decisions; and triggering updates to the product line architecture, the guidance for applying it, and the current prototype of the toolset. We describe our experiences in handling this evolution, assess lessons learned, and discuss potential relevance to other product line scenarios.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {263–272},
numpages = {10},
keywords = {software product line, reusability, performance, knowledge, interoperability, industrial analytics, extensibility, asset health},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3233027.3236395,
author = {Pereira, Juliana Alves and Maciel, Lucas and Noronha, Thiago F. and Figueiredo, Eduardo},
title = {Heuristic and exact algorithms for product configuration in software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236395},
doi = {10.1145/3233027.3236395},
abstract = {The Software Product Line (SPL) configuration field is an active area of research and has attracted both practitioners and researchers attention in the last years. A key part of an SPL configuration is a feature model that represents features and their dependencies (i.e., SPL configuration rules). This model can be extended by adding Non-Functional Properties (NFPs) as feature attributes resulting in Extended Feature Models (EFMs). Configuring products from an EFM requires considering the configuration rules of the model and satisfying the product functional and non-functional requirements. Although the configuration of a product arising from EFMs may reduce the space of valid configurations, selecting the most appropriate set of features is still an overwhelming task due to many factors including technical limitations and diversity of contexts. Consequently, configuring large and complex SPLs by using configurators is often beyond the users' capabilities of identifying valid combinations of features that match their (non-functional) requirements. To overcome this limitation, several approaches have modeled the product configuration task as a combinatorial optimization problem and proposed constraint programming algorithms to automatically derive a configuration. Although these approaches do not require any user intervention to guarantee the optimality of the generated configuration, due to the NP-hard computational complexity of finding an optimal variant, exact approaches have inefficient exponential time. Thus, to improve scalability and performance issues, we introduced the adoption of a greedy heuristic algorithm and a biased random-key genetic algorithm (BRKGA). Our experiment results show that our proposed heuristics found optimal solutions for all instances where those are known. For the instances where optimal solutions are not known, the greedy heuristic outperformed the best solution obtained by a one-hour run of the exact algorithm by up to 67.89%. Although the BRKGA heuristic slightly outperformed the greedy heuristic, it has shown larger running times (especially on the largest instances). Therefore, to ensure a good user experience and enable a very fast configuration task, we extended a state-of-the-art configurator with the proposed greedy heuristic approach.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {247},
numpages = {1},
keywords = {software product lines, software product line configuration, search-based software engineering, configuration optimization},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3634713.3634725,
author = {G\"{u}thing, Lukas and Bittner, Paul Maximilian and Schaefer, Ina and Th\"{u}m, Thomas},
title = {Explaining Edits to Variability Annotations in Evolving Software Product Lines},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634725},
doi = {10.1145/3634713.3634725},
abstract = {Software is subject to changes and revisions during its development life cycle. For configurable software systems, changes may be made to functionality of source code as well as variability information such as code-to-feature mappings. To explain how code-to-feature mappings change in edits made to configurable software, we relate the mappings before and after an edit in terms of the sets of variants they denote. We prove our explanations to be complete and unambiguous, meaning that every pair of code-to-feature mappings is explained in terms of exactly one relation. Based on a graph formalism, we provide an algorithm for fast detection of relations during commits to version control. In an initial study, we detect relations between feature annotations in 42 real-world software product-line repositories to better understand typical changes in the evolution of configurable software. We demonstrate that our formalism can be automated and that analyzing a commit requires only 135 ms on average.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {93–102},
numpages = {10},
keywords = {software evolution, software product lines, software variability},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.1145/2491627.2491631,
author = {Myll\"{a}rniemi, Varvana and Savolainen, Juha and M\"{a}nnist\"{o}, Tomi},
title = {Performance variability in software product lines: a case study in the telecommunication domain},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491631},
doi = {10.1145/2491627.2491631},
abstract = {In the research on software product lines, product variants typically differ by their functionality, and quality attributes are more or less similar across products. To accumulate empirical evidence, this paper presents a descriptive case study of performance variability in a software product line of mobile network base stations. The goal is to study the motivation to vary performance, and the strategy for realizing performance variability in the product line architecture. The results highlight that the evolution of customer needs motivates performance variability; performance variability can be realized either with software or hardware variability strategy, with the latter often being prevailing; and the software strategy can be kept focused by downgrading performance.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {32–41},
numpages = {10},
keywords = {variability, software product line, case study, architecture},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3503229.3547067,
author = {Martinez, Jabier and Str\"{u}ber, Daniel and Horcas, Jose Miguel and Burdusel, Alexandru and Zschaler, Steffen},
title = {Acapulco: an extensible tool for identifying optimal and consistent feature model configurations},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547067},
doi = {10.1145/3503229.3547067},
abstract = {Configuring feature-oriented variability-rich systems is complex because of the large number of features and, potentially, the lack of visibility of the implications on quality attributes when selecting certain features. We present Acapulco as an alternative to the existing tools for automating the configuration process with a focus on mono- and multi-criteria optimization. The soundness of the tool has been proven in a previous publication comparing it to SATIBEA and MODAGAME. The main advantage was obtained through consistency-preserving configuration operators (CPCOs) that guarantee the validity of the configurations during the IBEA genetic algorithm evolution process. We present a new version of Acapulco built on top of FeatureIDE, extensible through the easy integration of objective functions, providing pre-defined reusable objectives, and being able to handle complex feature model constraints.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {50–53},
numpages = {4},
keywords = {genetic algorithms, software product lines, variability management},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3546932.3547074,
author = {Dhungana, Deepak and Haselb\"{o}ck, Alois and Ruiz-Torrubiano, Rub\'{e}n and Wallner, Stefan},
title = {Variability of safety risks in production environments},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547074},
doi = {10.1145/3546932.3547074},
abstract = {One of the major steps between (re-)configuration of a factory and the start of production is the commissioning phase, where certification of safety requirements and assessment of potential hazards is a key activity. Typically, assessment of safety risks is a manual process that incorporates the experience and knowledge of the involved stakeholders. The flexibility and the speed gained by automated (re-)configuration of production environments is decelerated by the manual safety certification process, before the factory can start production. This paper is an attempt to eliminate this bottleneck by proposing a model-driven approach to safety risk assessment. An approach based on several models of safety risks enables potential safety risks to be "instantiated" for any given factory at hand. This shortens the recurring process of identifying the risks. A model-driven approach was chosen to capture and utilize the tacit knowledge of the involved stakeholders.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {178–187},
numpages = {10},
keywords = {safety certification, model-driven commissioning, hazards in production environments, automated identification of risks},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3461002.3473942,
author = {Kahraman, G\"{o}khan and Cleophas, Loek},
title = {Automated derivation of variants in manufacturing systems design},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473942},
doi = {10.1145/3461002.3473942},
abstract = {The Logistics Specification and Analysis Tool (LSAT) is a modelbased engineering tool used for design-space exploration of flexible manufacturing systems. LSAT provides domain specific languages to model a manufacturing system and means to analyze the productivity characteristics of such a system. In LSAT, developers can specify a system and model its deterministic operations as a set of activities. Given a set of activities, it is possible to construct an individual activity sequence that represents one valid system execution, and with minor variations in the specification individual systems can be obtained. To avoid modeling each variant separately, which means cloning and maintaining the common parts, new functionality is needed to deal with the variability of system specifications. In this study, we aim to establish integration between LSAT and product line engineering techniques. Specifically, we provide a realization of a toolchain including variability representation of LSAT realization artifacts and automated variant derivation for the LSAT model variants. Delta modeling, a transformational variability realization mechanism, is employed to model the variability within LSAT realization artifacts. Using the toolchain, we develop an industry-related case for a product line, the so called Extended Twilight System, a Cyber Physical System (CPS) inspired by the CPSs of our industrial partner.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {45–50},
numpages = {6},
keywords = {variability modeling, product lines, model-based engineering, manufacturing systems, delta modeling},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3307630.3342413,
author = {Arcaini, Paolo and Gargantini, Angelo and Radavelli, Marco},
title = {A Process for Fault-Driven Repair of Constraints Among Features},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342413},
doi = {10.1145/3307630.3342413},
abstract = {The variability of a Software Product Line is usually both described in the problem space (by using a variability model) and in the solution space (i.e., the system implementation). If the two spaces are not aligned, wrong decisions can be done regarding the system configuration. In this work, we consider the case in which the variability model is not aligned with the solution space, and we propose an approach to automatically repair (possibly) faulty constraints in variability models. The approach takes as input a variability model and a set of combinations of features that trigger conformance faults between the model and the real system, and produces the repaired set of constraints as output. The approach consists of three major phases. First, it generates a test suite and identifies the condition triggering the faults. Then, it modifies the constraints of the variability model according to the type of faults. Lastly, it uses a logic minimization method to simplify the modified constraints. We evaluate the process on variability models of 7 applications of various sizes. An empirical analysis on these models shows that our approach can effectively repair constraints among features in an automated way.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {73–81},
numpages = {9},
keywords = {variability model, system evolution, fault, automatic repair},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3307630.3342704,
author = {Ca\~{n}ete, Angel},
title = {Energy Efficient Assignment and Deployment of Tasks in Structurally Variable Infrastructures},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342704},
doi = {10.1145/3307630.3342704},
abstract = {The importance of cyber-physical systems is growing very fast, being part of the Internet of Things vision. These devices generate data that could collapse the network and can not be assumed by the cloud. New technologies like Mobile Cloud Computing and Mobile Edge Computing are taking importance as solution for this issue. The idea is offloading some tasks to devices situated closer to the user device, reducing network congestion and improving applications performance (e.g., in terms of latency and energy). However, the variability of the target devices' features and processing tasks' requirements is very diverse, being difficult to decide which device is more adequate to deploy and run such processing tasks. Once decided, task offloading used to be done manually. Then, it is necessary a method to automatize the task assignation and deployment process. In this thesis we propose to model the structural variability of the deployment infrastructure and applications using feature models, on the basis of a SPL engineering process. Combining SPL methodology with Edge Computing, the deployment of applications is addressed as the derivation of a product. The data of the valid configurations is used by a task assignment framework, which determines the optimal tasks offloading solution in different network devices, and the resources of them that should be assigned to each task/user. Our solution provides the most energy and latency efficient deployment solution, accomplishing the QoS requirements of the application in the process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {222–229},
numpages = {8},
keywords = {software product line, optimisation, mobile edge computing, mobile cloud computing, latency, energy efficiency},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3461001.3471147,
author = {Kenner, Andy and May, Richard and Kr\"{u}ger, Jacob and Saake, Gunter and Leich, Thomas},
title = {Safety, security, and configurable software systems: a systematic mapping study},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471147},
doi = {10.1145/3461001.3471147},
abstract = {Safety and security are important properties of any software system, particularly in safety-critical domains, such as embedded, automotive, or cyber-physical systems. Moreover, particularly those domains also employ highly-configurable systems to customize variants, for example, to different customer requirements or regulations. Unfortunately, we are missing an overview understanding of what research has been conducted on the intersection of safety and security with configurable systems. To address this gap, we conducted a systematic mapping study based on an automated search, covering ten years (2011--2020) and 65 relevant (out of 367) publications. We classified each publication based on established security and safety concerns (e.g., CIA triad) as well as the connection to configurable systems (e.g., ensuring security of such a system). In the end, we found that considerably more research has been conducted on safety concerns, but both properties seem under-explored in the context of configurable systems. Moreover, existing research focuses on two directions: Ensuring safety and security properties in product-line engineering; and applying product-line techniques to ensure safety and security properties. Our mapping study provides an overview of the current state-of-the-art as well as open issues, helping practitioners identify existing solutions and researchers define directions for future research.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {148–159},
numpages = {12},
keywords = {software product line engineering, security, safety, mapping study, configurable systems},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3546932.3547002,
author = {Schulze, Sandro and Kr\"{u}ger, Jacob and W\"{u}nsche, Johannes},
title = {Towards developer support for merging forked test cases},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547002},
doi = {10.1145/3546932.3547002},
abstract = {Developers rely on branching and forking mechanisms of modern versioning systems to evolve and maintain their software systems. As a result, systems often exist in the form of various short-living or even long-living (i.e., clone &amp; own development) variants. Such variants may have to be merged with the main system or other variants, for instance, to propagate features or bug fixes. Within such merging processes, test cases are highly interesting, since they allow to improve the test coverage and hopefully the reliability of the system (e.g., by merging missing tests and bug fixes in test code). However, as all source code, test cases may evolve independently between two or more variants, which makes it non-trivial to decide what changes of the test cases are relevant for the merging. For instance, some test cases in one variant may be irrelevant in another variant (e.g., because the feature shall not be propagated) or may subsume existing test cases. In this paper, we propose a technique that allows for a fine-grained comparison of test cases to support developers in deciding whether and how to merge these. Precisely, inspired by code-clone detection, we use abstract syntax trees to decide on the relations between test cases of different variants. We evaluate the applicability of our technique qualitatively on five open-source systems written in Java (e.g., JUnit 5, Guava). Our insights into the merge potential of 50 pull requests with test cases from these systems indicate that our technique can support the comprehension of differences in variants' test cases, and also highlight future research opportunities.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {131–141},
numpages = {11},
keywords = {variant-rich systems, test cases, merging, feature forks},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3382025.3414969,
author = {Bilic, Damir and Carlson, Jan and Sundmark, Daniel and Afzal, Wasif and Wallin, Peter},
title = {Detecting inconsistencies in annotated product line models},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414969},
doi = {10.1145/3382025.3414969},
abstract = {Model-based product line engineering applies the reuse practices from product line engineering with graphical modeling for the specification of software intensive systems. Variability is usually described in separate variability models, while the implementation of the variable systems is specified in system models that use modeling languages such as SysML. Most of the SysML modeling tools with variability support, implement the annotation-based modeling approach. Annotated product line models tend to be error-prone since the modeler implicitly describes every possible variant in a single system model. To identifying variability-related inconsistencies, in this paper, we firstly define restrictions on the use of SysML for annotative modeling in order to avoid situations where resulting instances of the annotated model may contain ambiguous model constructs. Secondly, inter-feature constraints are extracted from the annotated model, based on relations between elements that are annotated with features. By analyzing the constraints, we can identify if the combined variability- and system model can result in incorrect or ambiguous instances. The evaluation of our prototype implementation shows the potential of our approach by identifying inconsistencies in the product line model of our industrial partner which went undetected through several iterations of the model.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {20},
numpages = {11},
keywords = {variability modeling, product line engineering, model-based systems engineering, consistency checking, SysML},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3546932.3546996,
author = {Uta, Mathias and Felfernig, Alexander and Helic, Denis and Le, Viet-Man},
title = {Accuracy- and consistency-aware recommendation of configurations},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3546996},
doi = {10.1145/3546932.3546996},
abstract = {Constraint-based configurators support users in deciding which components and features should be included in a configuration. Due to the increasing size and complexity of configurable products and services, recommender systems are used to personalize the interaction with configurators. Since basic recommendation approaches such as collaborative filtering do not take into account constraints between variable values, recommendations can induce inconsistencies between user requirements and the underlying configuration knowledge base. In this paper, we introduce a constraint-based configuration approach that integrates the results of model-based collaborative filtering (e.g., implemented as feed forward neural network) into constraint solving in such a way that the solver (configurator) is able to determine consistency-preserving and user-relevant configurations.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {79–84},
numpages = {6},
keywords = {neural networks, feature models, constraint solving, configuration, collaborative filtering},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3503229.3547047,
author = {Felfernig, Alexander and Ortner, Bettina and Le, Viet-Man},
title = {Table-based knowledge representations for industrial feature models},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547047},
doi = {10.1145/3503229.3547047},
abstract = {Configuration knowledge engineering in industrial settings often has to face the challenge that product domain experts do not have background knowledge in formal configuration knowledge representations. In this context, graphical configuration models such as feature models provide a basis for the communication between domain experts and knowledge engineers. However, in many cases domain experts are used to define configuration knowledge in table-based representations. In this paper, we discuss ways of how to represent basic configuration constraint types in the form of a table-based representation thus allowing an alternative definition and exchange of product configuration knowledge.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {245–248},
numpages = {4},
keywords = {feature models, knowledge-based configuration, table-based knowledge representations},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3336294.3336318,
author = {Ebert, Rolf and Jolianis, Jahir and Kriebel, Stefan and Markthaler, Matthias and Pruenster, Benjamin and Rumpe, Bernhard and Salman, Karin Samira},
title = {Applying Product Line Testing for the Electric Drive System},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336318},
doi = {10.1145/3336294.3336318},
abstract = {The growth in electrification and digitalization of vehicles leads to increasing variability and complexity of automotive systems. This poses new challenges for verification and validation, identified in a Product Line Engineering case study for the electric drive system. To overcome those challenges we developed a Product Line Testing methodology called TIGRE. In this paper, we present the TIGRE methodology. TIGRE comprises the identification and documentation of relevant data for efficient product line testing and the application of this data in the test management of an agile project environment. Furthermore, we present our experiences from the introduction into a large-scale industrial context. Based on our results from the introduction, we conclude that the TIGRE approach reduces the testing effort for automotive product lines significantly and, furthermore, allows us to transfer the results to untested products.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {14–24},
numpages = {11},
keywords = {software product lines, product line testing, product line engineering, automotive industry},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3546932.3547009,
author = {J\'{e}z\'{e}quel, Jean-Marc and Kienzle, J\"{o}rg and Acher, Mathieu},
title = {From feature models to feature toggles in practice},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547009},
doi = {10.1145/3546932.3547009},
abstract = {Feature Toggles (often also referred to as Feature Flags) are a powerful technique, providing an alternative to maintaining multiple feature branches in source code. A condition within the code enables or disables a feature at runtime, hence providing a kind of runtime variability resolution. Several works have already identified the proximity of this concept with the notion of Feature found in Software Product Lines. In this paper, we propose to go one step further in unifying these concepts to provide a seamless transition between design time and runtime variability resolutions. We propose to model all the variability using a feature model. Then this feature model can be partially resolved at design time (yielding an incomplete product derivation), the unresolved variability being used to generate feature toggles that can be enabled/disabled at runtime. We first demonstrate these ideas on the toy example of the Expression Product Line, and then show how it can scale to build a configurable authentication system, where a partially resolved feature model can interface with popular feature toggle frameworks such as Togglz.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {234–244},
numpages = {11},
keywords = {variability, feature toggles and flags, configuration, binding times},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/2934466.2934473,
author = {Olaechea, Rafael and Fahrenberg, Uli and Atlee, Joanne M. and Legay, Axel},
title = {Long-term average cost in featured transition systems},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934473},
doi = {10.1145/2934466.2934473},
abstract = {A software product line is a family of software products that share a common set of mandatory features and whose individual products are differentiated by their variable (optional or alternative) features. Family-based analysis of software product lines takes as input a single model of a complete product line and analyzes all its products at the same time. As the number of products in a software product line may be large, this is generally preferable to analyzing each product on its own. Family-based analysis, however, requires that standard algorithms be adapted to accomodate variability.In this paper we adapt the standard algorithm for computing limit average cost of a weighted transition system to software product lines. Limit average is a useful and popular measure for the long-term average behavior of a quality attribute such as performance or energy consumption, but has hitherto not been available for family-based analysis of software product lines. Our algorithm operates on weighted featured transition systems, at a symbolic level, and computes limit average cost for all products in a software product line at the same time. We have implemented the algorithm and evaluated it on several examples.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {109–118},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3503229.3547046,
author = {Le, Viet-Man and Felfernig, Alexander and Tran, Thi Ngoc Trang},
title = {Test case aggregation for efficient feature model testing},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547046},
doi = {10.1145/3503229.3547046},
abstract = {The increasing size and complexity of feature models (FM) requires the provision of efficient testing and debugging techniques. Feature models can be tested, for example, with regard to their conformance with a pre-defined set of analysis operations. In this paper, we show how the number of consistency checks for FM testing can be reduced on the basis of test case aggregation. Using a divide-and-conquer based approach, we show how to transform a feature model test suite into a corresponding aggregated representation where individual test cases can be combined if specific consistency criteria are fulfilled. Performance improvements are also analyzed on the basis of a best- and worst-case runtime analysis.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {174–177},
numpages = {4},
keywords = {testing and debugging, variability modeling},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3546932.3546992,
author = {Le, Viet-Man and Felfernig, Alexander and Uta, Mathias and Tran, Thi Ngoc Trang and Silva, Cristian Vidal},
title = {WipeOutR: automated redundancy detection for feature models},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3546992},
doi = {10.1145/3546932.3546992},
abstract = {Feature models are used to specify variability and commonality properties of software artifacts. In order to assure high-quality models, different feature model analysis and testing operations can be applied. In this paper, we present two new algorithms that help to make feature model configuration as well as different kinds of analysis operations more efficient. Specifically, we focus on the automated identification of redundancies in feature models and cor-responding test suites. Redundant constraints in feature models can lead to low-performing configuration (solution) search and also to additional efforts in feature model debugging. Redundant feature model test cases can trigger inefficiencies in testing operations. In this paper, we introduce WipeOutR which is an algorithmic approach to support the automated identification of redundancies. This approach has the potential to significantly improve the quality of feature model development and configuration.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {164–169},
numpages = {6},
keywords = {variability modeling, testing and debugging, redundancy detection, quality assurance, feature models},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3336294.3336321,
author = {Ghofrani, Javad and Kozegar, Ehsan and Fehlhaber, Anna Lena and Soorati, Mohammad Divband},
title = {Applying Product Line Engineering Concepts to Deep Neural Networks},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336321},
doi = {10.1145/3336294.3336321},
abstract = {Deep Neural Networks (DNNs) are increasingly being used as a machine learning solution thanks to the complexity of their architecture and hyperparameters-weights. A drawback is the excessive demand for massive computational power during the training process. Not only as a whole but parts of neural networks can also be in charge of certain functionalities. We present a novel challenge in an intersection between machine learning and variability management communities to reuse modules of DNNs without further training. Let us assume that we are given a DNN for image processing that recognizes cats and dogs. By extracting a part of the network, without additional training a new DNN should be divisible with the functionality of recognizing only cats. Existing research in variability management can offer a foundation for a product line of DNNs composing the reusable functionalities. An ideal solution can be evaluated based on its speed, granularity of determined functionalities, and the support for adding variability to the network. The challenge is decomposed in three subchallenges: feature extraction, feature abstraction, and the implementation of a product line of DNNs.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {72–77},
numpages = {6},
keywords = {variability, transfer learning, software product lines, machine learning, deep neural networks},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3503229.3547049,
author = {Sandrin, Enrico and Forza, Cipriano and Leitner, Gerhard and Trentin, Alessio},
title = {Configuration manager: describing an emerging professional figure},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547049},
doi = {10.1145/3503229.3547049},
abstract = {The implementation and management of product configurators in enterprises are accompanied by several challenges and it is widely recognized that the organizational ones are among the biggest. In order to overcome such challenges, companies need people with adequate competencies. What are the required individual competencies to successfully implement and use product configurators (both sales and technical ones)? Which are the characteristics of an ideal professional figure that can have all these competencies? How could the needed competencies be developed through training? This paper brings these questions to the scientific discussion on product and sales configurators. However, these questions have a wider scope since they also relate to the enquiry on mass customization and on product variety management: they deepen the perspective on organizational design related to these issues.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {193–200},
numpages = {8},
keywords = {configuration, configuration manager, individual competencies, mass customization, organization design, professional profile},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3503229.3547030,
author = {Baranov, Eduard and Legay, Axel},
title = {Baital: an adaptive weighted sampling platform for configurable systems},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547030},
doi = {10.1145/3503229.3547030},
abstract = {The diversity of software application scenarios has led the evolution towards highly configurable systems. Testing of such systems is challenging due to an immense number of configurations and is usually performed on a small sample set. Sampling is a promising approach for the sample set generation. t-wise coverage is often used to measure the quality of sample sets. Uniform sampling being most known method can fail to achieve high coverage in presence of complex constraints on configurations. Another challenge is a scalability hurdle for the t-wise coverage computation leaving sampling for higher values of t unexplored.In this work, we present Baital, a platform that combines two novel techniques for sampling of configurable systems. It is based on the adaptive weighted sampling approach to generate sample sets with high t-wise coverage. The approximation techniques for the t-wise coverage computation allow the consideration of higher values of t; they improve scalability for both t-wise coverage computation and sampling process.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {46–49},
numpages = {4},
keywords = {adaptive weighted sampling, configurable systems, t-wise coverage},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3503229.3547039,
author = {Vandevelde, Simon and Callewaert, Benjamin and Vennekens, Joost},
title = {Interactive feature modeling with background knowledge for validation and configuration},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547039},
doi = {10.1145/3503229.3547039},
abstract = {Feature modeling enables a straightforward representation of a product's features, components, and the relations between them. In this way, feature models serve as an excellent approach to diagrammatically model a product design for manufacturing purposes. However, the actual usage of such a feature model to generate suitable designs in the context of real-life industry applications is often limited, as crucial background knowledge cannot be expressed. Moreover, even though expert validation of a feature model is an important aspect of its creation, current tooling often falls short on this aspect. Indeed, although state-of-the-art tools are capable of generating possible configurations, this is not sufficient to completely validate complex applications: instead, we should enable the expert to interactively explore the problem domain. In this paper, we present our feature modeling tool, called FM-IDP, which aims to overcome both of these shortcomings. In FM-IDP, background knowledge can be expressed in FO(·), a rich extension of classical first-order logic. Using an off-the-shelf logical reasoning engine and an integrated interactive configuration interface, modelers can interact with the feature model and its background knowledge to explore the problem space on-the-fly. We motivate our approach using an industrial use case focused on real-life component design.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {209–216},
numpages = {8},
keywords = {IDP, background knowledge, feature modeling, interactive configuration},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/2791060.2791118,
author = {ter Beek, Maurice H. and Fantechi, Alessandro and Gnesi, Stefania and Mazzanti, Franco},
title = {Using FMC for family-based analysis of software product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791118},
doi = {10.1145/2791060.2791118},
abstract = {We show how the FMC model checker can successfully be used to model and analyze behavioural variability in Software Product Lines. FMC accepts parameterized specifications in a process-algebraic input language and allows the verification of properties of such models by means of efficient on-the-fly model checking. The properties can be expressed in a logic that allows to correlate the parameters of different actions within the same formula. We show how this feature can be used to tailor formulas to the verification of only a specific subset of products of a Software Product Line, thus allowing for scalable family-based analyses with FMC. We present a proof-of-concept that shows the application of FMC to an illustrative Featured Transition System from the literature.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {432–439},
numpages = {8},
keywords = {variability, process algebra, model transformation, features, featured transition systems},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2491627.2491646,
author = {Marijan, Dusica and Gotlieb, Arnaud and Sen, Sagar and Hervieu, Aymeric},
title = {Practical pairwise testing for software product lines},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491646},
doi = {10.1145/2491627.2491646},
abstract = {One key challenge for software product lines is efficiently managing variability throughout their lifecycle. In this paper, we address the problem of variability in software product lines testing. We (1) identify a set of issues that must be addressed to make software product line testing work in practice and (2) provide a framework that combines a set of techniques to solve these issues. The framework integrates feature modelling, combinatorial interaction testing and constraint programming techniques. First, we extract variability in a software product line as a feature model with specified feature interdependencies. We then employ an algorithm that generates a minimal set of valid test cases covering all 2-way feature interactions for a given time interval. Furthermore, we evaluate the framework on an industrial SPL and show that using the framework saves time and provides better test coverage. In particular, our experiments show that the framework improves industrial testing practice in terms of (i) 17% smaller set of test cases that are (a) valid and (b) guarantee all 2-way feature coverage (as opposite to 19.2% 2-way feature coverage in the hand made test set), and (ii) full flexibility and adjustment of test generation to available testing time.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {227–235},
numpages = {9},
keywords = {variability management, software product lines, feature modelling},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3503229.3547040,
author = {Allibe, Mathis and Sylla, Abdourahim and Alpan, G\"{u}lg\"{u}n},
title = {A generic knowledge model for resource reconfiguration in the context of reconfigurable manufacturing systems},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547040},
doi = {10.1145/3503229.3547040},
abstract = {In this article, in order to help manufacturers to better manage manufacturing resource reconfiguration in the context of reconfigurable manufacturing systems, we propose a generic knowledge-based model that can support resource reconfiguration decision-making while considering various manufacturing requirements and constraints. The model is based on Constraint Satisfaction Problem (CSP) framework. The two presented scenarios demonstrate that the application of a Knowledge-Based System (KBS) is a great opportunity to improve manufacturing systems' responsiveness.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {217–223},
numpages = {7},
keywords = {constraint satisfaction problem (CSP), knowledge modeling, knowledge-based system (KBS), reconfigurable manufacturing system (RMS), resource reconfiguration},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3382025.3414943,
author = {Th\"{u}m, Thomas},
title = {A BDD for Linux? the knowledge compilation challenge for variability},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414943},
doi = {10.1145/3382025.3414943},
abstract = {What is the number of valid configurations for Linux? How to generate uniform random samples for Linux? Can we create a binary decision diagram for Linux? It seems that the product-line community tries hard to answer such questions for Linux and other configurable systems. However, attempts are often not published due to the publication bias (i.e., unsuccessful attempts are not published). As a consequence, researchers keep trying by potentially spending redundant effort. The goal of this challenge is to guide research on these computationally complex problems and to foster the exchange between researchers and practitioners.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {16},
numpages = {6},
keywords = {software product line, software configuration, satisfiability solving, product configuration, knownledge compilation, feature models, decision models, configurable system, binary decision diagrams, artificial intelligence},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/1629716.1629729,
author = {Liebig, J\"{o}rg and Apel, Sven and Lengauer, Christian and Leich, Thomas},
title = {RobbyDBMS: a case study on hardware/software product line engineering},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629729},
doi = {10.1145/1629716.1629729},
abstract = {The development of a highly configurable data management system is a challenging task, especially if it is to be implemented on an embedded system that provides limited resources. We present a case study of such a data management system, called RobbyDBMS, and give it a feature-oriented design. In our case study, we evaluate the system's efficiency and variability. We pay particular attention to the interaction between the features of the data management system and the components of the underlying embedded platform. We also propose an integrated development process covering both hardware and software.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {63–68},
numpages = {6},
keywords = {FeatureC++, domain engineering, feature oriented software development, hardware product lines, software product lines},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@inproceedings{10.1145/3461002.3473070,
author = {Acher, Mathieu and Perrouin, Gilles and Cordy, Maxime},
title = {BURST: a benchmarking platform for uniform random sampling techniques},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473070},
doi = {10.1145/3461002.3473070},
abstract = {We present BURST, a benchmarking platform for uniform random sampling techniques. With BURST, researchers have a flexible, controlled environment in which they can evaluate the scalability and uniformity of their sampling. BURST comes with an extensive --- and extensible --- benchmark dataset comprising 128 feature models, including challenging, real-world models of the Linux kernel. BURST takes as inputs a sampling tool, a set of feature models and a sampling budget. It automatically translates any feature model of the set in DIMACS and invokes the sampling tool to generate the budgeted number of samples. To evaluate the scalability of the sampling tool, BURST measures the time the tool needs to produce the requested sample. To evaluate the uniformity of the produced sample, BURST integrates the state-of-the-art and proven statistical test Barbarik. We envision BURST to become the starting point of a standardisation initiative of sampling tool evaluation. Given the huge interest of research for sampling algorithms and tools, this initiative would have the potential to reach and crosscut multiple research communities including AI, ML, SAT and SPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {36–40},
numpages = {5},
keywords = {variability model, software product lines, sampling, configurable systems, benchmark, SAT},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3382026.3425772,
author = {Ca\~{n}ete, Angel and Amor, Mercedes and Fuentes, Lidia},
title = {Supporting the evolution of applications deployed on edge-based infrastructures using multi-layer feature models},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3425772},
doi = {10.1145/3382026.3425772},
abstract = {The proliferation of cyber-physical systems has encouraged the emergence of new technologies and paradigms to improve the performance of IoT-based applications. Edge Computing proposes using the nearby devices in the frontier/Edge of the access network for deploying application tasks. However, the functionality of cyberphysical systems, which is usually distributed in several devices and computers, imposes specific requirements on the infrastructure to run properly. The evolution of an application to meet new user requirements and the high diversity of hardware and software technologies in the edge can complicate the deployment of evolved applications.The aim of our approach is to apply Multi Layer Feature Models, which capture the variability of applications and the infrastructure, to support the deployment in edge-based environments of cyber-physical systems applications. This separation can support the evolution of application and infrastructure. Considering that IoT/Edge/Cloud infrastructures are usually shared by many applications, the SPL deployment process has to assure that there will be enough resources for all of them, informing developers about the alternatives of deployment. Prior to its deployment and leaning on the infrastructure feature models, the developer can calculate what is the configuration of minimal set of devices supporting application requirements of the evolved application. In addition, the developer can find which is the application configuration that can be hosted in the current evolved infrastructure.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {79–87},
numpages = {9},
keywords = {Software Product Line, Software Evolution, Multi Layer Feature Models, Internet of Things, Edge Computing},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3461001.3473065,
author = {Michelon, Gabriela K. and Sotto-Mayor, Bruno and Martinez, Jabier and Arrieta, Aitor and Abreu, Rui and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Spectrum-based feature localization: a case study using ArgoUML},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3473065},
doi = {10.1145/3461001.3473065},
abstract = {Feature localization (FL) is a basic activity in re-engineering legacy systems into software product lines. In this work, we explore the use of the Spectrum-based localization technique for this task. This technique is traditionally used for fault localization but with practical applications in other tasks like the dynamic FL approach that we propose. The ArgoUML SPL benchmark is used as a case study and we compare it with a previous hybrid (static and dynamic) approach from which we reuse the manual and testing execution traces of the features. We conclude that it is feasible and sound to use the Spectrum-based approach providing promising results in the benchmark metrics.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {126–130},
numpages = {5},
keywords = {spectrum-based localization, dynamic feature localization, ArgoUML SPL benchmark},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2791060.2791093,
author = {Souto, Sabrina and Gopinath, Divya and d'Amorim, Marcelo and Marinov, Darko and Khurshid, Sarfraz and Batory, Don},
title = {Faster bug detection for software product lines with incomplete feature models},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791093},
doi = {10.1145/2791060.2791093},
abstract = {A software product line (SPL) is a family of programs that are differentiated by features --- increments in functionality. Systematically testing an SPL is challenging because it requires running each test of a test suite against a combinatorial number of programs. Feature models capture dependencies among features and can (1) reduce the space of programs to test and (2) enable accurate categorization of failing tests as failures of programs or the tests themselves, not as failures due to illegal combinations of features. In practice, sadly, feature models are not always available.We introduce SPLif, the first approach for testing SPLs that does not require the a priori availability of feature models. Our insight is to use a profile of passing and failing test runs to quickly identify failures that are indicative of real problems in test or code rather than specious failures due to illegal feature combinations.Experimental results on five SPLs and one large configurable system (GCC) demonstrate the effectiveness of our approach. SPLif enabled the discovery of five news bugs in GCC, three of which have already been fixed.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {151–160},
numpages = {10},
keywords = {software testing, feature models, GCC},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3233027.3233035,
author = {Varshosaz, Mahsa and Al-Hajjaji, Mustafa and Th\"{u}m, Thomas and Runge, Tobias and Mousavi, Mohammad Reza and Schaefer, Ina},
title = {A classification of product sampling for software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233035},
doi = {10.1145/3233027.3233035},
abstract = {The analysis of software product lines is challenging due to the potentially large number of products, which grow exponentially in terms of the number of features. Product sampling is a technique used to avoid exhaustive testing, which is often infeasible. In this paper, we propose a classification for product sampling techniques and classify the existing literature accordingly. We distinguish the important characteristics of such approaches based on the information used for sampling, the kind of algorithm, and the achieved coverage criteria. Furthermore, we give an overview on existing tools and evaluations of product sampling techniques. We share our insights on the state-of-the-art of product sampling and discuss potential future work.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {1–13},
numpages = {13},
keywords = {testing, software product lines, sampling algorithms, feature interaction, domain models},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3622748.3622754,
author = {Lima, Rafael and Bezerra, Carla and Machado, Ivan},
title = {A Self-Adaptation Mechanism for Variability Management in Dynamic Software Product Lines},
year = {2023},
isbn = {9798400709524},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622748.3622754},
doi = {10.1145/3622748.3622754},
abstract = {Variability management is crucial for companies that need to offer multiple product variants quickly. However, the increasing complexity of software-intensive systems has made variability management increasingly challenging. This challenge is compounded by the need for such systems to run continuously and adapt to changes in the environment and user needs at runtime. To address this challenge, Dynamic Software Product Line (DSPL) Engineering has emerged as a strategy for managing variability in complex and dynamic environments. The key challenge in DSPL engineering is to manage product configurations at runtime by detecting changes in the context and adapting accordingly. In this paper, we propose an adaptation mechanism for DSPL feature models that supports dynamic variability and is based on the MAPE-K model. The mechanism transforms feature model constraints into rules that enable the activation of each feature and annotates contexts in the corresponding features to be activated when changes occur. We have implemented the mechanism in the DyMMer 2.0 modeling tool and evaluated its performance using various DSPL feature models. Additionally, we performed a preliminary evaluation with a proof-of-concept study with an expert to assess its practical usage. Our results demonstrate the effectiveness and practicality of the proposed mechanism in managing variability in complex and dynamic environments.},
booktitle = {Proceedings of the 17th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {51–60},
numpages = {10},
keywords = {variability management, feature model, dynamic software product line},
location = {Campo Grande, Brazil},
series = {SBCARS '23}
}

@inproceedings{10.1145/3233027.3233030,
author = {Weckesser, Markus and Kluge, Roland and Pfannem\"{u}ller, Martin and Matth\'{e}, Michael and Sch\"{u}rr, Andy and Becker, Christian},
title = {Optimal reconfiguration of dynamic software product lines based on performance-influence models},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233030},
doi = {10.1145/3233027.3233030},
abstract = {Today's adaptive software systems (i) are often highly configurable product lines, exhibiting hundreds of potentially conflicting configuration options; (ii) are context dependent, forcing the system to reconfigure to ever-changing contextual situations at runtime; (iii) need to fulfill context-dependent performance goals by optimizing measurable nonfunctional properties. Usually, a large number of consistent configurations exists for a given context, and each consistent configuration may perform differently with regard to the current context and performance goal(s). Therefore, it is crucial to consider nonfunctional properties for identifying an appropriate configuration. Existing black-box approaches for estimating the performance of configurations provide no means for determining context-sensitive reconfiguration decisions at runtime that are both consistent and optimal, and hardly allow for combining multiple context-dependent quality goals. In this paper, we propose a comprehensive approach based on Dynamic Software Product Lines (DSPL) for obtaining consistent and optimal reconfiguration decisions. We use training data obtained from simulations to learn performance-influence models. A novel integrated runtime representation captures both consistency properties and the learned performance-influence models. Our solution provides the flexibility to define multiple context-dependent performance goals. We have implemented our approach as a standalone component. Based on an Internet-of-Things case study using adaptive wireless sensor networks, we evaluate our approach with regard to effectiveness, efficiency, and applicability.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {98–109},
numpages = {12},
keywords = {performance-influence models, machine learning, dynamic software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2791060.2791111,
author = {Cordy, Maxime and Davril, Jean-Marc and Greenyer, Joel and Gressi, Erika and Heymans, Patrick},
title = {All-at-once-synthesis of controllers from scenario-based product line specifications},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791111},
doi = {10.1145/2791060.2791111},
abstract = {Software-intensive systems often consist of multiple components that interact to realize complex requirements. An additional dimension of complexity arises when one designs many variants of a system at once, that is, a software product line (SPL). We propose a scenario-based approach to design SPLs, based on a combination of Modal Sequence Diagrams (MSDs) and a feature model. It consists in associating every MSD to the set of variants that have to satisfy its specification. Variability constitutes a new source of complexity, which can lead to inconsistencies in the specification of one or multiple variants. It is therefore crucial to detect these inconsistencies, and to produce a controller for each variant that makes it behave so that it satisfies its specification. We present a new controller synthesis technique that checks the absence of inconsistencies in all variants at once, thereby more radically exploiting the similarities between them. Our method first translates the MSD specification into a variability-aware B\"{u}chi game, and then solves this game for all variants in a single execution. We implemented the approach in ScenarioTools, a software tool which we use to evaluate our algorithms against competing methods.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {26–35},
numpages = {10},
keywords = {message sequence diagrams, features, controller synthesis},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and J\'{e}z\'{e}quel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3106195.3106205,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green Configurations of Functional Quality Attributes},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106205},
doi = {10.1145/3106195.3106205},
abstract = {Functional quality attributes (FQAs) are those quality attributes that, to be satisfied, require the incorporation of additional functionality into the application architecture. By adding an FQA (e.g., security) we can improve the quality of the final product, but there is also an increase in energy consumption. This paper proposes a solution to help the software architect to generate configurations of FQAs whilst keeping the energy consumed by the application as low as possible. For this, a usage model is defined for each FQA, taking into account the variables that affect the energy consumption, and that the values of these variables change according to the part of the application where the FQA is required. We extend a Software Product Line that models a family of FQAs to incorporate the variability of the usage model and the existing frameworks that implement FQAs. We generate the most eco-efficient configuration of FQAs by selecting the framework with the most suitable characteristics according to the requirements of the application.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {79–83},
numpages = {5},
keywords = {Variability, SPL, Quality Attributes, FQA, Energy Consumption},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3461002.3473066,
author = {Fortz, Sophie},
title = {LIFTS: learning featured transition systems},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473066},
doi = {10.1145/3461002.3473066},
abstract = {This PhD project aims to automatically learn transition systems capturing the behaviour of a whole family of software-based systems. Reasoning at the family level yields important economies of scale and quality improvements for a broad range of systems such as software product lines, adaptive and configurable systems. Yet, to fully benefit from the above advantages, a model of the system family's behaviour is necessary. Such a model is often prohibitively expensive to create manually due to the number of variants. For large long-lived systems with outdated specifications or for systems that continuously adapt, the modelling cost is even higher. Therefore, this PhD proposes to automate the learning of such models from existing artefacts. To advance research at a fundamental level, our learning target are Featured Transition Systems (FTS), an abstract formalism that can be used to provide a pivot semantics to a range of variability-aware state-based modelling languages. The main research questions addressed by this PhD project are: (1) Can we learn variability-aware models efficiently? (2) Can we learn FTS in a black-box fashion? (i.e., with access to execution logs but not to source code); (3) Can we learn FTS in a white/grey-box testing fashion? (i.e., with access to source code); and (4) How do the proposed techniques scale in practice?},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {1–6},
numpages = {6},
keywords = {variability mining, software product lines, model learning, featured transition systems, active automata learning},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3461001.3471146,
author = {Horcas, Jose-Miguel and Galindo, Jos\'{e} A. and Heradio, Ruben and Fernandez-Amoros, David and Benavides, David},
title = {Monte Carlo tree search for feature model analyses: a general framework for decision-making},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471146},
doi = {10.1145/3461001.3471146},
abstract = {The colossal solution spaces of most configurable systems make intractable their exhaustive exploration. Accordingly, relevant analyses remain open research problems. There exist analyses alternatives such as SAT solving or constraint programming. However, none of them have explored simulation-based methods. Monte Carlo-based decision making is a simulation-based method for dealing with colossal solution spaces using randomness. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo methods, which have proven to succeed in vast search spaces (e.g., game theory). Our general framework is described formally, and its flexibility to cope with a diversity of analysis problems is discussed (e.g., finding defective configurations, feature model reverse engineering or getting optimal performance configurations). Additionally, we present a Python implementation of the framework that shows the feasibility of our proposal. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state of the art a step forward.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {190–201},
numpages = {12},
keywords = {variability modeling, software product lines, monte carlo tree search, feature models, configurable systems},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2499777.2500723,
author = {Alsawalqah, Hamad and Kang, Sungwon and Lee, Danhyung},
title = {A method for software product platform design based on features},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500723},
doi = {10.1145/2499777.2500723},
abstract = {Due to the increased competition and the advent of mass customization, software firms are applying the Software Product Line Engineering (SPLE) approach to provide product variety in a cost-effective manner. Although the key to designing a successful software product family is the product platform, yet there is lack of measures and methods that are useful to optimize the product platform design. This paper proposes a method to provide decision support to determine the optimized product platform design. The method targets at identifying the optimized product platform design in order to maximize the cost savings and the amount of commonality while meeting the goals and needs of the envisioned customers' segments. It generates, validates, and evaluates alternative product platform designs while considering market concerns (e.g., customer preferences) and technical product platform concerns (e.g., decisions regarding shared features, economic benefit). We demonstrate its applicability with an example of platform design problem in smart phones domain.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {18–25},
numpages = {8},
keywords = {software product line, product platform design, commonality index, Kano scheme},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3461002.3473073,
author = {Pett, Tobias and Krieter, Sebastian and Th\"{u}m, Thomas and Lochau, Malte and Schaefer, Ina},
title = {AutoSMP: an evaluation platform for sampling algorithms},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473073},
doi = {10.1145/3461002.3473073},
abstract = {Testing configurable systems is a challenging task due to the combinatorial explosion problem. Sampling is a promising approach to reduce the testing effort for product-based systems by finding a small but still representative subset (i.e., a sample) of all configurations for testing. The quality of a generated sample wrt. evaluation criteria such as run time of sample generation, feature coverage, sample size, and sampling stability depends on the subject systems and the sampling algorithm. Choosing the right sampling algorithm for practical applications is challenging because each sampling algorithm fulfills the evaluation criteria to a different degree. Researchers keep developing new sampling algorithms with improved performance or unique properties to satisfy application-specific requirements. Comparing sampling algorithms is therefore a necessary task for researchers. However, this task needs a lot of effort because of missing accessibility of existing algorithm implementations and benchmarks. Our platform AutoSMP eases practitioners and researchers lifes by automatically executing sampling algorithms on predefined benchmarks and evaluating the sampling results wrt. specific user requirements. In this paper, we introduce the open-source application of AutoSMP and a set of predefined benchmarks as well as a set of T-wise sampling algorithms as examples.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {41–44},
numpages = {4},
keywords = {sampling evalutaion, sampling, product lines},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3336294.3336302,
author = {Str\"{u}ber, Daniel and Mukelabai, Mukelabai and Kr\"{u}ger, Jacob and Fischer, Stefan and Linsbauer, Lukas and Martinez, Jabier and Berger, Thorsten},
title = {Facing the Truth: Benchmarking the Techniques for the Evolution of Variant-Rich Systems},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336302},
doi = {10.1145/3336294.3336302},
abstract = {The evolution of variant-rich systems is a challenging task. To support developers, the research community has proposed a range of different techniques over the last decades. However, many techniques have not been adopted in practice so far. To advance such techniques and to support their adoption, it is crucial to evaluate them against realistic baselines, ideally in the form of generally accessible benchmarks. To this end, we need to improve our empirical understanding of typical evolution scenarios for variant-rich systems and their relevance for benchmarking. In this paper, we establish eleven evolution scenarios in which benchmarks would be beneficial. Our scenarios cover typical lifecycles of variant-rich system, ranging from clone &amp; own to adopting and evolving a configurable product-line platform. For each scenario, we formulate benchmarking requirements and assess its clarity and relevance via a survey with experts in variant-rich systems and software evolution. We also surveyed the existing benchmarking landscape, identifying synergies and gaps. We observed that most scenarios, despite being perceived as important by experts, are only partially or not at all supported by existing benchmarks-a call to arms for building community benchmarks upon our requirements. We hope that our work raises awareness for benchmarking as a means to advance techniques for evolving variant-rich systems, and that it will lead to a benchmarking initiative in our community.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {177–188},
numpages = {12},
keywords = {software variability, software evolution, product lines, benchmark},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3461001.3471149,
author = {Lesoil, Luc and Acher, Mathieu and T\'{e}rnava, Xhevahire and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {The interplay of compile-time and run-time options for performance prediction},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471149},
doi = {10.1145/3461001.3471149},
abstract = {Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {100–111},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2934466.2934478,
author = {Galindo, Jos\'{e} A. and Acher, Mathieu and Tirado, Juan Manuel and Vidal, Cristian and Baudry, Benoit and Benavides, David},
title = {Exploiting the enumeration of all feature model configurations: a new perspective with distributed computing},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934478},
doi = {10.1145/2934466.2934478},
abstract = {Feature models are widely used to encode the configurations of a software product line in terms of mandatory, optional and exclusive features as well as propositional constraints over the features. Numerous computationally expensive procedures have been developed to model check, test, configure, debug, or compute relevant information of feature models. In this paper we explore the possible improvement of relying on the enumeration of all configurations when performing automated analysis operations. We tackle the challenge of how to scale the existing enumeration techniques by relying on distributed computing. We show that the use of distributed computing techniques might offer practical solutions to previously unsolvable problems and opens new perspectives for the automated analysis of software product lines.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {74–78},
numpages = {5},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3336294.3336297,
author = {Munoz, Daniel-Jesus and Oh, Jeho and Pinto, M\'{o}nica and Fuentes, Lidia and Batory, Don},
title = {Uniform Random Sampling Product Configurations of Feature Models That Have Numerical Features},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336297},
doi = {10.1145/3336294.3336297},
abstract = {Analyses of Software Product Lines (SPLs) rely on automated solvers to navigate complex dependencies among features and find legal configurations. Often these analyses do not support numerical features with constraints because propositional formulas use only Boolean variables. Some automated solvers can represent numerical features natively, but are limited in their ability to count and Uniform Random Sample (URS) configurations, which are key operations to derive unbiased statistics on configuration spaces.Bit-blasting is a technique to encode numerical constraints as propositional formulas. We use bit-blasting to encode Boolean and numerical constraints so that we can exploit existing #SAT solvers to count and URS configurations. Compared to state-of-art Satisfiability Modulo Theory and Constraint Programming solvers, our approach has two advantages: 1) faster and more scalable configuration counting and 2) reliable URS of SPL configurations. We also show that our work can be used to extend prior SAT-based SPL analyses to support numerical features and constraints.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {289–301},
numpages = {13},
keywords = {software product lines, propositional formula, numerical features, model counting, feature model, bit-blasting},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3233027.3233046,
author = {Beek, Maurice H. ter and Fantechi, Alessandro and Gnesi, Stefania},
title = {Product line models of large cyber-physical systems: the case of ERTMS/ETCS},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233046},
doi = {10.1145/3233027.3233046},
abstract = {A product line perspective may help to understand the possible variants in interactions between the subsystems of a large, cyber-physical system. This observation is exemplified in this paper by proposing a feature model of the family of ERTMS/ETCS train control systems and their foreseen extensions. This model not only shows the different components that have to be installed when deploying the system at the different levels established by the ERTMS/ETCS standards, but it also helps to identify and discuss specific issues, such as the borders between onboard and wayside equipment, different manufacturers of the subsystems, interoperability among systems developed at different levels, backward compatibility of trains equipped with higher level equipment running on lines equipped with lower level equipment, and evolution towards future trends of railway signalling. The feature model forms the basis for formal modelling of the behaviour of the critical components of the system and for evaluating the overall cost, effectiveness and sustainability, for example by adding cost and performance attributes to the feature model.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {208–214},
numpages = {7},
keywords = {variability, product lines, feature models, cyber-physical systems, ERTMS/ETCS train control systems},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3307630.3342411,
author = {Meixner, Kristof and Rabiser, Rick and Biffl, Stefan},
title = {Towards Modeling Variability of Products, Processes and Resources in Cyber-Physical Production Systems Engineering},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342411},
doi = {10.1145/3307630.3342411},
abstract = {Planning and developing Cyber-Physical Production Systems (CPPS) are multi-disciplinary engineering activities that rely on effective and efficient knowledge exchange for better collaboration between engineers of different disciplines. The Product-Process-Resource (PPR) approach allows modeling products produced by industrial processes using specific production resources. In practice, a CPPS manufactures a portfolio of product type variants, i.e., a product line. Therefore, engineers need to create and maintain several PPR models to cover PPR variants and their evolving versions. In this paper, we detail a representative use case, identify challenges for using Variability Modeling (VM) methods to describe and manage PPR variants, and present a first solution approach based on cooperation with domain experts at an industry partner, a system integrator of automation for high-performance CPPS. We conclude that integrating basic variability concepts into PPR models is a promising first step and describe our further research plans to support PPR VM in CPPS.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {49–56},
numpages = {8},
keywords = {variability modelling, product-process-resource, cyber-physical production system},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2491627.2491630,
author = {Linsbauer, Lukas and Lopez-Herrejon, E. Roberto and Egyed, Alexander},
title = {Recovering traceability between features and code in product variants},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491630},
doi = {10.1145/2491627.2491630},
abstract = {Many companies offer a palette of similar software products though they do not necessarily have a Software Product Line (SPL). Rather, they start building and selling individual products which they then adapt, customize and extend for different customers. As the number of product variants increases, these companies then face the severe problem of having to maintain them all. Software Product Lines can be helpful here - not so much as a platform for creating new products but as a means of maintaining the existing ones with their shared features. Here, an important first step is to determine where features are implemented in the source code and in what product variants. To this end, this paper presents a novel technique for deriving the traceability between features and code in product variants by matching code overlaps and feature overlaps. This is a difficult problem because a feature's implementation not only covers its basic functionality (which does not change across product variants) but may include code that deals with feature interaction issues and thus changes depending on the combination of features present in a product variant. We empirically evaluated the approach on three non-trivial case studies of different sizes and domains and found that our approach correctly identifies feature to code traces except for code that traces to multiple disjunctive features, a rare case involving less than 1% of the code.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {131–140},
numpages = {10},
keywords = {traceability, product variants, features},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2791060.2791108,
author = {Berger, Thorsten and Lettner, Daniela and Rubin, Julia and Gr\"{u}nbacher, Paul and Silva, Adeline and Becker, Martin and Chechik, Marsha and Czarnecki, Krzysztof},
title = {What is a feature? a qualitative study of features in industrial software product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791108},
doi = {10.1145/2791060.2791108},
abstract = {The notion of features is commonly used to describe the functional and non-functional characteristics of a system. In software product line engineering, features often become the prime entities of software reuse and are used to distinguish the individual products of a product line. Properly decomposing a product line into features, and correctly using features in all engineering phases, is core to the immediate and long-term success of such a system. Yet, although more than ten different definitions of the term feature exist, it is still a very abstract concept. Definitions lack concrete guidelines on how to use the notion of features in practice.To address this gap, we present a qualitative empirical study on actual feature usage in industry. Our study covers three large companies and an in-depth, contextualized analysis of 23 features, perceived by the interviewees as typical, atypical (outlier), good, or bad representatives of features. Using structured interviews, we investigate the rationales that lead to a feature's perception, and identify and analyze core characteristics (facets) of these features. Among others, we find that good features precisely describe customer-relevant functionality, while bad features primarily arise from rashly executed processes. Outlier features, serving unusual purposes, are necessary, but do not require the full engineering process of typical features.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {16–25},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3233027.3233039,
author = {Pereira, Juliana Alves and Schulze, Sandro and Figueiredo, Eduardo and Saake, Gunter},
title = {N-dimensional tensor factorization for self-configuration of software product lines at runtime},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233039},
doi = {10.1145/3233027.3233039},
abstract = {Dynamic software product lines demand self-adaptation of their behavior to deal with runtime contextual changes in their environment and offer a personalized product to the user. However, taking user preferences and context into account impedes the manual configuration process, and thus, an efficient and automated procedure is required. To automate the configuration process, context-aware recommendation techniques have been acknowledged as an effective mean to provide suggestions to a user based on their recognized context. In this work, we propose a collaborative filtering method based on tensor factorization that allows an integration of contextual data by modeling an N-dimensional tensor User-Feature-Context instead of the traditional two-dimensional User-Feature matrix. In the proposed approach, different types of non-functional properties are considered as additional contextual dimensions. Moreover, we show how to self-configure software product lines by applying our N-dimensional tensor factorization recommendation approach. We evaluate our approach by means of an empirical study using two datasets of configurations derived for medium-sized product lines. Our results reveal significant improvements in the predictive accuracy of the configuration over a state-of-the-art non-contextual matrix factorization approach. Moreover, it can scale up to a 7-dimensional tensor containing hundred of configurations in a couple of milliseconds.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {87–97},
numpages = {11},
keywords = {software product lines, self-configuration, runtime decision-making, recommender systems},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2019136.2019187,
author = {Abbas, Nadeem},
title = {Towards autonomic software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019187},
doi = {10.1145/2019136.2019187},
abstract = {We envision an Autonomic Software Product Line (ASPL). The ASPL is a dynamic software product line that supports self adaptable products. We plan to use reflective architecture to model and develop ASPL. To evaluate the approach, we have implemented three autonomic product lines which show promising results. The ASPL approach is at initial stages, and require additional work. We plan to exploit online learning to realize more dynamic software product lines to cope with the problem of product line evolution. We propose on-line knowledge sharing among products in a product line to achieve continuous improvement of quality in product line products.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {44},
numpages = {8},
keywords = {self-adaptation, on-line learning, knowledge},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3106195.3106202,
author = {Wille, David and Wehling, Kenny and Seidl, Christoph and Pluchator, Martin and Schaefer, Ina},
title = {Variability Mining of Technical Architectures},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106202},
doi = {10.1145/3106195.3106202},
abstract = {Technical architectures (TAs) represent the computing infrastructure of a company with all its hardware and software components. Over the course of time, the number of TAs grows with the companies' requirements and usually a large variety of TAs has to be maintained. Core challenge is the missing information on relations between the existing variants of TAs, which complicates reuse of solutions across systems. However, identifying these relations is an expensive task as architects have to manually analyze each TA individually. Restructuring the existing TAs poses severe risks as often sufficient information is not available (e.g., due to time constraints). To avoid failures in productive systems and resulting loss of profit, companies continue to create new solutions without restructuring existing ones. This increased variability in TAs represents technical debt. In this paper, we adapt the idea of variability mining from the software product line domain and present an efficient and automatic mining algorithm to identify the common and varying parts of TAs by analyzing a potentially arbitrary number of TAs in parallel. Using the identified variability information, architects are capable of analyzing the relations of TAs, identifying reuse potential, and making well-founded maintenance decisions. We show the feasibility and scalability of our approach by applying it to a real-world industrial case study with large sets of TAs.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {39–48},
numpages = {10},
keywords = {variability mining, technical architecture, enterprise architecture},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3236405.3236407,
author = {Ghofrani, Javad and Fehlhaber, Anna Lena},
title = {ProductlinRE: online management tool for requirements engineering of software product lines},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236407},
doi = {10.1145/3236405.3236407},
abstract = {The lack of online tools for managing various artifacts of software product lines is problematic, and stands in contradiction to findings about the need to support collaboration. In this paper, we present ProductLinRE, a web application allowing product line engineers to work cooperatively on artifacts of requirements engineering for software product lines. Our proposed online tool allows distributed teamwork, using a tracking mechanism for projects, artifacts and features while tailoring the requirements artifacts according to the selected features.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {17–22},
numpages = {6},
keywords = {software product lines, requirements engineering, online tools},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3382025.3414954,
author = {Michelon, Gabriela Karoline and Obermann, David and Linsbauer, Lukas and Assun\c{c}\~{a}o, Wesley Klewerton G. and Gr\"{u}nbacher, Paul and Egyed, Alexander},
title = {Locating feature revisions in software systems evolving in space and time},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414954},
doi = {10.1145/3382025.3414954},
abstract = {Software companies encounter variability in space as variants of software systems need to be produced for different customers. At the same time, companies need to handle evolution in time because the customized variants need to be revised and kept up-to-date. This leads to a predicament in practice with many system variants significantly diverging from each other. Maintaining these variants consistently is difficult, as they diverge across space, i.e., different feature combinations, and over time, i.e., revisions of features. This work presents an automated feature revision location technique that traces feature revisions to their implementation. To assess the correctness of our technique, we used variants and revisions from three open source highly configurable software systems. In particular, we compared the original artifacts of the variants with the composed artifacts that were located by our technique. The results show that our technique can properly trace feature revisions to their implementation, reaching traces with 100% precision and 98% recall on average for the three analyzed subject systems, taking on average around 50 seconds for locating feature revisions per variant used as input.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {14},
numpages = {11},
keywords = {variants, repository mining, feature revisions, feature location},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3233027.3236399,
author = {Kuiter, Elias and Krieter, Sebastian and Kr\"{u}ger, Jacob and Ludwig, Kai and Leich, Thomas and Saake, Gunter},
title = {PClocator: a tool suite to automatically identify configurations for code locations},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236399},
doi = {10.1145/3233027.3236399},
abstract = {The source code of highly-configurable software is challenging to comprehend, analyze, and test. In particular, it is hard to identify all configurations that comprise a certain code location. We contribute PCLocator, a tool suite that solves this problem by utilizing static analysis tools for compile-time variability. Using BusyBox and the Variability Bugs Database (VBDb), we evaluate the correctness and performance of PCLocator. The results show that we are able to analyze files in a matter of seconds and derive correct configurations in 95% of all cases.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {284–288},
numpages = {5},
keywords = {static source code analysis, software product line, preprocessor, configuration, build system},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2362536.2362567,
author = {Savolainen, Juha and Mannion, Mike and Kuusela, Juha},
title = {Developing platforms for multiple software product lines},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362567},
doi = {10.1145/2362536.2362567},
abstract = {Many approaches to software product line engineering have been founded on the development of a single product line platform. However as customer requirements change and new products are added to the product line, software producers recognize that the platform cannot be "stretched" indefinitely and a significant problem is striking a balance between development efficiency by increasing platform commonality and customer dissatisfaction from products with additional undesirable features and properties.One alternative is to develop multiple product lines (MPLs). However the challenge remains about what to include in a multiple product line platform. Drawing upon industrial experience of working with 4 companies, this paper explores the characteristics of the contexts in which MPLs are a viable alternative development strategy and then proposes a framework of approaches to platform development.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {220–228},
numpages = {9},
keywords = {software reuse, multiple product lines, industrial experience},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3307630.3342410,
author = {Rosiak, Kamil and Urbaniak, Oliver and Schlie, Alexander and Seidl, Christoph and Schaefer, Ina},
title = {Analyzing Variability in 25 Years of Industrial Legacy Software: An Experience Report},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342410},
doi = {10.1145/3307630.3342410},
abstract = {In certain domains, safety-critical software systems may remain operational for decades. To comply with changing requirements, new system variants are commonly created by copying and modifying existing ones. Typically denoted clone-and-own, software quality and overall maintainability are adversely affected in the long-run. With safety being pivotal, a fault in one variant may require the entire portfolio to be assessed. Thus, engineers need to maintain legacy systems dating back decades, implemented in programming languages such as Pascal. Software product lines (SPLs) can be a remedy but migrating legacy systems requires their prior analysis and comparison. For industrial software systems, this remains a challenge.In this paper, we introduce a comparison procedure and customizable metrics to allow for a fine-grained comparison of Pascal modules to the level of individual expressions. By that, we identify common parts of while also capturing different parts between modules as a basis for a transition towards anSPLs practice. Moreover, we demonstrate the feasibility of our approach using a case study with seven Pascal modules totaling 13,271 lines of code with an evolution-history of 25 years and show our procedure to be fast and precise. Furthermore, we elaborate on the case study and detail peculiarities of the Pascal modules, which are characteristic for an evolution-history of a quarter century.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {65–72},
numpages = {8},
keywords = {variability, software prodct line, legacy software, clone-and-own},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3550355.3552411,
author = {Hentze, Marc and Sundermann, Chico and Th\"{u}m, Thomas and Schaefer, Ina},
title = {Quantifying the variability mismatch between problem and solution space},
year = {2022},
isbn = {9781450394666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550355.3552411},
doi = {10.1145/3550355.3552411},
abstract = {A software product line allows to derive individual software products based on a configuration. As the number of configurations is an indicator for the general complexity of a software product line, automatic #SAT analyses have been proposed to provide this information. However, the number of configurations does not need to match the number of derivable products. Due to this mismatch, using the number of configurations to reason about the software complexity (i.e., the number of derivable products) of a software product line can lead to wrong assumptions during implementation and testing. How to compute the actual number of derivable products, however, is unknown. In this paper, we mitigate this problem and present a concept to derive a solution-space feature model which allows to reuse existing #SAT analyses for computing the number of derivable products of a software product line. We apply our concept to a total of 119 subsystems of three industrial software product lines. The results show that the derivation scales for real world software product lines and confirm the mismatch between the number of configurations and the number of products.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems},
pages = {322–333},
numpages = {12},
keywords = {variability mismatch, solution-space analyses, product lines},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/2647908.2655972,
author = {Meinicke, Jens and Th\"{u}m, Thomas and Schr\"{o}ter, Reimar and Benduhn, Fabian and Saake, Gunter},
title = {An overview on analysis tools for software product lines},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655972},
doi = {10.1145/2647908.2655972},
abstract = {A software product line is a set of different software products that share commonalities. For a selection of features, specialized products of one domain can be generated automatically from domain artifacts. However, analyses of software product lines need to handle a large number of products that can be exponential in the number of features. In the last decade, many approaches have been proposed to analyze software product lines efficiently. For some of these approaches tool support is available. Based on a recent survey on analysis for software product lines, we provide a first overview on such tools. While our discussion is limited to analysis tools, we provide an accompanying website covering further tools for product-line development. We compare tools according to their analysis and implementation strategy to identify underrepresented areas. In addition, we want to ease the reuse of existing tools for researchers and students, and to simplify research transfer to practice.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {94–101},
numpages = {8},
keywords = {type checking, tool support, theorem proving, testing, static analysis, software product lines, sampling, non-functional properties, model checking, code metrics},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3307630.3342408,
author = {Schlie, Alexander and Rosiak, Kamil and Urbaniak, Oliver and Schaefer, Ina and Vogel-Heuser, Birgit},
title = {Analyzing Variability in Automation Software with the Variability Analysis Toolkit},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342408},
doi = {10.1145/3307630.3342408},
abstract = {Control software for automated production systems (aPs) becomes increasingly complex as it evolves due to changing requirements. To address varying customer demands or altered regulatory guidelines, it is common practice to create a new system variant by copying and subsequently modifying existing control software. Referred to as clone-and-own, proper documentation is typically not cherished, thereby entailing severe maintenance issues in the long-run. To mitigate such problems and to reinstate sustainable development, respective software systems need to be compared and their variability information needs to be reverse-engineered. However, recent work identified variability management in the domain of aPs to remain a challenging endevour and appropriate tool support to be missing.We bridge this gap and introduce the Variability Analysis Toolkit (VAT), an extensible platform that allows for the customizable definition of metrics to compare IEC61131-3 control software variants as well as providing means to visualize results. The VAT facilitates a working environment that allows for the exchange of produced results between users. By that, we aim to support engineers in re-engineering control software systems by providing them with means to define metrics based on their individual demands. We demonstrate the feasibility of the VAT using 24 software system variants implemented in accordance to the IEC61131-3 standard.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {191–198},
numpages = {8},
keywords = {variability, tooling, software product lines, legacy systems, automation software},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2934466.2934491,
author = {Fogdal, Thomas and Scherrebeck, Helene and Kuusela, Juha and Becker, Martin and Zhang, Bo},
title = {Ten years of product line engineering at Danfoss: lessons learned and way ahead},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934491},
doi = {10.1145/2934466.2934491},
abstract = {Software and systems product line engineering (PLE) has been an established approach for reducing time to market as well as cost and increasing quality in a set of related products for two decades now. Although there is a huge body of knowledge on PLE, adopting a concrete PLE approach is still not a trivial endeavor for interested companies. With the increasing importance of development speed, the advent of agile engineering approaches, and decreasing management interest in improvements that require large organizational transformations and only show benefits after several years, companies are facing challenges in successfully adopting this approach. They often hesitate as there is no clear adoption path, nor any certainty, that the intended improvement steps will also provide added value in the short- and mid-term perspective. In consequence, a considerable amount of PLE potential still remains unexploited.To help such companies with the adoption of PLE, the goal of this paper is to provide inspiration and evidence that PLE is a sound approach and its successful introduction is possible even in settings that differ substantially from those of pioneer product lines.To this end, this paper presents the following main contributions with the PLE adoption case at Danfoss Drives: an overview of the key change drivers and the motivation for adopting a PLE approach, a discussion of incremental PLE introduction in an agile engineering context, a presentation of the current PLE setting with a focus on key concepts, and finally a presentation of motivators and directions for future improvements.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {252–261},
numpages = {10},
keywords = {product line evaluation, product line adoption, industrial experiences},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2934466.2934474,
author = {Myll\"{a}rniemi, Varvana and Raatikainen, Mikko and Savolainen, Juha and M\"{a}nnist\"{o}, Tomi},
title = {Purposeful performance variability in software product lines: a comparison of two case studies},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934474},
doi = {10.1145/2934466.2934474},
abstract = {Within software product lines, customers may have different quality needs. To produce products with purposefully different quality attributes, several challenges must be addressed. First, one must be able to distinguish product quality attributes to the customers in a meaningful way. Second, one must create the desired quality attribute differences during product-line architecture design and derivation. To study how performance is varied purposefully in software product lines, we conducted a comparison and re-analysis of two industrial case studies in the telecommunication and mobile game domains. The results show that performance variants must be communicated to the customer in a way that links to customer value and her role. When performance or its adaptation are crucial for the customer, performance differences must be explicitly "designed in" with software or hardware means. Due to the emergent nature of performance, it is important to test performance and manage how other variability affects performance.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {144–153},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3336294.3342358,
author = {M\"{u}ller, Richard and Eisenecker, Ulrich},
title = {A Graph-Based Feature Location Approach Using Set Theory},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342358},
doi = {10.1145/3336294.3342358},
abstract = {The ArgoUML SPL benchmark addresses feature location in Software Product Lines (SPLs), where single features as well as feature combinations and feature negations have to be identified. We present a solution for this challenge using a graph-based approach and set theory. The results are promising. Set theory allows to exactly define which parts of feature locations can be computed and which precision and which recall can be achieved. This has to be complemented by a reliable identification of feature-dependent class and method traces as well as refinements. The application of our solution to one scenario of the benchmark supports this claim.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {88–92},
numpages = {5},
keywords = {static analysis, software product lines, set theory, reverse engineering, jQAssistant, graph database, feature location, extractive software product line adoption, cypher, benchmark, Neo4j, ArgoUML},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2362536.2362549,
author = {Cordy, Maxime and Schobbens, Pierre-Yves and Heymans, Patrick and Legay, Axel},
title = {Behavioural modelling and verification of real-time software product lines},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362549},
doi = {10.1145/2362536.2362549},
abstract = {In Software Product Line (SPL) engineering, software products are build in families rather than individually. Many critical software are nowadays build as SPLs and most of them obey hard real-time requirements. Formal methods for verifying SPLs are thus crucial and actively studied. The verification problem for SPL is, however, more complicated than for individual systems; the large number of different software products multiplies the complexity of SPL model-checking. Recently, promising model-checking approaches have been developed specifically for SPLs. They leverage the commonality between the products to reduce the verification effort. However, none of them considers real time.In this paper, we combine existing SPL verification methods with established model-checking procedures for real-time systems. We introduce Featured Timed Automata (FTA), a formalism that extends the classical Timed Automata with constructs for modelling variability. We show that FTA model-checking can be achieved through a smart combination of real-time and SPL model checking.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {66–75},
numpages = {10},
keywords = {software product lines, real-time, model checking, features},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3382025.3414965,
author = {Young, Jeffrey M. and Walkingshaw, Eric and Th\"{u}m, Thomas},
title = {Variational satisfiability solving},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414965},
doi = {10.1145/3382025.3414965},
abstract = {Incremental satisfiability (SAT) solving is an extension of classic SAT solving that allows users to efficiently solve a set of related SAT problems by identifying and exploiting shared terms. However, using incremental solvers effectively is hard since performance is sensitive to a problem's structure and the order sub-terms are fed to the solver, and the burden to track results is placed on the end user. For analyses that generate sets of related SAT problems, such as those in software product lines, incremental SAT solvers are either not used at all, used but not explicitly stated so in the literature, or used but suffer from the aforementioned usability problems. This paper translates the ordering problem to an encoding problem and automates the use of incremental SAT solving. We introduce variational SAT solving, which differs from incremental SAT solving by accepting all related problems as a single variational input and returning all results as a single variational output. Our central idea is to make explicit the operations of incremental SAT solving, thereby encoding differences between related SAT problems as local points of variation. Our approach automates the interaction with the incremental solver and enables methods to automatically optimize sharing of the input. To evaluate our methods we construct a prototype variational SAT solver and perform an empirical analysis on two real-world datasets that applied incremental solvers to software evolution scenarios. We show, assuming a variational input, that the prototype solver scales better for these problems than naive incremental solving while also removing the need to track individual results.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {18},
numpages = {12},
keywords = {variation, software product lines, satisfiability solving, choice calculus},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2647908.2655973,
author = {Cordy, Maxime and Willemart, Marco and Dawagne, Bruno and Heymans, Patrick and Schobbens, Pierre-Yves},
title = {An extensible platform for product-line behavioural analysis},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655973},
doi = {10.1145/2647908.2655973},
abstract = {Software Product-Line (SPL) model checking has reached an adequate level of efficiency and expressiveness to be applied on real-world cases. Yet a major challenge remains: model checkers should consist of black-box tools that do not require in-depth expertise to be used. In particular, it is essential to provide engineers with easy-to-learn languages to model both the behaviour of their SPL and the properties to check. In this paper, we propose a framework to build customized product-line verifiers modularly. Our extensible architecture allows one to plug new modelling languages or verifications algorithms without modifying other parts of it. It also provides means of representing and reasoning on variability that can facilitate the development of other SPL quality assurance techniques. We illustrate the benefits of our approach by detailing how we created a new domain-specific SPL modelling language and linked it to our tool.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {102–109},
numpages = {8},
keywords = {tool, software product lines, model checking, features},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2791060.2791080,
author = {Van Landuyt, Dimitri and Walraven, Stefan and Joosen, Wouter},
title = {Variability middleware for multi-tenant SaaS applications: a research roadmap for service lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791080},
doi = {10.1145/2791060.2791080},
abstract = {Software product line engineering (SPLE) and variability enforcement techniques have been applied to run-time adaptive systems for quite some years, also in the context of multi-tenant Software-as-a-Service (SaaS) applications. The focus has been mainly on (1) the pre-deployment phases of the development life cycle and (2) fine-grained (tenant-level), run-time activation of specific variants. However, with upcoming trends such as DevOps and continuous delivery and deployment, operational aspects become increasingly important.In this paper, we present our integrated vision on the positive interplay between SPLE and adaptive middleware for multi-tenant SaaS applications, focusing on the operational aspects of running and maintaining a successful SaaS offering. This vision, called Service Lines, is based on and motivated by our experience and frequent interactions with a number of Belgian SaaS providers.We concretely highlight and motivate a number of operational use cases that require advanced variability support in middleware and have promising added value for the economic feasibility of SaaS offerings. In addition, we provide a gap analysis of what is currently lacking from the perspectives of variability modeling and management techniques and middleware support, and as such sketch a concrete roadmap for continued research in this area.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {211–215},
numpages = {5},
keywords = {variability middleware, service lines, run-time variability, operational support, multi-tenant SaaS, models at run time},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2362536.2362562,
author = {Mohalik, Swarup and Ramesh, S. and Millo, Jean-Vivien and Krishna, Shankara Narayanan and Narwane, Ganesh Khandu},
title = {Tracing SPLs precisely and efficiently},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362562},
doi = {10.1145/2362536.2362562},
abstract = {In a Software Product Line (SPL), the central notion of implementability provides the requisite connection between specifications (feature sets) and their implementations (component sets), leading to the definition of products. While it appears to be a simple extension (to sets) of the trace-ability relation between components and features, it actually involves several subtle issues which are overlooked in the definitions in existing literature. In this paper, we give a precise and formal definition of implementability over a fairly expressive traceability relation to solve these issues. The consequent definition of products in the given SPL naturally entails a set of useful analysis problems that are either refinements of known problems, or are completely novel. We also propose a new approach to solve these analysis problems by encoding them as Quantified Boolean Formula(QBF) and solving them through Quantified Satisfiability (QSAT) solvers. The methodology scales much better than the SAT-based solutions hinted in the literature and is demonstrated through a prototype tool called SPLANE (SPL Analysis Engine), on a couple of fairly large case studies.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {186–195},
numpages = {10},
keywords = {software product line, formal methods, feature model, QSAT},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2648511.2648533,
author = {Simidchieva, Borislava I. and Osterweil, Leon J.},
title = {Generation, composition, and verification of families of human-intensive systems},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648533},
doi = {10.1145/2648511.2648533},
abstract = {Software products are rarely developed without providing different sets of features to better meet varying user needs, whether through tiered products as part of a product line or different subscription levels for software as a service (SaaS). Software product line approaches for generating and maintaining a family of different variants of software products address such needs for variation quite well. Real-world human-intensive systems (HISs) display similar needs for families of variants. A key contribution of this paper is to show how many of these needs can be rigorously and systematically addressed by adapting established techniques from system and software product line engineering (SPLE).In this paper, we present an approach for creating such families by explicitly modeling variation in HISs. We focus on two kinds of variation we have previously described in other work---functional detail variation and service variation. We describe a prototype system that is able to meet the need for these kinds of variation within an existing modeling framework and present a case study of the application of our prototype system to generate a family in an HIS from the domain of elections. Our approach also demonstrates how to perform model-checking of this family to discover whether any variants in the family may violate specified system requirements.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {207–216},
numpages = {10},
keywords = {system variation, software product lines, process families},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2019136.2019177,
author = {Abbas, Nadeem and Andersson, Jesper and Weyns, Danny},
title = {Knowledge evolution in autonomic software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019177},
doi = {10.1145/2019136.2019177},
abstract = {We describe ongoing work in knowledge evolution management for autonomic software product lines. We explore how an autonomic product line may benefit from new knowledge originating from different source activities and artifacts at run time. The motivation for sharing run-time knowledge is that products may self-optimize at run time and thus improve quality faster compared to traditional software product line evolution. We propose two mechanisms that support knowledge evolution in product lines: online learning and knowledge sharing. We describe two basic scenarios for runtime knowledge evolution that involves these mechanisms. We evaluate online learning and knowledge sharing in a small product line setting that shows promising results.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {36},
numpages = {8},
keywords = {software product-lines, software design, self-adaptation, product-line management, online learning, knowledge sharing},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3382025.3414951,
author = {Heradio, Ruben and Fernandez-Amoros, David and Galindo, Jos\'{e} A. and Benavides, David},
title = {Uniform and scalable SAT-sampling for configurable systems},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414951},
doi = {10.1145/3382025.3414951},
abstract = {Several relevant analyses on configurable software systems remain intractable because they require examining vast and highly-constrained configuration spaces. Those analyses could be addressed through statistical inference, i.e., working with a much more tractable sample that later supports generalizing the results obtained to the entire configuration space. To make this possible, the laws of statistical inference impose an indispensable requirement: each member of the population must be equally likely to be included in the sample, i.e., the sampling process needs to be "uniform". Various SAT-samplers have been developed for generating uniform random samples at a reasonable computational cost. Unfortunately, there is a lack of experimental validation over large configuration models to show whether the samplers indeed produce genuine uniform samples or not. This paper (i) presents a new statistical test to verify to what extent samplers accomplish uniformity and (ii) reports the evaluation of four state-of-the-art samplers: Spur, QuickSampler, Unigen2, and Smarch. According to our experimental results, only Spur satisfies both scalability and uniformity.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {17},
numpages = {11},
keywords = {variability modeling, uniform sampling, software product lines, configurable systems, SAT},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2019136.2019158,
author = {Guana, Victor and Correal, Dario},
title = {Variability quality evaluation on component-based software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019158},
doi = {10.1145/2019136.2019158},
abstract = {Quality assurance and evaluation in Model Driven Software Product Lines (MD-SPLs) are pivotal points for the growing and solidification of the generative software factories. They are framed as one of the future fact methodologies for the construction of software systems. Although several approximations address the problem of generative environments, software product line scope expression, and core asset definition, not many of them try to solve, as a fundamental step, the automation of the quality attribute evaluation in the MD-SPL development cycle. This paper presents a model-driven engineering method and a tool for the quality evaluation of product line configurations through a cross architectural view analysis.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {19},
numpages = {8},
keywords = {sensitivity point, quality attribute, model-driven software product line, model composition, domain specific modeling},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3489849.3489948,
author = {Lebiedz, Jacek and Wiszniewski, Bogdan},
title = {CAVE applications: from craft manufacturing to product line engineering},
year = {2021},
isbn = {9781450390927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489849.3489948},
doi = {10.1145/3489849.3489948},
abstract = {Product line engineering model is suitable for engineering related software products in an efficient manner, taking advantage of their similarities while managing their differences. Our feature driven software product line (SPL) solution based on that model allows for instantiation of different CAVE products based on the set of core assets and driven by a set of common VR features with the minimal budget and time to market.},
booktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},
articleno = {57},
numpages = {2},
keywords = {production stations, core assets, VR application features},
location = {Osaka, Japan},
series = {VRST '21}
}

@inproceedings{10.1145/3336294.3342383,
author = {Martin, Hugo and Pereira, Juliana Alves and Acher, Mathieu and Temple, Paul},
title = {Machine Learning and Configurable Systems: A Gentle Introduction},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342383},
doi = {10.1145/3336294.3342383},
abstract = {The goal of this tutorial is to give an introduction to how machine learning can be used to support activities related to the engineering of configurable systems and software product lines. To the best of our knowledge, this is the first practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance prediction) on real-world systems (VaryLaTeX, x264).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {325–326},
numpages = {2},
keywords = {software product lines, machine learning, configurable systems},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2364412.2364439,
author = {Vale, Tassio and Figueiredo, Gustavo Bittencourt and de Almeida, Eduardo Santana and de Lemos Meira, Silvio Romero},
title = {A study on service identification methods for software product lines},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364439},
doi = {10.1145/2364412.2364439},
abstract = {The combination of service-orientation and software product line engineering, called Service-Oriented Product Line Engineering (SOPLE) have received attention by researchers and practitioners in the last years, and these areas can address issues of each other. One service-orientation issue is service identification. It consists of determining candidate services to a service-oriented environment based on pre-existing software artifacts, e.g., business process, source code, and so on. In order to provide a systematic identification of services, there are many available service identification methods in the literature, regarding different understanding of services, goals, and techniques. Due to this heterogeneity, this paper presents an in-depth comparison of service identification methods as well as a recommendation of the most suitable ones in the SOPLE context. This work can help the decision making of the most suitable method according to stakeholders' needs.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {156–163},
numpages = {8},
keywords = {software product lines, service-oriented product lines, service-oriented computing, service identification},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2491627.2491636,
author = {Kato, Tadahisa and Kawakami, Masumi and Myojin, Tomoyuki and Ogawa, Hideto and Hirono, Koji and Hasegawa, Takashi},
title = {Case study of applying SPLE to development of network switch products},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491636},
doi = {10.1145/2491627.2491636},
abstract = {Software product line engineering has spread as a technique for promoting the efficient development of embedded products with many product line-ups. During the development of network switch products at Hitachi Metals, Ltd., the number of development man-months increased as the number of product line-ups increased. Therefore, we shifted our development paradigm to product line development for efficient product development. We classified software assets as implementation assets, test assets, and design assets, and from these three assets, we extracted common objects and integrated them as reusable elements. By doing so, we promoted the efficient development of software assets and reduced the contradictions between the contents of the software assets. As a result, we reduced the amount of the source code by 53.1%. In this paper, we discuss the details of our technique and the effect of applying it. In addition, we discuss how you can apply our technique in the development of other products.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {198–207},
numpages = {10},
keywords = {test automation, software reuse, software maintenance, software integration, document integration},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3106195.3106210,
author = {Markiegi, Urtzi and Arrieta, Aitor and Sagardui, Goiuria and Etxeberria, Leire},
title = {Search-based product line fault detection allocating test cases iteratively},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106210},
doi = {10.1145/3106195.3106210},
abstract = {The large number of possible configurations makes it unfeasible to test every single system variant in a product line. Consequently, a small subset of the product line products must be selected, typically following combinatorial interaction testing approaches. Recently, many product line engineering approaches have considered the selection and prioritization of relevant products within the product line. In a further step, these products are thoroughly tested individually. However, the test cases that must be executed in each of the products are not always insignificant, and in systems such as Cyber-Physical System Product Lines (CPSPLs), their test execution time can vary from tens to thousands of seconds. This issue leads to spending a lot of time testing each individual product. To solve this problem we propose a search-based approach to perform the testing of product lines by allocating small number of test cases in each of the products. This approach increases the probability of detecting faults faster. Specifically, our search-based approach obtains a set of products, which are derived from using any state-of-the-art approach as inputs, and a set of attributed test cases. As an output a list of allocated test cases for each product is obtained. We also define a novel fitness function to guide the search and we propose corresponding crossover and mutation operators. The search and test process is iteratively repeated until the time budget is consumed. We performed an evaluation with a CPSPL as a case study. Results suggest that our approach can reduce the fault detection time by 61% and 65% on average when compared with the traditional test process and the Random Search algorithm respectively.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {123–132},
numpages = {10},
keywords = {Search-based Software Engineering, Product Line Testing, Fault Detection},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3307630.3342398,
author = {Beek, Maurice H. ter and Schmid, Klaus and Eichelberger, Holger},
title = {Textual Variability Modeling Languages: An Overview and Considerations},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342398},
doi = {10.1145/3307630.3342398},
abstract = {During the three decades since the invention of the first variability modeling approach [28], there have been multiple attempts to introduce advanced variability modeling capabilities. More recently, we have seen increased attention on textual variability modeling languages. In this paper, we summarize the main capabilities of state of the art textual variability modeling languages, based on [23], including updates regarding more recent work. Based on this integrated characterization, we provide a discussion of additional concerns, opportunities and challenges that are relevant for designing future (textual) variability modeling languages. The paper also summarizes relevant contributions by the authors as input to further discussions on future (textual) variability modeling languages.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {151–157},
numpages = {7},
keywords = {variability modeling, textual specification languages, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3233027.3233031,
author = {Kaindl, Hermann and Kramer, Stefan and Hoch, Ralph},
title = {An inductive learning perspective on automated generation of feature models from given product specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233031},
doi = {10.1145/3233027.3233031},
abstract = {For explicit representation of commonality and variability of a product line, a feature model is mostly used. An open question is how a feature model can be inductively learned in an automated way from a limited number of given product specifications in terms of features.We propose to address this problem through machine learning, more precisely inductive generalization from examples. However, no counter-examples are assumed to exist. Basically, a feature model needs to be complete with respect to all the given example specifications. First results indicate the feasibility of this approach, even for generating hierarchies, but many open challenges remain.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {25–30},
numpages = {6},
keywords = {machine learning, inductive generalization from examples, generating feature models},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3691620.3695321,
author = {Gropengie\ss{}er, Uwe and Liphardt, Julian and Matth\'{e}, Michael and M\"{u}hlh\"{a}user, Max},
title = {Feature Model Slicing for Real-time Selection of Mission-critical Edge Application},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695321},
doi = {10.1145/3691620.3695321},
abstract = {At first glance, running mission-critical applications at the edge appears to be an opportunity to benefit from scalability and reusability. The low latency to the edge makes it particularly interesting for mission-critical applications. The hardware heterogeneity of the edge, coupled with the strict requirement for the execution time of a mission-critical application, creates the need for flexible application control and, at the same time, increases the complexity of modeling such systems. With its Feature Models (FMs), software product line engineering offers a modeling option for various alternative compositions of an application. However, the calculation of valid configurations takes too long for the dynamic adaptation of an application flow of a mission-critical application. This paper presents an approach for slicing FMs to support mission-critical applications. Our approach supports the strict requirements on the execution time of mission-critical applications.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {2446–2447},
numpages = {2},
keywords = {software product lines, feature model, approximate computing, edge computing},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3307630.3342417,
author = {Achtaich, Asmaa and Roudies, Ounsa and Souissi, Nissrine and Salinesi, Camille and Mazo, Ra\'{u}l},
title = {Evaluation of the State-Constraint Transition Modelling Language: A Goal Question Metric Approach},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342417},
doi = {10.1145/3307630.3342417},
abstract = {Self-adaptive systems (SAS) are exceptional systems, on account of their versatile composition, dynamic behavior and evolutive nature. Existing formal languages for the specification of SAS focus on adapting system elements to achieve a target goal, following specific rules, without much attention on the adaptation of requirements themselves. The State-Constraint Transition (SCT) modeling language enables the specification of dynamic requirements, both at the domain and application level, as a result of space or time variability. This language, evaluated in this paper, enables the specification of a variety of requirement types, for SASs from different domains, while generating a configuration, all configurations, and number of possible configurations, in milliseconds. This paper presents these results, namely; expressiveness, domain independence and scalability, from the viewpoint of designers and domain engineers, following a goal-question-metric approach. However, being primarily based on constraint programming (CP), the language suffers from drawbacks inherited from this paradigm, specifically time related requirements, like (e.g. order, frequency and staged requirements).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {106–113},
numpages = {8},
keywords = {state machine, modeling language, dynamic software product lines, constraint programming, IoT},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3106195.3106214,
author = {Couto, Marco and Borba, Paulo and Cunha, J\'{a}come and Fernandes, Jo\~{a}o Paulo and Pereira, Rui and Saraiva, Jo\~{a}o},
title = {Products go Green: Worst-Case Energy Consumption in Software Product Lines},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106214},
doi = {10.1145/3106195.3106214},
abstract = {The optimization of software to be (more) energy efficient is becoming a major concern for the software industry. Although several techniques have been presented to measure energy consumption for software, none has addressed software product lines (SPLs). Thus, to measure energy consumption of a SPL, the products must be generated and measured individually, which is too costly.In this paper, we present a technique and a prototype tool to statically estimate the worst case energy consumption for SPL. The goal is to provide developers with techniques and tools to reason about the energy consumption of all products in a SPL, without having to produce, run and measure the energy in all of them.Our technique combines static program analysis techniques and worst case execution time prediction with energy consumption analysis. This technique analyzes all products in a feature-sensitive manner, that is, a feature used in several products is analyzed only once, while the energy consumption is estimated once per product.We implemented our technique in a tool called Serapis. We did a preliminary evaluation using a product line for image processing implemented in C. Our experiments considered 7 products from such line and our initial results show that the tool was able to estimate the worst-case energy consumption with a mean error percentage of 9.4% and standard deviation of 6.2% when compared with the energy measured when running the products.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {84–93},
numpages = {10},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2648511.2648521,
author = {Olaechea, Rafael and Rayside, Derek and Guo, Jianmei and Czarnecki, Krzysztof},
title = {Comparison of exact and approximate multi-objective optimization for software product lines},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648521},
doi = {10.1145/2648511.2648521},
abstract = {Software product lines (SPLs) allow stakeholders to manage product variants in a systematical way and derive variants by selecting features. Finding a desirable variant is often difficult, due to the huge configuration space and usually conflicting objectives (e.g., lower cost and higher performance). This scenario can be characterized as a multi-objective optimization problem applied to SPLs. We address the problem using an exact and an approximate algorithm and compare their accuracy, time consumption, scalability, parameter setting requirements on five case studies with increasing complexity. Our empirical results show that (1) it is feasible to use exact techniques for small SPL multi-objective optimization problems, and (2) approximate methods can be used for large problems but require substantial effort to find the best parameter setting for acceptable approximation which can be ameliorated with known good parameter ranges. Finally, we discuss the tradeoff between accuracy and time consumption when using exact and approximate techniques for SPL multi-objective optimization and guide stakeholders to choose one or the other in practice.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {92–101},
numpages = {10},
keywords = {software product lines, multi-objective optimization},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3377930.3390215,
author = {Silva, Diego Fernandes da and Okada, Luiz Fernando and Colanzi, Thelma Elita and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Enhancing search-based product line design with crossover operators},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3390215},
doi = {10.1145/3377930.3390215},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA designing has been formulated as a multi-objective optimization problem and successfully solved by a state-of-the-art search-based approach. However, the majority of empirical studies optimize PLA designs without applying one of the fundamental genetic operators: the crossover. An operator for PLA design, named Feature-driven Crossover, was proposed in a previous study. In spite of the promising results, this operator occasionally generated incomplete solutions. To overcome these limitations, this paper aims to enhance the search-based PLA design optimization by improving the Feature-driven Crossover and introducing a novel crossover operator specific for PLA design. The proposed operators were evaluated in two well-studied PLA designs, using three experimental configurations of NSGA-II in comparison with a baseline that uses only mutation operators. Empirical results show the usefulness and efficiency of the presented operators on reaching consistent solutions. We also observed that the two operators complement each other, leading to PLA design solutions with better feature modularization than the baseline experiment.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {1250–1258},
numpages = {9},
keywords = {software product line, software architecture, recombination operators, multi-objective evolutionary algorithm},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1145/3109729.3109745,
author = {Markiegi, Urtzi},
title = {Test optimisation for Highly-Configurable Cyber-Physical Systems},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109745},
doi = {10.1145/3109729.3109745},
abstract = {Cyber-Physical Systems (CPS) have become one of the core-enabling technologies for multiple domains, such as manufacturing, healthcare, energy and transportation. Furthermore, these domains are demanding CPS to be highly-configurable in order to respond to multiple and changing market requirements. Testing these Highly-Configurable Cyber-Physical Systems (HCCPS) is challenging. First, when working with CPSs, considerable time is required in order to tackle physical processes during testing. And secondly, in highly-configurable systems, a large number of system variants need to be tested. Consequently, reducing HCCPS testing time is essential.In this context, a research work is presented to reduce the overall testing time of HCCPS, focusing on a merged strategy of product and test cases optimisation. In particular, two approaches are proposed in order to achieve the testing time reduction. The first approach aims to reduce the HCCPS testing time by an iterative allocation of products and test cases. The second approach aims to reduce the HCCPS testing time by a feedback driven dynamic and iterative allocation of products and test cases.A preliminary experiment has been undertaken to test the iterative allocation approach. In this experiment, products to be tested are selected and prioritised. Next, multiple testing iterations are perform until the time-budget is consumed. In each iteration a small number of test cases are allocated for each of the products to be tested. The experiment was evaluated with an academic HCCPS and preliminary results suggest that the proposed approach reduces the fault detection time when compared with traditional approaches.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {139–144},
numpages = {6},
keywords = {Software Engineering, Search-Based Software Engineering, Product Line Testing, Highly-Configurable Systems, Fault Detection, Cyber-Physical Systems},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3109729.3109758,
author = {Ben Snaiba, Ziad and de Vink, Erik P. and Willemse, Tim A.C.},
title = {Family-Based Model Checking of SPL based on mCRL2},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109758},
doi = {10.1145/3109729.3109758},
abstract = {We discuss how the general-purpose model checker mCRL2 can be used for family-based verification of behavioral properties of software product lines. This is achieved by exploiting a feature-oriented extension of the modal μ-calculus for the specification of SPL properties, and for its model checking by encoding it back into the logic of mCRL2. Using the example of the well-known minepump SPL an illustration of the possibilities of the approach is given.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {13–16},
numpages = {4},
keywords = {mCRL2, Software Product Lines, Family-based model checking},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2791060.2791087,
author = {ter Beek, M. H. and Legay, A. and Lafuente, A. Lluch and Vandin, A.},
title = {Statistical analysis of probabilistic models of software product lines with quantitative constraints},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791087},
doi = {10.1145/2791060.2791087},
abstract = {We investigate the suitability of statistical model checking for the analysis of probabilistic models of software product lines with complex quantitative constraints and advanced feature installation options. Such models are specified in the feature-oriented language QFLan, a rich process algebra whose operational behaviour interacts with a store of constraints, neatly separating product configuration from product behaviour. The resulting probabilistic configurations and behaviour converge seamlessly in a semantics based on DTMCs, thus enabling quantitative analyses ranging from the likelihood of certain behaviour to the expected average cost of products. This is supported by a Maude implementation of QFLan, integrated with the SMT solver Z3 and the distributed statistical model checker MultiVeStA. Our approach is illustrated with a bikes product line case study.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {11–15},
numpages = {5},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2791060.2791066,
author = {Dhungana, Deepak and Falkner, Andreas and Haselb\"{o}ck, Alois and Schreiner, Herwig},
title = {Smart factory product lines: a configuration perspective on smart production ecosystems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791066},
doi = {10.1145/2791060.2791066},
abstract = {Smart production aims to increase the flexibility of the production processes and be more efficient in the use of resources. Two important pillars of this initiative are "smart products" and "smart factories". From the perspective of product line engineering, these can be seen as two product lines (product line of factories and product line of goods) that need to be integrated for a common systems engineering approach. In this paper, we look at this problem from the perspective of configuration technologies, outline the research challenges in this area and illustrate our vision using an industrial example. The factory product line goes hand-in-hand with the product line of the products to be manufactured. Future research in product line engineering needs to consider an ecosystem of a multitude of stakeholders - e.g., factory component vendors, product designers, factory owners/operators and end-consumers.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {201–210},
numpages = {10},
keywords = {smart production, smart product, smart factory, product line of factories, product and production configuration},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3336294.3336306,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {Automated Search for Configurations of Convolutional Neural Network Architectures},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336306},
doi = {10.1145/3336294.3336306},
abstract = {Convolutional Neural Networks (CNNs) are intensively used to solve a wide variety of complex problems. Although powerful, such systems require manual configuration and tuning. To this end, we view CNNs as configurable systems and propose an end-to-end framework that allows the configuration, evaluation and automated search for CNN architectures. Therefore, our contribution is threefold. First, we model the variability of CNN architectures with a Feature Model (FM) that generalizes over existing architectures. Each valid configuration of the FM corresponds to a valid CNN model that can be built and trained. Second, we implement, on top of Tensorflow, an automated procedure to deploy, train and evaluate the performance of a configured model. Third, we propose a method to search for configurations and demonstrate that it leads to good CNN models. We evaluate our method by applying it on image classification tasks (MNIST, CIFAR-10) and show that, with limited amount of computation and training, our method can identify high-performing architectures (with high accuracy). We also demonstrate that we outperform existing state-of-the-art architectures handcrafted by ML researchers. Our FM and framework have been released to support replication and future research.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {119–130},
numpages = {12},
keywords = {neural architecture search, feature model, configuration search, NAS, AutoML},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2362536.2362548,
author = {Soltani, Samaneh and Asadi, Mohsen and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Bagheri, Ebrahim},
title = {Automated planning for feature model configuration based on functional and non-functional requirements},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362548},
doi = {10.1145/2362536.2362548},
abstract = {Feature modeling is one of the main techniques used in Software Product Line Engineering to manage the variability within the products of a family. Concrete products of the family can be generated through a configuration process. The configuration process selects and/or removes features from the feature model according to the stakeholders' requirements. Selecting the right set of features for one product from amongst all of the available features in the feature model is a complex task because: 1) the multiplicity of stakeholders' functional requirements; 2) the positive or negative impact of features on non-functional properties; and 3) the stakeholders' preferences w.r.t. the desirable non-functional properties of the final product. Many configurations techniques have already been proposed to facilitate automated product derivation. However, most of the current proposals are not designed to consider stakeholders' preferences and constraints especially with regard to non-functional properties. We address the software product line configuration problem and propose a framework, which employs an artificial intelligence planning technique to automatically select suitable features that satisfy both the stakeholders' functional and non-functional preferences and constraints. We also provide tooling support to facilitate the use of our framework. Our experiments show that despite the complexity involved with the simultaneous consideration of both functional and non-functional properties our configuration technique is scalable.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {56–65},
numpages = {10},
keywords = {software product line engineering, planning techniques, feature model, configuration, artificial intelligence},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3382025.3414962,
author = {Chrszon, Philipp and Baier, Christel and Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha},
title = {From features to roles},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414962},
doi = {10.1145/3382025.3414962},
abstract = {The detection of interactions is a challenging task present in almost all stages of software development. In feature-oriented system design, this task is mainly investigated for interactions of features within a single system, detected by their emergent behaviors. We propose a formalism to describe interactions in hierarchies of feature-oriented systems (hierarchical interactions) and the actual situations where features interact (active interplays). Based on the observation that such interactions are also crucial in role-based systems, we introduce a compositional modeling framework based on concepts and notions of roles, comprising role-based automata (RBAs). To describe RBAs, we present a modeling language that is close to the input language of the probabilistic model checker Prism. To exemplify the use of RBAs, we implemented a tool that translates RBA models into Prism and thus enables the formal analysis of functional and non-functional properties including system dynamics, contextual changes, and interactions. We carry out two case studies as a proof of concept of such analyses: First, a peer-to-peer protocol case study illustrates how undesired hierarchical interactions can be discovered automatically. Second, a case study on a self-adaptive production cell demonstrates how undesired interactions influence quality-of-service measures such as reliability and throughput.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {19},
numpages = {11},
keywords = {verification, roles, formal methods, feature-oriented systems},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2791060.2791073,
author = {Lachmann, Remo and Lity, Sascha and Lischke, Sabrina and Beddig, Simon and Schulze, Sandro and Schaefer, Ina},
title = {Delta-oriented test case prioritization for integration testing of software product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791073},
doi = {10.1145/2791060.2791073},
abstract = {Software product lines have potential to allow for mass customization of products. Unfortunately, the resulting, vast amount of possible product variants with commonalities and differences leads to new challenges in software testing. Ideally, every product variant should be tested, especially in safety-critical systems. However, due to the exponentially increasing number of product variants, testing every product variant is not feasible. Thus, new concepts and techniques are required to provide efficient SPL testing strategies exploiting the commonalities of software artifacts between product variants to reduce redundancy in testing. In this paper, we present an efficient integration testing approach for SPLs based on delta modeling. We focus on test case prioritization. As a result, only the most important test cases for every product variant are tested, reducing the number of executed test cases significantly, as testing can stop at any given point because of resource constraints while ensuring that the most important test cases have been covered. We present the general concept and our evaluation results. The results show a measurable reduction of executed test cases compared to single-software testing approaches.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {81–90},
numpages = {10},
keywords = {test case prioritization, regression testing, delta-oriented software product lines, architecture-based testing},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3382025.3414963,
author = {Creff, Stephen and Noir, J\'{e}r\^{o}me Le and Lenormand, Eric and Madel\'{e}nat, S\'{e}bastien},
title = {Towards facilities for modeling and synthesis of architectures for resource allocation problem in systems engineering},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414963},
doi = {10.1145/3382025.3414963},
abstract = {Exploring architectural design space is often beyond human capacity and makes architectural design a difficult task. Model-based systems engineering must include assistance to the system designer in identifying candidate architectures to subsequently analyze tradeoffs. Unfortunately, existing languages and approaches do not incorporate this concern, generally favoring solution analysis over exploring a set of candidate architectures.In this paper, we explore the advantages of designing and configuring the variability problem to solve one of the problems of exploring (synthesizing) candidate architectures in systems engineering: the resource allocation problem. More specifically, this work reports on the use of the Clafer modeling language and its gateway to the CSP Choco Solver, on an industrial case study of heterogeneous hardware resource allocation (GPP-GPGPU-FPGA).Based on experiments on the modeling in Clafer, and the impact of its translation into the constraint programming paradigm (performance studies), discussions highlight some issues concerning facilities for modeling and synthesis of architectures and recommendations are proposed towards the use of this variability approach.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {32},
numpages = {11},
keywords = {variability modeling, empirical study, constraint solving, architecture synthesis, allocation problem},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1145/3361146,
author = {Hierons, Robert M. and Li, Miqing and Liu, Xiaohui and Parejo, Jose Antonio and Segura, Sergio and Yao, Xin},
title = {Many-Objective Test Suite Generation for Software Product Lines},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3361146},
doi = {10.1145/3361146},
abstract = {A Software Product Line (SPL) is a set of products built from a number of features, the set of valid products being defined by a feature model. Typically, it does not make sense to test all products defined by an SPL and one instead chooses a set of products to test (test selection) and, ideally, derives a good order in which to test them (test prioritisation). Since one cannot know in advance which products will reveal faults, test selection and prioritisation are normally based on objective functions that are known to relate to likely effectiveness or cost. This article introduces a new technique, the grid-based evolution strategy (GrES), which considers several objective functions that assess a selection or prioritisation and aims to optimise on all of these. The problem is thus a many-objective optimisation problem. We use a new approach, in which all of the objective functions are considered but one (pairwise coverage) is seen as the most important. We also derive a novel evolution strategy based on domain knowledge. The results of the evaluation, on randomly generated and realistic feature models, were promising, with GrES outperforming previously proposed techniques and a range of many-objective optimisation algorithms.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {2},
numpages = {46},
keywords = {test selection, test prioritisation, multi-objective optimisation, Software product line}
}

@inproceedings{10.1145/2934466.2946046,
author = {Arrieta, Aitor and Wang, Shuai and Sagardui, Goiuria and Etxeberria, Leire},
title = {Search-based test case selection of cyber-physical system product lines for simulation-based validation},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946046},
doi = {10.1145/2934466.2946046},
abstract = {Cyber-Physical Systems (CPSs) are often tested at different test levels following "X-in-the-Loop" configurations: Model-, Software- and Hardware-in-the-loop (MiL, SiL and HiL). While MiL and SiL test levels aim at testing functional requirements at the system level, the HiL test level tests functional as well as non-functional requirements by performing a real-time simulation. As testing CPS product line configurations is costly due to the fact that there are many variants to test, test cases are long, the physical layer has to be simulated and co-simulation is often necessary. It is therefore extremely important to select the appropriate test cases that cover the objectives of each level in an allowable amount of time. We propose an efficient test case selection approach adapted to the "X-in-the-Loop" test levels. Search algorithms are employed to reduce the amount of time required to test configurations of CPS product lines while achieving the test objectives of each level. We empirically evaluate three commonly-used search algorithms, i.e., Genetic Algorithm (GA), Alternating Variable Method (AVM) and Greedy (Random Search (RS) is used as a baseline) by employing two case studies with the aim of integrating the best algorithm into our approach. Results suggest that as compared with RS, our approach can reduce the costs of testing CPS product line configurations by approximately 80% while improving the overall test quality.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {297–306},
numpages = {10},
keywords = {test case selection, search-based software engineering, cyber-physical system product lines},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2791060.2791077,
author = {Rumpe, Bernhard and Schulze, Christoph and von Wenckstern, Michael and Ringert, Jan Oliver and Manhart, Peter},
title = {Behavioral compatibility of simulink models for product line maintenance and evolution},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791077},
doi = {10.1145/2791060.2791077},
abstract = {Embedded software systems, e.g. automotive, robotic or automation systems are highly configurable and consist of many software components being available in different variants and versions. To identify the degree of reusability between these different occurrences of a component, it is necessary to determine the functional backward and forward compatibility between them. Based on this information it is possible to identify in which system context a component can be replaced safely by another version, e.g. exchanging an older component, or variant, e.g. introducing new features, to achieve the same functionality.This paper presents a model checking approach to determine behavioral compatibility of Simulink models, obtained from different component variants or during evolution. A prototype for automated compatibility checking demonstrates its feasibility. In addition implemented optimizations make the analysis more efficient, when the compared variants or versions are structurally similar.A case study on a driver assistance system provided by Daimler AG shows the effectiveness of the approach to automatically compare Simulink components.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {141–150},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2019136.2019150,
author = {Serajzadeh, Hadi and Shams, Fereidoon},
title = {The application of swarm intelligence in service-oriented product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019150},
doi = {10.1145/2019136.2019150},
abstract = {Changing markets and environments has made the ability to rapidly adapt to these changes a necessity in software systems. However the costs of changing and adapting systems to new requirements still remains an unsolved issue. In this context service-oriented software product lines were introduced with the aim to combine the reusability of software product line with the flexibility of service-oriented architecture. Although this approach helps build flexible software systems with high levels of reuse, certain issues are raised. The main issue is the complexity that a service-oriented product line will face. Developing systems from internal and external assets, taking into consideration the variety and number of these assets, can cause problems in deciding which asset is best suited for the system. To help solve these issues we propose the use of approaches based on artificial intelligence. In this paper we show how swarm intelligence can be used in service-oriented product lines to reduce complexity and find optimal solutions for the development of software systems. We also present an example of the application of swarm intelligence in finding the optimal product for a service-oriented product line.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {12},
numpages = {7},
keywords = {swarm intelligence, service-oriented product line, optimization},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2934466.2934472,
author = {Temple, Paul and Galindo, Jos\'{e} A. and Acher, Mathieu and J\'{e}z\'{e}quel, Jean-Marc},
title = {Using machine learning to infer constraints for product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934472},
doi = {10.1145/2934466.2934472},
abstract = {Variability intensive systems may include several thousand features allowing for an enormous number of possible configurations, including wrong ones (e.g. the derived product does not compile). For years, engineers have been using constraints to a priori restrict the space of possible configurations, i.e. to exclude configurations that would violate these constraints. The challenge is to find the set of constraints that would be both precise (allow all correct configurations) and complete (never allow a wrong configuration with respect to some oracle). In this paper, we propose the use of a machine learning approach to infer such product-line constraints from an oracle that is able to assess whether a given product is correct. We propose to randomly generate products from the product line, keeping for each of them its resolution model. Then we classify these products according to the oracle, and use their resolution models to infer cross-tree constraints over the product-line. We validate our approach on a product-line video generator, using a simple computer vision algorithm as an oracle. We show that an interesting set of cross-tree constraint can be generated, with reasonable precision and recall.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {209–218},
numpages = {10},
keywords = {variability modeling, software testing, software product lines, machine learning, constraints and variability mining},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2491627.2491647,
author = {Murashkin, Alexandr and Antkiewicz, Micha\l{} and Rayside, Derek and Czarnecki, Krzysztof},
title = {Visualization and exploration of optimal variants in product line engineering},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491647},
doi = {10.1145/2491627.2491647},
abstract = {The decision-making process in Product Line Engineering (PLE) is often concerned with variant qualities such as cost, battery life, or security. Pareto-optimal variants, with respect to a set of objectives such as minimizing a variant's cost while maximizing battery life and security, are variants in which no single quality can be improved without sacrificing other qualities. We propose a novel method and a tool for visualization and exploration of a multi-dimensional space of optimal variants (i.e., a Pareto front). The visualization method is an integrated, interactive, and synchronized set of complementary views onto a Pareto front specifically designed to support PLE scenarios, including: understanding differences among variants and their positioning with respect to quality dimensions; solving trade-offs; selecting the most desirable variants; and understanding the impact of changes during product line evolution on a variant's qualities. We present an initial experimental evaluation showing that the visualization method is a good basis for supporting these PLE scenarios.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {111–115},
numpages = {5},
keywords = {visualization, product line engineering, pareto front, optimal variant, feature modeling, exploration, clafer, ClaferMoo visualizer, ClaferMoo},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2791060.2791067,
author = {Yue, Tao and Ali, Shaukat and Selic, Bran},
title = {Cyber-physical system product line engineering: comprehensive domain analysis and experience report},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791067},
doi = {10.1145/2791060.2791067},
abstract = {Cyber-Physical Systems (CPSs) are the future generation of highly connected embedded systems having applications in diverse domains including Oil and Gas. Employing Product Line Engineering (PLE) is believed to bring potential benefits with respect to reduced cost, higher productivity, higher quality, and faster time-to-market. However, relatively few industrial field studies are reported regarding the application of PLE to develop large-scale systems, and more specifically CPSs. In this paper, we report about our experiences and insights gained from investigating the application of model-based PLE at a large international organization developing subsea production systems (typical CPSs) to manage the exploitation of oil and gas production fields. We report in this paper 1) how two systematic domain analyses (on requirements engineering and product configuration/derivation) were conducted to elicit CPS PLE requirements and challenges, 2) key results of the domain analysis (commonly observed in other domains), and 3) our initial experience of developing and applying two Model Based System Engineering (MBSE) PLE solution to address some of the requirements and challenges elicited during the domain analyses.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {338–347},
numpages = {10},
keywords = {requirements engineering, product line engineering (PLE), model based system engineering, domain analysis, cyber physical system (CPS)},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2934466.2934492,
author = {Groher, Iris and Weinreich, Rainer and Buchgeher, Georg and Schossleitner, Robert},
title = {Reusable architecture variants for customer-specific automation solutions},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934492},
doi = {10.1145/2934466.2934492},
abstract = {Manufacturing execution systems (MES) are key elements of industrial automation systems. MES can be deployed at different levels of scale from a single site or plant to a company with globally distributed production sites all over the world. Establishing or extending an MES is a complex process, which requires taking the already existing software and system architecture into account in addition to the desired MES features. We developed an approach and an associated tool to support the process of creating offers for customer-specific MES solutions based on a vendor-specific automation platform. We define architecture variants for selecting a specific MES feature set and for supporting different MES expansion stages. Additionally, we provide an architecture modeling approach to explore the integration with existing software and system infrastructures. The approach has been applied at the STIWA Group, a vendor of MES for industrial production lines.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {242–251},
numpages = {10},
keywords = {manufacturing execution system (MES), feature set, customer-specific offer, automation platform, architecture variants},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2648511.2648550,
author = {Dillon, Michael and Rivera, Jorge and Darbin, Rowland},
title = {A methodical approach to product line adoption},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648550},
doi = {10.1145/2648511.2648550},
abstract = {The evolution of the U.S. Army's Live Training Transformation (LT2) product line of combat training systems, including the move by the Army to consolidate management of the product line under a single contracting team, has provided a natural experiment that validates the hypothesis that product line engineering practices are more effective than traditional software engineering practices, and has demonstrated which product line adoption approaches are more successful than others. By analyzing this natural experiment, the product line team has been able to apply a methodical approach to product line adoption across the development organization and successfully adopt second generation product line processes. This paper explores that methodical approach. It will enumerate the steps that led to successes and explore the contributing factors and unintended consequences of failures along the way. Additionally this paper will explore how this approach is being employed to extend the LT2 product line beyond software.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {340–349},
numpages = {10},
keywords = {variation points, software product lines, second generation product line engineering, product portfolio, product line governance, product line engineering, product line adoption, product configurator, product baselines, feature profiles, feature modeling, feature constraints hierarchical product lines, bill-of-features},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2362536.2362563,
author = {Heider, Wolfgang and Rabiser, Rick and Gr\"{u}nbacher, Paul and Lettner, Daniela},
title = {Using regression testing to analyze the impact of changes to variability models on products},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362563},
doi = {10.1145/2362536.2362563},
abstract = {Industrial product lines are typically maintained for a long time and evolve continuously to address changing requirements and new technologies. Already derived products often have to be re-derived after such changes to benefit from new and updated features. Product line engineers thus frequently need to analyze the impact of changes to variability models to prevent unexpected changes of re-derived products. In this paper we present a tool-supported approach that informs engineers about the impacts of variability model changes on existing products. Regression tests are used to determine whether existing product configurations and generated product outputs can be re-derived without unexpected effects. We evaluate the feasibility of the approach based on changes observed in a real-world software product line. More specifically, we show how our approach helps engineers performing specific evolution tasks to analyze the change impacts on existing products. We also evaluate the performance and scalability of our approach. Our results show that variability change impact analyses can be automated using model regression testing and can help reducing the gap between domain engineering and application engineering.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {196–205},
numpages = {10},
keywords = {variability models, regression testing, product line evolution},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2362536.2362572,
author = {Leitner, Andrea and Zehetner, Josef and Toeglhofer, Philipp and Watzenig, Daniel},
title = {Requirement identification for variability management in a co-simulation environment},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362572},
doi = {10.1145/2362536.2362572},
abstract = {Co-simulation is a powerful approach to verify a system design and to support concept decisions early in the automotive development process. Due to the heterogeneous nature of the co-simulation framework there is a lot of potential for variability requiring the systematic handling of it.We identified two main scenarios for variability management techniques in a co-simulation environment. Variability management capabilities can be included in the co-simulation tool itself or provide variability mechanisms to configure the co-simulation externally from a software product line. Depending on the context, one or even both scenarios can be applied.This work addresses different types of variability in an independent co-simulation framework (ICOS) and defines requirements for a realization concept.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {269–274},
numpages = {6},
keywords = {variability management, software product line engineering, co-simulation},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791100,
author = {ter Beek, Maurice H. and Fantechi, Alessandro and Gnesi, Stefania},
title = {Applying the product lines paradigm to the quantitative analysis of collective adaptive systems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791100},
doi = {10.1145/2791060.2791100},
abstract = {Engineering a Collective Adaptive System (CAS) requires the support of a framework for quantitative modeling and analysis of the system. In order to jointly address variability and quantitative analysis, we apply the Product Lines paradigm, considered at the level of system engineering, to a case study of the European project QUANTICOL, by first defining a reference feature model and then adding feature attributes and global quantitative constraints, in the form of a Clafer attributed feature model. ClaferMOOVisualizer is subsequently used for quantitative analyses and multi-objective optimization of the resulting attributed feature model.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {321–326},
numpages = {6},
keywords = {variability analysis, quantitative modeling, quantitative analysis, multi-objective optimization, collective adaptive systems, ClaferMOO},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3233027.3233043,
author = {Masri, Samer AL and Nadi, Sarah and Gaudet, Matthew and Liang, Xiaoli and Young, Robert W.},
title = {Using static analysis to support variability implementation decisions in C++},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233043},
doi = {10.1145/3233027.3233043},
abstract = {Eclipse OMR is an open-source C++ framework for building robust language runtimes. The OMR toolkit includes a dynamic Just-In-Time (JIT) compiler, a garbage collector, a platform abstraction library, and a set of developer tooling capabilities. To support the diverse languages and architectures targeted by the framework, OMR's variability implementation uses a combination of build-system variability and static polymorphism. That is, all implementation classes that depend on the selected language and architecture are decided at compile time. However, OMR developers now realize that the current variability design decision, specifically the static polymorphism implementation, has its drawbacks. They are considering using dynamic polymorphism instead of static polymorphism. Before making such a fundamental design change, however, it is crucial to collect function information and overload/override statistics about the current variability in the code base.In this paper, we present OMRStatistics, a static analysis tool that we built for OMR developers to help them collect this information. Specifically, OMRStatistics (1) visualizes the class hierarchy from OMR's current static polymorphic implementation, (2) visualizes the function overloads and overrides with their respective locations in the source code, (3) collects important information about the classes and functions, and (4) stores all the collected information in a database for further analysis. Our tool OMRStatistics allows OMR developers to make better design decisions on which variability extension points should be switched from static polymorphism to dynamic polymorphism.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {236–245},
numpages = {10},
keywords = {static polymorphism, static analysis, software variability analysis, dynamic polymorphism, clang plugin, build path variability, C++},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2364412.2364434,
author = {Helvensteijn, Michiel},
title = {Dynamic delta modeling},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364434},
doi = {10.1145/2364412.2364434},
abstract = {Abstract Delta Modeling (ADM) offers an algebraic description of how a (software) product line may be built so that every product can be automatically derived by structured reuse of code. In traditional application engineering a single valid feature configuration is chosen, which does not change during the lifetime of the product. However, there are many useful applications for product lines that change their configuration at run time. We present a new technique for generating efficient dynamic product lines from their static counterparts. We use Mealy machines for their dynamic reconfiguration. Furthermore, we posit that monitoring some features will be more expensive than monitoring others, and present techniques for minimizing the cost of monitoring the system. We stay in the abstract setting of ADM but the techniques can be instantiated to any concrete domain. We illustrate them through the example of a mobile application for Android, which dynamically reconfigures a devices operating profile based on environmental factors.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {127–134},
numpages = {8},
keywords = {profile management, optimization, mealy machines, dynamic product lines, delta modeling},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2362536.2362553,
author = {Andersen, Nele and Czarnecki, Krzysztof and She, Steven and W\k{a}sowski, Andrzej},
title = {Efficient synthesis of feature models},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362553},
doi = {10.1145/2362536.2362553},
abstract = {Variability modeling, and in particular feature modeling, is a central element of model-driven software product line architectures. Such architectures often emerge from legacy code, but, unfortunately creating feature models from large, legacy systems is a long and arduous task.We address the problem of automatic synthesis of feature models from propositional constraints. We show that this problem is NP-hard. We design efficient techniques for synthesis of models from respectively CNF and DNF formulas, showing a 10- to 1000-fold performance improvement over known techniques for realistic benchmarks.Our algorithms are the first known techniques that are efficient enough to be applied to dependencies extracted from real systems, opening new possibilities of creating reverse engineering and model management tools for variability models. We discuss several such scenarios in the paper.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {106–115},
numpages = {10},
keywords = {variability models, software product lines, feature models},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1145/3611663,
author = {Oh, Jeho and Batory, Don and Heradio, Rub\'{e}n},
title = {Finding Near-optimal Configurations in Colossal Spaces with Statistical Guarantees},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3611663},
doi = {10.1145/3611663},
abstract = {A Software Product Line (SPL) is a family of similar programs. Each program is defined by a unique set of features, called a configuration, that satisfies all feature constraints. “What configuration achieves the best performance for a given workload?” is the SPLOptimization (SPLO) challenge. SPLO is daunting: just 80 unconstrained features yield 1024 unique configurations, which equals the estimated number of stars in the universe. We explain (a) how uniform random sampling and random search algorithms solve SPLO more efficiently and accurately than current machine-learned performance models and (b) how to compute statistical guarantees on the quality of a returned configuration; i.e., it is within x% of optimal with y% confidence.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {7},
numpages = {36},
keywords = {Software product lines, configuration optimization, product spaces, machine learning, uniform random sampling, random search, order statistics}
}

@inproceedings{10.1145/2791060.2791065,
author = {Gregg, Susan P. and Scharadin, Rick and Clements, Paul},
title = {The more you do, the more you save: the superlinear cost avoidance effect of systems product line engineering},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791065},
doi = {10.1145/2791060.2791065},
abstract = {Product lines that use automated tools to configure shared assets (e.g., software or requirements or test cases or user documentation) based on product descriptions have long been known to bring about substantial development cost avoidance when compared to clone-and-own or product-specific development techniques. Now, however, it can be shown that the cost avoidance for configuring multiple shared assets is superlinear -- that is, the overall cost avoidance exceeds the sum of the that brought about by working with each of the shared assets in isolation. That is, a product line that configures (for example) requirements and code will avoid more cost than the sum of code-based plus requirements-based cost avoidance. In addition, we also observe a superlinear effect in terms of the number of products in the portfolio as well. This paper explores why these effects occur, and presents analytical and empirical evidence for their existence from one of the largest and most successful product lines in the literature, the AEGIS Weapon System. The result may lead to new insight into the economics of product line engineering in the systems engineering realm.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {303–310},
numpages = {8},
keywords = {variation points, systems and software product lines, second generation product line engineering, product line measurement, product line engineering, product line economics, product derivation, product configurator, feature modeling, AEGIS},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3640310.3674090,
author = {Restrepo, Camilo Correa and Robin, Jacques and Mazo, Raul},
title = {Extensions and Scalability Experiments of a Generic Model-Driven Architecture for Variability Model Reasoning},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674090},
doi = {10.1145/3640310.3674090},
abstract = {Until recently, the state-of-the-art of Software Product Line (SPL) configuration and verification automation consisted of a collection of ad-hoc approaches tightly coupling a single input Variability Modeling Language (VML) with a single constraint solver. To remedy this situation, a novel generic model-driven architecture was then proposed that enables using a variety of VMLs and solvers. The key ideas of this proposal were (a) the use of a standard logical language (CLIF) as a pivot between VMLs and solvers, and (b) the use of a standard data exchange format (JSON) to explicilty and declaratively specify the abstract syntax and semantics of the VMLs to be used in an SPL engineering project and the automated reasoning task to be performed by the solvers.In this article, we overcome the limitations of this initial proposal in three key ways: (1) we add the ability to reason on textual or hybrid VMLs, rather than only on diagrammatic VMLs, enhancing the versatility of the architecture on the input side; (2) we enable the use of solvers from a third paradigm, enhancing the versatility of the architecture on the output side; and, (3) we present the results of scalability performance experiments of an implementation of this architecture. These results have been achieved without significantly altering the architecture, demonstrating its agnosticism with respect to specific VMLs and solvers. It also shows that it can underlie the implementation of practical variability reasoning tools that scale up to real sized variability model analysis and configuration needs.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {126–137},
numpages = {12},
keywords = {Automated Reasoning, Configuration Automation, Generic Architecture, Software Product Lines},
location = {Linz, Austria},
series = {MODELS '24}
}

@inproceedings{10.1145/2362536.2362547,
author = {Johansen, Martin Fagereng and Haugen, \O{}ystein and Fleurey, Franck},
title = {An algorithm for generating t-wise covering arrays from large feature models},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362547},
doi = {10.1145/2362536.2362547},
abstract = {A scalable approach for software product line testing is required due to the size and complexity of industrial product lines. In this paper, we present a specialized algorithm (called ICPL) for generating covering arrays from feature models. ICPL makes it possible to apply combinatorial interaction testing to software product lines of the size and complexity found in industry. For example, ICPL allows pair-wise testing to be readily applied to projects of about 7,000 features and 200,000 constraints, the Linux Kernel, one of the largest product lines where the feature model is available. ICPL is compared to three of the leading algorithms for t-wise covering array generation. Based on a corpus of 19 feature models, data was collected for each algorithm and feature model when the algorithm could finish 100 runs within three days. These data are used for comparing the four algorithms. In addition to supporting large feature models, ICPL is quick, produces small covering arrays and, even though it is non-deterministic, produces a covering array of a similar size within approximately the same time each time it is run with the same feature model.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {46–55},
numpages = {10},
keywords = {testing, product lines, feature models, combinatorial interaction testing},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1145/2853073.2853082,
author = {Soujanya, K. L.S. and AnandaRao, A.},
title = {A Generic Framework for Configuration Management of SPL and Controlling Evolution of Complex Software Products},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853082},
doi = {10.1145/2853073.2853082},
abstract = {Efficient configuration management system is crucial for the success of any software product line (SPL). Due to ever changing needs of customers, SPL undergoes constant changes that are to be tracked in real time. In the context of customer-driven development, anticipation and change management are to be given paramount importance. It demands implementation of software variability that drives home changed, extended and customized configurations besides economy at scale. Moreover, the emergence of distributed technologies, the unprecedented growth of component based, serviceoriented systems throw ever increasing challenges to software product line configuration management. Derivation of a new product is a dynamic process in software product line that should consider functionality and quality attributes. Very few approaches are found on configuration management (CM) of SPL though CM is enough matured for traditional products. They are tailor made and inadequate to provide a general solution. Stated differently, a comprehensive approach for SPL configuration management and product derivation is still to be desired. In this paper, we proposed a framework that guides in doing so besides helping in SPL definitions in generic way. Our framework facilitates SPL configuration management and product derivation based on critical path analysis, weight computation and feedback. We proposed two algorithms namely Quality Driven Product Derivation (QDPD) and Composition Analysis algorithm for generating satisfied compositions and to find best possible composition respectively. The usage of weights and critical path analysis improves quality of product derivation. The framework is extensible and flexible thus it can be leveraged with variability-aware design patterns and ontology. We built a prototype that demonstrates the proof of concept. We tested our approach with Dr. School product line. The results reveal that the framework supports configuration management of SPL and derivation of high quality product in the product line. We evaluated results with ground truth to establish significance of our implementation},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–10},
numpages = {10},
keywords = {weighted approach, product derivation, critical path analysis, configuration management, Software product line}
}

@inproceedings{10.1145/3425174.3425211,
author = {Ferreira, Thiago do Nascimento and Vergilio, Silvia Regina and Kessentini, Marouane},
title = {Applying Many-objective Algorithms to the Variability Test of Software Product Lines},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425211},
doi = {10.1145/3425174.3425211},
abstract = {The problem known as Variability Test of Software Product Line (VTSPL) is related to the selection of the most representative products for the SPL testing. This is an optimization problem because a possible exponential number of products can be derived from the SPL variability model, such as the Feature Model (FM). In the literature many works are dedicated to this research subject, each one applying a different search-based algorithm and using distinct criteria. However, there is no study encompassing all these criteria at the same time. To this end, this paper investigates the use of two Many-Objective Evolutionary Algorithms (MaOEAs). We apply the algorithm NSGA-III, widely used for many-objective algorithms, and the algorithm PCA-NSGA-II, a reduction dimensionality algorithm, which uses the Principal-Component Analysis (PCA) in combination with NSGA-II, to evaluate the objectives used in the literature for the VTSPL problem. PCA-NSGA-II reduces the search space dimensionality by eliminating the redundant objectives. The analysis shows the importance of some objectives such as the number of alive mutants, similarity between products, and unselected features. NSGA-III reaches the best results regarding the quality indicators for all instances, but taking a longer time. Besides, PCA-NSGA-II can find different solutions in the search space that are not found by NSGA-III.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {11–20},
numpages = {10},
keywords = {many-objective problems, dimensionality reduction, Software product line testing},
location = {Natal, Brazil},
series = {SAST '20}
}

@inproceedings{10.1145/2934466.2934485,
author = {Lape\~{n}a, Ra\'{u}l and Ballarin, Manuel and Cetina, Carlos},
title = {Towards clone-and-own support: locating relevant methods in legacy products},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934485},
doi = {10.1145/2934466.2934485},
abstract = {Clone-and-Own (CAO) is a common practice in families of software products consisting of reusing code from methods in legacy products in new developments. In industrial scenarios, CAO consumes high amounts of time and effort without guaranteeing good results. We propose a novel approach, Computer Assisted CAO (CACAO), that given the natural language requirements of a new product, and the legacy products from that family, ranks the legacy methods in the family for each of the new product requirements according to their relevancy to the new development. We evaluated our approach in the industrial domain of train control software. Without CACAO, software engineers tasked with the development of a new product had to manually review a total of 2200 methods in the family. Results show that CACAO can reduce the number of methods to be reviewed, and guide software engineers towards the identification of relevant legacy methods to be reused in the new product.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {194–203},
numpages = {10},
keywords = {software reuse, families of software products, clone and own},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2934466.2946045,
author = {Noir, J\'{e}rome Le and Madel\'{e}nat, S\'{e}bastien and Gailliard, Gr\'{e}gory and Labreuche, Christophe and Acher, Mathieu and Barais, Olivier and Constant, Olivier},
title = {A decision-making process for exploring architectural variants in systems engineering},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946045},
doi = {10.1145/2934466.2946045},
abstract = {In systems engineering, practitioners shall explore numerous architectural alternatives until choosing the most adequate variant. The decision-making process is most of the time a manual, time-consuming, and error-prone activity. The exploration and justification of architectural solutions is ad-hoc and mainly consists in a series of tries and errors on the modeling assets. In this paper, we report on an industrial case study in which we apply variability modeling techniques to automate the assessment and comparison of several candidate architectures (variants). We first describe how we can use a model-based approach such as the Common Variability Language (CVL) to specify the architectural variability. We show that the selection of an architectural variant is a multi-criteria decision problem in which there are numerous interactions (veto, favor, complementary) between criteria.We present a tooled process for exploring architectural variants integrating both CVL and the MYRIAD method for assessing and comparing variants based on an explicit preference model coming from the elicitation of stakeholders' concerns. This solution allows understanding differences among variants and their satisfactions with respect to criteria. Beyond variant selection automation improvement, this experiment results highlight that the approach improves rationality in the assessment and provides decision arguments when selecting the preferred variants.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {277–286},
numpages = {10},
keywords = {systems engineering, multi-criteria decision analysis, model-driven engineering, design exploration, decision-making, architecture},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3425269.3425278,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Supporting user preferences in search-based product line architecture design using Machine Learning},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425278},
doi = {10.1145/3425269.3425278},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA design requires intensive human effort as it involves several conflicting factors. In order to support this task, an interactive search-based approach, automated by a tool named OPLA-Tool, was proposed in a previous work. Through this tool the software architect evaluates the generated solutions during the optimization process. Considering that evaluating PLA is a complex task and search-based algorithms demand a high number of generations, the evaluation of all solutions in all generations cause human fatigue. In this work, we incorporated in OPLA-Tool a Machine Learning (ML) model to represent the architect in some moments during the optimization process aiming to decrease the architect's effort. Through the execution of a quantiqualitative exploratory study it was possible to demonstrate the reduction of the fatigue problem and that the solutions produced at the end of the process, in most cases, met the architect's needs.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Product Line Architecture, Machine Learning, Human-computer interaction},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1145/3106195.3106207,
author = {Li, Yang and Schulze, Sandro and Saake, Gunter},
title = {Reverse Engineering Variability from Natural Language Documents: A Systematic Literature Review},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106207},
doi = {10.1145/3106195.3106207},
abstract = {Identifying features and their relations (i.e., variation points) is crucial in the process of migrating single software systems to software product lines (SPL). Various approaches have been proposed to perform feature extraction automatically from different artifacts, for instance, feature location in legacy code. Usually such approaches a) omit variability information and b) rely on artifacts that reside in advanced phases of the development process, thus, being only of limited usefulness in the context of SPLs. In contrast, feature and variability extraction from natural language (NL) documents is more favorable, because a mapping to several other artifacts is usually established from the very beginning. In this paper, we provide a multi-dimensional overview of approaches for feature and variability extraction from NL documents by means of a systematic literature review (SLR). We selected 25 primary studies and carefully evaluated them regarding different aspects such as techniques used, tool support, or accuracy of the results. In a nutshell, our key insights are that i) standard NLP techniques are commonly used, ii) post-processing often includes clustering &amp; machine learning algorithms, iii) only in rare cases, the approaches support variability extraction, iv) tool support, apart from text pre-processing is often not available, and v) many approaches lack a comprehensive evaluation. Based on these observations, we derive future challenges, arguing that more effort need to be invested for making such approaches applicable in practice.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {133–142},
numpages = {10},
keywords = {Variability Extraction, Systematic Literature Review, Software Product Lines, Reverse Engineering, Natural Language Documents, Feature Identification},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2362536.2362551,
author = {Tischer, Christian and Boss, Birgit and M\"{u}ller, Andreas and Thums, Andreas and Acharya, Rajneesh and Schmid, Klaus},
title = {Developing long-term stable product line architectures},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362551},
doi = {10.1145/2362536.2362551},
abstract = {Product lines are usually built for the long term in order to repay the initial investment. While long-term stable software systems are already hard, if they are developed individually, it is even harder for complete product lines. At the time a new product line is created, the details of future product line characteristics are typically not known, no matter how well and detailed scoping and planning is done. Thus, any product line needs to evolve and adapt over time to incorporate new customer requirements as well as new technology constraints.Stability of the product line architecture is very important to the successful long-term evolution of a product line. In this paper, we discuss how a form of domain decomposition, which we call conceptual architecture, can be used to guide product line engineering towards long-term viability. We will illustrate this approach in the context of a large-scale product line development and analyze the evolution properties of the product line. Transferability of the approach is suggested to other embedded software systems that drive mature, well-understood physical control system.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {86–95},
numpages = {10},
keywords = {software product lines, software architecture, scoping, multi product lines, AUTOSAR},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2019136.2019152,
author = {Kozuka, Nobuaki and Ishida, Yuzo},
title = {Building a product line architecture for variant-rich enterprise applications using a data-oriented approach},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019152},
doi = {10.1145/2019136.2019152},
abstract = {IT industry in Japan has grown by providing specific made-to-order enterprise applications for various industries. Most of enterprise applications are built upon relational database management system (RDBMS), which takes the responsibility of keeping data integrity and data manipulation. However, data explosion in recent years especially in retail and telecommunication industries makes IT industry difficult to satisfy quality attributes such as scalability, availability and data consistency with traditional development techniques. From the beginning of this century, NRI has built and refined product line architecture as a primary core asset for such data intensive industries, which have very rich variations in functional and nonfunctional requirements of their enterprise applications. This paper summarizes key criteria to build such an architecture based on our ten years experience in developing dozens of mission critical IT systems as product families for those industries.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {14},
numpages = {6},
keywords = {relational database management system, quality attributes, product line architecture, enterprise applications, data oriented approach, data intensiveness, core asset development},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2934466.2934475,
author = {Sousa, Gustavo and Rudametkin, Walter and Duchien, Laurence},
title = {Extending feature models with relative cardinalities},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934475},
doi = {10.1145/2934466.2934475},
abstract = {Feature modeling is widely used to capture and manage commonalities and variabilities in software product lines. Cardinality-based feature models are used when variability applies not only to the selection or exclusion of features but also to the number of times a feature can be included in a product. Feature cardinalities are usually considered to apply in either a local or global scope. However, we have identified that these interpretations are insufficient to capture the variability of cloud environments. In this paper, we redefine cardinality-based feature models to allow multiple relative cardinalities between features and we discuss the effects of relative cardinalities on feature modeling semantics, consistency and cross-tree constraints. To evaluate our approach we conducted an analysis of relative cardinalities in four cloud computing providers. In addition, we developed tools for reasoning on feature models with relative cardinalities and performed experiments to verify the performance and scalability of the approach. The results from our study indicate that extending feature models with relative cardinalities is feasible and improves variability modeling, particularly in the case of cloud environments.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {79–88},
numpages = {10},
keywords = {feature model, constraints, cardinality},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2791060.2791102,
author = {Mu\~{n}oz-Fern\'{a}ndez, Juan C. and Tamura, Gabriel and Raicu, Irina and Mazo, Ra\'{u}l and Salinesi, Camille},
title = {REFAS: a PLE approach for simulation of self-adaptive systems requirements},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791102},
doi = {10.1145/2791060.2791102},
abstract = {Model simulation has demonstrated its usefulness in evaluation and decision-making for improving preliminary versions of artefacts before production. Particularly, one of the main goals of simulation is to verify model properties based on data collected from its execution. In this paper, we present the simulation capabilities of our REFAS framework for specifying requirements models for dynamic software products lines and self-adaptive systems. The simulation is controlled by a feedback loop and a reasoning engine that operates on the functional and non-functional requirements. The paper contribution is threefold. First, REFAS allows developers to evaluate and improve requirements models through their simulation capabilities. Second, REFAS provides rich feedback in its interactive simulations for the human modeller to make informed decisions to improve her model. Third, REFAS automates the generation of simulation scenarios required to verify the model adequacy and correctness. We evaluate our contribution by comparing the application of REFAS to a case study used in other approaches.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {121–125},
numpages = {5},
keywords = {simulation, requirements engineering, dynamic software product lines, dynamic adaptation, MAPE-K loops},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2791060.2791082,
author = {Hotz, Lothar and Wang, Yibo and Riebisch, Matthias and G\"{o}tz, Olaf and Lackhove, Josef},
title = {Evaluation across multiple views for variable automation systems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791082},
doi = {10.1145/2791060.2791082},
abstract = {Automation systems in industry are often software-intensive systems consisting of software and hardware components. During their development several engineers of different disciplines are involved, such as mechanical, electrical and software engineering. Each engineer focuses on specific system aspects to be developed. To enable an efficient development, product lines especially with feature models for variability modeling are promising technologies. In order to reduce the complexity of both feature models and development process, views on feature models can be applied. The use of views for filtering purposes constitutes an established method. However, views also enable further options missing in current approaches, such as evaluations regarding requirements, including non-functional ones. This paper presents an approach for evaluation across multiple views to enable collaborative development for developers who focus on different system aspects. We validate our approach by applying it in an industrial project for the planning of flying saws.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {311–315},
numpages = {5},
keywords = {product lines, multi-criteria evaluation, feature model, consistency check, configuration, automation systems},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3634713.3634714,
author = {Kodetzki, Maximilian and Bordis, Tabea and Runge, Tobias and Schaefer, Ina},
title = {Partial Proofs to Optimize Deductive Verification of Feature-Oriented Software Product Lines},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634714},
doi = {10.1145/3634713.3634714},
abstract = {Software product lines (SPLs) are a technique to efficiently develop families of software products. Code is implemented in functional features which are composed to individual software variants. SPLs are oftentimes used in safety-critical systems, which is why functional correctness is more important than ever. As an advanced approach, deductive verification offers the possibility to verify the behaviour of software against a formal specification. When deductive verification is applied for SPLs, it meets the challenges of an SPLs variability. Since most verification approaches do not scale for variant-rich product lines, we take up existing approaches of reuse of proof parts to develop our concept of partial proofs. We split proofs into a feature-specific and a product-specific part. The feature-specific part is only proven once for all products enabling advanced proof reuse. We implement our concept of partial proofs in the tool VarCorC and evaluate it on three case studies. We found that both the number of proof steps and the verification time can be reduced by using partial proofs. Further, we determine a trend of increasing improvements of verification costs for large-scale SPLs.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {17–26},
numpages = {10},
keywords = {deductive verification, formal methods, software product lines},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.1145/2791060.2791072,
author = {Patel, Sachin and Shah, Vipul},
title = {Automated testing of software-as-a-service configurations using a variability language},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791072},
doi = {10.1145/2791060.2791072},
abstract = {The benefits offered by cloud technologies have compelled enterprises to adopt the Software-as-a-Service (SaaS) model for their enterprise software needs. A SaaS has to be configured or customized to suit the specific requirements of every enterprise that subscribes to it. IT service providers have to deal with the problem of testing many such configurations created for different enterprises. The software gets upgraded periodically and the configurations need to be tested on an ongoing basis to ensure business continuity. In order to run the testing organization efficiently, it is imperative that the test cycle is automated. Developing automated test scripts for a large number of configurations is a non-trivial task because differences across them may range from a few user interface changes to business process level changes. We propose an approach that combines the benefits of model driven engineering and variability modeling to address this issue. The approach comprises of the Enterprise Software Test Modeling Language to model the test cases. We use the Common Variability Language to model variability in the test cases and apply model transformations on a base model to generate a test model for each configuration. These models are used to generate automated test scripts for all the configurations. We describe the test modelling language and an experiment which shows that the approach can be used to automatically generate variations in automated test scripts.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {253–262},
numpages = {10},
keywords = {variability specification, test automation, software-as-a-service, model based testing, enterprise software testing},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2934466.2934477,
author = {Krieter, Sebastian and Schr\"{o}ter, Reimar and Th\"{u}m, Thomas and Fenske, Wolfram and Saake, Gunter},
title = {Comparing algorithms for efficient feature-model slicing},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934477},
doi = {10.1145/2934466.2934477},
abstract = {Feature models are a well-known concept to represent variability in software product lines by defining features and their dependencies. During feature-model evolution, for information hiding, and for feature-model analyses, it is often necessary to remove certain features from a model. As the crude deletion of features can have undesirable effects on their dependencies, dependency-preserving algorithms, known as feature-model slicing, have been proposed. However, current algorithms do not perform well when removing a high number of features from large feature models. Therefore, we propose an efficient algorithm for feature-model slicing based on logical resolution and the minimization of logical formulas. We empirically evaluate the scalability of our algorithm on a number of feature models and find that our algorithm generally outperforms existing algorithms.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {60–64},
numpages = {5},
keywords = {software product lines, feature-model evolution, feature-model analyses},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3302333.3302340,
author = {Krieter, Sebastian and Thiem, Tobias and Leich, Thomas},
title = {Using Dynamic Software Product Lines to Implement Adaptive SGX-enabled Systems},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302340},
doi = {10.1145/3302333.3302340},
abstract = {In the light of computational outsourcing and external data storage, data protection and trusted execution become increasingly important. Novel hardware such as Intel's Software Guard extensions (SGX) attempts to provide a solution to protect data and computations from unauthorized access and manipulation, even against attackers with physical access to a machine. However, the current generation of SGX limits the protected memory space that can be efficiently used to 128 MiB, which must be shared between data and binary code. Thus, we propose to use a software product line approach to tailor an application's binary code in such a way that it can be updated during runtime, with the goal to only store relevant features in the protected memory at a given time. We provide a prototypical implementation that enables basic support for loading and unloading features during runtime and evaluate our prototype in terms of execution times against non-adaptive execution.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {9},
numpages = {9},
keywords = {Software Product Lines, Runtime Adaptation, Intel Software Guard Extensions},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/3663529.3663851,
author = {Zellmer, Philipp and Kr\"{u}ger, Jacob and Leich, Thomas},
title = {Decision Making for Managing Automotive Platforms: An Interview Survey on the State-of-Practice},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663851},
doi = {10.1145/3663529.3663851},
abstract = {The automotive industry is changing due to digitization, a growing focus on software, and the increasing use of electronic control units. Consequently, automotive engineering is shifting from hardware-focused towards software-focused platform concepts to address these challenges. This shift includes adopting and integrating methods like electrics/electronics platforms, software product-line engineering, and product generation. Although these concepts are well-known in their respective research fields and different industries, there is limited research on their practical effectiveness and issues—particularly when implementing and using these concepts for modern automotive platforms. The lack of research and practical experiences challenges particularly decision makers, who cannot build on reliable evidence or techniques. In this paper, we address this gap by reporting on the state-of-practice of supporting the decision making for managing automotive electrics/electronics platforms, which integrate hardware, software, and electrics/electronics artifacts. For this purpose, we conducted 26 interviews with experts from the automotive domain. We derived questions from a previous mapping study in which we collected current research on product-structuring concepts, aiming to derive insights on the consequent practical challenges and requirements. Specifically, we contribute an overview of the requirements and criteria for (re)designing the decision-making process for managing electrics/electronics platforms within the automotive domain from the practitioners’ view. Through this, we aim to assist practitioners in managing electrics/electronics platforms, while also providing starting points for future research on a real-world problem.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {318–328},
numpages = {11},
keywords = {automotive, cyber-physical system, decision making, electrics/electronics, life-cycle management, platform management, product line, product structuring concept},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/2499777.2499779,
author = {Antkiewicz, Micha\l{} and B\k{a}k, Kacper and Murashkin, Alexandr and Olaechea, Rafael and Liang, Jia Hui (Jimmy) and Czarnecki, Krzysztof},
title = {Clafer tools for product line engineering},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499779},
doi = {10.1145/2499777.2499779},
abstract = {Clafer is a lightweight yet expressive language for structural modeling: feature modeling and configuration, class and object modeling, and metamodeling. Clafer Tools is an integrated set of tools based on Clafer. In this paper, we describe some product-line variability modeling scenarios of Clafer Tools from the viewpoints of product-line owner, product-line engineer, and product engineer.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {130–135},
numpages = {6},
keywords = {clafer configurator, ClaferWiki, ClaferMOO visualizer, ClaferMOO, ClaferIG, Clafer},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3622748.3622750,
author = {Arasaki, Caio and Wolschick, Lucas and Freire, Willian and Amaral, Aline},
title = {Feature selection in an interactive search-based PLA design approach},
year = {2023},
isbn = {9798400709524},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622748.3622750},
doi = {10.1145/3622748.3622750},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line (SPL). PLA design can be formulated as an interactive optimization problem with many conflicting factors. Incorporating Decision Makers’ (DM) preferences during the search process may help the algorithms find more adequate solutions for their profiles. Interactive approaches allow the DM to evaluate solutions, guiding the optimization according to their preferences. However, this brings up human fatigue problems caused by the excessive amount of interactions and solutions to evaluate. A common strategy to prevent this problem is limiting the number of interactions and solutions evaluated by the DM. Machine Learning (ML) models were also used to learn how to evaluate solutions according to the DM profile and replace them after some interactions. Feature selection performs an essential task as non-relevant and/or redundant features used to train the ML model can reduce the accuracy and comprehensibility of the hypotheses induced by ML algorithms. This work aims to select features of an ML model used to prevent human fatigue in an interactive search-based PLA design approach. We applied four selectors and through results we were able to reduce 30% of features, obtaining an accuracy of 99%.},
booktitle = {Proceedings of the 17th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Machine Learning, Interactive search-based Software Engineering, Feature Selection},
location = {Campo Grande, Brazil},
series = {SBCARS '23}
}

@inproceedings{10.1145/2647908.2655957,
author = {Murguzur, Aitor and Capilla, Rafael and Trujillo, Salvador and Ortiz, \'{O}scar and Lopez-Herrejon, Roberto E.},
title = {Context variability modeling for runtime configuration of service-based dynamic software product lines},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655957},
doi = {10.1145/2647908.2655957},
abstract = {In emerging domains such as Cloud-based Industrial Control Systems (ICSs) and SCADA systems where data-intensive and high performance computing are needed, a higher degree of flexibility is being demanded to meet new stakeholder requirements, context changes and intrinsic complexity. In this light, Dynamic Software Product Lines (DSPLs) provide a way to build self-managing systems exploiting traditional product line engineering concepts at runtime. Although context-awareness is widely perceived to be a first-class concern in such runtime variability mechanisms, existing approaches do not provide the necessary level of formalization to model and enact context variability for DSPLs. This is crucial for operational analytics processes since variant configuration could differ from context to context depending on diverse data values linked to context features and cross-tree constraints in a feature model. In this paper, we propose a context variability modeling approach, demonstrate its applicability and usability via a wind farm use case, and present the fundamental building blocks of a framework for enabling context variability in service-based DSPLs which provide Workflow as a Service (WFaaS).},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {2–9},
numpages = {8},
keywords = {process variability, data-aware systems, context variability, context awareness},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2647908.2655977,
author = {El Yamany, Ahmed Eid and Shaheen, Mohamed and Sayyad, Abdel Salam},
title = {OPTI-SELECT: an interactive tool for user-in-the-loop feature selection in software product lines},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655977},
doi = {10.1145/2647908.2655977},
abstract = {Opti-Select is an Interactive Multi-objective feature analysis and optimization tool for software product lines configuration and feature models optimization based on an innovative UIL (User-In-the-loop) idea. In this tool, the experience of system analysts and stakeholders are merged with optimization techniques and algorithms.Opti-Select interactive tool is an integrated set of techniques providing step by step feature model and attribute configuration, selecting and excluding features, solution set optimization, and user interaction utilities that can all together reach satisfactory set of solutions that fits stakeholder preferences.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {126–129},
numpages = {4},
keywords = {user-in-the-loop (UIL), software product lines, search-based software engineering, product line engineering, optimal variant, optimal feature selection, multi-objective optimization, modeling, features, feature models, feature modeling, exploration, Pareto front visualization},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2019136.2019146,
author = {Gerlach, Simon},
title = {Improving efficiency when deriving numerous products from software product lines simultaneously},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019146},
doi = {10.1145/2019136.2019146},
abstract = {In-car infotainment systems must allow for product differentiation and the adaption to the needs of different markets. Product line approaches are applied because large numbers of different product variants need to be developed simultaneously. During development, updated versions of each projected product variant need to be derived from the product line assets repeatedly. Current build tools create each of the numerous product variants one after another. Accordingly, the creation process can take much time. This paper presents an approach to abbreviate this creation process based on the fact that multiple product variants created at once can have parts in common. To benefit from this optimization potential the workflow that creates an individual product variant is subdivided into multiple fragments. Whenever a set of such product variants needs to be created, an optimization algorithm then calculates an individual execution order of the fragments for this set. This order minimizes the total execution time by a systematic reuse of workflow fragment's results for the creation of multiple different product variants.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {9},
numpages = {4},
keywords = {software product lines, product derivation, product configuration, automotive, application engineering},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2362536.2362557,
author = {Elsner, Christoph},
title = {Light-weight tool support for staged product derivation},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362557},
doi = {10.1145/2362536.2362557},
abstract = {Tool support that checks for configuration errors and generates product parts from configurations can significantly improve on product derivation in product line engineering. Up to now, however, derivation tools commonly disregard the staged derivation process. They do not restrict configuration consistency checks to process entities such as configuration stages, stakeholders, or build tasks. As a result, constraints that are only valid for certain process entities must either be checked permanently, leading to false positive errors, or one must refrain from defining them at all.This paper contributes a light-weight approach to provide tailored tool support for staged product derivation. Compared to previous approaches, it is not tied to a single configuration mechanism (e.g., feature modeling), and also accounts for the stakeholders involved and the build tasks that generate product parts. First, the product line engineer describes the derivation process in a concise model. Then, based on constraint checks on the configuration (e.g., a feature model configuration) that are linked to the modeled entities, comprehensive tool support can be provided: Configuration actions can be guided and restricted depending on the configuring stakeholder in a fine-grained manner, and constraints attached to a build task will only be checked if it actually shall be executed. Finally, in combination with previous work, the paper provides evidence that the approach is applicable to legacy product lines in a light-weight manner and that it technically scales to thousands of constraint checks.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {146–155},
numpages = {10},
keywords = {tool support, staged product derivation, product line},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2019136.2019159,
author = {Otsuka, Jun and Kawarabata, Kouichi and Iwasaki, Takashi and Uchiba, Makoto and Nakanishi, Tsuneo and Hisazumi, Kenji},
title = {Small inexpensive core asset construction for large gainful product line development: developing a communication system firmware product line},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019159},
doi = {10.1145/2019136.2019159},
abstract = {Product line development of communication system firmware with more than 2,000 features was performed in a large-scale project that involved more than 300 engineers (at a maximum) across four distributed sites. However, since intense demands to reduce development costs and time made it prohibitive to construct core assets for all those identified features, the project screened a limited number of the features, for which core assets were constructed, and then performed partial application of product line engineering. Nevertheless, when compared with previously engineered derivative developments, when the second product of the product line was released, it was clear that the project had achieved significant improvements in quality, as well as reductions in development costs and time requirements. Automatic code generation also contributed to those improvements.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {20},
numpages = {5},
keywords = {product line, feature modeling, core assets, case study},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.5555/1753235.1753249,
author = {Montagud, Sonia and Abrah\~{a}o, Silvia},
title = {Gathering current knowledge about quality evaluation in software product lines},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Recently, a number of methods and techniques for assessing the quality of software product lines have been proposed. However, to the best of our knowledge, there is no study which summarizes all the existing evidence about them. This paper presents a systematic review that investigates what methods and techniques have been employed (in the last 10 years) to evaluate the quality of software product lines and how they were employed. A total of 39 research papers have been reviewed from an initial set of 1388 papers. The results show that 25% of the papers reported evaluations at the Design phase of the Domain Engineering phase. The most widely used mechanism for modeling quality attributes was extended feature models and the most evaluated artifact was the base architecture. In addition, the results of the review have identified several research gaps. Specifically, 77% of the papers employed case studies as a "proof of concept" whereas 23% of the papers did not perform any type of validation. Our results are particularly relevant in positioning new research activities and in the selection of quality evaluation methods or techniques that best fit a given purpose.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {91–100},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/3321408.3326676,
author = {Yan, Liu and Hu, Wenxin and Han, Longzhe},
title = {Optimize SPL test cases with adaptive simulated annealing genetic algorithm},
year = {2019},
isbn = {9781450371582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321408.3326676},
doi = {10.1145/3321408.3326676},
abstract = {In Software Product Line (SPL) testing, reduced test suite with high coverage is useful for early features interaction detection. sGA (simplified genetic algorithm) and SAGA(simulated annealing genetic algorithm) can generate high coverage test suite. However, small probability mutations in updating test suite may reduce search efficiency and thus miss better solutions. An improved test cases generation method based on ASAGA (Adaptive simulated annealing genetic algorithm) is proposed. Experiments on SPLOT (Software Product Lines Online Tools) feature models show that the proposed hybrid ASAGA method can ensure local optimization accuracy and achieve smaller-size test suite with higher coverage.},
booktitle = {Proceedings of the ACM Turing Celebration Conference - China},
articleno = {148},
numpages = {7},
keywords = {test case, software test, similarity measurement, feature model, ASAGA},
location = {Chengdu, China},
series = {ACM TURC '19}
}

@inproceedings{10.1145/2648511.2648528,
author = {Barreiros, Jorge and Moreira, Ana},
title = {A cover-based approach for configuration repair},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648528},
doi = {10.1145/2648511.2648528},
abstract = {Feature models are often used to describe variability and commonality in Software Product Lines, specifying admissible configurations of valid products. However, invalid configurations may arise in some scenarios. These include feature model evolution that invalidates pre-existing products or collaborative configuration by multiple stakeholders with conflicting goals, among others. This problem has been acknowledged in the literature and some techniques for configuration repair have already been proposed. However, common optimization criteria such as proximity between original and repaired configurations can result in a significant number of alternative repair possibilities, easily attaining thousands of alternatives for models of practical dimension. Consequently, rather than just efficiently providing an exhaustive list of possibilities, an approach that specifically addresses this issue should be able to offer the user a manageable and comprehensible view of the configuration problems and potential repair options. We offer a novel approach for configuration repair, based on partitioning and cover analysis, with high performance and generating high quality solutions, which allows efficient identification and presentation of multiple competing repairs.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {157–166},
numpages = {10},
keywords = {software product lines, feature modeling, configuration repair, configuration diagnosis, configuration},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2791060.2791070,
author = {Liang, Jia Hui and Ganesh, Vijay and Czarnecki, Krzysztof and Raman, Venkatesh},
title = {SAT-based analysis of large real-world feature models is easy},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791070},
doi = {10.1145/2791060.2791070},
abstract = {Modern conflict-driven clause-learning (CDCL) Boolean SAT solvers provide efficient automatic analysis of real-world feature models (FM) of systems ranging from cars to operating systems. It is well-known that solver-based analysis of real-world FMs scale very well even though SAT instances obtained from such FMs are large, and the corresponding analysis problems are known to be NP-complete. To better understand why SAT solvers are so effective, we systematically studied many syntactic and semantic characteristics of a representative set of large real-world FMs. We discovered that a key reason why large real-world FMs are easy-to-analyze is that the vast majority of the variables in these models are unrestricted, i.e., the models are satisfiable for both true and false assignments to such variables under the current partial assignment. Given this discovery and our understanding of CDCL SAT solvers, we show that solvers can easily find satisfying assignments for such models without too many backtracks relative to the model size, explaining why solvers scale so well. Further analysis showed that the presence of unrestricted variables in these real-world models can be attributed to their high-degree of variability. Additionally, we experimented with a series of well-known nonbacktracking simplifications that are particularly effective in solving FMs. The remaining variables/clauses after simplifications, called the core, are so few that they are easily solved even with backtracking, further strengthening our conclusions. We explain the connection between our findings and backdoors, an idea posited by theorists to explain the power of SAT solvers. This connection strengthens our hypothesis that SAT-based analysis of FMs is easy. In contrast to our findings, previous research characterizes the difficulty of analyzing randomly-generated FMs in terms of treewidth. Our experiments suggest that the difficulty of analyzing real-world FMs cannot be explained in terms of treewidth.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {91–100},
numpages = {10},
keywords = {feature model, SAT-based analysis},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1145/2580950,
author = {Th\"{u}m, Thomas and Apel, Sven and K\"{a}stner, Christian and Schaefer, Ina and Saake, Gunter},
title = {A Classification and Survey of Analysis Strategies for Software Product Lines},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2580950},
doi = {10.1145/2580950},
abstract = {Software-product-line engineering has gained considerable momentum in recent years, both in industry and in academia. A software product line is a family of software products that share a common set of features. Software product lines challenge traditional analysis techniques, such as type checking, model checking, and theorem proving, in their quest of ensuring correctness and reliability of software. Simply creating and analyzing all products of a product line is usually not feasible, due to the potentially exponential number of valid feature combinations. Recently, researchers began to develop analysis techniques that take the distinguishing properties of software product lines into account, for example, by checking feature-related code in isolation or by exploiting variability information during analysis. The emerging field of product-line analyses is both broad and diverse, so it is difficult for researchers and practitioners to understand their similarities and differences. We propose a classification of product-line analyses to enable systematic research and application. Based on our insights with classifying and comparing a corpus of 123 research articles, we develop a research agenda to guide future research on product-line analyses.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {6},
numpages = {45},
keywords = {type checking, theorem proving, static analysis, software product line, software analysis, program family, model checking, Product-line analysis}
}

@inproceedings{10.1145/2362536.2362560,
author = {Lettner, Daniela and Vierhauser, Michael and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {Supporting end users with business calculations in product configuration},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362560},
doi = {10.1145/2362536.2362560},
abstract = {Business calculations like break-even, return on investment, or cost are essential in many domains to support decision making while configuring products. For instance, customers and sales people need to estimate and compare the business value of different product variants. Some product line approaches provide initial support, e.g., by defining quality attributes in relation to features. However, an approach that allows domain engineers to easily define business calculations together with variability models is still lacking. In product configuration, calculation results need to be instantly presented to end users after making configuration choices. Further, due to the often high number of calculations, the presentation of calculation results to end users can be challenging. These challenges cannot be addressed by integrating off-the-shelf applications performing the calculations with product line tools. We thus present an approach based on dedicated calculation models that are related to variability models. Our approach seamlessly integrates business calculations with product configuration and provides support for formatting calculations and calculation results. We use the DOPLER tool suite to deploy calculations together with variability models to end users in product configuration. We evaluate the expressiveness and practical relevance of the approach by investigating the development of business calculations for 15 product lines from the domain of industrial automation.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {171–180},
numpages = {10},
keywords = {variability models, product configuration, business calculations},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3652620.3687812,
author = {Ochs, Philip and Pett, Tobias and Schaefer, Ina},
title = {Consistency Is Key: Can Your Product Line Realise What It Models?},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687812},
doi = {10.1145/3652620.3687812},
abstract = {Nowadays, automotive systems are modelled as cyber-physical product lines. However, often it is not clear whether a modelled configuration can be realised as a physical product. The combination of software- and hardware-artefacts harbours the risk of non-functioning products due to incompatible resource demands and provisionings of the components installed (realisability), thus can result in high financial loss for manufacturers. With new business models, such as over-the-air updates, they also face this risk for products already in the field, because it remains unclear whether a vehicle is still functioning after an update where resource demands have changed (update-ability). Manually analysing realisability and update-ability is infeasible in practice, as the number of product variants in a product line grows combinatorially (with respect to the number of configuration options). In this paper, we approach this challenge by proposing a novel baseline approach for the analysis of realisability and update-ability in automotive cyber-physical systems. We formally model resource demands and resource provisionings as well as the construction of a resource allocation problem per product variant based on constraint satisfaction problems. In the evaluation, we apply our method to an automotive case study by modelling realisation artefacts and investigate its feasibility and performance. Our results show that a non-realisable configuration of the product line can be identified in 206 ms median time. Finally, we discuss limitations and extensions of our ongoing work.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {690–699},
numpages = {10},
keywords = {product line engineering, cyber-physical systems, product variant analysis, product line consistency, realisability analysis},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/2364412.2364435,
author = {Saller, Karsten and Oster, Sebastian and Sch\"{u}rr, Andy and Schroeter, Julia and Lochau, Malte},
title = {Reducing feature models to improve runtime adaptivity on resource limited devices},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364435},
doi = {10.1145/2364412.2364435},
abstract = {Mobile devices like smartphones are getting increasingly important in our daily lifes. They are used in various environments and have to dynamically adapt themselves accordingly in order to provide an optimal runtime behavior. Naturally, adapting to continuously changing environmental conditions is a challenging task because mobile devices are always limited in their resources and have to adapt in real-time. In this paper, we introduce an approach that enables resource limited devices to adapt to changing conditions using dynamic software product lines techniques. Therefore, feature models are reduced to a specific hardware context before installing the adaptive mobile application on the device. This reduces the amount of possible configurations that are compatible with the device and, thereby, minimizes the costs and the duration of an adaptation during runtime.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {135–142},
numpages = {8},
keywords = {feature models, dynamic software product lines, context-awareness, adaptive systems},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.5555/2819009.2819244,
author = {Rubin, Julia and Botterweck, Goetz and Pleuss, Andreas and Weiss, David},
title = {5th international workshop on product line approaches in software engineering PLE for a sustainable society (PLEASE 2015)},
year = {2015},
publisher = {IEEE Press},
abstract = {This paper summarizes the motivation, objectives, and format of the 5th International Workshop on Product LinE Approaches in Software Engineering (PLEASE15). The main goal of the PLEASE workshop series is to encourage and promote the adoption of Software Product Line Engineering. This year's edition focuses on the link between software product line engineering (SPLE) and new challenges posed by emerging societal trends. Towards this end, we invited reports on (1) opportunities posed by societal challenges for SPLE research and practice and (2) concrete solutions exemplifying application of SPLE techniques to societal challenges.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {989–990},
numpages = {2},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/2019136.2019161,
author = {Pleuss, Andreas and Rabiser, Rick and Botterweck, Goetz},
title = {Visualization techniques for application in interactive product configuration},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019161},
doi = {10.1145/2019136.2019161},
abstract = {In product line engineering (PLE) a major challenge is the complexity of artifacts that have to be handled. In real-world product lines, variability models can become large and complex comprising thousands of elements with hundreds of non-trivial dependencies. Visual and interactive techniques aim to reduce the (cognitive) complexity and support the user during challenging PLE tasks like product configuration. There are many visualization techniques described in the literature -- e.g., in Software Visualization -- and some isolated techniques have been applied in PLE tools. Nevertheless, the full potential of visualization in the context of PLE has not been exploited so far. This paper provides an overview of (1) available visualization techniques and criteria to judge their benefits and drawbacks for product configuration, (2) which have been applied in product configuration in PLE, and (3) which could be beneficial to support product configuration. We propose a research agenda for future work in visual and interactive PLE techniques.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {22},
numpages = {8},
keywords = {software visualization, product line engineering, product configuration},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3194133.3194143,
author = {Olaechea, Rafael and Atlee, Joanne and Legay, Axel and Fahrenberg, Uli},
title = {Trace checking for dynamic software product lines},
year = {2018},
isbn = {9781450357159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194133.3194143},
doi = {10.1145/3194133.3194143},
abstract = {A key objective of self-adaptive systems is to continue to provide optimal quality of service when the environment changes. A dynamic software product line (DSPL) can benefit from knowing how its various product variants would have performed (in terms of quality of service) with respect to the recent history of inputs. We propose a family-based analysis that simulates all the product variants of a DSPL simultaneously, at runtime, on recent environmental inputs to obtain an estimate of the quality of service that each one of the product variants would have had, provided it had been executing. We assessed the efficiency of our DSPL analysis compared to the efficiency of analyzing each product individually on three case studies. We obtained mixed results due to the explosion of quality-of-service values for the product variants of a DSPL. After introducing a simple data abstraction on the values of quality-of- service variables, our DSPL analysis is between 1.4 and 7.7 times faster than analyzing the products one at a time.},
booktitle = {Proceedings of the 13th International Conference on Software Engineering for Adaptive and Self-Managing Systems},
pages = {69–75},
numpages = {7},
location = {Gothenburg, Sweden},
series = {SEAMS '18}
}

@inproceedings{10.1145/2364412.2364449,
author = {Helvensteijn, Michiel},
title = {Abstract delta modeling: my research plan},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364449},
doi = {10.1145/2364412.2364449},
abstract = {Software product lines are sets of software programs with well defined commonalities and variabilities that are distinguished by which features they support. There is need of a way to organize the underlying code to clearly link features on the feature modeling level to code artifacts on the implementation level, without code duplication or overspecification, so we can support automated product derivation. Existing approaches are still lacking in one way or another. My answer to this problem is delta modeling. My thesis will approach delta modeling from an abstract algebraic perspective called Abstract Delta Modeling. It will give a thorough formal treatment of the subject and extend it in several directions. A workflow for building a product line from scratch, a way to model dynamic product lines as well as plenty of practical examples and case studies.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {217–224},
numpages = {8},
keywords = {type systems, product lines, modal logic, dynamic product lines, development workflow, delta modeling, PhD thesis},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2648511.2648524,
author = {Quinton, Cl\'{e}ment and Pleuss, Andreas and Berre, Daniel Le and Duchien, Laurence and Botterweck, Goetz},
title = {Consistency checking for the evolution of cardinality-based feature models},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648524},
doi = {10.1145/2648511.2648524},
abstract = {Feature-models (fms) are a widely used approach to specify the commonalities and variability in variable systems and software product lines. Various works have addressed edits to fms for fm evolution and tool support to ensure consistency of fms. An important extension to fms are feature cardinalities and related constraints, as extensively used e.g., when modeling variability of cloud computing environments. Since cardinality-based fms pose additional complexity, additional support for evolution and consistency checking with respect to feature cardinalities would be desirable, but has not been addressed yet. In this paper, we discuss common cardinality-based fm edits and resulting inconsistencies based on experiences with fms in cloud domain. We introduce tool-support for automated inconsistency detection and explanation based on an off-the-shelf solver. We demonstrate the feasibility of the approach by an empirical evaluation showing the performance of the tool.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {122–131},
numpages = {10},
keywords = {feature model, edit, consistency, cardinality},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3593663.3593677,
author = {Willert, Nico and Eriksson, Janik},
title = {Towards a feature-based didactic framework for generating individualized programming tasks for an e-learning environment},
year = {2023},
isbn = {9781450399562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593663.3593677},
doi = {10.1145/3593663.3593677},
abstract = {Adaptive programming tasks are a promising approach for personalized learning that adapts to each student’s unique needs and abilities. However, developing effective adaptive programming tasks can be challenging, particularly when it comes to selecting the appropriate changes and adapting the difficulty of the exercise. In this paper, we propose a model for tracking student knowledge and adapting programming exercises to guide the selection and implementation of task features. Our model combines aspects of cognitive load, computational thinking and feature-oriented software product line engineering to identify core and optional features, so that they can be used in conjunction to adapt to the specific needs and abilities of each student. We provide an overview over the insights gained from an exploratory study with students. To support the creation process of feature-based programming tasks, we present an approach using a template-based generator.},
booktitle = {Proceedings of the 5th European Conference on Software Engineering Education},
pages = {246–255},
numpages = {10},
keywords = {instructional design, computational thinking, assessment, adaptivity},
location = {Seeon/Bavaria, Germany},
series = {ECSEE '23}
}

@inproceedings{10.1145/3023956.3023961,
author = {Lity, Sascha and Al-Hajjaji, Mustafa and Th\"{u}m, Thomas and Schaefer, Ina},
title = {Optimizing product orders using graph algorithms for improving incremental product-line analysis},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023961},
doi = {10.1145/3023956.3023961},
abstract = {The individual analysis of each product of a software product line (SPL) leads to redundant analysis steps due to the inherent commonality. Therefore, incremental SPL analyses exploit commonalities and focus on the differences between products to reduce the analysis effort. However, existing techniques are influenced by the order in which products are analyzed. The more similar subsequently analyzed products are, the greater is the potential reduction of the overall analysis effort as similar products imply less differences to be analyzed. Hence, an order of products, where the total number of differences is minimized, facilitates incremental SPL analyses. In this paper, we apply graph algorithms to determine optimized product orders. We capture products as nodes in a graph, where solution-space information defines edge weights between product nodes. We adopt existing heuristics for finding an optimal solution of the traveling salesperson problem to determine a path in the product graph with minimal costs. A path represents an optimized product order w.r.t. minimized differences between all products. We realize a prototype of our approach and evaluate its applicability and performance showing a significant optimization compared to standard and random orders.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {60–67},
numpages = {8},
keywords = {product orders, graph algorithms, delta-oriented software product lines},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/2648511.2648525,
author = {Stein, Jacob and Nunes, Ingrid and Cirilo, Elder},
title = {Preference-based feature model configuration with multiple stakeholders},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648525},
doi = {10.1145/2648511.2648525},
abstract = {Feature model configuration is known to be a hard, error-prone and time-consuming activity. This activity gets even more complicated when it involves multiple stakeholders in the configuration process. Research work has proposed approaches to aid multi-stakeholder feature model configuration, but they rely on systematic processes that constraint decisions of some of the stakeholders. In this paper, we propose a novel approach to improve the multi-stakeholder configuration process, considering stakeholders' preferences expressed through both hard and soft constraints. Based on such preferences, we recommend different product configurations using different strategies from the social choice theory. We conducted an empirical study to evaluate the effectiveness of our strategies with respect to individual stakeholder satisfaction and fairness among all stakeholders. Results indicate that particular strategies perform best with respect to these aspects.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {132–141},
numpages = {10},
keywords = {social choice, preferences, feature model configuration},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3129790.3129818,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green software development and research with the HADAS toolkit},
year = {2017},
isbn = {9781450352178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129790.3129818},
doi = {10.1145/3129790.3129818},
abstract = {Energy is a critical resource, and designing a sustainable software architecture is a non-trivial task. Developers require energy metrics that support sustainable software architectures reflecting quality attributes such as security, reliability, performance, etc., identifying what are the concerns that impact more in the energy consumption. A variability model of different designs and implementations of an energy model should exist for this task, as well as a service that stores and compares the experimentation results of energy and time consumption of each concern, finding out what is the most eco-efficient solution. The experimental measurements are performed by energy experts and researchers that share the energy model and metrics in a collaborative repository. HADAS confronts these tasks modelling and reasoning with the variability of energy consuming concerns for different energy contexts, connecting HADAS variability model with its energy efficiency collaborative repository, establishing a Software Product Line (SPL) service. Our main goal is to help developers to perform sustainability analyses finding out the eco-friendliest architecture configurations. A HADAS toolkit prototype is implemented based on a Clafer model and Choco solver, and it has been tested with several case studies.},
booktitle = {Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings},
pages = {205–211},
numpages = {7},
keywords = {variability, software product line, repository, optimisation, metrics, energy efficiency, clafer, CVL},
location = {Canterbury, United Kingdom},
series = {ECSA '17}
}

@inproceedings{10.1145/2499777.2500716,
author = {Saller, Karsten and Lochau, Malte and Reimund, Ingo},
title = {Context-aware DSPLs: model-based runtime adaptation for resource-constrained systems},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500716},
doi = {10.1145/2499777.2500716},
abstract = {Dynamic Software Product Lines (DSPLs) provide a promising approach for planning and applying runtime reconfiguration scenarios to adaptive software systems. However, applying DSPLs in the vital domain of highly context-aware systems, e.g., mobile devices, is obstructed by the inherently limited resources being insufficient to handle large, constrained (re-)configurations spaces. To tackle these drawbacks, we propose a novel model-based approach for designing DSPLs in a way that allows for a trade-off between precomputation of reconfiguration scenarios at development time and on-demand evolution at runtime. Therefore, we (1) enrich feature models with context information to reason about potential context changes, and (2) specify context-aware reconfiguration processes on the basis of a scalable transition system incorporating state space abstractions and incremental refinement at runtime. We illustrate our concepts by means of a smartphone case study and present an implementation and evaluation considering different trade-off metrics.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {106–113},
numpages = {8},
keywords = {state space reduction, feature models, contexts, adaptive systems, DSPL},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1109/AST.2017.7,
author = {Al-Hajjaji, Mustafa and Kr\"{u}ger, Jacob and Schulze, Sandro and Leich, Thomas and Saake, Gunter},
title = {Efficient product-line testing using cluster-based product prioritization},
year = {2017},
isbn = {9781538615485},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/AST.2017.7},
doi = {10.1109/AST.2017.7},
abstract = {A software product-line comprises a set of products that share a common set of features. These features can be reused to customize a product to satisfy specific needs of certain customers or markets. As the number of possible products increases exponentially for new features, testing all products is infeasible. Existing testing approaches reduce their effort by restricting the number of products (sampling) and improve their effectiveness by considering the order of tests (prioritization). In this paper, we propose a cluster-based prioritization technique to sample similar products with respect to the feature selection. We evaluate our approach using feature models of different sizes and show that cluster-based prioritization can enhance the effectiveness of product-line testing.},
booktitle = {Proceedings of the 12th International Workshop on Automation of Software Testing},
pages = {16–22},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {AST '17}
}

@inproceedings{10.1145/2499777.2500715,
author = {Ishida, Yuzo},
title = {Scalable variability management for enterprise applications with data model driven development},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500715},
doi = {10.1145/2499777.2500715},
abstract = {Unlike embedded systems, some of enterprise systems are evolved over the decades. The predictability of requirements is a key to success in building reusable assets however it is very hard to predict future business context changes, which are driving factors of requirements. Thus, both functional and context variability must be managed in order to satisfy ever-changing requirements. Scalability does matter for enterprise systems in two aspects. One aspect comes from data volume. Once data become big, it is difficult to maintain performance requirements without de-normalizing database schema. Since database de-normalization is driven by non-functional properties, a model driven approach is not feasible if the model cannot express such properties. Another aspect comes from the unpredictability of future functional requirements. A functional decomposition of enterprise systems usually introduces ever-increasing complexity among systems' interactions due to cross-cutting requirements across functional systems. This paper reflects our empirical studies in data intensive large enterprise systems such as retail and telecommunication industries with industry independent application framework to separate functional and non-functional concerns. Our variability management technique is based on database schema modeling, which can be evolved incrementally in scaling an enterprise system with both data and functional aspects.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {90–93},
numpages = {4},
keywords = {type theory, relational algebra, quality attributes, higher-order simple predicate logic, core assets},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/2362536.2362544,
author = {Dietrich, Christian and Tartler, Reinhard and Schr\"{o}der-Preikschat, Wolfgang and Lohmann, Daniel},
title = {A robust approach for variability extraction from the Linux build system},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362544},
doi = {10.1145/2362536.2362544},
abstract = {With more than 11,000 optional and alternative features, the Linux kernel is a highly configurable piece of software. Linux is generally perceived as a textbook example for preprocessor-based product derivation, but more than 65 percent of all features are actually handled by the build system. Hence, variability-aware static analysis tools have to take the build system into account.However, extracting variability information from the build system is difficult due to the declarative and turing-complete make language. Existing approaches based on text processing do not cover this challenges and tend to be tailored to a specific Linux version and architecture. This renders them practically unusable as a basis for variability-aware tool support -- Linux is a moving target!We describe a robust approach for extracting implementation variability from the Linux build system. Instead of extracting the variability information by a text-based analysis of all build scripts, our approach exploits the build system itself to produce this information. As our results show, our approach is robust and works for all versions and architectures from the (git-)history of Linux.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {21–30},
numpages = {10},
keywords = {static analysis, maintenance, kbuild, configurability, build systems, VAMOS, Linux},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3168365.3168374,
author = {Arcaini, Paolo and Gargantini, Angelo and Radavelli, Marco},
title = {An evolutionary process for product-driven updates of feature models},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168374},
doi = {10.1145/3168365.3168374},
abstract = {Feature models are a widely used modeling notation for variability and commonality management in software product line (SPL) engineering. In order to keep an SPL and its feature model aligned, feature models must be changed by including/excluding new features and products, either because faults in the model are found or to reflect the normal evolution of the SPL. The modification of the feature model able to satisfy these change requirements can be complex and error-prone. In this paper, we present a method that is able to automatically update a feature model in order to satisfy a given update request. Our method is based on an evolutionary algorithm and it iteratively applies structure-preserving mutations to the original model, until the model is completely updated. We evaluate the process on real-world feature models. Although our approach does not guarantee to completely update all possible feature models, empirical analysis shows that, on average, more than 80% of requested changes are applied.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {67–74},
numpages = {8},
keywords = {software product lines, search-based software engineering, mutation, feature models},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/2647908.2655958,
author = {Eichelberger, Holger and Schmid, Klaus},
title = {Resource-optimizing adaptation for big data applications},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655958},
doi = {10.1145/2647908.2655958},
abstract = {The resource requirements of Big Data applications may vary dramatically over time, depending on changes in the context. If resources should not be defined for the maximum case, but available resources are mostly static, there is a need to adapt resource usage by modifying the processing behavior. The QualiMaster project researches such an approach for the analysis of systemic risks in the financial markets.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {10–11},
numpages = {2},
keywords = {systematic-risks, stream-processing, resource adaptation, financial markets, adaptive systems, QualiMaster},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/1985484.1985490,
author = {Stallinger, Fritz and Neumann, Robert and Schossleitner, Robert and Kriener, Stephan},
title = {Migrating towards evolving software product lines: challenges of an SME in a core customer-driven industrial systems engineering context},
year = {2011},
isbn = {9781450305846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985484.1985490},
doi = {10.1145/1985484.1985490},
abstract = {In this paper we identify key challenges a medium-sized software organization is facing in migrating towards Software Product Line Engineering (SPLE). The software engineering context of the company is characterized by a two-fold access to the market - core customer driven product enhancement and product development for a broader, anonymous market - and the embedding of software engineering in multi-disciplinary systems and solutions engineering.Based on a characterization of the business, the software product subject to migration towards SPLE, and the goals and background of the SPLE initiative, seven key challenges with respect to the migration are identified. These challenges relate to process diversity in the face of multiple reuse approaches; the management of requirements and variability; the integration of requirements traceability and variability management; legacy software and discipline vs. software-specific modularization; integration with systems engineering; costing and pricing models; and project vs. product documentation.},
booktitle = {Proceedings of the 2nd International Workshop on Product Line Approaches in Software Engineering},
pages = {20–24},
numpages = {5},
keywords = {systems engineering, software product migration, software product line, software engineering, sme},
location = {Waikiki, Honolulu, HI, USA},
series = {PLEASE '11}
}

@inproceedings{10.5555/2820656.2820662,
author = {Chitchyan, Ruzanna and Noppen, Joost and Groher, Iris},
title = {What can software engineering do for sustainability: case of software product lines},
year = {2015},
publisher = {IEEE Press},
abstract = {Sustainable living, i.e., living within the bounds of the available environmental, social, and economic resources, is the focus of many present-day social and scientific discussions. But what does sustainability mean within the context of Software Product Line Engineering (SPLE)? And what does SPLE do for sustainable living? In this paper we take the first step towards identification of the sustainability-related characteristics relevant to SPLE. The paper also discusses how the key areas of interest to the current SPL community (as reflected by what is measured and optimised in SPLs today) relate to these sustainability characteristics.},
booktitle = {Proceedings of the Fifth International Workshop on Product LinE Approaches in Software Engineering},
pages = {11–14},
numpages = {4},
location = {Florence, Italy},
series = {PLEASE '15}
}

@inproceedings{10.1145/3023956.3023968,
author = {Mjeda, Anila and Wasala, Asanka and Botterweck, Goetz},
title = {Decision spaces in product lines, decision analysis, and design exploration: an interdisciplinary exploratory study},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023968},
doi = {10.1145/3023956.3023968},
abstract = {Context. From recent works on product properties resulting from configurations and the optimisation of these properties, one comes quickly to more complex challenges such as multi-objective optimisation, conflicting objectives, multiple stakeholders, and conflict resolution. The intuition is that Software Product Line Engineering (SPLE) can draw from other disciplines that deal with decision spaces and complex decision scenarios.Objectives. We aim to (1) explore links to such disciplines, (2) systematise and compare concepts, and (3) identify opportunities, where SPLE approaches can be enriched.Method. We undertake an exploratory study: Starting from common SPLE activities and artefacts, we identify aspects where we expect to find corresponding counterparts in other disciplines. We focus on Multiple Criteria Decision Analysis (MCDA), Multi-Objective Optimisation (MOO), and Design Space Exploration (DSE), and perform a comparison of the key concepts.Results. The resulting comparison relates SPLE activities and artefacts to concepts from MCDA, MOO, and DSE and identifies areas where SPLE approaches can be enriched. We also provide examples of existing work at the intersections of SPLE with the other fields. These findings are aimed to foster the conversation on research opportunities where SPLE can draw techniques from other disciplines dealing with complex decision scenarios.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {68–75},
numpages = {8},
keywords = {multi-objective optimisation, multi-criteria decision analysis, design-space exploration, decision modelling},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/2897053.2897058,
author = {Sharifloo, Amir Molzam and Metzger, Andreas and Quinton, Cl\'{e}ment and Baresi, Luciano and Pohl, Klaus},
title = {Learning and evolution in dynamic software product lines},
year = {2016},
isbn = {9781450341875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897053.2897058},
doi = {10.1145/2897053.2897058},
abstract = {A Dynamic Software Product Line (DSPL) aims at managing run-time adaptations of a software system. It is built on the assumption that context changes that require these adaptations at run-time can be anticipated at design-time. Therefore, the set of adaptation rules and the space of configurations in a DSPL are predefined and fixed at design-time. Yet, for large-scale and highly distributed systems, anticipating all relevant context changes during design-time is often not possible due to the uncertainty of how the context may change. Such design-time uncertainty therefore may mean that a DSPL lacks adaptation rules or configurations to properly reconfigure itself at run-time. We propose an adaptive system model to cope with design-time uncertainty in DSPLs. This model combines learning of adaptation rules with evolution of the DSPL configuration space. It takes particular account of the mutual dependencies between evolution and learning, such as using feedback from unsuccessful learning to trigger evolution. We describe concrete steps for learning and evolution to show how such feedback can be exploited. We illustrate the use of such a model with a running example from the cloud computing domain.},
booktitle = {Proceedings of the 11th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {158–164},
numpages = {7},
keywords = {machine learning, evolution, dynamic software product lines, adaptation},
location = {Austin, Texas},
series = {SEAMS '16}
}

@inproceedings{10.1145/2647908.2655969,
author = {ter Beek, Maurice H. and Mazzanti, Franco},
title = {VMC: recent advances and challenges ahead},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655969},
doi = {10.1145/2647908.2655969},
abstract = {The variability model checker VMC accepts a product family specified as a Modal Transition System (MTS) with additional variability constraints. Consequently, it offers behavioral variability analyses over both the family and its valid product behavior. This ranges from product derivation and simulation to efficient on-the-fly model checking of logical properties expressed in a variability-aware version of action-based CTL. In this paper, we first explain the reasons and assumptions underlying the choice for a modeling and analysis framework based on MTSs. Subsequently, we present recent advances on proving inheritance of behavioral analysis properties from a product family to its valid products. Finally, we illustrate challenges remaining for the future.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {70–77},
numpages = {8},
keywords = {product families, model checking, behavioral variability},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2019136.2019141,
author = {Ryssel, Uwe and Ploennigs, Joern and Kabitzsch, Klaus},
title = {Extraction of feature models from formal contexts},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019141},
doi = {10.1145/2019136.2019141},
abstract = {For economical reasons, the creation of feature oriented software should include previously created products and should not be done from scratch. To speed up this migration process, feature models have to be generated automatically from existing product variants. This work presents an approach based on formal concept analysis that analyzes incidence matrices containing matching relations as input and creates feature models as output. The resulting feature models describe exactly the given input variants. The introduced novel optimized approach performs this transformation in reasonable time even for large product libraries.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {4},
numpages = {8},
keywords = {formal concept analysis, feature models},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3447545.3451177,
author = {Canales, Felipe and Hecht, Geoffrey and Bergel, Alexandre},
title = {Optimization of Java Virtual Machine Flags using Feature Model and Genetic Algorithm},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451177},
doi = {10.1145/3447545.3451177},
abstract = {Optimizing the Java Virtual Machine (JVM) options in order to get the best performance out of a program for production is a challenging and time-consuming task. HotSpot, the Oracle's open-source Java VM implementation offers more than 500 options, called flags, that can be used to tune the JVM's compiler, garbage collector (GC), heap size and much more. In addition to being numerous, these flags are sometimes poorly documented and create a need of benchmarking to ensure that the flags and their associated values deliver the best performance and stability for a particular program to execute.Auto-tuning approaches have already been proposed in order to mitigate this burden. However, in spite of increasingly sophisticated search techniques allowing for powerful optimizations, these approaches take little account of the underlying complexities of JVM flags. Indeed, dependencies and incompatibilities between flags are non-trivial to express, which if not taken into account may lead to invalid or spurious flag configurations that should not be considered by the auto-tuner.In this paper, we propose a novel model, inspired by the feature model used in Software Product Line, which takes the complexity of JVM's flags into account. We then demonstrate the usefulness of this model, using it as an input of a Genetic Algorithm (GA) to optimize the execution times of DaCapo Benchmarks.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {183–186},
numpages = {4},
keywords = {optimization, java virtual machine, genetic algorithm, feature model, auto-tuning},
location = {Virtual Event, France},
series = {ICPE '21}
}

@inproceedings{10.1145/2866614.2866627,
author = {Devroey, Xavier and Perrouin, Gilles and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Search-based Similarity-driven Behavioural SPL Testing},
year = {2016},
isbn = {9781450340199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2866614.2866627},
doi = {10.1145/2866614.2866627},
abstract = {Dissimilar test cases have been proven to be effective to reveal faults in software systems. In the Software Product Line (SPL) context, this criterion has been applied successfully to mimic combinatorial interaction testing in an efficient and scalable manner by selecting and prioritising most dissimilar configurations of feature models using evolutionary algorithms. In this paper, we extend dissimilarity to behavioural SPL models (FTS) in a search-based approach, and evaluate its effectiveness in terms of product and fault coverage. We investigate different distances as well as as single-objective algorithms, (dissimilarity on actions, random, all-actions). Our results on four case studies show the relevance of dissimilarity-based test generation for behavioural SPL models, especially on the largest case-study where no other approach can match it.},
booktitle = {Proceedings of the 10th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {89–96},
numpages = {8},
keywords = {Software Product Line Testing, Featured Transition System, Dissimilarity Testing},
location = {Salvador, Brazil},
series = {VaMoS '16}
}

@inproceedings{10.5555/1753235.1753274,
author = {Pech, Daniel and Knodel, Jens and Carbon, Ralf and Schitter, Clemens and Hein, Dirk},
title = {Variability management in small development organizations: experiences and lessons learned from a case study},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Product line practices promise to reduce development and maintenance efforts, to improve the productivity and to reduce the time to market by systematic reuse of commonalities and variabilities. However, in order to reap the fruits of exploiting those, an upfront investment is required. This paper presents a case study, which analyzes the cost-benefit ratio for one product line discipline -- variability management. Wikon GmbH -- a small German development organization evolving a product line of remote monitoring and controlling devices -- switched from manual, file-based conditional compilation to tool-supported decision models. We discuss experiences made and show that the break-even was reached with the 4th product derivation.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {285–294},
numpages = {10},
keywords = {decision model, evolution, product line engineering, software architecture, variability management},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.5555/1753235.1753267,
author = {Mendonca, Marcilio and W\k{a}sowski, Andrzej and Czarnecki, Krzysztof},
title = {SAT-based analysis of feature models is easy},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Feature models are a popular variability modeling notation used in product line engineering. Automated analyses of feature models, such as consistency checking and interactive or offline product selection, often rely on translating models to propositional logic and using satisfiability (SAT) solvers.Efficiency of individual satisfiability-based analyses has been reported previously. We generalize and quantify these studies with a series of independent experiments. We show that previously reported efficiency is not incidental. Unlike with the general SAT instances, which fall into easy and hard classes, the instances induced by feature modeling are easy throughout the spectrum of realistic models. In particular, the phenomenon of phase transition is not observed for realistic feature models.Our main practical conclusion is a general encouragement for researchers to continued development of SAT-based methods to further exploit this efficiency in future.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {231–240},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1145/1183236.1183264,
author = {Batory, Don and Benavides, David and Ruiz-Cortes, Antonio},
title = {Automated analysis of feature models: challenges ahead},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183264},
doi = {10.1145/1183236.1183264},
journal = {Commun. ACM},
month = dec,
pages = {45–47},
numpages = {3}
}

@inproceedings{10.5555/1753235.1753263,
author = {Than Tun, Thein and Boucher, Quentin and Classen, Andreas and Hubaux, Arnaud and Heymans, Patrick},
title = {Relating requirements and feature configurations: a systematic approach},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {A feature model captures various possible configurations of products within a product family. When configuring a product, several features are selected and composed. Selecting features at the program level has a general limitation of not being able to relate the resulting configuration to its requirements. As a result, it is difficult to decide whether a given configuration of features is optimal. An optimal configuration satisfies all stakeholder requirements and quantitative constraints, while ensuring that there is no extraneous feature in it. In relating requirements and feature configurations, we use the description of the problem world context in which the software is designed to operate as the intermediate description between them. The advantage of our approach is that feature selection can be done at the requirements level, and an optimal program level configuration can be generated from the requirements selected. Our approach is illustrated with a real-life problem of configuring a satellite communication software. The use of an existing tool to support our approach is also discussed.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {201–210},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1145/2211616.2211617,
author = {K\"{a}stner, Christian and Apel, Sven and Th\"{u}m, Thomas and Saake, Gunter},
title = {Type checking annotation-based product lines},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2211616.2211617},
doi = {10.1145/2211616.2211617},
abstract = {Software product line engineering is an efficient means of generating a family of program variants for a domain from a single code base. However, because of the potentially high number of possible program variants, it is difficult to test them all and ensure properties like type safety for the entire product line. We present a product-line-aware type system that can type check an entire software product line without generating each variant in isolation. Specifically, we extend the Featherweight Java calculus with feature annotations for product-line development and prove formally that all program variants generated from a well typed product line are well typed. Furthermore, we present a solution to the problem of typing mutually exclusive features. We discuss how results from our formalization helped implement our own product-line tool CIDE for full Java and report of our experience with detecting type errors in four existing software product line implementations.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {14},
numpages = {39},
keywords = {type system, software product lines, conditional compilation, Featherweight Java, CIDE, CFJ, #ifdef}
}

@inproceedings{10.5555/1753235.1753245,
author = {Cetina, Carlos and Haugen, \O{}ystein and Zhang, Xiaorui and Fleurey, Franck and Pelechano, Vicente},
title = {Strategies for variability transformation at run-time},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {More and more approaches propose to use Software Product Lines (SPLs) modelling techniques to implement dynamic adaptive systems. The resulting Dynamic Software Product Lines (DSPLs) present new challenges since the variability transformations used to derive alternative configurations have to be intensively used at runtime. This paper proposes to use the Common Variability Language (CVL) for modelling runtime variability and evaluates a set of alternative strategies for implementing the associated variability transformations. All the proposed strategies have been implemented and evaluated on the case-study of a smart-home system. Results show that the proposed strategies provide the same reconfiguration service with significant differences in quality-of-service.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {61–70},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/3377024.3377031,
author = {El-Sharkawy, Sascha and Krafczyk, Adam and Schmid, Klaus},
title = {Fast static analyses of software product lines: an example with more than 42,000 metrics},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377031},
doi = {10.1145/3377024.3377031},
abstract = {Context: Software metrics, as one form of static analyses, is a commonly used approach in software engineering in order to understand the state of a software system, in particular to identify potential areas prone to defects. Family-based techniques extract variability information from code artifacts in Software Product Lines (SPLs) to perform static analysis for all available variants. Many different types of metrics with numerous variants have been defined in literature. When counting all metrics including such variants, easily thousands of metrics can be defined. Computing all of them for large product lines can be an extremely expensive process in terms of performance and resource consumption.Objective: We address these performance and resource challenges while supporting customizable metric suites, which allow running both, single system and variability-aware code metrics.Method: In this paper, we introduce a partial parsing approach used for the efficient measurement of more than 42,000 code metric variations. The approach covers variability information and restricts parsing to the relevant parts of the Abstract Syntax Tree (AST).Conclusions: This partial parsing approach is designed to cover all relevant information to compute a broad variety of variability-aware code metrics on code artifacts containing annotation-based variability, e.g., realized with C-preprocessor statements. It allows for the flexible combination of single system and variability-aware metrics, which is not supported by existing tools. This is achieved by a novel representation of partially parsed product line code artifacts, which is tailored to the computation of the metrics. Our approach consumes considerably less resources, especially when computing many metric variants in parallel.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {8},
numpages = {9},
keywords = {variability models, software product lines, metrics, implementation, feature models, abstract syntax trees, SPL, AST},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1145/3571788.3571791,
author = {Di Sandro, Alessio and Shahin, Ramy and Chechik, Marsha},
title = {Adding Product-Line Capabilities to Your Favourite Modeling Language},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571788.3571791},
doi = {10.1145/3571788.3571791},
abstract = {Software product lines are commonly adopted in industry to manage the development of complex families of software systems. Software engineering activities use models at their core, and extending a modeling language to support product lines is an expensive task. Moreover, many useful techniques and analyses such as model querying and model refactoring, are defined at the level of individual products. Before they can take advantage of a product line representation, they need to be lifted, i.e., reengineered to handle variability in a product line. Not only is this process non-trivial, it needs to be redone for each modeling language. In this paper, we propose an Eclipse-based framework, MMINT-PL, for creating and managing annotative product lines of software models in a language-agnostic way. Our framework allows extending any modeling language with product line capabilities, and facilitates lifting of a variety of modeling activities to the product line level. We also demonstrate how to use MMINT-PL to lift the Viatra Query Language.},
booktitle = {Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {3–12},
numpages = {10},
keywords = {variability, queries., modeling languages, Product lines},
location = {Odense, Denmark},
series = {VaMoS '23}
}

@inproceedings{10.1145/2993236.2993253,
author = {Al-Hajjaji, Mustafa and Krieter, Sebastian and Th\"{u}m, Thomas and Lochau, Malte and Saake, Gunter},
title = {IncLing: efficient product-line testing using incremental pairwise sampling},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993253},
doi = {10.1145/2993236.2993253},
abstract = {A software product line comprises a family of software products that share a common set of features. It enables customers to compose software systems from a managed set of features. Testing every product of a product line individually is often infeasible due to the exponential number of possible products in the number of features. Several approaches have been proposed to restrict the number of products to be tested by sampling a subset of products achieving sufficient combinatorial interaction coverage. However, existing sampling algorithms do not scale well to large product lines, as they require a considerable amount of time to generate the samples. Moreover, samples are not available until a sampling algorithm completely terminates. As testing time is usually limited, we propose an incremental approach of product sampling for pairwise interaction testing (called IncLing), which enables developers to generate samples on demand in a step-wise manner. Furthermore, IncLing uses heuristics to efficiently achieve pairwise interaction coverage with a reasonable number of products. We evaluated IncLing by comparing it against existing sampling algorithms using feature models of different sizes. The results of our approach indicate efficiency improvements for product-line testing.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {144–155},
numpages = {12},
keywords = {sampling, model-based testing, combinatorial interaction testing, Software product lines},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@inproceedings{10.1145/3624007.3624058,
author = {Medeiros, Raul and D\'{\i}az, Oscar and Benavides, David},
title = {Unleashing the Power of Implicit Feedback in Software Product Lines: Benefits Ahead},
year = {2023},
isbn = {9798400704062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624007.3624058},
doi = {10.1145/3624007.3624058},
abstract = {Software Product Lines (SPLs) facilitate the development of a complete range of software products through systematic reuse. Reuse involves not only code but also the transfer of knowledge gained from one product to others within the SPL. This transfer includes bug fixing, which, when encountered in one product, affects the entire SPL portfolio. Similarly, feedback obtained from the usage of a single product can inform beyond that product to impact the entire SPL portfolio. Specifically, implicit feedback refers to the automated collection of data on software usage or execution, which allows for the inference of customer preferences and trends. While implicit feedback is commonly used in single-product development, its application in SPLs has not received the same level of attention. This paper promotes the investigation of implicit feedback in SPLs by identifying a set of SPL activities that can benefit the most from it. We validate this usefulness with practitioners using a questionnaire-based approach (n=8). The results provide positive insights into the advantages and practical implications of adopting implicit feedback at the SPL level.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {113–121},
numpages = {9},
keywords = {User behavior, Software Product Lines, Implicit feedback, Code generation},
location = {Cascais, Portugal},
series = {GPCE 2023}
}

@inproceedings{10.1145/3132498.3133834,
author = {Santos, Marcelo C. B. and Colanzi, Thelma E. and Amaral, Aline M. M. M. and OliveiraJr, Edson},
title = {Preliminary study on the correlation of objective functions to optimize product-line architectures},
year = {2017},
isbn = {9781450353250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132498.3133834},
doi = {10.1145/3132498.3133834},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line (SPL). The Multi-Objective Approach for PLA Design (MOA4PLA) aims at optimizing the PLA design by using search algorithms easing the design activity. From an original PLA, MOA4PLA automatically obtains alternative designs to improve the original one in terms of the objectives selected for optimization. The use of search algorithms is an incipient research topic, which includes several open research questions. The evaluation model of MOA4PLA is composed of various objective functions, which use software metrics to evaluate different factors that influence on the PLA design. However, the simultaneous optimization of all objective functions is a computationally complex task. In this sense, it is worthwhile to investigate the possible correlation between objective functions because the discovery of correlated functions allows to reduce the number of objectives to be optimized by the search algorithm. Hence, in this paper we perform a preliminary study to investigate the correlation among five objective functions related to metrics that provide indicators on conventional architectural properties, such as coupling, cohesion and size. To accomplish the objective of this paper, four controlled experiments were carried out with four different PLA designs. Empirical results provide preliminary evidence that two pairs of functions are positively correlated and two other pairs of functions are negatively correlated. From such findings, several guidelines were derived to help architects to both reduce and select the objectives related to conventional architectural properties to be tackled during the PLA design optimization.},
booktitle = {Proceedings of the 11th Brazilian Symposium on Software Components, Architectures, and Reuse},
articleno = {11},
numpages = {10},
keywords = {search-based software engineering, product-line architecture, correlation study},
location = {Fortaleza, Cear\'{a}, Brazil},
series = {SBCARS '17}
}

@inproceedings{10.5555/2818754.2818819,
author = {Henard, Christopher and Papadakis, Mike and Harman, Mark and Le Traon, Yves},
title = {Combining multi-objective search and constraint solving for configuring large software product lines},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Software Product Line (SPL) feature selection involves the optimization of multiple objectives in a large and highly constrained search space. We introduce SATIBEA, that augments multi-objective search-based optimization with constraint solving to address this problem, evaluating it on five large real-world SPLs, ranging from 1,244 to 6,888 features with respect to three different solution quality indicators and two diversity metrics. The results indicate that SATIBEA statistically significantly outperforms the current state-of-the-art (p &lt; 0.01) for all five SPLs on all three quality indicators and with maximal effect size (\^{A}12 = 1.0). We also present results that demonstrate the importance of combining constraint solving with search-based optimization and the significant improvement SATIBEA produces over pure constraint solving. Finally, we demonstrate the scalability of SATIBEA: within less than half an hour, it finds thousands of constraint-satisfying optimized software products, even for the largest SPL considered in the literature to date.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {517–528},
numpages = {12},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/2771783.2771808,
author = {Tan, Tian Huat and Xue, Yinxing and Chen, Manman and Sun, Jun and Liu, Yang and Dong, Jin Song},
title = {Optimizing selection of competing features via feedback-directed evolutionary algorithms},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2771808},
doi = {10.1145/2771783.2771808},
abstract = {Software that support various groups of customers usually require complicated configurations to attain different functionalities. To model the configuration options, feature model is proposed to capture the commonalities and competing variabilities of the product variants in software family or Software Product Line (SPL). A key challenge for deriving a new product is to find a set of features that do not have inconsistencies or conflicts, yet optimize multiple objectives (e.g., minimizing cost and maximizing number of features), which are often competing with each other. Existing works have attempted to make use of evolutionary algorithms (EAs) to address this problem. In this work, we incorporated a novel feedback-directed mechanism into existing EAs. Our empirical results have shown that our method has improved noticeably over all unguided version of EAs on the optimal feature selection. In particular, for case studies in SPLOT and LVAT repositories, the feedback-directed Indicator-Based EA (IBEA) has increased the number of correct solutions found by 72.33% and 75%, compared to unguided IBEA. In addition, by leveraging a pre-computed solution, we have found 34 sound solutions for Linux X86, which contains 6888 features, in less than 40 seconds.},
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {246–256},
numpages = {11},
keywords = {evolutionary algorithms, Software product line, SAT solvers},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@inproceedings{10.1145/2556624.2556637,
author = {Machado, Ivan do Carmo and Santos, Alcemir Rodrigues and Cavalcanti, Yguarat\~{a} Cerqueira and Trzan, Eduardo Gomes and de Souza, Marcio Magalh\~{a}es and de Almeida, Eduardo Santana},
title = {Low-level variability support for web-based software product lines},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556637},
doi = {10.1145/2556624.2556637},
abstract = {The Web systems domain has faced an increasing number of devices, browsers, and platforms to cope with, driving software systems to be more flexible to accomodate them. Software product line (SPL) engineering can be used as a strategy to implement systems capable of handling such a diversity. To this end, automated tool support is almost indispensable. However, current tool support gives more emphasis to modeling variability in the problem domain, over the support of variability at the solution domain. There is a need for mapping the variability between both abstraction levels, so as to determine what implementation impact a certain variability has. In this paper, we propose the FeatureJS, a FeatureIDE extension aiming at Javascript and HTML support for SPL engineering. The tool combines feature-oriented programming and preprocessors, as a strategy to map variability at source code with the variability modeled at a higher level of abstraction. We carried out a preliminary evaluation with an industrial project, aiming to characterize the capability of the tool to handle SPL engineering in the Web systems domain.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {15},
numpages = {8},
keywords = {web systems domain, software product line engineering, feature oriented software development, feature composition, FeatureIDE, Eclipse plugin},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@inproceedings{10.1145/2568225.2568267,
author = {Salay, Rick and Famelis, Michalis and Rubin, Julia and Di Sandro, Alessio and Chechik, Marsha},
title = {Lifting model transformations to product lines},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568267},
doi = {10.1145/2568225.2568267},
abstract = {Software product lines and model transformations are two techniques used in industry for managing the development of highly complex software. Product line approaches simplify the handling of software variants while model transformations automate software manipulations such as refactoring, optimization, code generation, etc. While these techniques are well understood independently, combining them to get the benefit of both poses a challenge because most model transformations apply to individual models while model-level product lines represent sets of models. In this paper, we address this challenge by providing an approach for automatically ``lifting'' model transformations so that they can be applied to product lines. We illustrate our approach using a case study and evaluate it through a set of experiments.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {117–128},
numpages = {12},
keywords = {Software Product Lines, Model Transformations, Model Driven Engineering},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/3704558.3704560,
author = {Li, Lin and Mu, Bingzhu and Liu, Jingxia},
title = {Optimisation of energy storage configurations for integrated energy systems in low-carbon parks considering hydrogen-blended gas},
year = {2025},
isbn = {9798400710681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704558.3704560},
doi = {10.1145/3704558.3704560},
abstract = {For the multi-energy coupled park integrated energy system supported by hydrogen energy storage, reasonable allocation of energy storage capacity can effectively improve the energy utilization rate. Combining the advantages of Hydro-gen-combined natural gas technology in reducing carbon emissions and optimising the utilisation of system energy storage, a model for optimising the energy storage configuration of an integrated energy system in a park considering the utilisation of hydrogen gas mixing is proposed. Firstly, the architecture of the integrated energy system of the low-carbon park configured with hydrogen storage is established, and the energy flow relationships among electricity, heat, hydrogen and Hydrogen-combined natural gas of the system are sorted out. Based on this, a bi-level model of energy storage configuration-operation optimisation is constructed with the objective of optimising the investment cost, operation cost and carbon emission cost of the system. Finally, the simulation is analysed using a park in Northwest China as an example. Through comparative verification in different scenarios, it shows the effectiveness of the introduction of mixed hydrogen gas in reducing the system's purchased energy and carbon emissions, which can effectively reduce the system's annualised investment and operating costs, and points out the impact of carbon emission costs on the park's energy storage configuration.},
booktitle = {Proceedings of the 2024 2nd International Conference on Frontiers of Intelligent Manufacturing and Automation},
pages = {490–495},
numpages = {6},
keywords = {Bi-level optimisation model, Electric- heat -gas coupling, Energy storage configuration, Hydrogen-combined natural gas, Park integrated energy system},
location = {
},
series = {CFIMA '24}
}

@inproceedings{10.1145/2430502.2430511,
author = {Kolesnikov, Sergiy S. and Apel, Sven and Siegmund, Norbert and Sobernig, Stefan and K\"{a}stner, Christian and Senkaya, Semah},
title = {Predicting quality attributes of software product lines using software and network measures and sampling},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430511},
doi = {10.1145/2430502.2430511},
abstract = {Software product-line engineering aims at developing families of related products that share common assets to provide customers with tailor-made products. Customers are often interested not only in particular functionalities (i.e., features), but also in non-functional quality attributes, such as performance, reliability, and footprint. Measuring quality attributes of all products of a product line usually does not scale. In this research-in-progress report, we propose a systematic approach aiming at efficient and scalable prediction of quality attributes of products. To this end, we establish predictors for certain categories of quality attributes (e.g., a predictor for high memory consumption) based on software and network measures, and receiver operating characteristic analysis. We use these predictors to guide a sampling process that takes the assets of a product line as input and determines the products that fall into the category denoted by the given predictor (e.g., products with high memory consumption). We propose to use predictors to make the process of finding "acceptable" products more efficient. We discuss and compare several strategies to incorporate predictors in the sampling process.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {6},
numpages = {5},
keywords = {software product lines, sampling, quality attributes, prediction, metrics},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@inproceedings{10.1145/2577080.2577095,
author = {Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha and Baier, Christel},
title = {Probabilistic model checking for energy analysis in software product lines},
year = {2014},
isbn = {9781450327725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2577080.2577095},
doi = {10.1145/2577080.2577095},
abstract = {In a software product line (SPL), a collection of software products is defined by their commonalities in terms of features rather than explicitly specifying all products one-by-one. Several verification techniques were adapted to establish temporal properties of SPLs. Symbolic and family-based model checking have been proven to be successful for tackling the combinatorial blow-up arising when reasoning about several feature combinations. However, most formal verification approaches for SPLs presented in the literature focus on the static SPLs, where the features of a product are fixed and cannot be changed during runtime. This is in contrast to dynamic SPLs, allowing to adapt feature combinations of a product dynamically after deployment.The main contribution of the paper is a compositional modeling framework for dynamic SPLs, which supports probabilistic and nondeterministic choices and allows for quantitative analysis. We specify the feature changes during runtime within an automata-based coordination component, enabling to reason over strategies how to trigger dynamic feature changes for optimizing various quantitative objectives, e.g., energy or monetary costs and reliability. For our framework there is a natural and conceptually simple translation into the input language of the prominent probabilistic model checker PRISM. This facilitates the application of PRISM's powerful symbolic engine to the operational behavior of dynamic SPLs and their family-based analysis against various quantitative queries. We demonstrate feasibility of our approach by a case study issuing an energy-aware bonding network device.},
booktitle = {Proceedings of the 13th International Conference on Modularity},
pages = {169–180},
numpages = {12},
keywords = {software product lines, probabilistic model checking, energy analysis, dynamic features},
location = {Lugano, Switzerland},
series = {MODULARITY '14}
}

@inproceedings{10.1145/3701625.3701655,
author = {Paranhos, Isadora and Santos, Gleison},
title = {Critical Factors for Software Product Management: A Systematic Literature Review},
year = {2024},
isbn = {9798400717772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701625.3701655},
doi = {10.1145/3701625.3701655},
abstract = {Context: An adequate implementation of software product management brings benefits such as software quality, leadership and teamwork, control of stakeholder needs, and business objectives accountability, reducing delivery delays and improving productivity. Many organizations are unaware of the critical factors associated with software product management. Acknowledging them can assist in proactively avoiding relevant issues. Objective: We executed a systematic literature reivew to identify critical factors that can influence how software product management is executed. Results: Based on 37 studies, we identified 32 critical factors, such as strategic alignment of product management, defining the roles and activities required for software product management, Collaboration between organizational areas, definition and management of the product roadmap aligned with strategic goals, and aligning technical and architectural decisions with strategic product management. Conclusions: We present a framework organizing the factors into categories understanding the organizational context, providing the foundation for software product management, supporting effective management, and supporting product development. The identified factors can assist organizations to focus on the most relevant aspects for efficient software product management and, thus, increase the likelihood of delivering quality products, adding value to both the customer and the organization itself.},
booktitle = {Proceedings of the XXIII Brazilian Symposium on Software Quality},
pages = {158–168},
numpages = {11},
keywords = {Software Product Management, Critical Success Factors, Systematic Literature Review},
location = {
},
series = {SBQS '24}
}

@article{10.1145/3711119,
author = {Blot, Aymeric and Petke, Justyna},
title = {A Comprehensive Survey of Benchmarks for Improvement of Software's Non-Functional Properties},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3711119},
doi = {10.1145/3711119},
abstract = {Despite recent increase in research on improvement of non-functional properties of software, such as energy usage or program size, there is a lack of standard benchmarks for such work. This absence hinders progress in the field, and raises questions about the representativeness of current benchmarks of real-world software.To address these issues and facilitate further research on improvement of non-functional properties of software, we conducted a comprehensive survey on the benchmarks used in the field thus far. We searched five major online repositories of research work, collecting 5,499 publications (4,066 unique), and systematically identified relevant papers to construct a rich and diverse corpus of 425 relevant studies.We find that execution time is the most frequently improved property in research work (63%), while multi-objective improvement is rarely considered (7%). Static approaches for improvement of non-functional software properties are prevalent (51%), with exploratory approaches (18% evolutionary and 15% non-evolutionary) increasingly popular in the last 10 years. Only 39% of the 425 papers describe work that uses benchmark suites, rather than single software, of those SPEC is most popular (63 papers). We also provide recommendations for future work, noting, for instance, lack of benchmarks for non-functional improvement that covers Python, JavaScript, or mobile devices. All the details regarding the 425 identified papers are available on our dedicated webpage: .},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {168},
numpages = {36},
keywords = {Software performance, non-functional properties, benchmark}
}

@inproceedings{10.5555/3172795.3172831,
author = {Masri, Samer Al and Bhuiyan, Nazim Uddin and Nadi, Sarah and Gaudet, Matthew},
title = {Software variability through C++ static polymorphism: a case study of challenges and open problems in eclipse OMR},
year = {2017},
publisher = {IBM Corp.},
address = {USA},
abstract = {Software Product Line Engineering (SPLE) creates configurable platforms that can be used to efficiently produce similar, and yet different, product variants. SPLs are typically modular such that it is easy to connect different blocks of code together, creating different variations of the product. There are many variability implementation mechanisms to achieve an SPL. This paper shows how static polymorphism can be used to implement variability, through a case study of IBM's open-source Eclipse OMR project. We discuss the current open problems and challenges this variability implementation mechanism raises and highlight technology gaps for reasoning about variability in OMR. We then suggest steps to close these gaps.},
booktitle = {Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering},
pages = {285–291},
numpages = {7},
location = {Markham, Ontario, Canada},
series = {CASCON '17}
}

@inproceedings{10.1145/1409720.1409748,
author = {Nestor, Daren and Thiel, Steffen and Botterweck, Goetz and Cawley, Ciar\'{a}n and Healy, Patrick},
title = {Applying visualisation techniques in software product lines},
year = {2008},
isbn = {9781605581125},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1409720.1409748},
doi = {10.1145/1409720.1409748},
abstract = {Software product lines of industrial size can easily incorporate thousands of variation points. This scale of variability can become extremely complex to manage resulting in a product development process that bears significant costs. One technique that can be applied beneficially in this context is visualisation. Visualisation is widely used in software engineering and has proven useful to amplify human cognition in data intensive applications. Adopting this technique in software product line engineering can help stakeholders in supporting essential work tasks and in enhancing their understanding of large and complex product lines.The research presented in this paper describes an integrated meta-model and research tool that employs visualisation techniques to address significant software product line tasks such as variability management and product derivation. Examples of the tasks are described and the ways in which these tasks can be further supported by utilising visualisation techniques are explained.},
booktitle = {Proceedings of the 4th ACM Symposium on Software Visualization},
pages = {175–184},
numpages = {10},
keywords = {visualisation, software product lines, interaction, feature configuration},
location = {Ammersee, Germany},
series = {SoftVis '08}
}

@article{10.1145/3428225,
author = {Shahin, Ramy and Chechik, Marsha},
title = {Automatic and efficient variability-aware lifting of functional programs},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428225},
doi = {10.1145/3428225},
abstract = {A software analysis is a computer program that takes some representation of a software product as input and produces some useful information about that product as output. A software product line encompasses many software product variants, and thus existing analyses can be applied to each of the product variations individually, but not to the entire product line as a whole. Enumerating all product variants and analyzing them one by one is usually intractable due to the combinatorial explosion of the number of product variants with respect to product line features. Several software analyses (e.g., type checkers, model checkers, data flow analyses) have been redesigned/re-implemented to support variability. This usually requires a lot of time and effort, and the variability-aware version of the analysis might have new errors/bugs that do not exist in the original one. Given an analysis program written in a functional language based on PCF, in this paper we present two approaches to transforming (lifting) it into a semantically equivalent variability-aware analysis. A light-weight approach (referred to as shallow lifting) wraps the analysis program into a variability-aware version, exploring all combinations of its input arguments. Deep lifting, on the other hand, is a program rewriting mechanism where the syntactic constructs of the input program are rewritten into their variability-aware counterparts. Compositionally this results in an efficient program semantically equivalent to the input program, modulo variability. We present the correctness criteria for functional program lifting, together with correctness proof sketches of shallow lifting. We evaluate our approach on a set of program analyses applied to the BusyBox C-language product line.},
journal = {Proc. ACM Program. Lang.},
month = nov,
articleno = {157},
numpages = {27},
keywords = {Variability-aware Programming, Software Product Lines, Program Rewriting, PCF, Lifting}
}

@article{10.1145/2897760,
author = {Hierons, Robert M. and Li, Miqing and Liu, Xiaohui and Segura, Sergio and Zheng, Wei},
title = {SIP: Optimal Product Selection from Feature Models Using Many-Objective Evolutionary Optimization},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/2897760},
doi = {10.1145/2897760},
abstract = {A feature model specifies the sets of features that define valid products in a software product line. Recent work has considered the problem of choosing optimal products from a feature model based on a set of user preferences, with this being represented as a many-objective optimization problem. This problem has been found to be difficult for a purely search-based approach, leading to classical many-objective optimization algorithms being enhanced either by adding in a valid product as a seed or by introducing additional mutation and replacement operators that use an SAT solver. In this article, we instead enhance the search in two ways: by providing a novel representation and by optimizing first on the number of constraints that hold and only then on the other objectives. In the evaluation, we also used feature models with realistic attributes, in contrast to previous work that used randomly generated attribute values. The results of experiments were promising, with the proposed (SIP) method returning valid products with six published feature models and a randomly generated feature model with 10,000 features. For the model with 10,000 features, the search took only a few minutes.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {17},
numpages = {39},
keywords = {Product selection}
}

@inproceedings{10.1145/2517208.2517214,
author = {Kramer, Dean and Oussena, Samia and Komisarczuk, Peter and Clark, Tony},
title = {Using document-oriented GUIs in dynamic software product lines},
year = {2013},
isbn = {9781450323734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517208.2517214},
doi = {10.1145/2517208.2517214},
abstract = {Dynamic Software Product Line (DSPL) Engineering has gained interest through its promise of being able to unify software adaptation whereby software adaptation can be realised at compile time and runtime. While previous work has enabled program logic adaptation by the use of language extensions and platform support, little attention has been placed on Graphical User Interface (GUI) variability. Different design patterns including the Model View Controller are commonly used in GUI implementation, with GUI documents being used for declaring the GUI. To handle dynamic GUI variability currently, the developer needs to implement GUI refinements using multiple techniques. This paper proposes a solution for dealing with GUI document variability, statically and dynamically, in a unified way. In our approach, we currently use a compile time method for producing GUI variants, and code transformations to handle these variants within the application at runtime. To avoid GUI duplicates, only GUI variants that are unique, and related to a valid product configuration, are produced. To validate our approach, we implemented tool support to enable this for Android based applications.},
booktitle = {Proceedings of the 12th International Conference on Generative Programming: Concepts &amp; Experiences},
pages = {85–94},
numpages = {10},
keywords = {graphical user interfaces, dynamic software product lines},
location = {Indianapolis, Indiana, USA},
series = {GPCE '13}
}

@article{10.1145/3701236,
author = {Yates, William B. and Keedwell, Edward C. and Kheiri, Ahmed},
title = {Explainable Optimisation through Online and Offline Hyper-heuristics},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701236},
doi = {10.1145/3701236},
abstract = {Abstract - Research in the explainability of optimisation techniques has largely focused on metaheuristics and their movement of solutions around the search landscape. Hyper-heuristics create a different challenge for explainability as they make use of many more operators, or low-level heuristics and learning algorithms which modify their probability of selection online. This paper describes a set of methods for explaining hyper-heuristics decisions in both online and offline scenarios using selection hyper-heuristics as an example. These methods help to explain various aspects of the function of hyper-heuristics both at a particular juncture in the optimisation process and through time. Visualisations of each method acting on sequences provide an understanding of which operators are being utilised and when, and in which combinations to produce a greater understanding of the algorithm-problem nexus in hyper-heuristic search. These methods are demonstrated on a range of problems including those in operational research and water distribution network optimisation. They demonstrate the insight that can be generated from optimisation using selection hyper-heuristics, including building an understanding of heuristic usage, useful combinations of heuristics and heuristic parameterisations. Furthermore the dynamics of heuristic utility are explored throughout an optimisation run and we show that it is possible to cluster problem instances according to heuristic selection alone, providing insight into the perception of problems from a hyper-heuristic perspective.},
note = {Just Accepted},
journal = {ACM Trans. Evol. Learn. Optim.},
month = oct,
keywords = {explainable artificial intelligence, hyper-heuristics, optimisation}
}

@inproceedings{10.1145/3483899.3483905,
author = {Freire, Willian and Tonh\~{a}o, Simone and Bonetti, Tiago and Shigenaga, Marcelo and Cadette, William and Felizardo, Fernando and Amaral, Aline and OliveiraJr, Edson and Colanzi, Thelma},
title = {On the configuration of multi-objective evolutionary algorithms for PLA design optimization},
year = {2021},
isbn = {9781450384193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483899.3483905},
doi = {10.1145/3483899.3483905},
abstract = {Search-based algorithms have been successfully applied in the Product Line Architecture (PLA) optimization using the seminal approach called Multi-Objective Approach for Product-Line Architecture Design (MOA4PLA). This approach produces a set of alternative PLA designs intending to improve the different factors being optimized. Currently, the MOA4PLA uses the NSGA-II algorithm, a multi-objective evolutionary algorithm (MOEA) that can optimize several architectural properties simultaneously. Despite the promising results, studying the best values for the algorithm parameters is essential to obtain even better results. This is also crucial to ease the adoption of MOA4PLA by newcomers or non-expert companies willing to start using search-based software engineering to PLA design. Three crossover operators for the PLA design optimization were proposed recently. However, reference values for parameters have not been defined for PLA design optimization using crossover operators. In this context, the objective of this work is conducting an experimental study to discover which are the most effective crossover operators and the best values to configure the MOEA parameters, such as population size, number of generations, and mutation and crossover rates. A quantitative analysis based on quality indicators and statistical tests was performed using four PLA designs to determine the most suitable parameter values to the search-based algorithm. Empirical results pointed out the best combination of crossover operators and the most suitable values to configure MOA4PLA.},
booktitle = {Proceedings of the 15th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {software product line, software architecture, recombination operators, Multi-objective evolutionary algorithm},
location = {Joinville, Brazil},
series = {SBCARS '21}
}

@inproceedings{10.1145/2993236.2993252,
author = {Rothberg, Valentin and Dietrich, Christian and Ziegler, Andreas and Lohmann, Daniel},
title = {Towards scalable configuration testing in variable software},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993252},
doi = {10.1145/2993236.2993252},
abstract = {Testing a software product line such as Linux implies building the source with different configurations. Manual approaches to generate configurations that enable code of interest are doomed to fail due to the high amount of variation points distributed over the feature model, the build system and the source code. Research has proposed various approaches to generate covering configurations, but the algorithms show many drawbacks related to run-time, exhaustiveness and the amount of generated configurations. Hence, analyzing an entire Linux source can yield more than 30 thousand configurations and thereby exceeds the limited budget and resources for build testing.  In this paper, we present an approach to fill the gap between a systematic generation of configurations and the necessity to fully build software in order to test it. By merging previously generated configurations, we reduce the number of necessary builds and enable global variability-aware testing. We reduce the problem of merging configurations to finding maximum cliques in a graph. We evaluate the approach on the Linux kernel, compare the results to common practices in industry, and show that our implementation scales even when facing graphs with millions of edges.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {156–167},
numpages = {12},
keywords = {Software Testing, Software Product Lines, Sampling, Linux, Configurability},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@inproceedings{10.1145/3510003.3510053,
author = {Xiang, Yi and Huang, Han and Zhou, Yuren and Li, Sizhe and Luo, Chuan and Lin, Qingwei and Li, Miqing and Yang, Xiaowei},
title = {Search-based diverse sampling from real-world software product lines},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510053},
doi = {10.1145/3510003.3510053},
abstract = {Real-world software product lines (SPLs) often encompass enormous valid configurations that are impossible to enumerate. To understand properties of the space formed by all valid configurations, a feasible way is to select a small and valid sample set. Even though a number of sampling strategies have been proposed, they either fail to produce diverse samples with respect to the number of selected features (an important property to characterize behaviors of configurations), or achieve diverse sampling but with limited scalability (the handleable configuration space size is limited to 1013). To resolve this dilemma, we propose a scalable diverse sampling strategy, which uses a distance metric in combination with the novelty search algorithm to produce diverse samples in an incremental way. The distance metric is carefully designed to measure similarities between configurations, and further diversity of a sample set. The novelty search incrementally improves diversity of samples through the search for novel configurations. We evaluate our sampling algorithm on 39 real-world SPLs. It is able to generate the required number of samples for all the SPLs, including those which cannot be counted by sharpSAT, a state-of-the-art model counting solver. Moreover, it performs better than or at least competitively to state-of-the-art samplers regarding diversity of the sample set. Experimental results suggest that only the proposed sampler (among all the tested ones) achieves scalable diverse sampling.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1945–1957},
numpages = {13},
keywords = {distance metric, diverse sampling, novelty search, software product lines},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3180155.3180257,
author = {Xue, Yinxing and Li, Yan-Fu},
title = {Multi-objective integer programming approaches for solving optimal feature selection problem: a new perspective on multi-objective optimization problems in SBSE},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180257},
doi = {10.1145/3180155.3180257},
abstract = {The optimal feature selection problem in software product line is typically addressed by the approaches based on Indicator-based Evolutionary Algorithm (IBEA). In this study we first expose the mathematical nature of this problem --- multi-objective binary integer linear programming. Then, we implement/propose three mathematical programming approaches to solve this problem at different scales. For small-scale problems (roughly less than 100 features), we implement two established approaches to find all exact solutions. For medium-to-large problems (roughly, more than 100 features), we propose one efficient approach that can generate a representation of the entire Pareto front in linear time complexity. The empirical results show that our proposed method can find significantly more non-dominated solutions in similar or less execution time, in comparison with IBEA and its recent enhancement (i.e., IBED that combines IBEA and Differential Evolution).},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1231–1242},
numpages = {12},
keywords = {multi-objective integer programming (MOIP), multi-objective optimization (MOO), optimal feature selection problem},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/2556624.2556628,
author = {Lengauer, Philipp and Bitto, Verena and Angerer, Florian and Gr\"{u}nbacher, Paul and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {Where has all my memory gone? determining memory characteristics of product variants using virtual-machine-level monitoring},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556628},
doi = {10.1145/2556624.2556628},
abstract = {Non-functional properties such as memory footprint have recently gained importance in software product line research. However, determining the memory characteristics of individual features and product variants is extremely challenging. We present an approach that supports the monitoring of memory characteristics of individual features at the level of Java virtual machines. Our approach provides extensions to Java virtual machines to track memory allocations and deal-locations of individual features based on a feature-to-code mapping. The approach enables continuous monitoring at the level of features to detect anomalies such as memory leaks, excessive memory consumption, or abnormal garbage collection times in product variants. We provide an evaluation of our approach based on different product variants of the DesktopSearcher product line. Our experiment with different program inputs demonstrates the feasibility of our technique.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {13},
numpages = {8},
keywords = {monitoring, memory footprint, feature-oriented software development, Java},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@article{10.1145/3712185,
author = {Cortellessa, Vittorio and Diaz-Pace, J. Andres and Di Pompeo, Daniele and Frank, Sebastian and Jamshidi, Pooyan and Tucci, Michele and van Hoorn, Andr\'{e}},
title = {Introducing Interactions in Multi-Objective Optimization of Software Architectures},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3712185},
doi = {10.1145/3712185},
abstract = {Software architecture optimization aims to enhance non-functional attributes like performance and reliability while meeting functional requirements. Multi-objective optimization employs metaheuristic search techniques, such as genetic algorithms, to explore feasible architectural changes and propose alternatives to designers. However, this resource-intensive process may not always align with practical constraints.This study investigates the impact of designer interactions on multi-objective software architecture optimization. Designers can intervene at intermediate points in the fully automated optimization process, making choices that guide exploration towards more desirable solutions. Through several controlled experiments as well as an initial user study (14 subjects), we compare this interactive approach with a fully automated optimization process, which serves as a baseline. The findings demonstrate that designer interactions lead to a more focused solution space, resulting in improved architectural quality. By directing the search towards regions of interest, the interaction uncovers architectures that remain unexplored in the fully automated process. In the user study, participants found that our interactive approach provides a better trade-off between sufficient exploration of the solution space and the required computation time.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {search-based software engineering, interaction, optimization, refactoring, non-functional attributes}
}

@inproceedings{10.1145/3698587.3701354,
author = {Das, Manas Jyoti and Rao, Praveen and Xu, Lisong},
title = {Impact of the Networking Infrastructure on the Performance of Variant Calling on Human Genomes in Commodity Clusters},
year = {2024},
isbn = {9798400713026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698587.3701354},
doi = {10.1145/3698587.3701354},
abstract = {A whole genome sequence of a human can consume gigabytes of storage space. Analyzing a large number of such sequences is a compute and memory intensive process. In this regard, cluster computing has emerged as an attractive solution for large-scale human genome analysis. In this paper, we investigate how the underlying networking infrastructure of a commodity cluster can impact the performance of variant calling, which is a key task to identify variations in a human's genome compared to the reference genome. We measured the performance of variant calling and analyzed the network traffic in 16-node clusters with different hardware and network bandwidth configurations. We observed that by increasing the network link bandwidth, the execution time of variant calling did not improve significantly due to the degree of parallelism that was achievable leading to underutilization of the links. However, with low bandwidth links, data shuffling errors become more likely leading to failures. Furthermore, higher network latency among cluster nodes led to slower execution of variant calling due to lower utilization of the processor cores. By appropriately choosing the network link bandwidth for a cluster, good performance can be achieved while lowering the price of processing genomes especially in a pay-as-you-go pricing model.},
booktitle = {Proceedings of the 15th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {1},
numpages = {11},
keywords = {Variant calling, cluster computing, human genomes, networking},
location = {Shenzhen, China},
series = {BCB '24}
}

@inproceedings{10.1145/3442391.3442403,
author = {Michelon, Gabriela K. and Linsbauer, Lukas and Assun\c{c}\~{a}o, Wesley K.G. and Fischer, Stefan and Egyed, Alexander},
title = {A Hybrid Feature Location Technique for Re-engineeringSingle Systems into Software Product Lines},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442403},
doi = {10.1145/3442391.3442403},
abstract = {Software product lines (SPLs) are known for improving productivity and reducing time-to-market through the systematic reuse of assets. SPLs are adopted mainly by re-engineering existing system variants. Feature location techniques (FLTs) support the re-engineering process by mapping the variants’ features to their implementation. However, such FLTs do not perform well when applied to single systems. In this way, there is a lack of FLTs to aid the re-engineering process of a single system into an SPL. In this work, we present a hybrid technique that consists of two complementary types of analysis: i) a dynamic analysis by runtime monitoring traces of scenarios in which features of the system are exercised individually, and ii) a static analysis for refining overlapping traces. We evaluate our technique on three subject systems by computing the common metrics used in FL research. We thus computed Precision, Recall, and F-Score at the line- and method-level of source code. In addition to that, one of the systems has a ground truth available, which we also used for comparing results. Results show that our FLT reached an average of 68-78% precision and 72-81% recall on two systems at the line-level, and 67-65% precision and 68-48% recall at the method-level. In these systems, most of the implementation can be covered by the exercise of the features. For the largest system, our technique reached a precision of up to 99% at the line-level, 94% at the method-level, and 44% when comparing to traces. However, due to its size, it was difficult to reach high code coverage during execution, and thus the recall obtained was on average of 28% at the line-level, 25% at the method-level, and 30% when comparing to traces. The main contribution of this work is a hybrid FLT, its publicly available implementation, and a replication package for comparisons and future studies.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {11},
numpages = {9},
keywords = {traceability, software reuse, runtime monitoring, feature location},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.1145/3555776.3577830,
author = {Xu, Hao and Baarir, Souheib and Ziadi, Tewfik and Essodaigui, Siham and Bossu, Yves and Messan Hillah, Lom},
title = {Optimization of the Product Configuration System of Renault},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577830},
doi = {10.1145/3555776.3577830},
abstract = {The problem of configuring a variability model is widespread in many different domains. Renault has developed its technology internally to model vehicle diversity. This technology relies on the approach known as knowledge compilation to explore the configurations space. However, the growing variability and complexity of the vehicles' range hardens the space representation problem and impacts performance requirements. This paper tackles these issues by exploiting symmetries that represent isomorphic parts in the configurations space. A new method describes how these symmetries are exploited and integrated. The extensive experiments we conducted on datasets from the automobile manufacturer show our approach's robustness and effectiveness: the achieved gain is a reduction of 52.13% in space representation on average.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1486–1489},
numpages = {4},
keywords = {value symmetries, variability model, product line, knowledge compilation},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@article{10.1145/3708527,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Marchezan, Luciano and Arkoh, Lawrence and Egyed, Alexander and Ramler, Rudolf},
title = {Contemporary Software Modernization: Strategies, Driving Forces, and Research Opportunities},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708527},
doi = {10.1145/3708527},
abstract = {Software modernization is a common activity in software engineering, since technologies advance, requirements change, and business models evolve. Differently from conventional software evolution (e.g., adding new features, enhancing performance, or adapting to new requirements), software modernization involves re-engineering entire legacy systems (e.g., changing the technology stack, migrating to a new architecture style, or programming paradigms). Given the pervasive nature of software today, modernizing legacy systems is paramount to provide customers with competitive and innovative products and services, while keeping companies profitable. Despite the prevalent discussion of software modernization in gray literature, and the many papers in the literature, there is no work presenting a “big picture” of contemporary software modernization, describing challenges, and providing a well-defined research agenda. The goal of this work is to describe the state of the art in software modernization in the past 10 years. We collect the state of the art by performing a rapid review (searching five digital libraries), identifying potential 3,460 studies, leading to a final set of 127. We analyzed these studies to understand which strategies are employed, the driving forces that lead organizations to modernize their systems, and the challenges that need to be addressed. The results show that studies in the last 10 years have explored eight strategies for modernizing legacy systems, namely cloudification, architecture redesign, moving to a new programming language, targeting reuse optimization, software modernization for new hardware integration, practices to leverage automation, database modernization, and digital transformation. Modernization is triggered by 14 driving forces, with the most common ones being reducing operational costs, improving performance and scalability, and reducing complexity. In addition, based on the analysis of existing literature, we present a detailed discussion of research opportunities in this field. The main challenges are providing tooling support, followed by defining a modernization process and considering better evaluation metrics. The main contribution of our work is to equip practitioners and researchers with knowledge of the current state of contemporary software modernization so that they are aware of practices and challenges to be addressed when deciding to modernize legacy systems.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {Software Evolution, Software Migration, Re-designing, Re-engineering}
}

@article{10.1145/3709722,
author = {Sharma, Chandan and Genev\`{e}s, Pierre and Gesbert, Nils and Laya\"{\i}da, Nabil},
title = {Schema-Based Query Optimisation for Graph Databases},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/3709722},
doi = {10.1145/3709722},
abstract = {Recursive graph queries are increasingly popular for extracting information from interconnected data found in various domains such as social networks, life sciences, and business analytics. Graph data often come with schema information that describe how nodes and edges are organized. We propose a type inference mechanism that enriches recursive graph queries with relevant structural information contained in a graph schema. We show that this schema information can be useful in order to improve the performance when evaluating recursive graph queries. Furthermore, we prove that the proposed method is sound and complete, ensuring that the semantics of the query is preserved during the schema-enrichment process.},
journal = {Proc. ACM Manag. Data},
month = feb,
articleno = {72},
numpages = {29},
keywords = {graph databases, graph schema, query optimisation, relational algebra}
}

@inproceedings{10.1145/2463372.2463545,
author = {Wang, Shuai and Ali, Shaukat and Gotlieb, Arnaud},
title = {Minimizing test suites in software product lines using weight-based genetic algorithms},
year = {2013},
isbn = {9781450319638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463372.2463545},
doi = {10.1145/2463372.2463545},
abstract = {Test minimization techniques aim at identifying and eliminating redundant test cases from test suites in order to reduce the total number of test cases to execute, thereby improving the efficiency of testing. In the context of software product line, we can save effort and cost in the selection and minimization of test cases for testing a specific product by modeling the product line. However, minimizing the test suite for a product requires addressing two potential issues: 1) the minimized test suite may not cover all test requirements compared with the original suite; 2) the minimized test suite may have less fault revealing capability than the original suite. In this paper, we apply weight-based Genetic Algorithms (GAs) to minimize the test suite for testing a product, while preserving fault detection capability and testing coverage of the original test suite. The challenge behind is to define an appropriate fitness function, which is able to preserve the coverage of complex testing criteria (e.g., Combinatorial Interaction Testing criterion). Based on the defined fitness function, we have empirically evaluated three different weight-based GAs on an industrial case study provided by Cisco Systems, Inc. Norway. We also presented our results of applying the three weight-based GAs on five existing case studies from the literature. Based on these case studies, we conclude that among the three weight-based GAs, Random-Weighted GA (RWGA) achieved significantly better performance than the other ones.},
booktitle = {Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation},
pages = {1493–1500},
numpages = {8},
keywords = {weight-based gas, test minimization, feature pairwise coverage, fault detection capability},
location = {Amsterdam, The Netherlands},
series = {GECCO '13}
}

@inproceedings{10.1145/3629479.3629501,
author = {Oliveira, Elisandra Souza De and Neves, Jhuan Magno Pisa and Cruz, Andr\'{e} Figliuolo Da and Bezerra, Erick Costa},
title = {Work Product Review Process Applied to Test Cases Review for Software Testing},
year = {2023},
isbn = {9798400707865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629479.3629501},
doi = {10.1145/3629479.3629501},
abstract = {Testing is one of the most important factors in the software development cycle to obtain a quality product. It is essential to carry out effective tests to ensure that the software works as expected, meets all project requirements, and does not present defects after implementation. For this, the test cases must have a well-written and adequate structure, with actions and conditions that verify and cover all the functionalities and behaviors of the system. Even with several changes in requirements, business rules, new features, or adaptations that may occur during product development. This work describes an industrial case study where the work review process established by ISO/IEC 20246:2017 is implemented in the review of software test cases. As a result, a reduction in test execution time was achieved, as well as a reduction in the number of professionals involved as a result of better detection of defects during tests.},
booktitle = {Proceedings of the XXII Brazilian Symposium on Software Quality},
pages = {274–280},
numpages = {7},
keywords = {Test case review, software quality, software review, software testing, static testing, work product review},
location = {Bras\'{\i}lia, Brazil},
series = {SBQS '23}
}

@inproceedings{10.1145/3691620.3695023,
author = {Sun, Kexin and Ren, Yiding and Kuang, Hongyu and Gao, Hui and Ma, Xiaoxing and Rong, Guoping and Shao, Dong and Zhang, He},
title = {AVIATE: Exploiting Translation Variants of Artifacts to Improve IR-based Traceability Recovery in Bilingual Software Projects},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695023},
doi = {10.1145/3691620.3695023},
abstract = {Traceability plays a vital role in facilitating various software development activities by establishing the traces between different types of artifacts (e.g., issues and commits in software repositories). Among the explorations for automated traceability recovery, the IR (Information Retrieval)-based approaches leverage textual similarity to measure the likelihood of traces between artifacts and show advantages in many scenarios. However, the globalization of software development has introduced new challenges, such as the possible multilingualism on the same concept (e.g., "[SEE PDF]" vs. "attribute") in the artifact texts, thus significantly hampering the performance of IR-based approaches. Existing research has shown that machine translation can help address the term inconsistency in bilingual projects. However, the translation can also bring in synonymous terms that are not consistent with those in the bilingual projects (e.g., another translation of "[SEE PDF]" as "property"). Therefore, we propose an enhancement strategy called AVIATE that exploits translation variants from different translators by utilizing the word pairs that appear simultaneously across the translation variants from different kinds artifacts (a.k.a. consensual biterms). We use these biterms to first enrich the artifact texts, and then to enhance the calculated IR values for improving IR-based trace-ability recovery for bilingual software projects. The experiments on 17 bilingual projects (involving English and 4 other languages) demonstrate that AVIATE significantly outperformed the IR-based approach with machine translation (the state-of-the-art in this field) with an average increase of 16.67 in Average Precision (31.43%) and 8.38 (11.22%) in Mean Average Precision, indicating its effectiveness in addressing the challenges of multilingual traceability recovery.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {519–530},
numpages = {12},
keywords = {traceability recovery, cross-lingual information retrieval, biterm},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/2491411.2491455,
author = {Davril, Jean-Marc and Delfosse, Edouard and Hariri, Negar and Acher, Mathieu and Cleland-Huang, Jane and Heymans, Patrick},
title = {Feature model extraction from large collections of informal product descriptions},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491455},
doi = {10.1145/2491411.2491455},
abstract = {Feature Models (FMs) are used extensively in software product line engineering to help generate and validate individual product configurations and to provide support for domain analysis. As FM construction can be tedious and time-consuming, researchers have previously developed techniques for extracting FMs from sets of formally specified individual configurations, or from software requirements specifications for families of existing products. However, such artifacts are often not available. In this paper we present a novel, automated approach for constructing FMs from publicly available product descriptions found in online product repositories and marketing websites such as SoftPedia and CNET. While each individual product description provides only a partial view of features in the domain, a large set of descriptions can provide fairly comprehensive coverage. Our approach utilizes hundreds of partial product descriptions to construct an FM and is described and evaluated against antivirus product descriptions mined from SoftPedia.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {290–300},
numpages = {11},
keywords = {Product Lines, Feature Models, Domain Analysis},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inproceedings{10.1145/2884781.2884821,
author = {Devroey, Xavier and Perrouin, Gilles and Papadakis, Mike and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Featured model-based mutation analysis},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884821},
doi = {10.1145/2884781.2884821},
abstract = {Model-based mutation analysis is a powerful but expensive testing technique. We tackle its high computation cost by proposing an optimization technique that drastically speeds up the mutant execution process. Central to this approach is the Featured Mutant Model, a modelling framework for mutation analysis inspired by the software product line paradigm. It uses behavioural variability models, viz., Featured Transition Systems, which enable the optimized generation, configuration and execution of mutants. We provide results, based on models with thousands of transitions, suggesting that our technique is fast and scalable. We found that it outperforms previous approaches by several orders of magnitude and that it makes higher-order mutation practically applicable.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {655–666},
numpages = {12},
keywords = {variability, mutation analysis, featured transition systems},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/2491956.2491976,
author = {Bodden, Eric and Tol\^{e}do, T\'{a}rsis and Ribeiro, M\'{a}rcio and Brabrand, Claus and Borba, Paulo and Mezini, Mira},
title = {SPLLIFT: statically analyzing software product lines in minutes instead of years},
year = {2013},
isbn = {9781450320146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491956.2491976},
doi = {10.1145/2491956.2491976},
abstract = {A software product line (SPL) encodes a potentially large variety of software products as variants of some common code base. Up until now, re-using traditional static analyses for SPLs was virtually intractable, as it required programmers to generate and analyze all products individually. In this work, however, we show how an important class of existing inter-procedural static analyses can be transparently lifted to SPLs. Without requiring programmers to change a single line of code, our approach SPLLIFT automatically converts any analysis formulated for traditional programs within the popular IFDS framework for inter-procedural, finite, distributive, subset problems to an SPL-aware analysis formulated in the IDE framework, a well-known extension to IFDS. Using a full implementation based on Heros, Soot, CIDE and JavaBDD, we show that with SPLLIFT one can reuse IFDS-based analyses without changing a single line of code. Through experiments using three static analyses applied to four Java-based product lines, we were able to show that our approach produces correct results and outperforms the traditional approach by several orders of magnitude.},
booktitle = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {355–364},
numpages = {10},
keywords = {software product lines, inter-procedural static analysis, flow sensitive, context sensitive},
location = {Seattle, Washington, USA},
series = {PLDI '13}
}

@inproceedings{10.1145/3338906.3338928,
author = {Shahin, Ramy and Chechik, Marsha and Salay, Rick},
title = {Lifting Datalog-based analyses to software product lines},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338928},
doi = {10.1145/3338906.3338928},
abstract = {Applying program analyses to Software Product Lines (SPLs) has been a fundamental research problem at the intersection of Product Line Engineering and software analysis. Different attempts have been made to ”lift” particular product-level analyses to run on the entire product line. In this paper, we tackle the class of Datalog-based analyses (e.g., pointer and taint analyses), study the theoretical aspects of lifting Datalog inference, and implement a lifted inference algorithm inside the Souffl\'{e} Datalog engine. We evaluate our implementation on a set of benchmark product lines. We show significant savings in processing time and fact database size (billions of times faster on one of the benchmarks) compared to brute-force analysis of each product individually.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {39–49},
numpages = {11},
keywords = {Souffl'{e}, Software Product Lines, Program Analysis, Pointer Analysis, Lifting, Doop, Datalog},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/2851613.2851959,
author = {Noorian, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Quality-centric feature model configuration using goal models},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851959},
doi = {10.1145/2851613.2851959},
abstract = {In software product line engineering, a feature model represents the possible configuration space and can be customized based on the stakeholders' needs. Considering the complexity of feature models in addition to the diversity of the stake-holders' expectations, the configuration process is viewed as a complex optimization problem. In this paper, we propose a holistic approach for the configuration process that seeks to satisfy the stakeholders' requirements as well as the feature models' structural and integrity constraints. Here, we model stakeholders' functional and non-functional needs and their preferences using requirement engineering goal models. We formalize the structure of the feature model, the stake-holders' objectives, and their preferences in the form of an integer linear program to automatically perform feature selection.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1296–1299},
numpages = {4},
keywords = {configuration process, feature model, goal model},
location = {Pisa, Italy},
series = {SAC '16}
}

@article{10.5555/3722577.3722966,
author = {Ablin, Pierre and Vary, Simon and Gao, Bin and Absil, P.-A.},
title = {Infeasible deterministic, stochastic, and variance-reduction algorithms for optimization under orthogonality constraints},
year = {2024},
issue_date = {January 2024},
publisher = {JMLR.org},
volume = {25},
number = {1},
issn = {1532-4435},
abstract = {Orthogonality constraints naturally appear in many machine learning problems, from principal component analysis to robust neural network training. They are usually solved using Riemannian optimization algorithms, which minimize the objective function while enforcing the constraint. However, enforcing the orthogonality constraint can be the most time-consuming operation in such algorithms. Recently, Ablin and Peyr\'{e} (2022) proposed the landing algorithm, a method with cheap iterations that does not enforce the orthogonality constraints but is attracted towards the manifold in a smooth manner. This article provides new practical and theoretical developments for the landing algorithm. First, the method is extended to the Stiefel manifold, the set of rectangular orthogonal matrices. We also consider stochastic and variance reduction algorithms when the cost function is an average of many functions. We demonstrate that all these methods have the same rate of convergence as their Riemannian counterparts that exactly enforce the constraint, and converge to the manifold. Finally, our experiments demonstrate the promise of our approach to an array of machine-learning problems that involve orthogonality constraints.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {389},
numpages = {38},
keywords = {orthogonal manifold, stiefel manifold, stochastic optimization, variance reduction}
}

@inproceedings{10.1109/SC41406.2024.00063,
author = {You, Xin and Xuan, Zhibo and Yang, Hailong and Luan, Zhongzhi and Liu, Yi and Qian, Depei},
title = {GVARP: Detecting Performance Variance on Large-Scale Heterogeneous Systems},
year = {2024},
isbn = {9798350352917},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC41406.2024.00063},
doi = {10.1109/SC41406.2024.00063},
abstract = {Performance variance is one of the nasty pitfalls of large-scale heterogeneous systems, which can lead to unexpected and unpredictable performance degradation for parallel programs. Such performance issues typically arise from various random hardware and software faults, making it exceedingly difficult to pinpoint the exact causes of performance variance in specific instances. In this paper, we propose GVARP, a performance variance detection tool for large-scale heterogeneous systems. GVARP employs static analysis to identify the performance-critical parameters of kernel functions. Additionally, GVARP segments the program execution with external library calls and asynchronous kernel operations. Then GVARP constructs a state transfer graph and estimates the workload of each program segment to identify and cluster instances of similar workloads, facilitating the detection of performance variance. Our evaluation results demonstrate that GVARP effectively detects performance variance at a large scale with acceptable overhead and provides intuitive insights to locate the sources of performance variance.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {57},
numpages = {16},
keywords = {Large-Scale Heterogeneous System, Performance Analysis, Performance Variance},
location = {Atlanta, GA, USA},
series = {SC '24}
}

@article{10.1109/TCBB.2018.2881975,
author = {Gamaarachchi, Hasindu and Bayat, Arash and Gaeta, Bruno and Parameswaran, Sri},
title = {Cache Friendly Optimisation of de Bruijn Graph Based Local Re-Assembly in Variant Calling},
year = {2020},
issue_date = {July-Aug. 2020},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {17},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2018.2881975},
doi = {10.1109/TCBB.2018.2881975},
abstract = {A variant caller is used to identify variations in an individual genome (compared to the reference genome) in a genome processing pipeline. For the sake of accuracy, modern variant callers perform many local re-assemblies on small regions of the genome using a graph-based algorithm. However, such graph-based data structures are inefficiently stored in the linear memory of modern computers, which in turn reduces computing efficiency. Therefore, variant calling can take several CPU hours for a typical human genome. We have sped up the local re-assembly algorithm with no impact on its accuracy, by the effective use of the memory hierarchy. The proposed algorithm maximises data locality so that the fast internal processor memory (cache) is efficiently used. By the increased use of caches, accesses to main memory are minimised. The resulting algorithm is up to twice as fast as the original one when executed on a commodity computer and could gain even more speed up on computers with less complex memory subsystems.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = aug,
pages = {1125–1133},
numpages = {9}
}

@inproceedings{10.1145/2814228.2814229,
author = {Arzt, Steven and Nadi, Sarah and Ali, Karim and Bodden, Eric and Erdweg, Sebastian and Mezini, Mira},
title = {Towards secure integration of cryptographic software},
year = {2015},
isbn = {9781450336888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814228.2814229},
doi = {10.1145/2814228.2814229},
abstract = {While cryptography is now readily available to everyone and can, provably, protect private information from attackers, we still frequently hear about major data leakages, many of which are due to improper use of cryptographic mechanisms. The problem is that many application developers are not cryptographic experts. Even though high-quality cryptographic APIs are widely available, programmers often select the wrong algorithms or misuse APIs due to a lack of understanding. Such issues arise with both simple operations such as encryption as well as with complex secure communication protocols such as SSL. In this paper, we provide a long-term solution that helps application developers integrate cryptographic components correctly and securely by bridging the gap between cryptographers and application developers. Our solution consists of a software product line (with an underlying feature model) that automatically identifies the correct cryptographic algorithms to use, based on the developer's answers to high-level questions in non-expert terminology. Each feature (i.e., cryptographic algorithm) maps into corresponding Java code and a usage protocol describing API restrictions. By composing the user's selected features, we automatically synthesize a secure code blueprint and a usage protocol that corresponds to the selected usage scenario. Since the developer may change the application code over time, we use the usage protocols to statically analyze the program and ensure that the correct use of the API is not violated over time.},
booktitle = {2015 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (Onward!)},
pages = {1–13},
numpages = {13},
keywords = {typestate analysis, cryptography, Software product lines, API protocols},
location = {Pittsburgh, PA, USA},
series = {Onward! 2015}
}

@inproceedings{10.1145/3705618.3705622,
author = {Liu, Yuan and Zhan, Liuqi and Hao, Jingjing},
title = {Static Cloud Vector for Computing the General Quality performance of Outsourcing components in Complex Product Supply Chain},
year = {2025},
isbn = {9798400711855},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3705618.3705622},
doi = {10.1145/3705618.3705622},
abstract = {In the supply chains for complex products, customized and digitally crafted outsourcing components sourced globally from small samples face challenges in quality inspection. Subjective and objective factors lead to ambiguity and randomness in data, forming the cloud area of quality performance. This study introduces a novel data-driven cloud model to quantify this quality performance. We develop a static quality cloud vector and propose the Signal-to-Noise (SN) ratio to assess data stability. The weight of the cloud vector is designed based on the SN of the cloud. A case study in aircraft supply chains validates our model's feasibility and effectiveness, ultimately aiding manufacturers in better understanding and controlling outsourced quality levels.},
booktitle = {Proceedings of the 2024 International Conference on Digital Economy and Computer Science},
pages = {20–24},
numpages = {5},
keywords = {Complex products, Small samples, Static cloud vector, The SN ratio of cloud},
location = {
},
series = {DECS '24}
}

@inproceedings{10.1145/3564719.3568695,
author = {Hentze, Marc and Pett, Tobias and Sundermann, Chico and Krieter, Sebastian and Th\"{u}m, Thomas and Schaefer, Ina},
title = {Generic Solution-Space Sampling for Multi-domain Product Lines},
year = {2022},
isbn = {9781450399203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564719.3568695},
doi = {10.1145/3564719.3568695},
abstract = {Validating a configurable software system is challenging, as there are potentially millions of configurations, which makes testing each configuration individually infeasible. Thus, existing sampling algorithms allow to compute a representative subset of configurations, called sample, that can be tested instead. However, sampling on the set of configurations may miss potential error sources on implementation level. In this paper, we present solution-space sampling, a concept that mitigates this problem by allowing to sample directly on the implementation level. We apply solution-space sampling to six real-word, automotive product lines and show that it produces up to 56 % smaller samples, while also covering all potential error sources missed by problem-space sampling.},
booktitle = {Proceedings of the 21st ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {135–147},
numpages = {13},
keywords = {solution-space analysis, feature-model sampling, Multi-domain product lines},
location = {Auckland, New Zealand},
series = {GPCE 2022}
}

@inproceedings{10.1145/3688671.3688795,
author = {Maher, Mona and Baytar, Fatma},
title = {Predicting 3D garment fit in digital product development: Selecting input variables},
year = {2024},
isbn = {9798400709821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3688671.3688795},
doi = {10.1145/3688671.3688795},
abstract = {Three-dimensional (3D) technologies such as 3D digital prototyping have recently been used in apparel engineering to reduce the number of costly physical prototypes. This technology can simulate garment drapes by digitally sewing 2D patterns and placing them on a 3D avatar. Yet, current limitations in simulation accuracy mean physical prototypes are still indispensable. Integrating AI technologies shows promise in accelerating digital transformation in the apparel industry. AI can support decision-making in apparel design, size and fit prediction, and fashion recommendations. It can also assist decision-making in technical design by predicting garment fit before making physical prototypes or offering solutions for misfit areas. Research indicates that models such as Naive Bayes (NB), Support Vector Machine (SVM), and Artificial Neural Networks (ANN) offer robust solutions for fit evaluation. However, they require further testing across diverse garment styles and demographics. Developing a nuanced classification system for fit conditions could improve model accuracy, ultimately supporting the industry's shift towards digital and AI-enhanced product development. In this study, we propose a framework to achieve such a system.},
booktitle = {Proceedings of the 13th Hellenic Conference on Artificial Intelligence},
articleno = {60},
numpages = {5},
keywords = {3D simulations, AI, Garment fit evaluation, New Product development, Technical apparel design},
location = {
},
series = {SETN '24}
}

@article{10.1145/3176644,
author = {Xiang, Yi and Zhou, Yuren and Zheng, Zibin and Li, Miqing},
title = {Configuring Software Product Lines by Combining Many-Objective Optimization and SAT Solvers},
year = {2018},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3176644},
doi = {10.1145/3176644},
abstract = {A feature model (FM) is a compact representation of the information of all possible products from software product lines. The optimal feature selection involves the simultaneous optimization of multiple (usually more than three) objectives in a large and highly constrained search space. By combining our previous work on many-objective evolutionary algorithm (i.e., VaEA) with two different satisfiability (SAT) solvers, this article proposes a new approach named SATVaEA for handling the optimal feature selection problem. In SATVaEA, an FM is simplified with the number of both features and constraints being reduced greatly. We enhance the search of VaEA by using two SAT solvers: one is a stochastic local search--based SAT solver that can quickly repair infeasible configurations, whereas the other is a conflict-driven clause-learning SAT solver that is introduced to generate diversified products. We evaluate SATVaEA on 21 FMs with up to 62,482 features, including two models with realistic values for feature attributes. The experimental results are promising, with SATVaEA returning 100% valid products on almost all FMs. For models with more than 10,000 features, the search in SATVaEA takes only a few minutes. Concerning both effectiveness and efficiency, SATVaEA significantly outperforms other state-of-the-art algorithms.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {14},
numpages = {46},
keywords = {vector angle--based evolutionary algorithm (VaEA), satisfiability (SAT) solvers, many-objective optimization, Optimal feature selection}
}

@inproceedings{10.1145/3510466.3510484,
author = {Ratzenb\"{o}ck, Michael and Gr\"{u}nbacher, Paul and Assun\c{c}ao, Wesley K. G. and Egyed, Alexander and Linsbauer, Lukas},
title = {Refactoring Product Lines by Replaying Version Histories},
year = {2022},
isbn = {9781450396042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510466.3510484},
doi = {10.1145/3510466.3510484},
abstract = {When evolving software product lines, new features are added over time and existing features are revised. Engineers also decide to merge different features or split features in other cases. Such refactoring tasks are difficult when using manually maintained feature-to-code mappings. Intensional version control systems such as ECCO overcome this issue with automatically computed feature-to-code mappings. Furthermore, they allow creating variants that have not been explicitly committed before. However, such systems are still rarely used compared to extensional version control systems like Git, which keep track of the evolution history by assigning revisions to states of a system. This paper presents an approach combining both extensional and intensional version control systems, which relies on the extensional version control system Git to store versions. Developers selectively tag existing versions to describe the evolution at the level of features. Our approach then automatically replays the evolution history to create a repository of the intensional variation control system ECCO. The approach contributes to research on refactoring features of existing product lines and migrating existing systems to product lines. We provide an initial evaluation of the approach regarding correctness and performance based on an existing system.},
booktitle = {Proceedings of the 16th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {8},
numpages = {10},
keywords = {version control systems, refactoring, feature-level evolution},
location = {Florence, Italy},
series = {VaMoS '22}
}

@inproceedings{10.1145/3624062.3624167,
author = {Brown, Nick and Jamieson, Maurice and Lydike, Anton and Bauer, Emilien and Grosser, Tobias},
title = {Fortran performance optimisation and auto-parallelisation by leveraging MLIR-based domain specific abstractions in Flang},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624167},
doi = {10.1145/3624062.3624167},
abstract = {MLIR has become popular since it was open sourced in 2019. A sub-project of LLVM, the flexibility provided by MLIR to represent Intermediate Representations (IR) as dialects at different abstraction levels, to mix these, and to leverage transformations between dialects provides opportunities for automated program optimisation and parallelisation. In addition to general purpose compilers built upon MLIR, domain specific abstractions have also been developed. In this paper we explore complimenting the Flang MLIR general purpose compiler by combining with the domain specific Open Earth Compiler’s MLIR stencil dialect. Developing transformations to discover and extracts stencils from Fortran, this specialisation delivers between a 2- and 10-times performance improvement for our benchmarks on a Cray supercomputer compared to using Flang alone. Furthermore, by leveraging existing MLIR transformations we develop an auto-parallelisation approach targeting multi-threaded and distributed memory parallelism, and optimised execution on GPUs, without any modifications to the serial Fortran source code.},
booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {904–913},
numpages = {10},
keywords = {HPC, LLVM, MLIR, stencil based computation, xDSL},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@inproceedings{10.1145/1529282.1529388,
author = {Bure\v{s}, Tom\'{a}\v{s} and Hn\v{e}tynka, Petr and Malohlava, Michal},
title = {Using a product line for creating component systems},
year = {2009},
isbn = {9781605581668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1529282.1529388},
doi = {10.1145/1529282.1529388},
abstract = {Component systems have become a wide-spread technology and found their place in several application domains. Each component system has its specifics and particularities that reflect its focus and the application domain it is intended for. Although important, the diversity of component systems leads to a number of problems including having different tools for each systems, unnecessary duplication of functionality and problems with integration when several domains are to be targeted. Based on categorization of component application domains, we propose a "meta-component system", which provides a software product line for creating custom component systems. We focus especially on the deployment and execution environment, which is where most diversities are found. We demonstrate the usage of the "meta-component system" and propose how it is to be realized by two core concepts of SOFA 2, namely connector generator and microcomponents.},
booktitle = {Proceedings of the 2009 ACM Symposium on Applied Computing},
pages = {501–508},
numpages = {8},
keywords = {runtime environment, product line engineering, generative programming, component systems},
location = {Honolulu, Hawaii},
series = {SAC '09}
}

@inproceedings{10.5555/3712729.3712751,
author = {Moon, Sifat Afroj and Chen, Jiangzhuo and Espinoza, Baltazar and Lewis, Bryan and Marathe, Madhav and Outten, Joseph and Venkatramanan, Srinivasan and Vullikanti, Anil and Warren, Andrew},
title = {Agent-Based Simulation Framework for Multi-Variant Surveillance},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {Early detection of an emerging VOC (Variant-Of-Concern) is essential for effective preparedness for a disease like COVID-19. The spreading of an emerging VOC not only depends on the disease dynamics of itself but also depends on the state of the circulating variants and the susceptibility of the population. Resources for testing are typically quite limited, and a number of strategies have been considered for deploying them. However, it has been difficult to evaluate the performance of such strategies, especially higher order effects, and inequities, while incorporating constraints on these resources. Here, we develop an agent-based surveillance framework, NetworkDetect, to understand the early warning system of an emerging VOC. Our framework allows us to incorporate various population heterogeneities and resource constraints.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {276–287},
numpages = {12},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@inproceedings{10.1145/3385032.3385043,
author = {Bilic, Damir and Sundmark, Daniel and Afzal, Wasif and Wallin, Peter and Causevic, Adnan and Amlinger, Christoffer and Barkah, Dani},
title = {Towards a Model-Driven Product Line Engineering Process: An Industrial Case Study},
year = {2020},
isbn = {9781450375948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385032.3385043},
doi = {10.1145/3385032.3385043},
abstract = {Many organizations developing software-intensive systems face challenges with high product complexity and large numbers of variants. In order to effectively maintain and develop these product variants, Product-Line Engineering methods are often considered, while Model-based Systems Engineering practices are commonly utilized to tackle product complexity. In this paper, we report on an industrial case study concerning the ongoing adoption of Product Line Engineering in the Model-based Systems Engineering environment at Volvo Construction Equipment (Volvo CE) in Sweden. In the study, we identify and define a Product Line Engineering process that is aligned with Model-based Systems Engineering activities at the engines control department of Volvo CE. Furthermore, we discuss the implications of the migration from the current development process to a Model-based Product Line Engineering-oriented process. This process, and its implications, are derived by conducting and analyzing interviews with Volvo CE employees, inspecting artifacts and documents, and by means of participant observation. Based on the results of a first system model iteration, we were able to document how Model-based Systems Engineering and variability modeling will affect development activities, work products and stakeholders of the work products.},
booktitle = {Proceedings of the 13th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {9},
numpages = {11},
keywords = {Engine System Development, Model-Based Systems Engineering, Product Line Engineering},
location = {Jabalpur, India},
series = {ISEC '20}
}

@inproceedings{10.1145/2814251.2814263,
author = {Ochoa, Lina and Gonz\'{a}lez-Rojas, Oscar and Th\"{u}m, Thomas},
title = {Using decision rules for solving conflicts in extended feature models},
year = {2015},
isbn = {9781450336864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814251.2814263},
doi = {10.1145/2814251.2814263},
abstract = {Software Product Line Engineering has introduced feature modeling as a domain analysis technique used to represent the variability of software products and decision-making scenarios. We present a model-based transformation approach to solve conflicts among configurations performed by different stakeholders on feature models. We propose the usage of a domain-specific language named CoCo to specify attributes as non-functional properties of features, and to describe business-related decision rules in terms of costs, time, and human resources. These specifications along with the stakeholders' configurations and the feature model are transformed into a constraint programming problem, on which decision rules are executed to find a non-conflicting set of solution configurations that are aligned to business objectives. We evaluate CoCo's compositionality and model complexity simplification while using a set of motivating decision scenarios.},
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Software Language Engineering},
pages = {149–160},
numpages = {12},
keywords = {model transformation chain, extended feature model, domain-specific language, constraint satisfaction problem, conflicting configurations, Domain engineering},
location = {Pittsburgh, PA, USA},
series = {SLE 2015}
}

@article{10.1145/3524495,
author = {Jiang, Shouyong and Zou, Juan and Yang, Shengxiang and Yao, Xin},
title = {Evolutionary Dynamic Multi-objective Optimisation: A Survey},
year = {2022},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3524495},
doi = {10.1145/3524495},
abstract = {Evolutionary dynamic multi-objective optimisation (EDMO) is a relatively young but rapidly growing area of investigation. EDMO employs evolutionary approaches to handle multi-objective optimisation problems that have time-varying changes in objective functions, constraints, and/or environmental parameters. Due to the simultaneous presence of dynamics and multi-objectivity in problems, the optimisation difficulty for EDMO has a marked increase compared to that for single-objective or stationary optimisation. After nearly two decades of community effort, EDMO has achieved significant advancements on various topics, including theoretic research and applications. This article presents a broad survey and taxonomy of existing research on EDMO. Multiple research opportunities are highlighted to further promote the development of the EDMO research field.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {76},
numpages = {47},
keywords = {evolutionary dynamic multi-objective optimisation, dynamic environment, evolutionary algorithm, Multi-objective optimisation}
}

@article{10.1145/3687310,
author = {Ha, Soonhoi and Jeong, Eunjin},
title = {Software Optimization and Design Methodology for Low Power Computer Vision Systems},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {1539-9087},
url = {https://doi.org/10.1145/3687310},
doi = {10.1145/3687310},
abstract = {This tutorial article addresses a low power computer vision system as an example of a growing application domain of neural networks, exploring various technologies developed to enhance accuracy within the resource and performance constraints imposed by the hardware platform. Focused on a given hardware platform and network model, software optimization techniques, including pruning, quantization, low-rank approximation, and parallelization, aim to satisfy resource and performance constraints while minimizing accuracy loss. Due to the interdependence of model compression approaches, their systematic application is crucial, as evidenced by winning solutions in the Lower Power Image Recognition Challenge (LPIRC) of 2017 and 2018. Recognizing the typical heterogeneity of processing elements in contemporary hardware platforms, the effective utilization through parallelizing neural networks emerges as increasingly vital for performance enhancement. The article advocates for a more impactful strategy—designing a network architecture tailored to a specific hardware platform. For detailed information on each technique, the article provides corresponding references.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = dec,
articleno = {19},
numpages = {31},
keywords = {Optimization, neural architecture search, parallelization, embedded machine learning}
}

@inproceedings{10.1145/2684200.2684314,
author = {Murwantara, I Made and Bordbar, Behzad and Minku, Leandro L.},
title = {Measuring Energy Consumption for Web Service Product Configuration},
year = {2014},
isbn = {9781450330015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684200.2684314},
doi = {10.1145/2684200.2684314},
abstract = {Because of the economies of scale that Cloud provides, there is great interest in hosting web services on the Cloud. Web services are created from components such as Database Management Systems and HTTP servers. There is a wide variety of components that can be used to configure a web service. The choice of components influences the performance and energy consumption. Most current research in the web service technologies focuses on system performance, and only small number of researchers give attention to energy consumption. In this paper, we propose a method to select the web service configurations which reduce energy consumption. Our method has capabilities to manage feature configuration and predict energy consumption of web service systems. To validate, we developed a technique to measure energy consumption of several web service configurations running in a Virtualized environment. Our approach allows Cloud companies to provide choices of web service technology that consumes less energy.},
booktitle = {Proceedings of the 16th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {224–228},
numpages = {5},
keywords = {Web System, Software Product Line, Machine Learning, Energy Aware},
location = {Hanoi, Viet Nam},
series = {iiWAS '14}
}

@inproceedings{10.1145/3167132.3167353,
author = {Pereira, Juliana Alves and Martinez, Jabier and Gurudu, Hari Kumar and Krieter, Sebastian and Saake, Gunter},
title = {Visual guidance for product line configuration using recommendations and non-functional properties},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167353},
doi = {10.1145/3167132.3167353},
abstract = {Software Product Lines (SPLs) are a mature approach for the derivation of a family of products using systematic reuse. Different combinations of predefined features enable tailoring the product to fit the needs of each customer. These needs are related to functional properties of the system (optional features) as well as non-functional properties (e.g., performance or cost of the final product). In industrial scenarios, the configuration process of a final product is complex and the tool support is usually limited to check functional properties interdependencies. In addition, the importance of nonfunctional properties as relevant drivers during configuration has been overlooked. Thus, there is a lack of holistic paradigms integrating recommendation systems and visualizations that can help the decision makers. In this paper, we propose and evaluate an interrelated set of visualizations for the configuration process filling these gaps. We integrate them as part of the FeatureIDE tool and we evaluate its effectiveness, scalability, and performance.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {2058–2065},
numpages = {8},
keywords = {visualization, software product lines, recommendation systems, feature model, configuration},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3613904.3642409,
author = {Lev Ari, Eilat and Roichman, Maayan and Toch, Eran},
title = {Strategies of Product Managers: Negotiating Social Values in Digital Product Design},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642409},
doi = {10.1145/3613904.3642409},
abstract = {Product managers are central figures in digital product development, coordinating teams and prioritizing features. Despite their influence, little research explores how their decisions affect user experience, especially in integrating social values into product architecture. Employing a mixed-methods framework, we conducted semi-structured interviews with 20 product managers and an online survey with an additional 81, all based in Israel. Our study identifies four unique strategies product managers utilize to balance business goals, user satisfaction, and ethical considerations. The survey data further substantiates the prevalence of these strategies across diverse sectors, confirming they reflect industry-wide approaches in the Israeli tech sector rather than isolated practices. To conclude, we emphasize how “soft resistance” tactics, such as adjusting data interpretations based on personal values, impact digital product designs. Moreover, our findings highlight that maintaining an ethical reputation in the job market can be pivotal in shaping product design.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {227},
numpages = {17},
keywords = {Product managers, design, ethics, values},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/2991041.2991053,
author = {Alidra, Abdelghani and Kimour, Mohamed Tahar},
title = {Prototyping Software Product Lines analysis with Pharo},
year = {2016},
isbn = {9781450345248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2991041.2991053},
doi = {10.1145/2991041.2991053},
abstract = {Software Product Lines (SPLs) are an emerging software engineering paradigm that aims to optimize software development costs and time to market trough systematic development of reusable of core assets. At the heart of SPL engineering is variability modelling. Feature models (FM) are a common way to model variability and reason about it. Examples of reasoning are for instance checking that at least one product is represented by a given FM (satisfiability) or finding the product that best fits a given set of requirements. In practice however, such operations are often complex and time consuming. In order to address these challenges, we introduce in the present article the notion of transitive dependency between features and show how it can be used as the basis for efficient analysis and automatic reasoning on feature models. We exploit this new concept to implement a first platform for prototyping and reasoning on large SPLs in Pharo. Finally we illustrate the efficiency of our proposal on the problem of features selection optimisation.},
booktitle = {Proceedings of the 11th Edition of the International Workshop on Smalltalk Technologies},
articleno = {12},
numpages = {11},
keywords = {automatic reasoning, analysis environment, Software Product Lines, Feature models},
location = {Prague, Czech Republic},
series = {IWST'16}
}

@inproceedings{10.1145/3610548.3618244,
author = {Balint, Martin and Myszkowski, Karol and Seidel, Hans-Peter and Singh, Gurprit},
title = {Joint Sampling and Optimisation for Inverse Rendering},
year = {2023},
isbn = {9798400703157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610548.3618244},
doi = {10.1145/3610548.3618244},
abstract = {When dealing with difficult inverse problems such as inverse rendering, using Monte Carlo estimated gradients to optimise parameters can slow down convergence due to variance. Averaging many gradient samples in each iteration reduces this variance trivially. However, for problems that require thousands of optimisation iterations, the computational cost of this approach rises quickly. We derive a theoretical framework for interleaving sampling and optimisation. We update and reuse past samples with low-variance finite-difference estimators that describe the change in the estimated gradients between each iteration. By combining proportional and finite-difference samples, we continuously reduce the variance of our novel gradient meta-estimators throughout the optimisation process. We investigate how our estimator interlinks with Adam and derive a stable combination. We implement our method for inverse path tracing and demonstrate how our estimator speeds up convergence on difficult optimisation tasks.},
booktitle = {SIGGRAPH Asia 2023 Conference Papers},
articleno = {29},
numpages = {10},
keywords = {differentiable rendering, gradient descent, gradient estimation, inverse rendering},
location = {Sydney, NSW, Australia},
series = {SA '23}
}

@article{10.1145/3034827,
author = {Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz},
title = {CASE Tool Support for Variability Management in Software Product Lines},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3034827},
doi = {10.1145/3034827},
abstract = {Software product lines (SPL) aim at reducing time-to-market and increasing software quality through extensive, planned reuse of artifacts. An essential activity in SPL is variability management, i.e., defining and managing commonality and variability among member products. Due to the large scale and complexity of today's software-intensive systems, variability management has become increasingly complex to conduct. Accordingly, tool support for variability management has been gathering increasing momentum over the last few years and can be considered a key success factor for developing and maintaining SPLs. While several studies have already been conducted on variability management, none of these analyzed the available tool support in detail. In this work, we report on a survey in which we analyzed 37 existing variability management tools identified using a systematic literature review to understand the tools’ characteristics, maturity, and the challenges in the field. We conclude that while most studies on variability management tools provide a good motivation and description of the research context and challenges, they often lack empirical data to support their claims and findings. It was also found that quality attributes important for the practical use of tools such as usability, integration, scalability, and performance were out of scope for most studies.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {14},
numpages = {45},
keywords = {software variability, computer-aided software engineering, Software engineering}
}

@article{10.1145/3503461,
author = {Liu, Jie and Marriott, Kim and Dwyer, Tim and Tack, Guido},
title = {Increasing User Trust in Optimisation through Feedback and Interaction},
year = {2023},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {5},
issn = {1073-0516},
url = {https://doi.org/10.1145/3503461},
doi = {10.1145/3503461},
abstract = {User trust plays a key role in determining whether autonomous computer applications are relied upon. It will play a key role in the acceptance of emerging AI applications such as optimisation. Two important factors known to affect trust are system transparency, i.e., how well the user understands how the system works, and system performance. However, in the case of optimisation, it is difficult for the end-user to understand the underlying algorithms or to judge the quality of the solution. Through two controlled user studies, we explore whether the user is better able to calibrate their trust in the system when: (a) They are provided feedback on the system operation in the form of visualisation of intermediate solutions and their quality; (b) They can interactively explore the solution space by modifying the solution returned by the system. We found that showing intermediate solutions can lead to over-trust, while interactive exploration leads to more accurately calibrated trust.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = jan,
articleno = {42},
numpages = {34},
keywords = {vehicle routing, feedback, trust, human-in-the-loop optimisation, interactive optimisation, HCI}
}

@inproceedings{10.1145/2897845.2897856,
author = {Meng, Guozhu and Xue, Yinxing and Mahinthan, Chandramohan and Narayanan, Annamalai and Liu, Yang and Zhang, Jie and Chen, Tieming},
title = {Mystique: Evolving Android Malware for Auditing Anti-Malware Tools},
year = {2016},
isbn = {9781450342339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897845.2897856},
doi = {10.1145/2897845.2897856},
abstract = {In the arms race of attackers and defenders, the defense is usually more challenging than the attack due to the unpredicted vulnerabilities and newly emerging attacks every day. Currently, most of existing malware detection solutions are individually proposed to address certain types of attacks or certain evasion techniques. Thus, it is desired to conduct a systematic investigation and evaluation of anti-malware solutions and tools based on different attacks and evasion techniques. In this paper, we first propose a meta model for Android malware to capture the common attack features and evasion features in the malware. Based on this model, we develop a framework, MYSTIQUE, to automatically generate malware covering four attack features and two evasion features, by adopting the software product line engineering approach. With the help of MYSTIQUE, we conduct experiments to 1) understand Android malware and the associated attack features as well as evasion techniques; 2) evaluate and compare the 57 off-the-shelf anti-malware tools, 9 academic solutions and 4 App market vetting processes in terms of accuracy in detecting attack features and capability in addressing evasion. Last but not least, we provide a benchmark of Android malware with proper labeling of contained attack and evasion features.},
booktitle = {Proceedings of the 11th ACM on Asia Conference on Computer and Communications Security},
pages = {365–376},
numpages = {12},
keywords = {malware generation, evolutionary algorithm, defense capability, android feature model},
location = {Xi'an, China},
series = {ASIA CCS '16}
}

@inproceedings{10.1145/3442391.3442410,
author = {Pett, Tobias and Krieter, Sebastian and Runge, Tobias and Th\"{u}m, Thomas and Lochau, Malte and Schaefer, Ina},
title = {Stability of Product-Line Samplingin Continuous Integration},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442410},
doi = {10.1145/3442391.3442410},
abstract = {Companies strive to implement continuous integration into their development process to ensure the quality of their systems. Regression testing within the CI process considers the efficient re-test of systems after changes. However, even with regression testing, it is not feasible to test all configurations from a highly-configurable software system due to the combinatorial-explosion problem. Numerous sampling algorithms have been proposed that aim at computing a considerably smaller yet sufficiently representative set of configurations to be tested. Those algorithms are typically evaluated with regard to efficiency (i.e., number of configurations in a sample and computational effort for generating a sample) and effectiveness (i.e., feature-interaction coverage or number of faults detected). In this paper, we argue that a further crucial characteristic of sampling algorithms is their tendency to produce similar configurations when applied consecutively to an evolving configurable system. We propose sampling stability as a new evaluation criterion for sampling algorithms. We present a procedure to compute the sampling stability of sampling algorithms based on the similarity between consecutive samples. In our evaluation, we compare the sampling stability of multiple established t-wise sampling algorithms on large real-world systems.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {18},
numpages = {9},
keywords = {sampling, product-line evolution, product lines},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.1109/SEAMS.2017.6,
author = {Sousa, Gustavo and Rudametkin, Walter and Duchien, Laurence},
title = {Extending dynamic software product lines with temporal constraints},
year = {2017},
isbn = {9781538615508},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SEAMS.2017.6},
doi = {10.1109/SEAMS.2017.6},
abstract = {Due to the number of cloud providers, as well as the extensive collection of services, cloud computing provides very flexible environments, where resources and services can be provisioned and released on demand. However, reconfiguration and adaptation mechanisms in cloud environments are very heterogeneous and often exhibit complex constraints. For example, when reconfiguring a cloud system, a set of available services may be dependent on previous choices, or there may be alternative ways of adapting the system, with different impacts on performance, costs or reconfiguration time.Cloud computing systems exhibit high levels of variability, making dynamic software product lines (DSPLs) a promising approach for managing them. However, in DSPL approaches, verification is often limited to verifying conformance to a variability model, but this is insufficient to verify complex reconfiguration constraints that exist in cloud computing systems.In this paper, we propose the use of temporal constraints and reconfiguration operations to model a DSPL's reconfiguration lifecycle. We demonstrate how these concepts can be used to model the variability of cloud systems, and we use our approach to identify reconfigurations that meet given criteria.},
booktitle = {Proceedings of the 12th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {129–139},
numpages = {11},
keywords = {variability, feature models, dynamic software product lines, cloud computing},
location = {Buenos Aires, Argentina},
series = {SEAMS '17}
}

@inproceedings{10.1145/3616901.3616931,
author = {Xiong, Han Yang},
title = {Design of Virtual Reality Interactive Product Software Based on Artificial Intelligence},
year = {2024},
isbn = {9798400707544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616901.3616931},
doi = {10.1145/3616901.3616931},
abstract = {Abstract. Virtual reality (VR) technology is a new type of practical technology that was developed in the 20th century. Virtual reality involves the use and synthesis of computers, computer information and simulation technology. It is based on the latest high-tech developments in the use of computer technology, 3D graphics technology, multimedia technology, simulation technology, imaging technology, servo technology and other equipment to create a realistic 3D visual, tactile, olfactory and other sensory experience Virtual worlds create for people who are in them a sense of immersion. With the continuous development of social productivity and science and technology, the demand for virtual reality technology is growing in all walks of life, and virtual reality technology has made great strides and is gradually becoming a new field of science and technology. This paper first introduces artificial intelligence (AI), VR and interactive products, and then proves the progressiveness of AI based VR interactive product software through experiments (the interface interaction deviation is about 0.1 in the xyz axis, far lower than other VR interactive products).},
booktitle = {Proceedings of the 2023 International Conference on Frontiers of Artificial Intelligence and Machine Learning},
pages = {131–135},
numpages = {5},
keywords = {AI, Interactive Products, Software Design, VR},
location = {Beijing, China},
series = {FAIML '23}
}

@article{10.1145/3595297,
author = {Xu, Tiancheng and Rixner, Scott and Cox, Alan L.},
title = {An FPGA Accelerator for Genome Variant Calling},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1936-7406},
url = {https://doi.org/10.1145/3595297},
doi = {10.1145/3595297},
abstract = {In genome analysis, it is often important to identify variants from a reference genome. However, identifying variants that occur with low frequency can be challenging, as it is computationally intensive to do so accurately. LoFreq is a widely used program that is adept at identifying low-frequency variants. This article presents a design framework for an FPGA-based accelerator for LoFreq. In particular, this accelerator is targeted at virus analysis, which is particularly challenging, compared to human genome analysis, as the characteristics of the data to be analyzed are fundamentally different. Across the design space, this accelerator can achieve up to 120\texttimes{} speedups on the core computation of LoFreq and speedups of up to 51.7\texttimes{} across the entire program.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = sep,
articleno = {53},
numpages = {29},
keywords = {Variant Calling, FPGA, HLS}
}

@article{10.1145/3656643,
author = {Abouelhamayed, Ahmed and Cui, Angela and Fernandez-marques, Javier and Lane, Nicholas and Abdelfattah, Mohamed},
title = {PQA: Exploring the Potential of Product Quantization in DNN Hardware Acceleration},
year = {2024},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/3656643},
doi = {10.1145/3656643},
abstract = {Conventional multiply-accumulate (MAC) operations have long dominated computation time for deep neural networks (DNNs), especially convolutional neural networks (CNNs). Recently, product quantization (PQ) has been applied to these workloads, replacing MACs with memory lookups to pre-computed dot products. To better understand the efficiency tradeoffs of product-quantized DNNs (PQ-DNNs), we create a custom hardware accelerator to parallelize and accelerate nearest-neighbor search and dot-product lookups. Additionally, we perform an empirical study to investigate the efficiency–accuracy tradeoffs of different PQ parameterizations and training methods. We identify PQ configurations that improve performance-per-area for ResNet20 by up to 3.1\texttimes{}, even when compared to a highly optimized conventional DNN accelerator, with similar improvements on two additional compact DNNs. When comparing to recent PQ solutions, we outperform prior work by 4\texttimes{} in terms of performance-per-area with a 0.6% accuracy degradation. Finally, we reduce the bitwidth of PQ operations to investigate the impact on both hardware efficiency and accuracy. With only 2–6-bit precision on three compact DNNs, we were able to maintain DNN accuracy eliminating the need for DSPs.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = dec,
articleno = {6},
numpages = {29},
keywords = {Deep neural network (DNN), product quantization, FPGA acceleration, low arithmetic precision}
}

@inproceedings{10.1145/3023956.3023959,
author = {Ochoa, Lina and Pereira, Juliana Alves and Gonz\'{a}lez-Rojas, Oscar and Castro, Harold and Saake, Gunter},
title = {A survey on scalability and performance concerns in extended product lines configuration},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023959},
doi = {10.1145/3023956.3023959},
abstract = {Product lines have been employed as a mass customisation method that reduces production costs and time-to-market. Multiple product variants are represented in a product line, however the selection of a particular configuration depends on stakeholders' functional and non-functional requirements. Methods like constraint programming and evolutionary algorithms have been used to support the configuration process. They consider a set of product requirements like resource constraints, stakeholders' preferences, and optimization objectives. Nevertheless, scalability and performance concerns start to be an issue when facing large-scale product lines and runtime environments. Thus, this paper presents a survey that analyses strengths and drawbacks of 21 approaches that support product line configuration. This survey aims to: i) evidence which product requirements are currently supported by studied methods; ii) how scalability and performance is considered in existing approaches; and iii) point out some challenges to be addressed in future research.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {5–12},
numpages = {8},
keywords = {survey, scalability, product requirements, product line, performance, literature review, configuration},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/3676536.3676682,
author = {Yang, Chien-Yi and Zhou, Minxuan and Ponzina, Flavio and Sathya Prakash, Suraj and Ayoub, Raid and Mercati, Pietro and Subedar, Mahesh and Rosing, Tajana},
title = {Multi-Objective Software-Hardware Co-Optimization for HD-PIM via Noise-Aware Bayesian Optimization},
year = {2025},
isbn = {9798400710773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676536.3676682},
doi = {10.1145/3676536.3676682},
abstract = {In hardware accelerator design, software-hardware co-optimization requires intricate trade-offs and tight integration between software algorithms and hardware design to optimize performance, power efficiency, and area (PPA) while ensuring high accuracy. Furthermore, the inherent non-ideality in some emerging hardware technologies poses extra challenges to the co-optimization problem. This paper proposes a novel software-hardware co-optimization framework for hyperdimensional (HD) computing accelerators with emerging ReRAM-based processing in-memory (PIM) technologies, which have shown superior performance and energy efficiency over conventional machine learning accelerators. We first comprehensively characterize the non-trivial trade-offs between design parameters in HD-PIM and PPA and accuracy metrics in HD-PIM. Then, we develop a multi-objective noise-aware Bayesian optimization algorithm to find the Pareto set (optimal trade-offs between metrics) of the HD-PIM design. Our methodology uniquely addresses the stochastic nature of ReRAM by integrating error characteristics into the optimization process, thereby enhancing the quality of the generated designs. Experimental results show that our configurations achieve up to 4.28% accuracy improvement, 35.38% power reduction, 49x timing improvement, and 10% area reduction over a non-optimized design.},
booktitle = {Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design},
articleno = {158},
numpages = {9},
keywords = {multi-objective, bayesian optimization, hyperdimensional computing, processing-in-memory, resistive RAM},
location = {Newark Liberty International Airport Marriott, New York, NY, USA},
series = {ICCAD '24}
}

@inproceedings{10.1145/2188286.2188304,
author = {Tawhid, Rasha and Petriu, Dorina},
title = {User-friendly approach for handling performance parameters during predictive software performance engineering},
year = {2012},
isbn = {9781450312028},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2188286.2188304},
doi = {10.1145/2188286.2188304},
abstract = {A Software Product Line (SPL) is a set of similar software systems that share a common set of features. Instead of building each product from scratch, SPL development takes advantage of the reusability of the core assets shared among the SPL members. In this work, we integrate performance analysis in the early phases of SPL development process, applying the same reusability concept to the performance annotations. Instead of annotating from scratch the UML model of every derived product, we propose to annotate the SPL model once with generic performance annotations. After deriving the model of a product from the family model by an automatic transformation, the generic performance annotations need to be bound to concrete product-specific values provided by the developer. Dealing manually with a large number of performance annotations, by asking the developer to inspect every diagram in the generated model and to extract these annotations is an error-prone process. In this paper we propose to automate the collection of all generic parameters from the product model and to present them to the developer in a user-friendly format (e.g., a spreadsheet per diagram, indicating each generic parameter together with guiding information that helps the user in providing concrete binding values). There are two kinds of generic parametric annotations handled by our approach: product-specific (corresponding to the set of features selected for the product) and platform-specific (such as device choices, network connections, middleware, and runtime environment). The following model transformations for (a) generating a product model with generic annotations from the SPL model, (b) building the spreadsheet with generic parameters and guiding information, and (c) performing the actual binding are all realized in the Atlas Transformation Language (ATL).},
booktitle = {Proceedings of the 3rd ACM/SPEC International Conference on Performance Engineering},
pages = {109–120},
numpages = {12},
keywords = {uml, spl, performance model, performance completion, model-driven development, marte, atl},
location = {Boston, Massachusetts, USA},
series = {ICPE '12}
}

@inproceedings{10.1145/1062455.1062551,
author = {Verlage, Martin and Kiesgen, Thomas},
title = {Five years of product line engineering in a small company},
year = {2005},
isbn = {1581139632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062455.1062551},
doi = {10.1145/1062455.1062551},
abstract = {In 1999, a new team at MARKET MAKER Software AG began to develop a software product line for managing and displaying stock market data and financial market news. The basic idea was to use web technology in all applications for delivering services to customers. It soon turned out that the company had to change both the processes and the organization. This report summarizes the changes made and the lessons learned over the past five years, when the product line idea was introduced into a small company which faced the pressure to quickly market the first product line instances.},
booktitle = {Proceedings of the 27th International Conference on Software Engineering},
pages = {534–543},
numpages = {10},
keywords = {project management, product line engineering, experience report, SME},
location = {St. Louis, MO, USA},
series = {ICSE '05}
}

@inproceedings{10.1145/3571788.3571800,
author = {Sharma, Shubham and Fadhlillah, Hafiyyan Sayyid and Guti\'{e}rrez Fern\'{a}ndez, Antonio Manuel and Rabiser, Rick and Zoitl, Alois},
title = {Modularization Technique to Support Software Variability in Cyber-Physical Production Systems},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571788.3571800},
doi = {10.1145/3571788.3571800},
abstract = {Industries still have problems managing and organizing control software variants for different machine processes in Cyber-Physical Production Systems (CPPSs). They still rely mostly on clone-and-own approaches to create new control software variants when introducing new process variability in reaction to customers’ requirements. This approach not only results in code duplication but over time particularly makes it more difficult to maintain and evolve the software. Due to a lack of modularization, this approach also often results in unnecessary code in delivered software, locked by parameters, which can have a further negative effect on maintenance. In this paper, we discuss modularization approaches to organize control software in CPPSs. Specifically, for IEC 61499-based control software, we propose the combination of explicit variants described in 150% modules, standardized interfaces, and separation of concerns. We discuss how our approach could help industry to decrease the effort for new projects and at the same time get a better overview of the product and process variability of their CPPSs.},
booktitle = {Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {71–76},
numpages = {6},
keywords = {Software Product lines, Modularization, Model-Based System Engineering, Distributed Design, Cyber-Physical Production System, 150 Percent Model},
location = {Odense, Denmark},
series = {VaMoS '23}
}

@inproceedings{10.1145/3522664.3528602,
author = {Friesel, Birte and Spinczyk, Olaf},
title = {Black-box models for non-functional properties of AI software systems},
year = {2022},
isbn = {9781450392754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3522664.3528602},
doi = {10.1145/3522664.3528602},
abstract = {Non-functional properties (NFPs) such as latency, memory requirements, or hardware cost are an important characteristic of AI software systems, especially in the domain of resource-constrained embedded devices. Embedded AI products require sufficient resources for satisfactory latency and accuracy, but should also be cost-efficient and therefore not use more powerful hardware than strictly necessary. Traditionally, modeling and optimization efforts focus on the AI architecture, utilizing methods such as neural architecture search (NAS). However, before developers can start optimizing, they need to know which architectures are suitable candidates for their use case. To this end, architectures must be viewed in context: model post-processing (e.g. quantization), hardware platform, and run-time configuration such as batching all have significant effects on NFPs and therefore on AI architecture performance. Moreover, scalar parameters such as batch size cannot be benchmarked exhaustively. We argue that it is worthwhile to address this issue by means of black-box models before deciding on AI architectures for optimization and hardware/software platforms for inference. To support our claim, we present an AI product line with variable hardware and software components, perform benchmarks, and present notable results. Additionally, we evaluate both compactness and generalization capabilities of regression tree-based modeling approaches from the machine learning and product line engineering communities. We find that linear model trees perform best: they can capture NFPs of known AI configurations with a mean error of up to 13 %, and can predict unseen configurations with a mean error of 10 to 26 %. We find linear model trees to be more compact and interpretable than other tree-based approaches.},
booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
pages = {170–180},
numpages = {11},
keywords = {AI, performance prediction, product lines, regression trees},
location = {Pittsburgh, Pennsylvania},
series = {CAIN '22}
}

@inproceedings{10.1145/3700906.3700990,
author = {He, Chunhui and Liu, Haitao and Wang, Long and Chen, Xiangjia and Sun, Yongxin},
title = {Research on intelligent line selection and optimization scheme of transmission lines based on big data analysis and artificial intelligence},
year = {2024},
isbn = {9798400707032},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3700906.3700990},
doi = {10.1145/3700906.3700990},
abstract = {As the wave of data-driven informatization sweeps the world, society is moving from the information age (IT) to the data age (DT). GIS technology has become an important tool for complex regional development and prediction. However, the existing power grid design methods cannot meet the needs of improving production efficiency and controlling engineering costs. This paper proposes an intelligent line selection and optimization scheme for transmission lines based on big data analysis and artificial intelligence. The map model is designed using GIS multi-band raster maps, and data mining technology is used to maximize the application value of multi-factor geographic information data. The reverse dynamic programming algorithm is used for intelligent line selection, and the particle swarm optimization-convolutional neural network (PSO-CNN) model is used for engineering quantity prediction to improve the prediction accuracy and obtain more accurate optimization scheme ranking results. The scheme aims to reduce the design workload, reduce the industry design cost, improve the intelligence and standardization level of transmission and transformation engineering design, and provide strong support for the construction of smart grids.},
booktitle = {Proceedings of the International Conference on Image Processing, Machine Learning and Pattern Recognition},
pages = {525–531},
numpages = {7},
keywords = {GIS grid map, PSO-CNN, artificial intelligence, data mining, intelligent line selection, power transmission line, sequential dynamic programming},
location = {
},
series = {IPMLP '24}
}

@inproceedings{10.1145/3286978.3287014,
author = {Bokhari, Mahmoud A. and Alexander, Brad and Wagner, Markus},
title = {In-vivo and offline optimisation of energy use in the presence of small energy signals: A case study on a popular Android library},
year = {2018},
isbn = {9781450360937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286978.3287014},
doi = {10.1145/3286978.3287014},
abstract = {Energy demands of applications on mobile platforms are increasing. As a result, there has been a growing interest in optimising their energy efficiency. As mobile platforms are fast-changing, diverse and complex, the optimisation of energy use is a non-trivial task.To date, most energy optimisation methods either use models or external meters to estimate energy use. Unfortunately, it becomes hard to build widely applicable energy models, and external meters are neither cheap nor easy to set up. To address this issue, we run application variants in-vivo on the phone and use a precise internal battery monitor to measure energy use. We describe a methodology for optimising a target application in-vivo and with application-specific models derived from the device's own internal meter based on jiffies and lines of code. We demonstrate that this process produces a significant improvement in energy efficiency with limited loss of accuracy.},
booktitle = {Proceedings of the 15th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {207–215},
numpages = {9},
keywords = {multi-objective optimisation, mobile applications, energy consumption, Non-functional properties, Android},
location = {New York, NY, USA},
series = {MobiQuitous '18}
}

@inproceedings{10.1145/1987875.1987886,
author = {Belategi, Lorea and Sagardui, Goiuria and Etxeberria, Leire},
title = {Model based analysis process for embedded software product lines},
year = {2011},
isbn = {9781450307307},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1987875.1987886},
doi = {10.1145/1987875.1987886},
abstract = {Nowadays, embedded system development is increasing its complexity dealing with quality, cost and time-to-market among others. Quality attributes are an important issue to consider in embedded software development where time issues may be critical. Development paradigms such as Model Driven Development and Software Product Lines can be an adequate alternative to traditional software development and validation methods due to the characteristics of embedded systems. But for a proper validation and verification based on MARTE model analysis, all variability issues and critical quality attributes that take part in analysis must be properly modelled and managed. Therefore, a model analysis process for Model Driven Embedded Software Product Lines has been defined as some process lacks have been found.},
booktitle = {Proceedings of the 2011 International Conference on Software and Systems Process},
pages = {53–62},
numpages = {10},
keywords = {software product line, schedulability, quality attributes, performance, model driven development, model based analysis process},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSSP '11}
}

@inproceedings{10.1145/3624062.3624543,
author = {Rodriguez-Canal, Gabriel and Brown, Nick and Jamieson, Maurice and Bauer, Emilien and Lydike, Anton and Grosser, Tobias},
title = {Stencil-HMLS: A multi-layered approach to the automatic optimisation of stencil codes on FPGA},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624543},
doi = {10.1145/3624062.3624543},
abstract = {The challenges associated with effectively programming FPGAs have been a major blocker in popularising reconfigurable architectures for HPC workloads. However new compiler technologies, such as MLIR, are providing new capabilities which potentially deliver the ability to extract domain specific information and drive automatic structuring of codes for FPGAs. In this paper we explore domain specific optimisations for stencils, a fundamental access pattern in scientific computing, to obtain high performance on FPGAs via automated code structuring. We propose Stencil-HMLS, a multi-layered approach to automatic optimisation of stencil codes and introduce the HLS dialect, which brings FPGA programming into the MLIR ecosystem. Using the PSyclone Fortran DSL, we demonstrate an improvement of 14-100 \texttimes{} with respect to the next best performant state-of-the-art tool. Furthermore, our approach is 14-92 \texttimes{} more energy efficient than the next most energy efficient approach.},
booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {556–565},
numpages = {10},
keywords = {FPGAs, HPC, MLIR, U280, stencil based codes, xDSL},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@inproceedings{10.1145/2465478.2465495,
author = {Klatt, Benjamin and K\"{u}ster, Martin},
title = {Improving product copy consolidation by architecture-aware difference analysis},
year = {2013},
isbn = {9781450321266},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2465478.2465495},
doi = {10.1145/2465478.2465495},
abstract = {Software product lines (SPL) are a well-known concept to efficiently develop product variants. However, migrating customised product copies to a product line is still a labour-intensive challenge due to the required comprehension of differences among the implementations and SPL design decisions. Most existing SPL approaches are focused on forward engineering. Only few aim to handle SPL evolution, but even those lack support of variability reverse engineering, which is necessary for migrating product copies to a product line. In this paper, we present our continued concept on using component architecture information to enhance a variability reverse engineering process. Including this information particularly improves the difference identification as well as the variation point analysis and -aggregation steps. We show how the concept can be applied by providing an illustrating example.},
booktitle = {Proceedings of the 9th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {117–122},
numpages = {6},
keywords = {software product line, reverse engineering, component architecture},
location = {Vancouver, British Columbia, Canada},
series = {QoSA '13}
}

@inproceedings{10.1145/2866614.2866620,
author = {Mauro, Jacopo and Nieke, Michael and Seidl, Christoph and Yu, Ingrid Chieh},
title = {Context Aware Reconfiguration in Software Product Lines},
year = {2016},
isbn = {9781450340199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2866614.2866620},
doi = {10.1145/2866614.2866620},
abstract = {Software Product Lines (SPLs) are a mechanism for large-scale reuse where families of related software systems are represented in terms of commonalities and variabilities, e.g., using Feature Models (FMs). While FMs define all possible configurations of the SPL, when considering dynamic SPLs not every possible configuration may be valid in all possible contexts. Unfortunately, common FMs can not capture this context dependence. In this paper, we remedy this problem by extending attributed FMs with Validity Formulas (VFs) that constrain the selection of a particular feature to a specific context and that are located directly within the FM. We provide a reconfiguration engine that checks if the active configuration is valid in the current context and, if not, computes how to reconfigure it. Furthermore, we present our implementation and demonstrate its feasibility within a case study derived from scenarios of our industry partner in the automotive domain.},
booktitle = {Proceedings of the 10th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {41–48},
numpages = {8},
keywords = {Variability, Software Product Lines, Feature Models, Context},
location = {Salvador, Brazil},
series = {VaMoS '16}
}

@inproceedings{10.1145/3583780.3615268,
author = {Das, Manas Jyoti and Shehzad, Khawar and Rao, Praveen},
title = {Efficient Variant Calling on Human Genome Sequences Using a GPU-Enabled Commodity Cluster},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615268},
doi = {10.1145/3583780.3615268},
abstract = {Human genome sequences are very large in size and require significant compute and storage resources for processing and analysis. Variant calling is a key task performed on an individual's genome to identify different types of variants. Knowing these variants can lead to new advances in disease diagnosis and treatment. In this work, we propose a new approach for accelerating variant calling pipelines on a large workload of human genomes using a commodity cluster with graphics processing units (GPUs). Our approach has two salient features: First, it enables a pipeline stage to use GPUs and/or CPUs based on the availability of resources in the cluster. Second, it employs a mutual exclusion strategy for executing a pipeline stage on the GPUs of a cluster node so that the stages (for other sequences) can be executed using CPUs if needed. We evaluated our approach on a 8-node cluster with bare metal servers and virtual machines (VMs) containing different types of GPUs. On publicly available genome sequences, our approach was 3.6X-5X faster compared to an approach that used only the cluster CPUs.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {3843–3848},
numpages = {6},
keywords = {variant calling, human genomes, cluster computing, GPUs},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@article{10.1145/3720501,
author = {Watanabe, Kazuki and Junges, Sebastian and Rot, Jurriaan and Hasuo, Ichiro},
title = {A Unifying Approach to Product Constructions for Quantitative Temporal Inference},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3720501},
doi = {10.1145/3720501},
abstract = {Probabilistic programs are a powerful and convenient approach to formalising distributions over system executions. A classical verification problem for probabilistic programs is temporal inference: to compute the likelihood that the execution traces satisfy a given temporal property. This paper presents a general framework for temporal inference, which applies to a rich variety of quantitative models including those that arise in the operational semantics of probabilistic and weighted programs.   The key idea underlying our framework is that in a variety of existing approaches, the main construction that enables temporal inference is that of a product between the system of interest and the temporal property. We provide a unifying mathematical definition of product constructions, enabled by the realisation that 1) both systems and temporal properties can be modelled as coalgebras and 2) product constructions are distributive laws in this context. Our categorical framework leads us to our main contribution: a sufficient condition for correctness, which is precisely what enables to use the product construction for temporal inference.   We show that our framework can be instantiated to naturally recover a number of disparate approaches from the literature including, e.g., partial expected rewards in Markov reward models, resource-sensitive reachability analysis, and weighted optimization problems.  Furthermore, we demonstrate a product of weighted programs and weighted temporal properties as a new instance to show the scalability of our approach.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {136},
numpages = {29},
keywords = {coalgebra, probabilistic programming, temporal inference}
}

@inproceedings{10.1145/3555776.3577847,
author = {Parra-Ullauri, Juan and Zhang, Xunzheng and Bravalheri, Anderson and Nejabati, Reza and Simeonidou, Dimitra},
title = {Federated Hyperparameter Optimisation with Flower and Optuna},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577847},
doi = {10.1145/3555776.3577847},
abstract = {Federated learning (FL) is an emerging distributed machine learning technique in which multiple clients collaborate to learn a model under the management of a central server. An FL system depends on a set of initial conditions (i.e., hyperparameters) that affect the system's performance. However, defining a good choice of hyperparameters for the central server and clients is a challenging problem. Hyperparameter tuning in FL often requires manual or automated searches to find optimal values. Nonetheless, a noticeable limitation is the high cost of algorithm evaluation for server and client models, making the tuning process computationally expensive and time-consuming. We propose an implementation based on integrating the FL framework Flower, and the prime optimisation software Optuna for automated and efficient hyperparameter optimisation (HPO) in FL. Through this combination, it is possible to tune hyperparameters in both clients and server online, aiming to find the optimal values at runtime. We introduce the HPO factor to describe the number of rounds that the HPO will take place, and the HPO rate that defines the frequency for updating the hyperparameters and can be used for pruning. The HPO is managed by the FL server which updates clients' hyperparameters, with an HPO rate, using state-of-the-art optimisation algorithms enabled by Optuna. We tested our approach by updating multiple client models simultaneously in popular image recognition datasets which produced promising results compared to baselines.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1209–1216},
numpages = {8},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/3377811.3380372,
author = {Lienhardt, Michael and Damiani, Ferruccio and Johnsen, Einar Broch and Mauro, Jacopo},
title = {Lazy product discovery in huge configuration spaces},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380372},
doi = {10.1145/3377811.3380372},
abstract = {Highly-configurable software systems can have thousands of interdependent configuration options across different subsystems. In the resulting configuration space, discovering a valid product configuration for some selected options can be complex and error prone. The configuration space can be organized using a feature model, fragmented into smaller interdependent feature models reflecting the configuration options of each subsystem.We propose a method for lazy product discovery in large fragmented feature models with interdependent features. We formalize the method and prove its soundness and completeness. The evaluation explores an industrial-size configuration space. The results show that lazy product discovery has significant performance benefits compared to standard product discovery, which in contrast to our method requires all fragments to be composed to analyze the feature model. Furthermore, the method succeeds when more efficient, heuristics-based engines fail to find a valid configuration.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1509–1521},
numpages = {13},
keywords = {Linux distribution, composition, configurable software, feature models, software product lines, variability modeling},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1145/3665324,
author = {Bruch, Sebastian and Nardini, Franco Maria and Ingber, Amir and Liberty, Edo},
title = {Bridging Dense and Sparse Maximum Inner Product Search},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {6},
issn = {1046-8188},
url = {https://doi.org/10.1145/3665324},
doi = {10.1145/3665324},
abstract = {Maximum inner product search (MIPS) over dense and sparse vectors have progressed independently in a bifurcated literature for decades; the latter is better known as top- (k)  retrieval in Information Retrieval. This duality exists because sparse and dense vectors serve different end goals. That is despite the fact that they are manifestations of the same mathematical problem. In this work, we ask if algorithms for dense vectors could be applied effectively to sparse vectors, particularly those that violate the assumptions underlying top- (k)  retrieval methods. We study clustering-based approximate MIPS where vectors are partitioned into clusters and only a fraction of clusters are searched during retrieval. We conduct a comprehensive analysis of dimensionality reduction for sparse vectors, and examine standard and spherical k-means for partitioning. Our experiments demonstrate that clustering-based retrieval serves as an efficient solution for sparse MIPS. As byproducts, we identify two research opportunities and explore their potential. First, we cast the clustering-based paradigm as dynamic pruning and turn that insight into a novel organization of the inverted index for approximate MIPS over general sparse vectors. Second, we offer a unified regime for MIPS over vectors that have dense and sparse subspaces, that is robust to query distributions.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {151},
numpages = {38},
keywords = {Maximum inner product search, top-k retrieval, sparse vectors, dense vectors, hybrid vectors, sketching, clustering-based approximate nearest neighbor search}
}

@article{10.1145/3389397,
author = {Lu, Hong and Yue, Tao and Ali, Shaukat},
title = {Pattern-based Interactive Configuration Derivation for Cyber-physical System Product Lines},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2378-962X},
url = {https://doi.org/10.1145/3389397},
doi = {10.1145/3389397},
abstract = {Deriving a Cyber-Physical System (CPS) product from a product line requires configuring hundreds to thousands of configurable parameters of components and devices from multiple domains, e.g., computing, control, and communication. A fully automated configuration process for a CPS product line is seldom possible in practice, and a dynamic and interactive process is expected. Therefore, some configurable parameters are to be configured manually, and the rest can be configured either automatically or manually, depending on pre-defined constraints, the order of configuration steps, and previous configuration data in such a dynamic and interactive configuration process. In this article, we propose a pattern-based, interactive configuration derivation methodology (named as Pi-CD) to maximize opportunities of automatically deriving correct configurations of CPSs by benefiting from pre-defined constraints and configuration data of previous configuration steps. Pi-CD requires architectures of CPS product lines modeled with Unified Modeling Language extended with four types of variabilities, along with constraints specified in Object Constraint Language (OCL). Pi-CD is equipped with 324 configuration derivation patterns that we defined by systematically analyzing the OCL constructs and semantics. We evaluated Pi-CD by configuring 20 CPS products of varying complexity from two real-world CPS product lines. Results show that Pi-CD can achieve up to 72% automation degree with a negligible time cost. Moreover, its time performance remains stable with the increase in the number of configuration parameters as well as constraints.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = jun,
articleno = {44},
numpages = {24},
keywords = {product configuration, object constraint language, configuration derivation, Product line engineering}
}

@inproceedings{10.1145/3377930.3390245,
author = {Bokhari, Mahmoud A. and Alexander, Brad and Wagner, Markus},
title = {Towards rigorous validation of energy optimisation experiments},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3390245},
doi = {10.1145/3377930.3390245},
abstract = {The optimisation of software energy consumption is of growing importance across all scales of modern computing, i.e., from embedded systems to data-centres. Practitioners in the field of Search-Based Software Engineering and Genetic Improvement of Software acknowledge that optimising software energy consumption is difficult due to noisy and expensive fitness evaluations. However, it is apparent from results to date that more progress needs to be made in rigorously validating optimisation results. This problem is pressing because modern computing platforms have highly complex and variable behaviour with respect to energy consumption. To compare solutions fairly we propose in this paper a new validation approach called R3-validation which exercises software variants in a rotated-round-robin order. Using a case study, we present an in-depth analysis of the impacts of changing system states on software energy usage, and we show how R3-validation mitigates these. We compare it with current validation approaches across multiple devices and operating systems, and we show that it aligns best with actual platform behaviour.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {1232–1240},
numpages = {9},
keywords = {non-functional properties, mobile applications, energy consumption, Android},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1145/2110147.2110166,
author = {M\ae{}rsk-M\o{}ller, Hans Martin and J\o{}rgensen, Bo N\o{}rregaard},
title = {Cardinality-dependent variability in orthogonal variability models},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110166},
doi = {10.1145/2110147.2110166},
abstract = {During our work on developing and running a software product line for eco-sustainable greenhouse-production software tools, which currently have three products members we have identified a need for extending the notation of the Orthogonal Variability Model (OVM) to support what we refer to as cardinality range dependencies. The cardinality-range-dependency type enables expressing that the binding of a certain number of variants to a variation point can influence variability in other places in the model. In other words, we acknowledge that variability can be influenced, not necessarily by the specific variants being bound, but by their sheer numbers.This paper contributes with an extension to the meta-model underlying the OVM notation, suggesting a notation for the new type of dependency and shows its applicability. The specific case, which initially required this extension, will work as running example throughout the paper and underline the need for the extension. Finally, the paper evaluates and discusses the general applicability of the proposed notation extension and future perspectives.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {165–172},
numpages = {8},
keywords = {variability modeling language, software product line engineering, orthogonal variability model (OVM), documentation},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@inproceedings{10.1145/3477314.3507130,
author = {Assimi, Hirad and Koch, Ben and Garcia, Chris and Wagner, Markus and Neumann, Frank},
title = {Run-of-mine stockyard recovery scheduling and optimisation for multiple reclaimers},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3507130},
doi = {10.1145/3477314.3507130},
abstract = {Stockpiles are essential in the mining value chain, assisting in maximising value and production. Quality control of taken minerals from the stockpiles is a major concern for stockpile managers where failure to meet some requirements can lead to losing money. This problem was recently investigated using a single reclaimer, and basic assumptions. This study extends the approach to consider multiple reclaimers in preparing for short and long-term deliveries. The engagement of multiple reclaimers complicates the problem in terms of their interaction in preparing a delivery simultaneously and safety distancing of reclaimers. We also consider more realistic settings, such as handling different minerals with different types of reclaimers. We propose methods that construct a solution step by step to meet precedence constraints for all reclaimers in the stockyard. We study various instances of the problem using greedy algorithms, Ant Colony Optimisation (ACO), and propose an integrated local search method determining an efficient schedule. We fine-tune and compare the algorithms and show that the ACO combined with local search can yield efficient solutions.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {1074–1083},
numpages = {10},
keywords = {ant colony optimisation, greedy algorithm, iterative local search, parallel processing, stockpile},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.1145/3583133.3596311,
author = {Mitchell, Paul and Ochoa, Gabriela and Chassagne, Romain},
title = {Local Optima Networks of the Black Box Optimisation Benchmark Functions},
year = {2023},
isbn = {9798400701207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583133.3596311},
doi = {10.1145/3583133.3596311},
abstract = {Compressed monotonic local optima networks (CMLONs) provide a powerful means to visualise the global structure of the fitness landscapes of optimisation problems as network graphs. Historically, they have been developed for discrete optimisation problems, but they have more recently been extended to continuous problems. However, experience is limited because they have only been applied to a few analytical problems and even fewer real-world problems. This work aims to address that gap by providing a systematic and comprehensive catalogue of CMLONs for the well-known black box optimisation benchmark functions. These are a set of continuous analytical functions with diverse properties. CMLONs are calculated for each of the twenty-four benchmark functions in three, five, eight, twelve, and twenty dimensions. Network metrics are also calculated for each of the CMLONs and dimensionality reduction used to classify and compare the functions. It was found that the CMLONs have diverse representations that are related to both the functions' properties and their dimensionality. Network metrics were important for multimodal functions in higher dimensional problems, where the CMLONs were too dense to visualise meaningfully. These results provide an extensive catalogue of CMLONs for a variety of continuous functions and provide a useful reference for real-world optimisation problems.},
booktitle = {Proceedings of the Companion Conference on Genetic and Evolutionary Computation},
pages = {2072–2080},
numpages = {9},
keywords = {benchmark functions, fitness landscape, local optima networks},
location = {Lisbon, Portugal},
series = {GECCO '23 Companion}
}

@inproceedings{10.1145/3131151.3131152,
author = {Filho, Helson L. Jakubovski and Lima, Jackson A. Prado and Vergilio, Silvia R.},
title = {Automatic Generation of Search-Based Algorithms Applied to the Feature Testing of Software Product Lines},
year = {2017},
isbn = {9781450353267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131151.3131152},
doi = {10.1145/3131151.3131152},
abstract = {The selection of products for the variability testing of Feature Models (FMs) is a complex task impacted by many factors. To solve this problem, Multi-Objective Evolutionary Algorithms (MOEAs) have been successfully used in the field known as Search-Based Software Engineering (SBSE). However, the design of a search-based approach is not an easy task for the software engineer, who can find some difficulties such as: the choice and configuration of the best MOEAs, the choice of the best search operators to be implemented, and so on. In addition to this, existing approaches are dependent on the problem domain and do not allow reuse. In this way the use of Hyper-Heuristic (HH) can help to obtain more generic and reusable search-based approaches, and because of this is considered a trend in the SBSE field. Following this trend and to contribute to reduce the software engineer's efforts, this work explores the use of a hyper-heuristic for automatic generation of MOEAs to select test products from the FM, considering three factors: pairwise coverage, mutation score and cost, given by the number of products. The HH is based on a grammar that represents the elements, parameters and components of existing MOEAs and implements evolutionary operators, such as crossover and mutation, suitable for selection problems. In this way, it can be reused for other similar software engineering problems. Evaluation results show that the proposed approach obtains results that are better or statistically equivalent than similar approaches found in the literature.},
booktitle = {Proceedings of the XXXI Brazilian Symposium on Software Engineering},
pages = {114–123},
numpages = {10},
keywords = {Software Product Line Testing, Search-Based Software Engineering, Hyper-Heuristics},
location = {Fortaleza, CE, Brazil},
series = {SBES '17}
}

@inproceedings{10.1145/3278122.3278123,
author = {Nieke, Michael and Mauro, Jacopo and Seidl, Christoph and Th\"{u}m, Thomas and Yu, Ingrid Chieh and Franzke, Felix},
title = {Anomaly analyses for feature-model evolution},
year = {2018},
isbn = {9781450360456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278122.3278123},
doi = {10.1145/3278122.3278123},
abstract = {Software Product Lines (SPLs) are a common technique to capture families of software products in terms of commonalities and variabilities. On a conceptual level, functionality of an SPL is modeled in terms of features in Feature Models (FMs). As other software systems, SPLs and their FMs are subject to evolution that may lead to the introduction of anomalies (e.g., non-selectable features). To fix such anomalies, developers need to understand the cause for them. However, for large evolution histories and large SPLs, explanations may become very long and, as a consequence, hard to understand. In this paper, we present a method for anomaly detection and explanation that, by encoding the entire evolution history, identifies the evolution step of anomaly introduction and explains which of the performed evolution operations lead to it. In our evaluation, we show that our method significantly reduces the complexity of generated explanations.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {188–201},
numpages = {14},
keywords = {Software Product Line, Feature Model, Explanation, Evolution Operation, Evolution, Anomalies},
location = {Boston, MA, USA},
series = {GPCE 2018}
}

@article{10.1145/3695466,
author = {Cui, Cu},
title = {Acceleration of Tensor-Product Operations with Tensor Cores},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2329-4949},
url = {https://doi.org/10.1145/3695466},
doi = {10.1145/3695466},
abstract = {In this article, we explore the acceleration of tensor product operations in finite element methods, leveraging the computational power of the NVIDIA A100 GPU Tensor Cores. We provide an accessible overview of the necessary mathematical background and discuss our implementation strategies. Our study focuses on two common programming approaches for NVIDIA Tensor Cores: the C++ Warp Matrix Functions in nvcuda::wmma and the inline Parallel Thread Execution (PTX) instructions mma.sync.aligned. A significant focus is placed on the adoption of the versatile inline PTX instructions combined with a conflict-free shared memory access pattern, a key to unlocking superior performance. When benchmarked against traditional CUDA Cores, our approach yields a remarkable 2.3-fold increase in double-precision performance, achieving 8 TFLOPS/s—45% of the theoretical maximum. Furthermore, in half-precision computations, numerical experiments demonstrate a fourfold enhancement in solving the Poisson equation using the flexible GMRES (FGMRES) method, preconditioned by a multigrid method in 3D. This is achieved while maintaining the same discretization error as observed in double-precision computations. These results highlight the considerable benefits of using Tensor Cores for finite element operators with tensor products, achieving an optimal balance between computational speed and precision.},
journal = {ACM Trans. Parallel Comput.},
month = nov,
articleno = {15},
numpages = {24},
keywords = {Multigrid method, discontinuous galerkin method, matrix free method, GPU, tensor core, mixed precision}
}

@inproceedings{10.1145/3533767.3534413,
author = {Lu, Yifei and Pan, Minxue and Pei, Yu and Li, Xuandong},
title = {Detecting resource utilization bugs induced by variant lifecycles in Android},
year = {2022},
isbn = {9781450393799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533767.3534413},
doi = {10.1145/3533767.3534413},
abstract = {The lifecycle models of Android components such as Activities and Fragments predefine the possible orders in which the components' callback methods will be invoked during app executions. Correspondingly, resource utilization operations performed by Android components must comply with all possible lifecycles to ensure safe utilization of the resources in all circumstances, which, however, can be challenging to achieve. In response to the challenge, various techniques have been developed to detect resource utilization bugs that manifest themselves when components go through common lifecycles, but the fact that Android components may execute their callback methods in uncommon orders, leading to variant component lifecycles, has largely been overlooked by the existing techniques.  
	  
In this paper, we first identify three variant lifecycles for Android Activities and Fragments and then develop a technique called VALA to automatically detect bugs in Android apps that are induced by the variant lifecycles and may cause resource utilization errors like resource leaks and data losses. In an experimental evaluation conducted on 35 Android apps, a supporting tool for the VALA technique automatically detected 8 resource utilization bugs. All the 8 bugs were manually confirmed to be real defects and 7 of them were reported for the first time.},
booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {642–653},
numpages = {12},
keywords = {variant lifecycles, static analysis, resource utilization bugs, Android applications},
location = {Virtual, South Korea},
series = {ISSTA 2022}
}

@inproceedings{10.1145/3650200.3656611,
author = {Zhao, Wenxuan and Yuan, Liang and Yan, Baicheng and Ma, Penghao and Zhang, Yunquan and Wang, Long and Wang, Zhe},
title = {Stencil Computation with Vector Outer Product},
year = {2024},
isbn = {9798400706103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650200.3656611},
doi = {10.1145/3650200.3656611},
abstract = {Matrix computation units have been equipped in current architectures to accelerate AI and high performance computing applications. The matrix multiplication and vector outer product are two basic instruction types. The latter one is lighter since the inputs are vectors. Thus it provides more opportunities to develop flexible algorithms for problems other than dense linear algebra computing and more possibilities to optimize the implementation. Stencil computations represent a common class of nested loops in scientific and engineering applications. This paper proposes a novel stencil algorithm using vector outer products. Unlike previous work, the new algorithm arises from the stencil definition in the scatter mode and is initially expressed with formulas of vector outer products. The implementation incorporates a set of optimizations to improve the memory reference pattern, execution pipeline and data reuse by considering various algorithmic options and the data sharing between input vectors. Evaluation on a simulator shows that our design achieves a substantial speedup compared with vectorized stencil algorithms.},
booktitle = {Proceedings of the 38th ACM International Conference on Supercomputing},
pages = {247–258},
numpages = {12},
keywords = {Matrixization, Outer product, Stencil},
location = {Kyoto, Japan},
series = {ICS '24}
}

@inproceedings{10.1145/375212.375269,
author = {Gacek, Critina and Anastasopoules, Michalis},
title = {Implementing product line variabilities},
year = {2001},
isbn = {1581133588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375212.375269},
doi = {10.1145/375212.375269},
abstract = {Software product lines have numerous members. Thus, a product line infrastructure must cover various systems. This is the significant difference to usual software systems and the reason for additional requirements on the various assets present during software product line engineering. It is imperative that they support the description of the product line as a whole, as well as its instantiation for the derivation of individual products.Literature has already addressed how to create and instantiate generic product line assets, such as domain models and architectures to generate instance specific ones [1, 2, 3], yet little attention has been given on how to actually deal with this genericity at the code level.This paper addresses the issue of handling product line variability at the code level. To this end various implementation approaches are examined with respect to their use in a product line context.},
booktitle = {Proceedings of the 2001 Symposium on Software Reusability: Putting Software Reuse in Context},
pages = {109–117},
numpages = {9},
keywords = {traceability, software product lines, product line variability, implementing variabilities, implementation approaches},
location = {Toronto, Ontario, Canada},
series = {SSR '01}
}

@inproceedings{10.1145/2866614.2866628,
author = {Th\"{u}m, Thomas and Winkelmann, Tim and Schr\"{o}ter, Reimar and Hentschel, Martin and Kr\"{u}ger, Stefan},
title = {Variability Hiding in Contracts for Dependent Software Product Lines},
year = {2016},
isbn = {9781450340199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2866614.2866628},
doi = {10.1145/2866614.2866628},
abstract = {Software product lines are used to efficiently develop and verify similar software products. While they focus on reuse of artifacts between products, a product line may also be reused itself in other product lines. A challenge with such dependent product lines is evolution; every change in a product line may influence all dependent product lines. With variability hiding, we aim to hide certain features and their artifacts in dependent product lines. In prior work, we focused on feature models and implementation artifacts. We build on this by discussing how variability hiding can be extended to specifications in terms of method contracts. We illustrate variability hiding in contracts by means of a running example and share our insights with preliminary experiments on the benefits for formal verification. In particular, we find that not every change in a certain product line requires a re-verification of other dependent product lines.},
booktitle = {Proceedings of the 10th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {method contracts, deductive verification, Multi product line},
location = {Salvador, Brazil},
series = {VaMoS '16}
}

@inproceedings{10.1145/3674558.3674559,
author = {Falah, Bouchaib and Abufardeh, Sameer},
title = {Novel Approach: Prioritizing Test Cases Based on a Comprehensive Taxonomy of Product Metrics},
year = {2024},
isbn = {9798400716386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674558.3674559},
doi = {10.1145/3674558.3674559},
abstract = {Software product quality is still a significant problem, and testing is essential in ensuring software quality. Unfortunately, testing is hurried and frequently poorly executed because it is costly, time-consuming, and occurs near the end of the software engineering process. To address some of these issues and look at ways to lower software testing costs, this paper proposes prioritizing test cases based on a Comprehensive Taxonomy of Metrics (CTM). We have developed a taxonomy of product complexity metrics based on the product level, including the class, methods, and statements. To assess these metrics, we have developed a tool that utilizes them to target test cases. This tool allows us to identify three sample metric combinations. We conducted a series of experiments across three applications, applying these metric combinations to test case prioritization for each application. To investigate the efficiency of our test case prioritization, we created a large number of mutants using MuJava. Our experiments show that the mutant scores of all prioritized classes are remarkably high, and most of our test suites were able to discover 100 % of mutants.},
booktitle = {Proceedings of the 2024 10th International Conference on Computer Technology Applications},
pages = {1–7},
numpages = {7},
keywords = {Comprehensive Taxonomy Metrics, Mutant, Mutation, Prioritization, Software Testing, Test Cases},
location = {Vienna, Austria},
series = {ICCTA '24}
}

@article{10.1145/3635712,
author = {Ferrari, Alessio and Huichapa, Thaide and Spoletini, Paola and Novielli, Nicole and Fucci, Davide and Girardi, Daniela},
title = {Using Voice and Biofeedback to Predict User Engagement during Product Feedback Interviews},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3635712},
doi = {10.1145/3635712},
abstract = {Capturing users’ engagement is crucial for gathering feedback about the features of a software product. In a market-driven context, current approaches to collecting and analyzing users’ feedback are based on techniques leveraging information extracted from product reviews and social media. These approaches are hardly applicable in contexts where online feedback is limited, as for the majority of apps, and software in general. In such cases, companies need to resort to face-to-face interviews to get feedback on their products. In this article, we propose to utilize biometric data, in terms of physiological and voice features, to complement product feedback interviews with information about the engagement of the user on product-relevant topics. We evaluate our approach by interviewing users while gathering their physiological data (i.e., biofeedback) using an Empatica E4 wristband, and capturing their voice through the default audio-recorder of a common laptop. Our results show that we can predict users’ engagement by training supervised machine learning algorithms on biofeedback and voice data, and that voice features alone can be sufficiently effective. The best configurations evaluated achieve an average F1 ∼ 70% in terms of classification performance, and use voice features only. This work is one of the first studies in requirements engineering in which biometrics are used to identify emotions. Furthermore, this is one of the first studies in software engineering that considers voice analysis. The usage of voice features can be particularly helpful for emotion-aware feedback collection in remote communication, either performed by human analysts or voice-based chatbots, and can also be exploited to support the analysis of meetings in software engineering research.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {87},
numpages = {36},
keywords = {Software engineering, requirements engineering, emotion detection, voice analysis, speech analysis, biofeedback analysis, affective requirements engineering}
}

@article{10.1145/3688612,
author = {Lu, Xiaobo and Fang, Jianbin and Peng, Lin and Huang, Chun and Du, Zidong and Zhao, Yongwei and Wang, Zheng},
title = {Mentor: A Memory-Efficient Sparse-dense Matrix Multiplication Accelerator Based on Column-Wise Product},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3688612},
doi = {10.1145/3688612},
abstract = {Sparse-dense matrix multiplication (SpMM) is the performance bottleneck of many high-performance and deep-learning applications, making it attractive to design specialized SpMM hardware accelerators. Unfortunately, existing hardware solutions do not take full advantage of data reuse opportunities of the input and output matrices or suffer from irregular memory access patterns. Their strategies increase the off-chip memory traffic and bandwidth pressure, leaving much room for improvement. We present Mentor, a new approach to designing SpMM accelerators. Our key insight is that column-wise dataflow, while rarely exploited in prior works, can address these issues in SpMM computations. Mentor is a software-hardware co-design approach for leveraging column-wise dataflow to improve data reuse and regular memory accesses of SpMM. On the software level, Mentor incorporates a novel streaming construction scheme to preprocess the input matrix for enabling a streaming access pattern. On the hardware level, it employs a fully pipelined design to unlock the potential of column-wise dataflow further. The design of Mentor is underpinned by a carefully designed analytical model to find the tradeoff between performance and hardware resources. We have implemented an FPGA prototype of Mentor. Experimental results show that Mentor achieves speedup by geomean 2.05\texttimes{} (up to 3.98\texttimes{}), reduces the memory traffic by geomean 2.92\texttimes{} (up to 4.93\texttimes{}), and improves bandwidth utilization by geomean 1.38\texttimes{} (up to 2.89\texttimes{}), compared with the state-of-the-art hardware solutions.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {79},
numpages = {25},
keywords = {SpMM, sparse linear algebra}
}

@article{10.1145/3338851,
author = {Sun, Daniel and Chen, Shiping and Li, Guoqiang and Zhang, Yuanyuan and Atif, Muhammad},
title = {Multi-objective Optimisation of Online Distributed Software Update for DevOps in Clouds},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3338851},
doi = {10.1145/3338851},
abstract = {This article studies synchronous online distributed software update, also known as rolling upgrade in DevOps, which in clouds upgrades software versions in virtual machine instances even when various failures may occur. The goal is to minimise completion time, availability degradation, and monetary cost for entire rolling upgrade by selecting proper parameters. For this goal, we propose a stochastic model and a novel optimisation method. We validate our approach to minimise the objectives through both experiments in Amazon Web Service (AWS) and simulations.},
journal = {ACM Trans. Internet Technol.},
month = aug,
articleno = {43},
numpages = {20},
keywords = {stochastic modelling, software system reliability, rolling upgrade, multi-objective optimisation, Software operation}
}

@article{10.1145/3546182,
author = {V\'{e}stias, M\'{a}rio and Duarte, Rui P. and de Sousa, Jos\'{e} T. and Neto, Hor\'{a}cio},
title = {Efficient Design of Low Bitwidth Convolutional Neural Networks on FPGA with Optimized Dot Product Units},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/3546182},
doi = {10.1145/3546182},
abstract = {Designing hardware accelerators to run the inference of convolutional neural networks (CNN) is under intensive research. Several different architectures have been proposed along with hardware-oriented optimizations of the neural network models. One of the most used optimizations is quantization since it reduces the memory requirements to store weights and layer maps, the memory bandwidth requirements and the hardware complexity. As a consequence, the inference throughput has improved and the computing cost has been reduced, allowing inference to be executed on embedded devices. In this work, we propose highly efficient dot-product arithmetic units for ternary and non-ternary convolutional neural networks on FPGA. The non-ternary dot-product unit uses a fused multiply-add that avoids expensive adder trees, while the ternary dot-product unit uses a dual product unit followed by an optimized conditional adder tree structure. In both cases, designs with and without embedded DSP are considered. The solution is configurable and can be adapted to the available number of resources of the FPGA to achieve the best efficiency. A CNN architecture was developed and characterized using the proposed dot product units. The results show a performance improvement of 1.8 \texttimes{} with a 2\texttimes{} more area efficiency for low bit-width quantizations when compared to previous works running large CNNs in FPGA.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = dec,
articleno = {13},
numpages = {36},
keywords = {FPGA, hardware acceleration, convolutional neural network, dot-product, Fused multiply-add}
}

@inproceedings{10.1145/3664968.3664999,
author = {Ronduen, Renn Chester B and Young, Michael N and Redi, Anak Agung Ngurah P},
title = {Utilizing Regression Analysis and Analysis of Variance to Mitigate Production Delays from Using Linear Programming Model and Analytic Hierarchy Process at a Refinery Plant},
year = {2024},
isbn = {9798400717130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664968.3664999},
doi = {10.1145/3664968.3664999},
abstract = {This study utilized Regression Analysis to determine the degree of influence factors have on the different production delays in the Refinery Plant. Analytic Hierarchy Process was also used to choose between three alternatives to reduce the 2,852 hours of delays encountered in the 2020 production process. The results reinforced the idea that purchasing new equipment was the most effective strategy to prevent the recurrence or further increase of the delays in the coming years. This strategy received 53.7 % weighting in terms of importance for counteracting production delays. ANOVA was further employed to compare the mean output associated with each of the three strategies that have been executed at the Refinery Plant: purchasing equipment, implementing Six Sigma practices, and strategizing maintenance. Data analysis was performed using MINITAB software to assess their relative effectiveness. The mean production outputs observed were 5,275 metric tonnes for new equipment, 4,836 metric tonnes for Six Sigma, and 4,386 metric tonnes for strategizing maintenance. Key constraints considered included a maximum allowable delay of 2,000 hours per year to meet consumer demand and a budget of Php 20,200,000 allocated for purchasing new equipment, including installation and maintenance costs. Additionally, the results of the linear programming (LP) model indicated target allowable delay hours for machine, materials, shutdown, and methods at 200, 570, 1015, and 215, respectively. Eliminating 30% of the 2,852 hours of delay within one year was projected to increase profit by Php 233,672,099.84. This study underscores the importance of investing in new equipment for manufacturing efficiency improvement and eliminating process intermittent collection. The individual value plot of output versus strategy shows the varying effectiveness of each strategy. The findings from the two data analysis methods, Regression Analysis and ANOVA, corroborate the AHP-based decision to purchase new equipment. Moreover, a cost benefit analysis reveals the greater effectiveness of this approach compared to haphazard ways of dealing with the problem such as using Six Sigma and continuous repairs.},
booktitle = {Proceedings of the 2024 6th International Conference on Management Science and Industrial Engineering},
pages = {229–237},
numpages = {9},
keywords = {New Equipment, Production Delays, Refinery Plant, Regression Analysis},
location = {Bangkok, Thailand},
series = {MSIE '24}
}

@inproceedings{10.1109/SC41406.2024.00086,
author = {Metcalf, Mekena and Andr\'{e}s-Mart\'{\i}nez, Pablo and Fitzpatrick, Nathan},
title = {Realizing Quantum Kernel Models at Scale with Matrix Product State Simulation},
year = {2024},
isbn = {9798350352917},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC41406.2024.00086},
doi = {10.1109/SC41406.2024.00086},
abstract = {Data representation in quantum state space offers an alternative function space for machine learning tasks. However, benchmarking these algorithms at a practical scale has been limited by ineffective simulation methods. We develop a quantum kernel framework using a Matrix Product State (MPS) simulator and employ it to perform a classification task with 165 features and 6400 training data points, well beyond the scale of any prior work. We make use of a circuit ansatz on a linear chain of qubits with increasing interaction distance between qubits. We assess the MPS simulator performance on CPUs and GPUs and, by systematically increasing the qubit interaction distance, we identify a crossover point beyond which the GPU implementation runs faster. We show that quantum kernel model performance improves as the feature dimension and training data increases, which is the first evidence of quantum model performance at scale.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {80},
numpages = {20},
location = {Atlanta, GA, USA},
series = {SC '24}
}

@inproceedings{10.1145/3324884.3416573,
author = {M\"{u}hlbauer, Stefan and Apel, Sven and Siegmund, Norbert},
title = {Identifying software performance changes across variants and versions},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416573},
doi = {10.1145/3324884.3416573},
abstract = {We address the problem of identifying performance changes in the evolution of configurable software systems. Finding optimal configurations and configuration options that influence performance is already difficult, but in the light of software evolution, configuration-dependent performance changes may lurk in a potentially large number of different versions of the system.In this work, we combine two perspectives---variability and time---into a novel perspective. We propose an approach to identify configuration-dependent performance changes retrospectively across the software variants and versions of a software system. In a nutshell, we iteratively sample pairs of configurations and versions and measure the respective performance, which we use to update a model of likelihoods for performance changes. Pursuing a search strategy with the goal of measuring selectively and incrementally further pairs, we increase the accuracy of identified change points related to configuration options and interactions.We have conducted a number of experiments both on controlled synthetic data sets as well as in real-world scenarios with different software systems. Our evaluation demonstrates that we can pinpoint performance shifts to individual configuration options and interactions as well as commits introducing change points with high accuracy and at scale. Experiments on three real-world systems explore the effectiveness and practicality of our approach.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {611–622},
numpages = {12},
keywords = {active learning, configurable software systems, machine learning, software evolution, software performance},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1145/3608447,
author = {Metz, David and Kumar, Vineet and Sj\"{a}lander, Magnus},
title = {BISDU: A Bit-Serial Dot-Product Unit for Microcontrollers},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {1539-9087},
url = {https://doi.org/10.1145/3608447},
doi = {10.1145/3608447},
abstract = {Low-precision quantized neural networks (QNNs) reduce the required memory space, bandwidth, and computational power, and hence are suitable for deployment in applications such as IoT edge devices. Mixed-precision QNNs, where weights commonly have lower precision than activations or different precision is used for different layers, can limit the accuracy loss caused by low-bit quantization, while still benefiting from reduced memory footprint and faster execution. Previous multiple-precision functional units supporting 8-bit, 4-bit, and 2-bit SIMD instructions have limitations, such as large area overhead, under-utilization of multipliers, and wasted memory space for low and mixed bit-width operations.This article introduces BISDU, a bit-serial dot-product unit to support and accelerate execution of mixed-precision low-bit QNNs on resource-constrained microcontrollers. BISDU is a multiplier-less dot-product unit, with frugal hardware requirements (a population count unit and 2:1 multiplexers). The proposed bit-serial dot-product unit leverages the conventional logical operations of a microcontroller to perform multiplications, which enables efficient software implementations of binary (Xnor), ternary (Xor), and mixed-precision [W\texttimes{}A] (And) dot-product operations.The experimental results show that BISDU achieves competitive performance compared to two state-of-the-art units, XpulpNN and Dustin, when executing low-bit-width CNNs. We demonstrate the advantage that bit-serial execution provides by enabling trading accuracy against weight footprint and execution time. BISDU increases the area of the ALU by 68% and the ALU power consumption by 42% compared to a baseline 32-bit RISC-V (RV32IC) microcontroller core. In comparison, XpulpNN and Dustin increase the area by 6.9\texttimes{} and 11.1\texttimes{} and the power consumption by 3.8\texttimes{} and 5.97\texttimes{}, respectively. The bit-serial state-of-the-art, based on a conventional popcount instruction, increases the area by 42% and power by 32%, with BISDU providing a 37% speedup over it.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = sep,
articleno = {79},
numpages = {22},
keywords = {ISA extension, Low power, Quantized neural networks, Microcontroller, Dot-product, Bit-serial}
}

@inproceedings{10.1145/3071178.3071261,
author = {Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat},
title = {Mining cross product line rules with multi-objective search and machine learning},
year = {2017},
isbn = {9781450349208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3071178.3071261},
doi = {10.1145/3071178.3071261},
abstract = {Nowadays, an increasing number of systems are being developed by integrating products (belonging to different product lines) that communicate with each other through information networks. Cost-effectively supporting Product Line Engineering (PLE) and in particular enabling automation of configuration in PLE is a challenge. Capturing rules is the key for enabling automation of configuration. Product configuration has a direct impact on runtime interactions of communicating products. Such products might be within or across product lines and there usually don't exist explicitly specified rules constraining configurable parameter values of such products. Manually specifying such rules is tedious, time-consuming, and requires expert's knowledge of the domain and the product lines. To address this challenge, we propose an approach named as SBRM that combines multi-objective search with machine learning to mine rules. To evaluate the proposed approach, we performed a real case study of two communicating Video Conferencing Systems belonging to two different product lines. Results show that SBRM performed significantly better than Random Search in terms of fitness values, Hyper-Volume, and machine learning quality measurements. When comparing with rules mined with real data, SBRM performed significantly better in terms of Failed Precision (18%), Failed Recall (72%), and Failed F-measure (59%).},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1319–1326},
numpages = {8},
keywords = {rule mining, product line, multi-objective search, machine learning, configuration},
location = {Berlin, Germany},
series = {GECCO '17}
}

@inproceedings{10.1145/3275245.3275261,
author = {Campos, Denivan and Lima, Crescencio and do Carmo Machado, Ivan},
title = {MERCI: A Method to Evaluate Combinatorial Interaction Testing Tools for Software Product Lines},
year = {2018},
isbn = {9781450365659},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3275245.3275261},
doi = {10.1145/3275245.3275261},
abstract = {Testing a system is a routine activity, and it plays an important role in the software quality assurance process. However, testing highly-configurable systems, such as Software Product Lines (SPL), is a rather complex activity, due to the presence of variability in its engineering process, which increases the number of product configurations to test. The underlying idea to make testing feasible in SPL engineering is to select a small but representative subset of products to test, by employing techniques such as combinatorial interaction testing (CIT). This paper presents Method to Evaluate Combinatorial Interaction (MERCI), a novel method to evaluate the adequacy of existing CIT tools for SPL engineering, with respect to three measures: defect detection, test coverage, and test execution length. We carried out an empirical evaluation to compare four CIT tools: ACTS, CATS, PICTMaster and VPTag. The results show that the method may serve as an affordable strategy to evaluate how the CIT tools could behave in an SPL testing scenario.},
booktitle = {Proceedings of the XVII Brazilian Symposium on Software Quality},
pages = {151–159},
numpages = {9},
keywords = {Testing Tools, Software Testing Strategies, Software Product Lines, Combinatorial Interaction Testing},
location = {Curitiba, Brazil},
series = {SBQS '18}
}

@article{10.5555/3722577.3722812,
author = {Wacker, Jonas and Kanagawa, Motonobu and Filippone, Maurizio},
title = {Improved random features for dot product kernels},
year = {2024},
issue_date = {January 2024},
publisher = {JMLR.org},
volume = {25},
number = {1},
issn = {1532-4435},
abstract = {Dot product kernels, such as polynomial and exponential (softmax) kernels, are among the most widely used kernels in machine learning, as they enable modeling the interactions between input features, which is crucial in applications like computer vision, natural language processing, and recommender systems. We make several novel contributions for improving the efficiency of random feature approximations for dot product kernels, to make these kernels more useful in large scale learning. First, we present a generalization of existing random feature approximations for polynomial kernels, such as Rademacher and Gaussian sketches and TensorSRHT, using complex-valued random features. We show empirically that the use of complex features can significantly reduce the variances of these approximations. Second, we provide a theoretical analysis for understanding the factors affecting the efficiency of various random feature approximations, by deriving closed-form expressions for their variances. These variance formulas elucidate conditions under which certain approximations (e.g., TensorSRHT) achieve lower variances than others (e.g., Rademacher sketches), and conditions under which the use of complex features leads to lower variances than real features. Third, by using these variance formulas, which can be evaluated in practice, we develop a data-driven optimization approach to improve random feature approximations for general dot product kernels, which is also applicable to the Gaussian kernel. We describe the improvements brought by these contributions with extensive experiments on a variety of tasks and datasets.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {235},
numpages = {75},
keywords = {random features, randomized sketches, dot product kernels, polynomial kernels, large scale learning}
}

@inproceedings{10.1145/3647649.3647690,
author = {Westmacott, Henry and Ivrissimtzis, Ioannis and Weinzierl, Tobias},
title = {A Multiscale Optimisation Algorithm for Shape and Material Reconstruction from a Single X-ray Image},
year = {2024},
isbn = {9798400716720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3647649.3647690},
doi = {10.1145/3647649.3647690},
abstract = {We produce thickness and bone to soft tissue ratio estimations from a single, 2D medical X-ray image. For this, we simulate the scattering of the rays through a model of the object and embed this simulation into an optimiser which iteratively adjusts the model to match the X-ray simulation to the observed X-ray image. Utilising a combination of different techniques, first, a CNN-based image segmentation serves as a regularisation term to the underlying cost function to guide the descent, while domain knowledge about physical parameter correlations is injected by additional penalty terms. Next, the optimiser is embedded into a multilevel framework which, similar to multi-grid’s philosophy, successively improves the model on varying resolutions while individual resolutions focus on particular terms of the cost function. Initial results suggest that we can obtain meaningful thickness and material estimations.},
booktitle = {Proceedings of the 2024 7th International Conference on Image and Graphics Processing},
pages = {252–259},
numpages = {8},
keywords = {X-ray simulation, gradient descent, material estimation, model reconstruction, regularization},
location = {Beijing, China},
series = {ICIGP '24}
}

@inproceedings{10.1145/3675249.3675352,
author = {Lou, Fei and Xu, Jianing and Jiang, Ying and Chen, Qirui and Zhang, Yifan},
title = {Product recommendation algorithm based on user collaborative filtering},
year = {2024},
isbn = {9798400718267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675249.3675352},
doi = {10.1145/3675249.3675352},
abstract = {This significant article focuses on the product recommendation related issues involved in the entire process of power marketing. It takes the user collaborative filtering recommendation algorithm and project collaborative filtering algorithm as a solid foundation, and creatively proposes a collaborative filtering hybrid recommendation optimization model. By fully utilizing various means to deeply mine the historical data of users, we have successfully obtained their interests and preferences, and meticulously classified and divided different user interests and preferences, ultimately being able to accurately recommend various types of items that meet their preferences to users. A large number of experimental results fully demonstrate that the carefully proposed recommendation algorithm in this article has a significantly higher level of recommendation accuracy, and its algorithm performance also shows better performance.},
booktitle = {Proceedings of the 2024 International Conference on Computer and Multimedia Technology},
pages = {591–595},
numpages = {5},
keywords = {Combination recommendation prediction, Product recommendations, User negotiation for joint recommendation},
location = {Sanming, China},
series = {ICCMT '24}
}

@inproceedings{10.1145/2739482.2768422,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Assun\c{c}\~{a}o, Wesley K.G. and Fischer, Stefan and Vergilio, Silvia R. and Egyed, Alexander},
title = {Genetic Improvement for Software Product Lines: An Overview and a Roadmap},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2768422},
doi = {10.1145/2739482.2768422},
abstract = {Software Product Lines (SPLs) are families of related software systems that provide different combinations of features. Extensive research and application attest to the significant economical and technological benefits of employing SPL practices. However, there are still several challenges that remain open. Salient among them is reverse engineering SPLs from existing variants of software systems and their subsequent evolution. In this paper, we aim at sketching connections between research on these open SPL challenges and ongoing work on Genetic Improvement. Our hope is that by drawing such connections we can spark the interest of both research communities on the exciting synergies at the intersection of these subject areas.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {823–830},
numpages = {8},
keywords = {variability, software product lines, genetic programming, genetic improvement, evolutionary algorithms},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@inproceedings{10.1145/3675888.3676139,
author = {Yadav, Asmita and Usman, Mufti and Sati, Anurag and Jain, Sidham},
title = {Revolutionizing Software Development: Enhancing Quality and Performance Through Code Refactoring},
year = {2024},
isbn = {9798400709722},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675888.3676139},
doi = {10.1145/3675888.3676139},
abstract = {In software development, optimizing code for improved efficiency is akin to tidying up a cluttered space—it benefits both developers and end-users. However, while the focus often lies on enhancing usability and readability, there is a growing emphasis on reducing energy consumption. This paper investigates existing research on energy-efficient coding practices and explores strategies to address this challenge within the constraints of limited power resources. Our project presents a unique perspective on code optimization, emphasizing energy conservation without sacrificing code clarity. Through techniques such as the use of Enums, elimination of duplicate code, and simplification of complex logic, we aimed to minimize energy usage while maintaining code comprehensibility. Additionally, we implemented robust error-handling mechanisms to enhance code reliability. The effectiveness of these optimizations was evaluated through performance testing, measuring energy consumption, execution speed, and code complexity. By implementing these changes, our research contributes not only to improved user experiences but also to environmental sustainability through reduced energy consumption.},
booktitle = {Proceedings of the 2024 Sixteenth International Conference on Contemporary Computing},
pages = {715–725},
numpages = {11},
keywords = {Code Refactoring, Energy Efficiency, Error Handling, Optimization, Software Development},
location = {Noida, India},
series = {IC3-2024}
}

@inproceedings{10.5555/381473.381482,
author = {Bosch, Jan},
title = {Software product lines: organizational alternatives},
year = {2001},
isbn = {0769510507},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Software product lines enjoy increasingly wide adoption in the software industry. Most authors focus on the technical and process aspects and assume an organizational model consisting of a domain engineering unit and several application engineering units. In our cooperation with several software development organizations applying software product line principles, we have identified several other organizational models that are employed as well. In this article, we present a number of organizational alternatives, organized around four main models, i.e. development department, business units, domain engineering unit and hierarchical domain engineering units. For each model, its characteristics, applicability and advantages and disadvantages are discussed, as well as an example. Based on an analysis of these models, we present three factors that influence the choice of the organizational model, i.e. product-line assets, the responsibility levels and the type of organizational units.},
booktitle = {Proceedings of the 23rd International Conference on Software Engineering},
pages = {91–100},
numpages = {10},
location = {Toronto, Ontario, Canada},
series = {ICSE '01}
}

@inproceedings{10.1145/3550356.3561572,
author = {Gottschalk, Sebastian and Bhat, Rakshit and Weidmann, Nils and Kirchhoff, Jonas and Engels, Gregor},
title = {Low-code experimentation on software products},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3561572},
doi = {10.1145/3550356.3561572},
abstract = {The continuous development of software products can be supported by systematically testing different software variants with the users. During this so-called continuous experimentation, different variants are presented to dedicated user groups, and the results are compared to determine the better-performing one. However, the product owner often defines those experiments while the software developers do their implementation. This, in turn, results in additional communication and synchronization effort. To bridge the gap between the definition and implementation of experiments, we provide a solution based on low-code development. Low-code development, in turn, allows the development of software products by non-developers using a graphical user interface (GUI). Within our solution, the product owner can model the experiments, product variants, and user groups within a GUI. Code wrappers are generated from those models, which the software developers can modify. Last, those variants are executed by different users, and the results are visualized for the product owner. This workshop paper shows the technical feasibility based on a streaming application.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {798–807},
numpages = {10},
keywords = {split-tests, software experimentation, prototypes, model-driven engineering, low-code},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3581783.3613755,
author = {Du, Wenzhe and Haoyang, Su and Cam-Tu, Nguyen and Sun, Jian},
title = {Enhancing Product Representation with Multi-form Interactions for Multimodal Conversational Recommendation},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3613755},
doi = {10.1145/3581783.3613755},
abstract = {Multimodal Conversational Recommendation aims to find appropriate products based on a multi-turn dialogue, where user requests and products can be presented in both visual and textual modalities. While previous studies have focused on understanding user preferences from conversational contexts, the task of product modeling has been relatively unexplored. This study targets to fill this gap and demonstrates that information from multiple product views and cross-view interactions are essential for recommendation, along with dialog information. To this end, a product image is first encoded using a gated multi-view image encoder, and representations for the global and local views are obtained. On the textual side, two views are considered: the structure view (product attributes) and the sequence view (product description/reviews). Two forms of inter-modal interactions for product representation are then modeled: interactions between the global image view and the textual structure view, and interactions between the local image view and the textual sequence view. Furthermore, the representation is enhanced to attend to the latest user request in the dialog context, resulting in query-aware product representation. The experimental results indicate that our method, named Enteract, achieves state-of-the-art performance on two well-known datasets (MMD and SIMMC).},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {6491–6500},
numpages = {10},
keywords = {multimodal conversational recommendation, product modeling},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@article{10.1145/3707456,
author = {Zhou, Yufei and Tang, Haihua and Zhu, Longtao and Ding, Hao and Qian, Junyan},
title = {Refining Code-Line-Level Bugginess Identification: Getting the Best of Both Worlds by Fusing Syntactic and Semantic Features},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3707456},
doi = {10.1145/3707456},
abstract = {Background: Code-line-level bugginess identification (CLBI) is an important area within software quality assurance, aiming to pinpoint potential buggy source code lines in a given software product. Recently, two concurrent approaches, GLANCE and DeepLineDP, have showcased impressive performance by respectively leveraging syntactic and semantic features compared with the existing state-of-the-art (SOTA) approaches in this field. Problem: Yet, the literature lacks a thorough investigation that fuses these two types of features to enhance CLBI. Such fusion holds the promise of significantly improving the efficacy of identifying defective lines. Objective: We aim to advance CLBI by fusing syntactic and semantic features, thereby harnessing their respective strengths. Method: We propose to build a CLBI approach, booSting DeePLineDP wIth syntaCtic fEatures (SPLICE), by fusing syntactic features from GLANCE and semantic features from DeepLineDP. SPLICE comprises three variants—SPLICE-S, SPLICE-G, and SPLICE-F—each utilizing a unique line-level sorting approach. We make a comprehensive comparison with the existing SOTA approaches using six performance metrics. Result: Through an analysis of nine open source projects, our experimental results reveal that SPLICE is competitive with current SOTA CLBI approaches. Notably, SPLICE-F demonstrates superiority over all SOTA CLBI approaches, including GLANCE and DeepLineDP, across all six metrics, indicating a substantial improvement. Conclusion: This discovery underscores the critical importance of future CLBI research in fusing syntactic and semantic features to construct more effective bugginess identification approaches. It is worth noting that the analysis was conducted within the context of Java programs, which highlights the potential for exploring similar methods in other programming languages in future research.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {73},
numpages = {43},
keywords = {Defect prediction, line-level, deep learning, syntactic features, semantic features}
}

@inproceedings{10.1145/3674746.3674793,
author = {Falkowski, Piotr and Oleksiuk, Jan and Jeznach, Kajetan and Aktan, Mehmet Emin},
title = {Method of automatic biomedical signals interpretation for safety supervision and optimisation of the exoskeleton-aided physiotherapy of lower extremity},
year = {2024},
isbn = {9798400716782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674746.3674793},
doi = {10.1145/3674746.3674793},
abstract = {Rehabilitation robots help physiotherapists in performing repetitive and tiring tasks. Moreover, they are also capable of automating therapy. However, for such an application, they require algorithms assessing patient’s performance and monitoring their safety. In therapy of lower extremities aided with exoskeletons, biosignal measurements can be used additionally to dynamics parameters to provide the mentioned analyses. The paper presents a concept of assessing the treatment based on electromyography of selected muscular groups and assessing the patient’s emotional states based on electroencephalography. The preliminary measurements enabled defining expected muscular activity dependent on the extremity configuration and emotional states based on the frequency analysis of the scalp regions. The presented methodology contains measurement flow, interpreting algorithms, and reactions of the systems to the defined states. Use of this can contribute to the development of minimally supervised systems for home-like environment use. The presented universal method is easily transferable to the exoskeleton-based systems, even different in terms of kinematics. Furthermore, it can also be analogically introduced to the systems dedicated to the upper extremities.},
booktitle = {Proceedings of the 2024 4th International Conference on Robotics and Control Engineering},
pages = {57–63},
numpages = {7},
keywords = {EEG, EMG, automation, exoskeleton, rehabilitation robotics},
location = {Edinburgh, United Kingdom},
series = {RobCE '24}
}

@inproceedings{10.1145/1837154.1837157,
author = {Siegmund, Norbert and Feigenspan, Janet and Soffner, Michael and Fruth, Jana and K\"{o}ppen, Veit},
title = {Challenges of secure and reliable data management in heterogeneous environments},
year = {2010},
isbn = {9781605589923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1837154.1837157},
doi = {10.1145/1837154.1837157},
abstract = {Ubiquitous computing is getting more important since requirements for complex systems grow fast. In these systems, embedded devices have to fulfill different tasks. They have to monitor the environment, store data, communicate with other devices, and react to user input. In addition to this complexity, quality issues such as security and reliability have to be considered, as well, due to their increasing use in life critical application scenarios. Finally, different devices with different application goals are used, which results in interoperability problems. In this paper, we highlight challenges for interoperability, data management, and security, which arise with complex systems. Furthermore, we present approaches to overcome different problems and how an integrated solution can be realized using software product line techniques.},
booktitle = {Proceedings of the First International Workshop on Digital Engineering},
pages = {17–24},
numpages = {8},
keywords = {software product lines, security, digital engineering, data management},
location = {Magdeburg, Germany},
series = {IWDE '10}
}

@inproceedings{10.1145/3665348.3665381,
author = {Yuan, Mengzhen and Lu, Lili},
title = {Multimodal transportation path optimization of consumer electronics products considering transportation time uncertainty},
year = {2024},
isbn = {9798400709562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665348.3665381},
doi = {10.1145/3665348.3665381},
abstract = {This paper focuses on the multimodal transportation path optimization problem for consumer electronics products under the condition of transport time uncertainty. Approaching the problem from the perspective of multimodal transport operators within a defined network, the study integrates the stochastic travel time, stochastic transfer time, customer demands for timely delivery and the complexity of multiple orders. Combining with the high-value time-sensitive characteristics of consumer electronic products and taking the minimum of total cost as the optimization objective, we establish a multimodal transport path optimization model. In order to solve this model, a genetic algorithm with elite strategy based on Monte Carlo sampling is designed. Finally, we verify the validity of the model and algorithm through a specific case. And the case results are analyzed. The case results indicate that for the long-distance transportation of goods such as consumer electronics, which have a very high time-value cost, it is more appropriate to choose a multimodal transportation scheme based on road and railroad transportation, while waterway transportation does not have an advantage. In addition, further consideration of the uncertainty of the transportation time results in a transportation solution that ensures a higher probability of on-time delivery, while the total cost increases slightly but the transportation time decreases. The transportation scheme is more stable. The results of this paper provide a reference basis for the actual multimodal transportation path optimization problem of consumer electronics.},
booktitle = {Proceedings of the 2024 International Conference on Generative Artificial Intelligence and Information Security},
pages = {184–193},
numpages = {10},
location = {Kuala Lumpur, Malaysia},
series = {GAIIS '24}
}

@inproceedings{10.1145/3634713.3634732,
author = {Acher, Mathieu},
title = {A Demonstration of End-User Code Customization Using Generative AI},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634732},
doi = {10.1145/3634713.3634732},
abstract = {Producing a variant of code is highly challenging, particularly for individuals unfamiliar with programming. This demonstration introduces a novel use of generative AI to aid end-users in customizing code. We first describe how generative AI can be used to customize code through prompts and instructions, and further demonstrate its potential in building end-user tools for configuring code. We showcase how to transform an undocumented, technical, low-level TikZ into a user-friendly, configurable, Web-based customization tool written in Python, HTML, CSS, and JavaScript and itself configurable. We discuss how generative AI can support this transformation process and traditional variability engineering tasks, such as identification and implementation of features, synthesis of a template code generator, and development of end-user configurators. We believe it is a first step towards democratizing variability programming, opening a path for end-users to adapt code to their needs.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {139–145},
numpages = {7},
keywords = {LLM, code synthesis, customization, end-user programming, generative AI, generator, software product lines, variability},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.1145/3689031.3696082,
author = {Chen, Yuanliang and Ma, Fuchen and Zhou, Yuanhang and Yan, Zhen and Liao, Qing and Jiang, Yu},
title = {Themis: Finding Imbalance Failures in Distributed File Systems via a Load Variance Model},
year = {2025},
isbn = {9798400711961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689031.3696082},
doi = {10.1145/3689031.3696082},
abstract = {A distributed file system (DFS) is a file system that spans across multiple file servers or multiple locations. The load balancing mechanism in a DFS is crucial, as it optimizes resource utilization across all nodes and improves response times. However, incorrect load scheduling or implementation errors in load balancing algorithms can lead to system imbalance, hang-ups, and even crashes. Such imbalance failures may be critical and pose a significant threat to the availability and security of distributed file systems.This paper presents a detailed study of real-world imbalance failures in four widely used DFSes, exploring their symptoms and triggering conditions. We found that test cases that incorporate both client requests and system configuration inputs are crucial for exposing these imbalances. However, generating such high-quality test cases is challenging due to the extensive combinations of these two input spaces. Guided by our study, we designed a testing framework named Themis. To efficiently prune the search space, Themis first models both the request and configuration inputs and transforms them into operation sequences. It then employs load variance-guided fuzzing to thoroughly explore the operation sequence and constantly generate test cases that make nodes loaded as differently as possible. Finally, Themis introduces a load detector to monitor the resource usage of each distributed node and precisely identify any imbalances. Themis has detected 10 new imbalance failures in four real-world DFSes, which have been addressed by the respective maintainers.},
booktitle = {Proceedings of the Twentieth European Conference on Computer Systems},
pages = {329–344},
numpages = {16},
keywords = {Distributed File System, Load Balance, Testing},
location = {Rotterdam, Netherlands},
series = {EuroSys '25}
}

@inproceedings{10.1145/3688268.3688273,
author = {Murayama, Mao and Ito, Nobuhiro and Otsuka, Takanobu},
title = {Proposal and Implementation of Production Schedule Optimization Method Considering Job Processing Start Time and Processing Order},
year = {2024},
isbn = {9798400718038},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3688268.3688273},
doi = {10.1145/3688268.3688273},
abstract = {Recent social conditions have caused to rise of raw material prices in the manufacturing industry. In addition, efforts to realize a decarbonized society are gaining momentum worldwide. Therefore, the manufacturing industry is required to reduce manufacturing costs and energy consumption by improving production efficiency. The objective of this study is to equalize machine utilization rates to reduce product retention on the production line which causes lower production efficiency. To deal with this, this study aims to develop a production scheduling system. For this purpose, we propose and implement the system that optimizes production schedules by considering the start time of job processing and the processing order of jobs using the production line simulation. Experiments have shown that the proposed system can produce production schedules with shorter product retention time than those used in real factories.},
booktitle = {Proceedings of the 2024 12th International Conference on Computer and Communications Management},
pages = {25–31},
numpages = {7},
keywords = {Optimization algorithm, Parameter, Product retention time, Production scheduling},
location = {
},
series = {ICCCM '24}
}

@inproceedings{10.1145/3689484.3690735,
author = {Ghallab, Karim and Ziadi, Tewfik and Chalal, Zaak},
title = {An Extensible Feature-Oriented Approach for Fine-Grained Code Quality Analysis},
year = {2024},
isbn = {9798400712111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689484.3690735},
doi = {10.1145/3689484.3690735},
abstract = {Assessing code quality is crucial for effective software maintenance and evolution. Traditional tools like SonarQube offer valuable insights at the application level but lack the granularity needed for detailed, feature-specific analysis. This paper emphasizes the importance of feature-oriented code quality analysis, often overlooked by mainstream tools due to the challenge of correlating high-level feature descriptions with low-level code implementations. To tackle this issue, we leverage existing feature location techniques to introduce a novel approach enabling granular analysis tailored to specific application features. We discuss the motivations for this approach, highlighting its potential to improve precision in enhancement and maintenance strategies. Additionally, this paper introduces a tool-based approach known as InsightMapper. We also present a study demonstrating the benefits of this method through the analysis of two case studies, featuring a recognized benchmark in the feature location domain.},
booktitle = {Proceedings of the 23rd ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {15–28},
numpages = {14},
keywords = {ArgoUml, Code Quality Analysis, Feature Location, Feature-Oriented, InsightMapper, Mobioos Forge, Software Product Lines, SonarQube, eShopOnContainers},
location = {Pasadena, CA, USA},
series = {GPCE '24}
}

@article{10.5555/3546258.3546418,
author = {Grosnit, Antoine and Cowen-Rivers, Alexander I. and Tutunov, Rasul and Griffiths, Ryan-Rhys and Wang, Jun and Bou-Ammar, Haitham},
title = {Are we forgetting about compositional optimisers in Bayesian optimisation?},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Bayesian optimisation presents a sample-efficient methodology for global optimisation. Within this framework, a crucial performance-determining subroutine is the maximisation of the acquisition function, a task complicated by the fact that acquisition functions tend to be non-convex and thus nontrivial to optimise. In this paper, we undertake a comprehensive empirical study of approaches to maximise the acquisition function. Additionally, by deriving novel, yet mathematically equivalent, compositional forms for popular acquisition functions, we recast the maximisation task as a compositional optimisation problem, allowing us to benefit from the extensive literature in this field. We highlight the empirical advantages of the compositional approach to acquisition function maximisation across 3958 individual experiments comprising synthetic optimisation tasks as well as tasks from Bayesmark. Given the generality of the acquisition function maximisation subroutine, we posit that the adoption of compositional optimisers has the potential to yield performance improvements across all domains in which Bayesian optimisation is currently being applied.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {160},
numpages = {78},
keywords = {empirical analysis, acquisition functions, compositional optimisation, Bayesian optimisation, black box optimisation}
}

@inproceedings{10.1145/2451617.2451619,
author = {Kowal, Matthias and Schulze, Sandro and Schaefer, Ina},
title = {Towards efficient SPL testing by variant reduction},
year = {2013},
isbn = {9781450318679},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2451617.2451619},
doi = {10.1145/2451617.2451619},
abstract = {Testing software systems plays a pivotal role for quality, reliability, and safety of such systems. Several approaches exist that provide efficient algorithms to test one software system. However, in the context of variable software systems, called software product lines (SPLs), testing has to deal with potentially thousands of variants. Unfortunately, current approaches do not scale to this problem and thus testing SPLs efficiently is a challenging task. In this paper, we propose an approach to reduce the test set by explicitly modeling information about shared resources and communication in feature models. As a result, we can figure out features that interact with each other and thus are more likely to cause problems. We show with a small case study that our approach reduces both, the features under test as well as the time for computing all feature combinations to be tested.},
booktitle = {Proceedings of the 4th International Workshop on Variability &amp; Composition},
pages = {1–6},
numpages = {6},
keywords = {testing, software product lines, feature models},
location = {Fukuoka, Japan},
series = {VariComp '13}
}

@proceedings{10.1145/3634713,
title = {VaMoS '24: Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bern, Switzerland}
}

@article{10.1145/505532.505551,
author = {Butler, Greg},
title = {Generative techniques for product lines},
year = {2001},
issue_date = {November 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/505532.505551},
doi = {10.1145/505532.505551},
abstract = {A software product line leverages the knowledge of one or more domains in order to achieve short time-to-market, cost savings, and high quality software. The highest level of reuse comes by using domain-specific languages or visual builders to describe a member of the product line, and to generate the member from the description. Generative techniques can help us to capture the configuration knowledge for a product line and use it to generate concrete family members. This workshop focuses on technical issues of product lines, rather than economic issues.},
journal = {SIGSOFT Softw. Eng. Notes},
month = nov,
pages = {74–76},
numpages = {3}
}

@inproceedings{10.1145/3368826.3377914,
author = {Savage, Joe and Jones, Timothy M.},
title = {HALO: post-link heap-layout optimisation},
year = {2020},
isbn = {9781450370479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368826.3377914},
doi = {10.1145/3368826.3377914},
abstract = {Today, general-purpose memory allocators dominate the landscape of dynamic memory management. While these solutions can provide reasonably good behaviour across a wide range of workloads, it is an unfortunate reality that their behaviour for any particular workload can be highly suboptimal. By catering primarily to average and worst-case usage patterns, these allocators deny programs the advantages of domain-specific optimisations, and thus may inadvertently place data in a manner that hinders performance, generating unnecessary cache misses and load stalls. To help alleviate these issues, we propose HALO: a post-link profile-guided optimisation tool that can improve the layout of heap data to reduce cache misses automatically. Profiling the target binary to understand how allocations made in different contexts are related, we specialise memory-management routines to allocate groups of related objects from separate pools to increase their spatial locality. Unlike other solutions of its kind, HALO employs novel grouping and identification algorithms which allow it to create tight-knit allocation groups using the entire call stack and to identify these efficiently at runtime. Evaluation of HALO on contemporary out-of-order hardware demonstrates speedups of up to 28% over jemalloc, out-performing a state-of-the-art data placement technique from the literature.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {94–106},
numpages = {13},
keywords = {Memory management, binary rewriting, cache locality, dynamic allocation, profile-guided optimisation},
location = {San Diego, CA, USA},
series = {CGO '20}
}

@inproceedings{10.1145/3520304.3533637,
author = {Cully, Antoine and Mouret, Jean-Baptiste and Doncieux, St\'{e}phane},
title = {Quality-diversity optimisation},
year = {2022},
isbn = {9781450392686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520304.3533637},
doi = {10.1145/3520304.3533637},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {864–889},
numpages = {26},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@inproceedings{10.1145/3377024.3377043,
author = {Meixner, Kristof and Rabiser, Rick and Biffl, Stefan},
title = {Feature identification for engineering model variants in cyber-physical production systems engineering},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377043},
doi = {10.1145/3377024.3377043},
abstract = {In Cyber-Physical Production System (CPPS) engineering, Assembly Sequence (AS) models of products are primary engineering artifacts. Product variants are often designed as Product-Process-Resource (PPR) AS models that are initiated with clone-and-own approaches and by the manual derivation of shared features. This paper introduces the PPR Feature Candidate Identification (PPR-FCI) approach for identifying features from PPR AS models of product variants. From these features our approach derives a superimposed PPR that describes design options for engineers planning the CPPS. The approach is based on existing feature extraction research which we adapted to the scope of PPR models in CPPS engineering. Based on a real-world product line, we evaluate our PPR-FCI approach for feasibility and usefulness by comparing our automated approach to the traditional manual approach with domain experts. Initial findings show that the approach can identify relevant features from PPR AS models and domain experts found the results useful. However, further research is required to improve the PPR-FCI approach regarding the optimization of PPR Assembly Sequence models.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {18},
numpages = {5},
keywords = {product-process-resource (PPR), product lines, feature extraction, cyber-physical production system (CPPS)},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@article{10.1145/3715002,
author = {Blasco, Daniel and Iglesias, Antonio and Echeverr\'{\i}a, Jorge and P\'{e}rez, Francisca and Cetina, Carlos},
title = {Introducing Phylogenetics in Search-based Software Engineering: Phylogenetics-aware SBSE},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715002},
doi = {10.1145/3715002},
abstract = {Phylogenetics studies the relationships, in terms of biological history and kinship, of a set of taxa (e.g., species). We argue that in Search-based Software Engineering (SBSE), the individuals of an evolutionary computation-driven population could be considered as taxa for which the leverage of Phylogenetic Inference might be beneficial. In this work, we present our Phylogenetics-aware SBSE approach. Our approach introduces a novel Phylogenetic Operation to promote results which are sufficiently aligned (in terms of lineage) with a certain reference given by the domain expert. Our approach is evaluated in two heterogeneous industrial case studies: Procedural Content Generation from Game Software Engineering, and Feature Location from Software Maintenance. The results are analyzed using quality-of-the-solution and acceptance-by-developers measurements. We performed a statistical analysis to determine whether the impact on the results is significant compared to baselines that do not leverage Phylogenetics. The results show that our approach significantly outperforms two baselines in both case studies. Furthermore, two focus groups confirmed the acceptance of our approach and stressed that solution acceptance may make the difference in industrial environments. Our work has the potential to motivate a new breed of research work on Phylogenetics awareness to produce better results in Software Engineering.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Phylogenetics, Search-based Software Engineering, Evolutionary Computation, Game Software Engineering, Procedural Content Generation, Software Maintenance, Feature Location, Model-Driven Engineering}
}

@article{10.1145/3649319,
author = {Quattrocchi, Giovanni and Heuvel, Willem-Jan van den and Tamburri, Damian Andrew},
title = {The Data Product-service Composition Frontier: A Hybrid Learning Approach},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3649319},
doi = {10.1145/3649319},
abstract = {The service dominant logic is a base concept behind modern economies and software products, with service composition being a well-known practice for companies to gain a competitive edge over others by joining differentiated services together, typically assembled according to a number of features. At the other end of the spectrum, product compositions are a marketing device to sell products together in bundles that often augment the value for the customer, e.g., with suggested product interactions, sharing, and so on. Unfortunately, currently each of these two streams—namely, product and service composition—are carried out and delivered individually in splendid isolation: anything is being offered as a product and as a service, disjointly. We argue that the next wave of services computing features more and more service fusion with physical counterparts as well as data around them. Therefore a need emerges to investigate the interactive engagement of both (data) products and services. This manuscript offers a real-life implementation in support of this argument, using (1) genetic algorithms (GA) to shape product-service clusters, (2) end-user feedback to make the GAs interactive with a data-driven fashion, and (3) a hybridized approach which factors into our solution an ensemble machine-learning method considering additional features. All this research was conducted in an industrial environment. With such a cross-fertilized, data-driven, and multi-disciplinary approach, practitioners from both fields may benefit from their mutual state of the art as well as learn new strategies for product, service, and data product-service placement for increased value to the customer as well as the service provider. Results show promise but also highlight plenty of avenues for further research.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = mar,
articleno = {6},
numpages = {22},
keywords = {Anomaly detection, time series, unsupervised, literature review}
}

@inproceedings{10.1145/3560828.3564007,
author = {Cabrera Arteaga, Javier and Laperdrix, Pierre and Monperrus, Martin and Baudry, Benoit},
title = {Multi-variant Execution at the Edge},
year = {2022},
isbn = {9781450398787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560828.3564007},
doi = {10.1145/3560828.3564007},
abstract = {Edge-Cloud computing offloads parts of the computations that traditionally occurs in the cloud to edge nodes. The binary format WebAssembly is increasingly used to distribute and deploy services on such platforms. Edge-Cloud computing providers let their clients deploy stateless services in the form of WebAssembly binaries, which are then translated to machine code, sandboxed and executed at the edge. In this context, we propose a technique that (i) automatically diversifies WebAssembly binaries that are deployed to the edge and (ii) randomizes execution paths at runtime. Thus, an attacker cannot exploit all edge nodes with the same payload. Given a service, we automatically synthesize functionally equivalent variants for the functions providing the service. All the variants are then wrapped into a single multivariant WebAssembly binary. When the service endpoint is executed, every time a function is invoked, one of its variants is randomly selected. We implement this technique in the MEWE tool and we validate it with 7 services for which MEWE generates multivariant binaries that embed hundreds of function variants. We execute the multivariant binaries on the world-wide edge platform provided by Fastly, as part as a research collaboration. We show that multivariant binaries exhibit a real diversity of execution traces across the whole edge platform distributed around the globe.},
booktitle = {Proceedings of the 9th ACM Workshop on Moving Target Defense},
pages = {11–22},
numpages = {12},
keywords = {webassembly, multivariant execution, moving target defense, edge-cloud computing, diversification},
location = {Los Angeles, CA, USA},
series = {MTD'22}
}

@inproceedings{10.1109/SCW63240.2024.00182,
author = {Dr\'{a}vai, Bal\'{a}zs and Reguly, Istv\'{a}n Z.},
title = {Benchmarking the Evolution of Performance and Energy Efficiency Across Recent Generations of Intel Xeon Processors},
year = {2025},
isbn = {9798350355543},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SCW63240.2024.00182},
doi = {10.1109/SCW63240.2024.00182},
abstract = {In the span of 1.5 years, Intel has launched four families of Xeon Processors, with some novel architectural features; first the Sapphire Rapids generation which featured a version with on-package HBM, the Emerald Rapids generation, and then differentiated by releasing the performance-oriented Granite Rapids and the efficiency-oriented Sierra Forest families. In this work, we evaluate the performance and efficiency of CPUs from each of these generations and variants, with a particular focus on bandwidth-bound high performance computing (HPC) applications. We contrast runtime and energy consumption figures and track trends across generations.},
booktitle = {Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {1413–1419},
numpages = {7},
keywords = {Benchmarking, CFD, Cache, Energy, Performance, Xeon},
location = {Atlanta, GA, USA},
series = {SC-W '24}
}

@inproceedings{10.1145/3696443.3708935,
author = {Fan, Enming and Guan, Xiaofeng and Hu, Fan and Shi, Heng and Zhou, Hao and Yao, Jianguo},
title = {Postiz: Extending Post-increment Addressing for Loop Optimization and Code Size Reduction},
year = {2025},
isbn = {9798400712753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696443.3708935},
doi = {10.1145/3696443.3708935},
abstract = {Memory access instructions with auto-addressing modes are prevalent in various Instruction Set Architectures (ISAs), yet their use in compilers remains limited. Existing methods address code optimization in one of two ways: they either focus on reducing code size, but are constrained to basic block-level optimizations and may not fully exploit architectural benefits, or they optimize loop performance, often neglecting the advantages of post-increment instructions and focusing primarily on innermost loops while leaving outer loops unoptimized.  To address these shortcomings and meet the needs of real-world Machine Learning (ML) applications, we introduce Postiz, a novel post-increment loop optimization technique. Postiz extends post-increment optimizations beyond traditional limits, incorporating enhancements for inner loops, cross-loop regions, and nested loop structures. Through a profitability analysis, Postiz optimizes code judiciously, leveraging architectural advantages and reducing code size without compromising improvement made by other optimizations.  Our experiments show that Postiz is effective, achieving an optimization coverage of 98.04% on MobileNet and BERT benchmarks. In comparison to default LLVM optimization, Postiz generates approximately four times more post-increment instructions. Moreover, it reduces code size by an average of 9.45% across various platforms. These improvements represent significant advancements over current methods, showcasing Postiz’s potential to enhance compiler optimizations in a meaningful way.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {600–613},
numpages = {14},
keywords = {auto-addressing, compiler, domain-specific architecture, optimization, post-increment, strength reduction},
location = {Las Vegas, NV, USA},
series = {CGO '25}
}

@inproceedings{10.1109/PESOS.2009.5068815,
author = {Mietzner, Ralph and Metzger, Andreas and Leymann, Frank and Pohl, Klaus},
title = {Variability modeling to support customization and deployment of multi-tenant-aware Software as a Service applications},
year = {2009},
isbn = {9781424437160},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/PESOS.2009.5068815},
doi = {10.1109/PESOS.2009.5068815},
abstract = {More and more companies are offering their software by following the Software as a Service (SaaS) model. The promise of the SaaS model is to exploit economies of scale on the provider side by hosting multiple customers (or tenants) on the same hardware and software infrastructure. However, to attract a significant number of tenants, SaaS applications have to be customizable to fulfill the varying functional and quality requirements of individual tenants. In this paper, we describe how variability modeling techniques from software product line engineering can support SaaS providers in managing the variability of SaaS applications and their requirements. Specifically, we propose using explicit variability models to systematically derive customization and deployment information for individual SaaS tenants. We also demonstrate how variability models could be used to systematically consider information about already deployed SaaS applications for efficiently deploying SaaS applications for new tenants. We illustrate our approach by a running example for a meeting planning application.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Principles of Engineering Service Oriented Systems},
pages = {18–25},
numpages = {8},
series = {PESOS '09}
}

@inproceedings{10.1109/ASE.2011.6100118,
author = {Soltani, Samaneh and Asadi, Mohsen and Hatala, Marek and Gasevic, Dragan and Bagheri, Ebrahim},
title = {Automated planning for feature model configuration based on stakeholders' business concerns},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100118},
doi = {10.1109/ASE.2011.6100118},
abstract = {In Software Product Line Engineering, concrete products of a family can be generated through a configuration process over a feature model. The configuration process selects features from the feature model according to the stakeholders' requirements. Selecting the right set of features for one product from all the available features in the feature model is a cumbersome task because 1) the stakeholders may have diverse business concerns and limited resources that they can spend on a product and 2) features may have negative and positive contributions on different business concern. Many configurations techniques have been proposed to facilitate software developers' tasks through automated product derivation. However, most of the current proposals for automatic configuration are not devised to cope with business oriented requirements and stakeholders' resource limitations. We propose a framework, which employs an artificial intelligence planning technique to automatically select suitable features that satisfy the stakeholders' business concerns and resource limitations. We also provide tooling support to facilitate the use of our framework.},
booktitle = {Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {536–539},
numpages = {4},
series = {ASE '11}
}

@inproceedings{10.1145/3584871.3584885,
author = {Malhotra, Ruchika and Chawla, Sonali and Sharma, Anjali},
title = {An Artificial Neural Network Model based on Binary Particle Swarm Optimization for enhancing the efficiency of Software Defect Prediction},
year = {2023},
isbn = {9781450398237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584871.3584885},
doi = {10.1145/3584871.3584885},
abstract = {With the rise in the growth of the software industry, it is essential to identify software defects in earlier stages to save costs and improve the efficiency of the software development lifecycle process. We have devised a hybrid software defect prediction (SDP) model that integrates Binary Particle Swarm Optimization (Binary PSO), Synthetic Minority Oversampling Technique (SMOTE), and Artificial Neural Network (ANN). BPSO is applied as a wrapper feature selection process utilizing AUC as a fitness function, SMOTE handles the dataset imbalance, and ANN is used as a classification algorithm for predicting software defects. We analyze the proposed BPSO-SMOTE-ANN model's predictive capability using the AUC and G-mean performance metrics. The proposed hybrid model is found helpful in predicting software defects. The statistical results suggest the enhanced performance of the proposed hybrid model concerning AUC and G-mean values. Also, the hybrid model was found to be competitive with other machine learning(ML) algorithms in determining software defects.},
booktitle = {Proceedings of the 2023 6th International Conference on Software Engineering and Information Management},
pages = {92–100},
numpages = {9},
keywords = {Software Defect Prediction, Search-based Techniques, SMOTE, Particle Swarm Optimization, Artificial Neural Networks},
location = {Palmerston North, New Zealand},
series = {ICSIM '23}
}

@inproceedings{10.1145/3671016.3671405,
author = {Fu, Ming Quan and Wei, Minjie and Qiao, Minglang and Ji, Peng and Deng, Zhihao and Cui, Di and Zhao, Yutong},
title = {HGNN4Perf: Detecting Performance Optimization Opportunities via Hypergraph Neural Network},
year = {2024},
isbn = {9798400707056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3671016.3671405},
doi = {10.1145/3671016.3671405},
abstract = {Performance optimization in software engineering is crucial for enhancing user satisfaction and maintaining a competitive advantage. Traditional methods for detecting performance issues – dynamic profiling and static analysis – often fall short in addressing complex dependencies within software architectures. This paper introduces the HyperGraph Neural Network (HGNN), a novel approach that leverages both static and dynamic program analysis to identify and prioritize performance bottlenecks effectively. By analyzing interconnected method call within fundamental patterns, HGNN and its enhanced version, HGNN+, utilize hypergraph neural network techniques to capture and learn dependency relationship features, significantly improving detection accuracy. Our initial testing on the three projects shows that HGNN+, especially when combined with several specific pre-trained models, presents satisfactory results compared to traditional methods.These findings underline HGNN+’s ability to manage complex dependencies and offer a scalable solution for software performance engineering. The benefits observed across multiple initial tests promise a broad applicability for the approach, setting a solid foundation for future research and expansion to more diverse datasets and neural network models, enhancing the reliability and effectiveness of performance optimization detection.},
booktitle = {Proceedings of the 15th Asia-Pacific Symposium on Internetware},
pages = {279–282},
numpages = {4},
keywords = {Graph Embedding, Performance Optimization, Software Architecture, Software Performance},
location = {Macau, China},
series = {Internetware '24}
}

@inproceedings{10.1145/3576915.3616583,
author = {Dittmer, Samuel and Eldefrawy, Karim and Graham-Lengrand, St\'{e}phane and Lu, Steve and Ostrovsky, Rafail and Pereira, Vitor},
title = {Boosting the Performance of High-Assurance Cryptography: Parallel Execution and Optimizing Memory Access in Formally-Verified Line-Point Zero-Knowledge},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3616583},
doi = {10.1145/3576915.3616583},
abstract = {Despite the notable advances in the development of high-assurance, verified implementations of cryptographic protocols, such implementations typically face significant performance overheads, particularly due to the penalties induced by formal verification and automated extraction of executable code. In this paper, we address some core performance challenges facing computer-aided cryptography by presenting a formal treatment for accelerating such verified implementations based on multiple generic optimizations covering parallelism and memory access. We illustrate our techniques for addressing such performance bottlenecks using the Line-Point Zero-Knowledge (LPZK) protocol as a case study. Our starting point is a new verified implementation of LPZK that we formalize and synthesize using EasyCrypt; our first implementation is developed to reduce the proof effort and without considering the performance of the extracted executable code. We then show how such (automatically) extracted code can be optimized in three different ways to obtain a 3000x speedup and thus matching the performance of the manual implementation of LPZK of lpzkv2.[13] We obtain such performance gains by first modifying the algorithmic specifications, then by adopting a provably secure parallel execution model, and finally by optimizing the memory access structures. All optimizations are first formally verified inside EasyCrypt, and then executable code is automatically synthesized from each step of the formalization. For each optimization, we analyze performance gains resulting from it and also address challenges facing the computer-aided security proofs thereof, and challenges facing automated synthesis of executable code with such an optimization.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2098–2112},
numpages = {15},
keywords = {code synthesis, formal verification, parallelism, verified implementation, verified optimizations, zero-knowledge},
location = {Copenhagen, Denmark},
series = {CCS '23}
}

@inproceedings{10.1145/2377836.2377842,
author = {Gamez, Nadia and Romero, Daniel and Fuentes, Lidia and Rouvoy, Romain and Duchien, Laurence},
title = {Constraint-based self-adaptation of wireless sensor networks},
year = {2012},
isbn = {9781450315661},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2377836.2377842},
doi = {10.1145/2377836.2377842},
abstract = {In recent years, the Wireless Sensor Networks (WSNs) have become a useful mechanism to monitor physical phenomena in environments. The sensors that make part of these long-lived networks have to be reconfigured according to context changes in order to preserve the operation of the network. Such reconfigurations require to consider the distributed nature of the sensor nodes as well as their resource scarceness. Therefore, self-adaptations for WSNs have special requirements comparing with traditional information systems. In particular, the reconfiguration of the WSN requires a trade-off between critical dimensions for this kind of networks and devices, such as resource consumption or reconfiguration cost. Thus, in this paper, we propose to exploit Constraint-Satisfaction Problem (CSP) techniques in order to find a suitable configuration for self-adapting WSNs, modelled using a Dynamic Software Product Line (DSPL), when the context changes. We exploit CSP modeling to find a compromise between contradictory dimensions. To illustrate our approach, we use an Intelligent Transportation System scenario. This case study enables us to show the advantages of obtaining suitable and optimized configurations for self-adapting WSNs.},
booktitle = {Proceedings of the 2nd International Workshop on Adaptive Services for the Future Internet and 6th International Workshop on Web APIs and Service Mashups},
pages = {20–27},
numpages = {8},
keywords = {wireless sensor networks, self-adaptation, dynamic software product lines, constraint-satisfaction problem},
location = {Bertinoro, Italy},
series = {WAS4FI-Mashups '12}
}

@inproceedings{10.1145/3510003.3510190,
author = {Randrianaina, Georges Aaron and T\"{e}rnava, Xhevahire and Khelladi, Djamel Eddine and Acher, Mathieu},
title = {On the benefits and limits of incremental build of software configurations: an exploratory study},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510190},
doi = {10.1145/3510003.3510190},
abstract = {Software projects use build systems to automate the compilation, testing, and continuous deployment of their software products. As software becomes increasingly configurable, the build of multiple configurations is a pressing need, but expensive and challenging to implement. The current state of practice is to build independently (a.k.a., clean build) a software for a subset of configurations. While incremental build has been studied for software evolution and relatively small changes of the source code, it has surprisingly not been considered for software configurations. In this exploratory study, we examine the benefits and limits of building software configurations incrementally, rather than always building them cleanly. By using five real-life configurable systems as subjects, we explore whether incremental build works, outperforms a sequence of clean builds, is correct w.r.t. clean build, and can be used to find an optimal ordering for building configurations. Our results show that incremental build is feasible in 100% of the times in four subjects and in 78% of the times in one subject. In average, 88.5% of the configurations could be built faster with incremental build while also finding several alternatives faster incremental builds. However, only 60% of faster incremental builds are correct. Still, when considering those correct incremental builds with clean builds, we could always find an optimal order that is faster than just a collection of clean builds with a gain up to 11.76%.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1584–1596},
numpages = {13},
keywords = {build systems, configurable software systems, configuration build},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3624007.3624060,
author = {Correa Restrepo, Camilo and Robin, Jacques and Mazo, Raul},
title = {Generating Constraint Programs for Variability Model Reasoning: A DSL and Solver-Agnostic Approach},
year = {2023},
isbn = {9798400704062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624007.3624060},
doi = {10.1145/3624007.3624060},
abstract = {Verifying and configuring large Software Product Lines (SPL) requires automation tools. Current state-of-the-art approaches involve translating variability models into a formalism accepted as input by a constraint solver. There are currently no standards for variability modeling languages (VML). There is also a variety of constraint solver input languages. This has resulted in a multiplication of ad-hoc architectures and tools specialized for a single pair of VML and solver, fragmenting the SPL community. To overcome this limitation, we propose a novel architecture based on model-driven code generation, where the syntax and semantics of VMLs can be declaratively specified as data, and a standard, human-readable, formal pivot language is used between the VML and the solver input language. This architecture is the first to be fully generic by being agnostic to both VML and the solver paradigm. To validate the genericity of the approach, we have implemented a prototype tool together with declarative specifications for the syntax and semantics of two different VMLs and two different solver families. One VML is for classic, static SPL, and the other for run-time reconfigurable dynamic SPL with soft constraints to be optimized during configuration. The two solver families are Constraint Satisfaction Programs (CSP) and Constraint Logic Programs (CLP).},
booktitle = {Proceedings of the 22nd ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {138–152},
numpages = {15},
keywords = {Software Product Lines, Generic Architecture, Configuration Automation, Automated Reasoning},
location = {Cascais, Portugal},
series = {GPCE 2023}
}

@proceedings{10.1145/3643667,
title = {Q-SE 2024: Proceedings of the 5th ACM/IEEE International Workshop on Quantum Software Engineering},
year = {2024},
isbn = {9798400705700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 5th International Workshop on Quantum Software Engineering (Q-SE 2024), co-located with ICSE 2024, provides a platform for researchers and practitioners to discuss challenges in developing quantum software in high-level quantum languages, novel solutions to build correct methods for testing quantum programs, executing quantum software, developing best practices, and creating a research roadmap of quantum software engineering.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/2593489.2593493,
author = {Beek, Maurice H. ter and de Vink, Erik P.},
title = {Using mCRL2 for the analysis of software product lines},
year = {2014},
isbn = {9781450328531},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593489.2593493},
doi = {10.1145/2593489.2593493},
abstract = {We show how the formal specification language mCRL2 and its state-of-the-art toolset can be used successfully to model and analyze variability in software product lines. The mCRL2 toolset supports parametrized modeling, model reduction and quality assurance techniques like model checking. We present a proof-of-concept, which moreover illustrates the use of data in mCRL2 and also how to exploit its data language to manage feature attributes of software product lines and quantitative constraints between attributes and features.},
booktitle = {Proceedings of the 2nd FME Workshop on Formal Methods in Software Engineering},
pages = {31–37},
numpages = {7},
keywords = {variability analysis, product lines, mCRL2, Model checking},
location = {Hyderabad, India},
series = {FormaliSE 2014}
}

@proceedings{10.1145/3571788,
title = {VaMoS '23: Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Odense, Denmark}
}

@inproceedings{10.1145/1147249.1147252,
author = {Kolb, Ronny and Muthig, Dirk},
title = {Making testing product lines more efficient by improving the testability of product line architectures},
year = {2006},
isbn = {1595934596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1147249.1147252},
doi = {10.1145/1147249.1147252},
abstract = {Product line engineering is a recent approach to software development that has shown to enable organizations to achieve significant reductions in development and maintenance cost as well as time-to-market of increasingly complex software systems. Yet, the testing process has not kept up with these reductions and the relative cost for testing product lines is actually becoming higher than in traditional single system development. Also, testing often cannot keep pace with accelerated development in product line engineering due to technical and organizational issues. This paper advocates that testing of product lines can be made more efficient and effective by considering testability already during architectural design. It explores the relationship between testability and product line architecture and discusses the importance of high testability for reducing product line testing effort and achieving required coverage criteria. The paper also outlines a systematic approach that will support product line organizations in improving and evaluating testability of product lines at the architectural level.},
booktitle = {Proceedings of the ISSTA 2006 Workshop on Role of Software Architecture for Testing and Analysis},
pages = {22–27},
numpages = {6},
keywords = {testing, testability, software product line, evaluation, design, architecture},
location = {Portland, Maine},
series = {ROSATEA '06}
}

@inproceedings{10.1145/2576768.2598305,
author = {Lopez-Herrejon, Roberto Erick and Javier Ferrer, Javier and Chicano, Francisco and Haslinger, Evelyn Nicole and Egyed, Alexander and Alba, Enrique},
title = {A parallel evolutionary algorithm for prioritized pairwise testing of software product lines},
year = {2014},
isbn = {9781450326629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2576768.2598305},
doi = {10.1145/2576768.2598305},
abstract = {Software Product Lines (SPLs) are families of related software systems, which provide different feature combinations. Different SPL testing approaches have been proposed. However, despite the extensive and successful use of evolutionary computation techniques for software testing, their application to SPL testing remains largely unexplored. In this paper we present the Parallel Prioritized product line Genetic Solver (PPGS), a parallel genetic algorithm for the generation of prioritized pairwise testing suites for SPLs. We perform an extensive and comprehensive analysis of PPGS with 235 feature models from a wide range of number of features and products, using 3 different priority assignment schemes and 5 product prioritization selection strategies. We also compare PPGS with the greedy algorithm prioritized-ICPL. Our study reveals that overall PPGS obtains smaller covering arrays with an acceptable performance difference with prioritized-ICPL.},
booktitle = {Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {1255–1262},
numpages = {8},
keywords = {software product lines, pairwise testing, feature models, combinatorial interaction testing},
location = {Vancouver, BC, Canada},
series = {GECCO '14}
}

@inproceedings{10.1145/3651640.3651647,
author = {Zhai, Yi and Hahn, Michael and Caggiano, Mario and Sax, Eric},
title = {Automotive Software Partitioning: A Production-Centric Perspective},
year = {2024},
isbn = {9798400708817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3651640.3651647},
doi = {10.1145/3651640.3651647},
abstract = {Partitioning the software artifacts is a powerful technique to enhance modularity and manage the complexity of software systems. In the automotive industry, software partitioning is commonly used to isolate vehicular functions from various domains or vendors to accelerate function integration processes. This paper proposes a novel perspective on software partitioning for automotive software systems and demonstrates a redundancy-based approach that isolates the production-relevant part from the monolithic in-car software. This strategy can aid in reducing issues caused by frequent software updates and improving car assembly efficiency by reducing in-line flashing time. The paper also presents an overview of the costs and drawbacks associated with the proposed software partitioning approach.},
booktitle = {Proceedings of the 4th European Symposium on Software Engineering},
pages = {23–30},
numpages = {8},
keywords = {automotive software architecture, model-based programming, software refactoring},
location = {Napoli, Italy},
series = {ESSE '23}
}

@inproceedings{10.1145/3705677.3705694,
author = {Cao, Yi and Yin, Xuan and Zhang, Jiangchao and Ge, Yimeng},
title = {Parts sorting and adjustment automatic production line control system based on PLC and WinCC},
year = {2025},
isbn = {9798400711848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3705677.3705694},
doi = {10.1145/3705677.3705694},
abstract = {The present study presents the design and implementation of a PLC and WinCC based automatic production line control system for parts sorting and adjustment. Four Siemens S7-1200 PLCs are utilized as the primary controllers, while a computer equipped with WinCC software serves as the supervisory control unit for system monitoring and operation. The system consists of four stations: Appearance defect detection station, height and weight detection station, direction adjustment station, and parts sorting station; each controlled by an individual PLC. It successfully incorporates features such as automatic/manual mode switching, emergency stop functionality, status display capabilities, and efficient human-machine interaction. Upon thorough verification testing, the system exhibits remarkable stability, user-friendly operation interface, and robust real-time performance.},
booktitle = {Proceedings of the 4th International Conference on Computer, Internet of Things and Control Engineering},
pages = {97–101},
numpages = {5},
keywords = {PLC, WinCC, automatic sorting, supervision control system},
location = {
},
series = {CITCE '24}
}

@inproceedings{10.1145/3180155.3180159,
author = {Krieter, Sebastian and Th\"{u}m, Thomas and Schulze, Sandro and Schr\"{o}ter, Reimar and Saake, Gunter},
title = {Propagating configuration decisions with modal implication graphs},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180159},
doi = {10.1145/3180155.3180159},
abstract = {Highly-configurable systems encompass thousands of interdependent configuration options, which require a non-trivial configuration process. Decision propagation enables a backtracking-free configuration process by computing values implied by user decisions. However, employing decision propagation for large-scale systems is a time-consuming task and, thus, can be a bottleneck in interactive configuration processes and analyses alike. We propose modal implication graphs to improve the performance of decision propagation by precomputing intermediate values used in the process. Our evaluation results show a significant improvement over state-of-the-art algorithms for 120 real-world systems.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {898–909},
numpages = {12},
keywords = {configuration, decision propagation, software product line},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/1967486.1967572,
author = {Souer, Jurriaan and Joor, Dirk-Jan},
title = {An approach to identify commonalities in web application engineering for a web content management system},
year = {2010},
isbn = {9781450304214},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1967486.1967572},
doi = {10.1145/1967486.1967572},
abstract = {The process of Web applications engineering can be complex and time consuming. We argue that Web engineering based on a standardized platform with reusable components is a logical next step in the evolution of Web application development. One popular platform to create Web applications is called a Web Content Management Systems (WCMS) which allows organizations to develop Web applications in a time and resource efficient way. This paper presents a method to identify software commonalities in WCMS-based Web applications to improve the software product for future implementations based on feature modeling and e-business models. The resulting method provides insight in relevant e-business models and their corresponding functionalities. Moreover, this paper shows how these commonalities can be identified and how that could influence the software product line. The approach has been applied in a practical case study of a series of Web application engineering projects within the publishing vertical market. We have validated the approach with experts within the case study company and found that the approach is useful in aiding requirements engineers in the Web application engineering process and product managers in the software product management process.},
booktitle = {Proceedings of the 12th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {558–565},
numpages = {8},
keywords = {web engineering, web content management system, software product lines},
location = {Paris, France},
series = {iiWAS '10}
}

@inproceedings{10.1145/3671016.3671382,
author = {Qu, Daohan and Zhao, Chaoyi and Jiang, Yanyan and Xu, Chang},
title = {Towards Life-long Software Self-validation in Production},
year = {2024},
isbn = {9798400707056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3671016.3671382},
doi = {10.1145/3671016.3671382},
abstract = {The increasing complexity of software and its execution environment makes in-house software testing challenging. Field testing, which conducts software testing in production environments, is a potential solution to this issue. However, existing field testing systems have not seen widespread use due to their inconvenience, lack of generality, and limited capabilities. We identify four essential requirements that a practical field testing system must fulfill: robust, efficient, handy, and versatile. This paper presents the design and implementation of Jaft, a field testing system for Java software meeting the aforementioned requirements through its design of field testing API, isolation mechanism, and runtime module. Evaluation results show that it has acceptable runtime overhead and can improve test effectiveness.},
booktitle = {Proceedings of the 15th Asia-Pacific Symposium on Internetware},
pages = {357–366},
numpages = {10},
keywords = {field failures, field testing, in-vivo testing},
location = {Macau, China},
series = {Internetware '24}
}

@inproceedings{10.1109/SC41406.2024.00033,
author = {Ryu, Donghyeon and Park, Chanik},
title = {Toward High-Performance Blockchain System by Blurring the Line between Ordering and Execution},
year = {2024},
isbn = {9798350352917},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC41406.2024.00033},
doi = {10.1109/SC41406.2024.00033},
abstract = {The primary bottleneck of blockchain is shifting from consensus to execution due to recent advances in DAGbased consensus algorithms supporting over 100k TPS. Many blockchain systems segregate execution from ordering, missing the opportunity to harness potential parallelism in consensus-produced batches.In this paper, we propose a new deterministically orderable concurrency control algorithm, OptME, which improves the performance of execution phase by exploiting inherent parallelism among transactions. This algorithm analyzes transaction dependencies to extract parallelism, and determines the total order of transaction execution. OptME consists of three steps: (1) building a transaction dependency graph, (2) generating a parallel execution schedule, and (3) executing transactions based on the schedule. We employ several optimizations, including parallel dependency graph construction, early abort detection, and efficient reordering with an optimistic assumption. Our evaluation demonstrates that OptME achieves up to 350k TPS and outperforms a state-of-the-art concurrency control algorithm, even under high contention scenarios.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {27},
numpages = {16},
keywords = {Blockchains, Concurrency control, Distributed databases, Scheduling algorithms, Smart contracts},
location = {Atlanta, GA, USA},
series = {SC '24}
}

@inproceedings{10.1145/1159733.1159762,
author = {Denger, Christian and Kolb, Ronny},
title = {Testing and inspecting reusable product line components: first empirical results},
year = {2006},
isbn = {1595932186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1159733.1159762},
doi = {10.1145/1159733.1159762},
abstract = {In recent years, product line development has increasingly received attention in industry as it enables software-developing organizations to reduce both cost and time of developing and maintaining increasingly complex systems as well as to address the demands for individually customized products. Successful product line development requires high quality of reusable artifacts in order to achieve the promised benefits. The unique issues of quality assurance in the context of systematic reuse, however, have not been quantitatively investigated so far. This paper describes a first empirical study comparing the two defect detection techniques, code inspections and functional testing, in the context of product line development. The primary goal of the study was to initially investigate the defect finding potential of the techniques on reusable software components with common and variant features. The major findings of the study are that the two techniques identified different types of defects on variants of a reusable component. Inspections are on average 66.39% more effective and need on average 36.84% less effort to detect a defect We found that both the testing and inspection techniques applied in the experiment were ineffective in identifying variant-specific defects. Overall, the results indicate that the standard quality assurance techniques seem to be insufficient to address special characteristics of reusable components.},
booktitle = {Proceedings of the 2006 ACM/IEEE International Symposium on Empirical Software Engineering},
pages = {184–193},
numpages = {10},
keywords = {software product line, reusable components, quality assurance, inspection, functional testing, controlled experiment},
location = {Rio de Janeiro, Brazil},
series = {ISESE '06}
}

@inproceedings{10.1145/3379337.3415832,
author = {McIntosh, Jess and Zajac, Hubert Dariusz and Stefan, Andreea Nicoleta and Bergstr\"{o}m, Joanna and Hornb\ae{}k, Kasper},
title = {Iteratively Adapting Avatars using Task-Integrated Optimisation},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415832},
doi = {10.1145/3379337.3415832},
abstract = {Virtual Reality allows users to embody avatars that do not match their real bodies. Earlier work has selected changes to the avatar arbitrarily and it therefore remains unclear how to change avatars to improve users' performance. We propose a systematic approach for iteratively adapting the avatar to perform better for a given task based on users' performance. The approach is evaluated in a target selection task, where the forearms of the avatar are scaled to improve performance. A comparison between the optimised and real arm lengths shows a significant reduction in average tapping time by 18.7%, for forearms multiplied in length by 5.6. Additionally, with the adapted avatar, participants moved their real body and arms significantly less, and subjective measures show reduced physical demand and frustration. In a second study, we modify finger lengths for a linear tapping task to achieve a better performing avatar, which demonstrates the generalisability of the approach.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {709–721},
numpages = {13},
keywords = {virtual reality, optimisation, avatar adaptation},
location = {Virtual Event, USA},
series = {UIST '20}
}

@article{10.1145/3678184,
author = {Lubinski, Thomas and Coffrin, Carleton and McGeoch, Catherine and Sathe, Pratik and Apanavicius, Joshua and Bernal Neira, David and Quantum Economic Development Consortium(QED-C) Collaboration},
title = {Optimization Applications as Quantum Performance Benchmarks},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3678184},
doi = {10.1145/3678184},
abstract = {Combinatorial optimization is anticipated to be one of the primary use cases for quantum computation in the coming years. The Quantum Approximate Optimization Algorithm&nbsp;and Quantum Annealing can potentially demonstrate significant run-time performance benefits over current state-of-the-art solutions. Inspired by existing methods to characterize classical optimization algorithms, we analyze the solution quality obtained by solving Max-cut problems using gate-model quantum devices and a quantum annealing device. This is used to guide the development of an advanced benchmarking framework for quantum computers designed to evaluate the trade-off between run-time execution performance and the solution quality for iterative hybrid quantum-classical applications. The framework generates performance profiles through compelling visualizations that show performance progression as a function of time for various problem sizes and illustrates algorithm limitations uncovered by the benchmarking approach. As an illustration, we explore the factors that influence quantum computing system throughput, using results obtained through execution on various quantum simulators and quantum hardware systems.},
journal = {ACM Transactions on Quantum Computing},
month = aug,
articleno = {18},
numpages = {44},
keywords = {Quantum Computing, Benchmarks, Benchmarking, Algorithms, Application Benchmarks, QAOA, Quantum Approximate Optimization Algorithm, Max-cut}
}

@article{10.1145/3649594,
author = {Liu, Fang and Fu, Zhiyi and Li, Ge and Jin, Zhi and Liu, Hui and Hao, Yiyang and Zhang, Li},
title = {Non-Autoregressive Line-Level Code Completion},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3649594},
doi = {10.1145/3649594},
abstract = {Software developers frequently use code completion tools to accelerate software development by suggesting the following code elements. Researchers usually employ AutoRegressive (AR) decoders to complete code sequences in a left-to-right, token-by-token fashion. To improve the accuracy and efficiency of code completion, we argue that tokens within a code statement have the potential to be predicted concurrently. In this article, we first conduct an empirical study to analyze the dependency among the target tokens in line-level code completion. The results suggest that it is potentially practical to generate all statement tokens in parallel. To this end, we introduce SANAR, a simple and effective syntax-aware non-autoregressive model for line-level code completion. To further improve the quality of the generated code, we propose an adaptive and syntax-aware sampling strategy to boost the model’s performance. The experimental results obtained from two widely used datasets indicate that our model outperforms state-of-the-art code completion approaches of similar model size by a considerable margin, and is faster than these models with up to 9\texttimes{} speed-up. Moreover, the extensive results additionally demonstrate that the enhancements achieved by SANAR become even more pronounced with larger model sizes, highlighting their significance.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {120},
numpages = {34},
keywords = {Code completion, neural networks, non-autoregressive generation}
}

@inproceedings{10.1145/3673791.3698423,
author = {Breuer, Timo},
title = {Data Fusion of Synthetic Query Variants With Generative Large Language Models},
year = {2024},
isbn = {9798400707247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3673791.3698423},
doi = {10.1145/3673791.3698423},
abstract = {Considering query variance in information retrieval (IR) experiments is beneficial for retrieval effectiveness. Especially ranking ensembles based on different topically related queries retrieve better results than rankings based on a single query alone. Recently, generative instruction-tuned Large Language Models (LLMs) improved on a variety of different tasks in capturing human language. To this end, this work explores the feasibility of using synthetic query variants generated by instruction-tuned LLMs in data fusion experiments. More specifically, we introduce a lightweight, unsupervised, and cost-efficient approach that exploits principled prompting and data fusion techniques. In our experiments, LLMs produce more effective queries when provided with additional context information on the topic. Furthermore, our analysis based on four TREC newswire benchmarks shows that data fusion based on synthetic query variants is significantly better than baselines with single queries and also outperforms pseudo-relevance feedback methods. We publicly share the code and query datasets with the community as resources for follow-up studies.},
booktitle = {Proceedings of the 2024 Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region},
pages = {274–279},
numpages = {6},
keywords = {data fusion, large language models, query variants},
location = {Tokyo, Japan},
series = {SIGIR-AP 2024}
}

@article{10.5555/3455716.3455819,
author = {Zhou, Dongruo and Xu, Pan and Gu, Quanquan},
title = {Stochastic nested variance reduction for nonconvex optimization},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {We study nonconvex optimization problems, where the objective function is either an average of n nonconvex functions or the expectation of some stochastic function. We propose a new stochastic gradient descent algorithm based on nested variance reduction, namely, Stochastic Nested Variance-Reduced Gradient descent (SNVRG). Compared with conventional stochastic variance reduced gradient (SVRG) algorithm that uses two reference points to construct a semi-stochastic gradient with diminishing variance in each iteration, our algorithm uses K + 1 nested reference points to build a semi-stochastic gradient to further reduce its variance in each iteration. For smooth nonconvex functions, SNVRG converges to an ε-approximate first-order stationary point within \~{O}(n∧ε-2+ε-3∧n1/2ε-2)1 number of stochastic gradient evaluations. This improves the best known gradient complexity of SVRG O(n+n2/3ε-2) and that of SCSG O(n∧ε-2+ε-10/3∧n2/3ε-2). For gradient dominated functions, SNVRG also achieves better gradient complexity than the state-of-the-art algorithms.Based on SNVRG, we further propose two algorithms that can find local minima faster than state-of-the-art algorithms in both finite-sum and general stochastic (online) nonconvex optimization. In particular, for finite-sum optimization problems, the proposed SNVRG + Neon2finite algorithm achieves \~{O}(n1/2ε-2 + nε-3H + n3/4ε-7/2H) gradient complexity to converge to an (ε, εH)-second-order stationary point, which outperforms SVRG+Neon2finite (Allen-Zhu and Li, 2018), the best existing algorithm, in a wide regime. For general stochastic optimization problems, the proposed SNVRG+Neon2online achieves \~{O}(ε-3 +ε-5H +ε-2ε-3H) gradient complexity, which is better than both SVRG+Neon2online (Allen-Zhu and Li, 2018) and Natasha2 (Allen-Zhu, 2018a) in certain regimes. Thorough experimental results on different nonconvex optimization problems back up our theory.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {103},
numpages = {63},
keywords = {variance reduction, finding local minima, nonconvex optimization}
}

@article{10.1145/3547140,
author = {Hirsch, Sharon and Novgorodov, Slava and Guy, Ido and Nus, Alexander},
title = {The Tip of the Buyer: Extracting Product Tips from Reviews},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3547140},
doi = {10.1145/3547140},
abstract = {Product reviews play a key role in e-commerce platforms. Studies show that many users read product reviews before a purchase and trust them to the same extent as personal recommendations. However, in many cases, the number of reviews per product is large and extracting useful information becomes a challenging task. Several websites have recently added an option to post tips—short, concise, practical, and self-contained pieces of advice about the products. These tips are complementary to the reviews and usually add a new non-trivial insight about the product, beyond its title, attributes, and description. Yet, most if not all major e-commerce platforms lack the notion of a tip as a first-class citizen and customers typically express their advice through other means, such as reviews. In this work, we propose an extractive method for tip generation from product reviews. We focus on five popular e-commerce domains whose reviews tend to contain useful non-trivial tips that are beneficial for potential customers. We formally define the task of tip extraction in e-commerce by providing the list of tip types, tip timing (before and/or after the purchase), and connection to the surrounding context sentences. To extract the tips, we propose a supervised approach and leverage a publicly available dataset, annotated by human editors, containing 14,000 product reviews. To demonstrate the potential of our approach, we compare different tip generation methods and evaluate them both manually and over the labeled set. Our approach demonstrates particularly high performance for popular products in the Baby, Home Improvement, and Sports &amp; Outdoors domains, with precision of over 95% for the top 3 tips per product. In addition, we evaluate the performance of our methods on previously unseen domains. Finally, we discuss the practical usage of our approach in real-world applications. Concretely, we explain how tips generated from user reviews can be integrated in various use cases within e-commerce platforms and benefit both buyers and sellers.},
journal = {ACM Trans. Internet Technol.},
month = feb,
articleno = {4},
numpages = {30},
keywords = {deep learning, machine learning, product reviews, tips generation, E-commerce}
}

@inproceedings{10.1145/3691620.3695268,
author = {Liao, Haoyu and Guo, Jianmei and Huang, Bo and Han, Yujie and Yang, Dingyu and Shi, Kai and Ding, Jonathan and Xu, Guoyao and Yang, Guodong and Zhang, Liping},
title = {DeployFix: Dynamic Repair of Software Deployment Failures via Constraint Solving},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695268},
doi = {10.1145/3691620.3695268},
abstract = {Software deployment misconfiguration often happens and has been one of the major causes of deployment failures that give rise to service interruptions. However, there is currently no existing approach to automatically repairing deployment failures. We propose DeployFix, which automatically repairs software deployment failures via constraint solving in the dynamic-changing deployment environments. DeployFix first defines DeployIR as a unified intermediate representation to achieve the translation of heterogeneous specifications from different schedulers with different syntaxes. By reducing the root-cause analysis of deployment failures to the conflict resolution in propositional logic, DeployFix uses off-the-shelf constraint solvers to achieve automatic localization and diagnosis of conflicting constraints, which are the root causes of deployment failures. DeployFix finally resolves the conflicting constraints and generates repaired deployment configurations in terms of practical requirements. We evaluate DeployFix in both simulation and production environments with tens of thousands of nodes at Alibaba, on which tens of thousands of applications are running guided by hundreds of thousands of deployment constraints. Experimental results demonstrate that DeployFix outperforms the state of the art and it correctly repairs the deployment failures in minutes, even in a large production data center.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {2053–2064},
numpages = {12},
keywords = {cloud computing, deployment failures, dynamic repair, constraint solving},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/1842752.1842812,
author = {Abbas, Nadeem and Andersson, Jesper and L\"{o}we, Welf},
title = {Autonomic Software Product Lines (ASPL)},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842812},
doi = {10.1145/1842752.1842812},
abstract = {We describe ongoing work on a variability mechanism for Autonomic Software Product Lines (ASPL). The autonomic software product lines have self-management characteristics that make product line instances more resilient to context changes and some aspects of product line evolution. Instances sense the context, selects and bind the best component variants to variation-points at run-time. The variability mechanism we describe is composed of a profile guided dispatch based on off-line and on-line training processes. Together they form a simple, yet powerful variability mechanism that continuously learns, which variants to bind given the current context and system goals.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {324–331},
numpages = {8},
keywords = {variation-points, variants, variability, on-line, off-line training, goals, context, autonomic elements, MAPE-K},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@inproceedings{10.1109/ICSE.2009.5070526,
author = {Th\"{u}m, Thomas and Batory, Don and K\"{a}stner, Christian},
title = {Reasoning about edits to feature models},
year = {2009},
isbn = {9781424434534},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSE.2009.5070526},
doi = {10.1109/ICSE.2009.5070526},
abstract = {Features express the variabilities and commonalities among programs in a software product line (SPL). A feature model defines the valid combinations of features, where each combination corresponds to a program in an SPL. SPLs and their feature models evolve over time. We classify the evolution of a feature model via modifications as refactorings, specializations, generalizations, or arbitrary edits. We present an algorithm to reason about feature model edits to help designers determine how the program membership of an SPL has changed. Our algorithm takes two feature models as input (before and after edit versions), where the set of features in both models are not necessarily the same, and it automatically computes the change classification. Our algorithm is able to give examples of added or deleted products and efficiently classifies edits to even large models that have thousands of features.},
booktitle = {Proceedings of the 31st International Conference on Software Engineering},
pages = {254–264},
numpages = {11},
series = {ICSE '09}
}

@inproceedings{10.5555/2662572.2662581,
author = {Sayyad, Abdel Salam and Ingram, Joseph and Menzies, Tim and Ammar, Hany},
title = {Optimum feature selection in software product lines: let your model and values guide your search},
year = {2013},
isbn = {9781467362849},
publisher = {IEEE Press},
abstract = {In Search-Based Software Engineering, well-known metaheuristic search algorithms are utilized to find solutions to common software engineering problems. The algorithms are usually taken "off the shelf" and applied with trust, i.e. software engineers are not concerned with the inner workings of algorithms, only with the results. While this may be sufficient is some domains, we argue against this approach, particularly where the complexity of the models and the variety of user preferences pose greater challenges to the metaheuristic search algorithms. We build on our previous investigation which uncovered the power of Indicator-Based Evolutionary Algorithm (IBEA) over traditionally-used algorithms (such as NSGA-II), and in this work we scrutinize the time behavior of user objectives subject to optimization. This analysis brings out the business perspective, previously veiled under Pareto-collective gauges such as Hypervolume and Spread. In addition, we show how slowing down the rates of crossover and mutation can help IBEA converge faster, as opposed to following the higher rates used in many other studies as "rules of thumb".},
booktitle = {Proceedings of the 1st International Workshop on Combining Modelling and Search-Based Software Engineering},
pages = {22–27},
numpages = {6},
keywords = {software product lines, search-based software engineering, optimal feature selection, multiobjective optimization, indicator-based evolutionary algorithm, feature models},
location = {San Francisco, California},
series = {CMSBSE '13}
}

@article{10.1145/3588935,
author = {Wang, Feiyu and Chen, Qizhi and Li, Yuanpeng and Yang, Tong and Tu, Yaofeng and Yu, Lian and Cui, Bin},
title = {JoinSketch: A Sketch Algorithm for Accurate and Unbiased Inner-Product Estimation},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588935},
doi = {10.1145/3588935},
abstract = {Inner-product estimation is the base of many important tasks in a variety of big data scenarios, including measuring similarity of streams in data stream processing, estimating join size in database, and analyzing cosine similarity in various applications. Sketch, as a class of probability algorithms, is promising in inner-product estimation. However, existing sketch solutions suffer from low accuracy due to their neglect of the high skewness of real data. In this paper, we design a new sketch algorithm for accurate and unbiased inner-product estimation, namely JoinSketch. To improve accuracy, JoinSketch consists of multiple components, and records items with different frequency in different components. We theoretically prove that JoinSketch is unbiased, and has lower variance compared with the well-known AGMS and Fast-AGMS sketch. The experimental results show that JoinSketch improves the accuracy by 10 times in average while maintaining a comparable speed. All code is open-sourced at Github.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {81},
numpages = {26},
keywords = {data streams, inner-product estimation, join cardinality, join size, sketch}
}

@inproceedings{10.1145/1842752.1842769,
author = {Weyns, Danny and Capilla, Rafael},
title = {Current and emerging topics in software architecture (ECSA 2010 Workshops Summary)},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842769},
doi = {10.1145/1842752.1842769},
abstract = {Since 2004 in St. Andrews (Scotland, U.K.), ECSA the European Conference on Software Architecture (formerly EWSA, the European Workshop on Software Architecture) has been considered as an important meeting point for researchers and practitioners on the topic of software architecture. ECSA has matured from a workshop format to a full software engineering conference in the subfield of software architecture.This year, ECSA has become more ambitious and expanded its scope and schedule up to four full days. The program includes a series of tutorials, a doctoral mentoring program, and four full-day workshops. New and existing software challenges have led to a variety of trends in software architecture research, which makes the conference and workshops more attractive and promotes the discussion on current and emerging topics.Based on the scientific and technical interest of the topics, the innovativeness of workshop topics, and the capacity of the conference workshop program, the workshop co-chairs selected four workshops from the nine submitted proposals. We summarize the aims and goals of each workshop and the contributions accepted for the four workshops:• 2nd International Workshop on Software Ecosystems (EcoSys). Piers Campbell, Faheem Ahmed, Jan Bosch, Sliger Jansen.• 1st International Workshop on Measurability of Security in Software Architectures (MeSSa). Reijo Savola, Teemu Kranst\'{e}n, Antti Evesti.• 8th Nordic Workshop on Model Driven Software Engineering (NW-MODE). Andrzej Wasowski, Dragos Truscan, Ludwik Kuzniarz.• 1st International Workshop on Variability in Software Product Line Architectures (VARI-ARCH). Alexander Helleboogh, Paris Avgeriou, Nelis Boucke, Patryck Heymans.The ECSA 2010 Workshop co-chairs would like to thanks all workshop organizers for their effort and enthusiasm to attract submission in different software architecture research topics and make the ECSA 2010 workshops a success.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {59–62},
numpages = {4},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@inproceedings{10.1145/3324989.3325716,
author = {Sakai, Yoshiyuki and Mendez, Sandra and Strandenes, H\r{a}kon and Ohlerich, Martin and Pasichnyk, Igor and Allalen, Momme and Manhart, Michael},
title = {Performance Optimisation of the Parallel CFD Code MGLET across Different HPC Platforms},
year = {2019},
isbn = {9781450367707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324989.3325716},
doi = {10.1145/3324989.3325716},
abstract = {This paper presents the optimisation techniques implemented to run across four HPC platforms the finite-volume computational fluid dynamics (CFD) code MGLET (Multi Grid Large Eddy Turbulence). We analysed and applied refactoring to the parallel communication routines, and reduced the memory footprint significantly, resulting in a substantial improvement of the parallel-scaling capability and in an increase of the maximum number of degrees of freedom for applications. Data structures and files layout were redesigned for implementing parallel I/O in HDF5. The new parallel I/O strategy results in a considerable increase in the average data transfer rate compared with the former serial implementation. An I/O pattern analysis and detailed I/O profiling of the new implementation were then conducted and further performance improvement was achieved by increasing the size of I/O requests and reducing the number of I/O processes. We compare the improved parallel-scaling capability of MGLET on different architectures using representative CFD application test cases.},
booktitle = {Proceedings of the Platform for Advanced Scientific Computing Conference},
articleno = {6},
numpages = {13},
keywords = {Parallel Optimization, Parallel I/O, High Performance Computing, CFD},
location = {Zurich, Switzerland},
series = {PASC '19}
}

@article{10.1145/3300148,
author = {Li, Miqing and Yao, Xin},
title = {Quality Evaluation of Solution Sets in Multiobjective Optimisation: A Survey},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3300148},
doi = {10.1145/3300148},
abstract = {Complexity and variety of modern multiobjective optimisation problems result in the emergence of numerous search techniques, from traditional mathematical programming to various randomised heuristics. A key issue raised consequently is how to evaluate and compare solution sets generated by these multiobjective search techniques. In this article, we provide a comprehensive review of solution set quality evaluation. Starting with an introduction of basic principles and concepts of set quality evaluation, this article summarises and categorises 100 state-of-the-art quality indicators, with the focus on what quality aspects these indicators reflect. This is accompanied in each category by detailed descriptions of several representative indicators and in-depth analyses of their strengths and weaknesses. Furthermore, issues regarding attributes that indicators possess and properties that indicators are desirable to have are discussed, in the hope of motivating researchers to look into these important issues when designing quality indicators and of encouraging practitioners to bear these issues in mind when selecting/using quality indicators. Finally, future trends and potential research directions in the area are suggested, together with some guidelines on these directions.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {26},
numpages = {38},
keywords = {performance assessment, multobjective optimisation, multi-criteria optimisation, metric, metaheuristic, measure, indicator, heuristic, exact method, evolutionary algorithms, Quality evaluation}
}

@article{10.1145/3661484,
author = {Kr\"{u}ger, Jacob and Li, Yi and Lossev, Kirill and Zhu, Chenguang and Chechik, Marsha and Berger, Thorsten and Rubin, Julia},
title = {A Meta-Study of Software-Change Intentions},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3661484},
doi = {10.1145/3661484},
abstract = {Every software system undergoes changes, for example, to add new features, fix bugs, or refactor code. The importance of understanding software changes has been widely recognized, resulting in various techniques and studies, for instance, on change-impact analysis or classifying developers’ activities. Since changes are triggered by developers’ intentions—something they plan or want to change in the system—many researchers have studied intentions behind changes. While there appears to be a consensus among software-engineering researchers and practitioners that knowing the intentions behind software changes is important, it is not clear how developers can actually benefit from this knowledge. In fact, there is no consolidated, recent overview of the state of the art on software-change intentions (SCIs) and their relevance for software engineering. We present a meta-study of 122 publications, which we used to derive a categorization of SCIs and to discuss motivations, evidence, and techniques relating to SCIs. Unfortunately, we found that individual pieces of research are often disconnected from each other, because a common understanding is missing. Similarly, some publications showcase the potential of knowing SCIs, but more substantial research to understand the practical benefits of knowing SCIs is needed. Our contributions can help researchers and practitioners improve their understanding of SCIs and how SCIs can aid software engineering tasks.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {300},
numpages = {41},
keywords = {Intentions, software evolution, change management, version control}
}

@article{10.1145/3583560,
author = {Agullo, Emmanuel and Buttari, Alfredo and Guermouche, Abdou and Herrmann, Julien and Jego, Antoine},
title = {Task-based Parallel Programming for Scalable Matrix Product Algorithms},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {2},
issn = {0098-3500},
url = {https://doi.org/10.1145/3583560},
doi = {10.1145/3583560},
abstract = {Task-based programming models have succeeded in gaining the interest of the high-performance mathematical software community because they relieve part of the burden of developing and implementing distributed-memory parallel algorithms in an efficient and portable way.In increasingly larger, more heterogeneous clusters of computers, these models appear as a way to maintain and enhance more complex algorithms. However, task-based programming models lack the flexibility and the features that are necessary to express in an elegant and compact way scalable algorithms that rely on advanced communication patterns. We show that the Sequential Task Flow paradigm can be extended to write compact yet efficient and scalable routines for linear algebra computations. Although, this work focuses on dense General Matrix Multiplication, the proposed features enable the implementation of more complex algorithms. We describe the implementation of these features and of the resulting GEMM operation. Finally, we present an experimental analysis on two homogeneous supercomputers showing that our approach is competitive up to 32,768 CPU cores with state-of-the-art libraries and may outperform them for some problem dimensions. Although our code can use GPUs straightforwardly, we do not deal with this case because it implies other issues which are out of the scope of this work.},
journal = {ACM Trans. Math. Softw.},
month = jun,
articleno = {15},
numpages = {23},
keywords = {sequential task flow, distributed memory parallelism, runtime systems, scalable linear algebra algorithms, Parallel programming models}
}

@inproceedings{10.1145/3543507.3583411,
author = {Qiao, Yimeng and Jing, Yinan and Zhang, Hanbing and He, Zhenying and Zhang, Kai and Wang, X. Sean},
title = {BlinkViz: Fast and Scalable Approximate Visualization on Very Large Datasets using Neural-Enhanced Mixed Sum-Product Networks},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583411},
doi = {10.1145/3543507.3583411},
abstract = {Web-based online interactive visual analytics enjoys popularity in recent years. Traditionally, visualizations are produced directly from querying the underlying data. However, for a very large dataset, this way is so time-consuming that it cannot meet the low-latency requirements of interactive visual analytics. In this paper, we propose a learning-based visualization approach called BlinkViz, which uses a learned model to produce approximate visualizations by leveraging mixed sum-product networks to learn the distribution of the original data. In such a way, it makes visualization faster and more scalable by decoupling visualization and data. In addition, to improve the accuracy of approximate visualizations, we propose an enhanced model by incorporating a neural network with residual structures, which can refine prediction results, especially for visual requests with low selectivity. Extensive experiments show that BlinkViz is extremely fast even on a large dataset with hundreds of millions of data records (over 30GB), responding in sub-seconds (from 2ms to less than 500ms for different requests) while keeping a low error rate. Furthermore, our approach remains scalable on latency and memory footprint size regardless of data size.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {1734–1742},
numpages = {9},
keywords = {approximation, neural networks, sum-product networks, visualization},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/1621607.1621633,
author = {Sanen, Frans and Truyen, Eddy and Joosen, Wouter},
title = {Mapping problem-space to solution-space features: a feature interaction approach},
year = {2009},
isbn = {9781605584942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1621607.1621633},
doi = {10.1145/1621607.1621633},
abstract = {Mapping problem-space features into solution-space features is a fundamental configuration problem in software product line engineering. A configuration problem is defined as generating the most optimal combination of software features given a requirements specification and given a set of configuration rules. Current approaches however provide little support for expressing complex configuration rules between problem and solution space that support incomplete requirements specifications. In this paper, we propose an approach to model complex configuration rules based on a generalization of the concept of problem-solution feature interactions. These are interactions between solution-space features that only arise in specific problem contexts. The use of an existing tool to support our approach is also discussed: we use the DLV answer set solver to express a particular configuration problem as a logic program whose answer set corresponds to the optimal combinations of solution-space features. We motivate and illustrate our approach with a case study in the field of managing dynamic adaptations in distributed software, where the goal is to generate an optimal protocol for accommodating a given adaptation.},
booktitle = {Proceedings of the Eighth International Conference on Generative Programming and Component Engineering},
pages = {167–176},
numpages = {10},
keywords = {software product line engineering, problem-solution feature interactions, distributed runtime adaptation, default logic, configuration knowledge, DLV},
location = {Denver, Colorado, USA},
series = {GPCE '09}
}

@article{10.1145/3408980,
author = {Licker, Nandor and Jones, Timothy M.},
title = {Duplo: a framework for OCaml post-link optimisation},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {ICFP},
url = {https://doi.org/10.1145/3408980},
doi = {10.1145/3408980},
abstract = {We present a novel framework, Duplo, for the low-level post-link optimisation of OCaml programs, achieving a speedup of 7% and a reduction of at least 15% of the code size of widely-used OCaml applications. Unlike existing post-link optimisers, which typically operate on target-specific machine code, our framework operates on a Low-Level Intermediate Representation (LLIR) capable of representing both the OCaml programs and any C dependencies they invoke through the foreign-function interface (FFI). LLIR is analysed, transformed and lowered to machine code by our post-link optimiser, LLIR-OPT. Most importantly, LLIR allows the optimiser to cross the OCaml-C language boundary, mitigating the overhead incurred by the FFI and enabling analyses and transformations in a previously unavailable context. The optimised IR is then lowered to amd64 machine code through the existing target-specific code generator of LLVM, modified to handle garbage collection just as effectively as the native OCaml backend. We equip our optimiser with a suite of SSA-based transformations and points-to analyses capable of capturing the semantics and representing the memory models of both languages, along with a cross-language inliner to embed C methods into OCaml callers. We evaluate the gains of our framework, which can be attributed to both our optimiser and the more sophisticated amd64 backend of LLVM, on a wide-range of widely-used OCaml applications, as well as an existing suite of micro- and macro-benchmarks used to track the performance of the OCaml compiler.},
journal = {Proc. ACM Program. Lang.},
month = aug,
articleno = {98},
numpages = {29},
keywords = {post-link, optimisation, inlining, OCaml, LLVM, C}
}

@inproceedings{10.1145/3372787.3389300,
author = {Gupta, Rajeev K and B, Balaji and V, Mekanathan and J, Ferose Khan},
title = {Challenges in scaling AI-powered distributed software product: a case study of a healthcare organization},
year = {2020},
isbn = {9781450370936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372787.3389300},
doi = {10.1145/3372787.3389300},
abstract = {Artificial intelligence (AI) is transforming care delivery and expanding precision medicine. This paper presents experiences of a 110-person, spread across three countries, involves multiple business units and external suppliers that successfully achieved multiple milestones. The product is an organization visionary software system, a mission-critical software system that conforms to stringent healthcare regulatory standards.We are practicing three styles of coordination practices that brings a solution to communication challenges. We are also describing our experiences of SAFe practices, Spotify like team culture, and 'Psychological Safety' that that helps in time-critical situations.The authors bring our experiences as a Program Manager, Project Manager, Quality Manager, and chief Architect who has been an integral part of the journey and establishing these practices over since the incubation stage of the referred product. These practices have helped to an extent where we have achieved regulatory acceptance and milestones successfully within aggressive time and taking steady steps towards where other business units are adopting our practices for managing multiple healthcare software systems.},
booktitle = {Proceedings of the 15th International Conference on Global Software Engineering},
pages = {6–10},
numpages = {5},
keywords = {spotify engineering culture, psychological safety, artificial intelligence, SAfe},
location = {Seoul, Republic of Korea},
series = {ICGSE '20}
}

@inproceedings{10.1145/3564719.3568689,
author = {Butting, Arvid and Kirchhof, J\"{o}rg Christian and Kleiss, Anno and Michael, Judith and Orlov, Radoslav and Rumpe, Bernhard},
title = {Model-Driven IoT App Stores: Deploying Customizable Software Products to Heterogeneous Devices},
year = {2022},
isbn = {9781450399203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564719.3568689},
doi = {10.1145/3564719.3568689},
abstract = {Internet of Things (IoT) devices and the software they execute are often strongly coupled with vendors preinstalling their software at the factory. Future IoT applications are expected to be distributed via app stores. A strong coupling between hard- and software hinders the rise of such app stores. Existing model-driven approaches for developing IoT applications focus largely on the behavior specification and message exchange but generate code that targets a specific set of devices. By raising the level of abstraction, models can be utilized to decouple hard- and software and adapt to various infrastructures. We present a concept for a model-driven app store that decouples hardware and software development of product lines of IoT applications.},
booktitle = {Proceedings of the 21st ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {108–121},
numpages = {14},
keywords = {Model-Driven Engineering, Low-Code, Internet of Things, Architecture Description Language, App Store},
location = {Auckland, New Zealand},
series = {GPCE 2022}
}

@inproceedings{10.1145/2162049.2162052,
author = {Brabrand, Claus and Ribeiro, M\'{a}rcio and Tol\^{e}do, T\'{a}rsis and Borba, Paulo},
title = {Intraprocedural dataflow analysis for software product lines},
year = {2012},
isbn = {9781450310925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2162049.2162052},
doi = {10.1145/2162049.2162052},
abstract = {Software product lines (SPLs) are commonly developed using annotative approaches such as conditional compilation that come with an inherent risk of constructing erroneous products. For this reason, it is essential to be able to analyze SPLs. However, as dataflow analysis techniques are not able to deal with SPLs, developers must generate and analyze all valid methods individually, which is expensive for non-trivial SPLs. In this paper, we demonstrate how to take any standard intraprocedural dataflow analysis and automatically turn it into a feature-sensitive dataflow analysis in three different ways. All are capable of analyzing all valid methods of an SPL without having to generate all of them explicitly. We have implemented all analyses as extensions of SOOT's intraprocedural dataflow analysis framework and experimentally evaluated their performance and memory characteristics on four qualitatively different SPLs. The results indicate that the feature-sensitive analyses are on average 5.6 times faster than the brute force approach on our SPLs, and that they have different time and space tradeoffs.},
booktitle = {Proceedings of the 11th Annual International Conference on Aspect-Oriented Software Development},
pages = {13–24},
numpages = {12},
keywords = {software product lines, dataflow analysis},
location = {Potsdam, Germany},
series = {AOSD '12}
}

@inproceedings{10.1145/3637543.3654759,
author = {Khalifa, Dorra Ben and Martel, Matthieu},
title = {Compile-Time Optimization of the Energy Consumption of Numerical Computations},
year = {2024},
isbn = {9798400704925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637543.3654759},
doi = {10.1145/3637543.3654759},
abstract = {Over the past decade, precision tuning has become one of the key techniques for achieving significant gains in performance and energy efficiency. This process consists of substituting smaller data types to the original data types assigned to floating-point variables in numerical programs in such a way that accuracy requirements remain fulfilled. In this article, we discuss the time and energy savings achieved using our precision tuning tool, POPiX. We validate our results on a set of numerical benchmarks covering various fields.},
booktitle = {Proceedings of the 21st ACM International Conference on Computing Frontiers: Workshops and Special Sessions},
pages = {5–7},
numpages = {3},
keywords = {Computer Arithmetic, Mixed Precision, Numerical Accuracy, Program Optimization},
location = {Ischia, Italy},
series = {CF '24 Companion}
}

@inproceedings{10.1145/3607947.3608061,
author = {Saravanan, T and Saravanakumar, S},
title = {Energy efficient optimization algorithms for MANET},
year = {2023},
isbn = {9798400700224},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607947.3608061},
doi = {10.1145/3607947.3608061},
abstract = {Mobile Ad Hoc Network is an infrastructure-less network which has a group of nodes that moves in a random manner. The optimal cluster head can reduce the efficiency with which a MANET operates, which is hampered by the network's dynamic structure. Through the use of multipath multicast routing, this method improves overall performance and guarantees QoS. To extend the life of MANETs that are the extremely energy-efficient dynamic swarming architecture approach is used. This protocol makes effective use of the limited energy source. The various network nodes that collect data and send it to its intended recipients. In order to accomplish power-efficient data transmission and node categorization, several nodes are merged and presented. The network nodes broadcast information that is used to determine which paths to take and how to take them. If you want your data packet transmission to be as efficient as possible on the network, you need to take into account the data delivery ratio. Better data delivery rates are achieved at the expense of lower energy costs and lower administrative burdens. In this paper, to achieve an improved data delivery rate, an Enhanced Monarch Butterfly Optimization Algorithm (EMBOA) for the route in MANET is proposed. Routing energy efficiency and load balancing via various optimization methods is the primary goal at first. Nodes from MANET's mobile component are used in a clustering process using a variety of optimization algorithms. Estimating residual energy and bandwidth helps with load balancing by increasing the data delivery ratio with minimal routing overhead. The EMBOA is then used to boost the efficiency and effectiveness of load balancing and multipath multicast routing in MANET, all while reducing energy usage. In terms of MANET routing performance, the suggested EMBOA is roughly 5% more effective than the current.},
booktitle = {Proceedings of the 2023 Fifteenth International Conference on Contemporary Computing},
pages = {572–579},
numpages = {8},
keywords = {Enhanced Monarch Butterfly Optimization Algorithm, Optimization},
location = {Noida, India},
series = {IC3-2023}
}

@inproceedings{10.1145/3659914.3659932,
author = {Garcia-Gasulla, Marta and Vinyals-Ylla-Catala, Joan and Romazanov, Juri and Baumann, Christoph and Matveev, Dmitry},
title = {Performance Analysis and Optimizations of ERO2.0 Fusion Code},
year = {2024},
isbn = {9798400706394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3659914.3659932},
doi = {10.1145/3659914.3659932},
abstract = {In this paper, we present a thorough performance analysis of a highly parallel Monte Carlo code for modeling global erosion and redeposition in fusion devices, ERO2.0. The study shows that the main bottleneck preventing the code from efficiently using the resources is the load imbalance at different levels. Load imbalance is inherent to the problem being solved, particle transport, and deposition. Based on the findings of the analysis, we also describe optimizations implemented on the code to improve its performance on HPC clusters. The proposed optimizations use MPI and OpenMP features, making them portable across architectures and achieving a 3.34x speedup with respect to the original code.},
booktitle = {Proceedings of the Platform for Advanced Scientific Computing Conference},
articleno = {18},
numpages = {11},
keywords = {performance, optimization, MPI, OpenMP, MonteCarlo, particle deposition},
location = {Zurich, Switzerland},
series = {PASC '24}
}

@inproceedings{10.1145/3613424.3614282,
author = {Rashidi, Bahador and Gao, Chao and Lu, Shan and Wang, Zhisheng and Zhou, Chunhua and Niu, Di and Sun, Fengyu},
title = {UNICO: Unified Hardware Software Co-Optimization for Robust Neural Network Acceleration},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614282},
doi = {10.1145/3613424.3614282},
abstract = {Specialized hardware has become an indispensable component to deep neural network (DNN) acceleration. To keep up with the rapid evolution of neural networks, holistic and automated solutions for jointly optimizing both hardware (HW) architectures and software (SW) mapping have been studied. These studies face two major challenges. First, the combined HW-SW design space is vast, which hinders the finding of optimal or near-optimal designs. This issue is exacerbated for industrial cases when cycle accurate models are used for design evaluation in the joint optimization. Second, HW design is prone to overfitting to the input DNNs used in the HW-SW co-optimization. To address these issues, in this paper, we propose UNICO, an efficient Unified Co-Optimization framework with a novel Robustness metric for better HW generalization. Guided by a high-fidelity surrogate model, UNICO employs multi-objective Bayesian optimization to effectively explore the HW design space, and conducts adaptive, parallel and scalable software mapping search based on successive halving. To reduce HW overfitting, we propose a HW robustness metric by relating a HW configuration’s quality to its sensitivity in software mapping search, and quantitatively incorporate this metric to search for more robust HW design(s). We implement UNICO in open source accelerator platform, and compare it with the state-of-the-art solution HASCO. Experiments show that UNICO significantly outperforms HASCO; it finds design(s) with similar quality to HASCO up to 4 \texttimes{} faster, and eventually converges to better and more robust designs. Finally, we deploy UNICO for optimizing an industrial accelerator, and show that it generates enhanced HW design(s) for key real-world DNNs.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {77–90},
numpages = {14},
keywords = {Neural Network Accelerator, Multi-Level Optimization, HW-SW Co-Design, HW Robustness},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@article{10.1145/3571853,
author = {Chen, Tao and Li, Miqing},
title = {Do Performance Aspirations Matter for Guiding Software Configuration Tuning? An Empirical Investigation under Dual Performance Objectives},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3571853},
doi = {10.1145/3571853},
abstract = {Configurable software systems can be tuned for better performance. Leveraging on some Pareto optimizers, recent work has shifted from tuning for a single, time-related performance objective to two intrinsically different objectives that assess distinct performance aspects of the system, each with varying aspirations to be satisfied, e.g., “the latency is less than 10s” while “the memory usage is no more than 1GB”. Before we design better optimizers, a crucial engineering decision to make therein is how to handle the performance requirements with clear aspirations in the tuning process. For this, the community takes two alternative optimization models: either quantifying and incorporating the aspirations into the search objectives that guide the tuning, or not considering the aspirations during the search but purely using them in the later decision-making process only. However, despite being a crucial decision that determines how an optimizer can be designed and tailored, there is a rather limited understanding of which optimization model should be chosen under what particular circumstance, and why.In this article, we seek to close this gap. Firstly, we do that through a review of over 426 articles in the literature and 14 real-world requirements datasets, from which we summarize four performance requirement patterns that quantify the aspirations in the configuration tuning. Drawing on these, we then conduct a comprehensive empirical study that covers 15 combinations of the state-of-the-art performance requirement patterns, four types of aspiration space, three Pareto optimizers, and eight real-world systems/environments, leading to 1,296 cases of investigation. Our findings reveal that (1) the realism of aspirations is the key factor that determines whether they should be used to guide the tuning; (2) the given patterns and the position of the realistic aspirations in the objective landscape are less important for the choice, but they do matter to the extents of improvement; (3) the available tuning budget can also influence the choice for unrealistic aspirations but it is insignificant under realistic ones. To promote open science practice, we make our code and dataset publicly available at: .},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {68},
numpages = {41},
keywords = {multi-objective optimization, performance aspiration, performance requirement, software configuration tuning, Search-Based Software Engineering}
}

@article{10.1145/3626104,
author = {Misitano, Giovanni},
title = {Exploring the Explainable Aspects and Performance of a Learnable Evolutionary Multiobjective Optimization Method},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
url = {https://doi.org/10.1145/3626104},
doi = {10.1145/3626104},
abstract = {Multiobjective optimization problems have multiple conflicting objective functions to be optimized simultaneously. The solutions to these problems are known as Pareto optimal solutions, which are mathematically incomparable. Thus, a decision maker must be employed to provide preferences to find the most preferred solution. However, decision makers often lack support in providing preferences and insights in exploring the solutions available.We explore the combination of learnable evolutionary models with interactive indicator-based evolutionary multiobjective optimization to create a learnable evolutionary multiobjective optimization method. Furthermore, we leverage interpretable machine learning to provide decision makers with potential insights about the problem being solved in the form of rule-based explanations. In fact, we show that a learnable evolutionary multiobjective optimization method can offer advantages in the search for solutions to a multiobjective optimization problem. We also provide an open source software framework for other researchers to implement and explore our ideas in their own works.Our work is a step toward establishing a new paradigm in the field on multiobjective optimization: explainable and learnable multiobjective optimization. We take the first steps toward this new research direction and provide other researchers and practitioners with necessary tools and ideas to further contribute to this field.},
journal = {ACM Trans. Evol. Learn. Optim.},
month = feb,
articleno = {4},
numpages = {39},
keywords = {Multiobjective optimization, evolutionary multiobjective optimization, learnable evolutionary models, explainable artificial intelligence}
}

@inproceedings{10.1145/3503470.3503476,
author = {Tsuji, Miwako and Min, Misun and Kerkemeier, Stefan and Fischer, Paul and Merzari, Elia and Sato, Mitsuhisa},
title = {Performance tuning of the Helmholtz matrix-vector product kernel in the computational fluid dynamics solver Nek5000/RS for the A64FX processor},
year = {2022},
isbn = {9781450395649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503470.3503476},
doi = {10.1145/3503470.3503476},
abstract = {Nek5000/RS is an open source computational fluid dynamics solver based on the spectral element method. One of the important kernel of the Nek5000/RS is called “axhelm”, which computes the Helmholtz matrix-vector product. In this paper, we have evaluated the axhelm kernel on the A64FX processor for the simplest case of polynomial degree N = 7. We have optimized the kernel for the A64FX processor by using well known optimization techniques such as SIMDization, software pipelining, continuous access enhancing, and software prefetch. We also provide the performance analysis data to investigate the effects of the optimization techniques to help understanding the A64FX processor and the Fujitsu compiler.},
booktitle = {International Conference on High Performance Computing in Asia-Pacific Region Workshops},
pages = {49–59},
numpages = {11},
keywords = {computational fluid dynamics, Performance optimization, Arm-based processor},
location = {Virtual Event, Japan},
series = {HPCAsia '22 Workshops}
}

@inproceedings{10.1145/2797433.2797460,
author = {Azzolini, Raphael P. and Rubira, Cec\'{\i}lia M. F. and Tizzei, Leonardo P. and Gaia, Felipe N. and Montecchi, Leonardo},
title = {Evolving a Software Products Line for E-commerce Systems: a Case Study},
year = {2015},
isbn = {9781450333931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797433.2797460},
doi = {10.1145/2797433.2797460},
abstract = {Software Product Lines engineering is a technique that explores systematic reuse of software artifacts in large scale to implement applications that share a common domain and have some customized features. For improving Product Line Architecture evolution, it is advisable to develop Software Product Lines using a modular structure. This demand can be satisfied by an aspect-oriented and component-based feature-architecture method that integrates components, aspects and variation point aspect-connectors. This approach allows minimization of feature scattering in the architectural model and supports modular modelling of crosscutting features. A case study mapping major features of significant e-commerce systems operating in Brazil and other countries was performed to evaluate this approach. The assessment of our solution was performed comparing its stability and modularity with other two approaches. Our results indicate that change impact in the architectural model is reduced when using our solution in the context of Software Product Lines evolution.},
booktitle = {Proceedings of the 2015 European Conference on Software Architecture Workshops},
articleno = {26},
numpages = {7},
keywords = {software evolution, software architecture stability, e-commerce, component-based development, aspect-oriented development, Software product lines},
location = {Dubrovnik, Cavtat, Croatia},
series = {ECSAW '15}
}

@inproceedings{10.5555/3586210.3586500,
author = {V\"{o}lker, Tobias and M\"{o}nch, Lars and Uzsoy, Reha},
title = {Impact of Production Planning Approaches on Wafer Fab Performance during Product Mix Changes},
year = {2023},
publisher = {IEEE Press},
abstract = {We present the results of a series of simulation experiments examining the impact of product mix changes on global performance measures such as costs and profit. In these experiments, we apply three production planning models in a rolling horizon setting that differ in their anticipation of shop-floor behavior. The first two are based on exogenous, i.e. fixed, workload-independent lead times, while the third uses non-linear clearing functions to represent workload-dependent lead times. The simulation results clearly demonstrate the benefit of production planning models that correctly anticipate the queueing behavior of the wafer fab.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3430–3441},
numpages = {12},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.1145/3239372.3239377,
author = {de Lara, Juan and Guerra, Esther and Chechik, Marsha and Salay, Rick},
title = {Model Transformation Product Lines},
year = {2018},
isbn = {9781450349499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239372.3239377},
doi = {10.1145/3239372.3239377},
abstract = {Model transformations enable automation in Model-Driven Engineering (MDE) and are key to its success. The emphasis of MDE on using domain-specific languages has caused a proliferation of meta-models, many of them capturing variants of base languages. In this scenario, developing a transformation for a new meta-model is usually performed manually with no reuse, even if comparable transformations for similar meta-models exist. This is a suboptimal process that precludes a wider adoption of MDE in industry.To improve this situation, we propose applying ideas from software product lines to transformation engineering. Our proposal enables the definition of meta-model product lines to capture the variability within a domain, on top of which transformations can be defined in a modular way. We call this construction transformation product line (TPL), and propose mechanisms for their construction, extension and analysis. TPLs are supported by a tool, Merlin, which is agnostic to the transformation language and lifts analyses based on model finding to the TPL. Finally, we report on an evaluation showing the benefits of building and analysing TPLs compared to building and analysing each individual transformation.},
booktitle = {Proceedings of the 21th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
pages = {67–77},
numpages = {11},
keywords = {Reusability, Product Lines, Model Transformations},
location = {Copenhagen, Denmark},
series = {MODELS '18}
}

@inproceedings{10.1145/3678610.3678619,
author = {Srinivas, Manjunath and Krishna Reddy, S Vamsi and N M, Manoj and Miyazawa, Hatsuho},
title = {Evaluation of ChatGPT, Gemini and Llama-2 for E-commerce Product Attribute Extraction},
year = {2024},
isbn = {9798400716799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678610.3678619},
doi = {10.1145/3678610.3678619},
abstract = {This research presents an exploration into utilizing advanced language models (LMs) for the extraction of product attributes on e-commerce platforms, focusing particularly on the Japanese market within the fashion genre. The investigation encompasses a variety of models, including the prominent OpenAI’s GPT-3.5 Turbo, Google’s Gemini Pro, and open-source alternatives like Elyza-7B, based on the Llama-2 architecture. The primary challenge addressed is the manual, time-intensive, and inconsistent process of product detail entry by merchants. By integrating LMs, the study aims to streamline attribute extraction, enhancing both efficiency and consistency across millions of product listings.A detailed comparative analysis assesses these models based on precision, infrastructure needs, and cost-effectiveness, highlighting the nuances of deploying proprietary versus open-source LMs for business applications. Specifically, the study evaluates the performance of these models across 16 fashion genres, offering insights into their practicality and execution requirements. Notably, the experiments demonstrate that ChatGPT, despite its higher expense compared to Google Gemini, delivers stable and well-structured outputs, which are indispensable for integration into downstream systems.The findings contribute to a broader understanding of the potential and limitations of LMs in automating e-commerce operations, suggesting that carefully selected LMs can significantly alleviate the burden of manual attribute entry. This work lays the groundwork for future research on the integration of language technologies in e-commerce and highlights the critical factors businesses must consider when adopting such solutions.},
booktitle = {Proceedings of the 2024 10th International Conference on E-Society, e-Learning and e-Technologies (ICSLT)},
pages = {43–48},
numpages = {6},
keywords = {Large Language Model, Open AI, GPT-3.5 Turbo, ChatGpt, Llama2, LangChain, Google Gemini Pro},
location = {
},
series = {ICSLT '24}
}

@inproceedings{10.1109/ICSE-NIER52604.2021.00026,
author = {Shahin, Ramy},
title = {Towards modal software engineering},
year = {2021},
isbn = {9780738133249},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER52604.2021.00026},
doi = {10.1109/ICSE-NIER52604.2021.00026},
abstract = {In this paper we introduce the notion of Modal Software Engineering: automatically turning sequential, deterministic programs into semantically equivalent programs efficiently operating on inputs coming from multiple overlapping worlds. We are drawing an analogy between modal logics, and software application domains where multiple sets of inputs (multiple worlds) need to be processed efficiently. Typically those sets highly overlap, so processing them independently would involve a lot of redundancy, resulting in lower performance, and in many cases intractability. Three application domains are presented: reasoning about feature-based variability of Software Product Lines (SPLs), probabilistic programming, and approximate programming.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {86–90},
numpages = {5},
location = {Virtual Event, Spain},
series = {ICSE-NIER '21}
}

@inproceedings{10.1145/3603287.3651199,
author = {Shatnawi, Hazim and Saquer, Jamil},
title = {Encoding Feature Models in Neo4j Graph Database},
year = {2024},
isbn = {9798400702372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603287.3651199},
doi = {10.1145/3603287.3651199},
abstract = {This study introduces an innovative approach to encoding and analyzing feature models within the Network Exploration and Optimization for Java (Neo4j) graph database, significantly enhancing the management of complex Software Product Lines (SPLs). We present a comparative analysis of traditional loading techniques against Neo4j's batch importer and the Awesome Procedures on Cypher (APOC) library, demonstrating the superior efficiency and effectiveness of our proposed methods, especially in handling large datasets. Our methodology extends beyond mere encoding; it capitalizes on Neo4j's Graph Data Science (GDS) library, employing Depth-First Search (DFS) and other advanced traversal techniques to navigate and manipulate these complex structures. The findings reveal not only a significant enhancement in the processing and analysis of feature models but also underscore the potential for more sophisticated SPL management strategies. By integrating innovative loading techniques, encoding strategies, and GDS traversal methods, this study lays a robust foundation for future advancements in the field.},
booktitle = {Proceedings of the 2024 ACM Southeast Conference},
pages = {157–166},
numpages = {10},
keywords = {Cypher, data science, feature model, graph data science library, graph traversals, load data in Neo4j, performance measurement},
location = {Marietta, GA, USA},
series = {ACMSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00177,
author = {Weber, Max and Kaltenecker, Christian and Sattler, Florian and Apel, Sven and Siegmund, Norbert},
title = {Twins or False Friends? A Study on Energy Consumption and Performance of Configurable Software},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00177},
doi = {10.1109/ICSE48619.2023.00177},
abstract = {Reducing energy consumption of software is an increasingly important objective, and there has been extensive research for data centers, smartphones, and embedded systems. However, when it comes to software, we lack working tools and methods to directly reduce energy consumption. For performance, we can resort to configuration options for tuning response time or throughput of a software system. For energy, it is still unclear whether the underlying assumption that runtime performance correlates with energy consumption holds, especially when it comes to optimization via configuration. To evaluate whether and to what extent this assumption is valid for configurable software systems, we conducted the largest empirical study of this kind to date. First, we searched the literature for reports on whether and why runtime performance correlates with energy consumption. We obtained a mixed, even contradicting picture from positive to negative correlation, and that configurability has not been considered yet as a factor for this variance. Second, we measured and analyzed both the runtime performance and energy consumption of 14 real-world software systems. We found that, in many cases, it depends on the software system's configuration whether runtime performance and energy consumption correlate and that, typically, only few configuration options influence the degree of correlation. A fine-grained analysis at the function level revealed that only few functions are relevant to obtain an accurate proxy for energy consumption and that, knowing them, allows one to infer individual transfer factors between runtime performance and energy consumption.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2098–2110},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3629527.3651419,
author = {Niewenhuis, Dante and Talluri, Sacheendra and Iosup, Alexandru and De Matteis, Tiziano},
title = {FootPrinter: Quantifying Data Center Carbon Footprint},
year = {2024},
isbn = {9798400704451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629527.3651419},
doi = {10.1145/3629527.3651419},
abstract = {Data centers have become an increasingly significant contributor to the global carbon footprint. In 2021, the global data center industry was responsible for around 1% of the worldwide greenhouse gas emissions. With more resource-intensive workloads, such as Large Language Models, gaining popularity, this percentage is expected to increase further. Therefore, it is crucial for data center service providers to become aware of and accountable for the sustainability impact of their design and operational choices. However, reducing the carbon footprint of data centers has been a challenging process due to the lack of comprehensive metrics, carbon-aware design tools, and guidelines for carbon-aware optimization. In this work, we propose FootPrinter, a first-of-its-kind tool that supports data center designers and operators in assessing the environmental impact of their data center. FootPrinter uses coarse-grained operational data, grid energy mix information, and discrete event simulation to determine the data center's operational carbon footprint and evaluate the impact of infrastructural or operational changes. FootPrinter can simulate days of operations of a regional data center on a commodity laptop in a few seconds, returning the estimated footprint with marginal error. By making this project open source, we hope to engage the community in the development of methodologies and tools for systematically assessing and exploring the sustainability of data centers.},
booktitle = {Companion of the 15th ACM/SPEC International Conference on Performance Engineering},
pages = {189–195},
numpages = {7},
keywords = {carbon emission, carbon footprint, data center, simulation},
location = {London, United Kingdom},
series = {ICPE '24 Companion}
}

@inproceedings{10.5555/2486788.2486853,
author = {Sayyad, Abdel Salam and Menzies, Tim and Ammar, Hany},
title = {On the value of user preferences in search-based software engineering: a case study in software product lines},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Software design is a process of trading off competing objectives. If the user objective space is rich, then we should use optimizers that can fully exploit that richness. For example, this study configures software product lines (expressed as feature maps) using various search-based software engineering methods. As we increase the number of optimization objectives, we find that methods in widespread use (e.g. NSGA-II, SPEA2) perform much worse than IBEA (Indicator-Based Evolutionary Algorithm). IBEA works best since it makes most use of user preference knowledge. Hence it does better on the standard measures (hypervolume and spread) but it also generates far more products with 0% violations of domain constraints. Our conclusion is that we need to change our methods for search-based software engineering, particularly when studying complex decision spaces.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {492–501},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/3605423.3605437,
author = {Thaller, Sebastian and Batalla Martinez, Sergi},
title = {Process-Mining Potentials Alongside the Product Development Process in the Auto-motive Supplier Industry},
year = {2023},
isbn = {9781450399579},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605423.3605437},
doi = {10.1145/3605423.3605437},
abstract = {This study analyzes the benefits and potentials of process mining for the automotive supplier industries product development processes. The exploration of process mining consists of developing an understanding of the process mining in relation to other scientific disciplines and the types of process mining. The types, benefits, potentials, and success factors of real-world process mining applications are considered and compared. By assessing the data structures and tables that contain historical information about business operations a framework for the practical application of process mining is formed. After extracting this data and subjecting it to clean up and transformation activities, it is analyzed using the software provider Celonis. In this analysis, assumptions about PMs value contribution and benefits for the product development process in the automotive supplier industry are made. Subsequently these assumptions are discussed in detail with a variety of experts in the field. This study concludes the value contribution of process mining and the potentials as well as derailing factors. The relevance of methods and business information system specific components are assessed as well.},
booktitle = {Proceedings of the 2023 9th International Conference on Computer Technology Applications},
pages = {233–240},
numpages = {8},
keywords = {Product Development Process, Process Mining, Celonis},
location = {Vienna, Austria},
series = {ICCTA '23}
}

@article{10.1109/TNET.2022.3192167,
author = {Majidi, Akbar and Gao, Xiaofeng and Zhu, Shunjia and Jahanbakhsh, Nazila and Zheng, Jiaqi and Chen, Guihai},
title = {MiFi: Bounded Update to Optimize Network Performance in Software-Defined Data Centers},
year = {2022},
issue_date = {Feb. 2023},
publisher = {IEEE Press},
volume = {31},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2022.3192167},
doi = {10.1109/TNET.2022.3192167},
abstract = {A controller needs to solve the multi-commodity flow problem and globally update the network under tight time constraints to maintain optimal network configurations. This centralized optimization in data centers involves many variables and constraints, which has a slow convergence speed and little scalability. In this paper, we propose MiFi, which aims to Minimize Flow cost or intuitively average transmission delay (delay or latency of flows), under reconfiguration budget constraints in data centers. Thus, we formulate this optimization problem as a constrained Markov Decision Process and propose a set of algorithms to solve it in a scalable manner. We first develop a propagation algorithm to identify the flows mostly affected in terms of latency and configuration in the next update. Then, we set a limitation range (the subset of switches requiring network updates) for updating them to improve adaptability and scalability by updating a less number of flows each time to achieve fast operations. Further, based on the Drift-Plus-Penalty method in Lyapunov theory, we propose a heuristic policy without prior information of flow demand and a renewal policy with a performance guarantee to minimize the additive optimality gap. To the best of our knowledge, MiFi is the first paper that studies the range and frequency of flow reconfigurations, which has both theoretical and practical significance in the area. Emulations and numerical simulations, which are much better than the estimated theoretical bound, show that MiFi outperforms the state of the art algorithms in terms of latency by over 45% while making improvements in adaptability and scalability.},
journal = {IEEE/ACM Trans. Netw.},
month = jul,
pages = {322–335},
numpages = {14}
}

@inproceedings{10.1145/3663529.3663836,
author = {Dunay, Omer and Cheng, Daniel and Tait, Adam and Thakkar, Parth and Rigby, Peter C. and Chiu, Andy and Ahmad, Imad and Ganesan, Arun and Maddila, Chandra and Murali, Vijayaraghavan and Tayyebi, Ali and Nagappan, Nachiappan},
title = {Multi-line AI-Assisted Code Authoring},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663836},
doi = {10.1145/3663529.3663836},
abstract = {CodeCompose is an AI-assisted code authoring tool powered by large language models (LLMs) that provides inline suggestions all developers at Meta. In this paper, we present how we scaled the product from displaying single-line suggestions to multi-line suggestions. This evolution required us to overcome several unique challenges in improving the usability of these suggestions for developers.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
First, we discuss how multi-line suggestions can have a "jarring" effect, as the LLM’s suggestions constantly move around the developer’s existing code, which would otherwise result in decreased productivity and satisfaction.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Second, multi-line suggestions take significantly longer to generate; hence we present several innovative investments we made to reduce the perceived latency for users. These model-hosting optimizations sped up multi-line suggestion latency by 2.5x.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Finally, we conduct experiments on 10’s of thousands of engineers to understand how multi-line suggestions impact the user experience and contrast this with single-line suggestions. Our experiments reveal that (i) multi-line suggestions account for 42% of total characters accepted (despite only accounting for 16% for dis- played suggestions) (ii) multi-line suggestions almost doubled the percentage of keystrokes saved for users from 9% to 17%. Multi-line CodeCompose has been rolled out to all engineers at Meta, and less than 1% of engineers have opted out of multi-line suggestions.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {150–160},
numpages = {11},
keywords = {AI, Developer productivity, LLM code authoring, Neural code completion, Program synthesis, Responsiveness, User experience},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@article{10.1145/3321551,
author = {Roberts, Stephen I. and Wright, Steven A. and Fahmy, Suhaib A. and Jarvis, Stephen A.},
title = {The Power-optimised Software Envelope},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1544-3566},
url = {https://doi.org/10.1145/3321551},
doi = {10.1145/3321551},
abstract = {Advances in processor design have delivered performance improvements for decades. As physical limits are reached, refinements to the same basic technologies are beginning to yield diminishing returns. Unsustainable increases in energy consumption are forcing hardware manufacturers to prioritise energy efficiency in their designs. Research suggests that software modifications may be needed to exploit the resulting improvements in current and future hardware. New tools are required to capitalise on this new class of optimisation.In this article, we present the Power Optimised Software Envelope (POSE) model, which allows developers to assess the potential benefits of power optimisation for their applications. The POSE model is metric agnostic and in this article, we provide derivations using the established Energy-Delay Product metric and the novel Energy-Delay Sum and Energy-Delay Distance metrics that we believe are more appropriate for energy-aware optimisation efforts. We demonstrate POSE on three platforms by studying the optimisation characteristics of applications from the Mantevo benchmark suite. Our results show that the Pathfinder application has very little scope for power optimisation while TeaLeaf has the most, with all other applications in the benchmark suite falling between the two.Finally, we extend our POSE model with a formulation known as System Summary POSE—a meta-heuristic that allows developers to assess the scope a system has for energy-aware software optimisation independent of the code being run.},
journal = {ACM Trans. Archit. Code Optim.},
month = jun,
articleno = {21},
numpages = {27},
keywords = {power optimisation, energy-aware computing, Energy-efficiency}
}

@inproceedings{10.1145/3671016.3674811,
author = {Chen, Yijie and Zhang, Qiyang and Xing, Ruolin and Li, Yuanzhe and Ma, Xiao and Yu, Chaoxin and Zhang, Yiran and Zhou, Ao and Wang, Shangguang},
title = {Energy-Aware Satellite-Ground Co-Inference via Layer-Wise Processing Schedule Optimization},
year = {2024},
isbn = {9798400707056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3671016.3674811},
doi = {10.1145/3671016.3674811},
abstract = {Recent advancements in Low Earth Orbit (LEO) satellites are facilitating the provision of Deep Neural Networks (DNNs)-inherent services to achieve ubiquitous coverage via satellite computing. However, the computational demands and energy consumption of DNN models pose significant challenges for satellite computing with limited power and computation resources. Based on the hierarchical characteristics of DNN models, we propose a satellite-ground co-inference strategy that executing certain layers on satellites and the remaining layers on ground servers. However, identifying the optimal layers for in-orbit processing with latency constraints is challenging due to the uncertain energy consumption across diverse models. To explore the correlation between energy consumption and layer types, we conduct comprehensive measurements on a hardware device commonly found in commercial LEO satellites and develop a layer-based energy consumption prediction model. Then, we formulate an optimization problem of minimizing the energy consumption on the satellite within the latency constraint as an integer nonlinear programming problem. Solving this problem is difficult due to combinatorial explosion in the discrete solution space. To address this, we propose an improved algorithm based on genetic algorithms. Using configurations from a real satellite, we conduct simulation experiments, concluding that our algorithm significantly improves energy savings by an average of 27 \texttimes{}.},
booktitle = {Proceedings of the 15th Asia-Pacific Symposium on Internetware},
pages = {303–312},
numpages = {10},
keywords = {Deep Neural Networks, LEO satellite, energy prediction, satellite computing, satellite-ground co-inference},
location = {Macau, China},
series = {Internetware '24}
}

@inproceedings{10.1145/3180155.3180163,
author = {Guo, Jianmei and Shi, Kai},
title = {To preserve or not to preserve invalid solutions in search-based software engineering: a case study in software product lines},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180163},
doi = {10.1145/3180155.3180163},
abstract = {Multi-objective evolutionary algorithms (MOEAs) have been successfully applied for software product lines (SPLs) to search for optimal or near-optimal solutions that balance multiple objectives. However, MOEAs usually produce invalid solutions that violate the constraints predefined. As invalid solutions are unbuildable in practice, we debate the preservation of invalid solutions during the search. We conduct experiments on seven real-world SPLs, including five largest SPLs hitherto reported and two SPLs with realistic values and constraints of quality attributes. We identify three potential limitations of preserving invalid solutions. Furthermore, based on the state-of-the-art, we design five algorithm variants that adopt different evolutionary operators. By performance evaluation, we provide empirical guidance on how to preserve valid solutions. Our empirical study demonstrates that whether or not to preserve invalid solutions deserves more attention in the community, and in some cases, we have to preserve valid solutions all along the way.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1027–1038},
numpages = {12},
keywords = {constraint solving, multi-objective evolutionary algorithms, search-based software engineering, software product lines, validity},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/1985793.1985851,
author = {Apel, Sven and Beyer, Dirk},
title = {Feature cohesion in software product lines: an exploratory study},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1985851},
doi = {10.1145/1985793.1985851},
abstract = {Software product lines gain momentum in research and industry. Many product-line approaches use features as a central abstraction mechanism. Feature-oriented software development aims at encapsulating features in cohesive units to support program comprehension, variability, and reuse. Surprisingly, not much is known about the characteristics of cohesion in feature-oriented product lines, although proper cohesion is of special interest in product-line engineering due to its focus on variability and reuse. To fill this gap, we conduct an exploratory study on forty software product lines of different sizes and domains. A distinguishing property of our approach is that we use both classic software measures and novel measures that are based on distances in clustering layouts, which can be used also for visual exploration of product-line architectures. This way, we can draw a holistic picture of feature cohesion. In our exploratory study, we found several interesting correlations (e.g., between development process and feature cohesion) and we discuss insights and perspectives of investigating feature cohesion (e.g., regarding feature interfaces and programming style).},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {421–430},
numpages = {10},
keywords = {visual clustering, software product lines, featurevisu, feature-oriented software development, feature cohesion},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@inproceedings{10.1145/3512290.3528853,
author = {Lehre, Per Kristian},
title = {Runtime analysis of competitive co-evolutionary algorithms for maximin optimisation of a bilinear function},
year = {2022},
isbn = {9781450392372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512290.3528853},
doi = {10.1145/3512290.3528853},
abstract = {Co-evolutionary algorithms have a wide range of applications, such as in hardware design, evolution of strategies for board games, and patching software bugs. However, these algorithms are poorly understood and applications are often limited by pathological behaviour, such as loss of gradient, relative over-generalisation, and mediocre objective stasis. It is an open challenge to develop a theory that can predict when co-evolutionary algorithms find solutions efficiently and reliably.This paper provides a first step in developing runtime analysis for population-based competitive co-evolutionary algorithms. We provide a mathematical framework for describing and reasoning about the performance of co-evolutionary processes. An example application of the framework shows a scenario where a simple coevolutionary algorithm obtains a solution in polynomial expected time. Finally, we describe settings where the co-evolutionary algorithm needs exponential time with overwhelmingly high probability to obtain a solution.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1408–1416},
numpages = {9},
keywords = {co-evolution, runtime analysis},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@article{10.1145/3618297,
author = {Kimiaei, Morteza and Neumaier, Arnold and Faramarzi, Parvaneh},
title = {New Subspace Method for Unconstrained Derivative-Free Optimization},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {4},
issn = {0098-3500},
url = {https://doi.org/10.1145/3618297},
doi = {10.1145/3618297},
abstract = {This article defines an efficient subspace method, called SSDFO, for unconstrained derivative-free optimization problems where the gradients of the objective function are Lipschitz continuous but only exact function values are available. SSDFO employs line searches along directions constructed on the basis of quadratic models. These approximate the objective function in a subspace spanned by some previous search directions. A worst-case complexity bound on the number of iterations and function evaluations is derived for a basic algorithm using this technique. Numerical results for a practical variant with additional heuristic features show that, on the unconstrained CUTEst test problems, SSDFO has superior performance compared to the best solvers from the literature.},
journal = {ACM Trans. Math. Softw.},
month = dec,
articleno = {39},
numpages = {28},
keywords = {complexity, line search approach, subspace technique, Unconstrained derivative-free optimization}
}

@article{10.1145/3643751,
author = {Chen, Tao and Li, Miqing},
title = {Adapting Multi-objectivized Software Configuration Tuning},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643751},
doi = {10.1145/3643751},
abstract = {When tuning software configuration for better performance (e.g., latency or throughput), an important issue that many optimizers face is the presence of local optimum traps, compounded by a highly rugged configuration landscape and expensive measurements. To mitigate these issues, a recent effort has shifted to focus on the level of optimization model (called meta multi-objectivization or MMO) instead of designing better optimizers as in traditional methods. This is done by using an auxiliary performance objective, together with the target performance objective, to help the search jump out of local optima. While effective, MMO needs a fixed weight to balance the two objectives—a parameter that has been found to be crucial as there is a large deviation of the performance between the best and the other settings. However, given the variety of configurable software systems, the “sweet spot” of the weight can vary dramatically in different cases and it is not possible to find the right setting without time-consuming trial and error. In this paper, we seek to overcome this significant shortcoming of MMO by proposing a weight adaptation method, dubbed AdMMO. Our key idea is to adaptively adjust the weight at the right time during tuning, such that a good proportion of the nondominated configurations can be maintained. Moreover, we design a partial duplicate retention mechanism to handle the issue of too many duplicate configurations without losing the rich information provided by the “good” duplicates. 
 
 
 
 
 
 
 
Experiments on several real-world systems, objectives, and budgets show that, for 71% of the cases, AdMMO is considerably superior to MMO and a wide range of state-of-the-art optimizers while achieving generally better efficiency with the best speedup between 2.2x and 20x.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {25},
numpages = {23},
keywords = {Configuration tuning, multi-objectivization, performance optimization, search-based software engineering}
}

@inproceedings{10.5555/3314872.3314888,
author = {Mururu, Girish and Gavrilovska, Ada and Pande, Santosh},
title = {Quantifying and reducing execution variance in STM via model driven commit optimization},
year = {2019},
isbn = {9781728114361},
publisher = {IEEE Press},
abstract = {Simplified parallel programming coupled with an ability to express speculative computation is realized with Software Transactional Memory (STM). Although STMs are gaining popularity because of significant improvements in parallel performance, they exhibit enormous variation in transaction execution with non-repeatable performance behavior which is unacceptable in many application domains, especially in which frame rates and responsiveness should be predictable.In other domains reproducible transactional behavior helps towards system provisioning. Thus, reducing execution variance in STM is an important performance goal that has been mostly overlooked. In this work, we minimize the variance in execution time of threads in STM by reducing non-determinism exhibited due to speculation. We define the state of STM, and we use it to first quantify non-determinism and then generate an automaton that models the execution behavior of threads in STM. We finally use the state automaton to guide the STM to avoid non-predictable transactional behavior thus reducing non-determinism in rollbacks which in turn results in reduction in variance. We observed average reduction of variance in execution time of threads up to 74% in 16 cores and 53% in 8 cores by reducing non-determinism up to 24% in 16 cores and 44% in 8 cores, respectively, on STAMP benchmark suite while experiencing average slowdown of 4.8% in 8 cores and 19.2% in 16 cores. We also reduced the variance in frame rate by maximum of 65% on a version of real world game Quake3 without degradation in timing.},
booktitle = {Proceedings of the 2019 IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {109–121},
numpages = {13},
keywords = {Software-transactional memory, Quake3, Execution Variance},
location = {Washington, DC, USA},
series = {CGO 2019}
}

@inproceedings{10.5555/3643142.3643309,
author = {Rahman, S M Atikur and Rahman, Md Fashiar and Tseng, Tzu-Liang (Bill) and Kamal, Tamanna},
title = {A Simulation-Based Approach for Line Balancing under Demand Uncertainty in Production Environment},
year = {2024},
isbn = {9798350369663},
publisher = {IEEE Press},
abstract = {The management of the production line is a challenging task due to the high level of uncertainty in demand, which can lead to unbalanced utilization of resources. This may result in a potential deterioration of management satisfaction in terms of cost-effectiveness. Therefore, it requires efficient tools to optimize resource utilization. With such inherent needs, this paper presents a simulation-based decision support framework for garments industries. The Discrete Event Simulation (DES) is used to model different scenarios for the operational processes. The procedure focuses on the line balancing technique, which aims to eliminate bottlenecks and optimize the production process by balancing the workload. The results of this study demonstrate the effectiveness of the line balancing technique in improving line efficiency, reducing the idle time of the operators, and increasing productivity. The simulation was developed using AnyLogic simulation software. The outcome of the process is thoroughly evaluated and justified using a case study.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2020–2030},
numpages = {11},
location = {San Antonio, Texas, USA},
series = {WSC '23}
}

@inproceedings{10.5555/3712729.3712868,
author = {Bokor, Balwin and Seiringer, Wolfgang and Klaus, Altendorfer and Thomas, Felberbauer},
title = {Energy Price and Workload Related Dispatching Rule: Balancing Energy and Production Logistics Costs},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {In response to the escalating need for sustainable manufacturing practices amid fluctuating energy prices, this study introduces a novel dispatching rule that integrates energy price and workload considerations with Material Requirement Planning (MRP) to optimize production logistics and energy costs. The dispatching rule effectively adjusts machine operational states, i.e. turn the machine on or off, based on current energy prices and workload. By developing a stochastic multi-item multi-stage job shop simulation model, this research evaluates the performance of the dispatching rule through a comprehensive full-factorial simulation. Findings indicate a significant enhancement in shop floor decision-making through reduced overall costs. Moreover, the analysis of the Pareto front reveals trade-offs between minimizing energy and production logistics costs, aiding decision-makers in selecting optimal configurations.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1681–1692},
numpages = {12},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@article{10.1145/3664600,
author = {Madampe, Kashumi and Hoda, Rashina and Grundy, John},
title = {Supporting Emotional Intelligence, Productivity and Team Goals while Handling Software Requirements Changes},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3664600},
doi = {10.1145/3664600},
abstract = {Background: Research shows that emotional intelligence (EI) should be used alongside cognitive intelligence during requirements change (RC) handling in Software Engineering (SE), especially in agile settings. Objective: We wanted to study the role of EI in-depth during RC handling. Method: We conducted a mixed-methods study (an interview study followed by a survey study) with 124 software practitioners. Findings: We found the causal condition, intervening condition and causes lead to key direct consequences of regulating own emotions, managing relationships, and extended consequences of sustaining productivity, setting and sustaining team goals. We found several strategies of supporting EI during RC handling. Further, we found strong correlations between six strategies and one being aware of own emotions, regulating own emotions, sustaining team productivity, and setting and sustaining team goals. Conclusion: Empathising with others and tracking commitments and decisions as a team are key strategies that have strong correlations between managing emotions, between sustaining team productivity, and between setting and sustaining team goals. To the best of our knowledge, the framework we present in this paper is the first theoretical framework on EI in SE research. We provide recommendations for software practitioners to consider during RC handling.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {153},
numpages = {38},
keywords = {Emotions, emotional intelligence, affects, requirements, changes, human factors, software engineering, software teams, socio-technical grounded theory, agile, well-being, workplace awareness, productivity, team goals}
}

@article{10.1162/evco_a_00269,
author = {Li, Miqing and Yao, Xin},
title = {What Weights Work for You? Adapting Weights for Any Pareto Front Shape in Decomposition-Based Evolutionary Multiobjective Optimisation},
year = {2020},
issue_date = {Summer 2020},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {28},
number = {2},
issn = {1063-6560},
url = {https://doi.org/10.1162/evco_a_00269},
doi = {10.1162/evco_a_00269},
abstract = {The quality of solution sets generated by decomposition-based evolutionary multi-objective optimisation (EMO) algorithms depends heavily on the consistency between a given problem's Pareto front shape and the specified weights' distribution. A set of weights distributed uniformly in a simplex often leads to a set of well-distributed solutions on a Pareto front with a simplex-like shape, but may fail on other Pareto front shapes. It is an open problem on how to specify a set of appropriate weights without the information of the problem's Pareto front beforehand. In this article, we propose an approach to adapt weights during the evolutionary process (called AdaW). AdaW progressively seeks a suitable distribution of weights for the given problem by elaborating several key parts in weight adaptation—weight generation, weight addition, weight deletion, and weight update frequency. Experimental results have shown the effectiveness of the proposed approach. AdaW works well for Pareto fronts with very different shapes: 1) the simplex-like, 2) the inverted simplex-like, 3) the highly nonlinear, 4) the disconnect, 5) the degenerate, 6) the scaled, and 7) the high-dimensional.},
journal = {Evol. Comput.},
month = jun,
pages = {227–253},
numpages = {27},
keywords = {weight adaptation., decomposition-based EMO, evolutionary algorithms, many-objective optimisation, Multiobjective optimisation}
}

@article{10.1145/3542823,
author = {Rainford, Penny Faulkner and Porter, Barry},
title = {Code and Data Synthesis for Genetic Improvement in Emergent Software Systems},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1145/3542823},
doi = {10.1145/3542823},
abstract = {Emergent software systems are assembled from a collection of small code blocks, where some of those blocks have alternative implementation variants; they optimise at run-time by learning which compositions of alternative blocks best suit each deployment environment encountered.In this paper we study the automated synthesis of new implementation variants for a running system using genetic improvement (GI). Typical GI approaches, however, rely on large amounts of data for accurate training and large code bases from which to source genetic material. In emergent systems we have neither asset, with sparsely sampled runtime data and small code volumes in each building block.We therefore examine two approaches to more effective GI under these constraints: the synthesis of data from sparse samples to construct statistically representative larger training corpora; and the synthesis of code to counter the relative lack of genetic material in our starting population members.Our results demonstrate that a mixture of synthesised and existing code is a viable optimisation strategy, and that phases of increased synthesis can make GI more robust to deleterious mutations. On synthesised data, we find that we can produce equivalent optimisation compared to GI methods using larger data sets, and that this optimisation can produce both useful specialists and generalists.},
journal = {ACM Trans. Evol. Learn. Optim.},
month = aug,
articleno = {7},
numpages = {35},
keywords = {Genetic improvement, optimization, emergent systems, data synthesis, data sampling, fitness function, language}
}

@article{10.1145/3672461,
author = {Ghammam, Anwar and Khalsi, Rania and Kessentini, Marouane and Hassan, Foyzul},
title = {Efficient Management of Containers for Software Defined Vehicles},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {8},
issn = {1049-331X},
url = {https://doi.org/10.1145/3672461},
doi = {10.1145/3672461},
abstract = {Containerization technology, such as Docker, is gaining in popularity in newly established software-defined vehicle architectures (SDVA). However, executing those containers can quickly become computationally expensive in constrained environments, given the limited CPU, memory, and energy resources in the Electric Control Units (ECU) of SDVA. Consequently, the efficient management of these containers is crucial for enabling the on-demand usage of the applications in the vehicle based on the available resources while considering several constraints and priorities, including failure tolerance, security, safety, and comfort. In this article, we propose a dynamic software container management approach for constrained environments such as embedded devices/ECUs in SDVA within smart cars. To address the conflicting objectives and constraints within the vehicle, we design a novel search-based approach based on multi-objective optimization. This approach facilitates the allocation, movement, or suspension of containers between ECUs in the cluster. Collaborating with our industry partner, Ford Motor Company, we evaluate our approach using different real-world software-defined scenarios. These scenarios involve using heterogeneous clusters of ECU devices in vehicles based on real-world software containers and use-case studies from the automotive industry. The experimental results demonstrate that our scheduler outperforms existing scheduling algorithms, including the default Docker scheduler -Spread- commonly used in automotive applications. Our proposed scheduler exhibits superior performance in terms of energy and resource cost efficiency. Specifically, it achieves a 35% reduction in energy consumption in power-saving mode compared to the scheduler employed by Ford Motor Company. Additionally, our scheduler effectively distributes workload among the ECUs in the cluster, minimizing resource usage, and dynamically adjusts to the real-time requirements and constraints of the car environment. This work will serve as a fundamental building block in the automotive industry to efficiently manage software containers in smart vehicles, considering constraints and priorities in the real world.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {197},
numpages = {36},
keywords = {Dynamic scheduling, docker, management of software containers, many-objective optimization, software defined vehicles}
}

@inproceedings{10.1145/2695664.2695743,
author = {Garcia, Cleiton and Paludo, Marco and Malucelli, Andreia and Reinehr, Sheila},
title = {A software process line for service-oriented applications},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695743},
doi = {10.1145/2695664.2695743},
abstract = {The management of processes and systems is a complex and time-consuming activity for organizations and also an ongoing Information Technology (IT) challenge. Among the different approaches for bringing flexibility to the business processes and systems are Service-Oriented Architecture (SOA) and Business Process Management (BPM). The SOA approach has become popular providing services and interfaces, enabling integration of heterogeneous and distributed platforms and BPM leverages the cycles of improvements, control and evaluation of business processes. BPM and SOA should work together aiming at improving business processes and evolving systems architecture. One main problem to apply BPM and SOA is the lack of established processes and this work proposes a software process line in order to simplify variability control and enable the instantiation of new development process applying BPM and SOA. It also aims at developing an environment to support the proposed software process line in order to automate the process, integrating industrial tools with one specifically developed to perform the transformation of UMA models into BPM notation. The main contribution of this work is the definition of the software process line for engineering service-oriented products. It is highly relevant to software industry since software process lines lacks experiments, practices and tools.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1680–1687},
numpages = {8},
keywords = {web-based services, software process lines, SPL, SOA, BPM},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/3106237.3106273,
author = {Oh, Jeho and Batory, Don and Myers, Margaret and Siegmund, Norbert},
title = {Finding near-optimal configurations in product lines by random sampling},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106273},
doi = {10.1145/3106237.3106273},
abstract = {Software Product Lines (SPLs) are highly configurable systems. This raises the challenge to find optimal performing configurations for an anticipated workload. As SPL configuration spaces are huge, it is infeasible to benchmark all configurations to find an optimal one. Prior work focused on building performance models to predict and optimize SPL configurations. Instead, we randomly sample and recursively search a configuration space directly to find near-optimal configurations without constructing a prediction model. Our algorithms are simpler and have higher accuracy and efficiency.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {61–71},
numpages = {11},
keywords = {software product lines, searching configuration spaces, finding optimal configurations},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/2658761.2658767,
author = {Ruprecht, Andreas and Heinloth, Bernhard and Lohmann, Daniel},
title = {Automatic feature selection in large-scale system-software product lines},
year = {2014},
isbn = {9781450331616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658761.2658767},
doi = {10.1145/2658761.2658767},
abstract = {System software can typically be configured at compile time via a comfortable feature-based interface to tailor its functionality towards a specific use case. However, with the growing number of features, this tailoring process becomes increasingly difficult: As a prominent example, the Linux kernel in v3.14 provides nearly 14 000 configuration options to choose from. Even developers of embedded systems refrain from trying to build a minimized distinctive kernel configuration for their device – and thereby waste memory and money for unneeded functionality. In this paper, we present an approach for the automatic use-case specific tailoring of system software for special-purpose embedded systems. We evaluate the effectiveness of our approach on the example of Linux by generating tailored kernels for well-known applications of the Rasperry Pi and a Google Nexus 4 smartphone. Compared to the original configurations, our approach leads to memory savings of 15–70 percent and requires only very little manual intervention.},
booktitle = {Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences},
pages = {39–48},
numpages = {10},
keywords = {Software Tailoring, Software Product Lines, Linux, Feature Selection},
location = {V\"{a}ster\r{a}s, Sweden},
series = {GPCE 2014}
}

@article{10.1145/3342104,
author = {Lucambio P\'{e}rez, L. R. and Prudente, L. F.},
title = {A Wolfe Line Search Algorithm for Vector Optimization},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {4},
issn = {0098-3500},
url = {https://doi.org/10.1145/3342104},
doi = {10.1145/3342104},
abstract = {In a recent article, Lucambio P\'{e}rez and Prudente extended the Wolfe conditions for the vector-valued optimization. Here, we propose a line search algorithm for finding a step size satisfying the strong Wolfe conditions in the vector optimization setting. Well definedness and finite termination results are provided. We discuss practical aspects related to the algorithm and present some numerical experiments illustrating its applicability. Codes supporting this article are written in Fortran 90 and are freely available for download.},
journal = {ACM Trans. Math. Softw.},
month = dec,
articleno = {37},
numpages = {23},
keywords = {Line search algorithm, Wolfe conditions, vector optimization}
}

@inproceedings{10.1145/2993236.2993249,
author = {Pereira, Juliana Alves and Matuszyk, Pawel and Krieter, Sebastian and Spiliopoulou, Myra and Saake, Gunter},
title = {A feature-based personalized recommender system for product-line configuration},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993249},
doi = {10.1145/2993236.2993249},
abstract = {Today’s competitive marketplace requires the industry to understand unique and particular needs of their customers. Product line practices enable companies to create individual products for every customer by providing an interdependent set of features. Users configure personalized products by consecutively selecting desired features based on their individual needs. However, as most features are interdependent, users must understand the impact of their gradual selections in order to make valid decisions. Thus, especially when dealing with large feature models, specialized assistance is needed to guide the users in configuring their product. Recently, recommender systems have proved to be an appropriate mean to assist users in finding information and making decisions. In this paper, we propose an advanced feature recommender system that provides personalized recommendations to users. In detail, we offer four main contributions: (i) We provide a recommender system that suggests relevant features to ease the decision-making process. (ii) Based on this system, we provide visual support to users that guides them through the decision-making process and allows them to focus on valid and relevant parts of the configuration space. (iii) We provide an interactive open-source configurator tool encompassing all those features. (iv) In order to demonstrate the performance of our approach, we compare three different recommender algorithms in two real case studies derived from business experience.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {120–131},
numpages = {12},
keywords = {Software Product Lines, Recommenders, Product-Line Configuration, Personalized Recommendations},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@inproceedings{10.1109/ASE.2013.6693104,
author = {Sayyad, Abdel Salam and Ingram, Joseph and Menzies, Tim and Ammar, Hany},
title = {Scalable product line configuration: a straw to break the camel's back},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693104},
doi = {10.1109/ASE.2013.6693104},
abstract = {Software product lines are hard to configure. Techniques that work for medium sized product lines fail for much larger product lines such as the Linux kernel with 6000+ features. This paper presents simple heuristics that help the Indicator-Based Evolutionary Algorithm (IBEA) in finding sound and optimum configurations of very large variability models in the presence of competing objectives. We employ a combination of static and evolutionary learning of model structure, in addition to utilizing a pre-computed solution used as a "seed" in the midst of a randomly-generated initial population. The seed solution works like a single straw that is enough to break the camel's back -given that it is a feature-rich seed. We show promising results where we can find 30 sound solutions for configuring upward of 6000 features within 30 minutes.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {465–474},
numpages = {10},
keywords = {variability models, multiobjective optimization, evolutionary algorithms, automated configuration, SMT solvers},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@inproceedings{10.1145/3689492.3690047,
author = {Kell, Stephen and Stinnett, J. Ryan},
title = {Source-Level Debugging of Compiler-Optimised Code: Ill-Posed, but Not Impossible},
year = {2024},
isbn = {9798400712159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689492.3690047},
doi = {10.1145/3689492.3690047},
abstract = {Debuggability and optimisation are traditionally regarded as in fundamental tension. This paper disputes that idea, arguing instead that it is possible to compile programs such that they are both fully source-level-debuggable and fully optimised, and that the essential problem to be solved is loss of state. Although these two properties are usually not achievable at the same time, it argues the feasibility of providing the desired one 'on demand', and that metadata-based approaches extended with residual state can do so in a manner that generalises beyond dynamic deoptimisation. Correctness of debugging metadata is introduced as an ill-posed problem, a partial correctness criterion is proposed, and further approaches are discussed.},
booktitle = {Proceedings of the 2024 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {38–53},
numpages = {16},
keywords = {compilers, debug information, optimisation},
location = {Pasadena, CA, USA},
series = {Onward! '24}
}

@inproceedings{10.1145/2884781.2884823,
author = {Schr\"{o}ter, Reimar and Krieter, Sebastian and Th\"{u}m, Thomas and Benduhn, Fabian and Saake, Gunter},
title = {Feature-model interfaces: the highway to compositional analyses of highly-configurable systems},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884823},
doi = {10.1145/2884781.2884823},
abstract = {Today's software systems are often customizable by means of load-time or compile-time configuration options. These options are typically not independent and their dependencies can be specified by means of feature models. As many industrial systems contain thousands of options, the maintenance and utilization of feature models is a challenge for all stakeholders. In the last two decades, numerous approaches have been presented to support stakeholders in analyzing feature models. Such analyses are commonly reduced to satisfiability problems, which suffer from the growing number of options. While first attempts have been made to decompose feature models into smaller parts, they still require to compose all parts for analysis. We propose the concept of a feature-model interface that only consists of a subset of features, typically selected by experts, and hides all other features and dependencies. Based on a formalization of feature-model interfaces, we prove compositionality properties. We evaluate feature-model interfaces using a three-month history of an industrial feature model from the automotive domain with 18,616 features. Our results indicate performance benefits especially under evolution as often only parts of the feature model need to be analyzed again.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {667–678},
numpages = {12},
keywords = {variability modeling, software product line, modularity, feature model, configurable software, compositionality},
location = {Austin, Texas},
series = {ICSE '16}
}

@proceedings{10.1145/3696443,
title = {CGO '25: Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization},
year = {2025},
isbn = {9798400712753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 23nd ACM/IEEE International Symposium on Code Generation and Optimization (CGO ’25), where we invite you to fabulous Las Vegas.},
location = {Las Vegas, NV, USA}
}

@inproceedings{10.1145/3611643.3616319,
author = {Chen, Jinfu and Ding, Zishuo and Tang, Yiming and Sayagh, Mohammed and Li, Heng and Adams, Bram and Shang, Weiyi},
title = {IoPV: On Inconsistent Option Performance Variations},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616319},
doi = {10.1145/3611643.3616319},
abstract = {Maintaining a good performance of a software system is a primordial task when evolving a software system. The performance regression issues are among the dominant problems that large software systems face. In addition, these large systems tend to be highly configurable, which allows users to change the behaviour of these systems by simply altering the values of certain configuration options. However, such flexibility comes with a cost. Such software systems suffer throughout their evolution from what we refer to as “Inconsistent Option Performance Variation” (IoPV ). An IoPV indicates, for a given commit, that the performance regression or improvement of different values of the same configuration option is inconsistent compared to the prior commit. For instance, a new change might not suffer from any performance regression under the default configuration (i.e., when all the options are set to their default values), while altering one option’s value manifests a regression, which we refer to as a hidden regression as it is not manifested under the default configuration. Similarly, when developers improve the performance of their systems, performance regression might be manifested under a subset of the existing configurations. Unfortunately, such hidden regressions are harmful as they can go unseen to the production environment. In this paper, we first quantify how prevalent (in)consistent performance regression or improvement is among the values of an option. In particular, we study over 803 Hadoop and 502 Cassandra commits, for which we execute a total of 4,902 and 4,197 tests, respectively, amounting to 12,536 machine hours of testing. We observe that IoPV is a common problem that is difficult to manually predict. 69% and 93% of the Hadoop and Cassandra commits have at least one configuration that hides a performance regression. Worse, most of the commits have different options or tests leading to IoPV and hiding performance regressions. Therefore, we propose a prediction model that identifies whether a given combination of commit, test, and option (CTO) manifests an IoPV. Our evaluation for different models shows that random forest is the best performing classifier, with a median AUC of 0.91 and 0.82 for Hadoop and Cassandra, respectively. Our paper defines and provides scientific evidence about the IoPV problem and its prevalence, which can be explored by future work. In addition, we provide an initial machine learning model for predicting IoPV.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {845–857},
numpages = {13},
keywords = {Configurable software systems, Performance variation, Software performance},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@article{10.1145/3607870,
author = {Sridhar, Upasana and Tukanov, Nicholai and Binder, Elliott and Low, Tze Meng and McMillan, Scott and Schatz, Martin D.},
title = {SMaLL: Software for Rapidly Instantiating Machine Learning Libraries},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {1539-9087},
url = {https://doi.org/10.1145/3607870},
doi = {10.1145/3607870},
abstract = {Interest in deploying deep neural network (DNN) inference on edge devices has resulted in an explosion of the number and types of hardware platforms that machine learning (ML) libraries must support. High-level programming interfaces, such as TensorFlow, can be readily ported across different devices; however, maintaining performance when porting the low-level implementation is more nuanced. High-performance inference implementations require an effective mapping of the high-level interface to the target hardware platform. Commonly, this mapping may use optimizing compilers to generate code at compile time or high-performance vendor libraries that have been specialized to the target platform. Both approaches rely on expert knowledge across levels to produce an efficient mapping. This makes supporting new architectures difficult and time-consuming.In this work, we present a DNN library framework, SMaLL, that is easily extensible to new architectures. The framework uses a unified loop structure and shared, cache-friendly data format across all intermediate layers, eliminating the time and memory overheads incurred by data transformation between layers. Each layer is implemented by specifying its dimensions and a kernel, the key computing operation of that layer. The unified loop structure and kernel abstraction allows the reuse of code across layers and computing platforms. New architectures only require a few hundred lines in the kernel to be redesigned. To show the benefits of our approach, we have developed software that supports a range of layer types and computing platforms; this software is easily extensible for rapidly instantiating high-performance DNN libraries.An evaluation of the portability of our framework is shown by instantiating end-to-end networks from the MLPerf:tiny benchmark suite on five ARM platforms and one x86 platform (an AMD Zen 2). We also show that the end-to-end performance is comparable to or better than ML frameworks such as TensorFlow, TVM, and LibTorch.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = may,
articleno = {46},
numpages = {25},
keywords = {Mathematical software, machine learning libraries, high-performance, portability, embedded systems}
}

@inproceedings{10.1145/3654777.3676354,
author = {Sun, Zezhou and Balkcom, Devin and Whiting, Emily},
title = {StructCurves: Interlocking Block-Based Line Structures},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676354},
doi = {10.1145/3654777.3676354},
abstract = {We present a new class of curved block-based line structures whose component chains are flexible when separated, and provably rigid when assembled together into an interlocking double chain. The joints are inspired by traditional zippers, where a binding fabric or mesh connects individual teeth. Unlike traditional zippers, the joint design produces a rigid interlock with programmable curvature. This allows fairly strong curved structures to be built out of easily stored flexible chains. In this paper, we introduce a pipeline for generating these curved structures using a novel block design template based on revolute joints. Mesh embedded in these structures maintains block spacing and assembly order. We evaluate the rigidity of the curved structures through mechanical performance testing and demonstrate several applications.},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {39},
numpages = {11},
keywords = {deployable structures, interlocking structures, kinematics},
location = {Pittsburgh, PA, USA},
series = {UIST '24}
}

@inproceedings{10.1145/3658644.3670278,
author = {Li, Penghui and Zhang, Mingxue},
title = {FuzzCache: Optimizing Web Application Fuzzing Through Software-Based Data Cache},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3670278},
doi = {10.1145/3658644.3670278},
abstract = {Fuzzing has shown great promise in detecting vulnerabilities in server-side web applications. In this work, we introduce an innovative software-based data cache mechanism that complements and improves all existing web application fuzzing tools. Our key observation is that a great proportion of execution time (e.g., 50%) of web applications is spent on fetching data from two major sources: database and network; our in-depth investigation reveals that the same data is often repeatedly fetched across fuzzing trials. We thus design a new solution, FuzzCache, that stores the data into software-based caches, mitigating the need for repeated and expensive data fetches. FuzzCache exposes the cached data across fuzzing trials through inter-process shared memory segments. It also, as the first work, incorporates just-in-time compilation to avoid the performance overhead associated with interpreting PHP code in real time, thereby enhancing execution efficiency.We demonstrate that FuzzCache significantly enhances web application fuzzing performance. In our experiments, we integrated FuzzCache with both a black-box fuzzer (Black-Widow) and a grey-box fuzzer (WebFuzz). The results illustrate that FuzzCache accelerates both black-box and grey-box fuzzing, achieving a throughput increase of 3x to 4x. FuzzCache substantially improves code coverage by an average of 25%. Consequently, FuzzCache enables faster vulnerability detection, leading to the discovery of a greater number of vulnerabilities.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {511–524},
numpages = {14},
keywords = {data cache, system optimization, web fuzzing},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@inproceedings{10.1145/3701625.3701632,
author = {Lima, Rayfran Rocha and da Silva, Alexandre Teixeira},
title = {Accelerating the software development process through the application of MVP and monolithic architecture},
year = {2024},
isbn = {9798400717772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701625.3701632},
doi = {10.1145/3701625.3701632},
abstract = {CONTEXT: The growing demand for efficiency in software development and rapid value delivery has led organizations to explore diverse architectural and methodological approaches. In particular, adopting Minimum Viable Product (MVP) strategies and monolithic architecture has shown promise in accelerating the development process, especially in environments where resources and time are constrained. GOAL: This paper presents a detailed experience report on the implementation of MVP and monolithic architecture in a software development guild within a global software company. The main objective is to evaluate the impact of these approaches on development speed, team productivity and overall quality of the production of complementary software, which can be reused via API by other systems within the organization. METHOD: The evolution of the architectural and methodological approaches went through three phases, spanning over 18 months, involving the transition from unstructured development practices to standardized MVP and monolithic methodologies. Data were collected through direct observations, document analysis, and interviews with team members and users. RESULTS: MVP and monolithic architecture implementation significantly reduced development time and improved team productivity. The structured approach facilitated better resource management and enabled faster iteration based on user feedback. However, challenges related to system scalability and maintainability were noted. CONCLUSION: The findings suggest that while MVP and monolithic architecture can accelerate the software development process and enhance team efficiency, careful consideration must be given to the long-term implications on system scalability and maintainability.},
booktitle = {Proceedings of the XXIII Brazilian Symposium on Software Quality},
pages = {469–477},
numpages = {9},
keywords = {Minimum Viable Product, Software architecture, Value delivery},
location = {
},
series = {SBQS '24}
}

@inproceedings{10.1145/2047862.2047866,
author = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Pukall, Mario and Apel, Sven},
title = {Tailoring dynamic software product lines},
year = {2011},
isbn = {9781450306898},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047862.2047866},
doi = {10.1145/2047862.2047866},
abstract = {Software product lines (SPLs) and adaptive systems aim at variability to cope with changing requirements. Variability can be described in terms of features, which are central for development and configuration of SPLs. In traditional SPLs, features are bound statically before runtime. By contrast, adaptive systems support feature binding at runtime and are sometimes called dynamic SPLs (DSPLs). DSPLs are usually built from coarse-grained components, which reduces the number of possible application scenarios. To overcome this limitation, we closely integrate static binding of traditional SPLs and runtime adaptation of DSPLs. We achieve this integration by statically generating a tailor-made DSPL from a highly customizable SPL. The generated DSPL provides only the runtime variability required by a particular application scenario and the execution environment. The DSPL supports self-configuration based on coarse-grained modules. We provide a feature-based adaptation mechanism that reduces the effort of computing an optimal configuration at runtime. In a case study, we demonstrate the practicability of our approach and show that a seamless integration of static binding and runtime adaptation reduces the complexity of the adaptation process.},
booktitle = {Proceedings of the 10th ACM International Conference on Generative Programming and Component Engineering},
pages = {3–12},
numpages = {10},
keywords = {software product lines, feature-oriented programming, dynamic binding},
location = {Portland, Oregon, USA},
series = {GPCE '11}
}

@inproceedings{10.1145/2889160.2889216,
author = {Jagroep, Erik A. and van der Werf, Jan Martijn and Brinkkemper, Sjaak and Procaccianti, Giuseppe and Lago, Patricia and Blom, Leen and van Vliet, Rob},
title = {Software energy profiling: comparing releases of a software product},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889216},
doi = {10.1145/2889160.2889216},
abstract = {In the quest for energy efficiency of Information and Communication Technology, so far research has mostly focused on the role of hardware. However, as hardware technology becomes more sophisticated, the role of software becomes crucial. Recently, the impact of software on energy consumption has been acknowledged as significant by researchers in software engineering. In spite of that, measuring the energy consumption of software has proven to be a challenge, due to the large number of variables that need to be controlled to obtain reliable measurements. Due to cost and time constraints, many software product organizations are unable to effectively measure the energy consumption of software. This prevents them to be in control over the energy efficiency of their products.In this paper, we propose a software energy profiling method to reliably compare the energy consumed by a software product across different releases, from the perspective of a software organization. Our method allows to attribute differences in energy consumption to changes in the software. We validate our profiling method through an empirical experiment on two consecutive releases of a commercial software product. We demonstrate how the method can be applied by organizations and provide an analysis of the software related changes in energy consumption. Our results show that, despite a lack of precise measurements, energy consumption differences between releases of a software product can be quantified down to the level of individual processes. Additionally, the results provide insights on how specific software changes might affect energy consumption.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {523–532},
numpages = {10},
keywords = {software product, software architecture, profiling, energy efficiency},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/3632775.3661968,
author = {Priya, Aditya and Choudhury, Rajiv and Patni, Sujay and Sharma, Himkant and Mohanty, Moonmoon and Narayanam, Krishnasuri and Devi, Umamaheswari and Moogi, Pratibha and Patil, Preetam and Parag, Parimal},
title = {Energy-minimizing workload splitting and frequency selection for guaranteed performance over heterogeneous cores},
year = {2024},
isbn = {9798400704802},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632775.3661968},
doi = {10.1145/3632775.3661968},
abstract = {Heterogeneous computing involves CPU architectures that support more than one core type, and it aims to achieve energy efficiency while meeting the performance guarantees. This aim can be achieved by the operating system or the on-chip driver by exploiting the differential power-performance trade-off that heterogeneous cores offer. We characterize the power-performance trade-off for an Intel CPU with heterogeneous cores and provide a mathematical framework to study heterogeneous computing. In particular, we provide probabilistic workload split and operating frequency for all active cores that allow workload execution with minimal carbon emissions. We support the analytical findings with experimental evaluations for a few representative workloads. As compared to the default Linux frequency governors, our scheme can reduce the energy-delay product by up to 80%.},
booktitle = {Proceedings of the 15th ACM International Conference on Future and Sustainable Energy Systems},
pages = {308–322},
numpages = {15},
keywords = {Heterogeneous cores, energy optimization, mean latency guarantees, workload scheduling},
location = {Singapore, Singapore},
series = {e-Energy '24}
}

@inproceedings{10.1145/3452383.3452391,
author = {Kuri, Mohit and Karre, Sai Anirudh and Reddy, Y. Raghu},
title = {Understanding Software Quality Metrics for Virtual Reality Products - A Mapping Study},
year = {2021},
isbn = {9781450390460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452383.3452391},
doi = {10.1145/3452383.3452391},
abstract = {Virtual Reality (VR) Software is becoming more mainstream in recent years. It has provided an opportunity for VR practitioners to explore new domains and deliver cutting edge products. The success of the VR products depends primarily on the product contextual relevance and qualities exhibited. However, it is unclear how VR practitioners curb software quality challenges and improve the essence of the VR product over every release. In this paper, we present a Systematic Mapping Study of the software quality metrics adopted by VR practitioners for assessing the quality of their VR products. The study showed that practitioners used unique metrics to measure the quality of their VR products in addition to adopting some of existing enterprise software metrics. Further, we consolidate these metrics into different themes that future practitioners may use for developing VR products.},
booktitle = {Proceedings of the 14th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {8},
numpages = {11},
keywords = {Industrial Practices, Metrics, Software Quality, Virtual Reality},
location = {Bhubaneswar, Odisha, India},
series = {ISEC '21}
}

@article{10.1145/3647644,
author = {I. Garmendia, Andoni and Ceberio, Josu and Mendiburu, Alexander},
title = {Applicability of Neural Combinatorial Optimization: A Critical View},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
url = {https://doi.org/10.1145/3647644},
doi = {10.1145/3647644},
abstract = {Neural Combinatorial Optimization has emerged as a new paradigm in the optimization area. It attempts to solve optimization problems by means of neural networks and reinforcement learning. In the past few years, due to their novelty and presumably good performance, many research papers have been published introducing new neural architectures for a variety of combinatorial problems. However, the incorporation of such models in the conventional optimization portfolio raises many questions related to their performance compared to other existing methods, such as exact algorithms, heuristics, or metaheuristics. This article aims to present a critical view of these new proposals, discussing their benefits and drawbacks with respect to the tools and algorithms already present in the optimization field. For this purpose, a comprehensive study is carried out to analyze the fundamental aspects of such methods, including performance, computational cost, transferability, and reusability of the trained model. Moreover, this discussion is accompanied by the design and validation of a new neural combinatorial optimization algorithm on two well-known combinatorial problems: the Linear Ordering Problem and the Permutation Flowshop Scheduling Problem. Finally, new directions for future work in the area of Neural Combinatorial Optimization algorithms are suggested.},
journal = {ACM Trans. Evol. Learn. Optim.},
month = jul,
articleno = {15},
numpages = {26},
keywords = {Combinatorial optimization, reinforcement learning, Graph Neural Networks}
}

@inproceedings{10.1109/CGO57630.2024.10444847,
author = {Seeker, Volker and Cummins, Chris and Cole, Murray and Franke, Bj\"{o}rn and Hazelwood, Kim and Leather, Hugh},
title = {Revealing Compiler Heuristics through Automated Discovery and Optimization},
year = {2024},
isbn = {9798350395099},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CGO57630.2024.10444847},
doi = {10.1109/CGO57630.2024.10444847},
abstract = {Tuning compiler heuristics and parameters is well known to improve optimization outcomes dramatically. Prior works have tuned command line flags and a few expert identified heuristics. However, there are an unknown number of heuristics buried, unmarked and unexposed inside the compiler as a consequence of decades of development without auto-tuning being foremost in the minds of developers. Many may not even have been considered heuristics by the developers who wrote them. The result is that auto-tuning search and machine learning can optimize only a tiny fraction of what could be possible if all heuristics were available to tune. Manually discovering all of these heuristics hidden among millions of lines of code and exposing them to auto-tuning tools is a Herculean task that is simply not practical. What is needed is a method of automatically finding these heuristics to extract every last drop of potential optimization.In this work, we propose Heureka, a framework that automatically identifies potential heuristics in the compiler that are highly profitable optimization targets and then automatically finds available tuning parameters for those heuristics with minimal human involvement. Our work is based on the following key insight: When modifying the output of a heuristic within an acceptable value range, the calling code using that output will still function correctly and produce semantically correct results. Building on that, we automatically manipulate the output of potential heuristic code in the compiler and decide using a Differential Testing approach if we found a heuristic or not. During output manipulation, we also explore acceptable value ranges of the targeted code. Heuristics identified in this way can then be tuned to optimize an objective function.We used Heureka to search for heuristics among eight thousand functions from the LLVM optimization passes, which is about 2% of all available functions. We then use identified heuristics to tune the compilation of 38 applications from the NAS and Polybench benchmark suites. Compared to an -Ozbaseline we reduce binary sizes by up to 11.6% considering single heuristics only and up to 19.5% when stacking the effects of multiple identified tuning targets and applying a random search with minimal search effort. Generalizing from existing analysis results, Heureka needs, on average, a little under an hour on a single machine to identify relevant heuristic targets for a previously unseen application.},
booktitle = {Proceedings of the 2024 IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {55–66},
numpages = {12},
keywords = {search methodologies, compiler optimization, differential testing},
location = {Edinburgh, United Kingdom},
series = {CGO '24}
}

@inproceedings{10.1145/3676641.3716271,
author = {McMichen, Tommy and Dlott, David and Wongse-ammat, Panitan and Greiner, Nathan and Khajanchi, Hussain and Joseph, Russ and Campanoni, Simone},
title = {Saving Energy with Per-Variable Bitwidth Speculation},
year = {2025},
isbn = {9798400710797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676641.3716271},
doi = {10.1145/3676641.3716271},
abstract = {Tiny devices have become ubiquitous in people's daily lives. Their applications dictate tight energy budgets, but also require reasonable performance to meet user expectations. To this end, the hardware of tiny devices has been highly optimized, making further optimizations difficult. In this work, we identify a missed opportunity: the bitwidth selection of program variables. Today's compilers directly translate the bitwidth specified in the source code to the binary. However, we observe that most variables do not utilize the full bitwidth specified in the source code for the majority of execution. To leverage this opportunity, we propose BitSpec : a system that performs fine-grained speculation on the bitwidth of program variables. BitSpec is implemented as a compiler-architecture co-design, where the compiler transparently reduces the bitwidth of program variables to their expected needs and the hardware monitors speculative variables, reporting misspeculation to the software, which re-executes at the original bitwidth, ensuring correctness. BitSpec reduces energy consumption by 9.9% on average, up to 28.2% .},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {1044–1059},
numpages = {16},
keywords = {register packing, speculation},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/1629716.1629724,
author = {Elsner, Christoph and Lohmann, Daniel and Schr\"{o}der-Preikschat, Wolfgang},
title = {Product derivation for solution-driven product line engineering},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629724},
doi = {10.1145/1629716.1629724},
abstract = {Solution-driven product line engineering is a project business where products are created for each customer individually. Although reuse of results from former projects is widely done, configuration and integration of the results currently is often a manual, time-consuming, and error-prone task and needs considerable knowledge about implementation details.In this paper, we elaborate and approach the challenges when giving automated support for product derivation (i.e., product configuration and generation) in a large-scale solution-driven product line context. Our PLiC approach resembles the fact that, in practice, the domain of a large product line is divided into sub-domains. A PLiC (product line component) packages all results (configuration, generation, and implementation assets) of a sub-domain and offers interfaces for configuration and generation. With our approach we tackle the challenges of using multiple and different types of configuration models and text files, give support for automated product generation, and integrate feature modeling to support application engineering as an extensive development task.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {35–41},
numpages = {7},
keywords = {feature modeling, software product line development, solution-driven software development},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@inproceedings{10.1109/SCW63240.2024.00168,
author = {Klenk, Kyle and Moayeri, Mohammad Mahdi and Guo, Junwei and Clark, Martyn P. and Spiteri, Raymond J.},
title = {Mitigating synchronization bottlenecks in high-performance actor-model-based software},
year = {2025},
isbn = {9798350355543},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SCW63240.2024.00168},
doi = {10.1109/SCW63240.2024.00168},
abstract = {Bulk synchronous programming (in distributedmemory systems) and the fork-join pattern (in shared-memory systems) are often used for problems where independent processes must periodically synchronize. Frequent synchronization can greatly undermine the performance of software designed to solve such problems. We use the actor model of concurrent computing to balance the load of hundreds of thousands of shortlived tasks and mitigate synchronization bottlenecks by buffering communication via actor batching. The actor model is becoming increasingly popular in scientific and high-performance computing because it can handle heterogeneous tasks and computing environments with enhanced programming flexibility and ease relative to conventional paradigms like MPI. For a hydrologic simulation of continental North America with over 500,000 elements, the proposed buffering approach is approximately 4 times faster than no buffering, outperforms MPI on single and multiple nodes, and remains competitive with OpenMP on a single node and MPI+OpenMP on multiple nodes.},
booktitle = {Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {1274–1287},
numpages = {14},
keywords = {actor model of concurrent computing, bulk synchronous programming, fork-join pattern, high-throughput computing, scientific and high-performance computing},
location = {Atlanta, GA, USA},
series = {SC-W '24}
}

@article{10.1162/evco_a_00219,
author = {Ceberio, Josu and Calvo, Borja and Mendiburu, Alexander and Lozano, Jose A.},
title = {Multi-Objectivising Combinatorial Optimisation Problems by Means of Elementary Landscape Decompositions},
year = {2019},
issue_date = {Summer 2019},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {27},
number = {2},
issn = {1063-6560},
url = {https://doi.org/10.1162/evco_a_00219},
doi = {10.1162/evco_a_00219},
abstract = {In the last decade, many works in combinatorial optimisation have shown that, due to the advances in multi-objective optimisation, the algorithms from this field could be used for solving single-objective problems as well. In this sense, a number of papers have proposed multi-objectivising single-objective problems in order to use multi-objective algorithms in their optimisation. In this article, we follow up this idea by presenting a methodology for multi-objectivising combinatorial optimisation problems based on elementary landscape decompositions of their objective function. Under this framework, each of the elementary landscapes obtained from the decomposition is considered as an independent objective function to optimise. In order to illustrate this general methodology, we consider four problems from different domains: the quadratic assignment problem and the linear ordering problem (permutation domain), the 0-1 unconstrained quadratic optimisation problem (binary domain), and the frequency assignment problem (integer domain). We implemented two widely known multi-objective algorithms, NSGA-II and SPEA2, and compared their performance with that of a single-objective GA. The experiments conducted on a large benchmark of instances of the four problems show that the multi-objective algorithms clearly outperform the single-objective approaches. Furthermore, a discussion on the results suggests that the multi-objective space generated by this decomposition enhances the exploration ability, thus permitting NSGA-II and SPEA2 to obtain better results in the majority of the tested instances.},
journal = {Evol. Comput.},
month = jun,
pages = {291–311},
numpages = {21},
keywords = {multi-objective evolutionary algorithm., elementary landscape decomposition, combinatorial optimisation, Multi-objectivisation}
}

@inproceedings{10.1145/3673038.3673104,
author = {Jesus, Ricardo and Weiland, Mich\`{e}le},
title = {Evaluating and optimising compiler code generation for NVIDIA Grace},
year = {2024},
isbn = {9798400717932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3673038.3673104},
doi = {10.1145/3673038.3673104},
abstract = {In this paper, we explore the performance of the main optimising compiler toolchains currently available for high-performance AArch64 processors, namely the Arm Compiler for Linux (ACFL), GNU, LLVM and the NVIDIA HPC (NVHPC) compilers, on the recently released NVIDIA Grace CPU. We evaluate the performance of these compilers using the RAJA Performance Suite (RAJAPerf) to understand where each compiler does best and why. We find that compilers mostly generate well optimised code on baseline sequential runs, with the gap between the fastest and slowest being only 8% on average. However, they exhibit much larger variations on threaded parallel runs—with the gap between fastest and slowest code generated by the different compilers increasing to roughly 33%. Furthermore, we investigate in detail those kernels where LLVM performs worst relative to the remaining compilers and propose optimisations to improve code generation in those cases. We show scenarios where the default compiler behaviour produces sub-optimal code and where adjusting compiler flags, such as those explicitly controlling loop unrolling, can improve performance significantly. In cases where this is insufficient, we propose changes at the compiler level necessary to enable improved code generation and unlock further optimisations. These improvements account for speedups of over 70% in some kernels.},
booktitle = {Proceedings of the 53rd International Conference on Parallel Processing},
pages = {691–700},
numpages = {10},
keywords = {AArch64, LLVM, benchmarking, code generation, performance optimisation},
location = {Gotland, Sweden},
series = {ICPP '24}
}

@article{10.5555/3648699.3648984,
author = {Mousavi, Hamid and Drefs, Jakob and Hirschberger, Florian and L\"{u}cke, J\"{o}rg},
title = {Generic unsupervised optimization for a latent variable model with exponential family observables},
year = {2023},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {Latent variable models (LVMs) represent observed variables by parameterized functions of latent variables. Prominent examples of LVMs for unsupervised learning are probabilistic PCA or probabilistic sparse coding which both assume a weighted linear summation of the latents to determine the mean of a Gaussian distribution for the observables. In many cases, however, observables do not follow a Gaussian distribution. For unsupervised learning, LVMs which assume specific non-Gaussian observables (e.g., Bernoulli or Poisson) have therefore been considered. Already for specific choices of distributions, parameter optimization is challenging and only a few previous contributions considered LVMs with more generally defined observable distributions. In this contribution, we do consider LVMs that are defined for a range of different distributions, i.e., observables can follow any (regular) distribution of the exponential family. Furthermore, the novel class of LVMs presented here is defined for binary latents, and it uses maximization in place of summation to link the latents to observables. In order to derive an optimization procedure, we follow an expectation maximization approach for maximum likelihood parameter estimation. We then show, as our main result, that a set of very concise parameter update equations can be derived which feature the same functional form for all exponential family distributions. The derived generic optimization can consequently be applied (without further derivations) to different types of metric data (Gaussian and non-Gaussian) as well as to different types of discrete data. Moreover, the derived optimization equations can be combined with a recently suggested variational acceleration which is likewise generically applicable to the LVMs considered here. Thus, the combination maintains generic and direct applicability of the derived optimization procedure, but, crucially, enables efficient scalability. We numerically verify our analytical results using different observable distributions, and, furthermore, discuss some potential applications such as learning of variance structure, noise type estimation and denoising.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {285},
numpages = {59},
keywords = {latent variable models, unsupervised learning, exponential family distributions, expectation maximization, variational optimization}
}

@article{10.1145/3503925,
author = {Trotter, James D. and Cai, Xing and Funke, Simon W.},
title = {On Memory Traffic and Optimisations for Low-order Finite Element Assembly Algorithms on Multi-core CPUs},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {2},
issn = {0098-3500},
url = {https://doi.org/10.1145/3503925},
doi = {10.1145/3503925},
abstract = {Motivated by the wish to understand the achievable performance of finite element assembly on unstructured computational meshes, we dissect the standard cellwise assembly algorithm into four kernels, two of which are dominated by irregular memory traffic. Several optimisation schemes are studied together with associated lower and upper bounds on the estimated memory traffic volume. Apart from properly reordering the mesh entities, the two most significant optimisations include adopting a lookup table in adding element matrices or vectors to their global counterparts, and using a row-wise assembly algorithm for multi-threaded parallelisation. Rigorous benchmarking shows that, due to the various optimisations, the actual volumes of memory traffic are in many cases very close to the estimated lower bounds. These results confirm the effectiveness of the optimisations, while also providing a recipe for developing efficient software for finite element assembly.},
journal = {ACM Trans. Math. Softw.},
month = may,
articleno = {19},
numpages = {31},
keywords = {Cavium TX2, AMD Epyc, Intel Xeon, multi-core, assembly, Finite element methods}
}

@inproceedings{10.1145/3377929.3389852,
author = {Cully, Antoine and Mouret, Jean-Baptiste and Doncieux, St\'{e}phane},
title = {Quality-diversity optimisation},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3389852},
doi = {10.1145/3377929.3389852},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {701–723},
numpages = {23},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1145/3636480.3637093,
author = {Arai, Masaki and Fukumoto, Naoto and Murai, Hitoshi},
title = {Introducing software pipelining for the A64FX processor into LLVM},
year = {2024},
isbn = {9798400716522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636480.3637093},
doi = {10.1145/3636480.3637093},
abstract = {Software pipelining is an essential optimization for accelerating High-Performance Computing(HPC) applications on CPUs. Modern CPUs achieve high performance through many-core and wide SIMD instructions. Software pipelining is an optimization that promotes further performance improvement of HPC applications by cooperating with these functions. Although open source compilers such as GCC and LLVM have implemented software pipelining, it is underutilized for the AArch64 architecture. We have implemented software pipelining for the A64FX processor on LLVM to improve this situation. This paper describes the details of this implementation. We also confirmed that our implementation improves the performance of several benchmark programs.},
booktitle = {Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region Workshops},
pages = {1–6},
numpages = {6},
keywords = {A64FX, AArch64, LLVM, compiler, optimization, software pipelining},
location = {Nagoya, Japan},
series = {HPCAsia '24 Workshops}
}

@inproceedings{10.1145/3674558.3674562,
author = {Indirli, Fabrizio and Ornstein, Andrea Carlo and Desoli, Giuseppe and Buschini, Alessandro and Silvano, Cristina and Zaccaria, Vittorio},
title = {Layer-wise Exploration of a Neural Processing Unit Compiler's Optimization Space},
year = {2024},
isbn = {9798400716386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674558.3674562},
doi = {10.1145/3674558.3674562},
abstract = {Given the increasing popularity of Edge AI, embedded neural processing units (NPUs) are gradually becoming a standard feature in microcontrollers (MCUs) and System-on-a-Chip (SoCs). The deployment of neural networks on accelerators needs specialized neural network compilers that incorporate graph optimization stages, where layer-specific transformations are applied to reduce execution latency or memory footprint on platform-specific computing elements. For this reason, neural network compilers expose control parameters to be tuned for each individual network layer. The challenge addressed in this paper is finding an optimal combination of neural network compilation parameters for the efficient utilization of the computing resources of the target hardware accelerators. To address this task despite the huge space of parameters, we propose a greedy algorithm that iterates through the convolutional layers of the network, while preserving a set of solutions for the preceding layers. We evaluated this approach by transforming the graphs of some popular neural networks to optimize their performance and memory footprint, mapping them onto an experimental embedded NPU developed by STMicroelectronics using its associated neural network compiler. For the reported set of network models, the proposed technique has improved latency and memory footprint by 43% approximately compared to the baseline and exceeded the simulated annealing heuristics by 15% approximately.},
booktitle = {Proceedings of the 2024 10th International Conference on Computer Technology Applications},
pages = {20–26},
numpages = {7},
keywords = {Compilers., Design space exploration, NPU, Tiny machine learning, optimization},
location = {Vienna, Austria},
series = {ICCTA '24}
}

@inproceedings{10.1145/3067695.3082519,
author = {Bokhari, Mahmoud A. and Bruce, Bobby R. and Alexander, Brad and Wagner, Markus},
title = {Deep parameter optimisation on Android smartphones for energy minimisation: a tale of woe and a proof-of-concept},
year = {2017},
isbn = {9781450349390},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3067695.3082519},
doi = {10.1145/3067695.3082519},
abstract = {With power demands of mobile devices rising, it is becoming increasingly important to make mobile software applications more energy efficient. Unfortunately, mobile platforms are diverse and very complex which makes energy behaviours difficult to model. This complexity presents challenges to the effectiveness of off-line optimisation of mobile applications. In this paper, we demonstrate that it is possible to automatically optimise an application for energy on a mobile device by evaluating energy consumption in-vivo. In contrast to previous work, we use only the device's own internal meter. Our approach involves many technical challenges but represents a realistic path toward learning hardware specific energy models for program code features.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1501–1508},
numpages = {8},
keywords = {non-functional properties, multi-objective optimisation, mobile devices, dreaming smartphone, Android 6},
location = {Berlin, Germany},
series = {GECCO '17}
}

@inproceedings{10.1145/2701319.2701325,
author = {Devroey, Xavier and Perrouin, Gilles and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Covering SPL Behaviour with Sampled Configurations: An Initial Assessment},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701325},
doi = {10.1145/2701319.2701325},
abstract = {Structural approaches to Software Product Lines (SPL) testing (such as pairwise testing) have gained momentum as they are able to scale to larger SPLs described as feature diagrams (FD). However, these methods are agnostic with respect to behaviour: the sampled configurations have thus no reason to satisfy any given behavioural criterion. In this paper, we investigate the behavioural coverage of two structural testing criteria: pairwise and similarity. To do so, we modelled four SPLs in terms of feature diagrams and associated featured transitions systems (FTSs). We then computed state, action and transition coverage for a set of generated configurations. Preliminary results indicate that for relatively small variability models with few cross-tree constraints, structural coverage-driven tools tend to cover large parts of behaviour with less than 8 configurations. Though structural coverage cannot be used directly as a replacement for behavioural driven SPL test generation, opportunities to mix structural and behavioural coverage for efficient and effective SPL testing do exist.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {59–66},
numpages = {8},
keywords = {Structural Coverage, SPL Testing, Featured Transition System},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@inproceedings{10.1145/1385486.1385488,
author = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Schirmeier, Horst and Sincero, Julio and Apel, Sven and Leich, Thomas and Spinczyk, Olaf and Saake, Gunter},
title = {FAME-DBMS: tailor-made data management solutions for embedded systems},
year = {2008},
isbn = {9781595939647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1385486.1385488},
doi = {10.1145/1385486.1385488},
abstract = {Data management functionality is not only needed in large-scale server systems, but also in embedded systems. Resource restrictions and heterogeneity of hardware, however, complicate the development of data management solutions for those systems. In current practice, this typically leads to the redevelopment of data management because existing solutions cannot be reused and adapted appropriately. In this paper, we present our ongoing work on FAME-DBMS, a research project that explores techniques to implement highly customizable data management solutions, and illustrate how such systems can be created with a software product line approach. With this approach a concrete instance of a DBMS is derived by composing features of the DBMS product line that are needed for a specific application scenario. This product derivation process is getting complex if a large number of features is available. Furthermore, in embedded systems also non-functional properties, e.g., memory consumption, have to be considered when creating a DBMS instance. To simplify the derivation process we present approaches for its automation.},
booktitle = {Proceedings of the 2008 EDBT Workshop on Software Engineering for Tailor-Made Data Management},
pages = {1–6},
numpages = {6},
location = {Nantes, France},
series = {SETMDM '08}
}

@inproceedings{10.1145/2635868.2635919,
author = {Cordy, Maxime and Heymans, Patrick and Legay, Axel and Schobbens, Pierre-Yves and Dawagne, Bruno and Leucker, Martin},
title = {Counterexample guided abstraction refinement of product-line behavioural models},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2635919},
doi = {10.1145/2635868.2635919},
abstract = {The model-checking problem for Software Products Lines (SPLs) is harder than for single systems: variability constitutes a new source of complexity that exacerbates the state-explosion problem. Abstraction techniques have successfully alleviated state explosion in single-system models. However, they need to be adapted to SPLs, to take into account the set of variants that produce a counterexample. In this paper, we apply CEGAR (Counterexample-Guided Abstraction Refinement) and we design new forms of abstraction specifically for SPLs. We carry out experiments to evaluate the efficiency of our new abstractions. The results show that our abstractions, combined with an appropriate refinement strategy, hold the potential to achieve large reductions in verification time, although they sometimes perform worse. We discuss in which cases a given abstraction should be used.},
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {190–201},
numpages = {12},
keywords = {Software Product Lines, Model Checking, CEGAR, Abstraction},
location = {Hong Kong, China},
series = {FSE 2014}
}

@inproceedings{10.1145/3510455.3512792,
author = {Randrianaina, Georges Aaron and Khelladi, Djamel Eddine and Zendra, Olivier and Acher, Mathieu},
title = {Towards incremental build of software configurations},
year = {2022},
isbn = {9781450392242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510455.3512792},
doi = {10.1145/3510455.3512792},
abstract = {Building software is a crucial task to compile, test, and deploy software systems while continuously ensuring quality. As software is more and more configurable, building multiple configurations is a pressing need, yet, costly and challenging to instrument. The common practice is to independently build (a.k.a., clean build) a software for a subset of configurations. While incremental build has been considered for software evolution and relatively small modifications of the source code, it has surprisingly not been considered for software configurations. In this vision paper, we formulate the hypothesis that incremental build can reduce the cost of exploring the configuration space of software systems. We detail how we apply incremental build for two real-world application scenarios and conduct a preliminary evaluation on two case studies, namely x264 and Linux Kernel. For x264, we found that one can incrementally build configurations in an order such that overall build time is reduced. Nevertheless, we could not find any optimal order with the Linux Kernel, due to a high distance between random configurations. Therefore, we show it is possible to control the process of generating configurations: we could reuse commonality and gain up to 66% of build time compared to only clean builds.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {101–105},
numpages = {5},
keywords = {incremental build, highly configurable system, build system},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-NIER '22}
}

@article{10.1145/3653452,
author = {Bagchi, Aritra and Dharamjeet and Rishabh, Ohm and Suri, Manan and Panda, Preeti Ranjan},
title = {POEM: Performance Optimization and Endurance Management for Non-volatile Caches},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3653452},
doi = {10.1145/3653452},
abstract = {Non-volatile memories (NVMs), with their high storage density and ultra-low leakage power, offer promising potential for redesigning the memory hierarchy in next-generation Multi-Processor Systems-on-Chip (MPSoCs). However, the adoption of NVMs in cache designs introduces challenges such as NVM write overheads and limited NVM endurance. The shared NVM cache in an MPSoC experiences requests from different processor cores and responses from the off-chip memory when the requested data is not present in the cache. Besides, upon evictions of dirty data from higher-level caches, the shared NVM cache experiences another source of write operations, known as writebacks. These sources of write operations—writebacks and responses—further exacerbate the contention for the shared bandwidth of the NVM cache and create significant performance bottlenecks. Uncontrolled write operations can also affect the endurance of the NVM cache, posing a threat to cache lifetime and system reliability. Existing strategies often address either performance or cache endurance individually, leaving a gap for a holistic solution. This study introduces the Performance Optimization and Endurance Management (POEM) methodology, a novel approach that aggressively bypasses cache writebacks and responses to alleviate the NVM cache contention. Contrary to the existing bypass policies that do not pay adequate attention to the shared NVM cache contention and focus too much on cache data reuse, POEM’s aggressive bypass significantly improves the overall system performance, even at the expense of data reuse. POEM also employs effective wear leveling to enhance the NVM cache endurance by careful redistribution of write operations across different cache lines. Across diverse workloads, POEM yields an average speedup of 34% over a na\"{\i}ve baseline and 28.8% over a state-of-the-art NVM cache bypass technique while enhancing the cache endurance by 15% over the baseline. POEM also explores diverse design choices by exploiting a key policy parameter that assigns varying priorities to the two system-level objectives.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = sep,
articleno = {79},
numpages = {36},
keywords = {Non volatile memories (NVMs), caches, contention, bypass, endurance}
}

@inbook{10.1145/3658617.3697713,
author = {Vulchi, Lakshmi Sai Niharika and Valipireddy, Pranathi and Basavaraju, Mahati and Rao, Madhav},
title = {HyPPO: Hybrid Piece-wise Polynomial Approximation and Optimization for Hardware Efficient Designs},
year = {2025},
isbn = {9798400706356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658617.3697713},
abstract = {Piece-wise polynomial approximation - linear (PWL) and quadratic (PWQ) have proven to efficiently implement non-linear functions on hardware. This paper introduces Hybrid Piece-wise Polynomial Approximation (PW-Hybrid) where pieces of approximation for a function are obtained as the best combination of linear and quadratic polynomials, such that the error converges to the desired minimum. The hardware for PWL, PWQ and PW-Hybrid designs are further refined using Particle Swarm Optimization (PSO) algorithm to fine-tune the quantized bit-widths for realizing coefficients of the polynomial employed. This PSO optimised hardware design is evolved for a range of non-linear functions including i) Piece-wise polynomial linear optimized (PWLO) - Logarithmic, Hyperbolic-Tangent, Sigmoid and Softsign, and ii) Piece-wise polynomial quadratic optimized (PWQO) Exponential, and iii) Hybrid Piece-wise Polynomial optimized (HyPPO) - Sine and Sinc. The proposed design shows considerable decrease in hardware resource consumption and critical path delay, when synthesized using Cadence 45nm gpdk library. Highest improvements in HyPPO, PWLO and PWQO designs, are observed for Sine, Logarithmic and Exponential functions, with 65.06%, 24.47% and 9.67% gain in power-area-delay product (PADP) respectively, when compared with SOTA - PWL and PWQ designs. The proposed methods also exhibited minimal inference accuracy loss when tested on popular CNN architectures.},
booktitle = {Proceedings of the 30th Asia and South Pacific Design Automation Conference},
pages = {230–236},
numpages = {7}
}

@article{10.1145/3632931,
author = {Campora, John Peter and Khan, Mohammad Wahiduzzaman and Chen, Sheng},
title = {Type-Based Gradual Typing Performance Optimization},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {POPL},
url = {https://doi.org/10.1145/3632931},
doi = {10.1145/3632931},
abstract = {Gradual typing has emerged as a popular design point in programming languages, attracting significant interests from both academia and industry. Programmers in gradually typed languages are free to utilize static and dynamic typing as needed. To make such languages sound, runtime checks mediate the boundary of typed and untyped code. Unfortunately, such checks can incur significant runtime overhead on programs that heavily mix static and dynamic typing. To combat this overhead without necessitating changes to the underlying implementations of languages, we present discriminative typing. Discriminative typing works by optimistically inferring types for functions and implementing an optimized version of the function based on this type. To preserve safety it also implements an un-optimized version of the function based purely on the provided annotations. With two versions of each function in hand, discriminative typing translates programs so that the optimized functions are called as frequently as possible while also preserving program behaviors.  

We have implemented discriminative typing in Reticulated Python and have evaluated its performance  
compared to guarded Reticulated Python. Our results show that discriminative typing improves the performance across 95% of tested programs, when compared to Reticulated, and achieves more than 4\texttimes{} speedup in more than 56% of these programs. We also compare its performance against a previous optimization approach and find that discriminative typing improved performance across 93% of tested programs, with 30% of these programs receiving speedups between 4 to 25 times. Finally, our evaluation shows that discriminative typing remarkably reduces the overhead of gradual typing on many mixed type configurations of programs.  

In addition, we have implemented discriminative typing in Grift and evaluated its performance. Our  
evaluation demonstrations that DT significantly improves performance of Grift},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {89},
numpages = {33},
keywords = {gradual typing, performance optimization, type-based specialization, variational types}
}

@inproceedings{10.1145/3611643.3616300,
author = {Chen, Zhiming and Chen, Pengfei and Wang, Peipei and Yu, Guangba and He, Zilong and Mai, Genting},
title = {DiagConfig: Configuration Diagnosis of Performance Violations in Configurable Software Systems},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616300},
doi = {10.1145/3611643.3616300},
abstract = {Performance degradation due to misconfiguration in software systems that violates SLOs (service-level objectives) is commonplace.  
Diagnosing and explaining the root causes of such performance violations in configurable software systems is often challenging due to their increasing complexity.  
Although there are many tools and techniques for diagnosing performance violations, they provide limited evidence to attribute causes of observed performance violations to specific configurations. This is because the configuration is not originally considered in those tools.  
This paper proposes DiagConfig, specifically designed to conduct configuration diagnosis of performance violations. It leverages static code analysis to track configuration option propagation, identifies performance-sensitive options, detects performance violations, and constructs cause-effect chains that help stakeholders better understand the relationship between configuration and performance violations.  
Experimental evaluations with eight real-world software demonstrate that DiagConfig produces fewer false positives than a state-of-the-art documentation analysis-based tool (i.e., 5 vs 41) in the identification of performance-sensitive options, and outperforms a statistics-based debugging tool in the diagnosis of performance violations caused by configuration changes, offering more comprehensive results (recall: 0.892 vs 0.289). Moreover, we also show that DiagConfig can accelerate auto-tuning by compressing configuration space.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {566–578},
numpages = {13},
keywords = {Configuration diagnosis, Performance violation, Program analysis, Taint tracking},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3696443.3708959,
author = {Wang, Huanting and Lenihan, Patrick and Wang, Zheng},
title = {Enhancing Deployment-Time Predictive Model Robustness for Code Analysis and Optimization},
year = {2025},
isbn = {9798400712753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696443.3708959},
doi = {10.1145/3696443.3708959},
abstract = {Supervised machine learning techniques have shown promising results in code analysis and optimization problems. However, a learning-based solution can be brittle because minor changes in hardware or application workloads – such as facing a new CPU architecture or code pattern – may jeopardize decision accuracy, ultimately undermining model robustness. We introduce Prom, an open-source library to enhance the robustness and performance of predictive models against such changes during deployment. Prom achieves this by using statistical assessments to identify test samples prone to mispredictions and using feedback on these samples to improve a deployed model. We showcase Prom by applying it to 13 representative machine learning models across 5 code analysis and optimization tasks. Our extensive evaluation demonstrates that Prom can successfully identify an average of 96% (up to 100%) of mispredictions. By relabeling up to 5% of the Prom-identified samples through incremental learning, Prom can help a deployed model achieve a performance comparable to that attained during its model training phase.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {31–46},
numpages = {16},
keywords = {Machine learning, Model reliability, Statistical assessment},
location = {Las Vegas, NV, USA},
series = {CGO '25}
}

@inproceedings{10.1145/3691620.3695071,
author = {Li, Yue and Zhang, He and Jin, Yuzhe and Ren, Zhong and Dong, Liming and Lyu, Jun and Yang, Lanxin and Lo, David and Shao, Dong},
title = {An Explainable Automated Model for Measuring Software Engineer Contribution},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695071},
doi = {10.1145/3691620.3695071},
abstract = {Software engineers play an important role throughout the software development life-cycle, particularly in industry emphasizing quality assurance and timely delivery. Contribution measurement provides proper incentives to software engineers that motivate them to continuously improve the quality and efficiency of their work. However, existing research tends to ignore contribution measurement for software engineers in practice, relying heavily on peer review and lacking objectivity and transparency. Specifically, these studies still have two weaknesses. First, a few studies explore which metrics can be useful for contribution measurement in practice. Second, managers measure the contribution of software engineers based on their experience and lack of explainable automated tools to assist them.To this end, we conduct mixed-method studies to investigate contribution measurement in the enterprise, and then propose an explainable model for measuring the contribution of software engineers. First, we collect and synthesize 16 metrics for contribution measurement by interviewing 18 industrial practitioners. Second, we propose an explainable model, called Memento, integrating Multi-dimEnsional MEtrics for measuriNg conTributiOn. We conduct an industrial case study with a global enterprise to evaluate and refine Memento. Finally, we administer a survey to industrial practitioners to verify whether the explainable model is useful for contribution analysis, which resulted in 67 valid responses. Memento is used by the enterprise to measure the contribution of 5,174 employees. A total of 100 employees are identified as low contributors by Memento, and the five reasons for their low contribution are determined based on the results of our explainable model. The results indicate that Memento can effectively measure the contribution of software engineers, which provides a practical reference for researchers interested in contribution measurement.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {783–794},
numpages = {12},
keywords = {contribution measurement, mining software repository, quantitative measurement},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@article{10.1145/3312741,
author = {Wirth, Fabian R. and St\"{u}dli, Sonja and Yu, Jia Yuan and Corless, Martin and Shorten, Robert},
title = {Nonhomogeneous Place-dependent Markov Chains, Unsynchronised AIMD, and Optimisation},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {4},
issn = {0004-5411},
url = {https://doi.org/10.1145/3312741},
doi = {10.1145/3312741},
abstract = {A stochastic algorithm is presented for a class of optimisation problems that arise when a group of agents compete to share a single constrained resource in an optimal manner. The approach uses intermittent single-bit feedback, which indicates a constraint violation and does not require inter-agent communication. The algorithm is based on a positive matrix model of AIMD, which is extended to the nonhomogeneous Markovian case. The key feature is the assignment of back-off probabilities to the individual agents as a function of the past average access to the resource. This leads to a nonhomogeneous Markov chain in an extended state space, and we show almost sure convergence of the average access to the social optimum.},
journal = {J. ACM},
month = jun,
articleno = {24},
numpages = {37},
keywords = {optimisation, iterated function systems, invariant measure, almost sure convergence, Nonhomogeneous Markov chains, AIMD}
}

@article{10.1145/3492762,
author = {Sobhy, Dalia and Minku, Leandro and Bahsoon, Rami and Kazman, Rick},
title = {Continuous and Proactive Software Architecture Evaluation: An IoT Case},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3492762},
doi = {10.1145/3492762},
abstract = {Design-time evaluation is essential to build the initial software architecture to be deployed. However, experts’ assumptions made at design-time are unlikely to remain true indefinitely in systems that are characterized by scale, hyperconnectivity, dynamism, and uncertainty in operations (e.g. IoT). Therefore, experts’ design-time decisions can be challenged at run-time. A continuous architecture evaluation that systematically assesses and intertwines design-time and run-time decisions is thus necessary. This paper proposes the first proactive approach to continuous architecture evaluation of the system leveraging the support of simulation. The approach evaluates software architectures by not only tracking their performance over time, but also forecasting their likely future performance through machine learning of simulated instances of the architecture. This enables architects to make cost-effective informed decisions on potential changes to the architecture. We perform an IoT case study to show how machine learning on simulated instances of architecture can fundamentally guide the continuous evaluation process and influence the outcome of architecture decisions. A series of experiments is conducted to demonstrate the applicability and effectiveness of the approach. We also provide the architect with recommendations on how to best benefit from the approach through choice of learners and input parameters, grounded on experimentation and evidence.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {46},
numpages = {54},
keywords = {IoT, time series forecasting, software architecture evaluation, Continuous evaluation}
}

@inbook{10.5555/3712729.3712855,
author = {Kiefer, Maximilian and Buhle, Patrick and Clausen, Uwe},
title = {Simulation and Optimization-Based Planning of the Use of Tank Containers in the Production of Specialty Chemicals},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {Changing market conditions in the chemical industry are leading to an increased demand for fast and individually engineered chemicals. This results in a decline in mass production towards producing small, demand-driven quantities. A combination of changing demand and the need for short-term adjustments requires flexible production planning and logistics. To ensure logistics flexibility, primarily manual processes are used. However, this comes with the risk of direct contact between humans and chemicals. One way to avoid this contact and enable sufficient flexibility is to use tank containers directly connected to the production plants. This paper aims to develop a framework that combines simulation and optimization for planning the use of tank containers. Based on this framework, tank container storage materials will be selected using optimization. Furthermore, simulation helps to evaluate the influences of the selection on the logistics system. Here, the focus is on the management of general cargo containers.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1527–1538},
numpages = {12}
}

@inproceedings{10.1145/3627341.3630381,
author = {Hong, Sheng and Lai, Yizhong and Li, Yuzhou and You, Yang and Ou, Shuai and Han, Fei and Wu, Tiejun},
title = {A Reference Model for Information Security Applications of Generative Adversarial Network Variants},
year = {2023},
isbn = {9798400708701},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627341.3630381},
doi = {10.1145/3627341.3630381},
abstract = {Information security stemming from Generative Adversarial Network (GAN) variants has garnered significant attention. However, a complete reference model targeting this security problem has yet to be established. This paper selects several GAN variants as the research subject and proposes a reference model framework for the information security applications of adversarial generative network variants. The proposed framework is derived using the NIST information security reference model methodology. By conducting a comprehensive analysis of the structure and information security risks of GAN variants, this paper classifies information security attacks on information systems of GAN variants into three categories and maps them onto the security target reference model. The resulting security application reference model can serve as a basis and reference for improving system confidentiality, integrity, and availability, as well as facilitating the design, analysis, and verification of security against malicious attacks. Moreover, the research method employed in this paper is also applicable to information security research of other types of information systems. Therefore, the proposed reference model framework can serve as a valuable contribution to the field of information security and advance the development of effective countermeasures against adversarial generative network variants.},
booktitle = {Proceedings of the 2023 International Conference on Computer, Vision and Intelligent Technology},
articleno = {7},
numpages = {8},
keywords = {Adversarial Generative Network Variant, Information Security, Security Reference Model, System Security},
location = {Chenzhou, China},
series = {ICCVIT '23}
}

@inproceedings{10.1145/3678890.3678896,
author = {Yang, Zeyu and Pu, Hongyi and He, Liang and Yao, Chengtao and Zhou, Jianying and Cheng, Peng and Chen, Jiming},
title = {Deception-Resistant Stochastic Manufacturing for Automated Production Lines},
year = {2024},
isbn = {9798400709593},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678890.3678896},
doi = {10.1145/3678890.3678896},
abstract = {The advancement of Industrial Internet-of-Things (IIoT) magnifies the cyber risk of automated production lines, especially to deception attacks that tamper with the monitoring data to prevent the manipulated operation of production lines from being detected. To address this issue, we propose Stochastic Manufacturing (StoM), a new paradigm of manufacturing that is resistant to deception by design. StoM voids the foundation of deception attacks — i.e., the highly predictable operation data due to the cyclical manufacturing process — by injecting controlled stochasticity into the operation of production lines without degrading manufacturing efficiency or quality. StoM then examines if this stochasticity can be observed from the operation data and triggers an alarm of deception attack if not. We have experimentally evaluated StoM on two production line platforms, showing StoM to detect deception attacks with a detection rate exceeding 99.1%, a false alarm rate below 0.1%, and a latency of less than 1.2 manufacturing cycles. Our empirical analysis also shows that it is highly impractical for attackers to spoof the controlled stochasticity.},
booktitle = {Proceedings of the 27th International Symposium on Research in Attacks, Intrusions and Defenses},
pages = {546–560},
numpages = {15},
keywords = {Controlled Stochasticity, Deception Attacks, Production Line Manufacturing},
location = {Padua, Italy},
series = {RAID '24}
}

@inproceedings{10.1145/3658617.3697547,
author = {Fayyazi, Arya and Kamal, Mehdi and Pedram, Massoud},
title = {Dynamic Co-Optimization Compiler: Leveraging Multi-Agent Reinforcement Learning for Enhanced DNN Accelerator Performance},
year = {2025},
isbn = {9798400706356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658617.3697547},
doi = {10.1145/3658617.3697547},
abstract = {This paper introduces a novel Dynamic Co-Optimization Compiler (DCOC), which employs an adaptive Multi-Agent Reinforcement Learning (MARL) framework to enhance the efficiency of mapping machine learning (ML) models, particularly Deep Neural Networks (DNNs), onto diverse hardware platforms. DCOC incorporates three specialized actor-critic agents within MARL, each dedicated to different optimization facets: one for hardware and two for software. This cooperative strategy results in an integrated hardware/software co-optimization approach, improving the precision and speed of DNN deployments. By focusing on high-confidence configurations, DCOC effectively reduces the search space, achieving remarkable performance over existing methods. Our results demonstrate that DCOC enhances throughput by up to 37.95% while reducing optimization time by up to 42.2% across various DNN models, outperforming current state-of-the-art frameworks.},
booktitle = {Proceedings of the 30th Asia and South Pacific Design Automation Conference},
pages = {16–22},
numpages = {7},
keywords = {multi-agaent reinforcement learning, co-optimization, hardware accelerators, sampling, throughput},
location = {Tokyo, Japan},
series = {ASPDAC '25}
}

@inproceedings{10.1145/3672758.3672775,
author = {Zeng, Weijia},
title = {Optimization of the process of 3D character design and production for 3D artists},
year = {2024},
isbn = {9798400716942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672758.3672775},
doi = {10.1145/3672758.3672775},
abstract = {This article mainly discusses the optimization of the process of 3D character design and production for 3D artists. We address the problems faced in the current 3D character design and production process, emphasizing the concept of art first, allowing artists to more freely express their creativity in the design and production process. We propose a nonlinear process and method based on digital sculpting technology, supported by dynamic mesh, topological reconstruction, projecting and polypainting, texture baking, etc, to improve design efficiency and artistic expression.},
booktitle = {Proceedings of the 3rd International Conference on Computer, Artificial Intelligence and Control Engineering},
pages = {98–106},
numpages = {9},
location = {Xi' an, China},
series = {CAICE '24}
}

@inproceedings{10.1145/3674805.3686671,
author = {Liu, Yueyue and Zhang, Hongyu and Li, Zhiqiang and Miao, Yuantian},
title = {Optimizing the Utilization of Large Language Models via Schedule Optimization: An Exploratory Study},
year = {2024},
isbn = {9798400710476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674805.3686671},
doi = {10.1145/3674805.3686671},
abstract = {Background: Large Language Models (LLMs) have gained significant attention in machine-learning-as-a-service (MLaaS) offerings. In-context learning (ICL) is a technique that guides LLMs towards accurate query processing by providing additional information. However, longer prompts lead to higher costs of LLM service, creating a performance-cost trade-off. Aims: We aim to investigate the potential of combining schedule optimization with ICL to optimize LLM utilization. Method: We conduct an exploratory study. First, we consider the performance-cost trade-off in LLM utilization as a multi-objective optimization problem, aiming to select the most suitable prompt template for each LLM job to maximize accuracy (the percentage of correctly processed jobs) and minimize invocation cost. Next, we investigate three methods for prompt performance prediction to address the challenge of evaluating the accuracy objective in the fitness function, as the result can only be determined after submitting the job to the LLM. Finally, we apply widely used search-based techniques and evaluate their effectiveness. Results: The results indicate that the machine learning-based technique is an effective approach for prompt performance prediction and fitness function calculation. Schedule optimization can achieve higher accuracy or lower cost by selecting a suitable prompt template for each job, compared to simply submitting all jobs using a single prompt template, e.g., saving costs from 21.33% to 86.92% in our experiments on LLM-based log parsing. However, the performance of the evaluated search-based techniques varies across different instances and metrics, with no single technique consistently outperforming the others. Conclusions: This study demonstrates the potential of combining schedule optimization with ICL to improve the utilization of LLMs. However, there is still ample room for improving the searched-based techniques and prompt performance prediction techniques for more cost-effective LLM utilization.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {84–95},
numpages = {12},
keywords = {Large Language Models, Multi-objective Optimization, Schedule Optimization, Search-based Techniques},
location = {Barcelona, Spain},
series = {ESEM '24}
}

@inproceedings{10.1145/3551349.3556906,
author = {Zhu, Jie and Wang, Leye and Han, Xiao},
title = {Safety and Performance, Why not Both? Bi-Objective Optimized Model Compression toward AI Software Deployment},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556906},
doi = {10.1145/3551349.3556906},
abstract = {The size of deep learning models in artificial intelligence (AI) software is increasing rapidly, which hinders the large-scale deployment on resource-restricted devices (e.g., smartphones). To mitigate this issue, AI software compression plays a crucial role, which aims to compress model size while keeping high performance. However, the intrinsic defects in the big model may be inherited by the compressed one. Such defects may be easily leveraged by attackers, since the compressed models are usually deployed in a large number of devices without adequate protection. In this paper, we try to address the safe model compression problem from a safety-performance co-optimization perspective. Specifically, inspired by the test-driven development (TDD) paradigm in software engineering, we propose a test-driven sparse training framework called SafeCompress. By simulating the attack mechanism as the safety test, SafeCompress can automatically compress a big model to a small one following the dynamic sparse training paradigm. Further, considering a representative attack, i.e., membership inference attack (MIA), we develop a concrete safe model compression mechanism, called MIA-SafeCompress. Extensive experiments are conducted to evaluate MIA-SafeCompress on five datasets for both computer vision and natural language processing tasks. The results verify the effectiveness and generalization of our method. We also discuss how to adapt SafeCompress to other attacks besides MIA, demonstrating the flexibility of SafeCompress.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {88},
numpages = {13},
keywords = {test-driven development, membership inference attack, AI software safe compression},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@article{10.1145/3709355,
author = {Wang, Qing and Wang, Junjie and Li, Mingyang and Wang, Yawen and Liu, Zhe},
title = {A Roadmap for Software Testing in Open-Collaborative and AI-Powered Era},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3709355},
doi = {10.1145/3709355},
abstract = {Internet technology has given rise to an open-collaborative software development paradigm, necessitating the open-collaborative schema to software testing. It enables diverse and globally distributed contributions, but also presents significant challenges to efficient testing processes, coordination among personnel, and management of testing artifacts. At the same time, advancements in artificial intelligence (AI) have enhanced testing capabilities and enabling automation, while also introducing new testing needs and unique challenges for AI-based systems. In this context, this paper explores software testing in the open-collaborative and AI-powered era, focusing on the interrelated dimensions of process, personnel, and technology. Among them, process involves managing testing workflows and artifacts to improve efficiency, personnel emphasizes the role of individuals in ensuring testing quality through collaboration and contributions, while technology refers to AI methods that enhance testing capabilities and address challenges in AI-based systems. Furthermore, we delve into the challenges and opportunities arising from emerging technologies such as large language models (LLMs) and the AI model-centric development paradigm.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {Software Testing, Artificial Intelligence, AI, Large Language Model, LLM, Open Source, Open Collaborative}
}

@inproceedings{10.1145/3696443.3708960,
author = {Mamatha Ananda, Chaitanya and Gupta, Rajiv and Tallam, Sriraman and Shen, Han and Li, Xinliang David},
title = {PreFix: Optimizing the Performance of Heap-Intensive Applications},
year = {2025},
isbn = {9798400712753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696443.3708960},
doi = {10.1145/3696443.3708960},
abstract = {Analyses of heap-intensive applications show that a small fraction of heap objects account for the majority of heap accesses and data cache misses. Prior works like HDS and HALO have shown that allocating hot objects in separate memory regions can improve spatial locality leading to better application performance. However, these techniques are constrained in two primary ways, limiting their gains. First, these techniques have Imperfect Separation, polluting the hot memory region with several cold objects. Second, reordering of objects across allocations is not possible as the original object allocation order is preserved. This paper presents a novel technique that achieves near perfect separation of hot objects via a new context mechanism that efficiently identifies hot objects with high precision. This technique, named PreFix, is based upon Preallocating memory for a Fixed small number of hot objects. The program, guided by profiles, is instrumented to compute context information derived from dynamic object identifiers, that precisely identifies hot object allocations that are then placed at predetermined locations in the preallocated memory. The preallocated memory region for hot objects provides the flexibility to reorder objects across allocations and allows colocation of objects that are part of a hot data stream (HDS), improving spatial locality. The runtime overhead of identifying hot objects is not significant as this optimization is only focused on a small number of static hot allocation sites and dynamic hot objects. While there is an increase in the program's memory foot-print, it is manageable and can be controlled by limiting the size of the preallocated memory. In addition, PreFix incorporates an object recycling optimization that reuses the same preallocated space to store different objects whose lifetimes are not expected to overlap. Our experiments with 13 heap-intensive applications yield reductions in execution times ranging from 2.77% to 74%. On average PreFix reduces execution time by 21.7% compared to 7.3% by HDS and 14% by HALO. This is due to PreFix's precision in hot object identification, hot object colocation, and low runtime overhead.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {405–417},
numpages = {13},
keywords = {Cache locality, Hot data streams, Hot heap objects, Object colocation},
location = {Las Vegas, NV, USA},
series = {CGO '25}
}

@article{10.1109/TASLP.2023.3325136,
author = {Stanciu, Cristian-Lucian and Benesty, Jacob and Paleologu, Constantin and Costea, Ruxandra-Liana and Dogariu, Laura-Maria and Ciochin\u{a}, Silviu},
title = {Decomposition-Based Wiener Filter Using the Kronecker Product and Conjugate Gradient Method},
year = {2023},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3325136},
doi = {10.1109/TASLP.2023.3325136},
abstract = {The identification of long-length impulse responses represents a challenge in the context of many applications, like echo cancellation. Recently, the problem has been addressed in the framework of low-rank systems, using a decomposition of the impulse response based on the nearest Kronecker product and low-rank approximations. As a result, the original system identification problem that involves a long-length finite impulse response filter is reshaped as a combination of two (much) shorter filters, which leads to significant advantages. In this context, the benchmark Wiener filter can be formulated in terms of an iterative algorithm, where the estimates of the two component filters are sequently updated. However, matrix inversion operations are required within this algorithm. In this article, we develop a new version of the decomposition-based iterative Wiener filter, which relies on the conjugate gradient (CG) method and avoids matrix inversion. Simulations performed in the framework of echo cancellation indicate the good performance of the proposed solution, which outperforms the conventional Wiener filter (implemented using CG updates) and inherits the advantages of the decomposition-based approach.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {124–138},
numpages = {15}
}

@inproceedings{10.1145/3545258.3545283,
author = {Liu, Bo and Chen, Jinfu and Wang, Weijia and Cai, Saihua and Chen, Jingyi and Feng, Qiaowei},
title = {An adaptive search optimization algorithm for improving the detection capability of software vulnerability},
year = {2022},
isbn = {9781450397803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545258.3545283},
doi = {10.1145/3545258.3545283},
abstract = {Deep learning-based vulnerability detection frees human experts from the tedious task of defining features and allows for better detection capabilities. The common practice is to convert program code into vector representation for neural network model training. Since the length of the vector representation varies across program code, finding the optimal vector length is critical to ensuring detection accuracy. This paper proposes an adaptive search optimization algorithm for finding the optimal vector length. It sorts all the vector lengths obtained by word2vec and takes the vector length corresponding to the point where the trend changes from slow to fast as the output. We evaluate our algorithm on three publicly available datasets against state-of-the-art algorithms. The results show that, without significantly increasing the time overhead, our algorithm can more accurately choose an appropriate vector length instead of setting a value empirically or arbitrarily. Furthermore, it shows that while a larger vector length can usually produces a higher detection accuracy, the extra time overhead incurred often does not suffice to compensate for the corresponding accuracy improvement.},
booktitle = {Proceedings of the 13th Asia-Pacific Symposium on Internetware},
pages = {212–220},
numpages = {9},
keywords = {Vulnerability detection, Software security, Deep learning, Adaptive search optimization algorithm},
location = {Hohhot, China},
series = {Internetware '22}
}

@inproceedings{10.1145/302405.302409,
author = {DeBaud, Jean-Marc and Schmid, Klaus},
title = {A systematic approach to derive the scope of software product lines},
year = {1999},
isbn = {1581130740},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/302405.302409},
doi = {10.1145/302405.302409},
booktitle = {Proceedings of the 21st International Conference on Software Engineering},
pages = {34–43},
numpages = {10},
keywords = {software product line, reuse economic models, product line scoping, domain engineering},
location = {Los Angeles, California, USA},
series = {ICSE '99}
}

@article{10.1145/3582572,
author = {Guo, Zhaoqiang and Liu, Shiran and Liu, Xutong and Lai, Wei and Ma, Mingliang and Zhang, Xu and Ni, Chao and Yang, Yibiao and Li, Yanhui and Chen, Lin and Zhou, Guoqiang and Zhou, Yuming},
title = {Code-line-level Bugginess Identification: How Far have We Come, and How Far have We Yet to Go?},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3582572},
doi = {10.1145/3582572},
abstract = {Background. Code-line-level bugginess identification (CLBI) is a vital technique that can facilitate developers to identify buggy lines without expending a large amount of human effort. Most of the existing studies tried to mine the characteristics of source codes to train supervised prediction models, which have been reported to be able to discriminate buggy code lines amongst others in a target program.Problem. However, several simple and clear code characteristics, such as complexity of code lines, have been disregarded in the current literature. Such characteristics can be acquired and applied easily in an unsupervised way to conduct more accurate CLBI, which also can decrease the application cost of existing CLBI approaches by a large margin.Objective. We aim at investigating the status quo in the field of CLBI from the perspective of (1) how far we have really come in the literature, and (2) how far we have yet to go in the industry, by analyzing the performance of state-of-the-art (SOTA) CLBI approaches and tools, respectively.Method. We propose a simple heuristic baseline solution GLANCE (aiminG at controL- ANd ComplEx-statements) with three implementations (i.e., GLANCE-MD, GLANCE-EA, and GLANCE-LR). GLANCE is a two-stage CLBI framework: first, use a simple model to predict the potentially defective files; second, leverage simple code characteristics to identify buggy code lines in the predicted defective files. We use GLANCE as the baseline to investigate the effectiveness of the SOTA CLBI approaches, including natural language processing (NLP) based, model interpretation techniques (MIT) based, and popular static analysis tools (SAT).Result. Based on 19 open-source projects with 142 different releases, the experimental results show that GLANCE framework has a prediction performance comparable or even superior to the existing SOTA CLBI approaches and tools in terms of 8 different performance indicators.Conclusion. The results caution us that, if the identification performance is the goal, the real progress in CLBI is not being achieved as it might have been envisaged in the literature and there is still a long way to go to really promote the effectiveness of static analysis tools in industry. In addition, we suggest using GLANCE as a baseline in future studies to demonstrate the usefulness of any newly proposed CLBI approach.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {102},
numpages = {55},
keywords = {static analysis tool, quality assurance, defect prediction, bugginess, Code line}
}

@inproceedings{10.1145/1449913.1449918,
author = {Mendonca, Marcilio and Wasowski, Andrzej and Czarnecki, Krzysztof and Cowan, Donald},
title = {Efficient compilation techniques for large scale feature models},
year = {2008},
isbn = {9781605582672},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1449913.1449918},
doi = {10.1145/1449913.1449918},
abstract = {Feature modeling is used in generative programming and software product-line engineering to capture the common and variable properties of programs within an application domain. The translation of feature models to propositional logics enabled the use of reasoning systems, such as BDD engines, for the analysis and transformation of such models and interactive configurations. Unfortunately, the size of a BDD structure is highly sensitive to the variable ordering used in its construction and an inappropriately chosen ordering may prevent the translation of a feature model into a BDD representation of a tractable size. Finding an optimal order is NP-hard and has for long been addressed by using heuristics.We review existing general heuristics and heuristics from the hardware circuits domain and experimentally show that they are not effective in reducing the size of BDDs produced from feature models. Based on that analysis we introduce two new heuristics for compiling feature models to BDDs. We demonstrate the effectiveness of these heuristics using publicly available and automatically generated models. Our results are directly applicable in construction of feature modeling tools.},
booktitle = {Proceedings of the 7th International Conference on Generative Programming and Component Engineering},
pages = {13–22},
numpages = {10},
keywords = {software-product lines, model-driven development, formal verification, feature modeling, configuration},
location = {Nashville, TN, USA},
series = {GPCE '08}
}

@inproceedings{10.1145/3479242.3487325,
author = {Zhang, Yu and Gorlatch, Sergei},
title = {Optimizing Energy Efficiency of QoS-Based Routing in Software-Defined Networks},
year = {2021},
isbn = {9781450390804},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479242.3487325},
doi = {10.1145/3479242.3487325},
abstract = {We address the routing optimization problem in Software-Defined Networks (SDN) to minimize the energy consumption while satisfying multiple QoS constraints of network services. In this paper, we: 1) formally define the problem of routing optimization in SDN under multiple QoS constraints, and 2) propose a novel routing optimization algorithm that improves the classical Shuffled Frog Leaping Algorithm (SFLA). We develop a two-phase procedure for implementing our proposed routing optimization algorithm in an SDN environment. We report the results of experiments using the Mininet simulation framework that confirm the advantages of our proposed QoS routing optimization algorithm as compared to the state-of-the-art solutions.},
booktitle = {Proceedings of the 17th ACM Symposium on QoS and Security for Wireless and Mobile Networks},
pages = {87–94},
numpages = {8},
keywords = {software-defined networking, shuffled frog leaping algorithm, routing optimization, quality of service (qos), multiple qos constraints, energy efficiency},
location = {Alicante, Spain},
series = {Q2SWinet '21}
}

@inproceedings{10.1145/3511430.3511437,
author = {Saxena, Amol and Bhatnagar, Roheet and Kumar Srivastava, Devesh},
title = {Effective Lightweight Software Fault Localization based on Test Suite Optimization},
year = {2022},
isbn = {9781450396189},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511430.3511437},
doi = {10.1145/3511430.3511437},
abstract = {Automated software fault localization techniques aid developers in program debugging by identifying the probable locations of faults in a program with minimum human intervention. As software is growing in complexity and scale today, increasing the efficiency of fault localization techniques is very essential in order to reduce the overall software development cost. The effectiveness of the test suites used in the fault localization process has a significant impact on the efficiency of the process. Previous studies, on the other hand, have placed less focus on the adequacy of test suites for the fault localization process. We apply optimized test suites in this paper to improve the performance of software fault localization in a single-fault scenario. For our experiments, we use spectrum-based fault localization (SBFL) techniques. Because of its minimal computing overhead and scalability, spectrum-based fault localization is a popular, efficient, and yet lightweight fault localization technique. To optimize the test suite, we employ a heuristic that asserts that if a faulty statement is executed by a passing test case, that test case will have a negative impact on fault localization performance. In contrast, if a passing test case does not execute the faulty statement, the faulty statement's suspiciousness increases, which has a positive impact on fault localization performance. The test suite optimization approach used in this paper significantly improves fault localization performance, as demonstrated by our experiments. The results show that the proposed method efficiently reduces the number of statements examined by about 84.94 percent on average.},
booktitle = {Proceedings of the 15th Innovations in Software Engineering Conference},
articleno = {9},
numpages = {10},
location = {Gandhinagar, India},
series = {ISEC '22}
}

@inproceedings{10.1145/3626246.3654756,
author = {Huang, Yicong and Wang, Zuozhi and Li, Chen},
title = {Demonstration of Udon: Line-by-line Debugging of User-Defined Functions in Data Workflows},
year = {2024},
isbn = {9798400704222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626246.3654756},
doi = {10.1145/3626246.3654756},
abstract = {Many big data systems are written in languages such as C, C++, Java, and Scala for high efficiency, whereas data analysts often use Python to conduct data wrangling, statistical analysis, and machine learning. User-defined functions (UDFs) are commonly used in these systems to bridge the gap between the two ecosystems. Debugging complex UDFs in data-processing systems is challenging due to the required coordination between language debuggers and the data-processing engine, as well as the debugging overhead on large volumes of data. In this paper, we showcase Udon, a novel debugger to support line-by-line debugging of UDFs in data-processing systems. Udon encapsulates modern line-by-line debugging primitives, such as those to set breakpoints, perform code inspections, and make code modifications while executing a UDF on a single tuple. In this demonstration, we use real-world scenarios to showcase the experience of using Udon for line-by-line debugging of a UDF.},
booktitle = {Companion of the 2024 International Conference on Management of Data},
pages = {476–479},
numpages = {4},
keywords = {data workflows, debugging, python udf, user-defined functions},
location = {Santiago AA, Chile},
series = {SIGMOD '24}
}

@inproceedings{10.1145/3474624.3476010,
author = {Ferreira, Thiago do Nascimento and Vergilio, Silvia Regina and Kessentini, Marouane},
title = {Implementing Search-Based Software Engineering Approaches with Nautilus},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3476010},
doi = {10.1145/3474624.3476010},
abstract = {Search-Based Software Engineering (SBSE) approaches adopt search-based techniques to solve Software Engineering (SE) optimization problems. Among these techniques, evolutionary algorithms are the most popular and successfully used, such as multi-objective evolutionary algorithms. However, some challenges still need to be addressed. Firstly, SE problems are complex and commonly impacted by many conflicting factors. In this context, the use of many-objective algorithms is necessary. Secondly, the users very often do not recognise the found solutions as feasible because these solutions are usually not generated considering the users’ needs and preferences. Thus, to deal properly with this situation, preference-based algorithms should be applied. Moreover, there are some practical issues regarding the choice of operators, evaluation of algorithms and visualization of solutions. Existing frameworks do not provide support to address these challenges. To overcome these limitations, we present Nautilus, an open-source Java web-platform tool that works with plugins to ease the addition of new problem instances, implementation of search operators and different multi and many-objective optimization algorithms, guided (or not) by human participation. This paper describes Nautilus-NRP, an extension implemented to address the Next Release Problem (NRP). NRP refers to the selection of requirements to be implemented in the next release of a software and is used to illustrate Nautilus’ main functionalities and how it can be extended to solve a SE problem. Link for the video: https://youtu.be/2dbwslTrvhg.},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {303–308},
numpages = {6},
keywords = {preference-based algorithms, next release problem, many-objective optimization},
location = {Joinville, Brazil},
series = {SBES '21}
}

@inproceedings{10.1145/3449639.3459332,
author = {Yin, Kaiou and Arcaini, Paolo and Yue, Tao and Ali, Shaukat},
title = {Analyzing the impact of product configuration variations on advanced driver assistance systems with search},
year = {2021},
isbn = {9781450383509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449639.3459332},
doi = {10.1145/3449639.3459332},
abstract = {Due to the complexity of designing vehicle products and the inherent uncertainties in their operating environments, ensuring the safety of their Advanced Driver Assistance Systems (ADASs) becomes crucial. Especially, very minor changes to a vehicle design, for instance due to production errors or component degradation, might lead to failures of ADASs and, therefore, catastrophic consequences such as collision occurrences. Motivated by this, we propose a multi-objective search-based approach (employing NSGA-II) to find minimum changes to the configuration of a set of configurable parameters of a vehicle design, such that the collision probability is maximized, consequently leading to a reversal change in its safety. We conducted experiments, in a vehicle driving simulator, to evaluate the effectiveness of our approach. Results show that our approach with NSGA-II significantly outperforms the random search. Moreover, based on the detailed analyses of the results, we identify some parameters for which minor changes to their values lead the vehicle into collisions, and demonstrated the importance of studying the configuration of multiple parameters in a single search and the impact of their interactions on causing collisions.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1106–1114},
numpages = {9},
keywords = {simulation, search, configuration, collision, automotive},
location = {Lille, France},
series = {GECCO '21}
}

@inproceedings{10.5555/3466184.3466537,
author = {Schmaranzer, David and Kiefer, Alexander and Braune, Roland and Doerner, Karl F.},
title = {Simulation-based replacement line and headway optimization},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {We present a study on simulation-based replacement line and headway optimization for the Viennese public transportation system. The discussed problem focuses on scheduled closures of subway lines. A genetic algorithm is proposed to design replacement lines and potentially adjust the headways of all lines in the network. Candidate networks are simulated to evaluate their solution quality. The underlying discrete event simulation model has several stochastic elements (e.g., vehicle travel and turning maneuver times). Passenger creation is a Poisson process which uses daily origin destination matrices based on anonymous mobile phone and count data. Vehicles are subject to capacity restrictions. Computational insights are gained from three real-world based test instance. Our problem-specific genetic algorithm creates not only good but also robust solutions by taking stochastic elements into account.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3069–3080},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@article{10.1145/3592609,
author = {Cucinotta, Tommaso and Amory, Alexandre and Ara, Gabriele and Paladino, Francesco and Natale, Marco Di},
title = {Multi-criteria Optimization of Real-time DAGs on Heterogeneous Platforms under P-EDF},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1539-9087},
url = {https://doi.org/10.1145/3592609},
doi = {10.1145/3592609},
abstract = {This article tackles the problem of optimal placement of complex real-time embedded applications on heterogeneous platforms. Applications are composed of directed acyclic graphs of tasks, with each directed-acyclic-graph (DAG) having a minimum inter-arrival period for its activation requests and an end-to-end deadline within which all of the computations need to terminate since each activation. The platforms of interest are heterogeneous power-aware multi-core platforms with Dynamic Voltage and Frequency Scaling (DVFS) capabilities, including big.LITTLE Arm architectures and platforms with GPU or FPGA hardware accelerators with Dynamic Partial Reconfiguration capabilities. Tasks can be deployed on CPUs using partitioned EDF-based scheduling. Additionally, some of the tasks may have an alternate implementation available for one of the accelerators on the target platform, which are assumed to serve requests in non-preemptive FIFO order. The system can be optimized by minimizing power consumption, respecting precise timing constraints, maximizing the applications’ slack, respecting given power consumption constraints, or even a combination of these, in a multi-objective formulation.We propose an off-line optimization of the mentioned problem based on mixed-integer quadratic constraint programming (MIQCP). The optimization provides the DVFS configuration of all the CPUs (or accelerators) capable of frequency switching and the placement to be followed by each task in the DAGs, including the software-vs.-hardware implementation choice for tasks that can be hardware accelerated. For relatively big problems, we developed heuristic solvers capable of providing suboptimal solutions in a significantly reduced time compared to the MIQCP strategy, thus widening the applicability of the proposed framework.We validate the approach by running a set of randomly generated DAGs on Linux under SCHED_DEADLINE, deployed onto two real boards, one with Arm big.LITTLE architecture, the other with FPGA acceleration, verifying that the experimental runs meet the theoretical expectations in terms of timing and power optimization goals.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = jan,
articleno = {3},
numpages = {35},
keywords = {SCHED_DEADLINE, Linux kernel, heuristic optimization, mixed-integer quadratic constraint programming, software-to-hardware mapping, DVFS, energy efficiency, End-to-end timing requirements}
}

@proceedings{10.1145/3622748,
title = {SBCARS '23: Proceedings of the 17th Brazilian Symposium on Software Components, Architectures, and Reuse},
year = {2023},
isbn = {9798400709524},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Campo Grande, Brazil}
}

@inproceedings{10.1145/3485447.3511949,
author = {Fan, Lu and Li, Qimai and Liu, Bo and Wu, Xiao-Ming and Zhang, Xiaotong and Lv, Fuyu and Lin, Guli and Li, Sen and Jin, Taiwei and Yang, Keping},
title = {Modeling User Behavior with Graph Convolution for Personalized Product Search},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511949},
doi = {10.1145/3485447.3511949},
abstract = {User preference modeling is a vital yet challenging problem in personalized product search. In recent years, latent space based methods have achieved state-of-the-art performance by jointly learning semantic representations of products, users, and text tokens. However, existing methods are limited in their ability to model user preferences. They typically represent users by the products they visited in a short span of time using attentive models and lack the ability to exploit relational information such as user-product interactions or item co-occurrence relations. In this work, we propose to address the limitations of prior arts by exploring local and global user behavior patterns on a user successive behavior graph, which is constructed by utilizing short-term actions of all users. To capture implicit user preference signals and collaborative patterns, we use an efficient jumping graph convolution to explore high-order relations to enrich product representations for user preference modeling. Our approach can be seamlessly integrated with existing latent space based methods and be potentially applied in any product retrieval method that uses purchase history to model user preferences. Extensive experiments on eight Amazon benchmarks demonstrate the effectiveness and potential of our approach. The source code is available at https://github.com/floatSDSDS/SBG .},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {203–212},
numpages = {10},
keywords = {Graph Convolution, Personalized Product Search, User Preference Modeling},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3593434.3593442,
author = {Arcelli Fontana, Francesca and Camilli, Mateo and Rendina, Davide and Taraboi, Andrei Gabriel and Trubiani, Catia},
title = {Impact of Architectural Smells on Software Performance: an Exploratory Study},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593434.3593442},
doi = {10.1145/3593434.3593442},
abstract = {Architectural smells have been studied in the literature looking at several aspects, such as their impact on maintainability as a source of architectural debt, their correlations with code smells, and their evolution in the history of complex projects. The goal of this paper is to extend the study of architectural smells from a different perspective. We focus our attention on software performance, and we aim to quantify the impact of architectural smells as support to explain the root causes of system performance hindrances. Our method consists of a study design matching the occurrence of architectural smells with performance metrics. We exploit state-of-the-art tools for architectural smell detection, software performance profiling, and testing the systems under analysis. The removal of architectural smells generates new versions of systems from which we derive some observations on design changes improving/worsening performance metrics. Our experimentation considers two complex open-source projects, and results show that the detection and removal of two common types of architectural smells yield lower response time (up to ) with a large effect size, i.e., for - of the hotspot methods. The median memory consumption is also lower (up to ) with a large effect size for all the services.},
booktitle = {Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
pages = {22–31},
numpages = {10},
keywords = {Software Performance, Software Architecture, Architectural Smells},
location = {Oulu, Finland},
series = {EASE '23}
}

@inproceedings{10.1145/3696355.3696362,
author = {Liu, Wei and Chen, Jian-Jia and Yang, Yongjie},
title = {Schedulability Analysis and Performance Optimization for Constrained-Deadline Elastic Tasks},
year = {2025},
isbn = {9798400717246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696355.3696362},
doi = {10.1145/3696355.3696362},
abstract = {The elastic task model is a flexible paradigm for representing recurring real-time workloads, in which the frequencies of periodic/sporadic tasks are allowed to be adjusted within their specified ranges. This paper introduces a novel approach to identify the minimum inter-arrival times of sporadic constrained-deadline elastic tasks. Our method utilizes approximate demand bound functions to perform schedulability tests. The problem is formulated as an optimization problem that aims to meet the schedulability under Earliest Deadline First (EDF) scheduling while optimizing a performance metric. Our proposed algorithm achieves a speedup factor of (frac{1.5026}{1-sigma }) in polynomial time, resulting in a total system benefit value that is at least 1 − ϵ when compared to the optimal solution based on the test. The parameters 0 &lt; σ &lt; 1 and 0 &lt; ϵ &lt; 1 are defined by the system designers. Our algorithm effectively balances efficiency and accuracy. The experimental results demonstrate the effectiveness of the proposed method in identifying feasible solutions and optimizing performance, particularly for large inputs.},
booktitle = {Proceedings of the 32nd International Conference on Real-Time Networks and Systems},
pages = {207–217},
numpages = {11},
keywords = {Sporadic Constrained-Deadline Elastic tasks, Preemptive Uniprocessors, Earliest Deadline First},
location = {
},
series = {RTNS '24}
}

@inproceedings{10.1145/3620666.3651387,
author = {Mao, Jinsong and Ding, Hailun and Zhai, Juan and Ma, Shiqing},
title = {Merlin: Multi-tier Optimization of eBPF Code for Performance and Compactness},
year = {2024},
isbn = {9798400703867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620666.3651387},
doi = {10.1145/3620666.3651387},
abstract = {eBPF (extended Berkeley Packet Filter) significantly enhances observability, performance, and security within the Linux kernel, playing a pivotal role in various real-world applications. Implemented as a register-based kernel virtual machine, eBPF features a customized Instruction Set Architecture (ISA) with stringent kernel safety requirements, e.g., a limited number of instructions. This constraint necessitates substantial optimization efforts for eBPF programs to meet performance objectives. Despite the availability of compilers supporting eBPF program compilation, existing tools often overlook key optimization opportunities, resulting in suboptimal performance. In response, this paper introduces Merlin, an optimization framework leveraging customized LLVM passes and bytecode rewriting for Instruction Representation (IR) transformation and bytecode refinement. Merlin employs two primary optimization strategies, i.e., instruction merging and strength reduction. These optimizations are deployed before eBPF verification. We evaluate Merlin across 19 XDP programs (drawn from the Linux kernel, Meta, hXDP, and Cilium) and three eBPF-based systems (Sysdig, Tetragon, and Tracee, each comprising several hundred eBPF programs). The results show that all optimized programs pass the kernel verification. Meanwhile, Merlin can reduce number of instructions by 73% and runtime overhead by 60% compared with the original programs. Merlin can also improve the throughput by 0.59% and reduce the latency by 5.31%, compared to state-of-the-art technique K2, while being 106 times faster and more scalable to larger and more complex programs without additional manual efforts.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {639–653},
numpages = {15},
keywords = {eBPF optimization, LLVM},
location = {La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3534678.3539059,
author = {Markov, Igor L. and Wang, Hanson and Kasturi, Nitya S. and Singh, Shaun and Garrard, Mia R. and Huang, Yin and Yuen, Sze Wai Celeste and Tran, Sarah and Wang, Zehui and Glotov, Igor and Gupta, Tanvi and Chen, Peng and Huang, Boshuang and Xie, Xiaowen and Belkin, Michael and Uryasev, Sal and Howie, Sam and Bakshy, Eytan and Zhou, Norm},
title = {Looper: An End-to-End ML Platform for Product Decisions},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539059},
doi = {10.1145/3534678.3539059},
abstract = {Modern software systems and products increasingly rely on machine learning models to make data-driven decisions based on interactions with users, infrastructure and other systems. For broader adoption, this practice must (i) accommodate product engineers without ML backgrounds, (ii) support finegrain product-metric evaluation and (iii) optimize for product goals. To address shortcomings of prior platforms, we introduce general principles for and the architecture of an ML platform, Looper, with simple APIs for decision-making and feedback collection. Looper covers the end-to-end ML lifecycle from collecting training data and model training to deployment and inference, and extends support to personalization, causal evaluation with heterogenous treatment effects, and Bayesian tuning for product goals. During the 2021 production deployment, Looper simultaneously hosted 440-1,000 ML models that made 4-6 million real-time decisions per second. We sum up experiences of platform adopters and describe their learning curve.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3513–3523},
numpages = {11},
keywords = {MLOps, machine learning, platform},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3372799.3394361,
author = {Cereda, Stefano and Palermo, Gianluca and Cremonesi, Paolo and Doni, Stefano},
title = {A Collaborative Filtering Approach for the Automatic Tuning of Compiler Optimisations},
year = {2020},
isbn = {9781450370943},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372799.3394361},
doi = {10.1145/3372799.3394361},
abstract = {Selecting the right compiler optimisations has a severe impact on programs' performance. Still, the available optimisations keep increasing, and their effect depends on the specific program, making the task human intractable. Researchers proposed several techniques to search in the space of compiler optimisations. Some approaches focus on finding better search algorithms, while others try to speed up the search by leveraging previously collected knowledge. The possibility to effectively reuse previous compilation results inspired us toward the investigation of techniques derived from the Recommender Systems field. The proposed approach exploits previously collected knowledge and improves its characterisation over time. Differently from current state-of-the-art solutions, our approach is not based on performance counters but relies on Reaction Matching, an algorithm able to characterise programs looking at how they react to different optimisation sets. The proposed approach has been validated using two widely used benchmark suites, cBench and PolyBench, including 54 different programs. Our solution, on average, extracted 90% of the available performance improvement 10 iterations before current state-of-the-art solutions,which corresponds to 40% fewer compilations and performance tests to perform.},
booktitle = {The 21st ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems},
pages = {15–25},
numpages = {11},
keywords = {tuning, selection, recommender systems, reaction, performance, optimization, flag, embedded, compiler, collaborative filtering, characterization, autotuning},
location = {London, United Kingdom},
series = {LCTES '20}
}

@article{10.1145/3697009,
author = {Zhang, Ting and Irsan, Ivana Clairine and Thung, Ferdian and Lo, David},
title = {Revisiting Sentiment Analysis for Software Engineering in the Era of Large Language Models},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3697009},
doi = {10.1145/3697009},
abstract = {Software development involves collaborative interactions where stakeholders express opinions across various platforms. Recognizing the sentiments conveyed in these interactions is crucial for the effective development and ongoing maintenance of software systems. For software products, analyzing the sentiment of user feedback, e.g., reviews, comments, and forum posts can provide valuable insights into user satisfaction and areas for improvement. This can guide the development of future updates and features. However, accurately identifying sentiments in software engineering datasets remains challenging.This study investigates bigger large language models (bLLMs) in addressing the labeled data shortage that hampers fine-tuned smaller large language models (sLLMs) in software engineering tasks. We conduct a comprehensive empirical study using five established datasets to assess three open source bLLMs in zero-shot and few-shot scenarios. Additionally, we compare them with fine-tuned sLLMs, using sLLMs to learn contextual embeddings of text from software platforms.Our experimental findings demonstrate that bLLMs exhibit state-of-the-art performance on datasets marked by limited training data and imbalanced distributions. bLLMs can also achieve excellent performance under a zero-shot setting. However, when ample training data are available or the dataset exhibits a more balanced distribution, fine-tuned sLLMs can still achieve superior results.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {60},
numpages = {30},
keywords = {Large Language Models, Sentiment Analysis, Software Engineering}
}

@inproceedings{10.1145/1551722.1551730,
author = {Laguna, Miguel A. and Finat, Javier and Gonz\'{a}lez, Jos\'{e} A.},
title = {Mobile health monitoring and smart sensors: a product line approach},
year = {2009},
isbn = {9781605583983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1551722.1551730},
doi = {10.1145/1551722.1551730},
abstract = {The evolution of the population pyramid in developed countries, with an increasing proportion of aged people introduces new challenges to the public and private assistance services. A form of improving these services while keeping controlled the associated costs is to use remote continuous assistance. Wireless sensors allow obtaining real-time information of health parameters in a non-intrusive way. The determination of alert values for these parameters and the computing possibilities of the current mobile devices can facilitate a faster intervention which will minimize risks linked to delays in medical assistance. However, the diversity of risk situations is a factor that increases costs as many similar but not exactly identical products will be necessary now and in the future. We aim to solve this problem using an approach of software product lines, as multiple options can be easily incorporated to each final product implementation. This article presents the product line generic architecture and some examples of application, using wireless sensors connected to a central station by means of a smart phone, which is able to detect alarm situations.},
booktitle = {Proceedings of the 2009 Euro American Conference on Telematics and Information Systems: New Opportunities to Increase Digital Citizenship},
articleno = {8},
numpages = {8},
keywords = {software product line, sensor, remote health monitoring},
location = {Prague, Czech Republic},
series = {EATIS '09}
}

@article{10.1145/3705308,
author = {Chen, Xiangxiang and Lin, Xingwei and Wang, Jingyi and Sun, Jun and Wang, Jiashui and Wang, Wenhai},
title = {Scuzer: A Scheduling Optimization Fuzzer for TVM},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3705308},
doi = {10.1145/3705308},
abstract = {The concept of deep learning (DL) compiler was proposed to deploy DL models more efficiently on diverse hardware through optimization techniques. As one of the most popular DL compilers, TVM incorporates three levels (high-level, schedule, and low-level) of optimizations, which can inadvertently introduce code logic bugs and build failure bugs. Among these optimizations, scheduling optimization is the core component of DL compilers, which ensures the acceleration of models on all devices. However, the existing works only focus on the testing of high-level and low-level optimizations in TVM, fail to take the most important and challenging intermediate scheduling optimization layer into consideration.To fill the gap, we propose a Scheduling optimization oriented fuzzer for TVM, named Scuzer, which is specially designed to effectively detect bugs introduced by the scheduling optimization. In particular, Scuzer first proposes a set of schedule-triggering mutators to actively trigger many scheduling optimizations. Meanwhile, observing that scheduling optimization is closely coupled with program data flow and operator type, Scuzer additionally proposes a set of structure-enriching mutators to enrich the structure of data flows and operators. Based on these carefully designed mutators, Scuzer then devises a multi-objective algorithm that can adaptively select different combinations of objectives at each period to guide the selection of seeds and mutators during fuzzing. We conduct extensive experiments comparing with three state-of-the-art fuzzers that can be applied in testing scheduling optimization to evaluate the effectiveness of Scuzer. The experimental results demonstrate that Scuzer outperforms the 2nd-best state-of-the-art fuzzer by 7.4% in edge coverage and achieves 7 (times)  improvement in rule-operator coverage. Scuzer has successfully detected 17 previously unknown bugs (9 are inconsistent results and 5 are inconsistent compilations) in TVM, out of which 10 have been confirmed and 5 been fixed.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {Deep Learning Compiler, Fuzzing, Scheduling Optimization, TVM}
}

@article{10.1145/3514233,
author = {Chen, Tao and Li, Miqing},
title = {The Weights Can Be Harmful: Pareto Search versus Weighted Search in Multi-objective Search-based Software Engineering},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3514233},
doi = {10.1145/3514233},
abstract = {In presence of multiple objectives to be optimized in Search-Based Software Engineering (SBSE), Pareto search has been commonly adopted. It searches for a good approximation of the problem’s Pareto-optimal solutions, from which the stakeholders choose the most preferred solution according to their preferences. However, when clear preferences of the stakeholders (e.g., a set of weights that reflect relative importance between objectives) are available prior to the search, weighted search is believed to be the first choice, since it simplifies the search via converting the original multi-objective problem into a single-objective one and enables the search to focus on what only the stakeholders are interested in.This article questions such a “weighted search first” belief. We show that the weights can, in fact, be harmful to the search process even in the presence of clear preferences. Specifically, we conduct a large-scale empirical study that consists of 38 systems/projects from three representative SBSE problems, together with two types of search budget and nine sets of weights, leading to 604 cases of comparisons. Our key finding is that weighted search reaches a certain level of solution quality by consuming relatively less resources at the early stage of the search; however, Pareto search is significantly better than its weighted counterpart the majority of the time (up to 77% of the cases), as long as we allow a sufficient, but not unrealistic search budget. This is a beneficial result, as it discovers a potentially new “rule-of-thumb” for the SBSE community: Even when clear preferences are available, it is recommended to always consider Pareto search by default for multi-objective SBSE problems, provided that solution quality is more important. Weighted search, in contrast, should only be preferred when the resource/search budget is limited, especially for expensive SBSE problems. This, together with other findings and actionable suggestions in the article, allows us to codify pragmatic and comprehensive guidance on choosing weighted and Pareto search for SBSE under the circumstance that clear preferences are available. All code and data can be accessed at .},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {5},
numpages = {40},
keywords = {self-adaptive systems, adaptive systems, configurable systems, user preference, quality indicator, quality evaluation, pareto optimization, multi-objective optimization, Search-based software engineering}
}

@article{10.1145/3712002,
author = {Murillo, Juan M. and Garcia-Alonso, Jose and Moguel, Enrique and Barzen, Johanna and Leymann, Frank and Ali, Shaukat and Yue, Tao and Arcaini, Paolo and P\'{e}rez-Castillo, Ricardo and Garc\'{\i}a Rodr\'{\i}guez de Guzm\'{a}n, Ignacio and Piattini, Mario and Ruiz-Cort\'{e}s, Antonio and Brogi, Antonio and Zhao, Jianjun and Miranskyy, Andriy and Wimmer, Manuel},
title = {Quantum Software Engineering: Roadmap and Challenges Ahead},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3712002},
doi = {10.1145/3712002},
abstract = {As quantum computers advance, the complexity of the software they can execute increases as well. To ensure this software is efficient, maintainable, reusable, and cost-effective —key qualities of any industry-grade software— mature software engineering practices must be applied throughout its design, development, and operation. However, the significant differences between classical and quantum software make it challenging to directly apply classical software engineering methods to quantum systems. This challenge has led to the emergence of Quantum Software Engineering as a distinct field within the broader software engineering landscape. In this work, a group of active researchers analyse in depth the current state of quantum software engineering research. From this analysis, the key areas of quantum software engineering are identified and explored in order to determine the most relevant open challenges that should be addressed in the next years. These challenges help identify necessary breakthroughs and future research directions for advancing Quantum Software Engineering.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Quantum Software Engineering, open challenges, Quantum Computing, QSE}
}

@inproceedings{10.1145/3577193.3593714,
author = {Tr\"{u}mper, Lukas and Ben-Nun, Tal and Schaad, Philipp and Calotoiu, Alexandru and Hoefler, Torsten},
title = {Performance Embeddings: A Similarity-Based Transfer Tuning Approach to Performance Optimization},
year = {2023},
isbn = {9798400700569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577193.3593714},
doi = {10.1145/3577193.3593714},
abstract = {Performance optimization is an increasingly challenging but often repetitive task. While each platform has its quirks, the underlying code transformations rely on data movement and computational characteristics that recur across applications. This paper proposes to leverage those similarities by constructing an embedding space for subprograms. The continuous space captures both static and dynamic properties of loop nests via symbolic code analysis and performance profiling, respectively. Performance embeddings enable direct knowledge transfer of performance tuning between applications, which can result from autotuning or tailored improvements. We demonstrate this transfer tuning approach on case studies in deep neural networks, dense and sparse linear algebra compositions, and numerical weather prediction stencils. Transfer tuning reduces the search complexity by up to four orders of magnitude and outperforms the MKL library in sparse-dense matrix multiplication. The results exhibit clear correspondences between program characteristics and optimizations, outperforming prior specialized state-of-the-art approaches and generalizing beyond their capabilities.},
booktitle = {Proceedings of the 37th ACM International Conference on Supercomputing},
pages = {50–62},
numpages = {13},
keywords = {autotuning, performance optimization, peephole optimization, transfer tuning, embeddings, compilers},
location = {Orlando, FL, USA},
series = {ICS '23}
}

@article{10.1145/3610536,
author = {Karl, Florian and Pielok, Tobias and Moosbauer, Julia and Pfisterer, Florian and Coors, Stefan and Binder, Martin and Schneider, Lennart and Thomas, Janek and Richter, Jakob and Lang, Michel and Garrido-Merch\'{a}n, Eduardo C. and Branke, Juergen and Bischl, Bernd},
title = {Multi-Objective Hyperparameter Optimization in Machine Learning—An Overview},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
url = {https://doi.org/10.1145/3610536},
doi = {10.1145/3610536},
abstract = {Hyperparameter optimization constitutes a large part of typical modern machine learning (ML) workflows. This arises from the fact that ML methods and corresponding preprocessing steps often only yield optimal performance when hyperparameters are properly tuned. But in many applications, we are not only interested in optimizing ML pipelines solely for predictive accuracy; additional metrics or constraints must be considered when determining an optimal configuration, resulting in a multi-objective optimization problem. This is often neglected in practice, due to a lack of knowledge and readily available software implementations for multi-objective hyperparameter optimization. In this work, we introduce the reader to the basics of multi-objective hyperparameter optimization and motivate its usefulness in applied ML. Furthermore, we provide an extensive survey of existing optimization strategies from the domains of evolutionary algorithms and Bayesian optimization. We illustrate the utility of multi-objective optimization in several specific ML applications, considering objectives such as operating conditions, prediction time, sparseness, fairness, interpretability, and robustness.},
journal = {ACM Trans. Evol. Learn. Optim.},
month = dec,
articleno = {16},
numpages = {50},
keywords = {Multi-objective hyperparameter optimization, neural architecture search, Bayesian optimization}
}

@inproceedings{10.1145/3617651.3622994,
author = {Pe\v{c}im\'{u}th, Andrej and Leopoldseder, David and T\r{u}ma, Petr},
title = {Diagnosing Compiler Performance by Comparing Optimization Decisions},
year = {2023},
isbn = {9798400703805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617651.3622994},
doi = {10.1145/3617651.3622994},
abstract = {Modern compilers apply a set of optimization passes aiming to speed up the generated code. The combined effect of individual optimizations is difficult to predict. Thus, changes to a compiler's code may hinder the performance of generated code as an unintended consequence.  

Performance regressions in compiled code are often related to misapplied optimizations. The regressions are hard to investigate, considering the vast number of compilation units and applied optimizations. A compilation unit consists of a root method and inlined methods. Thus, a method may be part of several compilation units and may be optimized differently in each. Moreover, inlining decisions are not invariant across runs of the virtual machine (VM).  

We propose to solve the problem of diagnosing performance regressions by capturing the compiler's optimization decisions. We do so by representing the applied optimization phases, optimization decisions, and inlining decisions in the form of trees. This paper introduces an approach utilizing tree edit distance (TED) to detect optimization differences in a semi-automated way. We present an approach to compare optimization decisions in differently inlined methods. We employ these techniques to pinpoint the causes of performance problems in various benchmarks of the Graal compiler.},
booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Managed Programming Languages and Runtimes},
pages = {47–61},
numpages = {15},
keywords = {Virtual Machines, Tree Matching, Just-In-Time Compilation, Compiler Optimizations},
location = {Cascais, Portugal},
series = {MPLR 2023}
}

@inproceedings{10.1109/ASE56229.2023.00026,
author = {Stradowski, Szymon and Madeyski, Lech},
title = {Bridging the Gap between Academia and Industry in Machine Learning Software Defect Prediction: Thirteen Considerations},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00026},
doi = {10.1109/ASE56229.2023.00026},
abstract = {This experience paper describes thirteen considerations for implementing machine learning software defect prediction (ML SDP) in vivo. Specifically, we provide the following report on the ground of the most important observations and lessons learned gathered during a large-scale research effort and introduction of ML SDP to the system-level testing quality assurance process of one of the leading telecommunication vendors in the world --- Nokia. We adhere to a holistic and logical progression based on the principles of the business analysis body of knowledge: from identifying the need and setting requirements, through designing and implementing the solution, to profitability analysis, stakeholder management, and handover. Conversely, for many years, industry adoption has not kept up the pace of academic achievements in the field, despite promising potential to improve quality and decrease the cost of software products for many companies worldwide. Therefore, discussed considerations hopefully help researchers and practitioners bridge the gaps between academia and industry.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1098–1110},
numpages = {13},
keywords = {machine learning, software defect prediction, nokia 5G, industry introduction, experience paper},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@article{10.1145/3708990,
author = {Xu, Jia and Chen, Wenbin and Dai, Haipeng and Xu, Lijie and Xiao, Fu and Liu, Linfeng},
title = {Wireless Charging Scheduling for Long-term Utility Optimization},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1550-4859},
url = {https://doi.org/10.1145/3708990},
doi = {10.1145/3708990},
abstract = {Wireless power transmission has been widely used to replenish energy for wireless sensor networks, where the energy consumption rate of sensor nodes is usually time varying and indefinite. However, few works have investigated the problem of long-term charging scheduling with random variable. This article designs an optimization model for the long-term scheduling of chargers to maximize the time-averaged charging utility while ensuring its time-averaged constraints of budget and response rate. The Lyapunov optimization technique is adopted to transform the stochastic optimization problem into a deterministic optimization problem, which remains NP-hard. Thus, an approximation algorithm following greedy approach is proposed to solve the deterministic optimization problem. We further provide the theoretical analysis of feasibility and performance guarantee of the proposed scheduling algorithm. The simulation results show that our algorithm outperforms three comparison algorithms by 6.53%, 20.04%, and 19.97% in terms of time-averaged charging utility, as well as by 11.25%, 4.42%, and 3.73% in terms of time-averaged response rate on average.},
journal = {ACM Trans. Sen. Netw.},
month = jan,
articleno = {8},
numpages = {31},
keywords = {Wireless power transmission, wireless rechargeable sensor network, charging scheduling, Lyapunov optimization, submodular function maximization}
}

@article{10.1145/3505264,
author = {Vargas-Solar, Genoveva and Khalil, Maysaa and Espinosa-Oviedo, Javier A. and Zechinelli-Martini, Jos\'{e}-Luis},
title = {GREENHOME: A Household Energy Consumption and CO2 Footprint Metering Environment},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3505264},
doi = {10.1145/3505264},
abstract = {This article presents the GREENHOME environment, a toolkit providing several data analytical tools for metering household energy consumption and CO2 footprint under different perspectives. GREENHOME enables a multi-perspective analysis of household energy consumption and CO2 footprint using and combining several variables through various statistics and data mining algorithms. To test GREENHOME, the article reports on experiments conducted for modelling and forecasting energy consumption and CO2 footprint in the context of the Triple-A European project.},
journal = {ACM Trans. Internet Technol.},
month = jan,
articleno = {72},
numpages = {31},
keywords = {internet of things, big data, CO2 footprint, Energy consumption}
}

@inproceedings{10.1145/3674805.3686675,
author = {Le, Triet Huynh Minh and Babar, Muhammad Ali},
title = {Automatic Data Labeling for Software Vulnerability Prediction Models: How Far Are We?},
year = {2024},
isbn = {9798400710476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674805.3686675},
doi = {10.1145/3674805.3686675},
abstract = {Background: Software Vulnerability (SV) prediction needs large-sized and high-quality data to perform well. Current SV datasets mostly require expensive labeling efforts by experts (human-labeled) and thus are limited in size. Meanwhile, there are growing efforts in automatic SV labeling at scale. However, the fitness of auto-labeled data for SV prediction is still largely unknown. Aims: We quantitatively and qualitatively study the quality and use of the state-of-the-art auto-labeled SV data, D2A, for SV prediction. Method: Using multiple sources and manual validation, we curate clean SV data from human-labeled SV-fixing commits in two well-known projects for investigating the auto-labeled counterparts. Results: We discover that 50+% of the auto-labeled SVs are noisy (incorrectly labeled), and they hardly overlap with the publicly reported ones. Yet, SV prediction models utilizing the noisy auto-labeled SVs can perform up to 22% and 90% better in Matthews Correlation Coefficient and Recall, respectively, than the original models. We also reveal the promises and difficulties of applying noise-reduction methods for automatically addressing the noise in auto-labeled SV data to maximize the data utilization for SV prediction. Conclusions: Our study informs the benefits and challenges of using auto-labeled SVs, paving the way for large-scale SV prediction.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {131–142},
numpages = {12},
keywords = {Data quality, Empirical study, Machine learning, Software security, Software vulnerability},
location = {Barcelona, Spain},
series = {ESEM '24}
}

@article{10.1145/3708526,
author = {Moreira, Ana and Lago, Patricia and Heldal, Rogardt and Betz, Stefanie and Brooks, Ian and Capilla, Rafael and Coroam\u{a}, Vlad Constantin and Duboc, Leticia and Fernandes, Jo\~{a}o Paulo and Leifler, Ola and Nguyen, Ngoc-Thanh and Oyedeji, Shola and Penzenstadler, Birgit and Peters, Anne-Kathrin and Porras, Jari and Venters, Colin C.},
title = {A Roadmap for Integrating Sustainability into Software Engineering Education},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708526},
doi = {10.1145/3708526},
abstract = {The world faces escalating crises: record-breaking temperatures, widespread fires, severe flooding, increased oceanic microplastics, and unequal resource distribution. Academia introduces courses around sustainability to meet the new demand, but software engineering education lags behind. While software systems contribute to environmental issues through high energy consumption, they also hold the potential for solutions, such as more efficient and equitable resource management. Yet, sustainability remains a low priority for many businesses, including those in the digital sector. Business as usual is no longer viable. A transformational change in software engineering education is urgently needed. We must move beyond traditional curriculum models and fully integrate sustainability into every aspect of software development. By embedding sustainability as a core competency, we can equip future engineers not only to minimise harm but also to innovate solutions that drive positive, sustainable change. Only with such a shift can software engineering education meet the demands of a world in crisis and prepare students to lead the next generation of sustainable technology. This paper discusses a set of challenges and proposes a customisable education roadmap for integrating sustainability into the software engineering curricula. These challenges reflect our perspective on key considerations, stemming from regular, intensive discussions in regular workshops among the authors and the community, as well as our extensive research and teaching experience in the field.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {Software engineering, Sustainability, Computing, Education, Software sustainability, Sustainable software, Sustainable development goals, Software competencies, Sustainability skills}
}

@inproceedings{10.1145/3603166.3632544,
author = {Leghari, Ahmed Khan and M\o{}llenbach, Emilie},
title = {IoT Based Closed Loop Production Optimization using MQTT and Rule-based System},
year = {2024},
isbn = {9798400702341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603166.3632544},
doi = {10.1145/3603166.3632544},
abstract = {The growing competition by cloud service providers to make IoT products affordable and accessible have opened many doors of opportunities for small and medium enterprises, ambitious to digitalize their operations. One such area where most of the companies are investing is IoT based production optimization. There are several methods of production optimization, an important method is to introduce IoT based close-loop control systems to reduce any manual involvement in production operations. This paper presents an architecture for implementing IoT based closed loop control, relevant implementation, and some useful tools which can be used in variety of other IoT applications as well.},
booktitle = {Proceedings of the IEEE/ACM 16th International Conference on Utility and Cloud Computing},
articleno = {43},
numpages = {6},
location = {Taormina (Messina), Italy},
series = {UCC '23}
}

@inproceedings{10.1145/1368088.1368131,
author = {K\"{a}stner, Christian and Apel, Sven and Kuhlemann, Martin},
title = {Granularity in software product lines},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1368088.1368131},
doi = {10.1145/1368088.1368131},
abstract = {Building software product lines (SPLs) with features is a challenging task. Many SPL implementations support features with coarse granularity - e.g., the ability to add and wrap entire methods. However, fine-grained extensions, like adding a statement in the middle of a method, either require intricate workarounds or obfuscate the base code with annotations. Though many SPLs can and have been implemented with the coarse granularity of existing approaches, fine-grained extensions are essential when extracting features from legacy applications. Furthermore, also some existing SPLs could benefit from fine-grained extensions to reduce code replication or improve readability. In this paper, we analyze the effects of feature granularity in SPLs and present a tool, called Colored IDE (CIDE), that allows features to implement coarse-grained and fine-grained extensions in a concise way. In two case studies, we show how CIDE simplifies SPL development compared to traditional approaches.},
booktitle = {Proceedings of the 30th International Conference on Software Engineering},
pages = {311–320},
numpages = {10},
keywords = {virtual separation of concerns, software product lines, ide, feature refactoring},
location = {Leipzig, Germany},
series = {ICSE '08}
}

@inproceedings{10.1145/1858996.1859009,
author = {Vierhauser, Michael and Gr\"{u}nbacher, Paul and Egyed, Alexander and Rabiser, Rick and Heider, Wolfgang},
title = {Flexible and scalable consistency checking on product line variability models},
year = {2010},
isbn = {9781450301169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858996.1859009},
doi = {10.1145/1858996.1859009},
abstract = {The complexity of product line variability models makes it hard to maintain their consistency over time regardless of the modeling approach used. Engineers thus need support for detecting and resolving inconsistencies. We describe experiences of applying a tool-supported approach for incremental consistency checking on variability models. Our approach significantly improves the overall performance and scalability compared to batch-oriented techniques and allows providing immediate feedback to modelers. It is extensible as new consistency constraints can easily be added. Furthermore, the approach is flexible as it is not limited to variability models and it also checks the consistency of the models with the underlying code base of the product line. We report the results of a thorough evaluation based on real-world product line models and discuss lessons learned.},
booktitle = {Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering},
pages = {63–72},
numpages = {10},
keywords = {variability models, software product lines, performance, model consistency, memory consumption, lessons learned, incremental consistency checking},
location = {Antwerp, Belgium},
series = {ASE '10}
}

@inproceedings{10.1145/3622781.3674188,
author = {Xie, Zifan and Wen, Ming and Qiu, Shiyu and Jin, Hai},
title = {Validating JVM Compilers via Maximizing Optimization Interactions},
year = {2025},
isbn = {9798400703911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622781.3674188},
doi = {10.1145/3622781.3674188},
abstract = {This paper introduces the concept of optimization interaction, which refers to the practice in modern compilers where multiple optimization phases, such as inlining, loop unrolling, and dead code elimination, are not completed in a one-off sequential order while being interacted instead. Therefore, while optimizing a certain phase, the compiler needs to ensure that the results of other optimization phases will not be disrupted, as this could lead to compiler crashes or unpredictable results. To verify whether compilers can correctly handle the optimization process across various phases, we propose MopFuzzer, which aims at maximizing runtime optimization interactions during fuzzing. Specifically, it encourages the JVM to perform multi-stage optimizations and verifies the correctness of the compiler's optimized code through differential testing. Currently, MopFuzzer has implemented 13 mutators, and each is intended to trigger a certain optimization behavior. Such mutators are applied iteratively to the same program point, aiming to maximize optimization interactions. Subsequently, the testing process is guided by a novel method based on profile data, which records the optimization behaviors performed by the compiler. The guidance enables MopFuzzer to generate mutants that are able to maximize optimization behaviors and their interactions. Our evaluation has led to 59 bug reports for widely used production JVMs, OpenJDK and OpenJ9.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4},
pages = {345–360},
numpages = {16},
keywords = {JVM, JIT compiler, optimization},
location = {Hilton La Jolla Torrey Pines, La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/2897937.2898044,
author = {Shafaei, Alireza and Afzali-Kusha, Hassan and Pedram, Massoud},
title = {Minimizing the energy-delay product of SRAM arrays using a device-circuit-architecture co-optimization framework},
year = {2016},
isbn = {9781450342360},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897937.2898044},
doi = {10.1145/2897937.2898044},
abstract = {The objective of this paper is to minimize the energy-delay product of static random access memory (SRAM) arrays by using a device-circuit architecture co-optimization framework. More specifically, at the device-level, high-Vt FinFETs are adopted for the 6T SRAM cell, which significantly reduces the leakage power and improves static noise margins. However, due to the lower ON current, the bit-line delay of the read access is increased. Accordingly, at the circuit-level, the voltage level of assist circuits, and at the architecture-level (i.e., the array organization), key parameters of the SRAM array are jointly optimized to derive a design that results in the minimum energy-delay product point. By using the proposed optimization framework, for SRAM array capacities ranging from 1KB to 16KB, on average 59% lower energy-delay product with maximum 12% (and on average 9%) performance penalty is achieved.},
booktitle = {Proceedings of the 53rd Annual Design Automation Conference},
articleno = {107},
numpages = {6},
keywords = {energy-efficient memory design, assist techniques, SRAM array},
location = {Austin, Texas},
series = {DAC '16}
}

@article{10.1145/3570638,
author = {Hijma, Pieter and Heldens, Stijn and Sclocco, Alessio and van Werkhoven, Ben and Bal, Henri E.},
title = {Optimization Techniques for GPU Programming},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {11},
issn = {0360-0300},
url = {https://doi.org/10.1145/3570638},
doi = {10.1145/3570638},
abstract = {In the past decade, Graphics Processing Units have played an important role in the field of high-performance computing and they still advance new fields such as IoT, autonomous vehicles, and exascale computing. It is therefore important to understand how to extract performance from these processors, something that is not trivial. This survey discusses various optimization techniques found in 450 articles published in the last 14 years. We analyze the optimizations from different perspectives which shows that the various optimizations are highly interrelated, explaining the need for techniques such as auto-tuning.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {239},
numpages = {81},
keywords = {performance bottleneck, optimization techniques, optimization, GPU, Survey}
}

@article{10.1145/3510426,
author = {Wang, Hao and Vermetten, Diederick and Ye, Furong and Doerr, Carola and B\"{a}ck, Thomas},
title = {IOHanalyzer: Detailed Performance Analyses for Iterative Optimization Heuristics},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3510426},
doi = {10.1145/3510426},
abstract = {Benchmarking and performance analysis play an important role in understanding the behaviour of iterative optimization heuristics (IOHs) such as local search algorithms, genetic and evolutionary algorithms, Bayesian optimization algorithms, etc. This task, however, involves manual setup, execution, and analysis of the experiment on an individual basis, which is laborious and can be mitigated by a generic and well-designed platform. For this purpose, we propose IOHanalyzer, a new user-friendly tool for the analysis, comparison, and visualization of performance data of IOHs.Implemented in R and C++, IOHanalyzer is fully open source. It is available on CRAN and GitHub. IOHanalyzer provides detailed statistics about fixed-target running times and about fixed-budget performance of the benchmarked algorithms with a real-valued codomain, single-objective optimization tasks. Performance aggregation over several benchmark problems is possible, for example in the form of empirical cumulative distribution functions. Key advantages of IOHanalyzer over other performance analysis packages are its highly interactive design, which allows users to specify the performance measures, ranges, and granularity that are most useful for their experiments, and the possibility to analyze not only performance traces, but also the evolution of dynamic state parameters.IOHanalyzer can directly process performance data from the main benchmarking platforms, including the COCO platform, Nevergrad, the SOS platform, and IOHexperimenter. An R programming interface is provided for users preferring to have a finer control over the implemented functionalities.},
journal = {ACM Trans. Evol. Learn. Optim.},
month = apr,
articleno = {3},
numpages = {29},
keywords = {Evolutionary computation, black-box optimization, performance analysis}
}

@article{10.1145/3702314,
author = {De Sutter, Bjorn and Schrittwieser, Sebastian and Coppens, Bart and Kochberger, Patrick},
title = {Evaluation Methodologies in Software Protection Research},
year = {2024},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3702314},
doi = {10.1145/3702314},
abstract = {Man-at-the-end (MATE) attackers have full control over the system on which the attacked software runs, and try to break the confidentiality or integrity of assets embedded in the software. Both companies and malware authors want to prevent such attacks. This has driven an arms race between attackers and defenders, resulting in a plethora of different protection and analysis methods. However, it remains difficult to measure the strength of protections because MATE attackers can reach their goals in many different ways and a universally accepted evaluation methodology does not exist. This survey systematically reviews the evaluation methodologies of papers on obfuscation, a major class of protections against MATE attacks. For 571 papers, we collected 113 aspects of their evaluation methodologies, ranging from sample set types and sizes, over sample treatment, to performed measurements. We provide detailed insights into how the academic state of the art evaluates both the protections and analyses thereon. In summary, there is a clear need for better evaluation methodologies. We identify nine challenges for software protection evaluations, which represent threats to the validity, reproducibility, and interpretation of research results in the context of MATE attacks and formulate a number of concrete recommendations for improving the evaluations reported in future research papers.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {86},
numpages = {41},
keywords = {Survey, software protection, obfuscation, deobfuscation, diversification}
}

@inproceedings{10.5555/2486788.2486851,
author = {Cordy, Maxime and Schobbens, Pierre-Yves and Heymans, Patrick and Legay, Axel},
title = {Beyond boolean product-line model checking: dealing with feature attributes and multi-features},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Model checking techniques for software product lines (SPL) are actively researched. A major limitation they currently have is the inability to deal efficiently with non-Boolean features and multi-features. An example of a non-Boolean feature is a numeric attribute such as maximum number of users which can take different numeric values across the range of SPL products. Multi-features are features that can appear several times in the same product, such as processing units which number is variable from one product to another and which can be configured independently. Both constructs are extensively used in practice but currently not supported by existing SPL model checking techniques. To overcome this limitation, we formally define a language that integrates these constructs with SPL behavioural specifications. We generalize SPL model checking algorithms correspondingly and evaluate their applicability. Our results show that the algorithms remain efficient despite the generalization.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {472–481},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/3669940.3707249,
author = {Narayan, Shravan and Garfinkel, Tal and Johnson, Evan and Yedidia, Zachary and Wang, Yingchen and Brown, Andrew and Vahldiek-Oberwagner, Anjo and LeMay, Michael and Huang, Wenyong and Wang, Xin and Sun, Mingqiu and Tullsen, Dean and Stefan, Deian},
title = {Segue &amp; ColorGuard: Optimizing SFI Performance and Scalability on Modern Architectures},
year = {2025},
isbn = {9798400706981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3669940.3707249},
doi = {10.1145/3669940.3707249},
abstract = {Software-based fault isolation (SFI) enables in-process isolation through compiler instrumentation of memory accesses, and is a critical part of WebAssembly (Wasm). We present two optimizations that improve SFI performance and scalability: Segue uses x86-64 segmentation to reduce the cost of instrumentation on memory accesses, e.g., it eliminates 44.7% of Wasm's overhead on a Wasm-compatible subset of SPEC CPU 2006, and reduces overhead of Wasm-sandboxed font rendering in Firefox by 75%; ColorGuard leverages memory tagging (e.g., MPK), to enable up to a 15\texttimes{} increase in the number of Wasm instances that can run concurrently in a single address space, improving efficiency for high scale server-side workloads. We also explore the challenges of deploying these optimizations in three production toolchains: Wasm2c, WAMR and Wasmtime.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1},
pages = {987–1002},
numpages = {16},
keywords = {optimization, sandboxing, sfi, wasm},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/3650215.3650257,
author = {Wang, Zhi and Li, Yayun and Liang, Fuwei and Xu, Hui and Shi, Xinwei},
title = {Ethylene production prediction using Attention-LSTM model based on Golden jackal optimization},
year = {2024},
isbn = {9798400709449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650215.3650257},
doi = {10.1145/3650215.3650257},
abstract = {As one of the most important petrochemical products, ethylene is widely used in daily life. Due to the complexity of the industrial ethylene production process, there are inaccurate problems in the application of neural network models for production prediction. Therefore, a prediction method based on attention mechanism-based long short-term memory neural network combined with golden jackal optimization algorithm is proposed to realize accurate prediction of output in ethylene production process. LSTM extracts important features of ethylene data to predict ethylene production. The attention mechanism makes the model more flexible to learn important features of the input data and improves the representation ability of LSTM. Then, GJO optimizes the hyperparameters of LSTM, sets the number of hidden layer nodes, batch size and training times as optimization variables, and the root mean square error (RMSE) as the optimization goal to obtain the optimal hyperparameters of the Attention-LSTM model. Compared with Back Propagation network, Random Forest algorithm and LSTM, GJO-ALSTM achieves higher prediction accuracy.},
booktitle = {Proceedings of the 2023 4th International Conference on Machine Learning and Computer Application},
pages = {239–244},
numpages = {6},
location = {Hangzhou, China},
series = {ICMLCA '23}
}

@inproceedings{10.1145/3674225.3674281,
author = {Yan, Tingting and Fu, Yunxi and Gao, Ziting and Yan, Chenyang and Zhang, Wenjuan and Zhang, Min and Wang, Zhenya},
title = {A simulation platform for optimization scheduling of comprehensive energy system},
year = {2024},
isbn = {9798400716638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674225.3674281},
doi = {10.1145/3674225.3674281},
abstract = {The optimization scheduling of the integrated energy system is based on the topology data of the integrated energy system as well as the electricity, cooling, heating, forecast loads during the scheduling period, and the forecast output values of wind power and photovoltaic. Scheduling simulation calculation is constrained by the safety and stable operation of the integrated energy system, and takes economic or green operation as the optimization objective. The final generation of this calculation are scheduling plans for various types of equipment in the comprehensive energy system within a given scheduling time period, as well as simulation results of the distribution of multi energy flow and carbon emission flow at each time section. The power flow simulation algorithm of the integrated energy system is mainly including power flow calculation and economic dispatch calculation. The power flow calculation module adopts a sequential iteration method based on the Newton-Raphson method for the decomposition of the electric thermal sub network, achieving fast and reliable evaluation of the comprehensive energy system power flow. The economic dispatch calculation module is based on the open-source Python optimization package Pyomo to establish a refined scheduling model for comprehensive energy systems. At the same time, we independently designed and developed a linear programming accelerated solution algorithm based on the improved simplicity method.},
booktitle = {Proceedings of the 2024 International Conference on Power Electronics and Artificial Intelligence},
pages = {303–309},
numpages = {7},
location = {Xiamen, China},
series = {PEAI '24}
}

@inproceedings{10.1109/PACT52795.2021.00016,
author = {Baek, Daehyeon and Hwang, Soojin and Heo, Taekyung and Kim, Daehoon and Huh, Jaehyuk},
title = {InnerSP: A Memory Efficient Sparse Matrix Multiplication Accelerator with Locality-aware Inner Product Processing},
year = {2024},
isbn = {9781665442787},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/PACT52795.2021.00016},
doi = {10.1109/PACT52795.2021.00016},
abstract = {Sparse matrix multiplication is one of the key computational kernels in large-scale data analytics. However, a naive implementation suffers from the overheads of irregular memory accesses due to the representation of sparsity. To mitigate the memory access overheads, recent accelerator designs advocated the outer product processing which minimizes input accesses but generates intermediate products to be merged to the final output matrix. Using real-world sparse matrices, this study first identifies the memory bloating problem of the outer product designs due to the unpredictable intermediate products. Such an unpredictable increase in memory requirement during computation can limit the applicability of accelerators. To address the memory bloating problem, this study revisits an alternative inner product approach, and proposes a new accelerator design called InnerSP. This study shows that nonzero element distributions in real-world sparse matrices have a certain level of locality. Using a smart caching scheme designed for inner product, the locality is effectively exploited with a modest on-chip cache. However, the row-wise inner product relies on on-chip aggregation of intermediate products. Due to uneven sparsity per row, overflows or underflows of the on-chip storage for aggregation can occur. To maximize the parallelism while avoiding costly overflows, the proposed accelerator uses pre-scanning for row splitting and merging. The simulation results show that the performance of InnerSP can exceed or be similar to those of the prior outer product approaches without any memory bloating problem.},
booktitle = {Proceedings of the 30th International Conference on Parallel Architectures and Compilation Techniques},
pages = {116–128},
numpages = {13},
keywords = {hardware accelerator, inner product, sparse matrix multiplication},
location = {Atlanta, GA, USA},
series = {PACT '21}
}

@inproceedings{10.1109/SC41406.2024.00022,
author = {Lazuka, Malgorzata and Anghel, Andreea and Parnell, Thomas},
title = {LLM-Pilot: Characterize and Optimize Performance of your LLM Inference Services},
year = {2024},
isbn = {9798350352917},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC41406.2024.00022},
doi = {10.1109/SC41406.2024.00022},
abstract = {As Large Language Models (LLMs) are rapidly growing in popularity, LLM inference services must be able to serve requests from thousands of users while satisfying performance requirements. The performance of an LLM inference service is largely determined by the hardware onto which it is deployed, but understanding of which hardware will deliver on performance requirements remains challenging. In this work we present LLM-Pilot - a first-of-its-kind system for characterizing and predicting performance of LLM inference services. LLM-Pilot performs benchmarking of LLM inference services, under a realistic workload, across a variety of GPUs, and optimizes the service configuration for each considered GPU to maximize performance. Finally, using this characterization data, LLM-Pilot learns a predictive model, which can be used to recommend the most cost-effective hardware for a previously unseen LLM. Compared to existing methods, LLM-Pilot can deliver on performance requirements 33% more frequently, whilst reducing costs by 60% on average.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {16},
numpages = {18},
keywords = {benchmarking, inference services, large language models, performance, prediction},
location = {Atlanta, GA, USA},
series = {SC '24}
}

@inproceedings{10.1145/3674805.3686670,
author = {Nguyen, Anh The and Le, Triet Huynh Minh and Babar, M. Ali},
title = {Automated Code-centric Software Vulnerability Assessment: How Far Are We? An Empirical Study in C/C++},
year = {2024},
isbn = {9798400710476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674805.3686670},
doi = {10.1145/3674805.3686670},
abstract = {Background: The C/C++ languages hold significant importance in Software Engineering research because of their widespread use in practice. Numerous studies have utilized Machine Learning (ML) and Deep Learning (DL) techniques to detect software vulnerabilities (SVs) in the source code written in these languages. However, the application of these techniques in function-level SV assessment has been largely unexplored. SV assessment is increasingly crucial as it provides detailed information on the exploitability, impacts, and severity of security defects, thereby aiding in their prioritization and remediation. Aims: We conduct the first empirical study to investigate and compare the performance of ML and DL models, many of which have been used for SV detection, for function-level SV assessment in C/C++. Method: Using 9,993 vulnerable C/C++ functions, we evaluated the performance of six multi-class ML models and five multi-class DL models for the SV assessment at the function level based on the Common Vulnerability Scoring System (CVSS). We further explore multi-task learning, which can leverage common vulnerable code to predict all SV assessment outputs simultaneously in a single model, and compare the effectiveness and efficiency of this model type with those of the original multi-class models. Results: We show that ML has matching or even better performance compared to the multi-class DL models for function-level SV assessment with significantly less training time. Employing multi-task learning allows the DL models to perform significantly better, with an average of 8–22% increase in Matthews Correlation Coefficient (MCC), than the multi-class models. Conclusions: We distill the practices of using data-driven techniques for function-level SV assessment in C/C++, including the use of multi-task DL to balance efficiency and effectiveness. This can establish a strong foundation for future work in this area.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {72–83},
numpages = {12},
keywords = {Deep Learning, Machine Learning, Mining Software Repositories, Security Vulnerability, Vulnerability Assessment},
location = {Barcelona, Spain},
series = {ESEM '24}
}

@inproceedings{10.1145/1353482.1353496,
author = {Chakravarthy, Venkat and Regehr, John and Eide, Eric},
title = {Edicts: implementing features with flexible binding times},
year = {2008},
isbn = {9781605580449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1353482.1353496},
doi = {10.1145/1353482.1353496},
abstract = {In a software product line, the binding time of a feature is the time at which one decides to include or exclude a feature from a product. Typical binding site implementations are intended to support a single binding time only, e.g., compile time or run time. Sometimes, however, a product line must support features with variable binding times. For instance, a product line may need to include both embedded system configurations, in which features are selected and optimized early, and desktop configurations, in which client programs choose features on demand.We present a new technique for implementing the binding sites of features that require flexible binding times. Our technique combines design patterns and aspect-oriented programming: a pattern encapsulates the variation point, and targeted aspects---called edicts---set the binding times of the pattern participants. We describe our approach and demonstrate its usefulness by creating a middleware product line capable of serving the desktop and embedded domains. Our product line is based on JacORB, a middleware platform with many dynamically configurable features. By using edicts to select features at compile time, we create a version of JacORB more suited to resource-constrained environments. By configuring four JacORB subsystems via edicts, we achieve a 32.2% reduction in code size. Our examples show that our technique effectively modularizes binding-time concerns, supporting both compile-time optimization and run-time flexibility as needed.},
booktitle = {Proceedings of the 7th International Conference on Aspect-Oriented Software Development},
pages = {108–119},
numpages = {12},
location = {Brussels, Belgium},
series = {AOSD '08}
}

@article{10.1145/3626712,
author = {Huang, Yicong and Wang, Zuozhi and Li, Chen},
title = {Udon: Efficient Debugging of User-Defined Functions in Big Data Systems with Line-by-Line Control},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
url = {https://doi.org/10.1145/3626712},
doi = {10.1145/3626712},
abstract = {Many big data systems are written in languages such as C, C++, Java, and Scala to process large amounts of data efficiently, while data analysts often use Python to conduct data wrangling, statistical analysis, and machine learning. User-defined functions (UDFs) are commonly used in these systems to bridge the gap between the two ecosystems. In this paper, we propose Udon, a novel debugger to support fine-grained debugging of UDFs. Udon encapsulates the modern line-by-line debugging primitives, such as the ability to set breakpoints, perform code inspections, and make code modifications while executing a UDF on a single tuple. It includes a novel debug-aware UDF execution model to ensure the responsiveness of the operator during debugging. It utilizes advanced state-transfer techniques to satisfy breakpoint conditions that span across multiple UDFs. It incorporates various optimization techniques to reduce the runtime overhead. We conduct experiments with multiple UDF workloads on various datasets and show its high efficiency and scalability.},
journal = {Proc. ACM Manag. Data},
month = dec,
articleno = {225},
numpages = {26},
keywords = {big data systems, debugging, user-defined functions (UDFs)}
}

@article{10.1145/3428492,
author = {Kumari, Moothedath Chandran and Sagar, Biradar},
title = {Global Pandemic and Rapid New Product Development of Medical Products},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1145/3428492},
doi = {10.1145/3428492},
abstract = {New product development is a strenuous and long process for companies. When the COVID-19 disease turned into a global pandemic, governments announced the need for various medical products that were scarce and at the same time imposed lockdowns and trade restrictions. During this period, rapid new product development of medical products was attempted by many companies worldwide using their existing supply chains. Data of 240 companies from non-medical product-segment background launching new medical products were collected and analyzed. Two case studies of Indian companies responding to shortages of ventilators and PPE are presented. A critical reflection of a new way of new product development and cooperation across firms is discussed.},
journal = {Digit. Gov.: Res. Pract.},
month = dec,
articleno = {17},
numpages = {38},
keywords = {new product development, medical product design, innovation, industrial resilience, global governance, COVID -19}
}

@article{10.1145/3662180,
author = {Lipp, Lukas and Hahn, David and Ecormier-Nocca, Pierre and Rist, Florian and Wimmer, Michael},
title = {View-Independent Adjoint Light Tracing for Lighting Design Optimization},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/3662180},
doi = {10.1145/3662180},
abstract = {Differentiable rendering methods promise the ability to optimize various parameters of three-dimensional (3D) scenes to achieve a desired result. However, lighting design has so far received little attention in this field. In this article, we introduce a method that enables continuous optimization of the arrangement of luminaires in a 3D scene via differentiable light tracing. Our experiments show two major issues when attempting to apply existing methods from differentiable path tracing to this problem: First, many rendering methods produce images, which restricts the ability of a designer to define lighting objectives to image space. Second, most previous methods are designed for scene geometry or material optimization and have not been extensively tested for the case of optimizing light sources. Currently available differentiable ray-tracing methods do not provide satisfactory performance, even on fairly basic test cases in our experience. In this article, we propose, to the best of our knowledge, a novel adjoint light tracing method that overcomes these challenges and enables gradient-based lighting design optimization in a view-independent (camera-free) way. Thus, we allow the user to paint illumination targets directly onto the 3D scene or use existing baked illumination data (e.g.,&nbsp;light maps). Using modern ray-tracing hardware, we achieve interactive performance. We find light tracing advantageous over path tracing in this setting, as it naturally handles irregular geometry, resulting in less noise and improved optimization convergence. We compare our adjoint gradients to state-of-the-art image-based differentiable rendering methods. We also demonstrate that our gradient data works with various common optimization algorithms, providing good convergence behaviour. Qualitative comparisons with real-world scenes underline the practical applicability of our method.},
journal = {ACM Trans. Graph.},
month = may,
articleno = {35},
numpages = {16},
keywords = {Lighting design, differentiable rendering, global illumination, optimization, ray tracing}
}

@inproceedings{10.5555/3571885.3571970,
author = {Schaad, Philipp and Ben-Nun, Tal and Hoefler, Torsten},
title = {Boosting performance optimization with interactive data movement visualization},
year = {2022},
isbn = {9784665454445},
publisher = {IEEE Press},
abstract = {Optimizing application performance in today's hardware architecture landscape is an important, but increasingly complex task, often requiring detailed performance analyses. In particular, data movement and reuse play a crucial role in optimization and are often hard to improve without detailed program inspection. Performance visualizations can assist in the diagnosis of performance problems, but generally rely on data gathered through lengthy program executions. In this paper, we present a performance visualization geared towards analyzing data movement and reuse to inform impactful optimization decisions, without requiring program execution. We propose an approach that combines static dataflow analysis with parameterized program simulations to analyze both global data movement and fine-grained data access and reuse behavior, and visualize insights in-situ on the program representation. Case studies analyzing and optimizing real-world applications demonstrate our tool's effectiveness in guiding optimization decisions and making the performance tuning process more interactive.},
booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
articleno = {64},
numpages = {16},
keywords = {software performance, performance analysis},
location = {Dallas, Texas},
series = {SC '22}
}

@inproceedings{10.1145/3637528.3672353,
author = {Zhou, Hao and Huang, Rongxiao and Li, Shaoming and Jiang, Guibin and Zheng, Jiaqi and Cheng, Bing and Lin, Wei},
title = {Decision Focused Causal Learning for Direct Counterfactual Marketing Optimization},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3672353},
doi = {10.1145/3637528.3672353},
abstract = {Marketing optimization plays an important role to enhance user engagement in online Internet platforms. Existing studies usually formulate this problem as a budget allocation problem and solve it by utilizing two fully decoupled stages, i.e., machine learning (ML) and operation research (OR). However, the learning objective in ML does not take account of the downstream optimization task in OR, which causes that the prediction accuracy in ML may be not positively related to the decision quality.Decision Focused Learning (DFL) integrates ML and OR into an end-to-end framework, which takes the objective of the downstream task as the decision loss function and guarantees the consistency of the optimization direction between ML and OR. However, deploying DFL in marketing is non-trivial due to multiple technological challenges. Firstly, the budget allocation problem in marketing is a 0-1 integer stochastic programming problem and the budget is uncertain and fluctuates a lot in real-world settings, which is beyond the general problem background in DFL. Secondly, the counterfactual in marketing causes that the decision loss cannot be directly computed and the optimal solution can never be obtained, both of which disable the common gradient-estimation approaches in DFL. Thirdly, the OR solver is called frequently to compute the decision loss during model training in DFL, which produces huge computational cost and cannot support large-scale training data. In this paper, we propose a decision focused causal learning framework (DFCL) for direct counterfactual marketing optimization, which overcomes the above technological challenges. Both offline experiments and online A/B testing demonstrate the effectiveness of DFCL over the state-of-the-art methods. Currently, DFCL has been deployed in several marketing scenarios in Meituan, one of the largest online food delivery platform in the world.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {6368–6379},
numpages = {12},
keywords = {causal inference, decision focused learning, marketing optimization},
location = {Barcelona, Spain},
series = {KDD '24}
}

@article{10.1145/3643678,
author = {Hu, Qiang and Guo, Yuejun and Xie, Xiaofei and Cordy, Maxime and Ma, Lei and Papadakis, Mike and Le Traon, Yves},
title = {Test Optimization in DNN Testing: A Survey},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3643678},
doi = {10.1145/3643678},
abstract = {This article presents a comprehensive survey on test optimization in deep neural network&nbsp;(DNN) testing. Here, test optimization refers to testing with low data labeling effort. We analyzed 90 papers, including 43 from the software engineering (SE) community, 32 from the machine learning (ML) community, and 15 from other communities. Our study: (i) unifies the problems as well as terminologies associated with low-labeling cost testing, (ii) compares the distinct focal points of SE and ML communities, and (iii) reveals the pitfalls in existing literature. Furthermore, we highlight the research opportunities in this domain.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {111},
numpages = {42},
keywords = {Test optimization, DNN testing, low-labeling cost}
}

@article{10.1145/3641541,
author = {Ollando, Rapha\"{e}l and Shin, Seung Yeob and Briand, Lionel C.},
title = {Learning Failure-Inducing Models for Testing Software-Defined Networks},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3641541},
doi = {10.1145/3641541},
abstract = {Software-defined networks (SDN) enable flexible and effective communication systems that are managed by centralized software controllers. However, such a controller can undermine the underlying communication network of an SDN-based system and thus must be carefully tested. When an SDN-based system fails, in order to address such a failure, engineers need to precisely understand the conditions under which it occurs. In this article, we introduce a machine learning-guided fuzzing method, named FuzzSDN, aiming at both (1)&nbsp;generating effective test data leading to failures in SDN-based systems and (2)&nbsp;learning accurate failure-inducing models that characterize conditions under which such system fails. To our knowledge, no existing work simultaneously addresses these two objectives for SDNs. We evaluate FuzzSDN by applying it to systems controlled by two open-source SDN controllers. Furthermore, we compare FuzzSDN with two state-of-the-art methods for fuzzing SDNs and two baselines for learning failure-inducing models. Our results show that (1)&nbsp;compared to the state-of-the-art methods, FuzzSDN generates at least 12 times more failures, within the same time budget, with a controller that is fairly robust to fuzzing and (2)&nbsp;our failure-inducing models have, on average, a precision of 98% and a recall of 86%, significantly outperforming the baselines.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {113},
numpages = {25},
keywords = {Software-defined networks, software testing, fuzzing, machine learning}
}

@article{10.1145/3678165,
author = {Porter, Barry and Faulkner Rainford, Penn and Rodrigues-Filho, Roberto},
title = {Self-Designing Software},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {68},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/3678165},
doi = {10.1145/3678165},
abstract = {Exploring ways to include a software system as an active member of its own design team, able to reason about its own design and to synthesize better variants of its own building blocks as it encounters different deployment conditions.},
journal = {Commun. ACM},
month = dec,
pages = {50–59},
numpages = {10}
}

@article{10.1145/3680465,
author = {Sundermann, Chico and Raab, Heiko and He\ss{}, Tobias and Th\"{u}m, Thomas and Schaefer, Ina},
title = {Reusing d-DNNFs for Efficient Feature-Model Counting},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {8},
issn = {1049-331X},
url = {https://doi.org/10.1145/3680465},
doi = {10.1145/3680465},
abstract = {Feature models are commonly used to specify valid configurations of a product line. In industry, feature models are often complex due to numerous features and constraints. Thus, a multitude of automated analyses have been proposed. Many of those rely on computing the number of valid configurations, which typically depends on solving a #SAT problem, a computationally expensive operation. Even worse, most counting-based analyses require evaluation for multiple features or partial configurations resulting in numerous #SAT computations on the same feature model. Instead of repetitive computations on highly similar formulas, we aim to improve the performance by reusing knowledge between these computations. In this work, we are the first to propose reusing d-DNNFs for performing repetitive counting queries on features and partial configurations. In our experiments, reusing d-DNNFs saved up-to  (sim) 99.98% compared to repetitive invocations of #SAT solvers even when including compilation times. Overall, our tool ddnnife combined with the d-DNNF compiler d4 appears to be the most promising option when dealing with many repetitive feature-model counting queries.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {208},
numpages = {32},
keywords = {d-DNNF, feature model, product line, knowledge compilation, model counting, #SAT}
}

@inproceedings{10.1145/3689492.3690051,
author = {Bragg, Nate F. F. and Foster, Jeffrey S. and Zucker, Philip},
title = {Scimitar: Functional Programs as Optimization Problems},
year = {2024},
isbn = {9798400712159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689492.3690051},
doi = {10.1145/3689492.3690051},
abstract = {Mixed integer linear programming is a powerful and widely used approach to solving optimization problems, but its expressiveness is limited.
 
 
 
 
 
 
 

 
 
 
 
 
 
 
In this paper we introduce the optimization-aided language Scimitar, which encodes optimization problems using an expressive functional language, with a compiler that targets a mixed integer linear program solver.
 
 
 
 
 
 
 

 
 
 
 
 
 
 
Scimitar provides easy access to encoding techniques that normally require expert knowledge, enabling solve-time conditional constraints, inlining, loop unrolling, and many other high-level language constructs.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
We give operational semantics for Scimitar and constraint encodings of various features.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
To demonstrate Scimitar, we present a number of examples and benchmarks including classic optimization domains and more complex problems.
 
 
 
 
 
 
 

 
 
 
 
 
 
 
Our results indicate that Scimitar's use of a dedicated MILP solver is effective for expressively modeling optimization problems embedded within functional programs.},
booktitle = {Proceedings of the 2024 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {96–112},
numpages = {17},
keywords = {compilers, functional programming, mixed integer linear programming},
location = {Pasadena, CA, USA},
series = {Onward! '24}
}

@article{10.1145/3712591,
author = {Zhou, Yuhao and Jiang, Jianhui and He, Zhenxue and Zhang, Ying and Chen, Chengcheng and Shi, Zhanhui and Zhang, Wei and Yang, Keying},
title = {An Efficient Area and Reliability Optimization Method for MPRM Circuits Based on High-dimensional Genetic Algorithm},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1084-4309},
url = {https://doi.org/10.1145/3712591},
doi = {10.1145/3712591},
abstract = {Area and reliability optimization have become the primary constraints in circuits logic synthesis. To address the increasing area and transient fault susceptibility in combinational circuits, we propose a high-dimensional genetic algorithm (HGA). HGA adopts an evolutionary scheme based on ternary tree, and uses adaptive crossover operator and flight operator to jump out of local optimum. Moreover, based on the HGA, we propose an area and reliability optimization method (AROM) for mixed polarity Reed-Muller logic circuits, which searches the best polarity with minimum area and soft error rate. The experimental results confirm that AROM can search for more desirable nondominated solutions in less time compared to existing optimization methods, and can be used as an effective electronic design automation tool for multi-objective optimization.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = feb,
articleno = {31},
numpages = {22},
keywords = {Area and reliability optimization, high-dimensional genetic algorithm, mixed polarity Reed-Muller, multi-objective optimization}
}

@article{10.1145/3702987,
author = {Zhang, Sai and Xing, Zhenchang and Guo, Ronghui and Xu, Fangzhou and Chen, Lei and Zhang, Zhaoyuan and Zhang, Xiaowang and Feng, Zhiyong and Zhuang, Zhiqiang},
title = {Empowering Agile-Based Generative Software Development through Human-AI Teamwork},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3702987},
doi = {10.1145/3702987},
abstract = {In software development, the raw requirements proposed by users are frequently incomplete, which impedes the complete implementation of software functionalities. With the emergence of large language models, the exploration of generating software through user requirements has attracted attention. Recent methods with the top-down waterfall model employ a questioning approach for requirement completion, attempting to explore further user requirements. However, users, constrained by their domain knowledge, result in a lack of effective acceptance criteria during the requirement completion, failing to fully capture the implicit needs of the user. Moreover, the cumulative errors of the waterfall model can lead to discrepancies between the generated code and user requirements. The Agile methodologies reduce cumulative errors of the waterfall model through lightweight iteration and collaboration with users, but the challenge lies in ensuring semantic consistency between user requirements and the code generated by the agent. To address these challenges, we propose AgileGen, an agile-based generative software development through human-AI teamwork. Unlike existing questioning agents, AgileGen adopts a novel collaborative approach that breaks free from the constraints of domain knowledge by initiating the end-user perspective to complete the acceptance criteria. By introducing the Gherkin language, AgileGen attempts for the first time to use testable requirement descriptions as a bridge for semantic consistency between requirements and code, aiming to ensure that software products meet actual user requirements by defining user scenarios that include acceptance criteria. Additionally, we innovate in the human-AI teamwork model, allowing users to participate in decision-making processes they do well and significantly enhancing the completeness of software functionality. To ensure semantic consistency between requirements and generated code, we derive consistency factors from Gherkin to drive the subsequent software code generation. Finally, to improve the reliability of user scenarios, we also introduce a memory pool mechanism, collecting user decision-making scenarios and recommending them to new users with similar requirements. AgileGen, as a user-friendly interactive system, significantly outperformed existing best methods by 16.4% and garnered higher user satisfaction.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Agile, Human-AI Teamwork, Generative Software Development, User Requirement, Gherkin}
}

@inproceedings{10.1145/3718491.3718510,
author = {Liu, Daosheng and Wang, Yongsheng and Huang, Guoxuan},
title = {Optimization Design of Amorphous Metal Dry-type Transformer Based on Adaptive Chaotic Particle Swarm Optimization Algorithm},
year = {2025},
isbn = {9798400710865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3718491.3718510},
doi = {10.1145/3718491.3718510},
abstract = {Amorphous metal dry-type transformer (AMDT) is a kind of green and energy-saving power distribution equipment, which is widely used in power distribution systems in various fields. The material cost and loss of AMDT are important factors that determine their economy and energy efficiency. Based on Visual Basic 6.0 software platform, the adaptive chaotic particle swarm optimization algorithm (ACPSO) is combined to optimize the total loss and main material cost of AMDT. Comparing the optimized results with the manual design and the data optimized by Particle Swarm Optimization Algorithm (PSO) and Quantum Particle Swarm Optimization Algorithm (QPSO), it is proved that the application of ACPSO can achieve the optimal and most efficient AMDT loss and cost reduction scheme.},
booktitle = {Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum},
pages = {101–106},
numpages = {6},
keywords = {ACPSO, AMDT, Main material cost, Total loss},
location = {
},
series = {AIBDF '24}
}

@inproceedings{10.1145/3411016.3411158,
author = {Xiao, Litian and Xiao, Nan and Li, Mengyuan and Xie, Shanshan},
title = {Relativity-Driven Optimization for Test Schedule of Spaceflight Products at Launch Site},
year = {2020},
isbn = {9781450375498},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411016.3411158},
doi = {10.1145/3411016.3411158},
abstract = {The test preparation of flight products determines the length of the entire cycle from flight products entering the launch site to launch. The improvement of test efficiency is beneficial to the launch efficiency of the launch site. According to the test scheduling plan of flight products at the launch site, we propose test scheduling model based on related data-driven optimization and superimpose test elements on the test network schedule diagram. By means of the correlation between test item input, excitation and output response, the optimization strategy of test process was proposed to decompose and shorten the critical path on test network schedule. The strategy shortens the test cycle of flight products. Through practice, integration testing and good results have been obtained.},
booktitle = {Proceedings of the 2nd International Conference on Industrial Control Network And System Engineering Research},
pages = {49–56},
numpages = {8},
keywords = {Network Scheduling, Optimization strategy, Relativity-driven optimization, Spaceflight products test, Test schedule},
location = {Kuala Lumpur, Malaysia},
series = {ICNSER2020}
}

@article{10.1145/3715012,
author = {Deutel, Mark and Kontes, Georgios and Mutschler, Christopher and Teich, J\"{u}rgen},
title = {Combining Multi-Objective Bayesian Optimization with Reinforcement Learning for TinyML},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715012},
doi = {10.1145/3715012},
abstract = {Deploying deep neural networks (DNNs) on microcontrollers (TinyML) is a common trend to process the increasing amount of sensor data generated at the edge, but in practice, resource and latency constraints make it difficult to find optimal DNN candidates. Neural architecture search (NAS) is an excellent approach to automate this search and can easily be combined with DNN compression techniques commonly used in TinyML. However, many NAS techniques are not only computationally expensive, especially hyperparameter optimization (HPO), but also often focus on optimizing only a single objective, e.g., maximizing accuracy, without considering additional objectives such as memory requirements or computational complexity of a DNN, which are key to making deployment at the edge feasible. In this paper, we propose a novel NAS strategy for TinyML based on multi-objective Bayesian optimization (MOBOpt) and an ensemble of competing parametric policies trained using Augmented Random Search (ARS) reinforcement learning (RL) agents. Our methodology aims at efficiently finding tradeoffs between a DNN’s predictive accuracy, memory requirements on a given target system, and computational complexity. Our experiments show that we consistently outperform existing MOBOpt approaches on different datasets and architectures such as ResNet-18 and MobileNetV3.},
note = {Just Accepted},
journal = {ACM Trans. Evol. Learn. Optim.},
month = jan,
keywords = {Neural Architecture Search, Embedded Systems, Reinforcement Learning, Multi-Objective Bayesian Optimization}
}

@inbook{10.1145/3544585.3544596,
author = {Lengauer, Christian},
title = {A Personal View of Edsger W. Dijkstra and His Stance on Software Construction},
year = {2022},
isbn = {9781450397735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3544585.3544596},
booktitle = {Edsger Wybe Dijkstra: His Life,Work, and Legacy},
pages = {205–214},
numpages = {10}
}

@inproceedings{10.1145/2420942.2420944,
author = {Olaechea, Rafael and Stewart, Steven and Czarnecki, Krzysztof and Rayside, Derek},
title = {Modelling and multi-objective optimization of quality attributes in variability-rich software},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420944},
doi = {10.1145/2420942.2420944},
abstract = {Variability-rich software, such as software product lines, offers optional and alternative features to accommodate varying needs of users. Designers of variability-rich software face the challenge of reasoning about the impact of selecting such features on the quality attributes of the resulting software variant. Attributed feature models have been proposed to model such features and their impact on quality attributes, but existing variability modelling languages and tools have limited or no support for such models and the complex multi-objective optimization problem that arises. This paper presents ClaferMoo, a language and tool that addresses these shortcomings. ClaferMoo uses type inheritance to modularize the attribution of features in feature models and allows specifying multiple optimization goals. We evaluate an implementation of the language on a set of attributed feature models from the literature, showing that the optimization infrastructure can handle small-scale feature models with about a dozen features within seconds.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {2},
numpages = {6},
keywords = {multi-objective optimization, software product lines},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@article{10.1145/3712007,
author = {Cruz, Lu\'{\i}s and Franch Gutierrez, Xavier and Mart\'{\i}nez-Fern\'{a}ndez, Silverio},
title = {Innovating for Tomorrow: The Convergence of Software Engineering and Green AI},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3712007},
doi = {10.1145/3712007},
abstract = {The latest advancements in machine learning, specifically in foundation models, are revolutionizing the frontiers of existing software engineering (SE) processes. This is a bi-directional phenomena, where 1) software systems are now challenged to provide AI-enabled features to their users, and 2) AI is used to automate tasks within the software development lifecycle. In an era where sustainability is a pressing societal concern, our community needs to adopt a long-term plan enabling a conscious transformation that aligns with environmental sustainability values. In this paper, we reflect on the impact of adopting environmentally friendly practices to create AI-enabled software systems and make considerations on the environmental impact of using foundation models for software development.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Green AI, Green Software, Sustainability, Software Engineering}
}

@article{10.1145/3678259,
author = {Balasubramaniam, Balaji and Ahmed, Iftekhar and Bagheri, Hamid and Bradley, Justin},
title = {Carving Out Control Code: Automated Identification of Control Software in Autopilot Systems},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2378-962X},
url = {https://doi.org/10.1145/3678259},
doi = {10.1145/3678259},
abstract = {Cyber-physical systems interact with the world through software controlling physical effectors. Carefully designed controllers, implemented as safety-critical control software, also interact with other parts of the software suite, and may be difficult to separate, verify, or maintain. Moreover, some software changes, not intended to impact control system performance, do change controller response through a variety of means including interaction with external libraries or unmodeled changes only existing in the cyber system (e.g., exception handling). As a result, identifying safety-critical control software, its boundaries with other embedded software in the system, and the way in which control software evolves could help developers isolate, test, and verify control implementation, and improve control software development. In this work we present an automated technique, based on a novel application of machine learning, to detect commits related to control software, its changes, and how the control software evolves. We leverage messages from developers (e.g., commit comments), and code changes themselves to understand how control software is refined, extended, and adapted over time. We examine three distinct, popular, real-world, safety-critical autopilots—ArduPilot, Paparazzi UAV, and LibrePilot to test our method demonstrating an effective detection rate of 0.95 for control-related code changes.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = nov,
articleno = {39},
numpages = {20},
keywords = {Autopilot Software, Control Software, Small Uncrewed Aerial Vehicle, and Software code changes}
}

@inproceedings{10.1145/3689937.3695792,
author = {Alhanahnah, Mohannad and Boshmaf, Yazan and Gehani, Ashish},
title = {SoK: Software Debloating Landscape and Future Directions},
year = {2024},
isbn = {9798400712333},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689937.3695792},
doi = {10.1145/3689937.3695792},
abstract = {Software debloating seeks to mitigate security risks and improve performance by eliminating unnecessary code. In recent years, a plethora of debloating tools have been developed, creating a dense and varied landscape. Several studies have delved into the literature, focusing on comparative analysis of these tools. To build upon these efforts, this paper presents a comprehensive systematization of knowledge (SoK) of the software debloating landscape. We conceptualize the software debloating workflow, which serves as the basis for developing a multilevel taxonomy. This framework classifies debloating tools according to their input/output artifacts, debloating strategies, and evaluation criteria. Lastly, we apply the taxonomy to pinpoint open problems in the field, which, together with the SoK, provide a foundational reference for researchers aiming to improve software security and efficiency through debloating.},
booktitle = {Proceedings of the 2024 Workshop on Forming an Ecosystem Around Software Transformation},
pages = {11–18},
numpages = {8},
keywords = {sbom, sdlc, software debloating, software security, systematization of knowledge, taxonomy},
location = {Salt Lake City, UT, USA},
series = {FEAST '24}
}

@article{10.1145/3568680,
author = {Lesch, Veronika and Hadry, Marius and Krupitzer, Christian and Kounev, Samuel},
title = {Self-aware Optimization of Adaptation Planning Strategies},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1556-4665},
url = {https://doi.org/10.1145/3568680},
doi = {10.1145/3568680},
abstract = {In today’s world, circumstances, processes, and requirements for software systems are becoming increasingly complex. To operate properly in such dynamic environments, software systems must adapt to these changes, which has led to the research area of Self-Adaptive Systems&nbsp;(SAS). Platooning is one example of adaptive systems in Intelligent Transportation Systems, which is the ability of vehicles to travel with close inter-vehicle distances. This technology leads to an increase in road throughput and safety, which directly addresses the increased infrastructure needs due to increased traffic on the roads. However, the No-Free-Lunch theorem states that the performance of one adaptation planning strategy is not necessarily transferable to other problems. Moreover, especially in the field of SAS, the selection of the most appropriate strategy depends on the current situation of the system. In this article, we address the problem of self-aware optimization of adaptation planning strategies by designing a framework that includes situation detection, strategy selection, and parameter optimization of the selected strategies. We apply our approach on the case study platooning coordination and evaluate the performance of the proposed framework.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = sep,
articleno = {10},
numpages = {35},
keywords = {Self-awareness, optimization, cyber-physical systems, adaptation planning strategies, platooning, framework}
}

@inproceedings{10.1145/3641343.3641361,
author = {Li, Xiong and Liang, Wei},
title = {Research on Software Defect Prediction Method Based on Model Reuse},
year = {2024},
isbn = {9798400716775},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641343.3641361},
doi = {10.1145/3641343.3641361},
abstract = {Software reuse has been the most effective way to improve the quality and efficiency of software development for a long time. Model-based software reuse(MBSR) development method can greatly improve the efficiency and quality of reuse by providing reusable software components and models for reuse. However, this reuse will inevitably lead to software defects, and software defect prediction has always been an important research direction in the field of software engineering. The accuracy of software defect prediction depends not only on the choice of prediction methods, but also on the metrics of software. Therefore, for software defect prediction based on model reuse, a software defect prediction method based on multi-metrics is proposed. The multi-metrics of the model are constructed by the characteristics of defect type, defect severity, possibility of defect generation, defect priority, defect state, defect source, defect root and so on. The correlation and significance methods were used to determine the decisive influencing factors. The defect samples are trained by the hybrid neural network to obtain the optimal solution of the training results, and the defect prediction model based on linear regression is constructed. Through the similarity screening of the historical data of the model library, the least square method is used to solve the regression coefficient to reduce the error of the predicted value and improve the accuracy of the defect prediction.},
booktitle = {Proceedings of the 3rd International Conference on Electronic Information Technology and Smart Agriculture},
pages = {106–112},
numpages = {7},
keywords = {Hybrid neural network, Linear regression model, Model reuse, Multi-metrics, Software defect prediction},
location = {Sanya, China},
series = {ICEITSA '23}
}

@inproceedings{10.1109/SCW63240.2024.00228,
author = {Breyer, Marcel and Van Craen, Alexander and Domanski, Peter and Pfl\"{u}ger, Dirk},
title = {hws: A Tool for Monitoring Hardware Metrics Across Diverse Vendors: A Case Study on Hyperparameter Optimization Algorithms},
year = {2025},
isbn = {9798350355543},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SCW63240.2024.00228},
doi = {10.1109/SCW63240.2024.00228},
abstract = {Due to modern hardware's constantly growing energy demands, it is important to consider energy efficiency and power consumption. Especially in the age of AI, where a massive amount of computational power is necessary, energy consumption and the costs involved can become a significant problem. However, gathering this power information in a vendor-independent and portable way is far from trivial.Therefore, we propose hws, a hardware sampling library for Python and C++, which makes it extremely easy to gather hardware information like the current power draw or total power consumption, as well as other metrics like clock frequencies, memory consumption, or utilizations, for CPUs and GPUs from NVIDIA, AMD, and Intel. In a case study, we use our library to analyze three common hyperparameter optimization algorithms for two Neural Network architectures and one GPU-accelerated SVM implementation.},
booktitle = {Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {1815–1825},
numpages = {11},
keywords = {CPUs, GPUs, energy consumption, hardware sampling, machine learning},
location = {Atlanta, GA, USA},
series = {SC-W '24}
}

@article{10.1145/3485136,
author = {Traini, Luca and Di Pompeo, Daniele and Tucci, Michele and Lin, Bin and Scalabrino, Simone and Bavota, Gabriele and Lanza, Michele and Oliveto, Rocco and Cortellessa, Vittorio},
title = {How Software Refactoring Impacts Execution Time},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3485136},
doi = {10.1145/3485136},
abstract = {Refactoring aims at improving the maintainability of source code without modifying its external behavior. Previous works proposed approaches to recommend refactoring solutions to software developers. The generation of the recommended solutions is guided by metrics acting as proxy for maintainability (e.g., number of code smells removed by the recommended solution). These approaches ignore the impact of the recommended refactorings on other non-functional requirements, such as performance, energy consumption, and so forth. Little is known about the impact of refactoring operations on non-functional requirements other than maintainability.We aim to fill this gap by presenting the largest study to date to investigate the impact of refactoring on software performance, in terms of execution time. We mined the change history of 20 systems that defined performance benchmarks in their repositories, with the goal of identifying commits in which developers implemented refactoring operations impacting code components that are exercised by the performance benchmarks. Through a quantitative and qualitative analysis, we show that refactoring operations can significantly impact the execution time. Indeed, none of the investigated refactoring types can be considered “safe” in ensuring no performance regression. Refactoring types aimed at decomposing complex code entities (e.g., Extract Class/Interface, Extract Method) have higher chances of triggering performance degradation, suggesting their careful consideration when refactoring performance-critical code.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {25},
numpages = {23},
keywords = {Software maintainability, performance, execution time, refactoring}
}

@inproceedings{10.1145/3628353.3628540,
author = {Xu, Ye and Bader, Sebastian and Oelmann, Bengt},
title = {Self-powered RPM Sensor using a Single-Anchor Variable Reluctance Energy Harvester with Pendulum Effects},
year = {2023},
isbn = {9798400704383},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628353.3628540},
doi = {10.1145/3628353.3628540},
abstract = {The feasibility of energy harvesting as a viable alternative for powering low-energy electronics has been demonstrated through advancements in transduction mechanisms. Energy harvesters incorporating counterweights have gained attention in rotational energy harvesting to develop single-anchored devices with flexible placement and easy installation. In this work, a three-phase variable reluctance energy harvester (VREH) with low torque ripple is combined with a counterweight to facilitate a single-anchored design, specifically targeting low rotational speed applications. The energy harvester is integrated with a low-power sensor system to enable energy-neutral operation. We present the design, implementation, and evaluation of an on-rotor RPM sensor system powered by the single-anchored three-phase VREH. Experimental evaluations on a laboratory test bench demonstrate the system performance under varying conditions, with the ability to supply the sensor system at low speeds achieving, for example, a 3.5 Hz sample rate at a low speed of 3 rpm. Evaluations of the system illustrate that pendulum effects induced by the interaction of the cogging torque and the gravitational torque improve the output power of the harvester under low-speed conditions. This promises for the proposed design to be suitable to power wireless sensors for industrial condition monitoring, providing a flexible solution for energy-neutral sensor systems with reduced installation complexity.},
booktitle = {Proceedings of the 11th International Workshop on Energy Harvesting &amp; Energy-Neutral Sensing Systems},
pages = {72–78},
numpages = {7},
keywords = {RPM sensor, electromagnetic transduction, energy harvesting, pendulum effect, self-powered wireless sensor, variable reluctance},
location = {Istanbul, Turkiye},
series = {ENSsys '23}
}

@inproceedings{10.1145/3592149.3592160,
author = {Monteiro, Rui Pedro C. and Silva, Jo\~{a}o Marco C.},
title = {Flexcomm Simulator: Exploring Energy Flexibility in Software Defined Networks with ns-3},
year = {2023},
isbn = {9798400707476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3592149.3592160},
doi = {10.1145/3592149.3592160},
abstract = {The digitalization of energy generation and distribution systems opens new opportunities for devising network operation and traffic engineering strategies capable of adapting to the energy availability and sources. Despite the potential, developing and testing new approaches are challenging in production environments. Furthermore, no simulators support such integration between the communication infrastructure and the power grid. Thus, this paper introduces Flexcomm Simulator, a tool based on ns-3 that supports developing and assessing multiple strategies toward green networking and communications driven by real-time information from the power grid (i.e., Energy Flexibility). The proof-of-concept results demonstrate this contribution’s potential by implementing an energy-aware routing algorithm that adapts to real-world Energy Flexibility&nbsp;data in a Metropolitan Area Network (MAN). Also, it showcases the simulator’s capacity to deal with large-scale simulations through MPI-based distributed environments.},
booktitle = {Proceedings of the 2023 Workshop on Ns-3},
pages = {94–101},
numpages = {8},
keywords = {Energy Flexibility, Simulation, Software Defined Networking, ns-3},
location = {Arlington, VA, USA},
series = {WNS3 '23}
}

@inproceedings{10.1145/1808937.1808943,
author = {Maman, Itay and Botterweck, Goetz},
title = {SPLGraph: towards a graph-based formalism for software product lines},
year = {2010},
isbn = {9781605589688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1808937.1808943},
doi = {10.1145/1808937.1808943},
abstract = {This paper presents SPLGraph a graph-based model for Software Product Lines, including (1) a formal definition; (2) an algorithm that applies configuration decisions to an SPLGraph thus yielding a product specific graph; (3) a set of patterns for typical SPLGraph structures, such as Boolean operators, reuse of expressions, named configurations, optional and alternative features and staged configuration; and (4) an algorithm that infers product configuration per a variability point.SPLGraph is generic, simple, and self sustaining in the sense that an SPLGraph instance can apply variability to itself. These properties make SPLGraph a basis for a solid and complete formalism for Software Product Lines.},
booktitle = {Proceedings of the 2010 ICSE Workshop on Product Line Approaches in Software Engineering},
pages = {40–47},
numpages = {8},
keywords = {graph, software product lines, variability modeling},
location = {Cape Town, South Africa},
series = {PLEASE '10}
}

@inproceedings{10.1145/3676536.3676740,
author = {Dong, Yanchi and Liu, Xueping and Hao, Xiaochen and Liang, Yun and Huang, Ru and Ye, Le and Jia, Tianyu},
title = {Hierarchical Power Co-Optimization and Management for LLM Chiplet Designs},
year = {2025},
isbn = {9798400710773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676536.3676740},
doi = {10.1145/3676536.3676740},
abstract = {The demand for efficient and high-performance hardware for large language models (LLMs) has driven the development of scalable chiplet design, which requires careful power optimization and management. This paper presents a co-optimization and management methodology for hierarchical power delivery of chiplet designs targeting LLM applications. To model LLM workload mapping and power delivery, we first build a scalable chiplet simulator, which demonstrates different power strategies have notable efficiency impact and require careful and thorough optimizations. We further develop a co-optimization framework ScalePoM for chiplet power management. Based on given LLM model and PPA requirements, ScalePoM can automatically explore the chiplet architecture and workload mapping for optimal hierarchical power delivery. Our co-optimization methodology is evaluated through two scaled LLM chiplets with different interconnect topologies, achieving an average of 45% and up to 62% energy saving for large language model inferences with various sparsity levels.},
booktitle = {Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design},
articleno = {145},
numpages = {9},
keywords = {power management, co-optimization methodology, chiplets, LLMs},
location = {Newark Liberty International Airport Marriott, New York, NY, USA},
series = {ICCAD '24}
}

@inproceedings{10.1145/3592979.3593403,
author = {Clasc\`{a}, Marc and Garcia-Gasulla, Marta and Montagud, Arnau and Carbonell Caballero, Jos\'{e} and Valencia, Alfonso},
title = {Lessons Learned from a Performance Analysis and Optimization of a Multiscale Cellular Simulation},
year = {2023},
isbn = {9798400701900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3592979.3593403},
doi = {10.1145/3592979.3593403},
abstract = {This work presents a comprehensive performance analysis and optimization of a multiscale agent-based cellular simulation. The optimizations applied are guided by detailed performance analysis and include memory management, load balance, and a locality-aware parallelization. The outcome of this paper is not only the speedup of 2.4x achieved by the optimized version with respect to the original PhysiCell code, but also the lessons learned and best practices when developing parallel HPC codes to obtain efficient and highly performant applications, especially in the computational biology field.},
booktitle = {Proceedings of the Platform for Advanced Scientific Computing Conference},
articleno = {4},
numpages = {10},
keywords = {load balance, optimization, multiscale simulation, memory locality},
location = {Davos, Switzerland},
series = {PASC '23}
}

@inproceedings{10.1145/3510003.3510200,
author = {Dubslaff, Clemens and Weis, Kallistos and Baier, Christel and Apel, Sven},
title = {Causality in configurable software systems},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510200},
doi = {10.1145/3510003.3510200},
abstract = {Detecting and understanding reasons for defects and inadvertent behavior in software is challenging due to their increasing complexity. In configurable software systems, the combinatorics that arises from the multitude of features a user might select from adds a further layer of complexity. We introduce the notion of feature causality, which is based on counterfactual reasoning and inspired by the seminal definition of actual causality by Halpern and Pearl. Feature causality operates at the level of system configurations and is capable of identifying features and their interactions that are the reason for emerging functional and non-functional properties. We present various methods to explicate these reasons, in particular well-established notions of responsibility and blame that we extend to the feature-oriented setting. Establishing a close connection of feature causality to prime implicants, we provide algorithms to effectively compute feature causes and causal explications. By means of an evaluation on a wide range of configurable software systems, including community benchmarks and real-world systems, we demonstrate the feasibility of our approach: We illustrate how our notion of causality facilitates to identify root causes, estimate the effects of features, and detect feature interactions.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {325–337},
numpages = {13},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3691620.3695540,
author = {Kula, Elvan and van Deursen, Arie and Gousios, Georgios},
title = {Context-Aware Automated Sprint Plan Generation for Agile Software Development},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695540},
doi = {10.1145/3691620.3695540},
abstract = {Sprint planning is essential for the successful execution of agile software projects. While various prioritization criteria influence the selection of user stories for sprint planning, their relative importance remains largely unexplored, especially across different project contexts. In this paper, we investigate how prioritization criteria vary across project settings and propose a model for generating sprint plans that are tailored to the context of individual teams. Through a survey conducted at ING, we identify urgency, sprint goal alignment, and business value as the top prioritization criteria, influenced by project factors such as resource availability and client type. These results highlight the need for contextual support in sprint planning. To address this need, we develop an optimization model that generates sprint plans aligned with the specific goals and performance of a team. By integrating teams' planning objectives and sprint history, the model adapts to unique team contexts, estimating prioritization criteria and identifying patterns in planning behavior. We apply our approach to real-world data from 4,841 sprints at ING, demonstrating significant improvements in team alignment and sprint plan effectiveness. Our model boosts team performance by generating plans that deliver more business value, align more closely with sprint goals, and better mitigate delay risks. Overall, our results show that the efficiency and outcomes of sprint planning practices can be significantly improved through the use of context-aware optimization methods.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1745–1756},
numpages = {12},
keywords = {agile methods, sprint planning, context-aware optimization},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@article{10.1145/3708529,
author = {Lin, Zhihao and Ma, Wei and Lin, Tao and Zheng, Yaowen and Ge, Jingquan and Wang, Jun and Klein, Jacques and Bissyande, Tegawende and Liu, Yang and Li, Li},
title = {Open-Source AI-based SE Tools: Opportunities and Challenges of Collaborative Software Learning},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708529},
doi = {10.1145/3708529},
abstract = {Large Language Models (LLMs) have become instrumental in advancing software engineering (SE) tasks, showcasing their efficacy in code understanding and beyond. AI code models has demonstrated their value not only in code generating but also in defect detection, enhancing security measures, and improving overall software quality. They are emerging as crucial tools for both software development and maintaining software quality. Like traditional SE tools, open-source collaboration is key in realising the excellent products. However, with AI models, the essential need is in data. The collaboration of these AI-based SE models hinges on maximising the sources of high-quality data. However, data especially of high quality, often holds commercial or sensitive value, making it less accessible for open-source AI-based SE projects. This reality presents a significant barrier to the development and enhancement of AI-based SE tools within the software engineering community. Therefore, researchers need to find solutions for enabling open-source AI-based SE models to tap into resources by different organizations. Addressing this challenge, our position paper investigates one solution to facilitate access to diverse organizational resources for open-source AI models, ensuring privacy and commercial sensitivities are respected. We introduce a governance framework centered on federated learning (FL), designed to foster the joint development and maintenance of open-source AI code models while safeguarding data privacy and security. Additionally, we present guidelines for developers on AI-based SE tool collaboration, covering data requirements, model architecture, updating strategies, and version control. Given the significant influence of data characteristics on federated learning, our research examines the effect of code data heterogeneity on federated learning performance. We consider 6 different scenarios of data distributions and include 4 code models. We also include 4 most common federated learning algorithms.  Our experimental findings highlight the potential for employing federated learning in the collaborative development and maintenance of AI-based software engineering models. We also discuss the key issues to be addressed in the co-construction process and future research directions.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {Data Privacy, Software Engineering, Open-source Code Model, Federated Learning}
}

@inproceedings{10.1145/3673038.3673101,
author = {Mo, Haotian and Wang, Qinglin and Liao, Linyu and Li, Biao and Chi, Lihua and Liu, Jie},
title = {Detailed Analysis and Optimization of Irregular-Shaped Matrix Multiplication on Multi-Core DSPs},
year = {2024},
isbn = {9798400717932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3673038.3673101},
doi = {10.1145/3673038.3673101},
abstract = {Irregular-shaped General Matrix Multiplication (GEMM) is extensively used in diverse workloads, such as scientific simulation and deep learning. In response to energy efficiency constraints, low-power multi-core digital signal processors (DSPs) have emerged as a viable alternative architecture in HPC systems. This study examines the performance of existing GEMM implementations working on irregular-shaped matrices and observes sub-optimal outcomes due to deficiencies in memory optimization and core-level parallelism extraction. For multi-core DSPs in FT-M7032, a CPU-DSP heterogeneous processor for HPC, we introduce dspIMM - a new multi-core parallel implementation for irregular-shaped matrix multiplications. dspIMM incorporates a new loop ordering, stack-space optimization, multi-dimensional core-level parallelization, a communication-computation overlap implementation with data prefetch across loops, and blocking optimization. These optimizations effectively enhance memory access and the core-level parallelism in irregular-shaped GEMMs. Experimental results demonstrate that the proposed communication-computation overlap optimization achieves the highest performance improvement in dspIMM, and the average speedup of dspIMM over previous implementations finally achieves up to 3.34 times.},
booktitle = {Proceedings of the 53rd International Conference on Parallel Processing},
pages = {1176–1186},
numpages = {11},
keywords = {DSPs, Irregular-shaped matrix, Matrix-matrix multiplication, Parallel algorithm, Performance optimization},
location = {Gotland, Sweden},
series = {ICPP '24}
}

@inproceedings{10.1145/3503222.3507769,
author = {Miano, Sebastiano and Sanaee, Alireza and Risso, Fulvio and R\'{e}tv\'{a}ri, G\'{a}bor and Antichi, Gianni},
title = {Domain specific run time optimization for software data planes},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507769},
doi = {10.1145/3503222.3507769},
abstract = {State-of-the-art approaches to design, develop and optimize software packet-processing programs are based on static compilation: the compiler's input is a description of the forwarding plane semantics and the output is a binary that can accommodate any control plane configuration or input traffic.  In this paper, we demonstrate that tracking control plane actions and packet-level traffic dynamics at run time opens up new opportunities for code specialization. We present Morpheus, a system working alongside static compilers that continuously optimizes the targeted networking code. We introduce a number of new techniques, from static code analysis to adaptive code instrumentation, and we implement a toolbox of domain specific optimizations that are not restricted to a specific data plane framework or programming language. We apply Morpheus to several eBPF and DPDK programs including Katran, Facebook's production-grade load balancer. We compare Morpheus against state-of-the-art optimization frameworks and show that it can bring up to 2x throughput improvement, while halving the 99th percentile latency.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {1148–1164},
numpages = {17},
keywords = {DPDK, Data Plane Compilation, LLVM, XDP, eBPF},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@inproceedings{10.1145/3669940.3707231,
author = {Wang, Zibo and Zhang, Yijia and Wei, Fuchun and Wang, Bingqiang and Liu, Yanlin and Hu, Zhiheng and Zhang, Jingyi and Xu, Xiaoxin and He, Jian and Wang, Xiaoliang and Dou, Wanchun and Chen, Guihai and Tian, Chen},
title = {Using Analytical Performance/Power Model and Fine-Grained DVFS to Enhance AI Accelerator Energy Efficiency},
year = {2025},
isbn = {9798400706981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3669940.3707231},
doi = {10.1145/3669940.3707231},
abstract = {Recent advancements in deep learning have significantly increased AI processors' energy consumption, which is becoming a critical factor limiting AI development. Dynamic Voltage and Frequency Scaling (DVFS) stands as a key method in power optimization. However, due to the latency of DVFS control in AI processors, previous works typically apply DVFS control at the granularity of a program's entire duration or sub-phases, rather than at the level of AI operators.The advent of millisecond-level DVFS capabilities on the latest Ascend NPU platforms enables us to set frequency individually for single or multiple operators, opening up the opportunity for further enhancing energy efficiency through fine-grained DVFS control. To ensure performance is unaffected in DVFS, our work builds performance and power models for each operator. Through in-depth timeline analysis, we demonstrate that the cycle count of an operator can be modeled as a convex piecewise linear function of frequency, resulting in a performance model with an average error of 1.96%. Moreover, we build power models that incorporate temperature-dependent terms, which enhances the model's precision and results in an average error of 4.62%.Based on our performance and power models as well as the fine-grained DVFS functionality of Ascend NPU, we propose a DVFS strategy that integrates operator classification, preprocessing, and a genetic algorithm-based search. Experiments on applications including GPT-3 training achieve a reduction in AICore (the computing component within the Ascend NPU) power by 13.44% and NPU chip power by 4.95%, while limiting performance degradation to 1.76%.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1},
pages = {1118–1132},
numpages = {15},
keywords = {ai accelerator, fine-grained dvfs, genetic algorithm, performance model, power model},
location = {Rotterdam, Netherlands},
series = {ASPLOS '25}
}

@inproceedings{10.1145/3698062.3698087,
author = {Fedorov, Nikolay and Tamada, Haruaki and Inayoshi, Hiroki and Monden, Akito},
title = {Comparison of Similarity Functions for n-gram Software Birthmarks},
year = {2024},
isbn = {9798400717086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698062.3698087},
doi = {10.1145/3698062.3698087},
abstract = {Various software birthmarks have been proposed to detect software plagiarism or unauthorized reuse of code. If two software products have very similar birthmarks, the existence of reused code between them is strongly suspected. However, the accuracy of detecting reused code depends on the similarity function for comparing birthmarks and the threshold for distinguishing reused and non-reused codes. Moreover, since software products usually consist of many program modules such as source files and class files, an appropriate method to compare the set of modules between two software products is needed. In this paper we propose a method for comparing birthmarks between two software products containing many modules, as well as a method for determining the threshold for distinguishing reuse. Afterwards, focusing on the n-gram birthmarks, which is one of the most easy-to-measure birthmarks, we empirically compare 4 similarity functions: cosine similarity, Dice index, Jaccard coefficient and Simpson index. In the experiment, we employ multiple versions of 15 open-source software (OSS) projects of 5 categories, making sure that they contain project pairs that contain reused code and that do not contain reused code. The experimental result shows that, for both 3-gram and 6-gram birthmarks, Jaccard coefficient showed the best performance in terms of F1-score to identify reused code.},
booktitle = {Proceedings of the 2024 The 6th World Symposium on Software Engineering (WSSE)},
pages = {169–176},
numpages = {8},
keywords = {Java, plagiarism detection, software birthmark, software similarity, static analysis},
location = {
},
series = {WSSE '24}
}

@inproceedings{10.1145/3583133.3596371,
author = {Gabor, Thomas and Zielinski, Sebastian and Henghuber, Sofie and Linnhoff-Popien, Claudia},
title = {A Relative Approach to Comparative Performance Analysis for Quantum Optimization},
year = {2023},
isbn = {9798400701207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583133.3596371},
doi = {10.1145/3583133.3596371},
abstract = {We discuss a small study on how to compare the performance of various solving techniques for quadratic unconstrained binary optimization (QUBO). Since well-known metrics are seldomly applicable, we suggest comparing the relative performance, i.e., how much the quality of solution (compared to other solutions of the same solver) for a QUBO shifts between different solving techniques. We propose looking for big shifts systematically for an empirical complexity analysis.Code is available at github.com/thomasgabor/gecco-relative.},
booktitle = {Proceedings of the Companion Conference on Genetic and Evolutionary Computation},
pages = {2211–2215},
numpages = {5},
keywords = {quantum optimization, simulated annealing, tabu search},
location = {Lisbon, Portugal},
series = {GECCO '23 Companion}
}

@inproceedings{10.1109/SCW63240.2024.00156,
author = {Antepara, Oscar and Williams, Samuel and Carlson, Max and Watkins, Jerry},
title = {Performance Portable Optimizations of an Ice-sheet Modeling Code on GPU-supercomputers},
year = {2025},
isbn = {9798350355543},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SCW63240.2024.00156},
doi = {10.1109/SCW63240.2024.00156},
abstract = {In this paper, we present GPU-optimizations for an ice-sheet modeling code known as MPAS-Albany Land Ice (MALI). MALI is a C++ template code that leverages the Kokkos programming model for portability and the Trilinos library for data structures, nonlinear and linear solvers and optimization packages for ice-sheet simulations. Performance of the most expensive kernel is assessed via the Roofline model to highlight the potential for code improvement according to the underlying GPU architecture. We perform a collection of optimizations consisting of loop fusions, loop optimizations and local accumulation to productively and portably attain an overall speedup of 3\texttimes{} in either NVIDIA and AMD GPU. We analyze the performance gains using a time-oriented performance portability model based on time per invocation and GPU data movement. Results show an increment between 20% and 50% on the performance portability metric by improving data locality on the GPU kernels of a Stokes solver and highlights the importance of optimizing GPU-ported scientific applications to maximize memory bandwidth and minimize data movement on modern supercomputers.},
booktitle = {Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {1141–1151},
numpages = {11},
keywords = {GPUs, Kokkos, finite-element, ice-sheet modeling, loop optimization, performance portability},
location = {Atlanta, GA, USA},
series = {SC-W '24}
}

@inproceedings{10.1145/3677333.3678158,
author = {Capel, Manuel I. and Holgado-Terriza, Juan A. and Galiana-Velasco, Sergio and Salguero, Alberto G.},
title = {A Distributed Particle Swarm Optimization Algorithm Based on Apache Spark for Asynchronous Parallel Training of Deep Neural Networks},
year = {2024},
isbn = {9798400718021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677333.3678158},
doi = {10.1145/3677333.3678158},
abstract = {Deep Learning Neural Networks (DLNN) require an immense amount of computation, especially in the training phase when multiple layers of intermediate neurons need to be built. The situation is even more dramatic today with the proliferation of applications with intelligence at the edge, not just in the cloud. Therefore, to meet the new requirements of edge computing, it is imperative to accelerate the execution phases of neural networks as much as possible. In this paper, we will focus on the algorithm known as Particle Swarm Optimization (PSO). It is a bio-inspired, stochastic optimization approach whose goal is to iteratively improve the solution to a given (usually complex) problem by attempting to approximate a given objective. The use of PSO in an edge computing environment has the potential to make the training of the DLNN there without the need to transfer resource-intensive tasks to the cloud. However, implementing an efficient PSO is not a straightforward process due to the complexity of the computations performed on the swarm of particles and the iterative execution until until a near-target solution with minimal error is achieved. In the present work, two parallelizations of the PSO algorithm have been implemented, both designed for a distributed execution environment (Apache Spark). The first PSO parallelization follows a synchronous scheme; i.e., the best global position found by particles is globally updated before the execution of the next iteration of the algorithm. This implementation proved to be more efficient for medium-sized datasets (&lt; 40000 data points). In contrast, the second implementation is an asynchronous parallel variant of the PSO algorithm, which showed lower execution time for large datasets (&gt; 170,000 data points) compared to the first one. Additionally, it exhibits better scalability and elasticity with respect to increasing dataset size. Both variants of the PSO have been implemented to distribute the computational load (particle fitness evaluation and position update) across the different executor nodes of the Spark cluster to effectively achieve a coarse-grained parallelism, resulting in a significant performance imporvement over the current sequential versions of the PSO.},
booktitle = {Workshop Proceedings of the 53rd International Conference on Parallel Processing},
pages = {76–85},
numpages = {10},
keywords = {Apache Spark, Classification recall, Deep Neural Networks, GPU Parallelism, Optimization research, Particle Swarm Optimization (PSO), Predictive Accuracy},
location = {Gotland, Sweden},
series = {ICPP Workshops '24}
}

@proceedings{10.1145/3643664,
title = {WSESE '24: Proceedings of the 1st IEEE/ACM International Workshop on Methodological Issues with Empirical Studies in Software Engineering},
year = {2024},
isbn = {9798400705670},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {WSESE 2024 was a one-day event held on April 16, 2024, in Lisbon, Portugal. The theme of the workshop was "Methodological Issues with Empirical Studies in Software Engineering". The primary goal was to gain a better understanding of the adoption of the empirical paradigm in SE. Specifically, our focus was on identifying, discussing and finding solutions for the issues in the empirical methods currently employed. The workshop provided an opportunity for researchers and practitioners to discuss current methodological challenges and explore ways to address them.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3295500.3356167,
author = {Su, Pengfei and Jiao, Shuyin and Chabbi, Milind and Liu, Xu},
title = {Pinpointing performance inefficiencies via lightweight variance profiling},
year = {2019},
isbn = {9781450362290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295500.3356167},
doi = {10.1145/3295500.3356167},
abstract = {Execution variance among different invocation instances of the same procedure is often an indicator of performance losses. On the one hand, instrumentation-based tools can insert calipers around procedures and identify execution variance; however, they can introduce high overheads. On the other hand, sampling-based tools insert no instrumentation and have low overheads; however, they cannot synchronize samples with procedure entry and exit.In this paper, we propose FVSampler, a lightweight, sampling-based variance profiler. FVSampler employs hardware performance monitoring units in conjunction with hardware debug registers to sample and monitor whole procedure instances (invocation till return) and collect hardware metrics in each sampled procedure instance. FVSampler, typically, incurs only 6% runtime overhead and negligible memory overhead making it suitable for HPC-scale production codes. We evaluate FVSampler with several parallel applications and demonstrate its effectiveness in pinpointing execution variance. Guided by FVSampler, we tune data structures and algorithms to obtain significant speedups.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {19},
numpages = {19},
keywords = {PMU, debug register, lightweight measurement, variance profiling},
location = {Denver, Colorado},
series = {SC '19}
}

@inproceedings{10.1109/SCW63240.2024.00247,
author = {Matsumura, Kazuaki and De Gonzalo, Simon Garcia and Pe\~{n}a, Antonio J.},
title = {ACC Saturator: Automatic Kernel Optimization for Directive-Based GPU Code},
year = {2025},
isbn = {9798350355543},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SCW63240.2024.00247},
doi = {10.1109/SCW63240.2024.00247},
abstract = {Automatic code optimization is a complex process that typically involves the application of multiple discrete algorithms that modify the program structure irreversibly. However, the design of these algorithms is often monolithic, and they require repetitive implementation to perform similar analyses due to the lack of cooperation. To address this issue, modern optimization techniques, such as equality saturation, allow for exhaustive term rewriting at various levels of inputs, thereby simplifying compiler design.In this paper, we propose equality saturation to optimize sequential codes utilized in directive-based programming for GPUs. Our approach realizes less computation, less memory access, and high memory throughput simultaneously. Our fully-automated framework constructs single-assignment forms from inputs to be entirely rewritten while keeping dependencies and extracts optimal cases. Through practical benchmarks, we demonstrate a significant performance improvement on several compilers. Furthermore, we highlight the advantages of computational reordering and emphasize the signficance of memory-access order for modern GPUs.},
booktitle = {Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {1979–1990},
numpages = {12},
keywords = {Code Generation, Compiler, Directive-Based Programming, GPUs, Program Optimization},
location = {Atlanta, GA, USA},
series = {SC-W '24}
}

@article{10.1145/3708476,
author = {Fu, Xiaoqin and Zaman, Asif and Cai, Haipeng},
title = {DistMeasure: A Framework for Runtime Characterization and Quality Assessment of Distributed Software via Interprocess Communications},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708476},
doi = {10.1145/3708476},
abstract = {A defining, unique aspect of distributed systems lies in interprocess communication (IPC) through which distributed components interact and collaborate toward the holistic system behaviors. This highly decoupled construction intuitively contributes to the scalability, performance, and resiliency advantages of distributed software, but also adds largely to their greater complexity, compared to centralized software. Yet despite the importance of IPC in distributed systems, little is known about how to quantify IPC-induced behaviors in these systems through IPC measurement and how such behaviors may be related to the quality of distributed software. To answer these questions, in this article, we present DistMeasure, a framework for measuring distributed software systems via the lens of IPC hence enabling the study of its correlation with distributed system quality. Underlying DistMeasure is a novel set of IPC metrics that focus on gauging the coupling and cohesion of distributed processes. Through these metrics, DistMeasure quantifies relevant runtime characteristics of distributed systems and their quality relevance, covering a range of quality aspects each via respective direct quality metrics. Further, DistMeasure enables predictive assessment of distributed system quality in those aspects via learning-based anomaly detection with respect to the corresponding quality metrics based on their significant correlations with related IPC metrics. Using DistMeasure, we demonstrated the practicality and usefulness of IPC measurement against 11 real-world distributed systems and their diverse execution scenarios. Among other findings, our results revealed that IPC has a strong correlation with distributed system complexity, performance efficiency, and security. Higher IPC coupling between distributed processes tended to be negatively indicative of distributed software quality, while more cohesive processes have positive quality implications. Yet overall IPC-induced behaviors are largely independent of the system scale, and higher (lower) process coupling does not necessarily come with lower (higher) process cohesion. We also show promising merits (with 98% precision/recall/F1) of IPC measurement (e.g., class-level coupling and process-level cohesion) for predictive anomaly assessment of various aspects (e.g., attack surface and performance efficiency) of distributed system quality.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {74},
numpages = {53},
keywords = {distributed system, interprocess communication, dynamic metrics, software quality}
}

@inproceedings{10.5555/3615924.3615927,
author = {Nascimento, Nathalia and Alencar, Paulo and Cowan, Donald},
title = {Artificial Intelligence vs. Software Engineers: An Empirical Study on Performance and Efficiency using ChatGPT},
year = {2023},
publisher = {IBM Corp.},
address = {USA},
abstract = {In the realm of Software Engineering (SE), automation has become a tangible reality. Artificial Intelligence (AI) has suc-cessfully addressed challenges in project management, mod-eling, testing, and development. Among the latest innova-tions is ChatGPT, an ML-infused chatbot capable of gen-erating programming codes and software testing strategies. Although there is speculation that AI-based computation can boost productivity and even substitute software engineers in software development, empirical evidence supporting such claims is lacking. Moreover, questions remain about their po-tential to address overlooked evaluation metrics like energy efficiency, vulnerability, fairness (i.e., human bias), and safety. This paper probes into these issues with an empirical study, comparing ChatGPT with both novice and expert program-mers using LeetCode contest problems. The investigation focuses on performance and memory-efficiency, while also acknowledging the need for a broader assessment of non-functional requirements. The results suggest that ChatGPT is better than beginners at solving easy and medium prob-lems, but it is not yet proven to beat expert programmers. This paper posits that a comprehensive comparison of soft-ware engineers and AI-based solutions, considering various evaluation criteria, is pivotal in fostering human-machine collaboration, enhancing the reliability of AI-based meth-ods, and understanding task suitability for humans or AI. Furthermore, it facilitates the effective implementation of co-operative work structures and human-in-the-loop processes.},
booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering},
pages = {24–33},
numpages = {10},
keywords = {Software Engineering, AI-based solutions, Performance Evaluation, ChatGPT, Machine Learning},
location = {Las Vegas, NV, USA},
series = {CASCON '23}
}

@inproceedings{10.1145/3701625.3701645,
author = {Dallilo, Felipe Diniz and Delamaro, Marcio Eduardo and Souza, Simone Senger},
title = {A methodology to support the execution of proficiency tests for software quality assessment},
year = {2024},
isbn = {9798400717772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701625.3701645},
doi = {10.1145/3701625.3701645},
abstract = {Consumers often expect our technology products to work perfectly from the first use. However, unlike many physical products whose imperfections are visible and tangible, the software presents a unique challenge. The quest for functional perfection is mitigated by an inherent complexity that often results in unexpected failures.Comparatively, products such as automobiles, household appliances, and even medicines are subjected to rigorous manufacturing and validation processes by laboratories before reaching the consumer. Consolidated methodologies ensure that these products meet pre-defined quality and safety standards.To ensure adequate validation procedures, these laboratories undergo accreditation processes coordinated by regulatory bodies. This activity is known as Proficiency Testing and is applied in several areas, such as Chemistry, Medicine, Forensic Science, among others, with significant academic efforts to improve these procedures.However, in the area of Information Technology (IT), as will be discussed in this work, there is a significant gap. In this context, this article aims to offer a methodological contribution when applying Proficiency Tests to software products. We use Business Process Management and Action Design Research to develop a model adaptable to the dynamism of software products. The proposed model was matured over several Proficiency Tests, where we established a process and supported its execution through computational support that allows structuring, recording, real-time sharing of information, and task automation to apply Proficiency Tests to software products.},
booktitle = {Proceedings of the XXIII Brazilian Symposium on Software Quality},
pages = {49–59},
numpages = {11},
keywords = {Process Assessment, Software Quality Assurance, Software Testing},
location = {
},
series = {SBQS '24}
}

@inproceedings{10.1145/3687997.3695638,
author = {Cunha, Sim\~{a}o and Silva, Lu\'{\i}s and Saraiva, Jo\~{a}o and Fernandes, Jo\~{a}o Paulo},
title = {Trading Runtime for Energy Efficiency: Leveraging Power Caps to Save Energy across Programming Languages},
year = {2024},
isbn = {9798400711800},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3687997.3695638},
doi = {10.1145/3687997.3695638},
abstract = {Energy efficiency of software is crucial in minimizing environmental impact and reducing operational costs of ICT systems. Energy efficiency is therefore a key area of contemporary software language engineering research. A recurrent discussion that excites our community is whether runtime performance is always a proxy for energy efficiency. While a generalized intuition seems to suggest this is the case, this intuition does not align with the fact that energy is the accumulation of power over time; hence, time is only one of the factors in this accumulation. We focus on the other factor, power, and the impact that capping it has on the energy efficiency of running software.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
We conduct an extensive investigation comparing regular and power-capped executions of 9 benchmark programs obtained from The Computer Language Benchmarks Game, across 20 distinct programming languages. Our results show that employing power caps can be used to trade running time, which is degraded, for energy efficiency, which is improved, in all the programming languages and in all benchmarks that were considered. We observe overall energy savings of almost 14% across the 20 programming languages, with notable savings of 27% in Haskell. This saving, however, comes at the cost of an overall increase of the program's execution time of 91% in average.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
We are also able to draw similar observations using language specific benchmarks for programming languages of different paradigms and with different execution models. This is achieved analyzing a wide range of benchmark programs from the nofib Benchmark Suite of Haskell Programs, DaCapo Benchmark Suite for Java, and the Python Performance Benchmark Suite. We observe energy savings of approximately 8% to 21% across the test suites, with execution time increases ranging from 21% to 46%. Notably, the DaCapo suite exhibits the most significant values, with 20.84% energy savings and a 45.58% increase in execution time.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Our results have the potential to drive significant energy savings in the context of computational tasks for which runtime is not critical, including Batch Processing Systems, Background Data Processing and Automated Backups.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {130–142},
numpages = {13},
keywords = {Energy Efficiency, Green Software, Language Benchmarking, Power Cap, Programming Languages},
location = {Pasadena, CA, USA},
series = {SLE '24}
}

@article{10.1145/3717413.3717431,
author = {Bettermann, Michael and De Meer, Hermann},
title = {A Flexible and Holistic Multi-Agent Framework for Local Energy Aggregation Approaches},
year = {2025},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/3717413.3717431},
doi = {10.1145/3717413.3717431},
abstract = {A growing number of communities have expressed a desire for self-sufficiency and the reduction of carbon emissions. These are two of the reasons why the interest of self-governed local energy community approaches has caught the attention of many. In [2], the notion of locality is kept rather vague. However, often the range of a low- or medium-voltage grid level is chosen for, e.g., local energy market designs [1], which coincides with the locality definition in this paper. A key difference between these concepts lies in the coordination approach. For instance, local energy markets focus on utilising market mechanisms, while other aggregation concepts, such as energy cells, rely on more technical approaches and may directly control assets to optimise energy consumption. This paper refers to these methodologies as local aggregation approaches. The goal of this paper is to provide a holistic and flexible model of local aggregation approaches, which provides structure during the design phase, but leaves enough flexibility to allow for adjustments, according to the local communities' needs and capabilities. Throughout this paper, a holistic model refers to a model, which operates in its whole local spectrum (e.g., a low-voltage grid), summarising all major entities and their concerns. The contributions of this paper are:(1) An approach for the development of a holistic model of local aggregation approaches, allowing a coherent realisation of variations, such as virtual power plants or energy islands(2) A holistic model which allows the integration of:(a) Infrastructure provider signalling for intelligent grid operation, such as flexible grid tariffs(b) Communication with external entities, such as the wholesale market(c) Coordination algorithms adjusted to the local environment(d) Strategies for preference maximisation on the consumer level},
journal = {SIGENERGY Energy Inform. Rev.},
month = feb,
pages = {190–192},
numpages = {3},
keywords = {energy optimisation, energy system, multi-agent system, system architecture}
}

@article{10.1145/3529258,
author = {Chang, Tyler H. and Watson, Layne T. and Larson, Jeffrey and Neveu, Nicole and Thacker, William I. and Deshpande, Shubhangi and Lux, Thomas C. H.},
title = {Algorithm 1028: VTMOP: Solver for Blackbox Multiobjective Optimization Problems},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {3},
issn = {0098-3500},
url = {https://doi.org/10.1145/3529258},
doi = {10.1145/3529258},
abstract = {VTMOP is a Fortran 2008 software package containing two Fortran modules for solving computationally expensive bound-constrained blackbox multiobjective optimization problems. VTMOP implements the algorithm of  [32], which handles two or more objectives, does not require any derivatives, and produces well-distributed points over the Pareto front. The first module contains a general framework for solving multiobjective optimization problems by combining response surface methodology, trust region methodology, and an adaptive weighting scheme. The second module features a driver subroutine that implements this framework when the objective functions can be wrapped as a Fortran subroutine. Support is provided for both serial and parallel execution paradigms, and VTMOP is demonstrated on several test problems as well as one real-world problem in the area of particle accelerator optimization.},
journal = {ACM Trans. Math. Softw.},
month = sep,
articleno = {36},
numpages = {34},
keywords = {Multiobjective optimization, blackbox optimization, trust region methods, response surface methodology, adaptive weighting schemes}
}

@inproceedings{10.1145/3634737.3637651,
author = {Marinaro, Tiziano and Buiras, Pablo and Lindner, Andreas and Guanciale, Roberto and Nemati, Hamed},
title = {Beyond Over-Protection: A Targeted Approach to Spectre Mitigation and Performance Optimization},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634737.3637651},
doi = {10.1145/3634737.3637651},
abstract = {Since the advent of Spectre attacks, researchers and practitioners have developed a range of hardware and software measures to counter transient execution attacks. A prime example of such mitigation is speculative load hardening (slh) in LLVM, which protects against leaks by tracking the speculation state and masking values during misspeculation. LLVM relies on static analysis to harden programs using slh that often results in over-protection, which incurs performance overhead. We extended an existing side-channel model validation framework, Scam-V, to check the vulnerability of programs to Spectre-PHT attacks and optimize the protection of programs using the slh approach. We illustrate the efficacy of Scam-V by first demonstrating that it can automatically identify Spectre vulnerabilities in programs, e.g., fragments of crypto-libraries. We then develop an optimization mechanism to validate the necessity of slh hardening w.r.t. the target platform. Our experiments showed that hardening introduced by LLVM in most cases could be improved when the underlying microarchitecture properties are considered.},
booktitle = {Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
pages = {203–216},
numpages = {14},
keywords = {hardware security, side-channel attacks, countermeasures, spectre},
location = {Singapore, Singapore},
series = {ASIA CCS '24}
}

@inproceedings{10.1145/3628354.3629533,
author = {Ibba, G. and Khullar, S. and Tesfai, E. and Neykova, R. and Aufiero, S. and Ortu, M. and Bartolucci, S. and Destefanis, G.},
title = {A Preliminary Analysis of Software Metrics in Decentralised Applications},
year = {2024},
isbn = {9798400704390},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628354.3629533},
doi = {10.1145/3628354.3629533},
abstract = {This study examines software metrics in decentralized applications (dApps) to analyze their structural and behavioral characteristics as they grow in complexity. Sixty dApps were categorized into Small (3 to 29 contracts), Medium (30 to 46 contracts), and Large (47 to 206 contracts) based on their contract count. Initial analysis showed a non-normal data distribution, leading to the use of Spearman's correlation method. Findings revealed that Medium dApps have strong correlations between metrics like 'Average Local Variables' and 'Maximum Local Variables', while Large dApps show higher correlations between 'Number of Functions' and 'State Variable Count', indicating more complex contract structures. The higher Coupling Between Objects (CBO) in large dApps suggests increased interactions with other contracts or libraries, potentially elevating security risks. These insights are valuable for developers and stakeholders in the blockchain and IoT sectors, aiding in understanding how dApps evolve with increasing complexity and the implications on software metric relationships.},
booktitle = {Proceedings of the Fifth ACM International Workshop on Blockchain-Enabled Networked Sensor Systems},
pages = {27–33},
numpages = {7},
keywords = {Code, Correct, Do, Not, Paper, Put, Terms, This, Us, Your, for, the},
location = {Istanbul, Turkiye},
series = {BlockSys '23}
}

@proceedings{10.1145/3639477,
title = {ICSE-SEIP '24: Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3691620.3695336,
author = {Cinkusz, Konrad and Chudziak, Jaroslaw A.},
title = {Towards LLM-augmented multiagent systems for agile software engineering},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695336},
doi = {10.1145/3691620.3695336},
abstract = {A cognitive multi-agent ecosystem designed for efficient software engineering using Agile methodologies can significantly improve software development processes. Key components include the integration of Multi-Agent Systems (MAS) and Large Language Models (LLMs), utilizing Dynamic Context techniques for agent profiling, and Theory of Mind to enhance collaboration. The CogniSim Ecosystem analyzes problems, proposes solutions, constructs and validates plans, and coordinates specialized agents playing roles such as developers, executors, quality checkers, and methodology reviewers. These agents produce documentation, models, and diagrams (e.g., UML) while adhering to predefined quality and performance measures. The ecosystem also simulates the impact of various team configurations on problem-solving effectiveness, helping organizations identify optimal team structures. Case studies and simulations demonstrate its practical applications.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {2476–2477},
numpages = {2},
keywords = {multi-agent systems, large language models, software engineering, collaboration automation, methodologies, SAFe, cognisim},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@proceedings{10.1145/3559712,
title = {SBCARS '22: Proceedings of the 16th Brazilian Symposium on Software Components, Architectures, and Reuse},
year = {2022},
isbn = {9781450397452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Uberlandia, Brazil}
}

@article{10.1145/3709352,
author = {K\"{o}nig, Christoph and Lang, Daniel J. and Schaefer, Ina},
title = {Sustainable Software Engineering: Concepts, Challenges, and Vision},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3709352},
doi = {10.1145/3709352},
abstract = {Information and communication technology (ICT) offers promising opportunities to address global sustainability challenges such as climate change and social inequality by enabling energy savings and social innovations. At the same time, ICT threatens to exacerbate these crises, as evident in the increasing consumption of resources and widening digital inequalities. As one of the enablers of ICT, software engineering plays a key role to tackle the problems and explore the potentials of ICT for sustainability. However, sustainability in software engineering is still a niche topic, with little structure, a limited understanding of sustainability and few comprehensive strategies. In this paper, we introduce the main concepts of sustainable software engineering, critically review the state of research and identify seven future research challenges across all research areas. We further present our research vision – sustainability-driven software engineering and transdisciplinary research formats – and outline a research roadmap with the key steps to be achieved by 2030.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {Sustainability, Sustainable development, ICT, Sustainable Software Engineering, Sustainability assessments, Transdisciplinary research, Real-world lab}
}

@article{10.1145/3719005,
author = {Ran, Dezhi and Wu, Mengzhou and Yang, Wei and Xie, Tao},
title = {Foundation Model Engineering: Engineering Foundation Models Just as Engineering Software},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3719005},
doi = {10.1145/3719005},
abstract = {By treating data and models as source code, Foundation Models (FMs) become a new type of software. Mirroring the concept of software crisis, the increasing complexity of FMs makes FM crisis a tangible concern in the coming decade, appealing for new theories and methodologies from the field of software engineering. In this article, we outline our vision of introducing FM engineering, a strategic response to the anticipated FM crisis with principled engineering methodologies. FM engineering aims to mitigate potential issues in FM development and application through the introduction of declarative, automated, and unified programming interfaces for both data and model management, reducing the complexities involved in working with FMs by providing a more structured and intuitive process for developers. Through the establishment of FM engineering, we aim to provide a robust, automated, and extensible framework that addresses the imminent challenges, and discover new research opportunities for the software engineering field.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
keywords = {Software Engineering, Artificial Intelligence, Machine Learning, Foundation Model, Data Management, Model Management, Version Control Systems}
}

@article{10.1145/3355089.3356575,
author = {Cornill\`{e}re, Victor and Djelouah, Abdelaziz and Yifan, Wang and Sorkine-Hornung, Olga and Schroers, Christopher},
title = {Blind image super-resolution with spatially variant degradations},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3355089.3356575},
doi = {10.1145/3355089.3356575},
abstract = {Existing deep learning approaches to single image super-resolution have achieved impressive results but mostly assume a setting with fixed pairs of high resolution and low resolution images. However, to robustly address realistic upscaling scenarios where the relation between high resolution and low resolution images is unknown, blind image super-resolution is required. To this end, we propose a solution that relies on three components: First, we use a degradation aware SR network to synthesize the HR image given a low resolution image and the corresponding blur kernel. Second, we train a kernel discriminator to analyze the generated high resolution image in order to predict errors present due to providing an incorrect blur kernel to the generator. Finally, we present an optimization procedure that is able to recover both the degradation kernel and the high resolution image by minimizing the error predicted by our kernel discriminator. We also show how to extend our approach to spatially variant degradations that typically arise in visual effects pipelines when compositing content from different sources and how to enable both local and global user interaction in the upscaling process.},
journal = {ACM Trans. Graph.},
month = nov,
articleno = {166},
numpages = {13},
keywords = {blind image super-resolution, deep learning, image super-resolution}
}

@inproceedings{10.1145/3382494.3410677,
author = {Shu, Yangyang and Sui, Yulei and Zhang, Hongyu and Xu, Guandong},
title = {Perf-AL: Performance Prediction for Configurable Software through Adversarial Learning},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410677},
doi = {10.1145/3382494.3410677},
abstract = {Context: Many software systems are highly configurable. Different configuration options could lead to varying performances of the system. It is difficult to measure system performance in the presence of an exponential number of possible combinations of these options.Goal: Predicting software performance by using a small configuration sample.Method: This paper proposes Perf-AL to address this problem via adversarial learning. Specifically, we use a generative network combined with several different regularization techniques (L1 regularization, L2 regularization and a dropout technique) to output predicted values as close to the ground truth labels as possible. With the use of adversarial learning, our network identifies and distinguishes the predicted values of the generator network from the ground truth value distribution. The generator and the discriminator compete with each other by refining the prediction model iteratively until its predicted values converge towards the ground truth distribution.Results: We argue that (i) the proposed method can achieve the same level of prediction accuracy, but with a smaller number of training samples. (ii) Our proposed model using seven real-world datasets show that our approach outperforms the state-of-the-art methods. This help to further promote software configurable performance.Conclusion: Experimental results on seven public real-world datasets demonstrate that PERF-AL outperforms state-of-the-art software performance prediction methods.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {16},
numpages = {11},
keywords = {Software performance prediction, adversarial learning, configurable systems, regularization},
location = {Bari, Italy},
series = {ESEM '20}
}

@article{10.1145/3676960,
author = {Meng, Fanyi and Wang, Ying and Chong, Chun Yong and Yu, Hai and Zhu, Zhiliang},
title = {Evolution-Aware Constraint Derivation Approach for Software Remodularization},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {8},
issn = {1049-331X},
url = {https://doi.org/10.1145/3676960},
doi = {10.1145/3676960},
abstract = {Existing software clustering techniques tend to ignore prior knowledge from domain experts, leading to results (suggested big-bang remodularization actions) that cannot be acceptable to developers. Incorporating domain experts knowledge or constraints during clustering ensures the obtained modularization aligns with developers’ perspectives, enhancing software quality. However, manual review by knowledgeable domain experts for constraint generation is time-consuming and labor-intensive. In this article, we propose an evolution-aware constraint derivation approach, Escort, which automatically derives clustering constraints based on the evolutionary history from the analyzed software. Specifically, Escort can serve as an alternative approach to derive implicit and explicit constraints in situations where domain experts are absent. In the subsequent constrained clustering process, Escort can be considered as a framework to help supplement and enhance various unconstrained clustering techniques to improve their accuracy and reliability. We evaluate Escort based on both quantitative and qualitative analysis. In quantitative validation, Escort, using generated clustering constraints, outperforms seven classic unconstrained clustering techniques. Qualitatively, a survey with developers from five IT companies indicates that 89% agree with Escort’s clustering constraints. We also evaluate the utility of refactoring suggestions from our constrained clustering approach, with 54% acknowledged by project developers, either implemented or planned for future releases.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {207},
numpages = {43},
keywords = {Constrained clustering, software remodularization, software refactoring}
}

@article{10.1145/3711935,
author = {Wagner, Friedrich and N\"{u}\ss{}lein, Jonas and Liers, Frauke},
title = {Enhancing Quantum Algorithms for Quadratic Unconstrained Binary Optimization via Integer Programming},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
url = {https://doi.org/10.1145/3711935},
doi = {10.1145/3711935},
abstract = {To date, research in quantum computation promises potential for outperforming classical heuristics in combinatorial optimization. However, when aiming at provable optimality, one has to rely on classical exact methods like integer programming. State-of-the-art integer programming algorithms can compute strong relaxation bounds even for hard instances, but may have to enumerate a large number of subproblems for determining an optimum solution. If the potential of quantum computing is realized, it can be expected that in particular finding high-quality solutions for hard problems can be done fast. Still, near-future quantum hardware considerably limits the size of treatable problems. In this work, we go one step into integrating the potentials of quantum and classical techniques for combinatorial optimization. We propose a hybrid heuristic for the weighted maximum-cut problem and for quadratic unconstrained binary optimization. The heuristic employs a linear programming relaxation, rendering it well-suited for integration into exact branch-and-cut algorithms. For large instances, we reduce the problem size according to a linear relaxation such that the reduced problem can be handled by quantum machines of limited size. Moreover, we improve the applicability of depth-1 QAOA, a parameterized quantum algorithm, by deriving a parameter estimate for arbitrary instances. We present numerous computational results from real quantum hardware.},
journal = {ACM Transactions on Quantum Computing},
month = feb,
articleno = {12},
numpages = {28},
keywords = {Integer programming, combinatorial optimization, quantum computation}
}

@article{10.1145/3720538,
author = {Li, Li and Gao, Xiang and Sun, Hailong and Hu, Chunming and Sun, Xiaoyu and Wang, Haoyu and Cai, Haipeng and Su, Ting and Luo, Xiapu and Bissyande, Tegawend\'{e} and Klein, Jacques and Grundy, John and Xie, Tao and Chen, Haibo and Wang, Huaimin},
title = {Software Engineering for OpenHarmony: A Research Roadmap},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3720538},
doi = {10.1145/3720538},
abstract = {Mobile software engineering has been a hot research topic for decades. Our fellow researchers have proposed various approaches (with over 7,000 publications for Android alone) in this field that essentially contributed to the great success of the current mobile ecosystem. Existing research efforts mainly focus on popular mobile platforms, namely Android and iOS. OpenHarmony, a newly open-sourced mobile platform, has rarely been considered, although it is the one requiring the most attention as OpenHarmony is expected to occupy one-third of the market in China (if not in the world). To fill the gap, we present to the mobile software engineering community a research roadmap for encouraging our fellow researchers to contribute promising approaches to OpenHarmony. Specifically, we start by presenting a tertiary study of mobile software engineering, attempting to understand what problems have been targeted by the mobile community and how they have been resolved. We then summarize the existing (limited) achievements of OpenHarmony and subsequently highlight the research gap between Android/iOS and OpenHarmony. This research gap eventually helps in forming the roadmap for conducting software engineering research for OpenHarmony.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = feb
}

@inproceedings{10.1145/3593434.3593436,
author = {Ramos-Vidal, Delfina},
title = {Reengineering legacy document information systems: Challenges and solutions},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593434.3593436},
doi = {10.1145/3593434.3593436},
abstract = {Since internet applications have reached a satisfactory level of maturity, large information systems have been developed to manage and facilitate access to documents. Simultaneously, there was an enormous international effort to digitise documents, enabling access via the internet. This endeavour facilitated the access of researchers to extensive document repositories and libraries, while also aiding companies in organising their documents. Two decades later, these vast databases are reasonably clean and well-organised, although the software used to manage and feed them is gradually becoming obsolete. Therefore, it is imperative to continuously reengineer the software to maintain optimal functionality. Furthermore, after the initial effort to digitise documents and create the initial metadata, it is reasonable to augment the metadata information pertaining to the documents. As such, two necessities are apparent: improving support for reengineering legacy document information systems and enabling data model updates and schema evolution to accommodate new information. Our goal is to automate the reengineering process as a whole.},
booktitle = {Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
pages = {286–291},
numpages = {6},
keywords = {automated development, document information systems, schema evolution, software reengineering},
location = {Oulu, Finland},
series = {EASE '23}
}

@inproceedings{10.1145/3524844.3528069,
author = {Ghari, Soude},
title = {Devops for digital business: optimizing the performance and economic efficiency of software products for digital business},
year = {2022},
isbn = {9781450393058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524844.3528069},
doi = {10.1145/3524844.3528069},
abstract = {Software is playing an increasingly crucial role for any modern enterprise. Due to the high volatility of software and its environment, management of software change has become a crucial ability of a digital enterprise. Indeed, the ever-changing technical aspects of cloud-based software might influence its economic aspects at the same rate. This PhD work proposes an autonomic management framework for economic-driven adaptive resource allocation, during which performance and economics are considered simultaneously. The framework consists of a hybrid performance-economic model for cloud software systems and a self-adaptive system for resource management at run time. The proposed framework will be developed and evaluated on use cases from the domain of financial services. The objective in these use cases is to improve productivity of data scientists when they train and deploy analytical and ML models, while maintaining a low resource consumption. Considering productivity and cost as the economic factors, and latency and resource consumption as the technical ones, we will use the proposed hybrid model to guide initial deployment, and the self-adaptive system to adjust the deployment at run time.},
booktitle = {Proceedings of the 17th Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {53–57},
numpages = {5},
location = {Pittsburgh, Pennsylvania},
series = {SEAMS '22}
}

@inproceedings{10.5555/3712729.3712770,
author = {Chen, Xiaotie and Woodruff, David L.},
title = {Importance Sampling in Optimization under Uncertainty Using Surrogate Models},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {For the purpose of computing the expected value of a stochastic optimization problem via simulation, we propose a method for efficiently constructing importance sampling distributions using surrogate modeling. A software implementation of the methods called SMAIS is available on github. We use this software in experiments to demonstrate that our method can outperform Monte Carlo simulation. We also show good parallel efficiency for up to 16 processors allowing a speed up of more than 10. Our method uses adaptive sample sizes so it is not very sensitive to sample size parameters.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {501–512},
numpages = {12},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@article{10.1145/3701232,
author = {Jiang, He and Zou, Peiyu and Li, Xiaochen and Zhou, Zhide and Zhao, Xu and Zhang, Yi and Guo, Shikai},
title = {DeLoSo: Detecting Logic Synthesis Optimization Faults Based on Configuration Diversity},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {1084-4309},
url = {https://doi.org/10.1145/3701232},
doi = {10.1145/3701232},
abstract = {Logic synthesis tools are the core components of digital circuit design, which convert programs written in hardware description languages into gate-level netlists and optimize the netlists. However, the netlist optimization is complex, with numerous optimization parameters to be configured. Any minor optimization faults in logic synthesis tools may cause circuit diagrams to significantly deviate from the original design, posing risks in target systems. We propose DeLoSo, the Detector of Logic Synthesis optimization faults, the first method specifically designed to identify potential faults in the optimization processes of logic synthesis tools. DeLoSo relies on netlist differences and parameter variations to guide the generation of diverse Logic Synthesis Optimization Configuration (LSOC) combinations to thoroughly test the optimization process. DeLoSo consists of three components: LSOC generator, which generates diverse LSOC combinations through configuration recombination and mutation; LSOC diversity evaluator, which assesses the diversity of optimization configurations; and LSOC validator, which validates the generated LSOC combinations to discover optimization faults. DeLoSo identified 19 faults in two established logic synthesis tools (i.e., Vivado and Yosys); 15 of them have been fixed by vendors. Particularly, the community maintainers of Yosys have considered incorporating DeLoSo into Yosys’ existing test suite.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = dec,
articleno = {12},
numpages = {26},
keywords = {Logic synthesis tools, optimization configurations, fault detection}
}

@article{10.1145/3707455,
author = {Jiang, Yingjie and Mo, Ran and Zhan, Wenjing and Wang, Dongyu and Li, Zengyang and Ma, Yutao},
title = {Leveraging Modular Architecture for Bug Characterization and Analysis in Automated Driving Software},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3707455},
doi = {10.1145/3707455},
abstract = {With the rapid advancement of automated driving technology, numerous manufacturers deploy vehicles with auto-driving features. This highlights the importance of ensuring the quality of automated driving software. To achieve this, characterizing bugs in automated driving software is important, as it can facilitate bug detection and bug fixes, thereby ensuring software quality. Automated driving software typically has a modular architecture, where software is divided into multiple modules, each designed for its own functionality for automated driving. This may lead to varying bug characteristics. Additionally, our recent study has shown a correlation between bugs caused by code clones and the functionalities of modules in automated driving software. Hence, we consider the modular structure when analyzing bug characteristics. In this paper, we analyze 3,078 bugs from two representative open-source Level-4 automated driving systems, Apollo and Autoware. By analyzing the bug report description, title, and developers’ discussions, we have identified 20 bug symptoms and 17 bug-fixing strategies, and analyzed their relationships with the respective modules. Our analysis achieves 12 main findings offering a comprehensive view of bug characteristics in automated driving software. We believe our findings can help developers better understand and manage bugs in automated driving software, thereby improving software quality and reliability.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {Automated Driving Software, Software Modules, Bug Characterization, Bug Analysis}
}

@article{10.5555/3586589.3586795,
author = {Thi, Hoai An Le and Luu, Hoang Phuc Hau and Le, Hoai Minh and Dinh, Tao Pham},
title = {Stochastic DCA with variance reduction and applications in machine learning},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {We design stochastic Difference-of-Convex-functions Algorithms (DCA) for solving a class of structured Difference-of-Convex-functions (DC) problems. As the standard DCA requires the full information of (sub)gradients which could be expensive in large-scale settings, stochastic approaches rely upon stochastic information instead. However, stochastic estimations generate additional variance terms making stochastic algorithms unstable. Therefore, we integrate some novel variance reduction techniques including SVRG and SAGA into our design. The almost sure convergence to critical points of the proposed algorithms is established and the algorithms' complexities are analyzed. To study the efficiency of our algorithms, we apply them to three important problems in machine learning: nonnegative principal component analysis, group variable selection in multiclass logistic regression, and sparse linear regression. Numerical experiments have shown the merits of our proposed algorithms in comparison with other state-of-the-art stochastic methods for solving nonconvex large-sum problems.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {206},
numpages = {44},
keywords = {DC programming, DCA, DCA-SVRG, DCA-SAGA, variance reduction technique}
}

@article{10.1145/3324783,
author = {Eilers, Marco and M\"{u}ller, Peter and Hitz, Samuel},
title = {Modular Product Programs},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {0164-0925},
url = {https://doi.org/10.1145/3324783},
doi = {10.1145/3324783},
abstract = {Many interesting program properties like determinism or information flow security are hyperproperties, that is, they relate multiple executions of the same program. Hyperproperties can be verified using relational logics, but these logics require dedicated tool support and are difficult to automate. Alternatively, constructions such as self-composition represent multiple executions of a program by one product program, thereby reducing hyperproperties of the original program to trace properties of the product. However, existing constructions do not fully support procedure specifications, for instance, to derive the determinism of a caller from the determinism of a callee, making verification non-modular.We present modular product programs, a novel kind of product program that permits hyperproperties in procedure specifications and, thus, can reason about calls modularly. We provide a general formalization of our product construction and prove it sound and complete. We demonstrate its expressiveness by applying it to information flow security with advanced features such as declassification and termination-sensitivity. Modular product programs can be verified using off-the-shelf verifiers; we have implemented our approach for both secure information flow and general hyperproperties using the Viper verification infrastructure. Our evaluation demonstrates that modular product programs can be used to prove hyperproperties for challenging examples in reasonable time.},
journal = {ACM Trans. Program. Lang. Syst.},
month = nov,
articleno = {3},
numpages = {37},
keywords = {Hyperproperties, product programs}
}

@proceedings{10.1145/3611643,
title = {ESEC/FSE 2023: Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to ESEC/FSE 2023, the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ESEC/FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. ESEC/FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {San Francisco, CA, USA}
}

@inproceedings{10.1145/3718491.3718576,
author = {Yao, Na and Liu, Yang and Liu, Nian},
title = {Modeling and Simulation of Export Path Design for Digital Economy Products Based on Machine Learning Algorithms},
year = {2025},
isbn = {9798400710865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3718491.3718576},
doi = {10.1145/3718491.3718576},
abstract = {In the rapidly evolving landscape of the digital economy, understanding and optimizing export paths for digital products is crucial for enhancing competitiveness and market reach. This study presents a comprehensive framework for modeling and simulating export paths of digital economy products utilizing advanced machine learning algorithms. By integrating data from various sources, including market trends, consumer behavior, and regulatory environments, we develop predictive models that identify optimal export strategies. Our methodology employs techniques such as clustering, regression analysis, and neural networks to analyze complex datasets and uncover hidden patterns in export dynamics. The results demonstrate the efficacy of machine learning in forecasting demand, assessing market entry risks, and optimizing supply chain logistics. Additionally, we provide insights into the impact of external factors, such as geopolitical shifts and technological advancements, on export performance. This research not only contributes to the theoretical understanding of digital product exports but also offers practical guidelines for businesses seeking to navigate the global digital marketplace effectively. Future work will focus on refining the models through real-time data integration and exploring the role of emerging technologies in shaping export strategies.},
booktitle = {Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum},
pages = {517–522},
numpages = {6},
keywords = {Digital Economy, Export Paths, Machine Learning Algorithms, Modeling and Simulation, Predictive Analytics},
location = {
},
series = {AIBDF '24}
}

@inproceedings{10.1145/3551349.3556899,
author = {Fernandez-Amoros, David and Heradio, Ruben and Mayr-Dorn, Christoph and Egyed, Alexander},
title = {Scalable Sampling of Highly-Configurable Systems: Generating Random Instances of the Linux Kernel},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556899},
doi = {10.1145/3551349.3556899},
abstract = {Software systems are becoming increasingly configurable. A paradigmatic example is the Linux kernel, which can be adjusted for a tremendous variety of hardware devices, from mobile phones to supercomputers, thanks to the thousands of configurable features it supports. In principle, many relevant problems on configurable systems, such as completing a partial configuration to get the system instance that consumes the least energy or optimizes any other quality attribute, could be solved through exhaustive analysis of all configurations. However, configuration spaces are typically colossal and cannot be entirely computed in practice. Alternatively, configuration samples can be analyzed to approximate the answers. Generating those samples is not trivial since features usually have inter-dependencies that constrain the configuration space. Therefore, getting a single valid configuration by chance is extremely unlikely. As a result, advanced samplers are being proposed to generate random samples at a reasonable computational cost. However, to date, no sampler can deal with highly configurable complex systems, such as the Linux kernel. This paper proposes a new sampler that does scale for those systems, based on an original theoretical approach called extensible logic groups. The sampler is compared against five other approaches. Results show our tool to be the fastest and most scalable one.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {89},
numpages = {12},
keywords = {Kconfig, SAT, binary decision diagrams, configurable systems, random sampling, software product lines, variability modeling},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@article{10.1145/3695988,
author = {Hou, Xinyi and Zhao, Yanjie and Liu, Yue and Yang, Zhou and Wang, Kailong and Li, Li and Luo, Xiapu and Lo, David and Grundy, John and Wang, Haoyu},
title = {Large Language Models for Software Engineering: A Systematic Literature Review},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {8},
issn = {1049-331X},
url = {https://doi.org/10.1145/3695988},
doi = {10.1145/3695988},
abstract = {Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a Systematic Literature Review (SLR) on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We selected and analyzed 395 research articles from January 2017 to January 2024 to answer four key Research Questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, pre-processing, and application, highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and highlighting promising areas for future study. Our artifacts are publicly available at .},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {220},
numpages = {79},
keywords = {Software Engineering, Large Language Model, Survey}
}

@article{10.1145/3708524,
author = {Robinson, Diana and Cabrera, Christian and Gordon, Andrew D. and Lawrence, Neil D. and Mennen, Lars},
title = {Requirements are All You Need: The Final Frontier for End-User Software Engineering},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708524},
doi = {10.1145/3708524},
abstract = {What if end users could own the software development life cycle from conception to deployment using only requirements expressed in language, images, video or audio? We explore this idea, building on the capabilities that Generative Artificial Intelligence brings to software generation and maintenance techniques. How could designing software in this way better serve end users? What are the implications of this process for the future of end-user software engineering and the software development life cycle? We discuss the research needed to bridge the gap between where we are today and these imagined systems of the future.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {End-User Software Engineering, End-User Programming, Large Language Models}
}

@inproceedings{10.1145/3656019.3676947,
author = {Rhyner, Steve and Luo, Haocong and G\'{o}mez-Luna, Juan and Sadrosadati, Mohammad and Jiang, Jiawei and Olgun, Ataberk and Gupta, Harshita and Zhang, Ce and Mutlu, Onur},
title = {PIM-Opt: Demystifying Distributed Optimization Algorithms on a Real-World Processing-In-Memory System},
year = {2024},
isbn = {9798400706318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3656019.3676947},
doi = {10.1145/3656019.3676947},
abstract = {Modern Machine Learning (ML) training on large-scale datasets is a very time-consuming workload. It relies on the optimization algorithm Stochastic Gradient Descent (SGD) due to its effectiveness, simplicity, and generalization performance (i.e., test performance on unseen data). Processor-centric architectures (e.g., CPUs, GPUs) commonly used for modern ML training workloads based on SGD are bottlenecked by data movement between the processor and memory units due to the poor data locality in accessing large training datasets. As a result, processor-centric architectures suffer from low performance and high energy consumption while executing ML training workloads. Processing-In-Memory (PIM) is a promising solution to alleviate the data movement bottleneck by placing the computation mechanisms inside or near memory. Several prior works propose PIM techniques to accelerate ML training; however, prior works either do not consider real-world PIM systems or evaluate algorithms that are not widely used in modern ML training. Our goal is to understand the capabilities and characteristics of popular distributed SGD algorithms on real-world PIM systems to accelerate data-intensive ML training workloads. To this end, we 1) implement several representative centralized parallel SGD algorithms, i.e., based on a central node responsible for synchronization and orchestration, on the real-world general-purpose UPMEM PIM system, 2) rigorously evaluate these algorithms for ML training on large-scale datasets in terms of performance, accuracy, and scalability, 3) compare to conventional CPU and GPU baselines, and 4) discuss implications for future PIM hardware. We highlight the need for a shift to an algorithm-hardware codesign to enable decentralized parallel SGD algorithms in real-world PIM systems, which significantly reduces the communication cost and improves scalability. Our results demonstrate three major findings: 1) The general-purpose UPMEM PIM system can be a viable alternative to state-of-the-art CPUs and GPUs for many memory-bound ML training workloads, especially when operations and datatypes are natively supported by PIM hardware, 2) it is important to carefully choose the optimization algorithms that best fit PIM, and 3) the UPMEM PIM system does not scale approximately linearly with the number of nodes for many data-intensive ML training workloads. We open source all our code to facilitate future research at https://github.com/CMU-SAFARI/PIM-Opt.},
booktitle = {Proceedings of the 2024 International Conference on Parallel Architectures and Compilation Techniques},
pages = {201–218},
numpages = {18},
keywords = {Distributed Optimization Algorithms, ML, ML training, Optimization, Processing-In-Memory, Scalability, Stochastic Gradient Descent},
location = {Long Beach, CA, USA},
series = {PACT '24}
}

@inproceedings{10.1145/3503221.3508411,
author = {Zheng, Liyan and Zhai, Jidong and Tang, Xiongchao and Wang, Haojie and Yu, Teng and Jin, Yuyang and Song, Shuaiwen Leon and Chen, Wenguang},
title = {Vapro: performance variance detection and diagnosis for production-run parallel applications},
year = {2022},
isbn = {9781450392044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503221.3508411},
doi = {10.1145/3503221.3508411},
abstract = {Performance variance is a serious problem for parallel applications, which can cause performance degradation and make applications' behavior hard to understand. Therefore, detecting and diagnosing performance variance are of crucial importance for users and application developers. However, previous detection approaches either bring too large overhead and hurt applications' performance, or rely on nontrivial source code analysis that is impractical for production-run parallel applications.In this work, we propose Vapro, a performance variance detection and diagnosis framework for production-run parallel applications. Our approach is based on an important observation that most parallel applications contain code snippets that are repeatedly executed with fixed workload, which can be used for performance variance detection. To effectively identify these snippets at runtime even without program source code, we introduce State Transition Graph (STG) to track program execution and then conduct lightweight workload analysis on STG to locate variance. To diagnose the detected variance, Vapro leverages a progressive diagnosis method based on a hybrid model leveraging variance breakdown and statistical analysis. Results show that the performance overhead of Vapro is only 1.38% on average. Vapro can detect the variance in real applications caused by hardware bugs, memory, and IO. After fixing the detected variance, the standard deviation of the execution time is reduced by up to 73.5%. Compared with the state-of-the-art variance detection tool based on source code analysis, Vapro achieves 30.0% higher detection coverage.},
booktitle = {Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {150–162},
numpages = {13},
keywords = {anomaly detection, performance variance, system noise},
location = {Seoul, Republic of Korea},
series = {PPoPP '22}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00012,
author = {Nuryyev, Batyr and Nadi, Sarah and Bhuiyan, Nazim Uddin and Banderali, Leonardo},
title = {Challenges of implementing software variability in eclipse OMR: an interview study},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00012},
doi = {10.1109/ICSE-SEIP52600.2021.00012},
abstract = {Software variability is the ability of a software system to be customized or configured for a particular context. In this paper, we discuss our experience investigating software variability implementation challenges in practice. Eclipse OMR, developed by IBM, is a set of highly configurable C++ components for building language runtimes; it supports multiple programming languages and target architectures. We conduct an interview study with 6 Eclipse OMR developers and identify 8 challenges incurred by the existing variability implementation, and 3 constraints that need to be taken into account for any reengineering effort. We discuss these challenges and investigate the literature and existing open-source systems for potential solutions. We contribute a solution for one of the challenges, namely adding variability to enumerations and arrays. We also share our experiences and lessons learned working with a large-scale highly configurable industry project. For example, we found that the "latest and greatest" research solutions may not always be favoured by developers due to small practical considerations such as build dependencies, or even C++ version constraints.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {31–40},
numpages = {10},
keywords = {eclipse OMR, language runtimes, software variability, variability implementation},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1109/SHARK.2009.5069114,
author = {Unphon, Hataichanok},
title = {Making use of architecture throughout the software life cycle - How the build hierarchy can facilitate product line development},
year = {2009},
isbn = {9781424437269},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SHARK.2009.5069114},
doi = {10.1109/SHARK.2009.5069114},
abstract = {This paper presents an empirical study of how the application of genuine architecture can be employed beyond the design phase of product line development. The study is based on a co-operative research project with a company developing product line architecture for hydraulic modelling software. By concretising the architecture as a build hierarchy the architecture mediates the evolution of the design throughout the whole software life cycle. The empirical evidence has confirmed the improvements of (1) the software quality and flexibility, (2) the communication and cooperation with new developers, (3) the distribution of work and parallel implementation, and (4) the foreseen usage by hydraulic and environmental consultants who tailor the software. Our research further indicates requirements for the architectural analysis tools that are deliberately embedded in the daily development practices.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Sharing and Reusing Architectural Knowledge},
pages = {41–48},
numpages = {8},
series = {SHARK '09}
}

@article{10.1145/3039207,
author = {Hirzel, Martin and Schneider, Scott and Gedik, Bu\u{g}ra},
title = {SPL: An Extensible Language for Distributed Stream Processing},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0164-0925},
url = {https://doi.org/10.1145/3039207},
doi = {10.1145/3039207},
abstract = {Big data is revolutionizing how all sectors of our economy do business, including telecommunication, transportation, medical, and finance. Big data comes in two flavors: data at rest and data in motion. Processing data in motion is stream processing. Stream processing for big data analytics often requires scale that can only be delivered by a distributed system, exploiting parallelism on many hosts and many cores. One such distributed stream processing system is IBM Streams. Early customer experience with IBM Streams uncovered that another core requirement is extensibility, since customers want to build high-performance domain-specific operators for use in their streaming applications. Based on these two core requirements of distribution and extensibility, we designed and implemented the Streams Processing Language (SPL). This article describes SPL with an emphasis on the language design, distributed runtime, and extensibility mechanism. SPL is now the gateway for the IBM Streams platform, used by our customers for stream processing in a broad range of application domains.},
journal = {ACM Trans. Program. Lang. Syst.},
month = mar,
articleno = {5},
numpages = {39},
keywords = {Stream processing}
}

@proceedings{10.1145/3679005,
title = {ICOOOLPS 2024: Proceedings of the 19th ACM International Workshop on Implementation, Compilation, Optimization of OO Languages, Programs and Systems},
year = {2024},
isbn = {9798400711169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 2024 edition of International Workshop on Implementation, Compilation, Optimization of Object-Oriented Languages, Programs and Systems (ICOOOLPS 2024), held on September 20th at the Vienna University of Technology, co-located with ECOOP and ISSTA 2024.},
location = {Vienna, Austria}
}

@inproceedings{10.1109/ASE.2013.6693103,
author = {Pohl, Richard and Stricker, Vanessa and Pohl, Klaus},
title = {Measuring the structural complexity of feature models},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693103},
doi = {10.1109/ASE.2013.6693103},
abstract = {The automated analysis of feature models (FM) is based on SAT, BDD, and CSP - known NP-complete problems. Therefore, the analysis could have an exponential worst-case execution time. However, for many practical relevant analysis cases, state-of-the-art (SOTA) analysis tools quite successfully master the problem of exponential worst-case execution time based on heuristics. So far, however, very little is known about the structure of FMs that cause the cases in which the execution time (hardness) for analyzing a given FM increases unpredictably for SOTA analysis tools. In this paper, we propose to use width measures from graph theory to characterize the structural complexity of FMs as a basis for an estimation of the hardness of analysis operations on FMs with SOTA analysis tools. We present an experiment that we use to analyze the reasonability of graph width measures as metric for the structural complexity of FMs and the hardness of FM analysis. Such a complexity metric can be used as a basis for a unified method to systematically improve SOTA analysis tools.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {454–464},
numpages = {11},
keywords = {automated analysis, feature model, performance measurement, software product line},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@article{10.1145/3676847,
author = {Gao, Jianhua and Ji, Weixing and Wang, Yizhuo},
title = {Optimization of Large-Scale Sparse Matrix-Vector Multiplication on Multi-GPU Systems},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3676847},
doi = {10.1145/3676847},
abstract = {Sparse matrix-vector multiplication (SpMV) is one of the important kernels of many iterative algorithms for solving sparse linear systems. The limited storage and computational resources of individual GPUs restrict both the scale and speed of SpMV computing in problem-solving. As real-world engineering problems continue to increase in complexity, the imperative for collaborative execution of iterative solving algorithms across multiple GPUs is increasingly apparent. Although the multi-GPU-based SpMV takes less kernel execution time, it also introduces additional data transmission overhead, which diminishes the performance gains derived from parallelization across multi-GPUs. Based on the non-zero elements distribution characteristics of sparse matrices and the tradeoff between redundant computations and data transfer overhead, this article introduces a series of SpMV optimization techniques tailored for multi-GPU environments and effectively enhances the execution efficiency of iterative algorithms on multiple GPUs. First, we propose a two-level non-zero elements-based matrix partitioning method to increase the overlap of kernel execution and data transmission. Then, considering the irregular non-zero elements distribution in sparse matrices, a long-row-aware matrix partitioning method is proposed to hide more data transmissions. Finally, an optimization using redundant and inexpensive short-row execution to exchange costly data transmission is proposed. Our experimental evaluation demonstrates that, compared with the SpMV on a single GPU, the proposed method achieves an average speedup of 2.00\texttimes{} and 1.85\texttimes{} on platforms equipped with two RTX 3090 and two Tesla V100-SXM2, respectively. The average speedup of 2.65\texttimes{} is achieved on a platform equipped with four Tesla V100-SXM2.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {69},
numpages = {24},
keywords = {Multi-GPU system, sparse matrix-vector multiplication, data transmission hiding, sparse matrix partitioning}
}

@article{10.1145/3712004,
author = {Casadei, Roberto and Aguzzi, Gianluca and Audrito, Giorgio and Damiani, Ferruccio and Pianini, Danilo and Scarso, Giordano and Torta, Gianluca and Viroli, Mirko},
title = {Software Engineering for Collective Cyber-Physical Ecosystems},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3712004},
doi = {10.1145/3712004},
abstract = {Today's distributed and pervasive computing addresses large-scale cyber-physical ecosystems, characterised by dense and large networks of devices capable of computation, communication and interaction with the environment and people. While most research focuses on treating these systems as “composites” (i.e., heterogeneous functional complexes), recent developments in fields such as self-organising systems and swarm robotics have opened up a complementary perspective: treating systems as “collectives” (i.e., uniform, collaborative, and self-organising groups of entities). This article explores the motivations, state of the art, and implications of this “collective computing paradigm” in software engineering. In particular, it discusses its peculiar challenges, implied by characteristics like distribution, situatedness, large scale, and cooperative nature. These challenges outline significant directions for future research in software engineering, touching on aspects such as macro-programming, collective intelligence, self-adaptive middleware, learning/synthesis of collective behaviour, human involvement, safety and security in collective cyber-physical ecosystems.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {cyber-physical ecosystems, collective adaptive systems, swarm intelligence, macro-programming, edge-cloud continuum, multi-agent systems, distributed artificial intelligence}
}

@proceedings{10.1145/3639474,
title = {ICSE-SEET '24: Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@article{10.1145/3314203,
author = {Hong, Junyuan and Li, Yang and Chen, Huanhuan},
title = {Variant Grassmann Manifolds: A Representation Augmentation Method for Action Recognition},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3314203},
doi = {10.1145/3314203},
abstract = {In classification tasks, classifiers trained with finite examples might generalize poorly to new data with unknown variance. For this issue, data augmentation is a successful solution where numerous artificial examples are added to training sets. In this article, we focus on the data augmentation for improving the accuracy of action recognition, where action videos are modeled by linear dynamical systems and approximately represented as linear subspaces. These subspace representations lie in a non-Euclidean space, named Grassmann manifold, containing points as orthonormal matrixes. It is our concern that poor generalization may result from the variance of manifolds when data come from different sources or classes. Thus, we introduce infinitely many variant Grassmann manifolds (VGM) subject to a known distribution, then represent each action video as different Grassmann points leading to augmented representations. Furthermore, a prior based on the stability of subspace bases is introduced, so the manifold distribution can be adaptively determined, balancing discrimination and representation. Experimental results of multi-class and multi-source classification show that VGM softmax classifiers achieve lower test error rates compared to methods with a single manifold.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {23},
numpages = {23},
keywords = {Grassmann manifold, Representation learning, action recognition, data augmentation}
}

@article{10.1145/3660808,
author = {Zhou, Shasha and Huang, Mingyu and Sun, Yanan and Li, Ke},
title = {Evolutionary Multi-objective Optimization for Contextual Adversarial Example Generation},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660808},
doi = {10.1145/3660808},
abstract = {The emergence of the 'code naturalness' concept, which suggests that software code shares statistical properties with natural language, paves the way for deep neural networks (DNNs) in software engineering (SE). However, DNNs can be vulnerable to certain human imperceptible variations in the input, known as adversarial examples (AEs), which could lead to adverse model performance. Numerous attack strategies have been proposed to generate AEs in the context of computer vision and natural language processing, but the same is less true for source code of programming languages in SE. One of the challenges is derived from various constraints including syntactic, semantics and minimal modification ratio. These constraints, however, are subjective and can be conflicting with the purpose of fooling DNNs. This paper develops a multi-objective adversarial attack method (dubbed MOAA), a tailored NSGA-II, a powerful evolutionary multi-objective (EMO) algorithm, integrated with CodeT5 to generate high-quality AEs based on contextual information of the original code snippet. Experiments on 5 source code tasks with 10 datasets of 6 different programming languages show that our approach can generate a diverse set of high-quality AEs with promising transferability. In addition, using our AEs, for the first time, we provide insights into the internal behavior of pre-trained models.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {101},
numpages = {24},
keywords = {adversarial example, multi-objective optimization, neural networks}
}

@inproceedings{10.1145/3690771.3690796,
author = {Pendyala, Kalyani and Buyya, Rajkumar},
title = {An Infrastructure Cost Optimised Algorithm for Partitioning of Microservices},
year = {2025},
isbn = {9798400710018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3690771.3690796},
doi = {10.1145/3690771.3690796},
abstract = {The evolution and advances made in the field of Cloud engineering influence the constant changes in software application development cycle and practices. Software architecture has evolved along with other domains and capabilities of software engineering. As migrating applications into the cloud is universally adopted by the software industry, microservices have proven to be the most suitable and widely accepted architecture pattern for applications deployed on distributed cloud. Their efficacy is enabled by both technical benefits like reliability, fault isolation, scalability and productivity benefits like ease of asset maintenance and clear ownership boundaries which in turn lead to fewer interdependencies and shorter development cycles thereby resulting in faster time to market. Though microservices have been established as an architecture pattern over the last decade, many organizations fail to optimize the architecture design to maximize efficiency. In some cases, the complexity of migrating an existing application into the microservices architecture becomes overwhelmingly complex and expensive. Additionally, automation and tool support for this problem are still at an early stage as there isn't a single well-acknowledged pattern or tool which could support the decomposition. This paper discusses a few impactful previous research and survey efforts to identify the lack of infrastructure cost optimization as a parameter in any of the approaches present. This paper proposes an Infrastructure-optimised predictive algorithm for partitioning monolithic software into microservices. It also summarizes the scope for future research opportunities within the area of microservices architecture and distributed cloud networks.},
booktitle = {Proceedings of the 2024 6th Asia Conference on Machine Learning and Computing},
pages = {85–91},
numpages = {7},
keywords = {Additional Keywords partitioning, cloud applications, infrastructure optimization, microservices, software architecture},
location = {
},
series = {ACMLC '24}
}

@inproceedings{10.1145/3581784.3607052,
author = {You, Xin and Yang, Hailong and Lei, Kelun and Luan, Zhongzhi and Qian, Depei},
title = {TrivialSpy: Identifying Software Triviality via Fine-grained and Dataflow-based Value Profiling},
year = {2023},
isbn = {9798400701092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581784.3607052},
doi = {10.1145/3581784.3607052},
abstract = {Trivial operations cause software inefficiencies that waste functional units and memory bandwidth for executing useless instructions. Although previous works have identified a significant amount of trivial operations in widely used programs, the proposed solutions only provide useful observations, other than actionable guidance to eliminate trivial operations for better performance. In this paper, we propose TrivialSpy - a fine-grained and dataflow-based value profiler to effectively identify software triviality with optimization potential estimation. With the help of dataflow analysis, TrivialSpy can detect software trivialities of heavy operation, trivial chain, and redundant backward slice. In addition, TrivialSpy can identify trivial breakpoints that combine multiple trivial conditions for more optimization opportunities. The evaluation results demonstrate TrivialSpy is capable of identifying software triviality in highly optimized programs. Based on the optimization guidance provided by TrivialSpy, we can achieve 52.09% performance speedup at maximum after eliminating trivial operations.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {90},
numpages = {13},
keywords = {dynamic binary instrumentation, software triviality, performance analysis, performance optimization},
location = {Denver, CO, USA},
series = {SC '23}
}

@inproceedings{10.5555/318773.319262,
author = {Bayer, Joachim and Girard, Jean-Fran\c{c}ois and W\"{u}rthner, Martin and DeBaud, Jean-Marc and Apel, Martin},
title = {Transitioning legacy assets to a product line architecture},
year = {1999},
isbn = {3540665382},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A successful software system evolves over time, but this evolution often occurs in an ad-hoc fashion. One approach to structure system evolution is the concept of software product lines where a core architecture supports a variety of application contexts. However, in practice, the high cost and high risks of redevelopment as well as the substantial investments made to develop the existing systems most often mandate significant leverage of the legacy assets. Yet, there is little guidance in the literature on how to transition legacy assets into a product line set-up.In this paper, we present RE-PLACE, an approach developed to support the transition of existing software assets towards a product line architecture while taking into account anticipated new system variants. We illustrate this approach with its application in an industrial setting.},
booktitle = {Proceedings of the 7th European Software Engineering Conference Held Jointly with the 7th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {446–463},
numpages = {18},
keywords = {architecture recovery, domain-specific software architecture, reengineering, reuse, software product line},
location = {Toulouse, France},
series = {ESEC/FSE-7}
}

@article{10.1145/3447580,
author = {Russo, Daniel and Stol, Klaas-Jan},
title = {PLS-SEM for Software Engineering Research: An Introduction and Survey},
year = {2021},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3447580},
doi = {10.1145/3447580},
abstract = {Software Engineering (SE) researchers are increasingly paying attention to organizational and human factors. Rather than focusing only on variables that can be directly measured, such as lines of code, SE research studies now also consider unobservable variables, such as organizational culture and trust. To measure such latent variables, SE scholars have adopted Partial Least Squares Structural Equation Modeling (PLS-SEM), which is one member of the larger SEM family of statistical analysis techniques. As the SE field is facing the introduction of new methods such as PLS-SEM, a key issue is that not much is known about how to evaluate such studies. To help SE researchers learn about PLS-SEM, we draw on the latest methodological literature on PLS-SEM to synthesize an introduction. Further, we conducted a survey of PLS-SEM studies in the SE literature and evaluated those based on recommended guidelines.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {78},
numpages = {38},
keywords = {Partial least squares, critical review, research methodology, structural equation modeling}
}

@inproceedings{10.1145/3691620.3695554,
author = {Xie, Zifan and Wen, Ming and Li, Tinghan and Zhu, Yiding and Hou, Qinsheng and Jin, Hai},
title = {How Does Code Optimization Impact Third-party Library Detection for Android Applications?},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695554},
doi = {10.1145/3691620.3695554},
abstract = {Android applications (apps) widely use third-party libraries (TPLs) to reuse functionalities and simplify the development process. Unfortunately, these TPLs often suffer from vulnerabilities that attackers can exploit, leading to catastrophic consequences for app users. To mitigate this threat, researchers have developed tools to detect TPL versions in the app. If an app is found using a TPL vulnerable version, these tools will issue warnings. Although these tools claim to resist the effects of code obfuscation, our preliminary study indicates that code optimization is common during the app release process. A lack of consideration for the impact of code optimizations significantly reduces the effectiveness of existing tools. To fill this gap, this work systematically investigates how and to what extent different optimization strategies affect existing tools. Our findings have led to a new tool named LibHunter, designed to against major code optimization strategies (e.g., Inlining and CallSite Optimization) while also resisting code obfuscation and shrinking. Extensive evaluations on a dataset of apps with optimization, obfuscation, and shrinking enabled show LibHunter significantly outperforms existing tools. It achieves F1 value that surpass the best tools by 29.3% and 36.1% at the library and version levels, respectively. We also applied LibHunter to detect vulnerable TPLs in the top Google Play apps, which shows the scalability of our approach, as well as the potential of our approach to facilitate malware detection.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1919–1931},
numpages = {13},
keywords = {code optimization, third-party library, android},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3650212.3680306,
author = {Blumschein, Christoph and Niephaus, Fabio and Stancu, Codru\c{t} and Wimmer, Christian and Lincke, Jens and Hirschfeld, Robert},
title = {Finding Cuts in Static Analysis Graphs to Debloat Software},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680306},
doi = {10.1145/3650212.3680306},
abstract = {As software projects grow increasingly more complex, debloating gains traction.
 
While static analyses yield a coarse over-approximation of reachable code, approaches based on dynamic execution traces risk program correctness.
 
By allowing the developer to reconsider only a few methods and still achieve a significant reduction in code size, cut-based debloating can minimize the risk.
 
In this paper, we propose the idea of finding small cuts in the rule graphs produced by static analysis.
 
After introducing an analysis with suitable semantics, we discuss how to encode its rules into a directed hypergraph.
 
We then present an algorithm for efficiently finding the most effective single cut in the graph.
 
The execution time of the proposed operations allows for the deployment in interactive tools.
 
Finally, we show that our graph model is able to expose methods worthwhile to reconsider.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {603–614},
numpages = {12},
keywords = {Call-Graph Construction, Graph Cuts, Interactive Feedback, Software Debloating, Static Analysis},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3543507.3583323,
author = {Yang, Xinlei and Liu, Wei and Lin, Hao and Li, Zhenhua and Qian, Feng and Wang, Xianlong and Liu, Yunhao and Xu, Tianyin},
title = {Visual-Aware Testing and Debugging for Web Performance Optimization},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583323},
doi = {10.1145/3543507.3583323},
abstract = {Web performance optimization services, or web performance optimizers (WPOs), play a critical role in today’s web ecosystem by improving page load speed and saving network traffic. However, WPOs are known for introducing visual distortions that disrupt the users’ web experience. Unfortunately, visual distortions are hard to analyze, test, and debug, due to their subjective measure, dynamic content, and sophisticated WPO implementations. This paper presents Vetter, a novel and effective system that automatically tests and debugs visual distortions. Its key idea is to reason about the morphology of web pages, which describes the topological forms and scale-free geometrical structures of visual elements. Vetter efficiently calculates morphology and comparatively analyzes the morphologies of web pages before and after a WPO, which acts as a differential test oracle. Such morphology analysis enables Vetter to detect visual distortions accurately and reliably. Vetter further diagnoses the detected visual distortions to pinpoint the root causes in WPOs’ source code. This is achieved by morphological causal inference, which localizes the offending visual elements that trigger the distortion and maps them to the corresponding code. We applied Vetter to four representative WPOs. Vetter discovers 21 unknown defects responsible for 98% visual distortions; 12 of them have been confirmed and 5 have been fixed.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {2948–2959},
numpages = {12},
keywords = {Visual-Aware Testing and Debugging, Web Page Distortion, Web Performance Optimization},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3548606.3559385,
author = {Dittmer, Samuel and Ishai, Yuval and Lu, Steve and Ostrovsky, Rafail},
title = {Improving Line-Point Zero Knowledge: Two Multiplications for the Price of One},
year = {2022},
isbn = {9781450394505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548606.3559385},
doi = {10.1145/3548606.3559385},
abstract = {Recent advances in fast protocols for vector oblivious linear evaluation (VOLE) have inspired a family of new VOLE-based lightweight designated-verifier NIZK protocols (Weng et al., S&amp;P 2021, Baum et al., Crypto 2021, Dittmer et al., ITC 2021, Yang et al., CCS 2021). In particular, the Line-Point Zero Knowledge (LPZK) protocol of Dittmer et al. has the advantage of being entirely non-cryptographic given a single instance of a random VOLE correlation.We present improvements to LPZK through the introduction of additional structure to the correlated randomness. Using an efficiently realizable variant of the VOLE correlation, we reduce the online proof size of LPZK by roughly 2x: from roughly 2 field elements per multiplication gate, or 1 element in the random oracle variant, to only 1 or 1/2 elements respectively. In particular, we get the first practical VOLE-based NIZK that breaks the 1-element-per-multiplication barrier.We implemented an optimized version of our protocol and compared it with other recent VOLE-based NIZK protocols. In the typical case where communication is the bottleneck, we get at least 2x performance improvement over all previous VOLE-based protocols. When prover computation is the bottleneck, we outperform all non-LPZK protocols by at least 2-3x and (our optimized implementation of) LPZK by roughly 30%, obtaining a 2-3x slowdown factor compared to plain circuit evaluation.},
booktitle = {Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},
pages = {829–841},
numpages = {13},
keywords = {non-interactive zero knowledge, oblivious linear evaluation, vector oblivious linear evaluation, zero knowledge proofs},
location = {Los Angeles, CA, USA},
series = {CCS '22}
}

@article{10.1145/3702005,
author = {Ferikoglou, Aggelos and Kakolyris, Andreas and Masouros, Dimosthenis and Soudris, Dimitrios and Xydis, Sotirios},
title = {CollectiveHLS: A Collaborative Approach to High-Level Synthesis Design Optimization},
year = {2024},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/3702005},
doi = {10.1145/3702005},
abstract = {High-Level Synthesis (HLS) has played a pivotal role in making FPGAs accessible to a broader audience by facilitating high-level device programming and rapid microarchitecture customization through the use of directives. However, manually selecting the right directives can be a formidable challenge for programmers lacking a hardware background. This article presents CollectiveHLS, an ultra-fast, knowledge-driven approach to optimizing HLS designs. It automates the identification and application of optimal directive configurations from the original source code, focusing on minimizing design latency and ensuring synthesizability. This optimization approach is entirely data-driven, offering a generalized HLS tuning solution without reliance on Quality of Result (QoR) models or meta-heuristics. CollectiveHLS is designed, implemented, and evaluated using around 60 applications sourced from well-established benchmark suites and GitHub repositories, all running on a Xilinx UltraScale+ MPSoC ZCU104. It achieves an average geometric mean speedup of up to  (23.1times)  compared to the official source code without directives, while maintaining synthesizability and feasibility rates of 100% and 96.6%, respectively, matching those of Vitis, the industry-standard framework for FPGA acceleration. Comparisons with resource over-provisioning, traditional genetic algorithm-based Design Space Exploration (DSE), and State-of-the-Art (SotA) approaches demonstrate that CollectiveHLS produces designs of comparable quality  (14.6times)  faster on average. These results underscore the potential of our approach as an ultra-fast and automated solution for HLS optimization.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = dec,
articleno = {11},
numpages = {32},
keywords = {High-Level Synthesis (HLS), Design Space Exploration (DSE), FPGA Accelerators, Auto-tuning, Data-driven Optimization}
}

@article{10.1145/3715907,
author = {Jahanshahi, Mahmoud and Reid, David and Mockus, Audris},
title = {Beyond Dependencies: The Role of Copy-Based Reuse in Open Source Software Development},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715907},
doi = {10.1145/3715907},
abstract = {In Open Source Software, resources of any project are open for reuse by introducing dependencies or copying the resource itself. In contrast to dependency-based reuse, the infrastructure to systematically support copy-based reuse appears to be entirely missing. Our aim is to enable future research and tool development to increase efficiency and reduce the risks of copy-based reuse. We seek a better understanding of such reuse by measuring its prevalence and identifying factors affecting the propensity to reuse. To identify reused artifacts and trace their origins, our method exploits World of Code infrastructure. We begin with a set of theory-derived factors related to the propensity to reuse, sample instances of different reuse types, and survey developers to better understand their intentions. Our results indicate that copy-based reuse is common, with many developers being aware of it when writing code. The propensity for a file to be reused varies greatly among languages and between source code and binary files, consistently decreasing over time. Files introduced by popular projects are more likely to be reused, but at least half of reused resources originate from “small” and “medium” projects. Developers had various reasons for reuse but were generally positive about using a package manager.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Reuse, Open Source Software, Software Development, Copy-based Reuse, Software Supply Chain, World of Code}
}

@inproceedings{10.1145/3477495.3531805,
author = {Farber, Miriam and Carmel, David and Kuchy, Lital and Mejer, Avihai},
title = {Analyzing the Support Level for Tips Extracted from Product Reviews},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531805},
doi = {10.1145/3477495.3531805},
abstract = {Useful tips extracted from product reviews assist customers to take a more informed purchase decision, as well as making a better, easier, and safer usage of the product. In this work we argue that extracted tips should be examined based on the amount of support and opposition they receive from all product reviews. A classifier, developed for this purpose, determines the degree to which a tip is supported or contradicted by a single review sentence. These support-levels are then aggregated over all review sentences, providing a global support score, and a global contradiction score, reflecting the support-level of all reviews to the given tip, thus improving the customer confidence in the tip validity. By analyzing a large set of tips extracted from product reviews, we propose a novel taxonomy for categorizing tips as highly-supported, highly-contradicted, controversial (supported and contradicted), and anecdotal (neither supported nor contradicted).},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2059–2064},
numpages = {6},
keywords = {support-level analysis, tip extraction},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@article{10.5555/3546258.3546493,
author = {Moss, Henry B. and Leslie, David S. and Gonz\'{a}lez, Javier and Rayson, Paul},
title = {GIBBON: general-purpose information-based Bayesian optimisation},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {This paper describes a general-purpose extension of max-value entropy search, a popular approach for Bayesian Optimisation (BO). A novel approximation is proposed for the information gain -- an information-theoretic quantity central to solving a range of BO problems, including noisy, multi-fidelity and batch optimisations across both continuous and highly-structured discrete spaces. Previously, these problems have been tackled separately within information-theoretic BO, each requiring a different sophisticated approximation scheme, except for batch BO, for which no computationally-lightweight information-theoretic approach has previously been proposed. GIBBON (General-purpose Information-Based Bayesian OptimisatioN) provides a single principled framework suitable for all the above, out-performing existing approaches whilst incurring substantially lower computational overheads. In addition, GIBBON does not require the problem's search space to be Euclidean and so is the first high-performance yet computationally light-weight acquisition function that supports batch BO over general highly structured input spaces like molecular search and gene design. Moreover, our principled derivation of GIBBON yields a natural interpretation of a popular batch BO heuristic based on determinantal point processes. Finally, we analyse GIBBON across a suite of synthetic benchmark tasks, a molecular search loop, and as part of a challenging batch multi-fidelity framework for problems with controllable experimental noise.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {235},
numpages = {49},
keywords = {Bayesian optimisation, entropy search, experimental design, multi-fidelity, batch}
}

@inproceedings{10.1145/2908812.2908844,
author = {Wu, Junhua and Shekh, Slava and Sergiienko, Nataliia Y. and Cazzolato, Benjamin S. and Ding, Boyin and Neumann, Frank and Wagner, Markus},
title = {Fast and Effective Optimisation of Arrays of Submerged Wave Energy Converters},
year = {2016},
isbn = {9781450342063},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2908812.2908844},
doi = {10.1145/2908812.2908844},
abstract = {Renewable forms of energy are becoming increasingly important to consider, as the global energy demand continues to grow. Wave energy is one of these widely available forms, but it is largely unexploited. A common design for a wave energy converter is called a point absorber or buoy. The buoy typically floats on the surface or just below the surface of the water, and captures energy from the movement of the waves. It can use the motion of the waves to drive a pump to generate electricity and to create potable water. Since a single buoy can only capture a limited amount of energy, large-scale wave energy production necessitates the deployment of buoys in large numbers called arrays. However, the efficiency of arrays of buoys is affected by highly complex intra-buoy interactions. The contributions of this article are two-fold. First, we present an approximation of the buoy interactions model that results in a 350-fold computational speed-up to enable the use inside of iterative optimisation algorithms, Second, we study arrays of fully submerged three-tether buoys, with and without shared mooring points.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference 2016},
pages = {1045–1052},
numpages = {8},
keywords = {evolutionary algorithm, renewable energy, wave energy},
location = {Denver, Colorado, USA},
series = {GECCO '16}
}

@inbook{10.5555/3712729.3712961,
author = {Pecker-Marcosig, Ezequiel and Romczyk, Ger\'{o}nimo and Bonaventura, Mat\'{\i}as and Castro, Rodrigo},
title = {Systematic Performance Optimization for the Powerdevs Simulator and Models of Large-Scale Real-World Applications},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {As simulation models grow in size and complexity, their performance comes under increasing pressure. In order to deliver results within an acceptable timeframe, new mechanisms are required that reduce the simulation performance burden while avoiding simplifications of model behavior. We present a systematic methodology for optimizing the PowerDEVS toolkit, improving both the core simulation engine and general-purpose behavioural models in a way that they stand as independent yet compatible tasks, thanks to the strict separation between simulator and models promoted by the DEVS formalism. We tested our optimizations on a variety of models that emphasize different effects. The main target system of this work is the large-scale network model of the Data Acquisition System (DAQ) at CERN, which is used to derive hardware requirements for future accelerator upgrades. Speedups for the DAQ model more than doubles the number of experiments that can be run within the same amount of time.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2797–2808},
numpages = {12}
}

@article{10.1145/3664602,
author = {Nguyen, Van and Le, Trung and Tantithamthavorn, Chakkrit and Grundy, John and Phung, Dinh},
title = {Deep Domain Adaptation With Max-Margin Principle for Cross-Project Imbalanced Software Vulnerability Detection},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3664602},
doi = {10.1145/3664602},
abstract = {Software vulnerabilities (SVs) have become a common, serious, and crucial concern due to the ubiquity of computer software. Many AI-based approaches have been proposed to solve the software vulnerability detection (SVD) problem to ensure the security and integrity of software applications (in both the development and testing phases). However, there are still two open and significant issues for SVD in terms of (i) learning automatic representations to improve the predictive performance of SVD, and (ii) tackling the scarcity of labeled vulnerability datasets that conventionally need laborious labeling effort by experts. In this paper, we propose a novel approach to tackle these two crucial issues. We first exploit the automatic representation learning with deep domain adaptation for SVD. We then propose a novel cross-domain kernel classifier leveraging the max-margin principle to significantly improve the transfer learning process of SVs from imbalanced labeled into imbalanced unlabeled projects. Our approach is the first work that leverages solid body theories of the max-margin principle, kernel methods, and bridging the gap between source and target domains for imbalanced domain adaptation (DA) applied in cross-project SVD. The experimental results on real-world software datasets show the superiority of our proposed method over state-of-the-art baselines. In short, our method obtains a higher performance on F1-measure, one of the most important measures in SVD, from 1.83% to 6.25% compared to the second highest method in the used datasets.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {162},
numpages = {34},
keywords = {Software security, automated cross-project vulnerability detection}
}

@proceedings{10.1145/3663529,
title = {FSE 2024: Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to FSE 2024, the ACM International Conference on the Foundations of Software Engineering (FSE) 2024. The conference now has a shorter name! FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {Porto de Galinhas, Brazil}
}

@inproceedings{10.5555/3320516.3320778,
author = {Rhodes-Leader, Luke and Worthington, David J. and Nelson, Barry L. and Onggo, Bhakti Stephan},
title = {Multi-fidelity simulation optimisation for airline disruption management},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {The airline industry faces many causes of disruption. To minimise financial and reputational impact, the airline must adapt its schedules. Due to the complexity of the environment, simulation is a natural modelling approach. However, the large solution space, time constraints and system constraints make the search for revised schedules difficult. This paper presents a method for the aircraft recovery problem that uses multi-fidelity modelling including a trust region simulation optimisation algorithm to mitigate the computational costs of using high-fidelity simulations with its benefits for providing good estimates of the true performance.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {2179–2190},
numpages = {12},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@inproceedings{10.1145/3626246.3653369,
author = {Bruno, Nicolas and Galindo-Legaria, C\'{e}sar and Joshi, Milind and Calvo Vargas, Esteban and Mahapatra, Kabita and Ravindran, Sharon and Chen, Guoheng and Cervantes Ju\'{a}rez, Ernesto and Sezgin, Beysim},
title = {Unified Query Optimization in the Fabric Data Warehouse},
year = {2024},
isbn = {9798400704222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626246.3653369},
doi = {10.1145/3626246.3653369},
abstract = {Over a decade ago, Microsoft introduced Parallel Data Warehouse (PDW), a massively parallel processing system to manage and query large amounts of data. Its optimizer was built by reusing SQL Server's infrastructure with minimal changes, which was an effective approach to bring cost-based query optimization quickly to PDW. Over time, learnings from production as well as architectural changes in the product (such as moving from an appliance form factor to the cloud, separation of compute and storage, and serverless components) required evolving the query optimizer in Fabric DW, the latest offering from Microsoft in the cloud data warehouse space. In this paper we describe the changes to the optimization process in Fabric DW, compare them to the earlier architecture used in PDW, and validate our new approach.},
booktitle = {Companion of the 2024 International Conference on Management of Data},
pages = {18–30},
numpages = {13},
keywords = {cascades framework, distributed database systems, query optimization},
location = {Santiago AA, Chile},
series = {SIGMOD '24}
}

@article{10.1145/3708532,
author = {H\"{a}m\"{a}l\"{a}inen, Joonas and Das, Teerath and Mikkonen, Tommi},
title = {A Systematic Literature Review of Multi-Label Learning in Software Engineering},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708532},
doi = {10.1145/3708532},
abstract = {In this paper, we provide the first systematic literature review of the intersection of two research areas, Multi-Label Learning (MLL) and Software Engineering (SE). We refer to this intersection as MLL4SE. In recent years, MLL problems have increased in many applications and research areas because real-world datasets often have a multi-label nature. For multi-label data, simplifying the assumption of traditional classification approaches that an instance can only be associated with one class only leads to worse accuracy. Thus, a better match of methods and assumptions about the data is required. We identified 50 primary studies in our systematic literature review in the MLL4SE domain. Based on this review, we identified six main SE application domains where MLL has been applied. These domains include Software Requirement Engineering, Issue Tracking and Management, Community and Knowledge Management, API Usage and Management, Code Quality and Maintenance, and Mobile Application Development. We summarized the methods used and the data nature of the MLL4SE applications. Moreover, we separately provide taxonomies of future work directions from machine learning and software engineering perspectives. In general, we highlight current trends, research gaps, and shortcomings.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {Machine Learning, Multi-Label Learning, Software Engineering, Systematic Literature Review, Software Development Life Cycle (SDLC) Activities}
}

@inproceedings{10.1145/3627703.3650061,
author = {Tang, Yupeng and Zhou, Ping and Zhang, Wenhui and Hu, Henry and Yang, Qirui and Xiang, Hao and Liu, Tongping and Shan, Jiaxin and Huang, Ruoyun and Zhao, Cheng and Chen, Cheng and Zhang, Hui and Liu, Fei and Zhang, Shuai and Ding, Xiaoning and Chen, Jianjun},
title = {Exploring Performance and Cost Optimization with ASIC-Based CXL Memory},
year = {2024},
isbn = {9798400704376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627703.3650061},
doi = {10.1145/3627703.3650061},
abstract = {As memory-intensive applications continue to drive the need for advanced architectural solutions, Compute Express Link (CXL) has risen as a promising interconnect technology that enables seamless high-speed, low-latency communication between host processors and various peripheral devices. In this study, we explore the application performance of ASIC CXL memory in various data-center scenarios. We then further explore multiple potential impacts (e.g., throughput, latency, and cost reduction) of employing CXL memory via carefully designed policies and strategies. Our empirical results show the high potential of CXL memory, reveal multiple intriguing observations of CXL memory and contribute to the wide adoption of CXL memory in real-world deployment environments. Based on our benchmarks, we also develop an Abstract Cost Model that can estimate the cost benefit from using CXL memory.},
booktitle = {Proceedings of the Nineteenth European Conference on Computer Systems},
pages = {818–833},
numpages = {16},
keywords = {CXL-Memory, Datacenters, Memory Management, Operating Systems, measurement},
location = {Athens, Greece},
series = {EuroSys '24}
}

@inproceedings{10.1145/3510454.3517070,
author = {Wiesmayr, Bianca},
title = {Towards facilitating software engineering for production systems in industry 4.0 with behavior models},
year = {2022},
isbn = {9781450392235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510454.3517070},
doi = {10.1145/3510454.3517070},
abstract = {With the growing adoption of Industry 4.0 concepts in production systems, new challenges arise in engineering control software. Highly distributed control with tight real-time constraints and safety regulations results in increasingly complex software. Current research focuses on increasing the abstraction with new architectures and modularization of software. The presented PhD research addresses modeling of the interactions between control software components, and of the emergent behavior of these compositions. Such behavior models can support the initial implementation, and facilitate (semi-)automated testing and monitoring of control software. Finally, visualizing behavior in a model can enhance under-standability of existing control software, when software developers need not access abstracted hierarchy levels to deduct their functionality. This work aims at optimizing the benefit of behavior models in developing control software: Modeling the expected behavior directly for new software will allow using them throughout the software life-cycle. For legacy software, the initial development effort of behavior models will be minimized by automatically capturing behavior models from the implementation. The approach is evaluated in case studies and user studies to integrate experiences from the industrial domain into this software engineering research.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings},
pages = {305–309},
numpages = {5},
keywords = {IEC 61499, control software, model-driven software engineering},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@article{10.1145/3709354,
author = {Kessel, Marcus and Atkinson, Colin},
title = {Morescient GAI for Software Engineering},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3709354},
doi = {10.1145/3709354},
abstract = {The ability of Generative AI (GAI) technology to automatically check, synthesize and modify software engineering artifacts promises to revolutionize all aspects of software engineering. Using GAI for software engineering tasks is consequently one of the most rapidly expanding fields of software engineering research, with over a hundred LLM-based code models having been published since 2021. However, the overwhelming majority of existing code models share a major weakness – they are exclusively trained on the syntactic facet of software, significantly lowering their trustworthiness in tasks dependent on software semantics. To address this problem, a new class of “Morescient” GAI is needed that is “aware” of (i.e., trained on) both the semantic and static facets of software. This, in turn, will require a new generation of software observation platforms capable of generating large quantities of execution observations in a structured and readily analyzable way. In this paper, we present a vision and roadmap for how such “Morescient” GAI models can be engineered, evolved and disseminated according to the principles of open science.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {generative AI, morescience, semantics, dynamic, analysis, behavior-aware, observation, dataset, vision, roadmap}
}

@proceedings{10.1145/3540250,
title = {ESEC/FSE 2022: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of all members of the organizing committee, we are delighted to welcome everyone to the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2022. The event continues the long, distinguished ESEC/FSE tradition of presenting the most innovative research, and facilitating interactions between scientists and engineers who are passionate about advancing the theory and practice of software engineering.},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3660605.3660941,
author = {Rao, Kunal and Coviello, Giuseppe and Benedetti, Priscilla and Giuseppe De Vita, Ciro and Mellone, Gennaro and Chakradhar, Srimat},
title = {ECO-LLM: LLM-based Edge Cloud Optimization},
year = {2024},
isbn = {9798400706523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660605.3660941},
doi = {10.1145/3660605.3660941},
abstract = {AI/ML techniques have been used to solve systems problems, but their applicability to customize solutions on-the-fly has been limited. Traditionally, any customization required manually changing the AI/ML model or modifying the code, configuration parameters, application settings, etc. This incurs too much time and effort, and is very painful. In this paper, we propose a novel technique using Generative Artificial Intelligence (GenAI) technology, wherein instructions can be provided in natural language and actual code to handle any customization is automatically generated, integrated and applied on-the-fly. Such capability is extremely powerful since it makes customization of application settings or solution techniques super easy. Specifically, we propose ECO-LLM (LLM-based Edge Cloud Optimization), which leverages Large Language Models (LLM) to dynamically adjust placement of application tasks across edge and cloud computing tiers, in response to changes in application workload, such that insights are delivered quickly with low cost of operation (systems problem). Our experiments with real-world video analytics applications i.e. face recognition, human attributes detection and license plate recognition show that ECO-LLM is able to automatically generate code on-the-fly and adapt placement of application tasks across edge and cloud computing tiers. We note that the trigger workload (to switch between edge and cloud) for ECO-LLM is exactly the same as the baseline (manual) and actual placement performed by ECO-LLM is only slightly different i.e. on average (across 2 days) only 1.45% difference in human attributes detection and face recognition, and 1.11% difference in license plate recognition. Although we tackle this specific systems problem in this paper, our proposed GenAI-based technique is applicable to solve other systems problems too.},
booktitle = {Proceedings of the 2024 Workshop on AI For Systems},
pages = {7–12},
numpages = {6},
keywords = {large language models (LLM), generative artificial intelligence (GenAI), machine learning (ML), customization, optimization, edge computing, cloud computing, video analytics},
location = {Pisa, Italy},
series = {AI4Sys '24}
}

@inproceedings{10.1109/CGO57630.2024.10444840,
author = {Zhao, Qidong and Chabbi, Milind and Liu, Xu},
title = {EasyView: Bringing Performance Profiles into Integrated Development Environments},
year = {2024},
isbn = {9798350395099},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CGO57630.2024.10444840},
doi = {10.1109/CGO57630.2024.10444840},
abstract = {Dynamic program performance analysis (also known as profiling) is well-known for its powerful capabilities of identifying performance inefficiencies in software packages. Although a large number of profiling techniques are developed in academia and industry, very few of them are widely used by software developers in their regular software developing activities. There are three major reasons. First, the profiling tools (also known as profilers) are disjoint from the coding environments such as IDEs and editors; frequently switching focus between them significantly complicates the entire cycle of software development. Second, mastering various tools to interpret their analysis results requires substantial efforts; even worse, many tools have their own design of graphical user interfaces (GUI) for data presentation, which steepens the learning curves. Third, most existing profilers expose few interfaces to support user-defined analysis, which makes the tools less customizable to fulfill diverse user demands.We develop EasyView, a general solution to integrate the interpretation and visualization of various profiling results in the coding environments, which bridges software developers closer with profilers during the code development cycle. The novelty of EasyView lies in its significant improvement on the usability of profilers. EasyView not only provides deep insights to support intuitive analysis and optimization in a simple interface, but also enhances user experiences in using the profilers effectively and efficiently in the IDEs. Our evaluation shows that EasyView is able to support various profilers for different languages and provide unique insights into performance inefficiencies in different domains. Our user studies show that EasyView can largely improve the usability of profilers in software development cycles via facilitating performance debugging efforts.},
booktitle = {Proceedings of the 2024 IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {386–398},
numpages = {13},
keywords = {profiling, software optimization, performance measurement, visualization, tools},
location = {Edinburgh, United Kingdom},
series = {CGO '24}
}

@article{10.1145/3597617,
author = {Renzullo, Joseph and Weimer, Westley and Forrest, Stephanie},
title = {Evolving Software: Combining Online Learning with Mutation-Based Stochastic Search},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
url = {https://doi.org/10.1145/3597617},
doi = {10.1145/3597617},
abstract = {Evolutionary algorithms and related mutation-based methods have been used in software engineering, with recent emphasis on the problem of repairing bugs. In this work, programs are typically not synthesized from a random start. Instead, existing solutions—which may be flawed or inefficient—are taken as starting points, with the evolutionary process searching for useful improvements. This approach, however, introduces a challenge for the search algorithm: what is the optimal number of neutral mutations that should be combined? Too much is likely to introduce errors and break the program while too little hampers the search process, inducing the classic tradeoff between exploration and exploitation.In the context of software improvement, this work considers MWRepair, an algorithm for enhancing mutation-based searches, which uses online learning to optimize the tradeoff between exploration and exploitation. The aggressiveness parameter governs how many individual mutations should be applied simultaneously to an individual between fitness evaluations. MWRepair is evaluated in the context of automated program repair problems, where the goal is repairing software bugs with minimal human involvement. The article analyzes the search space for automated program repair induced by neutral mutations, finding that the greatest probability of finding successful repairs often occurs when many neutral mutations are applied to the original program. Moreover, repair probability follows a characteristic, unimodal distribution. MWRepair uses online learning to leverage this property, finding both rare and multi-edit repairs to defects in the popular Defects4J benchmark set of buggy Java programs.},
journal = {ACM Trans. Evol. Learn. Optim.},
month = dec,
articleno = {13},
numpages = {32},
keywords = {Neutral mutations, automated program repair, software mutational robustness, multiplicative weights update}
}

@article{10.1145/3690632,
author = {Li, Xuetao and Zhang, Yuxia and Osborne, Cailean and Zhou, Minghui and Jin, Zhi and Liu, Hui},
title = {Systematic Literature Review of Commercial Participation in Open Source Software},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3690632},
doi = {10.1145/3690632},
abstract = {Open source software (OSS) has been playing a fundamental role in not only information technology but also our social lives. Attracted by various advantages of OSS, increasing commercial companies are participating extensively in open source development, and this has had a broad impact. Enormous research efforts have been devoted to understanding this phenomenon and trying to pursue a win-win result. To characterize the current research achievement and identify challenges, this article provides a comprehensive systematic literature review (SLR) of existing research on company participation in OSS. We collected 105 papers and organized them based on their research topics, which cover three main directions, i.e., participation motivation, contribution model, and impact on OSS development. We found that companies have diverse motivations from economic, technological, and social aspects, and no one study covered all the motivation categories. Existing studies categorize five main companies’ contribution models in OSS projects through their objectives and how they shape OSS communities. Researchers also explored how commercial participation affects OSS development, including companies, developers, and OSS projects. This study contributes to a comprehensive understanding of commercial participation in OSS development. Based on our findings, we present a set of research challenges and promising directions for companies’ better participation in OSS.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {33},
numpages = {31},
keywords = {Open Source Ecosystem, Software Development, Commercial Participation, Survey}
}

@proceedings{10.1145/3639475,
title = {ICSE-SEIS'24: Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Society},
year = {2024},
isbn = {9798400704994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3650212.3652143,
author = {Wang, Ruixin and Lu, Minghai and Yu, Cody Hao and Lai, Yi-Hsiang and Zhang, Tianyi},
title = {Automated Deep Learning Optimization via DSL-Based Source Code Transformation},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3652143},
doi = {10.1145/3650212.3652143},
abstract = {As deep learning models become increasingly bigger and more complex, it is critical to improve model training and inference efficiency. Though a variety of highly optimized libraries and packages (known as DL kernels) have been developed, it is tedious and time-consuming to figure out which kernel to use, where to use, and how to use them correctly. To address this challenge, we propose an Automated Deep learning OPTimization approach called Adopter. We design a Domain-Specific Language (DSL) to represent DL model architectures and leverage this DSL to specify model transformation rules required to integrate a DL kernel into a model. Given the source code of a DL model and the transformation rules for a set of kernels, Adopter first performs inter-procedural analysis to identify and express the model architecture in our DSL. Then, Adopter performs scope analysis and sub-sequence matching to identify locations in the model architecture where the transformation rules can be applied. Finally, Adopter proposes a synthesis-based code transformation method to apply the transformation rule. We curated a benchmark with 199 models from Hugging Face and a diverse set of DL kernels. We found that, compared to a state-of-the-art automated code transformation technique, Adopter helps improve the precision and recall by 3% and 56%, respectively. An in-depth analysis of 9 models revealed that on average, Adopter improved the training speed by 22.7% while decreasing the GPU memory usage by 10.5%.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {479–490},
numpages = {12},
keywords = {Deep Learning Optimization, Program Transformation},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3721146.3721935,
author = {Heer, Maximilian Jakob and Ramhorst, Benjamin and Alonso, Gustavo},
title = {Machine Learning-based Deep Packet Inspection at Line Rate for RDMA on FPGAs},
year = {2025},
isbn = {9798400715389},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3721146.3721935},
doi = {10.1145/3721146.3721935},
abstract = {FPGAs are becoming increasingly important in the cloud and data centers, especially as network-attached accelerators or reconfigurable Network Interface Cards (NICs). In the cloud, Remote Direct Memory Access (RDMA) over Converged Ethernet (RoCEv2) has emerged as the de facto standard protocol for data transport due to its low latency and high throughput. However, RDMA has several access control weaknesses limiting its applicability in the cloud. In this paper, we explore using machine learning-based deep packet inspection (DPI) as an enhancement to an open-source FPGA RDMA stack. The ultra low-latency ML model is integrated on the RDMA datapath and allows for detection of specific content in RDMA payloads (e.g., executables) at a line rate of 100Gbps while using less than 1% of the available resources. Compared with existing work, our solution operates on the full message payload, at the transport level, and on a complete RDMA stack without sacrificing compatibility with RoCEv2 and its native performance characteristics, proving its potential as an end-to-end solution.},
booktitle = {Proceedings of the 5th Workshop on Machine Learning and Systems},
pages = {148–155},
numpages = {8},
keywords = {FPGA, remote direct memory access (RDMA), deep packet inspection, machine learning},
location = {World Trade Center, Rotterdam, Netherlands},
series = {EuroMLSys '25}
}

@article{10.1145/3607184,
author = {Clark, Andrew G. and Foster, Michael and Prifling, Benedikt and Walkinshaw, Neil and Hierons, Robert M. and Schmidt, Volker and Turner, Robert D.},
title = {Testing Causality in Scientific Modelling Software},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3607184},
doi = {10.1145/3607184},
abstract = {From simulating galaxy formation to viral transmission in a pandemic, scientific models play a pivotal role in developing scientific theories and supporting government policy decisions that affect us all. Given these critical applications, a poor modelling assumption or bug could have far-reaching consequences. However, scientific models possess several properties that make them notoriously difficult to test, including a complex input space, long execution times, and non-determinism, rendering existing testing techniques impractical. In fields such as epidemiology, where researchers seek answers to challenging causal questions, a statistical methodology known as Causal inference has addressed similar problems, enabling the inference of causal conclusions from noisy, biased, and sparse data instead of costly experiments. This article introduces the causal testing framework: a framework that uses causal inference techniques to establish causal effects from existing data, enabling users to conduct software testing activities concerning the effect of a change, such as metamorphic testing, a posteriori. We present three case studies covering real-world scientific models, demonstrating how the causal testing framework can infer metamorphic test outcomes from reused, confounded test data to provide an efficient solution for testing scientific modelling software.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {10},
numpages = {42},
keywords = {Software testing, causal inference, causal testing}
}

@article{10.1145/3709360,
author = {Mastropaolo, Antonio and Escobar-Vel\'{a}squez, Camilo and Linares-V\'{a}squez, Mario},
title = {From Triumph to Uncertainty: The Journey of Software Engineering in the AI Era},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3709360},
doi = {10.1145/3709360},
abstract = {Over the last ten years, the realm of Artificial Intelligence (AI) has experienced an explosion of revolutionary breakthroughs, transforming what seemed like a far-off dream into a reality that is now deeply embedded in our everyday lives. AI’s widespread impact is revolutionizing virtually all aspects of human life, and software engineering (SE) is no exception. As we explore this changing landscape, we are faced with questions about what the future holds for SE and how AI will reshape the roles, duties, and methodologies within the field. The introduction of these groundbreaking technologies highlights the inevitable shift towards a new paradigm, suggesting a future where AI’s capabilities may redefine the boundaries of SE, potentially even more than human input.In this paper, we aim at outlining the key elements that, based on our expertise, are vital for the smooth integration of AI into SE, all while preserving the intrinsic human creativity that has been the driving force behind the field. First, we provide a brief description of SE and AI evolution. Afterward, we delve into the intricate interplay between AI-driven automation and human innovation, exploring how these two components can work together to advance SE practices to new methods and standards.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {Software engineering, Artificial Intelligence, History, AI4SE, LLM4Code}
}

@inproceedings{10.1007/978-3-642-33666-9_34,
author = {Vierhauser, Michael and Gr\"{u}nbacher, Paul and Heider, Wolfgang and Holl, Gerald and Lettner, Daniela},
title = {Applying a consistency checking framework for heterogeneous models and artifacts in industrial product lines},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_34},
doi = {10.1007/978-3-642-33666-9_34},
abstract = {Product line engineering relies on heterogeneous models and artifacts to define and implement the product line's reusable assets. The complexity and heterogeneity of product line artifacts as well as their interdependencies make it hard to maintain consistency during development and evolution, regardless of the modeling approaches used. Engineers thus need support for detecting and resolving inconsistencies within and between the various artifacts. In this paper we present a framework for checking and maintaining consistency of arbitrary product line artifacts. Our approach is flexible and extensible regarding the supported artifact types and the definition of constraints. We discuss tool support developed for the DOPLER product line tool suite. We report the results of applying the approach to sales support applications of industrial product lines.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {531–545},
numpages = {15},
keywords = {consistency checking, model-based product lines, sales support},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@inproceedings{10.1145/3629526.3645049,
author = {Di Menna, Federico and Traini, Luca and Cortellessa, Vittorio},
title = {Time Series Forecasting of Runtime Software Metrics: An Empirical Study},
year = {2024},
isbn = {9798400704444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629526.3645049},
doi = {10.1145/3629526.3645049},
abstract = {Software applications can produce a wide range of runtime software metrics (e.g., number of crashes, response times), which can be closely monitored to ensure operational efficiency and prevent significant software failures. These metrics are typically recorded as time series data. However, runtime software monitoring has become a high-effort task due to the growing complexity of today's software systems. In this context, time series forecasting (TSF) offers unique opportunities to enhance software monitoring and facilitate proactive issue resolution. While TSF methods have been widely studied in areas like economics and weather forecasting, our understanding of their effectiveness for software runtime metrics remains somewhat limited. In this paper, we investigate the effectiveness of four TSF methods on 25 real-world runtime software metrics recorded over a period of one and a half years. These methods comprise three recurrent neural network (RNN) models and one traditional time series analysis technique (i.e., SARIMA). The metrics are gathered from a large-scale IT infrastructure involving tens of thousands of digital devices. Our results indicate that, in general, RNN models are very effective in the runtime software metrics prediction, although in some scenarios and for certain specific metrics (e.g., waiting times) SARIMA proves to outperform RNN models. Additionally, our findings suggest that the advantages of using RNN models vanish when the prediction horizon becomes too wide, in our case when it exceeds one week.},
booktitle = {Proceedings of the 15th ACM/SPEC International Conference on Performance Engineering},
pages = {48–59},
numpages = {12},
keywords = {runtime software metrics, software monitoring, time series forecasting},
location = {London, United Kingdom},
series = {ICPE '24}
}

@inproceedings{10.1109/ASE56229.2023.00052,
author = {Xiao, Mingxuan and Xiao, Yan and Dong, Hai and Ji, Shunhui and Zhang, Pengcheng},
title = {LEAP: Efficient and Automated Test Method for NLP Software},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00052},
doi = {10.1109/ASE56229.2023.00052},
abstract = {The widespread adoption of DNNs in NLP software has highlighted the need for robustness. Researchers proposed various automatic testing techniques for adversarial test cases. However, existing methods suffer from two limitations: weak error-discovering capabilities, with success rates ranging from 0% to 24.6% for BERT-based NLP software, and time inefficiency, taking 177.8s to 205.28s per test case, making them challenging for time-constrained scenarios.To address these issues, this paper proposes LEAP, an automated test method that uses LEvy flight-based Adaptive Particle swarm optimization integrated with textual features to generate adversarial test cases. Specifically, we adopt Levy flight for population initialization to increase the diversity of generated test cases. We also design an inertial weight adaptive update operator to improve the efficiency of LEAP's global optimization of high-dimensional text examples and a mutation operator based on the greedy strategy to reduce the search time.We conducted a series of experiments to validate LEAP's ability to test NLP software and found that the average success rate of LEAP in generating adversarial test cases is 79.1%, which is 6.1% higher than the next best approach (PSOattack). While ensuring high success rates, LEAP significantly reduces time overhead by up to 147.6s compared to other heuristic-based methods. Additionally, the experimental results demonstrate that LEAP can generate more transferable test cases and significantly enhance the robustness of DNN-based systems.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1136–1148},
numpages = {13},
keywords = {NLP software testing, particle swarm optimization},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3573428.3573679,
author = {Wei, Muhua and Zhao, Xuyu and Zhang, Zheng and Zhang, Wei and Pan, Hongyun},
title = {New IT Service Tracks Study based on Product Trends and Typical Forecast Model Analysis},
year = {2023},
isbn = {9781450397148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573428.3573679},
doi = {10.1145/3573428.3573679},
abstract = {New industries and technologies continue to emerge as a result of the digital economy's explosive growth, by creating new markets and tracks for economic advancement. A practical issue worth doing research for business is how to select and acquire better advantages in the new tracks. In this paper, we analyze the product development trends, use multiple regression analysis and Vector Auto-Regression (VAR) model to study the key factors affecting the development of potential tracks, and carry out empirical analysis on the identified development tracks to make a prediction on the track development and help relevant enterprises to choose new tracks and layout in advance.},
booktitle = {Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering},
pages = {1420–1427},
numpages = {8},
location = {Xiamen, China},
series = {EITCE '22}
}

@inproceedings{10.1145/3510466.3511274,
author = {Meixner, Kristof and Feichtinger, Kevin and Rabiser, Rick and Biffl, Stefan},
title = {Efficient Production Process Variability Exploration},
year = {2022},
isbn = {9781450396042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510466.3511274},
doi = {10.1145/3510466.3511274},
abstract = {Cyber-Physical Production Systems (CPPSs) manufacture highly-customizable products from a product family following a sequence of production steps. For a CPPS, basic planners design feasible production process sequences by arranging atomic production steps based on implicit domain knowledge. However, the manual design of production sequences is inefficient and hard to reproduce due to the large configuration space. In this paper, we introduce the Iterative Process Sequence Exploration (IPSE) approach that (i) elicits domain knowledge in an industrial variability artifact, using the Product-Process-Resource Domain-Specific Language (PPR–DSL); (ii) reduces configuration space size regarding structural product variability and behavioral process variability; and (iii) facilitates efficiently exploring the configuration space in a process decision model. For production process sequence design, IPSE is a first approach to combine structural and behavioral variability models. We investigated the feasibility of the IPSE in a study on a typical manufacturing work line in automotive production. We compare the IPSE to a traditional process sequence planning approach. Our study indicates IPSE to be more efficient than the traditional manual approach.},
booktitle = {Proceedings of the 16th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {14},
numpages = {9},
keywords = {Configuration Reduction., Cyber-Physical Production System, Process Variability, Variability Modeling},
location = {Florence, Italy},
series = {VaMoS '22}
}

@article{10.1145/3630252,
author = {Lustosa, Andre and Menzies, Tim},
title = {Learning from Very Little Data: On the Value of Landscape Analysis for Predicting Software Project Health},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3630252},
doi = {10.1145/3630252},
abstract = {When data is scarce, software analytics can make many mistakes. For example, consider learning predictors for open source project health (e.g., the number of closed pull requests in 12 months time). The training data for this task may be very small (e.g., 5 years of data, collected every month means just 60 rows of training data). The models generated from such tiny datasets can make many prediction errors.Those errors can be tamed by a landscape analysis that selects better learner control parameters. Our niSNEAK tool (a)&nbsp;clusters the data to find the general landscape of the hyperparameters, then (b)&nbsp;explores a few representatives from each part of that landscape. niSNEAK is both faster and more effective than prior state-of-the-art hyperparameter optimization algorithms (e.g., FLASH, HYPEROPT, OPTUNA).The configurations found by niSNEAK have far less error than other methods. For example, for project health indicators such as C = number of commits, I = number of closed issues, and R = number of closed pull requests, niSNEAK’s 12-month prediction errors are {I=0%, R=33%&nbsp;C=47%}, whereas other methods have far larger errors of {I=61%,R=119%&nbsp;C=149%}. We conjecture that niSNEAK works so well since it finds the most informative regions of the hyperparameters, then jumps to those regions. Other methods (that do not reflect over the landscape) can waste time exploring less informative options.Based on the preceding, we recommend landscape analytics (e.g., niSNEAK) especially when learning from very small datasets. This article only explores the application of niSNEAK to project health. That said, we see nothing in principle that prevents the application of this technique to a wider range of problems.To assist other researchers in repeating, improving, or even refuting our results, all our scripts and data are available on GitHub at https://github.com/zxcv123456qwe/niSneak.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {58},
numpages = {22},
keywords = {Hyperparameter tuning, software health, indepedent variable clustering}
}

@article{10.1145/3720513,
author = {Kuraj, Ivan and Feser, John and Polikarpova, Nadia and Solar-Lezama, Armando},
title = {Peepco: Batch-Based Consistency Optimization},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3720513},
doi = {10.1145/3720513},
abstract = {We present batch-based consistency, a new approach for consistency optimization that allows programmers to specialize consistency with application-level integrity properties. We implement the approach with a two-step process: we statically infer optimal consistency requirements for executions of bounded sets of operations, and then, use the inferred requirements to parameterize a new distributed protocol to relax operation reordering at run time when it is safe to do so. Our approach supports standard notions of consistency. We implement batch-based consistency in Peepco, demonstrate its expressiveness for partial data replication, and examine Peepco’s run-time performance impact in different settings.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {119},
numpages = {29},
keywords = {Data Consistency, Distributed Systems, Integrity, Invariants, Model Checking, Program Synthesis}
}

@inproceedings{10.1145/3510454.3517057,
author = {Terzimehi\'{c}, Tarik},
title = {Architecture synthesis for optimized and flexible production},
year = {2022},
isbn = {9781450392235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510454.3517057},
doi = {10.1145/3510454.3517057},
abstract = {The fourth industrial revolution (Industry 4.0) anticipates frequent synthesis and optimization of different architectural design decisions (ADDs) - such as deployment of software components to hardware components, service composition, production planning, and topology (plant layout) synthesis. The frequent manual search for valid and optimal architectural designs is a time- and cognition-consuming task for an engineer. This asks for automating the process of deriving different ADDs. Although automating different ADDs is intensely investigated in other domains, the current research works 1) require higher engineering effort for specifying architecture optimization problems; 2) conduct (only) sequential ADDs, leading to lower solution quality (i.e., sub-optimal production); 3) neglect reconfigurability and reliability of architectures, and, thereby, offer no solution for production downtime; 4) neglect event-based execution semantics while considering timing-related issues. Therefore, I propose a Satisfiability Modulo Theories (SMT)-based framework for joint synthesis and optimization of multi-dimensional ADDs using industrial automation domain models (e.g., plant topology, product recipes, stations capabilities, etc.). This research should bring following benefits for the practitioners and researchers: 1) reduction of engineering effort for conducting different ADDs; 2) improvement of different quality attributes (e.g., production performance, reconfigurability, reliability, etc.); 3) guideline/support for a practitioner in choosing ADDs workflow to improve given quality attributes.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings},
pages = {251–255},
numpages = {5},
keywords = {architecture synthesis, deployment, design space exploration, industry 4.0, model-based development, optimization, service composition},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3644523.3644690,
author = {Zheng, Lili and Jiang, Xinhui},
title = {Bridging Internet Audiovisual Products Social Responsibility with User Loyalty: Processes and Context},
year = {2024},
isbn = {9798400709517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644523.3644690},
doi = {10.1145/3644523.3644690},
abstract = {User loyalty (UL) is the key to the economic effectiveness of internet audiovisual products, and internet audiovisual products social responsibility (IAPSR) is widely recognized as a driver of UL. In order to explore the path relationship between IAPSR and UL, this study developed a framework to study how IAPSR of three different dimensions economic responsibility (ECOR), legal responsibility (LEGR) and environmental responsibility (ENVR), affects UL, taking into account the role of user identification (UI), user trust (UT) and immersion (IMM). The data were collected from 524 internet audiovisual users in China, and the structural equation model (SEM) was used for empirical analysis using Smart PLS4 software. The results show that UI partially mediates the relationship between ECOR and UL, and completely mediates the relationship between ENVR and UL. UT partially mediates the relationship between ECOR and UL, and completely mediates the relationship between LEGR, ENVR and UL. IMM positively moderates the influence of UI on UL, and negatively moderates the influence of UT on UL.},
booktitle = {Proceedings of the 2023 4th International Conference on Computer Science and Management Technology},
pages = {930–936},
numpages = {7},
location = {Xi'an, China},
series = {ICCSMT '23}
}

@inproceedings{10.1145/3689492.3690046,
author = {Str\"{o}mb\"{a}ck, Filip and Varr\'{o}, D\'{a}niel},
title = {Active DSU: Dynamic Software Updates for Active Functions},
year = {2024},
isbn = {9798400712159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689492.3690046},
doi = {10.1145/3689492.3690046},
abstract = {Dynamic Software Updating (DSU) is a technique for updating programs without restarting them. It is useful in systems that provide live programming or in system with high availability needs. As a common limitation, existing DSU systems cannot update active functions. Hence, they are unable to update long-running functions, such as the main loop in a web server, or a state machine in an embedded system implemented as a loop. Updating active functions is challenging as it requires updating local variables and control flow to create a consistent state in the new version of the function. In this paper, we propose Active DSU, which updates the call stack to migrate data and control flow of active functions without the need to wait for the program to reach a particular state beforehand. This is achieved by replacing return addresses with stubs to avoid moving other stack frames on the call stack. Active DSU can migrate control flow automatically without input from the programmer in most cases.},
booktitle = {Proceedings of the 2024 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {26–37},
numpages = {12},
keywords = {active functions, dynamic software updating, live programming, stack rewriting},
location = {Pasadena, CA, USA},
series = {Onward! '24}
}

@inproceedings{10.1145/3674805.3686693,
author = {Chakraborty, Shalini and Liebel, Grischa},
title = {Evaluating Software Modelling Recommendations: Towards Systematic Guidelines for Modelling},
year = {2024},
isbn = {9798400710476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674805.3686693},
doi = {10.1145/3674805.3686693},
abstract = {Background: Despite having several advantages, software modelling remains unpopular for developers. Similarly, university students do not see the benefits of software modelling in the university curriculum. Prior research show the lack of guidance for students to do so. Aims: We aim to evaluate the effectiveness of four modelling recommendations made in related work to improve student modelling knowledge. Additionally, we aim to discover students’ perceptions of software modelling after taking a course with the recommendations included. Method: We conducted a mixed method study, including interviews with teaching assistants, student surveys, and a focus group study involving students, teaching assistants, and experts from both modelling and education. Results: We find that the four recommendations overall have a positive impact as they help students better understand the modelling knowledge from the course. Students express that specific recommendations help them grasp the concept of software modelling well. We also extend the recommendations by adding more details specific to software modelling and solidifying the recommendations into systematic guidelines. Conclusions: The guidelines can potentially enhance education and training in software modelling, catering to both academic settings and industrial environments. Additionally, the guidelines contribute to improved communication between students and the course itself by outlining what students can expect from modelling assignments and the value inherent in each of these assignments.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {337–347},
numpages = {11},
keywords = {Education, Focus Group, Interview, Software Modelling, Survey, UML},
location = {Barcelona, Spain},
series = {ESEM '24}
}

@article{10.1145/3712003,
author = {He, Junda and Treude, Christoph and Lo, David},
title = {LLM-Based Multi-Agent Systems for Software Engineering: Literature Review, Vision and the Road Ahead},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3712003},
doi = {10.1145/3712003},
abstract = {Integrating Large Language Models (LLMs) into autonomous agents marks a significant shift in the research landscape by offering cognitive abilities that are competitive with human planning and reasoning. This paper explores the transformative potential of integrating Large Language Models into Multi-Agent (LMA) systems for addressing complex challenges in software engineering (SE). By leveraging the collaborative and specialized abilities of multiple agents, LMA systems enable autonomous problem-solving, improve robustness, and provide scalable solutions for managing the complexity of real-world software projects. In this paper, we conduct a systematic review of recent primary studies to map the current landscape of LMA applications across various stages of the software development lifecycle (SDLC). To illustrate current capabilities and limitations, we perform two case studies to demonstrate the effectiveness of state-of-the-art LMA frameworks. Additionally, we identify critical research gaps and propose a comprehensive research agenda focused on enhancing individual agent capabilities and optimizing agent synergy. Our work outlines a forward-looking vision for developing fully autonomous, scalable, and trustworthy LMA systems, laying the foundation for the evolution of Software Engineering 2.0.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Large Language Models, Autonomous Agents, Multi-Agent Systems, Software Engineering}
}

@article{10.1145/3544489,
author = {Audet, Charles and Le Digabel, S\'{e}bastien and Montplaisir, Viviane Rochon and Tribes, Christophe},
title = {Algorithm&nbsp;1027: NOMAD Version&nbsp;4: Nonlinear Optimization with the MADS Algorithm},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {3},
issn = {0098-3500},
url = {https://doi.org/10.1145/3544489},
doi = {10.1145/3544489},
abstract = {NOMADis a state-of-the-art software package for optimizing blackbox problems. In continuous development since 2001, it constantly evolved with the integration of new algorithmic features published in scientific publications. These features are motivated by real applications encountered by industrial partners. The latest major release of NOMAD, version&nbsp;3, dates to 2008. Minor releases are produced as new features are incorporated. The present work describes NOMAD &nbsp;4, a complete redesign of the previous version, with a new architecture providing more flexible code, added functionalities, and reusable code. We introduce algorithmic components, which are building blocks for more complex algorithms and can initiate other components, launch nested algorithms, or perform specialized tasks. They facilitate the implementation of new ideas, including the MegaSearchPoll component, warm and hot restarts, and a revised version of the PsdMads algorithm. Another main improvement of NOMAD &nbsp;4 is the usage of parallelism, to simultaneously compute multiple blackbox evaluations and to maximize usage of available cores. Running different algorithms, tuning their parameters, and comparing their performance for optimization are simpler than before, while overall optimization performance is maintained between versions&nbsp;3 and&nbsp;4. NOMAD is freely available at www.gerad.ca/nomad
 and the whole project is visible at github.com/bbopt/nomad.},
journal = {ACM Trans. Math. Softw.},
month = sep,
articleno = {35},
numpages = {22},
keywords = {Optimization software, blackbox optimization, derivative-free optimization, mesh adaptive direct search}
}

@inproceedings{10.1145/3690134.3694816,
author = {Heinl, Michael P. and Embacher, Victor},
title = {BT2X: Multi-Leveled Binary Transparency to Protect the Software Supply Chain of Operational Technology},
year = {2024},
isbn = {9798400712449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3690134.3694816},
doi = {10.1145/3690134.3694816},
abstract = {An increasing number of attacks targeting software supply chains poses a significant threat to software-reliant systems such as Operational Technology (OT). One noteworthy variant of software supply chain attacks is the circumvention of code signing by utilizing stolen signing keys. Binary Transparency (BT) serves as a mechanism to detect and deter such attacks by mandating that every signed binary is stored in a trusted append-only log. We introduce BT-To-The-X (BT2X) which brings BT to OT. To support retrofitting of computationally less capable devices, BT2X introduces well-defined audit levels and assisting infrastructure. Furthermore, it includes a federated gossiping protocol to detect misbehaving logs presenting inconsistent views to different observers. We implemented BT2X on low-power microcontrollers using Rust and evaluated it with regard to size and performance to demonstrate its practical feasibility.},
booktitle = {Proceedings of the Sixth Workshop on CPS&amp;IoT Security and Privacy},
pages = {41–54},
numpages = {14},
keywords = {code signing, constrained devices, defense-in-depth, iec 62443, software supply chain security, software transparency},
location = {Salt Lake City, UT, USA},
series = {CPSIoTSec'24}
}

@inproceedings{10.1145/3561877.3561882,
author = {Chua, Choon Hoe and Ng, Sok Choo},
title = {Open-Source VPN Software: Performance Comparison for Remote Access},
year = {2022},
isbn = {9781450396837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3561877.3561882},
doi = {10.1145/3561877.3561882},
abstract = {The use of Virtual Private Networks (VPN) as a mean to secure communication has become increasingly important with the growth in remote working. While vendor supplied VPN solutions are widely available, there is a growing trend to evaluate and implement solutions based on open-source software. In this study, we investigate three popular open-source VPN software (OpenVPN, OpenConnect and Wireguard) and compare their relative merits in terms of performance. Several studies have shown Wireguard's superiority in performance over OpenVPN and other VPN software but neither it nor OpenVPN has been benchmarked against OpenConnect. This study presents the performance differences between the three products across a range of client devices in selected typical deployment scenarios. We also evaluate the performance differences of the software on desktop servers functioning as VPN gateways behind a router and Network Address Translator. Our study shows that VPN performance, besides the server side, is highly dependent on implementation of the client application. Even though Wireguard is ahead in performance in some operating environments, there are still cases where OpenVPN and OpenConnect come out on par.},
booktitle = {Proceedings of the 5th International Conference on Information Science and Systems},
pages = {29–34},
numpages = {6},
keywords = {Open Source, Performance, Remote Access, VPN},
location = {Beijing, China},
series = {ICISS '22}
}

@inproceedings{10.1145/3593013.3593976,
author = {Laufer, Benjamin and Gilbert, Thomas and Nissenbaum, Helen},
title = {Optimization’s Neglected Normative Commitments},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593013.3593976},
doi = {10.1145/3593013.3593976},
abstract = {Optimization is offered as an objective approach to resolving complex, real-world decisions involving uncertainty and conflicting interests. It drives business strategies as well as public policies and, increasingly, lies at the heart of sophisticated machine learning systems. A paradigm used to approach potentially high-stakes decisions, optimization relies on abstracting the real world to a set of decision(s), objective(s) and constraint(s). Drawing from the modeling process and a range of actual cases, this paper describes the normative choices and assumptions that are necessarily part of using optimization. It then identifies six emergent problems that may be neglected: 1) Misspecified values can yield optimizations that omit certain imperatives altogether or incorporate them incorrectly as a constraint or as part of the objective, 2) Problematic decision boundaries can lead to faulty modularity assumptions and feedback loops, 3) Failing to account for multiple agents’ divergent goals and decisions can lead to policies that serve only certain narrow interests, 4) Mislabeling and mismeasurement can introduce bias and imprecision, 5) Faulty use of relaxation and approximation methods, unaccompanied by formal characterizations and guarantees, can severely impede applicability, and 6) Treating optimization as a justification for action, without specifying the necessary contextual information, can lead to ethically dubious or faulty decisions. Suggestions are given to further understand and curb the harms that can arise when optimization is used wrongfully.},
booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
pages = {50–63},
numpages = {14},
keywords = {Optimization, ethics, modeling assumptions, values},
location = {Chicago, IL, USA},
series = {FAccT '23}
}

@article{10.1145/3559755,
author = {Stripinis, Linas and Paulavi\v{c}ius, Remigijus},
title = {DIRECTGO: A New DIRECT-Type MATLAB Toolbox for Derivative-Free Global Optimization},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0098-3500},
url = {https://doi.org/10.1145/3559755},
doi = {10.1145/3559755},
abstract = {In this work, we introduce DIRECTGO, a new MATLAB toolbox for derivative-free global optimization. DIRECTGO collects various deterministic derivative-free DIRECT-type algorithms for box-constrained, generally constrained, and problems with hidden constraints. Each sequential algorithm is implemented in two ways: using static and dynamic data structures for more efficient information storage and organization. Furthermore, parallel schemes are applied to some promising algorithms within DIRECTGO. The toolbox is equipped with a graphical user interface (GUI), ensuring the user-friendly use of all functionalities available in DIRECTGO. Available features are demonstrated in detailed computational studies using a comprehensive DIRECTGOLib v1.0 library of global optimization test problems. Additionally, 11 classical engineering design problems illustrate the potential of DIRECTGO to solve challenging real-world problems. Finally, the appendix gives examples of accompanying MATLAB programs and provides a synopsis of its use on the test problems with box and general constraints.},
journal = {ACM Trans. Math. Softw.},
month = dec,
articleno = {41},
numpages = {46},
keywords = {Global optimization, derivative-free optimization, DIRECT-type algorithms, benchmarking, optimization software, MATLAB, TOMLAB}
}

@article{10.1145/3714468,
author = {Oldfield, Noah H. and Laaber, Christoph and Yue, Tao and Ali, Shaukat},
title = {Faster and Better Quantum Software Testing through Specification Reduction and Projective Measurements},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3714468},
doi = {10.1145/3714468},
abstract = {Quantum computing (QC) promises polynomial and exponential speedups in many domains, such as unstructured search and prime number factoring. However, quantum programs yield probabilistic outputs from exponentially growing distributions and are vulnerable to quantum-specific faults. Existing quantum software testing (QST) approaches treat quantum superpositions as classical distributions. This leads to two major limitations when applied to quantum programs: (1) an exponentially growing sample space distribution and (2) failing to detect quantum-specific faults such as phase flips. To overcome these limitations, we introduce a QST approach, which applies a reduction algorithm to a quantum program specification. The reduced specification alleviates the limitations (1) by enabling faster sampling through quantum parallelism and (2) by performing projective measurements in the mixed Hadamard basis. Our evaluation of 143 quantum programs across four categories demonstrates significant improvements in test runtimes and fault detection with our reduction approach. Average test runtimes improved from 169.9s to 11.8s, with notable enhancements in programs with large circuit depths (383.1s to 33.4s) and large program specifications (464.8s to 7.7s). Furthermore, our approach increases mutation scores from  (54.5%)  to  (74.7%) , effectively detecting phase flip faults that non-reduced specifications miss. These results underline our approach’s importance to improve QST efficiency and effectiveness},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Quantum computing, software testing, quantum program specification, projective measurements}
}

@inproceedings{10.1109/SC41406.2024.00037,
author = {Khan, Awais and Lange, John R. and Hagerty, Nick and Posada, Edwin F. and Holmen, John and White, James B. and Harris, Austin and Vergara, Ver\'{o}nica Melesse and Zimmer, Christopher and Atchley, Scott},
title = {An Evaluation of the Effect of Network Cost Optimization for Leadership Class Supercomputers},
year = {2024},
isbn = {9798350352917},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC41406.2024.00037},
doi = {10.1109/SC41406.2024.00037},
abstract = {Dragonfly-based networks are an extensively deployed network topology in large-scale high-performance computing due to their cost-effectiveness and efficiency. The US will soon have three Exascale supercomputers for leadership class workloads deployed using dragonfly networks. Compared to indirect networks of similar scale, the dragonfly network has considerably reduced cable lengths, cable counts, and switch counts, resulting in significant network cost savings for a given system size, however, these cost reductions result in reduced global minimal paths and more challenging routing. Additionally, large scale dragonfly networks often require a taper at the global link level, resulting in less bisection bandwidth than is achievable in other traditional non-blocking topologies of equivalent scale. While dragonfly networks have been extensively studied, they have yet to be fully evaluated in an extreme scale (i.e., exascale) system that targets capability workloads. In this paper, we present the results of the first large scale evaluation of a dragonfly network on an exascale system (Frontier) and compare its behavior to a similar scale fat-tree network on a previous generation TOP500 system (Summit). This evaluation aims to determine the effect of network cost optimizations by measuring a tapered topology's impact on capability workloads. Our evaluation is based on a collection of synthetic microbenchmarks, mini-apps, and full scale applications. It compares the scaling efficiencies of each benchmark between the dragonfly-based Frontier and the fat-tree-based Summit systems. Our results show that a dragonfly network is ~30% more cost efficient than a fat-tree topology, which amortizes to ~3% of an exascale system cost. Furthermore, while tapered dragonfly networks impose significant tradeoffs, the impacts are not as broad as initially thought and are mostly seen in applications with global communication patterns, particularly all-to-all (e.g., FFT-based algorithms), but also local communication patterns (e.g., nearest-neighbor algorithms) that are sensitive to network performance variability.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {31},
numpages = {16},
keywords = {Dragonfly &amp; Fat-tree network topologies, HPC systems, network cost optimization},
location = {Atlanta, GA, USA},
series = {SC '24}
}

@article{10.1145/3709674,
author = {Zhang, Chi and Rigger, Manuel},
title = {Constant Optimization Driven Database System Testing},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/3709674},
doi = {10.1145/3709674},
abstract = {Logic bugs are bugs that can cause database management systems (DBMSs) to silently produce incorrect results for given queries. Such bugs are severe, because they can easily be overlooked by both developers and users, and can cause applications that rely on the DBMSs to malfunction. In this work, we propose Constant-Optimization-Driven Database Testing (CODDTest) as a novel approach for detecting logic bugs in DBMSs. This method draws inspiration from two well-known optimizations in compilers: constant folding and constant propagation. Our key insight is that for a certain database state and query containing a predicate, we can apply constant folding on the predicate by replacing an expression in the predicate with a constant, anticipating that the results of this predicate remain unchanged; any discrepancy indicates a bug in the DBMS. We evaluated CODDTest on five mature and extensively-tested DBMSs--SQLite, MySQL, CockroachDB, DuckDB, and TiDB--and found 45 unique, previously unknown bugs in them. Out of these, 24 are unique logic bugs. Our manual analysis of the state-of-the-art approaches indicates that 11 logic bugs are detectable only by CODDTest. We believe that CODDTest is easy to implement, and can be widely adopted in practice.},
journal = {Proc. ACM Manag. Data},
month = feb,
articleno = {24},
numpages = {24},
keywords = {DBMSs testing, logic bugs, test oracle}
}

@article{10.1145/3660767,
author = {Liang, Jenny T. and Badea, Carmen and Bird, Christian and DeLine, Robert and Ford, Denae and Forsgren, Nicole and Zimmermann, Thomas},
title = {Can GPT-4 Replicate Empirical Software Engineering Research?},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660767},
doi = {10.1145/3660767},
abstract = {Empirical software engineering research on production systems has brought forth a better understanding of the software engineering process for practitioners and researchers alike. However, only a small subset of production systems is studied, limiting the impact of this research. While software engineering practitioners could benefit from replicating research on their own data, this poses its own set of challenges, since performing replications requires a deep understanding of research methodologies and subtle nuances in software engineering data. Given that large language models (LLMs), such as GPT-4, show promise in tackling both software engineering- and science-related tasks, these models could help replicate and thus democratize empirical software engineering research.
 

 
In this paper, we examine GPT-4’s abilities to perform replications of empirical software engineering research on new data. We specifically study their ability to surface assumptions made in empirical software engineering research methodologies, as well as their ability to plan and generate code for analysis pipelines on seven empirical software engineering papers. We perform a user study with 14 participants with software engineering research expertise, who evaluate GPT-4-generated assumptions and analysis plans (i.e., a list of module specifications) from the papers. We find that GPT-4 is able to surface correct assumptions, but struggles to generate ones that apply common knowledge about software engineering data. In a manual analysis of the generated code, we find that the GPT-4-generated code contains correct high-level logic, given a subset of the methodology. However, the code contains many small implementation-level errors, reflecting a lack of software engineering knowledge. Our findings have implications for leveraging LLMs for software engineering research as well as practitioner data scientists in software teams.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {60},
numpages = {24},
keywords = {Large language models, empirical software engineering, study replication}
}

@inproceedings{10.5555/3643142.3643192,
author = {Widmer, Simon and Shaukat, Syed and Wu, Cheng-Lung},
title = {Aircraft Line Maintenance Scheduling Using Simulation and Reinforcement Learning},
year = {2024},
isbn = {9798350369663},
publisher = {IEEE Press},
abstract = {This paper presents a reinforcement learning (RL) algorithm prototype to solve the aircraft line maintenance scheduling problem. The Line Maintenance Scheduling Problem (LMSP) is concerned with scheduling a set of maintenance tasks during an aircraft's ground time. To address this problem, we introduce a novel LMSP method combining a hybrid simulation model and reinforcement learning to schedule maintenance tasks at multiple airports. Initially, this paper briefly reviews the existing literature on optimization-based and AI-enhanced aircraft maintenance scheduling. Secondly, the novel reinforcement learning LMSP method is introduced, evaluated using industry data, and compared with optimization-based LMSP solutions. Our experiments demonstrate that the LMSP method using reinforcement learning is capable of identifying near-optimal policies for scheduling line maintenance jobs when compared to the exact and heuristics-based methods. The proposed model provides an excellent foundation for future studies on AI-enhanced scheduling problems.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {600–611},
numpages = {12},
location = {San Antonio, Texas, USA},
series = {WSC '23}
}

@article{10.1145/3672451,
author = {Wan, Chengcheng and Liu, Shicheng and Xie, Sophie and Liu, Yuhan and Hoffmann, Henry and Maire, Michael and Lu, Shan},
title = {Keeper: Automated Testing and Fixing of Machine Learning Software},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3672451},
doi = {10.1145/3672451},
abstract = {The increasing number of software applications incorporating machine learning (ML) solutions has led to the need for testing techniques. However, testing ML software requires tremendous human effort to design realistic and relevant test inputs and to judge software output correctness according to human common sense. Even when misbehavior is exposed, it is often unclear whether the defect is inside ML API or the surrounding code and how to fix the implementation. This article tackles these challenges by proposing Keeper, an automated testing and fixing tool for ML software. The core idea of Keeper is designing pseudo-inverse functions that semantically reverse the corresponding ML task in an empirical way and proxy common human judgment of real-world data. It incorporates these functions into a symbolic execution engine to generate tests. Keeper also detects code smells that degrade software performance. Once misbehavior is exposed, Keeper attempts to change how ML APIs are used to alleviate the misbehavior.Our evaluation on a variety of applications shows that Keeper greatly improves branch coverage, while identifying 74 previously unknown failures and 19 code smells from 56 out of 104 applications. Our user studies show that 78% of end-users and 95% of developers agree with Keeper’s detection and fixing results.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {167},
numpages = {33},
keywords = {Software testing, machine learning, machine learning API}
}

@article{10.1145/3380930,
author = {Anzt, Hartwig and Cojean, Terry and Yen-Chen, Chen and Dongarra, Jack and Flegar, Goran and Nayak, Pratik and Tomov, Stanimire and Tsai, Yuhsiang M. and Wang, Weichung},
title = {Load-balancing Sparse Matrix Vector Product Kernels on GPUs},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {2329-4949},
url = {https://doi.org/10.1145/3380930},
doi = {10.1145/3380930},
abstract = {Efficient processing of Irregular Matrices on Single Instruction, Multiple Data (SIMD)-type architectures is a persistent challenge. Resolving it requires innovations in the development of data formats, computational techniques, and implementations that strike a balance between thread divergence, which is inherent for Irregular Matrices, and padding, which alleviates the performance-detrimental thread divergence but introduces artificial overheads. To this end, in this article, we address the challenge of designing high performance sparse matrix-vector product (SpMV) kernels designed for Nvidia Graphics Processing Units (GPUs). We present a compressed sparse row (CSR) format suitable for unbalanced matrices. We also provide a load-balancing kernel for the coordinate (COO) matrix format and extend it to a hybrid algorithm that stores part of the matrix in SIMD-friendly Ellpack format (ELL) format. The ratio between the ELL- and the COO-part is determined using a theoretical analysis of the nonzeros-per-row distribution. For the over 2,800 test matrices available in the Suite Sparse matrix collection, we compare the performance against SpMV kernels provided by NVIDIA’s cuSPARSE library and a heavily-tuned sliced ELL (SELL-P) kernel that prevents unnecessary padding by considering the irregular matrices as a combination of matrix blocks stored in ELL format.},
journal = {ACM Trans. Parallel Comput.},
month = mar,
articleno = {2},
numpages = {26},
keywords = {GPUs, Sparse Matrix Vector Product (SpMV), irregular matrices}
}

@inproceedings{10.1145/3649329.3656252,
author = {Chen, Guojin and He, Hongquan and Xu, Peng and Geng, Hao and Yu, Bei},
title = {Efficient Bilevel Source Mask Optimization},
year = {2024},
isbn = {9798400706011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649329.3656252},
doi = {10.1145/3649329.3656252},
abstract = {Resolution Enhancement Techniques (RETs) are critical to meet the demands of advanced technology nodes. Among RETs, Source Mask Optimization (SMO) is pivotal, concurrently optimizing both the source and the mask to expand the process window. Traditional SMO methods, however, are limited by sequential and alternating optimizations, leading to extended runtimes without performance guarantees. This paper introduces a unified SMO framework utilizing the accelerated Abbe forward imaging to enhance precision and efficiency. Further, we propose the innovative BiSMO framework, which reformulates SMO through a bilevel optimization approach, and present three gradient-based methods to tackle the challenges of bilevel SMO. Our experimental results demonstrate that BiSMO achieves a remarkable 40% reduction in error metrics and 8\texttimes{} increase in runtime efficiency, signifying a major leap forward in SMO.},
booktitle = {Proceedings of the 61st ACM/IEEE Design Automation Conference},
articleno = {142},
numpages = {6},
location = {San Francisco, CA, USA},
series = {DAC '24}
}

@inproceedings{10.1145/3629526.3645033,
author = {Khosravi Tabrizi, Amirmahdi and Ezzati-Jivan, Naser and Tetreault, Francois},
title = {An Adaptive Logging System (ALS): Enhancing Software Logging with Reinforcement Learning Techniques},
year = {2024},
isbn = {9798400704444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629526.3645033},
doi = {10.1145/3629526.3645033},
abstract = {The efficient management of software logs is crucial in software performance evaluation, enabling detailed examination of runtime information for postmortem analysis. Recognizing the importance of logs and the challenges developers face in making informed log-placement decisions, there is a clear need for a robust log-placement framework that supports developers. Existing frameworks, however, are limited by their inability to adapt to customized logging objectives, a concern highlighted by our industrial partner, Ciena, who required a system for their specific logging goals in resource-limited environments like routers. Moreover, these frameworks often show poor cross-project consistency. This study introduces a novel performance logging objective designed to uncover potential performance-bugs, categorized into three classes-Loops, Synchronization, and API Misuses-and defines 12 source code features for their detection. We present an Adaptive Logging System (ALS), based on reinforcement learning, which adjusts to specified logging objectives, particularly for identifying performance-bugs. This framework, not restricted to specific projects, demonstrates stable cross-project performance. We trained and evaluated ALS on Python source code from 17 diverse open-source projects within the Apache and Django ecosystems. Our findings suggest that ALS has the potential to significantly enhance current logging practices by providing a more targeted, efficient, and context-aware logging approach, particularly beneficial for our industry partner who requires a flexible system that adapts to varied performance objectives and logging needs in their unique operational environments.},
booktitle = {Proceedings of the 15th ACM/SPEC International Conference on Performance Engineering},
pages = {37–47},
numpages = {11},
keywords = {adaptive systems, performance bugs, reinforcement learning, software logging},
location = {London, United Kingdom},
series = {ICPE '24}
}

@inproceedings{10.1109/ISLPED52811.2021.9502484,
author = {Crafton, Brian and Spetalnick, Samuel and Yoon, Jong-Hyeok and Raychowdhury, Arijit},
title = {Statistical optimization of compute in-memory performance under device variation},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISLPED52811.2021.9502484},
doi = {10.1109/ISLPED52811.2021.9502484},
abstract = {Compute in-memory (CIM) is a promising technique that minimizes data transport, maximizes memory throughput, and performs computation on the bitline of memory sub-arrays. Utilizing embedded non-volatile memories (eNVM) such as resistive random access memory (RRAM), various forms of neural networks can be implemented. Unfortunately, CIM faces new challenges traditional CMOS architectures have avoided. In this work, we explore the impact of device variation (calibrated with measured data on foundry RRAM arrays) and propose a new algorithm based on device variation to increase both performance and accuracy for CIM designs. We demonstrate up to 36% power improvement and 44% performance improvement, while satisfying any error constraint.},
booktitle = {Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design},
articleno = {14},
numpages = {6},
location = {Boston, Massachusetts},
series = {ISLPED '21}
}

@inproceedings{10.1145/3624062.3624597,
author = {Westerhout, Tom and Chamberlain, Bradford L.},
title = {Implementing scalable matrix-vector products for the exact diagonalization methods in quantum many-body physics},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624597},
doi = {10.1145/3624062.3624597},
abstract = {Exact diagonalization is a well-established method for simulating small quantum systems. Its applicability is limited by the exponential growth of the Hamiltonian matrix that needs to be diagonalized. Physical symmetries are usually utilized to reduce the matrix dimension, and distributed-memory parallelism is employed to explore larger systems. This paper focuses on an implementation of the core distributed algorithms, with a special emphasis on the matrix-vector product. Instead of the conventional MPI+X paradigm, Chapel is chosen as the language in this work. We provide a comprehensive description of the algorithms and present performance and scalability tests. Our implementation outperforms the state-of-the-art MPI-based solution by a factor of 7–8 on 32 compute nodes or 4096 cores and scales well through 256 nodes or 32 768 cores. The implementation has 3 times fewer software lines of code than the current state of the art, but is still able to handle generic Hamiltonians.},
booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {1140–1150},
numpages = {11},
keywords = {Chapel, Nix, distributed algorithms, exact diagonalization, quantum many-body},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@inproceedings{10.1145/3629526.3645048,
author = {Geldenhuys, Morgan K. and Scheinert, Dominik and Kao, Odej and Thamsen, Lauritz},
title = {Demeter: Resource-Efficient Distributed Stream Processing under Dynamic Loads with Multi-Configuration Optimization},
year = {2024},
isbn = {9798400704444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629526.3645048},
doi = {10.1145/3629526.3645048},
abstract = {Distributed Stream Processing (DSP) focuses on the near real-time processing of large streams of unbounded data. To increase processing capacities, DSP systems are able to dynamically scale across a cluster of commodity nodes, ensuring a good Quality of Service despite variable workloads. However, selecting scaleout configurations which maximize resource utilization remains a challenge. This is especially true in environments where workloads change over time and node failures are all but inevitable. Furthermore, configuration parameters such as memory allocation and checkpointing intervals impact performance and resource usage as well. Sub-optimal configurations easily lead to high operational costs, poor performance, or unacceptable loss of service.In this paper, we present Demeter, a method for dynamically optimizing key DSP system configuration parameters for resource efficiency. Demeter uses Time Series Forecasting to predict future workloads and Multi-Objective Bayesian Optimization to model runtime behaviors in relation to parameter settings and workload rates. Together, these techniques allow us to determine whether or not enough is known about the predicted workload rate to proactively initiate short-lived parallel profiling runs for data gathering. Once trained, the models guide the adjustment of multiple, potentially dependent system configuration parameters ensuring optimized performance and resource usage in response to changing workload rates. Our experiments on a commodity cluster using Apache Flink demonstrate that Demeter significantly improves the operational efficiency of long-running benchmark jobs.},
booktitle = {Proceedings of the 15th ACM/SPEC International Conference on Performance Engineering},
pages = {142–153},
numpages = {12},
keywords = {cloud computing, cluster resource management, distributed stream processing, performance modeling, performance profiling, runtime optimization, time series forecasting},
location = {London, United Kingdom},
series = {ICPE '24}
}

@article{10.1145/3660792,
author = {Chen, Yuntianyi and Huai, Yuqi and Li, Shilong and Hong, Changnam and Garcia, Joshua},
title = {Misconfiguration Software Testing for Failure Emergence in Autonomous Driving Systems},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660792},
doi = {10.1145/3660792},
abstract = {The optimization of a system’s configuration options is crucial for determining its performance and functionality, particularly in the case of autonomous driving software (ADS) systems because they possess a multitude of such options. Research efforts in the domain of ADS have prioritized the development of automated testing methods to enhance the safety and security of self-driving cars. Presently, search-based approaches are utilized to test ADS systems in a virtual environment, thereby simulating real-world scenarios. However, such approaches rely on optimizing the waypoints of ego cars and obstacles to generate diverse scenarios that trigger violations, and no prior techniques focus on optimizing the ADS from the perspective of configuration. To address this challenge, we present a framework called ConfVE, which is the first automated configuration testing framework for ADSes. ConfVE’s design focuses on the emergence of violations through rerunning scenarios generated by different ADS testing approaches under different configurations, leveraging 9 test oracles to enable previous ADS testing approaches to find more types of violations without modifying their designs or implementations and employing a novel technique to identify bug-revealing violations and eliminate duplicate violations. Our evaluation results demonstrate that ConfVE can discover 1,818 unique violations and reduce 74.19% of duplicate violations.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {85},
numpages = {24},
keywords = {Autonomous driving systems, Software configuration}
}

@inbook{10.1145/3129743.3129752,
author = {Coppens, Bart and De Sutter, Bjorn and Volckaert, Stijn},
title = {Multi-variant execution environments},
year = {2018},
isbn = {9781970001839},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3129743.3129752},
abstract = {Memory corruption vulnerabilities are a common problem in software implemented in C/C++. Attackers can exploit these vulnerabilities to steal sensitive data and to seize or disrupt the system on which the software is executed. Memory safety techniques can, in principle, eliminate these vulnerabilities [Nagarakatte et al. 2009, Nagarakatte et al. 2010] but are prohibitively expensive in terms of runtime overhead [Szekeres et al. 2013].Instead, modern operating systems and compilers deploy exploit mitigations such as Address Space Layout Randomization (ASLR) [PaX Team 2004a], Data Execution Prevention (DEP, a.k.a.W.X) [PaX Team 2004b], and stack canaries [Cowan et al. 1998]. These exploit mitigations incur minimal performance overhead, but are limited in scope.often only defending against one particular type of exploit.and can be bypassed with only modest effort.Up-and-coming exploit mitigations, such as control-flow integrity [Abadi et al. 2005a, Tice et al. 2014], require more effort to bypass [G\"{o}ktas et al. 2014a, Davi et al. 2014, Carlini et al. 2015e, Evans et al. 2015, Schuster et al. 2015], but, similar to the aforementioned defenses, they defend only against attacks of one particular type: code reuse.The ubiquity of multi-core processors has made Multi-Variant Execution Environments (MVEEs) an increasingly attractive option to provide strong, comprehensive protection against memory corruption exploits, while still incurring only a fraction of the runtime overhead of full memory safety. MVEEs have been shown to successfully defend against several types of attacks, including code reuse [Volckaert et al. 2015], information leakage [Koning et al. 2016], stack buffer overflows [Salamat et al. 2009], and code injection [Cox et al. 2006].The underlying idea is to run several diversified instances of the same program, often referred to as variants or replicas, side by side on equivalent program inputs. The MVEE's main component, the monitor, feeds all variants these equivalent inputs and monitors the variants' behavior. The diversity techniques used to generate the variants ensure that the variants respond differently to malicious inputs, while leaving the behavior under normal operating conditions unaffected. The MVEE monitor detects the diverging behavior and halts the execution of the variants before they can harm the system. This implies that the variants must, to some extent, be executed in lockstep: potentially harmful operations in a variant are only executed when the consistency with the other variants has been validated.In recent years, over half a dozen systems have been proposed that match the above description. While most of them show many similarities, some authors have made radically different design choices. In this chapter, we discuss the design of MVEEs and provide implementation details about our own MVEE, the Ghent University Multi-Variant Execution Environment, orGHUMVEE, and its extensions. GHUMVEEhas been open sourced and can be downloaded from http://github.com/ stijn-volckaert/ReMon/.},
booktitle = {The Continuing Arms Race: Code-Reuse Attacks and Defenses},
pages = {211–258},
numpages = {48}
}

@inproceedings{10.1145/3421766.3421779,
author = {Zhang, Haibo and Li, Yingqiu},
title = {Optimal Design of Carrying Auxiliary Fixtures in an Automobile Production Line Based on Static Analysis Method},
year = {2020},
isbn = {9781450375535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3421766.3421779},
doi = {10.1145/3421766.3421779},
abstract = {In this paper, through the multi-objective optimization design theory, a mathematical model with the maximum structural strength and minimum mass as the objective function is established, and the multi-objective optimization mathematical model can be solved, which the thickness of the square tube in the side frame components and support components is reduced from 3mm to 1.5mm can get the non-inferior optimal solution. Through ANSYS, the static analysis of the structure under full load and stacking conditions is carried out. Under the conditions of satisfying the structural rigidity, strength and deformation, the optimized structure is proposed, and the optimized structure is simulated and verified. The simulation results show that the optimized auxiliary carrying structure can reduce the weight by 17 percent on the basis of satisfying the requirements of use, which can better achieve the design goal and achieve the purpose of lightweight design of the carrying auxiliary fixture structure. This has engineering application value for the improved design of the carrying auxiliary fixture structure, and also provides a design idea for the same type of mechanical structure problems.},
booktitle = {Proceedings of the 2nd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {422–427},
numpages = {6},
keywords = {Auto automatic production line, Carrying auxiliary fixture, Multi-objective optimization design, Statics analysis, Welding fixture},
location = {Manchester, United Kingdom},
series = {AIAM2020}
}

@article{10.1145/3664924,
author = {Wang, Yizhuo and Chang, Fangli and Wei, Bingxin and Gao, Jianhua and Ji, Weixing},
title = {Optimization of Sparse Matrix Computation for Algebraic Multigrid on GPUs},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1544-3566},
url = {https://doi.org/10.1145/3664924},
doi = {10.1145/3664924},
abstract = {AMG is one of the most efficient and widely used methods for solving sparse linear systems. The computational process of AMG mainly consists of a series of iterative calculations of generalized sparse matrix-matrix multiplication (SpGEMM) and sparse matrix-vector multiplication (SpMV). Optimizing these sparse matrix calculations is crucial for accelerating solving linear systems. In this paper, we first focus on optimizing the SpGEMM algorithm in AmgX, a popular AMG library for GPUs. We propose a new algorithm called SpGEMM-upper, which achieves an average speedup of 2.02\texttimes{} on Tesla V100 and 1.96\texttimes{} on RTX 3090 against the original algorithm. Next, through experimental investigation, we conclude that no single SpGEMM library or algorithm performs optimally for most sparse matrices, and the same holds true for SpMV. Therefore, we build machine learning-based models to predict the optimal SpGEMM and SpMV used in the AMG calculation process. Finally, we integrate the prediction models, SpGEMM-upper, and other selected algorithms into a framework for adaptive sparse matrix computation in AMG. Our experimental results prove that the framework achieves promising performance improvements on the test set.},
journal = {ACM Trans. Archit. Code Optim.},
month = sep,
articleno = {54},
numpages = {27},
keywords = {Algebraic multigrid, generalized sparse matrix-matrix multiplication, sparse matrix-vector multiplication, machine learning, GPU}
}

@article{10.1145/3506800,
author = {Albert, Elvira and Gordillo, Pablo and Hern\'{a}ndez-Cerezo, Alejandro and Rubio, Albert and Schett, Maria A.},
title = {Super-optimization of Smart Contracts},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3506800},
doi = {10.1145/3506800},
abstract = {Smart contracts are programs deployed on a blockchain. They are executed for a monetary fee paid in gas—a clear optimization target for smart contract compilers. Because smart contracts are a young, fast-moving field without (manually) fine-tuned compilers, they highly benefit from automated and adaptable approaches, especially as smart contracts are effectively immutable, and as such need a high level of assurance. This makes them an ideal domain for applying formal methods. Super-optimization is a technique to find the best translation of a block of instructions by trying all possible sequences of instructions that produce the same result. We present a framework for super-optimizing smart contracts based on Max-SMT with two main ingredients: (1) a stack functional specification extracted from the basic blocks of a smart contract, which is simplified using rules capturing the semantics of arithmetic, bit-wise, and relational operations, and (2) the synthesis of optimized blocks, which finds—by means of an efficient SMT encoding—basic blocks with minimal gas cost whose stack functional specification is equal (modulo commutativity) to the extracted one.We implemented our framework in the tool syrup&nbsp;2.0. Through large-scale experiments on real-world smart contracts, we analyze performance improvements for different SMT encodings, as well as tradeoffs between quality of optimizations and required optimization time.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {70},
numpages = {29},
keywords = {Smart contracts, synthesis, optimization, Max-SMT solvers}
}

@inproceedings{10.1145/3278186.3278189,
author = {Kiss, \'{A}kos and Hodov\'{a}n, Ren\'{a}ta and Gyim\'{o}thy, Tibor},
title = {HDDr: a recursive variant of the hierarchical Delta debugging algorithm},
year = {2018},
isbn = {9781450360531},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278186.3278189},
doi = {10.1145/3278186.3278189},
abstract = {The minimization of failure-inducing test cases is an important first step in the process of bug fixing. It helps focusing the expensive software engineering resources on the root of the problem by pruning down the excess from the input that is not contributing to the failure. Naturally, minimization is most helpful if it is automated. The original minimizing Delta Debugging algorithm and the follow-up Hierarchical Delta Debugging approach have been invented to give a solution to this challenge. Although automated, the minimization of inputs from real-life scenarios can take hours for both approaches. This paper builds on and improves the hierarchical minimization algorithm and experiments with a recursive variant called HDDr. After evaluating HDDr on various test cases, it turns out that it can give minimal results in 29–65% less time than the baseline hierarchical algorithm. On our largest test case, this means that the minimization process gets shorter by more than 4 hours.},
booktitle = {Proceedings of the 9th ACM SIGSOFT International Workshop on Automating TEST Case Design, Selection, and Evaluation},
pages = {16–22},
numpages = {7},
keywords = {hierarchical delta debugging, recursive, test case minimization},
location = {Lake Buena Vista, FL, USA},
series = {A-TEST 2018}
}

@inproceedings{10.1145/3644032.3644467,
author = {Khan, Md Asif and Azim, Akramul and Liscano, Ramiro and Smith, Kevin and Chang, Yee-Kang and Tauseef, Qasim and Seferi, Gkerta},
title = {Machine Learning-based Test Case Prioritization using Hyperparameter Optimization},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644467},
doi = {10.1145/3644032.3644467},
abstract = {Continuous integration pipelines execute extensive automated test suites to validate new software builds. In this fast-paced development environment, delivering timely testing results to developers is critical to ensuring software quality. Test case prioritization (TCP) emerges as a pivotal solution, enabling the prioritization of fault-prone test cases for immediate attention. Recent advancements in machine learning have showcased promising results in TCP, offering the potential to revolutionize how we optimize testing workflows. Hyperparameter tuning plays a crucial role in enhancing the performance of ML models. However, there needs to be more work investigating the effects of hyperparameter tuning on TCP. Therefore, we explore how optimized hyperparameters influence the performance of various ML classifiers, focusing on the Average Percentage of Faults Detected (APFD) metric. Through empirical analysis of ten real-world, large-scale, diverse datasets, we conduct a grid search-based tuning with 885 hyperparameter combinations for four machine learning models. Our results provide model-specific insights and demonstrate an average 15% improvement in model performance with hyperparameter tuning compared to default settings. We further explain how hyperparameter tuning improves precision (max = 1), recall (max = 0.9633), F1-score (max = 0.9662), and influences APFD value (max = 0.9835), indicating a direct connection between tuning and prioritization performance. Hence, this study underscores the importance of hyperparameter tuning in optimizing failure prediction models and their direct impact on prioritization performance.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {125–135},
numpages = {11},
keywords = {hyperparameter optimization, test case prioritization, machine learning, continuous integration},
location = {Lisbon, Portugal},
series = {AST '24}
}

@article{10.1145/3721123,
author = {Gao, Wenzhi and Ge, Dongdong and Ye, Yinyu},
title = {Algorithm xxxx: HDSDP: Software for Semidefinite Programming},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0098-3500},
url = {https://doi.org/10.1145/3721123},
doi = {10.1145/3721123},
abstract = {HDSDP is a numerical software solving semidefinite programming problems. The main framework of HDSDP resembles the dual-scaling interior point solver DSDP[Benson and Ye, 2008] and several new features, including a dual method based on the simplified homogeneous self-dual embedding, have been implemented. The embedding technique enhances the stability of the dual method , and several new heuristics and computational techniques are designed to accelerate its convergence. HDSDP aims to show how the dual-scaling algorithm benefits from the self-dual embedding, and it is developed in parallel to DSDP5.8. Numerical experiments over several classical benchmark datasets exhibit its robustness and efficiency, particularly its advantages on SDP instances featuring low-rank structure and sparsity. HDSDP is open-sourced under an MIT license and available at .},
note = {Just Accepted},
journal = {ACM Trans. Math. Softw.},
month = feb,
keywords = {semidefinite programming, interior point methods, dual-scaling algorithm}
}

@article{10.1007/s00165-021-00566-z,
author = {Luteberget, Bj\o{}rnar and Johansen, Christian},
title = {Drawing with SAT: four methods and A tool for producing railway infrastructure schematics},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {6},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-021-00566-z},
doi = {10.1007/s00165-021-00566-z},
abstract = {Schematic drawings showing railway tracks and equipment are commonly used
to visualize railway operations and to communicate system specifications and
construction blueprints.  Recent advances in on-line collaboration and modeling
tools have raised the expectations for quickly making changes to models,
resulting in frequent changes to layouts, text, and/or symbols in schematic
drawings.  Automating the creation of high-quality schematic views from
geographical and topological models can help engineers produce and update
drawings efficiently. This paper introduces four methods for automatically producing
schematic railway drawings with increasing level of quality and control
over the result.  The final method, implemented in the open-source tool that we have developed,
can use any combination of the following optimization criteria, which can have
different priorities in different use cases: width and height of the drawing,
the diagonal line lengths, and the number of bends. We show how to encode schematic railway drawings  as an optimization
problem over Boolean and numerical domains, using combinations of unary number
encoding, lazy difference constraints, and numerical optimization into an
incremental SAT formulation. We compare drawings resulting from each of the four methods, applied to
models of real-world engineering projects and existing railway infrastructure.
We also show how to add symbols and labels to the track plan, which is
important for the usefulness of the final outputs.  Since the proposed tool is
customizable and efficiently produces high-quality drawings from railML 2.x
models, it can be used (as it is or extended) both as an integrated module in
an industrial design tool like RailCOMPLETE, or by researchers for
visualization purposes.},
journal = {Form. Asp. Comput.},
month = dec,
pages = {829–854},
numpages = {26},
keywords = {SAT, Schematics, Railway, Maps, Railplot, Optimization, Quality}
}

@inproceedings{10.1145/3397271.3401128,
author = {Tzaban, Hen and Guy, Ido and Greenstein-Messica, Asnat and Dagan, Arnon and Rokach, Lior and Shapira, Bracha},
title = {Product Bundle Identification using Semi-Supervised Learning},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401128},
doi = {10.1145/3397271.3401128},
abstract = {Many sellers on e-commerce platforms offer buyers product bundles, which package together two or more different items. The identification of such bundles is a necessary step to support a variety of related services, from recommendation to dynamic pricing. In this work, we present a comprehensive study of bundle identification on a large e-commerce website. Our analysis of bundle compared to non-bundle listed items reveals several key differentiating characteristics, spanning the listing's title, image, and attributes. Following, we experiment with a multi-modal classifier, which takes advantage of these characteristics as features. Our analysis also shows that a bundle indicator input by sellers tends to be highly noisy and carries only a weak signal. The bundle identification task therefore faces the challenge of having a small set of manually-labeled clean examples and a larger set of noisy-labeled examples, in conjunction with class imbalance due to the relative scarcity of bundles.Our experiments with basic supervised classifiers, using the manually-labeled and/or the noisy-labeled data for training, demonstrates only moderate performance. We therefore turn to a semisupervised approach and propose GREED, a self-training ensemblebased algorithm with a greedy model selection. Our evaluation over two different meta-categories shows a superior performance of semi-supervised approaches for the bundle identification task, with GREED outperforming several semi-supervised alternatives. The combination of textual, image, and some metadata features is shown to yield the best performance, reaching an AUC of 0.89 and 0.92 for the two meta-categories, respectively},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {791–800},
numpages = {10},
keywords = {electronic commerce, ensemble learning, product bundling, self-training, semi-supervised learning},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/1982185.1982522,
author = {Mohabbati, Bardia and Hatala, Marek and Ga\v{s}evi\'{c}, Dragan and Asadi, Mohsen and Bo\v{s}kovi\'{c}, Marko},
title = {Development and configuration of service-oriented systems families},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982522},
doi = {10.1145/1982185.1982522},
abstract = {Software Product Lines (SPLs) are families of software systems which share a common sets of feature and are developed through common set of core assets in order to promotes software reusability, mass customization, reducing cost, time-to-market and improving the quality of the product. SPLs are sets (i.e., families) of software applications developed as a whole for a specific business domain. Particular applications are derived from software families by selecting the desired features through configuration process. Traditionally, SPLs are implemented with systematically developed components, shared by members of the SPLs and reused every time a new application is derived. In this paper, we propose an approach to the development and configuration of Service-Oriented SPLs in which services are used as reusable assets and building blocks of implementation. Our proposed approach also suggests prioritization of family features according to stakeholder's non-functional requirements (NFRs) and preferences. Priorities of NFRs are used to filter the most important features of the family, which is performed by Stratified Analytic Hierarchical Process (S-AHP). The priorities also are used further for the selection of appropriate services implementation for business processes realizing features. We apply Mixed Integer Linear Programming to find the optimal service selection within the constraints boundaries specified by stakeholders.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {1606–1613},
numpages = {8},
keywords = {feature-oriented development, optimization, service selection, service-oriented architecture, software product line},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@proceedings{10.1145/3658549,
title = {I-DO '24: Proceedings of the 2024 International Conference on Information Technology, Data Science, and Optimization},
year = {2024},
isbn = {9798400709180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Taipei, Taiwan}
}

@article{10.1145/3716876,
author = {Wang, Xueying and Li, Shigang and Qian, Hao and Luo, Fan and Hao, Zhaoyang and Wu, Tong and Xu, Ruiyuan and Cui, Huimin and Feng, Xiaobing and Li, Guangli and Xue, Jingling},
title = {OptiFX: Automatic Optimization for Convolutional Neural Networks with Aggressive Operator Fusion on GPUs},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1544-3566},
url = {https://doi.org/10.1145/3716876},
doi = {10.1145/3716876},
abstract = {Convolutional Neural Networks (CNNs) are fundamental to advancing computer vision technologies. As CNNs become more complex and larger, optimizing model inference remains a critical challenge in both industry and academia. On modern GPU platforms, CNN operators are typically memory-bound, leading to significant performance degradation due to memory wall effects. While recent advancements have utilized operator fusion—merging multiple operators into one—to enhance inference performance, the fusion of multiple region-based operators like convolution is seldom addressed. This paper introduces AFusion, a novel operator fusion technique aimed at improving inference performance, and OptiFX, an automatic optimization framework based on this approach. OptiFX employs a cost-based backtracking search to identify optimal sub-graphs for fusion and utilizes template-based code generation to create efficient kernels for these fused sub-graphs. We evaluate OptiFX across seven prominent CNN architectures—GoogLeNet, ResNet, DenseNet, MobileNet, SqueezeNet, NasNet, and UNet—on Nvidia A6000 Ada, RTX 4090, and Jetson AGX Orin platforms. Our results demonstrate that OptiFX significantly outperforms existing methods, achieving average speedups of 2.91 \texttimes{}, 3.30 \texttimes{}, and 2.09 \texttimes{} in accelerating inference performance on these platforms, respectively.},
note = {Just Accepted},
journal = {ACM Trans. Archit. Code Optim.},
month = feb,
keywords = {Deep Learning Systems, Convolutional Neural Networks, Operator Fusion}
}

@inproceedings{10.5555/2818754.2818780,
author = {Ben-David, Shoham and Sterin, Baruch and Atlee, Joanne M. and Beidu, Sandy},
title = {Symbolic model checking of product-line requirements using SAT-based methods},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Product line (PL) engineering promotes the development of families of related products, where individual products are differentiated by which optional features they include. Modelling and analyzing requirements models of PLs allows for early detection and correction of requirements errors -- including unintended feature interactions, which are a serious problem in feature-rich systems. A key challenge in analyzing PL requirements is the efficient verification of the product family, given that the number of products is too large to be verified one at a time. Recently, it has been shown how the high-level design of an entire PL, that includes all possible products, can be compactly represented as a single model in the SMV language, and model checked using the NuSMV tool. The implementation in NuSMV uses BDDs, a method that has been outperformed by SAT-based algorithms.In this paper we develop PL model checking using two leading SAT-based symbolic model checking algorithms: IMC and IC3. We describe the algorithms, prove their correctness, and report on our implementation. Evaluating our methods on three PL models from the literature, we demonstrate an improvement of up to 3 orders of magnitude over the existing BDD-based method.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {189–199},
numpages = {11},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/3531437.3539712,
author = {Chung, Sehyeon and Jeong, Jooyeon and Kim, Taewhan},
title = {Improving Performance and Power by Co-Optimizing Middle-of-Line Routing, Pin Pattern Generation, and Contact over Active Gates in Standard Cell Layout Synthesis},
year = {2022},
isbn = {9781450393546},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531437.3539712},
doi = {10.1145/3531437.3539712},
abstract = {This paper addresses the combined problem of the three core tasks, namely routing on the middle-of-line (MOL) layer, generating I/O pin patterns (PP), and allocating contacts over active gates (COAG) in cell layout synthesis with 7nm and below technology. As yet, the existing cell layout generators have paid partial or little attention to those tasks, even with no awareness of the synergistic effects. This work overcomes this limitation by proposing a systematic and tightly-linked solution to the combined problem to boost the synergistic effects on chip implementation. Precisely, we solve the problem in three steps: (1) fully utilizing the horizontal routing resource on MOL layer by formulating the problem of in-cell routing into a weighted interval scheduling problem, (2) simultaneously performing the remaining horizontal in-cell routing and PP generation on metal 1 layer through the COAG exploitation while ensuring the pin accessibility constraint, and (3) completing in-cell routing by allocating vertical routing resource on MOL layer. Through experiments with benchmark designs, it is shown that our proposed layout method is able to generate standard cells with on average 34.2% shorter total length of metal 1 wire while retaining pin patterns that ensure pin accessibility, resulting in the chip implementations with up to 72.5% timing slack improvement and up to 15.6% power reduction that produced by using the conventional best available cells. In addition, by using less wire and vias, our in-cell router is able to consistently reduce the worst delay of cells, noticeably, reducing the sum of setup time and clock-to-Q delay of flip-flops by 1.2% ∼ 3.0% on average over that by the existing best cells.},
booktitle = {Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design},
articleno = {17},
numpages = {6},
location = {Boston, MA, USA},
series = {ISLPED '22}
}

@inproceedings{10.1145/3647444.3647847,
author = {Gupta, Divya and Bansal, Ankit and Kaur, Jaspreet and Wadehra, Sanchit},
title = {Artificial Intelligence empowered content caching for energy optimization in vehicular networks},
year = {2024},
isbn = {9798400709418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3647444.3647847},
doi = {10.1145/3647444.3647847},
abstract = {Indeed, the development of future autonomous and intelligent transportation systems depends heavily on vehicular networks. They allow vehicles to connect with one another, with roadside infrastructure, and with centralized control systems. RSUs can store regularly accessed data including map updates, traffic data, and software updates. Data caching is one of the main responsibilities of Road side Units (RSUs) in Vehicular application as sending the same content repeatedly wastes a lot of system resources. Utilizing caching resources effectively can lower the link load imposed by content transmission, thus enhancing the quality of experience (QoE) for users. However, inefficient collaboration among network routers may lead to substantial energy wastage. To address the aforementioned problem and reduce the requested content energy consumption, this paper proposes artificial intelligence empowered collaborative content caching scheme utilizing the concept of both on path caching and off path caching. Performance analyses show that the proposed technique outperforms other current caching strategies.},
booktitle = {Proceedings of the 5th International Conference on Information Management &amp; Machine Intelligence},
articleno = {21},
numpages = {4},
location = {Jaipur, India},
series = {ICIMMI '23}
}

@inproceedings{10.1145/3663529.3663835,
author = {Pan, Shengyi and Bao, Lingfeng and Zhou, Jiayuan and Hu, Xing and Xia, Xin and Li, Shanping},
title = {Unveil the Mystery of Critical Software Vulnerabilities},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663835},
doi = {10.1145/3663529.3663835},
abstract = {Today’s software industry heavily relies on open source software (OSS). However, the rapidly increasing number of OSS software vulnerabilities (SVs) poses huge security risks to the software supply chain. Managing the SVs in the relied OSS components has become a critical concern for software vendors. Due to the limited resources in practice, an essential focus for the vendors is to locate and prioritize the remediation of critical SVs (CSVs), i.e., those tend to cause huge losses. Particularly, in the software industry, vendors are obliged to comply with the security service level agreement (SLA), which mandates the fix of CSVs within a short time frame (e.g., 15 days). However, to the best of our knowledge, there is no empirical study that specifically investigates CSVs. The existing works only target at general SVs, missing a view of the unique characteristics of CSVs. In this paper, we investigate the distributions (from temporal, type, and repository dimension) and the current remediation practice of CSVs in the OSS ecosystem, especially their differences compared with non-critical SVs (NCSVs). We adopt the industry standard to refer SVs with a 9+ Common Vulnerability Scoring System (CVSS) score as CSVs and others as NCSVs. We collect a large-scale dataset containing 14,867 SVs and artifacts associated with their remediation (e.g., issue report, commit) across 4,462 GitHub repositories. Our findings regarding CSV distributions can help practitioners better locate these hot spots. Regarding the remediation practice, we observe that though CSVs receive higher priorities, some practices (e.g., complicated review and testing pro-cess) may unintentionally cause the delay to their fixes. We also point out the risks of SV information leakage during remediation process, which could leave a window-of-opportunity of over 30 days on median for zero-day attacks. Based on our findings, we provide implications to improve the current CSV remediation practice.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {138–149},
numpages = {12},
keywords = {CVSS, Critical Software Vulnerability, Empirical Study},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/3582935.3582954,
author = {Chen, Haozhe and Gong, Chuanbin and Shi, Jiangzhen},
title = {New Energy License Plate Recognition Based on Image Processing Optimization},
year = {2023},
isbn = {9781450396806},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582935.3582954},
doi = {10.1145/3582935.3582954},
abstract = {Today's social conditions car has become an important tool for modern society, in the face of today's increasingly complex traffic situation, the need for intelligent transportation systems based on license plate recognition is growing. And with the popularity of new energy vehicles, because the new energy number plate than ordinary photo more one and the bottom for gradient color, so the previous ordinary license plate recognition system is no longer applicable. The efficient and accurate recognition of the new energy license plate has become a problem that the license plate recognition system needs to solve now. For this situation we developed a image processing optimization-based intelligent traffic in the license plate recognition design, first through the image processing technology reference, through image processing optimization to the high-definition camera image pre-processing, and image pre-processing includes image grayscale conversion, as well as image switching operations, image filtering, so that the original image to remove the noise and the interference of external conditions. The pre-processed image is then subjected to number plate positioning and plate segmentation, which further solves the problems of locating and segmenting number plate areas in natural backgrounds and plate skewing, showing the powerful advantages of image processing optimization in modern image processing and realising automatic recognition of car number plates.},
booktitle = {Proceedings of the 5th International Conference on Information Technologies and Electrical Engineering},
pages = {103–109},
numpages = {7},
keywords = {Image processing optimization, New energy license plate recognition, Signal processing, Template matching},
location = {Changsha, China},
series = {ICITEE '22}
}

@inproceedings{10.1145/3701625.3701644,
author = {S. Lisboa de Andrade, Anielle and Luz de Quadros, Everton Luis and Oliveira Fortes, Luciane and Prikladnicki, Rafael},
title = {Developing a Collaborative Recommendations Guide for Hybrid Software Development: A Focus Group Study},
year = {2024},
isbn = {9798400717772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701625.3701644},
doi = {10.1145/3701625.3701644},
abstract = {In the middle of the emergence of the COVID-19 pandemic, hybrid software development teams have notably gained prominence. However, coordinating these teams unveils many challenges, mainly around effective communication, the setup of a good working environment, and cooperative recommendations. To contribute to this field, we carried out a study to identify recommendations for managing hybrid software development teams within the IT department of a Brazilian multinational company. The study was in an industry-academia collaboration and was structured into three stages: (1) a multivocal literature review, (2) a case study containing discursive textual analysis through semi-structured interviews and non-participant observation (previously published), and (3) a focus group for the formulation of a collaborative recommendations guide - the stage we will present in this paper. We recruited 11 software engineers from eight software teams for the focus group. The study resulted in 23 recommendations categorized into three dimensions: individuals, processes, and spaces. We believe the recommendations can contribute to the growing knowledge of managing hybrid teams. The resultant guide also contributes to disseminating hybrid work strategies to the broader community, sharing insights for improving hybrid work in software teams and taking social and human aspects of work from anywhere in software development.},
booktitle = {Proceedings of the XXIII Brazilian Symposium on Software Quality},
pages = {39–48},
numpages = {10},
keywords = {Collaborative recommendations, Communication, Hybrid software development, Team management, Working environment.},
location = {
},
series = {SBQS '24}
}

@inproceedings{10.1109/CGO57630.2024.10444884,
author = {Li, Haofeng and Lu, Jie and Meng, Haining and Cao, Liqing and Li, Lian and Gao, Lin},
title = {Boosting the Performance of Multi-Solver IFDS Algorithms with Flow-Sensitivity Optimizations},
year = {2024},
isbn = {9798350395099},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CGO57630.2024.10444884},
doi = {10.1109/CGO57630.2024.10444884},
abstract = {The IFDS (Inter-procedural, Finite, Distributive, Subset) algorithms are popularly used to solve a wide range of analysis problems. In particular, many interesting problems are formulated as multi-solver IFDS problems which expect multiple interleaved IFDS solvers to work together. For instance, taint analysis requires two IFDS solvers, one forward solver to propagate tainted data-flow facts, and one backward solver to solve alias relations at the same time. For such problems, large amount of additional data-flow facts need to be introduced for flow-sensitivity. This often leads to poor performance and scalability, as evident in our experiments and previous work. In this paper, we propose a novel approach to reduce the number of introduced additional data-flow facts while preserving flow-sensitivity and soundness.We have developed a new taint analysis tool, SADroid, and evaluated it on 1,228 open-source Android APPs. Evaluation results show that SADroid significantly outperforms FlowDroid (the state-of-the-art multi-solver IFDS taint analysis tool) without affecting precision and soundness: the run time performance is sped up by up to 17.89X and memory usage is optimized by up to 9X.},
booktitle = {Proceedings of the 2024 IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {296–307},
numpages = {12},
keywords = {multi-solver IFDS},
location = {Edinburgh, United Kingdom},
series = {CGO '24}
}

@inproceedings{10.1145/3674805.3686678,
author = {Weber, Thomas and Winiker, Christina and Mayer, Sven},
title = {An Investigation of How Software Developers Read Machine Learning Code},
year = {2024},
isbn = {9798400710476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674805.3686678},
doi = {10.1145/3674805.3686678},
abstract = {Background&nbsp;Machine Learning plays an ever-growing role in everyday software. This means a paradigmatic shift in how software operators from algorithm-centered software where the developers defines the functionality to data-driven development where behavior is inferred from data. Aims&nbsp;The goal of our research is to determine how this paradigmatic shift materializes in the written code and whether developers are aware of these changes and how they affect their behavior. Method&nbsp;To this end, we perform static analysis of 3,515 software repositories to determine structural differences in the code. Following this, we conducted a user study using eye tracking (N=18) to determine how the code reading of developers differs when reading Machine Learning source code versus traditional code. Results&nbsp;The results show that there are structural differences in the code of this paradigmatically different software. Developers appear to adapt their mental models with growing experience resulting in distinctly different reading patterns. Conclusions&nbsp;These difference highlight that we cannot treat all code the same but require paradigm-specific, empirically validated support mechanisms to help developers write high-quality code.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {165–176},
numpages = {12},
keywords = {code reading, eye tracking, human computer interaction, software developers},
location = {Barcelona, Spain},
series = {ESEM '24}
}

@article{10.1145/3443707,
author = {Biswas, Arnab Kumar},
title = {Cryptographic Software IP Protection without Compromising Performance or Timing Side-channel Leakage},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/3443707},
doi = {10.1145/3443707},
abstract = {Program obfuscation is a widely used cryptographic software intellectual property (IP) protection technique against reverse engineering attacks in embedded systems. However, very few works have studied the impact of combining various obfuscation techniques on the obscurity (difficulty of reverse engineering) and performance (execution time) of obfuscated programs. In this article, we propose a Genetic Algorithm&nbsp;(GA)-based framework that not only optimizes obscurity and performance of obfuscated cryptographic programs, but it also ensures very low timing side-channel leakage. Our proposed Timing Side Channel Sensitive Program Obfuscation Optimization Framework (TSC-SPOOF) determines the combination of obfuscation transformation functions that produce optimized obfuscated programs with preferred optimization parameters. In particular, TSC-SPOOF employs normalized compression distance (NCD) and channel capacity to measure obscurity and timing side-channel leakage, respectively. We also use RISC-V rocket core running on a Xilinx Zynq FPGA device as part of our framework to obtain realistic results. The experimental results clearly show that our proposed solution leads to cryptographic programs with lower execution time, higher obscurity, and lower timing side-channel leakage than unguided obfuscation.},
journal = {ACM Trans. Archit. Code Optim.},
month = feb,
articleno = {20},
numpages = {20},
keywords = {FPGA, LLVM, NCD, Obfuscation, channel capacity, optimization, timing side-channel}
}

@article{10.5555/1181901.1181950,
author = {Hunt, John M. and McGregor, John D.},
title = {Software product lines: a pedagogical application},
year = {2006},
issue_date = {December 2006},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {22},
number = {2},
issn = {1937-4771},
abstract = {This paper provides an overview of Software Product Lines and discusses issues involved in using Software Product Lines in courses. An SPL designs and produces common assets for a group of related products as a family; rather then building the products one at a time. SPL has been successful in delivering 80% to 100% reuse. SPL is moving out of an early adaptor phase and into mainstream reuse, which should increase industry demand for developers familiar with SPL. Understanding SPL is best done with a complete example. We discuss a complete set of publicly available SPL related assets that we developed, and discuss their use in the classroom.},
journal = {J. Comput. Sci. Coll.},
month = dec,
pages = {295–302},
numpages = {8}
}

@article{10.1145/3702975,
author = {de Lara, Juan and Guerra, Esther},
title = {Adaptive Modelling Languages: Abstract Syntax and Model Migration},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3702975},
doi = {10.1145/3702975},
abstract = {Modelling languages are heavily used in many disciplines, including software engineering. However, current languages are rigid, since they do not get adapted to fit the users’ expertise, the modelling task or the usage platform. This may turn some languages unsuitable for a range of users (from unexperienced to experts), goals (from informal discussion to precise specification) and platforms (from desktops to mobile phones). We claim that making languages adaptive to the modelling scenario would alleviate these issues and help simplifying recurring tasks such as language evolution or inter-operability between the languages of a family.In this article, we propose the new notion of adaptive modelling language. This concept combines meta-modelling and product lines to support variants of a given language, and encompasses contextual conditions triggering language reconfigurations, and mechanisms for model migration across the language variants. The article presents a theory and its realisation atop the Eclipse Modelling Framework. Our tool includes an Eclipse workbench to specify adaptive languages and produce Eclipse modelling editors with adaptation support. We report on an evaluation demonstrating the advantages of using our framework to express migrations across the variants of adaptive languages, which moreover have generally fast execution times.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {66},
numpages = {54},
keywords = {modelling language engineering, flexible modelling, model transformation, graph transformation, model migration, software product lines}
}

@inproceedings{10.1145/3597503.3639100,
author = {Jiang, Ling and An, Junwen and Huang, Huihui and Tang, Qiyi and Nie, Sen and Wu, Shi and Zhang, Yuqun},
title = {BinaryAI: Binary Software Composition Analysis via Intelligent Binary Source Code Matching},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639100},
doi = {10.1145/3597503.3639100},
abstract = {While third-party libraries (TPLs) are extensively reused to enhance productivity during software development, they can also introduce potential security risks such as vulnerability propagation. Software composition analysis (SCA), proposed to identify reused TPLs for reducing such risks, has become an essential procedure within modern DevSecOps. As one of the mainstream SCA techniques, binary-to-source SCA identifies the third-party source projects contained in binary files via binary source code matching, which is a major challenge in reverse engineering since binary and source code exhibit substantial disparities after compilation. The existing binary-to-source SCA techniques leverage basic syntactic features that suffer from redundancy and lack robustness in the large-scale TPL dataset, leading to inevitable false positives and compromised recall. To mitigate these limitations, we introduce BinaryAI, a novel binary-to-source SCA technique with two-phase binary source code matching to capture both syntactic and semantic code features. First, BinaryAI trains a transformer-based model to produce function-level embeddings and obtain similar source functions for each binary function accordingly. Then by applying the link-time locality to facilitate function matching, BinaryAI detects the reused TPLs based on the ratio of matched source functions. Our experimental results demonstrate the superior performance of BinaryAI in terms of binary source code matching and the downstream SCA task. Specifically, our embedding model outperforms the state-of-the-art model CodeCMR, i.e., achieving 22.54% recall@1 and 0.34 MRR compared with 10.75% and 0.17 respectively. Additionally, BinaryAI outperforms all existing binary-to-source SCA tools in TPL detection, increasing the precision from 73.36% to 85.84% and recall from 59.81% to 64.98% compared with the well-recognized commercial SCA product Black Duck.https://www.binaryai.net},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {224},
numpages = {13},
keywords = {software composition analysis, static binary analysis},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ESEM.2017.39,
author = {Verdecchia, Roberto and Procaccianti, Giuseppe and Malavolta, Ivano and Lago, Patricia and Koedijk, Joost},
title = {Estimating energy impact of software releases and deployment strategies: the KPMG case study},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.39},
doi = {10.1109/ESEM.2017.39},
abstract = {Background. Often motivated by optimization objectives, software products are characterized by different subsequent releases and deployed through different strategies. The impact of these two aspects of software on energy consumption has still to be completely understood and can be improved by carrying out ad-hoc analyses for specific software products.Aims. In this research we report on an industrial collaboration aiming at assessing the different impact that releases and deployment strategies of a software product can have on the energy consumption of its underlying hardware infrastructure.Method. We designed and performed an empirical experiment in a controlled environment. Deployment strategies, releases and use case scenarios of an industrial third-party software product were adopted as experimental factors. The use case scenarios were used as a blocking factor and adopted to dynamically load-test the software product. Power consumption and execution time were selected as response variables to measure the energy consumption.Results. We observed that both deployment strategies and software releases significantly influence the energy consumption of the hardware infrastructure. A strong interaction between the two factors was identified. The impact of such interaction highly varied depending on which use case scenario was considered, making the identification of the most frequently adopted use case scenario critical for energy optimisation. The collaboration between industry and academia has been productive for both parties, even if some practitioners manifested low interest/awareness on software energy efficiency.Conclusions. For the software product considered there is no absolute preferable release or deployment strategy with respect to energy efficiency, as the interaction of these factors has to be considered. The number of machines involved in a software deployment strategy does not simply constitute an additive effect of the energy consumption of the underlying hardware infrastructure.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {257–266},
numpages = {10},
keywords = {deployment, energy, software releases},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@inproceedings{10.1145/3368089.3409675,
author = {Siegmund, Norbert and Ruckel, Nicolai and Siegmund, Janet},
title = {Dimensions of software configuration: on the configuration context in modern software development},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409675},
doi = {10.1145/3368089.3409675},
abstract = {With the rise of containerization, cloud development, and continuous integration and delivery, configuration has become an essential aspect not only to tailor software to user requirements, but also to configure a software system’s environment and infrastructure. This heterogeneity of activities, domains, and processes blurs the term configuration, as it is not clear anymore what tasks, artifacts, or stakeholders are involved and intertwined. However, each re- search study and each paper involving configuration places their contributions and findings in a certain context without making the context explicit. This makes it difficult to compare findings, translate them to practice, and to generalize the results. Thus, we set out to evaluate whether these different views on configuration are really distinct or can be summarized under a common umbrella. By interviewing practitioners from different domains and in different roles about the aspects of configuration and by analyzing two qualitative studies in similar areas, we derive a model of configuration that provides terminology and context for research studies, identifies new research opportunities, and allows practitioners to spot possible challenges in their current tasks. Although our interviewees have a clear view about configuration, it substantially differs due to their personal experience and role. This indicates that the term configuration might be overloaded. However, when taking a closer look, we see the interconnections and dependencies among all views, arriving at the conclusion that we need to start considering the entire spectrum of dimensions of configuration.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {338–349},
numpages = {12},
keywords = {Dimensions of software configuration, configuration management and life cycle, developer study, variability},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@proceedings{10.1145/3689904,
title = {EAAMO '24: Proceedings of the 4th ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization},
year = {2024},
isbn = {9798400712227},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Luis Potosi, Mexico}
}

@inproceedings{10.1145/3617651.3622982,
author = {Chevalier-Boisvert, Maxime and Kokubun, Takashi and Gibbs, Noah and Wu, Si Xing (Alan) and Patterson, Aaron and Issroff, Jemma},
title = {Evaluating YJIT’s Performance in a Production Context: A Pragmatic Approach},
year = {2023},
isbn = {9798400703805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617651.3622982},
doi = {10.1145/3617651.3622982},
abstract = {Ruby is a dynamically-typed programming language with a large breadth of features which  
has grown in popularity with the rise of the modern web, and remains at the core of the  
implementation of widely-used online platforms such as Shopify, GitHub, Discourse,  
and Mastodon.  

There have been many attempts to integrate JIT compilation into Ruby implementations, but until recently, despite impressive  
performance on benchmarks, none had seen widespread adoption.  
This has changed with the arrival of YJIT, a new JIT compiler based on a Lazy Basic Block Versioning (LBBV) architecture which has recently been upstreamed into CRuby,  
and has since seen multiple large-scale production deployments.  

This paper extends on previous work on YJIT and takes a pragmatic approach towards evaluating YJIT's performance in a production context. We evaluate and compare its performance on benchmarks as well as a large-scale real-world production deployment, and we look not only at peak performance, but also at memory usage and warm-up time.  

On all of our benchmarks, YJIT is able to consistently outperform the CRuby interpreter by a wide  
margin. It offers consistent speedups, full compatibility with existing Ruby code, much less memory overhead and faster warm-up compared to JRuby and TruffleRuby.  
We also show that YJIT is able to deliver significant speedups on a real-world deployment on Shopify's worldwide StoreFront Renderer infrastructure, an application for which it is currently the only viable JIT compiler.},
booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Managed Programming Languages and Runtimes},
pages = {20–33},
numpages = {14},
keywords = {bytecode, compiler, dynamically typed, just-in-time, optimization, ruby, virtual machine},
location = {Cascais, Portugal},
series = {MPLR 2023}
}

@inproceedings{10.1145/3643690.3648245,
author = {Van Schothorst, Casper and Schuurmans, Robbert and Jansen, Slinger},
title = {Software Ecosystem Orchestration with Topic Modeling},
year = {2024},
isbn = {9798400705717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643690.3648245},
doi = {10.1145/3643690.3648245},
abstract = {Software ecosystems are typically networks of organizations collaboratively serving a market for software around a technical platform. Software ecosystem orchestrators are responsible for the health of these networks, for instance by attracting new applications to application stores and by opening new parts of the platform for extension. Considering the amounts of application data that orchestrators have to deal with, we propose an approach for strategic analysis of the software ecosystem that uses topic modeling. With our approach, orchestrators are supported in their task of growing and pruning their platform software ecosystem. We illustrate the use of the approach in two case studies of rapidly expanding and competing software platform ecosystems.},
booktitle = {Proceedings of the 7th ACM/IEEE International Workshop on Software-Intensive Business},
pages = {72–78},
numpages = {7},
keywords = {software ecosystem health, domain analysis, repository mining, topic modelling applications, app store mining},
location = {Lisbon, Portugal},
series = {IWSiB '24}
}

@article{10.1145/3701986,
author = {Bo\"{e}zennec, Robin and Dufoss\'{e}, Fanny and Pallez, Guillaume},
title = {Qualitatively Analyzing Optimization Objectives in the Design of HPC Resource Manager},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {2376-3639},
url = {https://doi.org/10.1145/3701986},
doi = {10.1145/3701986},
abstract = {A correct evaluation of scheduling algorithms and a good understanding of their optimization criteria are key components of resource management in HPC. In this work, we discuss bias and limitations of the most frequent optimization metrics from the literature. We provide elements on how to evaluate performance when studying HPC batch scheduling.We experimentally demonstrate these limitations by focusing on two use-cases: a study on the impact of runtime estimates on scheduling performance, and the reproduction of a recent high-impact work that designed an HPC batch scheduler based on a network trained with reinforcement learning. We demonstrate that focusing on quantitative optimization criterion (“our work improves the literature by X%”) may hide extremely important caveat, to the point that the results obtained are opposed to the actual goals of the authors.Key findings show that mean bounded slowdown and mean response time are hazardous for a purely quantitative analysis in the context of HPC. Despite some limitations, utilization appears to be a good objective. We propose to complement it with the standard deviation of the throughput in some pathological cases. Finally, we argue for a larger use of area-weighted response time, that we find to be a very relevant objective.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = dec,
articleno = {15},
numpages = {28},
keywords = {State of the practice, high performance computing, runtime estimates, mean bounded slowdown, response time, utilization}
}

@article{10.1145/2532780.2532789,
author = {Graziotin, Daniel and Jedlitschka, Andreas},
title = {Recent developments in product-focused software process improvement: PROFES 2013 conference report},
year = {2013},
issue_date = {November 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/2532780.2532789},
doi = {10.1145/2532780.2532789},
abstract = {This report summarizes the presentations and discussions that happened at PROFES 2013, the 14th International Conference on Product- Focused Software Process Improvement, which was held June 12-14, 2013 in Paphos, Cyprus. The main theme of PROFES is software process improvement (SPI) motivated by product, process, and service quality needs. PROFES 2013 addressed both quality engineering and management topics, divided into the areas of Decision Support in Software Engineering, Empirical Software Engineering, Managing Software Processes, Safety-Critical Software Engineering, Software Measurement, Software Process Improvement, and Software Maintenance.},
journal = {SIGSOFT Softw. Eng. Notes},
month = nov,
pages = {29–34},
numpages = {6},
keywords = {experimentation, human factors, management, measurement, performance, security, standardization}
}

@inproceedings{10.1145/3643991.3644919,
author = {Le, Triet Huynh Minh and Du, Xiaoning and Babar, M. Ali},
title = {Are Latent Vulnerabilities Hidden Gems for Software Vulnerability Prediction? An Empirical Study},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644919},
doi = {10.1145/3643991.3644919},
abstract = {Collecting relevant and high-quality data is integral to the development of effective Software Vulnerability (SV) prediction models. Most of the current SV datasets rely on SV-fixing commits to extract vulnerable functions and lines. However, none of these datasets have considered latent SVs existing between the introduction and fix of the collected SVs. There is also little known about the usefulness of these latent SVs for SV prediction. To bridge these gaps, we conduct a large-scale study on the latent vulnerable functions in two commonly used SV datasets and their utilization for function-level and line-level SV predictions. Leveraging the state-of-the-art SZZ algorithm, we identify more than 100k latent vulnerable functions in the studied datasets. We find that these latent functions can increase the number of SVs by 4\texttimes{} on average and correct up to 5k mislabeled functions, yet they have a noise level of around 6%. Despite the noise, we show that the state-of-the-art SV prediction model can significantly benefit from such latent SVs. The improvements are up to 24.5% in the performance (F1-Score) of function-level SV predictions and up to 67% in the effectiveness of localizing vulnerable lines. Overall, our study presents the first promising step toward the use of latent SVs to improve the quality of SV datasets and enhance the performance of SV prediction tasks.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {716–727},
numpages = {12},
keywords = {software vulnerability, software security, deep learning, data quality, SZZ algorithm},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3624062.3624197,
author = {Afzal, Ayesha and Hager, Georg and Wellein, Gerhard},
title = {SPEChpc 2021 Benchmarks on Ice Lake and Sapphire Rapids Infiniband Clusters: A Performance and Energy Case Study},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624197},
doi = {10.1145/3624062.3624197},
abstract = {In this work, fundamental performance, power, and energy characteristics of the full SPEChpc 2021 benchmark suite are assessed on two different clusters based on Intel Ice Lake and Sapphire Rapids CPUs using the MPI-only codes’ variants. We use memory bandwidth, data volume, and scalability metrics in order to categorize the benchmarks and pinpoint relevant performance and scalability bottlenecks on the node and cluster levels. Common patterns such as memory bandwidth limitation, dominating communication and synchronization overhead, MPI serialization, superlinear scaling, and alignment issues could be identified, in isolation or in combination, showing that SPEChpc 2021 is representative of many HPC workloads. Power dissipation and energy measurements indicate that the modern Intel server CPUs have such a high idle power level that race-to-idle is the paramount strategy for energy to solution and energy-delay product minimization. On the chip level, only memory-bound code shows a clear advantage of Sapphire Rapids compared to Ice Lake in terms of energy to solution.},
booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {1245–1254},
numpages = {10},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@inproceedings{10.1109/SC41406.2024.00026,
author = {Liu, Xiaoyan and Yang, Xinyu and Ma, Kejie and Liu, Shanghao and Zhang, Kaige and Yang, Hailong and Liu, Yi and Luan, Zhongzhi and Qian, Depei},
title = {Moirae: Generating High-Performance Composite Stencil Programs with Global Optimizations},
year = {2024},
isbn = {9798350352917},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC41406.2024.00026},
doi = {10.1109/SC41406.2024.00026},
abstract = {Stencil computation is one of the most universal computation motifs in scientific applications such as weather prediction. Due to the complexity of scientific simulation, the stencil computation can contain a set of complex stencil operations that form a directed acyclic graph (referred to composite stencil). Unfortunately, most existing stencil optimizations and compilers only focus on intra-stencil operation, and cannot fully explore the performance improvement potential of composite stencils in nowadays applications. To this end, we propose Moirae, a framework that explores a novel optimization space and generates high-performance code for composite stencils. We first propose a lightweight cost model with a fine-grained analysis of memory access behavior to predict the performance. Based on the cost model, we propose an evolutionary search method to find a high-performance optimization, leveraging a search space pruning method with stencil domain knowledge. Experimental results show that Moirae can outperform the state-of-the-art composite stencil compilers.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {20},
numpages = {15},
location = {Atlanta, GA, USA},
series = {SC '24}
}

@inproceedings{10.1145/3663529.3663849,
author = {Hassan, Ahmed E. and Lin, Dayi and Rajbahadur, Gopi Krishnan and Gallaba, Keheliya and Cogo, Filipe Roseiro and Chen, Boyuan and Zhang, Haoxiang and Thangarajah, Kishanthan and Oliva, Gustavo and Lin, Jiahuei (Justina) and Abdullah, Wali Mohammad and Jiang, Zhen Ming (Jack)},
title = {Rethinking Software Engineering in the Era of Foundation Models: A Curated Catalogue of Challenges in the Development of Trustworthy FMware},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663849},
doi = {10.1145/3663529.3663849},
abstract = {Foundation models (FMs), such as Large Language Models (LLMs), have revolutionized software development by enabling new use cases and business models. We refer to software built using FMs as FMware. The unique properties of FMware (e.g., prompts, agents and the need for orchestration), coupled with the intrinsic limitations of FMs (e.g., hallucination) lead to a completely new set of software engineering challenges. Based on our industrial experience, we identified ten key SE4FMware challenges that have caused enterprise FMware development to be unproductive, costly, and risky. For each of those challenges, we state the path for innovation that we envision. We hope that the disclosure of the challenges will not only raise awareness but also promote deeper and further discussions, knowledge sharing, and innovative solutions.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {294–305},
numpages = {12},
keywords = {AIware, FMware, Foundation models, Large Language Models},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/3650212.3680388,
author = {Xue, Zhiyi and Li, Liangguo and Tian, Senyue and Chen, Xiaohong and Li, Pingping and Chen, Liangyu and Jiang, Tingting and Zhang, Min},
title = {LLM4Fin: Fully Automating LLM-Powered Test Case Generation for FinTech Software Acceptance Testing},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680388},
doi = {10.1145/3650212.3680388},
abstract = {FinTech software, crucial for both safety and timely market deployment, presents a compelling case for automated acceptance testing against regulatory business rules. However, the inherent challenges of comprehending unstructured natural language descriptions of these rules and crafting comprehensive test cases demand human intelligence. The emergence of Large Language Models (LLMs) holds promise for automated test case generation, leveraging their natural language processing capabilities. Yet, their dependence on human intervention for effective prompting hampers efficiency.    In response, we introduce a groundbreaking, fully automated approach for generating high-coverage test cases from natural language business rules. Our methodology seamlessly integrates the versatility of LLMs with the predictability of algorithmic methods. We fine-tune pre-trained LLMs for improved information extraction accuracy and algorithmically generate comprehensive testable scenarios for the extracted business rules.	Our prototype, LLM4Fin, is designed for testing real-world stock-trading software. Experimental results demonstrate LLM4Fin’s superiority over both state-of-the-art LLM, such as ChatGPT, and skilled testing engineers. We achieve remarkable performance, with up to 98.18% and an average of 20%−110% improvement on business scenario coverage, and up to 93.72% on code coverage, while reducing the time cost from 20 minutes to a mere 7 seconds. These results provide robust evidence of the framework’s practical applicability and efficiency, marking a significant advancement in FinTech software testing.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1643–1655},
numpages = {13},
keywords = {Software acceptance testing, fintech software, large language model, test case generation},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3689031.3696093,
author = {Schwyn, Daniel and Liu, Zikai and Roscoe, Timothy},
title = {Efeu: generating efficient, verified, hybrid hardware/software drivers for I2C devices},
year = {2025},
isbn = {9798400711961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689031.3696093},
doi = {10.1145/3689031.3696093},
abstract = {Writing device drivers is notoriously hard, and driver bugs are a major cause of system failures and vulnerabilities. The problem is particularly acute in bus-based protocols like I2C, where driver correctness is only half the story: correct functioning of the complete subsystem depends on all components on the bus interoperating correctly. Unfortunately, developers cannot control all aspects of a platform, and must interact with existing devices (peripherals and/or hardware bus controllers) which may misbehave. Failures in a protocol like I2C, often used in critical low-level system management, can result in permanent damage to the hardware, whether a server or a satellite.Existing techniques for creating high assurance drivers rarely tackle this interoperability issue. We present Efeu, a framework for implementing verifiably interoperable drivers for I2C devices. Using model checking-based verification, Efeu generates driver implementations in software, reconfigurable logic for FPGAs, and, notably, combinations of both. The split between software and hardware can be varied at implementation time and the hardware/software interface is generated automatically, enabling efficient exploration of the design space. Using Efeu, we design and evaluate a verified I2C driver stack, and demonstrate that Efeu finds optimal hardware/software tradeoffs to favor either throughput, CPU usage or FPGA footprint. For each objective, Efeu generates drivers with performance comparable with hand-optimized hardware/software drivers.},
booktitle = {Proceedings of the Twentieth European Conference on Computer Systems},
pages = {76–93},
numpages = {18},
keywords = {I2C, interoperability, verified drivers},
location = {Rotterdam, Netherlands},
series = {EuroSys '25}
}

@inproceedings{10.1145/3699538.3699541,
author = {Korpimies, Kai and Laaksonen, Antti and Luukkainen, Matti},
title = {Unrestricted Use of LLMs in a Software Project Course: Student Perceptions on Learning and Impact on Course Performance},
year = {2024},
isbn = {9798400710384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3699538.3699541},
doi = {10.1145/3699538.3699541},
abstract = {Large language models (LLMs) provide round-the-clock personalized programming assistance, unlike course instructors or traditional online information sources such as Stack Overflow. While LLMs can aid in code generation, concerns about over-reliance and the impact on learning persist. This study discusses students’ experiences with LLMs in a software project course where students were allowed to use LLMs freely except for unit test generation. We conducted surveys during course instances in autumn 2023 and spring 2024. The surveys assessed the extent of LLM usage, methods of application, and perceived impact on learning. Results indicate diverse usage patterns, with many students finding LLMs beneficial for efficiency and problem-solving, though over-reliance and poor-quality outputs were noted concerns. The usage patterns can be linked to course performance and time spent on the project.},
booktitle = {Proceedings of the 24th Koli Calling International Conference on Computing Education Research},
articleno = {23},
numpages = {7},
keywords = {Large language models, Computer Science Education, User Study, Code generation, Software project},
location = {
},
series = {Koli Calling '24}
}

@inproceedings{10.1145/3665314.3670800,
author = {Jin, Seunghyun and Lee, Hyunwuk and Ro, Won Woo},
title = {GUMSO: Gating Unnecessary On-Chip Memory Slices for Power Optimization on GPUs},
year = {2024},
isbn = {9798400706882},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665314.3670800},
doi = {10.1145/3665314.3670800},
abstract = {The importance of power efficiency in GPUs has grown significantly for data centers, as it directly impacts costs and sustainability. While there have been many works on power optimization for GPUs, their approaches often target applications consuming high parallelism for their entire application sequence. However, graph applications, characterized by their large data size and inherent irregularity in graph structures, present distinct challenges as they suffer from kernel launches with limited parallelism, resulting in under-utilization of GPU hardware components. Even with this under-utilization, last-level cache (LLC) and network-on-chip (NoC) of the GPU are fully activated even with a single thread running, incurring large leakage power. In our analysis, we find out that LLC and NoC contribute up to 39.9% of the total GPU power consumption during graph applications. To address this power inefficiency, we propose GUMSO, an energy-efficient design that enables adaptive power-gating of LLC slices for small kernel executions in GPUs. By managing the utilization of LLC slices, our approach reduces GPU energy consumption by an average of 18.3% across various graph applications with minimum performance overheads.},
booktitle = {Proceedings of the 29th ACM/IEEE International Symposium on Low Power Electronics and Design},
pages = {1–6},
numpages = {6},
keywords = {GPU, graph processing, LLC, power-gating, power optimization},
location = {Newport Beach, CA, USA},
series = {ISLPED '24}
}

@inproceedings{10.1145/2491411.2491445,
author = {Greenyer, Joel and Brenner, Christian and Cordy, Maxime and Heymans, Patrick and Gressi, Erika},
title = {Incrementally synthesizing controllers from scenario-based product line specifications},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491445},
doi = {10.1145/2491411.2491445},
abstract = {Many software-intensive systems consist of components that interact to fulfill complex functionality. Moreover, often many variants of such systems have to be designed at once. This adds complexity to the design task. Recently, we proposed a scenario-based approach to design product lines, which combines feature diagrams and Modal Sequence Diagrams. We proposed a consistency-checking technique based on a dedicated product line model checker. One limitation of this technique is that it is incomplete, i.e., it may fail to show the consistency of some consistent specifications. In this paper we propose a new game-based approach that overcomes this incompleteness and, in addition, automatically synthesizes controllers for the consistent product specifications. We exploit the fact that many variants are similar and efficiently synthesize product controllers incrementally. We provide a prototype tool and evaluate the efficiency of the approach.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {433–443},
numpages = {11},
keywords = {Controller synthesis, Features, Message sequence diagrams},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@article{10.1145/3660766,
author = {Beyer, Dirk and Kettl, Matthias and Lemberger, Thomas},
title = {Decomposing Software Verification using Distributed Summary Synthesis},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660766},
doi = {10.1145/3660766},
abstract = {There are many approaches for automated software verification, but they are either imprecise, do not scale
 
well to large systems, or do not sufficiently leverage parallelization. This hinders the integration of software
 
model checking into the development process (continuous integration). We propose an approach to decompose
 
one large verification task into multiple smaller, connected verification tasks, based on blocks in the program
 
control flow. For each block, summaries (block contracts) are computed — based on independent, distributed,
 
continuous refinement by communication between the blocks. The approach iteratively synthesizes preconditions
 
to assume at the block entry (computed from postconditions received from block predecessors, i.e., which
 
program states reach this block) and violation conditions to check at the block exit (computed from violation
 
conditions received from block successors, i.e., which program states lead to a specification violation). This
 
separation of concerns leads to an architecture in which all blocks can be analyzed in parallel, as independent
 
verification problems. Whenever new information (as a postcondition or violation condition) is available from
 
other blocks, the verification can decide to restart with this new information. We realize our approach on
 
the basis of configurable program analysis and implement it for the verification of C programs in the widely
 
used verifier CPAchecker. A large experimental evaluation shows the potential of our new approach: The
 
distribution of the workload to several processing units works well, and there is a significant reduction of the
 
response time when using multiple processing units. There are even cases in which the new approach beats
 
the highly-tuned, existing single-threaded predicate abstraction.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {59},
numpages = {23},
keywords = {Block Summaries, Contract Synthesis, Decomposition Strategies, Formal Verification, Multi-process Model Checking, Parallelization, Program Analysis, Software Model Checking}
}

@inproceedings{10.1145/3674805.3695404,
author = {Nguyen, Thanh and Baldassarre, Maria Teresa and de Lima, Luiz Fernando and de Souza Santos, Ronnie},
title = {From Literature to Practice: Exploring Fairness Testing Tools for the Software Industry Adoption},
year = {2024},
isbn = {9798400710476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674805.3695404},
doi = {10.1145/3674805.3695404},
abstract = {Context: The increasing integration of artificial intelligence and machine learning into software systems has highlighted the critical importance of ensuring fairness in these technologies. Bias in software can lead to inequitable outcomes, making fairness testing essential. However, the current landscape of fairness testing tools remains underexplored, particularly regarding their practical applicability and usability for software development practitioners. Goal: This study aimed to evaluate the practical applicability of existing fairness testing tools for software development practitioners, assessing their usability, documentation, and overall effectiveness in real-world industry settings. Method: We identified 41 fairness testing tools from the literature and conducted a heuristic evaluation and documentary analysis of their installation processes, user interfaces, supporting documentation, and update frequencies. Technical analysis included assessing configurability for diverse datasets. The analysis focused on identifying strengths and deficiencies to determine their suitability for industry use. Findings: Our findings revealed that most fairness testing tools show significant deficiencies, particularly in user-friendliness, detailed documentation, and configurability. These limitations restrict their practical use in industry settings. The tools also lack regular updates and possess a narrow focus on specific datasets, which constrains their versatility and scalability. Despite some strengths, such as cost-effectiveness and compatibility with several environments, the overall landscape of fairness testing tools requires substantial improvements to meet industry needs. Conclusion: There is a pressing need to develop fairness testing tools that align more closely with industry requirements, offering enhanced usability, comprehensive documentation, and greater configurability to effectively support software development practitioners. LAY ABSTRACT. In today’s world, we need to ensure that AI systems are fair and unbiased. Our study looked at tools designed to test the fairness of software to see if they are practical and easy for software developers to use. We found that while some tools are cost-effective and compatible with various programming environments, many are hard to use and lack detailed instructions. They also tend to focus on specific types of data, which limits their usefulness in real-world situations. Overall, current fairness testing tools need significant improvements to better support software developers in creating fair and equitable technology. We suggest that new tools should be user-friendly, well-documented, and flexible enough to handle different kinds of data, helping developers identify and fix biases early in the development process. This will lead to more trustworthy and fair software for everyone.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {549–555},
numpages = {7},
keywords = {fairness testing, software fairness, software testing},
location = {Barcelona, Spain},
series = {ESEM '24}
}

@article{10.1145/3680470,
author = {Rajput, Saurabhsingh and Widmayer, Tim and Shang, Ziyuan and Kechagia, Maria and Sarro, Federica and Sharma, Tushar},
title = {Enhancing Energy-Awareness in Deep Learning through Fine-Grained Energy Measurement},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {8},
issn = {1049-331X},
url = {https://doi.org/10.1145/3680470},
doi = {10.1145/3680470},
abstract = {With the increasing usage, scale, and complexity of Deep Learning (dl) models, their rapidly growing energy consumption has become a critical concern. Promoting green development and energy awareness at different granularities is the need of the hour to limit carbon emissions of dl systems. However, the lack of standard and repeatable tools to accurately measure and optimize energy consumption at fine granularity (e.g., at the api level) hinders progress in this area.This paper introduces FECoM (Fine-grained Energy Consumption Meter), a framework for fine-grained dl energy consumption measurement. FECoM enables researchers and developers to profile dl apis from energy perspective. FECoM addresses the challenges of fine-grained energy measurement using static instrumentation while considering factors such as computational load and temperature stability. We assess FECoM’s capability for fine-grained energy measurement for one of the most popular open-source dl frameworks, namely TensorFlow. Using FECoM, we also investigate the impact of parameter size and execution time on energy consumption, enriching our understanding of TensorFlow apis’ energy profiles. Furthermore, we elaborate on the considerations and challenges while designing and implementing a fine-grained energy measurement tool. This work will facilitate further advances in dl energy measurement and the development of energy-aware practices for dl systems.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {211},
numpages = {34},
keywords = {Energy measurement, Green Artificial Intelligence, Fine-grained energy measurement}
}

@article{10.1145/3649597,
author = {Betz, Stefanie and Penzenstadler, Birgit and Duboc, Leticia and Chitchyan, Ruzanna and Kocak, Sedef Akinli and Brooks, Ian and Oyedeji, Shola and Porras, Jari and Seyff, Norbert and Venters, Colin C.},
title = {Lessons Learned from Developing a Sustainability Awareness Framework for Software Engineering Using Design Science},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3649597},
doi = {10.1145/3649597},
abstract = {To foster a sustainable society within a sustainable environment, we must dramatically reshape our work and consumption activities, most of which are facilitated through software. Yet, most software engineers hardly consider the effects on the sustainability of the IT products and services they deliver. This issue is exacerbated by a lack of methods and tools for this purpose. Despite the practical need for methods and tools that explicitly support consideration of the effects that IT products and services have on the sustainability of their intended environments, such methods and tools remain largely unavailable. Thus, urgent research is needed to understand how to design such tools for the IT community properly. In this article, we describe our experience using design science to create the Sustainability Awareness Framework (SusAF), which supports software engineers in anticipating and mitigating the potential sustainability effects during system development. More specifically, we identify and present the challenges faced during this process. The challenges that we have faced and addressed in the development of the SusAF are likely to be relevant to others who aim to create methods and tools to integrate sustainability analysis into their IT products and services development. Thus, the lessons learned in SusAF development are shared for the benefit of researchers and other professionals who design tools for that end.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {136},
numpages = {39},
keywords = {IT products, IT services, sustainability analysis}
}

@article{10.1145/3617683,
author = {Knodt, Julian and Pan, Zherong and Wu, Kui and Gao, Xifeng},
title = {Joint UV Optimization and Texture Baking},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0730-0301},
url = {https://doi.org/10.1145/3617683},
doi = {10.1145/3617683},
abstract = {Level of detail has been widely used in interactive computer graphics. In current industrial 3D modeling pipelines, artists rely on commercial software to generate highly detailed models with UV maps and then bake textures for low-poly counterparts. In these pipelines, each step is performed separately, leading to unsatisfactory visual appearances for low polygon count models. Moreover, existing texture baking techniques assume the low-poly mesh has a small geometric difference from the high-poly, which is often not true in practice, especially with extremely low poly count models.To alleviate the visual discrepancy of the low-poly mesh, we propose to jointly optimize UV mappings during texture baking, allowing for low-poly models to faithfully replicate the appearance of the high-poly even with large geometric differences. We formulate the optimization within a differentiable rendering framework, allowing the automatic adjustment of texture regions to encode appearance information. To compensate for view parallax when two meshes have large geometric differences, we introduce a spherical harmonic parallax mapping, which uses spherical harmonic functions to modulate per-texel UV coordinates based on the view direction. We evaluate the effectiveness and robustness of our approach on a dataset composed of online downloaded models, with varying complexities and geometric discrepancies. Our method achieves superior quality over state-of-the-art techniques and commercial solutions.},
journal = {ACM Trans. Graph.},
month = sep,
articleno = {2},
numpages = {20},
keywords = {Texture baking, differentiable rendering, UV optimization}
}

@article{10.1145/3570305,
author = {Korostelev, Ivan and L. De Carvalho, Jo\~{a}o P. and Moreira, Jos\'{e} and Amaral, Jos\'{e} Nelson},
title = {YaConv: Convolution with Low Cache Footprint},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3570305},
doi = {10.1145/3570305},
abstract = {This article introduces YaConv, a new algorithm to compute convolution using GEMM microkernels from a Basic Linear Algebra Subprograms library that is efficient for multiple CPU architectures. Previous approaches either create a copy of each image element for each filter element or reload these elements into cache for each GEMM call, leading to redundant instances of the image elements in cache. Instead, YaConv loads each image element once into the cache and maximizes the reuse of these elements. The output image is computed by scattering results of the GEMM microkernel calls to the correct locations in the output image. The main advantage of this new algorithm—which leads to better performance in comparison to the existing im2col approach on several architectures—is a more efficient use of the memory hierarchy. The experimental evaluation on convolutional layers from PyTorch, along with a parameterized study, indicates an average 24% speedup over im2col convolution. Increased performance comes as a result of 3\texttimes{} reduction in L3 cache accesses and 2\texttimes{} fewer branch instructions.},
journal = {ACM Trans. Archit. Code Optim.},
month = feb,
articleno = {18},
numpages = {18},
keywords = {GEMM, convolution, cache performance}
}

@inproceedings{10.1145/3650212.3680383,
author = {Guan, Hao and Bai, Guangdong and Liu, Yepang},
title = {Large Language Models Can Connect the Dots: Exploring Model Optimization Bugs with Domain Knowledge-Aware Prompts},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680383},
doi = {10.1145/3650212.3680383},
abstract = {Model optimization, such as pruning and quantization, has become the de facto pre-deployment phase when deploying deep learning&nbsp;(DL) models on resource-constrained platforms.         However, the complexity of DL models often leads to non-trivial bugs in model optimizers, known as model optimization bugs&nbsp;(MOBs).         These MOBs are characterized by involving complex data types and layer structures inherent to DL models, causing significant hurdles in detecting them through traditional static analysis and dynamic testing techniques.        In this work, we leverage Large Language Models (LLMs) with prompting techniques to generate test cases for MOB detection.        We explore how LLMs can draw an understanding of the MOB domain from scattered bug instances and generalize to detect new ones, a paradigm we term as concentration and diffusion.        We extract MOB domain knowledge from the artifacts of known MOBs, such as their issue reports and fixes, and design knowledge-aware prompts to guide LLMs in generating effective test cases.         The domain knowledge of code structure and error description provides precise in-depth depictions of the problem domain, i.e., the concentration, and heuristic directions to generate innovative test cases, i.e., the diffusion.         Our approach is implemented as a tool named YanHui and benchmarked against existing few-shot LLM-based fuzzing techniques.         Test cases generated by YanHui demonstrate enhanced capability to find relevant API and data combinations for exposing MOBs, leading to an 11.4% increase in generating syntactically valid code and a 22.3% increase in generating on-target code specific to model optimization.         YanHui detects 17 MOBs, and among them, five are deep MOBs that are difficult to reveal without our prompting technique.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1579–1591},
numpages = {13},
keywords = {Large Language Model, Library Testing, Model Optimization},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@proceedings{10.1145/3579990,
title = {CGO '23: Proceedings of the 21st ACM/IEEE International Symposium on Code Generation and Optimization},
year = {2023},
isbn = {9798400701016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 21st ACM/IEEE International Symposium on Code Generation and Optimization (CGO ’23), and to Montreal. After two years of virtual CGO, we are finally back in person! 

CGO provides a premier venue to bring together researchers and practitioners working at the interface of hardware and software on a wide range of optimization and code generation techniques and related issues. The conference spans the spectrum from purely static to fully dynamic approaches, and from pure software-based methods to specific architectural features and support for code generation and optimization.},
location = {Montr\'{e}al, QC, Canada}
}

@article{10.1145/3436238,
author = {Weiss, Iris and Vogel-Heuser, Birgit and Trunzer, Emanuel and Kruppa, Simon},
title = {Product Quality Monitoring in Hydraulic Presses Using a Minimal Sample of Sensor and Actuator Data},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3436238},
doi = {10.1145/3436238},
abstract = {Machine learning and artificial intelligence provide methods and algorithms to take advantage of sensor and actuator data in automated production systems. Product quality monitoring is one of the promising applications available for data-driven modeling, particularly in cases where the quality parameters cannot be measured with reasonable effort. This is the case for defects such as cracks in workpieces of hydraulic metal powder presses. However, the variety of shapes produced at a powder press requires training of individual models based on a minimal sample size of unlabeled data to adapt to changing settings. Therefore, this article proposes an unsupervised product quality monitoring approach based on dynamic time warping and non-linear regression to detect anomalies in unlabeled sensor and actuator data. A preprocessing step that isolates only the relevant intervals of the process is further introduced, facilitating efficient product quality monitoring. The evaluation on an industrial dataset with 37 samples, generated in test runs, shows a true-positive rate for detected product quality defects of 100% while preserving an acceptable accuracy. Moreover, the approach achieves the output within less than 10 seconds, assuring that the result is available before the next workpiece is processed. In this way, efficient product quality management is possible, reducing time- and cost-intensive quality inspections.},
journal = {ACM Trans. Internet Technol.},
month = may,
articleno = {37},
numpages = {23},
keywords = {Product quality monitoring, hydraulic metal powder press, unsupervised machine learning, minimal sample size, cyber-physical system}
}

@inbook{10.1145/3658617.3697602,
author = {Lee, Jaeseung and Jang, Sunggyu and Lee, Jakang and Kang, Seokhyeong},
title = {LIBMixer: An all-MLP Architecture for Cell Library Characterization towards Design Space Optimization},
year = {2025},
isbn = {9798400706356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658617.3697602},
abstract = {Cell library characterization is a fundamental stage of electronic design automation (EDA), as it provides essential electrical models for circuit simulation and design quality assessment. However, the development of advanced nodes demands increasing computational resources and engineering efforts for characterization. We introduce LIBMixer, a machine learning-based framework for fast and accurate library characterization designed to enhance design space optimization. Leveraging multi-layer perceptron architectures, LIBMixer efficiently manages complex relationships between technology and electrical characteristics. It achieves a 31.5\texttimes{} faster runtime than conventional EDA tools while enhancing alignment. Compared to state-of-the-art methods, LIBMixer targets 6.4\texttimes{} more standard cells for both power and timing information. This scalability improvement enables the practical synthesis of IP cores, demonstrating high correlation across of power, performance, and area results. Additionally, Pareto fronts of synthesis design results with LIBMixer-inferred libraries closely match those from foundry files. Experimental results highlight the effectiveness of LIBMixer as a fast and reliable alternative for PVT analysis.},
booktitle = {Proceedings of the 30th Asia and South Pacific Design Automation Conference},
pages = {814–820},
numpages = {7}
}

@article{10.1145/3708498,
author = {Fei, Yanhong and Liu, Yingjie and Jia, Chentao and Li, Zhengyu and Wei, Xian and Chen, Mingsong},
title = {A Survey of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3708498},
doi = {10.1145/3708498},
abstract = {Deep Learning (DL) has achieved remarkable success in tackling complex Artificial Intelligence tasks. The standard training of neural networks employs backpropagation to compute gradients and utilizes various optimization algorithms in the Euclidean space ℛn. However, this optimization process faces challenges, such as the local optimal issues and the problem of gradient vanishing and exploding. To address these problems, Riemannian optimization offers a powerful extension to solve optimization problems in deep learning. By incorporating the prior constraint structure and the metric information of the underlying geometric information, Riemannian optimization-based DL offers a more stable and reliable optimization process, as well as enhanced adaptability to complex data structures. This article presents a comprehensive survey of applying geometric optimization in DL, including the basic procedure of geometric optimization, various geometric optimizers, and some concepts of the Riemannian manifold. In addition, it investigates various applications of geometric optimization in different DL networks for diverse tasks and discusses typical public toolboxes that implement optimization on the manifold. This article also includes a performance comparison among different deep geometric optimization methods in image recognition scenarios. Finally, this article elaborates on future opportunities and challenges in this field.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {123},
numpages = {37},
keywords = {Orthogonal projection, Stiefel manifold, Oblique manifold, Gra\ss{}mann manifold, geometrical stochastic gradient algorithm, supervised discrimination learning, representation learning, sparse representation, dimension reduction, image retrieval, dictionary learning, geometric optimization, differential geometry, manifold parameterization, deep learning}
}

@article{10.1109/TNET.2024.3421356,
author = {Li, Rui and Ouyang, Tao and Zeng, Liekang and Liao, Guocheng and Zhou, Zhi and Chen, Xu},
title = {Online Optimization of DNN Inference Network Utility in Collaborative Edge Computing},
year = {2024},
issue_date = {Oct. 2024},
publisher = {IEEE Press},
volume = {32},
number = {5},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2024.3421356},
doi = {10.1109/TNET.2024.3421356},
abstract = {Collaborative Edge Computing (CEC) is an emerging paradigm that collaborates heterogeneous edge devices as a resource pool to compute DNN inference tasks in proximity such as edge video analytics. Nevertheless, as the key knob to improve network utility in CEC, existing works mainly focus on the workload routing strategies among edge devices with the aim of minimizing the routing cost, remaining an open question for joint workload allocation and routing optimization problem from a system perspective. To this end, this paper presents a holistic, learned optimization for CEC towards maximizing the total network utility in an online manner, even though the utility functions of task input rates are unknown a priori. In particular, we characterize the CEC system in a flow model and formulate an online learning problem in a form of cross-layer optimization. We propose a nested-loop algorithm to solve workload allocation and distributed routing iteratively, using the tools of gradient sampling and online mirror descent. To improve the convergence rate over the nested-loop version, we further devise a single-loop algorithm. Rigorous analysis is provided to show its inherent convexity, efficient convergence, as well as algorithmic optimality. Finally, extensive numerical simulations demonstrate the superior performance of our solutions.},
journal = {IEEE/ACM Trans. Netw.},
month = jul,
pages = {4414–4426},
numpages = {13}
}

@inproceedings{10.1145/3611643.3616285,
author = {Zhang, Yiran and Xu, Zhengzi and Liu, Chengwei and Chen, Hongxu and Sun, Jianwen and Qiu, Dong and Liu, Yang},
title = {Software Architecture Recovery with Information Fusion},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616285},
doi = {10.1145/3611643.3616285},
abstract = {Understanding the architecture is vital for effectively maintaining and managing large software systems. However, as software systems evolve over time, their architectures inevitably change. To keep up with the change, architects need to track the implementation-level changes and update the architectural documentation accordingly, which is time-consuming and error-prone. Therefore, many automatic architecture recovery techniques have been proposed to ease this process. Despite efforts have been made to improve the accuracy of architecture recovery, existing solutions still suffer from two limitations. First, most of them only use one or two type of information for the recovery, ignoring the potential usefulness of other sources. Second, they tend to use the information in a coarse-grained manner, overlooking important details within it. To address these limitations, we propose SARIF, a fully automated architecture recovery technique, which incorporates three types of comprehensive information, including dependencies, code text and folder structure. SARIF can recover architecture more accurately by thoroughly analyzing the details of each type of information and adaptively fusing them based on their relevance and quality. To evaluate SARIF, we collected six projects with published ground-truth architectures and three open-source projects labeled by our industrial collaborators. We compared SARIF with nine state-of-the-art techniques using three commonly-used architecture similarity metrics and two new metrics. The experimental results show that SARIF is 36.1% more accurate than the best of the previous techniques on average. By providing comprehensive architecture, SARIF can help users understand systems effectively and reduce the manual effort of obtaining ground-truth architectures.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1535–1547},
numpages = {13},
keywords = {architecture comparison, reverse engineering, software architecture recovery, software module clustering},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3650212.3680336,
author = {Gao, Xuanqi and Jiang, Weipeng and Zhai, Juan and Ma, Shiqing and Zhang, Xiaoyu and Shen, Chao},
title = {Efficient DNN-Powered Software with Fair Sparse Models},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680336},
doi = {10.1145/3650212.3680336},
abstract = {With the emergence of the Software 3.0 era, there is a growing trend of compressing and integrating large models into software systems, with significant societal implications.         Regrettably, in numerous instances, model compression techniques impact the fairness performance of these models and thus the ethical behavior of DNN-powered software.         One of the most notable example is the Lottery Ticket Hypothesis&nbsp;(LTH), a prevailing model pruning approach.        This paper demonstrates that fairness issue of LTH-based pruning arises from both its subnetwork selection and training procedures, highlighting the inadequacy of existing remedies.        To address this, we propose a novel pruning framework, Ballot, which employs a novel conflict-detection-based subnetwork selection to find accurate and fair subnetworks, coupled with a refined training process to attain a high-performance model, thereby improving the fairness of DNN-powered software.        By means of this procedure, Ballot improves the fairness of pruning by 38.00%, 33.91%, 17.96%, and 35.82% compared to state-of-the-art baselines, namely Magnitude Pruning, Standard LTH, SafeCompress, and FairScratch respectively, based on our evaluation of five popular datasets and three widely used models.        Our code is available at https://anonymous.4open.science/r/Ballot-506E.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {983–995},
numpages = {13},
keywords = {deep neural network, fairness, pruning},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@article{10.1109/TNET.2023.3346286,
author = {Miano, Sebastiano and Sanaee, Alireza and Risso, Fulvio and R\'{e}tv\'{a}ri, G\'{a}bor and Antichi, Gianni},
title = {Morpheus: A Run Time Compiler and Optimizer for Software Data Planes},
year = {2023},
issue_date = {June 2024},
publisher = {IEEE Press},
volume = {32},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2023.3346286},
doi = {10.1109/TNET.2023.3346286},
abstract = {State-of-the-art approaches to design, develop and optimize software packet-processing programs are based on static compilation: the compiler’s input is a description of the forwarding plane semantics and the output is a binary that can accommodate any control plane configuration or input traffic. In this paper, we demonstrate that tracking control plane actions and packet-level traffic dynamics at run time opens up new opportunities for code specialization. We present Morpheus, a system working alongside static compilers that continuously optimizes the targeted networking code. We introduce a number of new techniques, from static code analysis to adaptive code instrumentation, and we implement a toolbox of domain specific optimizations that are not restricted to a specific data plane framework or programming language. We apply Morpheus to several systems, from eBPF and DPDK programs including Katran, Meta’s production-grade load balancer to container orchestration solutions such a Kubernets. We compare Morpheus to state-of-the-art optimization frameworks and show that it can bring up to 2x throughput improvement, while halving the 99th percentile latency.},
journal = {IEEE/ACM Trans. Netw.},
month = dec,
pages = {2269–2284},
numpages = {16}
}

@article{10.1145/3657643,
author = {Mo, George and Dudley, John and Chan, Liwei and Liao, Yi-Chi and Oulasvirta, Antti and Kristensson, Per Ola},
title = {Cooperative Multi-Objective Bayesian Design Optimization},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/3657643},
doi = {10.1145/3657643},
abstract = {Computational methods can potentially facilitate user interface design by complementing designer intuition, prior experience, and personal preference. Framing a user interface design task as a multi-objective optimization problem can help with operationalizing and structuring this process at the expense of designer agency and experience. While offering a systematic means of exploring the design space, the optimization process cannot typically leverage the designer’s expertise in quickly identifying that a given “bad” design is not worth evaluating. We here examine a cooperative approach where both the designer and optimization process share a common goal and work in partnership by establishing a shared understanding of the design space. We tackle the research question: How can we foster cooperation between the designer and a systematic optimization process in order to best leverage their combined strength? We introduce and present an evaluation of a cooperative approach that allows the user to express their design insight and work in concert with a multi-objective design process. We find that the cooperative approach successfully encourages designers to explore more widely in the design space than when they are working without assistance from an optimization process. The cooperative approach also delivers design outcomes that are comparable to an optimization process run without any direct designer input but achieves this with greater efficiency and substantially higher designer engagement levels.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = jun,
articleno = {13},
numpages = {28},
keywords = {Interaction technique, interface design, Bayesian optimization}
}

@inproceedings{10.1145/3650212.3680400,
author = {Guan, Kevin and Legunsen, Owolabi},
title = {An In-Depth Study of Runtime Verification Overheads during Software Testing},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680400},
doi = {10.1145/3650212.3680400},
abstract = {Runtime verification (RV) monitors program executions against formal specifications (specs). Researchers showed that RV during software testing amplifies the bug-finding ability of tests, and found hundreds of new bugs by using RV to monitor passing tests in open-source projects. But, RV’s runtime overhead is widely seen as a hindrance to its broad adoption, especially during continuous integration. Yet, there is no in-depth study of the prevalence, usefulness for bug finding, and components of these overheads during testing, so that researchers can better understand how to speed up RV.
 
 
 
We study RV overhead during testing, monitoring developer-written unit tests in 1,544 open-source projects against 160 specs of correct JDK API usage. We make four main findings. (1) RV overhead is below 12.48 seconds, which others considered acceptable, in 40.9% of projects, but up to 5,002.9x (or, 28.7 hours) in the other projects. (2) 99.87% of monitors that RV generates to dynamically check program traces are wasted; they can only find bugs that the other 0.13% find. (3) Contrary to conventional wisdom, RV overhead in most projects is dominated by instrumentation, not monitoring. (4) 36.74% of monitoring time is spent in test code or libraries.
 
 
 
As evidence that our study provides a new basis that future work can exploit, we perform two more experiments. First, we show that offline instrumentation (when possible) greatly reduces RV runtime overhead for single versions of many projects. Second, we show that simply amortizing high instrumentation costs across multiple program versions can outperform, by up to 4.53x, a recent evolution-aware RV technique that uses complex program analysis.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1798–1810},
numpages = {13},
keywords = {Runtime Verification, software evolution, software testing},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@article{10.1145/3725529,
author = {Lambiase, Stefano and Catolino, Gemma and Palomba, Fabio and Ferrucci, Filomena and Russo, Daniel},
title = {Investigating the Role of Cultural Values in Adopting Large Language Models for Software Engineering},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3725529},
doi = {10.1145/3725529},
abstract = {As a socio-technical activity, software development involves the close interconnection of people and technology. The integration of Large Language Models (LLMs) into this process exemplifies the socio-technical nature of software development. Although LLMs influence the development process, software development remains fundamentally human-centric, necessitating an investigation of the human factors in this adoption. Thus, with this study we explore the factors influencing the adoption of LLMs in software development, focusing on the role of professionals’ cultural values. Guided by the Unified Theory of Acceptance and Use of Technology (UTAUT2) and Hofstede’s cultural dimensions, we hypothesized that cultural values moderate the relationships within the UTAUT2 framework. Using Partial Least Squares-Structural Equation Modelling and data from 188 software engineers, we found that habit and performance expectancy are the primary drivers of LLM adoption, while cultural values do not significantly moderate this process. These findings suggest that, by highlighting how LLMs can boost performance and efficiency, organizations can encourage their use, no matter the cultural differences. Practical steps include offering training programs to demonstrate LLM benefits, creating a supportive environment for regular use, and continuously tracking and sharing performance improvements from using LLMs.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
keywords = {UTAUT2, Culture, Hofstede, Generative AI, LLM, Empirical Software Engineering}
}

@inproceedings{10.1145/3313831.3376560,
author = {Gorkovenko, Katerina and Burnett, Daniel J. and Thorp, James K. and Richards, Daniel and Murray-Rust, Dave},
title = {Exploring The Future of Data-Driven Product Design},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376560},
doi = {10.1145/3313831.3376560},
abstract = {Connected devices present new opportunities to advance design through data collection in the wild, similar to the way digital services evolve through analytics. However, it is still unclear how live data transmitted by connected devices informs the design of these products, going beyond performance optimisation to support creative practices. Design can be enriched by data captured by connected devices, from usage logs to environmental sensors, and data about the devices and people around them. Through a series of workshops, this paper contributes industry and academia perspectives on the future of data-driven product design. We highlight HCI challenges, issues and implications, including sensemaking and the generation of design insight. We further challenge current notions of data-driven design and envision ways in which future HCI research can develop ways to work with data in the design process in a connected, rich, human manner.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {data-driven design, design research, human-centred design, in the wild, iot, smart devices},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.5555/3586210.3586459,
author = {Nunziatini, Andrea and Fani, Virginia and Bindi, Bianca and Bandinelli, Romeo and Tucci, Mario},
title = {Data-Driven Simulation for Production Balancing and Optimization: A Case Study in the Fashion Luxury Industry},
year = {2023},
publisher = {IEEE Press},
abstract = {The paper presents the definition of a data-driven simulation framework and its development using AnyLogic® software for production balancing and optimization in the leather luxury accessories industry. The model has been developed including an industry-oriented set of objects in order to easily replicate the resource configuration and layout mainly used within the analysed context. A case study is therefore conducted to validate the model and confirm its usability. The results of this work demonstrated that the proposed framework can be easily adapted to a shoe joiner using the pre-configured objects. Scenarios have been carried out based on what-if analyses regarding productivity, resources saturation and bottlenecks. This tool can therefore be used as a valuable support to production planning, scheduling optimization, workload and production cycle balancing.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2957–2967},
numpages = {11},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.1145/3701625.3701627,
author = {Cruz, Dhyego Tavares and Almeida, Erlon Pereira and Santos, Jander Pereira and Paix\~{a}o, Felipe de Sant’Anna and de Santana, Enio Garcia and Gomes e Souza, Rodrigo Rocha and Iwamoto, H\'{e}rsio Massanori and Dur\~{a}o, Frederico Ara\'{u}jo and Serafim Prazeres, C\'{a}ssio Vinicius and Machado, Ivan do Carmo and Figueiredo, Gustavo Bittencourt and Maciel Peixoto, Maycon Leone and de Almeida, Eduardo Santana},
title = {Software Development Practices and Tools for University-Industry R&amp;D projects},
year = {2024},
isbn = {9798400717772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701625.3701627},
doi = {10.1145/3701625.3701627},
abstract = {Research and development (R&amp;D) projects involving universities and industry drive innovation by bringing scientific knowledge closer to practical problems. Nevertheless, the partnership comes with challenges, such as high developer turnover, team members with diverse backgrounds and experience levels, and part-time contributors. In this paper, we report experiences in a large R&amp;D project in the smart home field, in which we proposed software development practices and tools with the goal of promoting short feedback cycles and dissemination of best practices. By surveying the developers of this project, we discovered that the practices and tools were generally well accepted and also determined specific areas that need improvement. The insights collected in this study can be used by other teams conducting R&amp;D projects.},
booktitle = {Proceedings of the XXIII Brazilian Symposium on Software Quality},
pages = {426–437},
numpages = {12},
keywords = {Software Engineering, R&amp;D, Artificial Intelligence, IA},
location = {
},
series = {SBQS '24}
}

@proceedings{10.1145/3639478,
title = {ICSE-Companion '24: Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ICSE is the leading and, by far, the largest conference in Software Engineering, attracting researchers, practitioners, and students worldwide. ICSE2024 is co-located with 11 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3643660,
title = {Designing '24: Proceedings of the 1st International Workshop on Designing Software},
year = {2024},
isbn = {9798400705632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The goals of this workshop are to: (1) bring together a group of researchers, practitioners, and educators interested in software design, (2) identify open challenges and new directions for the design of modern software systems, including grand challenges for the community, and (3) discuss novel approaches to designing as well as teaching design. Although the workshop welcomes discussions related to any aspect of software design, the primary focus will be on improving our understanding of design as an activity rather than as an artifact or end product (hence the word designing in the title).},
location = {Lisbon, Portugal}
}

@article{10.5555/3722577.3722650,
author = {Borsos, Zal\'{a}n and Mutn\'{y}, Mojm\'{\i}r and Tagliasacchi, Marco and Krause, Andreas},
title = {Data summarization via bilevel optimization},
year = {2024},
issue_date = {January 2024},
publisher = {JMLR.org},
volume = {25},
number = {1},
issn = {1532-4435},
abstract = {The increasing availability of massive data sets poses various challenges for machine learning. Prominent among these is learning models under hardware or human resource constraints. In such resource-constrained settings, a simple yet powerful approach is operating on small subsets of the data. Coresets are weighted subsets of the data that provide approximation guarantees for the optimization objective. However, existing coreset constructions are highly model-specific and are limited to simple models such as linear regression, logistic regression, and k-means. In this work, we propose a generic coreset construction framework that formulates the coreset selection as a cardinality-constrained bilevel optimization problem. In contrast to existing approaches, our framework does not require model-specific adaptations and applies to any twice differentiable model, including neural networks. We show the effectiveness of our framework for a wide range of models in various settings, including training non-convex models online and batch active learning.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {73},
numpages = {93},
keywords = {data summarization, coresets, bilevel optimization, continual learning, streaming, batch active learning}
}

@article{10.1145/3592144,
author = {Lai, Zeqiang and Wei, Kaixuan and Fu, Ying and H\"{a}rtel, Philipp and Heide, Felix},
title = {∇-Prox: Differentiable Proximal Algorithm Modeling for Large-Scale Optimization},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3592144},
doi = {10.1145/3592144},
abstract = {Tasks across diverse application domains can be posed as large-scale optimization problems, these include graphics, vision, machine learning, imaging, health, scheduling, planning, and energy system forecasting. Independently of the application domain, proximal algorithms have emerged as a formal optimization method that successfully solves a wide array of existing problems, often exploiting problem-specific structures in the optimization. Although model-based formal optimization provides a principled approach to problem modeling with convergence guarantees, at first glance, this seems to be at odds with black-box deep learning methods. A recent line of work shows that, when combined with learning-based ingredients, model-based optimization methods are effective, interpretable, and allow for generalization to a wide spectrum of applications with little or no extra training data. However, experimenting with such hybrid approaches for different tasks by hand requires domain expertise in both proximal optimization and deep learning, which is often error-prone and time-consuming. Moreover, naively unrolling these iterative methods produces lengthy compute graphs, which when differentiated via autograd techniques results in exploding memory consumption, making batch-based training challenging. In this work, we introduce ∇-Prox, a domain-specific modeling language and compiler for large-scale optimization problems using differentiable proximal algorithms. ∇-Prox allows users to specify optimization objective functions of unknowns concisely at a high level, and intelligently compiles the problem into compute and memory-efficient differentiable solvers. One of the core features of ∇-Prox is its full differentiability, which supports hybrid model- and learning-based solvers integrating proximal optimization with neural network pipelines. Example applications of this methodology include learning-based priors and/or sample-dependent inner-loop optimization schedulers, learned with deep equilibrium learning or deep reinforcement learning. With a few lines of code, we show ∇-Prox can generate performant solvers for a range of image optimization problems, including end-to-end computational optics, image deraining, and compressive magnetic resonance imaging. We also demonstrate ∇-Prox can be used in a completely orthogonal application domain of energy system planning, an essential task in the energy crisis and the clean energy transition, where it outperforms state-of-the-art CVXPY and commercial Gurobi solvers.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {105},
numpages = {19},
keywords = {computational photography, image reconstruction, differentiable optimization}
}

@inproceedings{10.1145/3696443.3708957,
author = {Zayed, Abd-El-Aziz and Dubach, Christophe},
title = {DialEgg: Dialect-Agnostic MLIR Optimizer using Equality Saturation with Egglog},
year = {2025},
isbn = {9798400712753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696443.3708957},
doi = {10.1145/3696443.3708957},
abstract = {MLIR’s ability to optimize programs at multiple levels of abstraction is key to enabling domain-specific optimizing compilers. However, expressing optimizations remains tedious. Optimizations can interact in unexpected ways, making it hard to unleash full performance.
 

 
Equality saturation promises to solve these challenges. First, it simplifies the expression of optimizations using rewrite rules. Secondly, it considers all possible optimization interactions, through saturation, selecting the best program variant. Despite these advantages, equality saturation remains absent from production compilers such as MLIR.
 

 
This paper proposes to integrate Egglog, a recent equality saturation engine, with MLIR, in a dialect-agnostic manner. This paper shows how the main MLIR constructs such as operations, types or attributes can be modeled in Egglog. It also presents DialEgg, a tool that pre-defines a large set of common MLIR constructs in Egglog and automatically translates between the MLIR and Egglog program representations. Using a few use-cases, this paper demonstrates the potential for combining equality saturation and MLIR.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {271–283},
numpages = {13},
keywords = {Egg, Egglog, Equality Saturation, MLIR},
location = {Las Vegas, NV, USA},
series = {CGO '25}
}

@inproceedings{10.1145/3594806.3596554,
author = {Konstantinidis, Fotios K. and Balaska, Vasiliki and Symeonidis, Symeon and Psarommatis, Foivos and Psomoulis, Athanasios and Giakos, Georgios and Mouroutsos, Spyridon G. and Gasteratos, Antonios},
title = {Achieving Zero Defected Products in Diary 4.0 using Digital Twin and Machine Vision},
year = {2023},
isbn = {9798400700699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594806.3596554},
doi = {10.1145/3594806.3596554},
abstract = {The digitalization of traditional industrial processes has profoundly influenced every step of the manufacturing value chain during the past two decades, having as its main goal to achieve zero-defected products. Moreover, since dairy production is at the heart of food industry, it is critical to leverage innovative technologies to increase their efficiency and continuously meet the demanding standards from the farm level to market and reduce the amount of waste. Towards this end, we propose a Dairy 4.0 architecture capable of utilising information to detect and prevent flaws to the final dairy products. The architecture layers are based on machine vision and the digital twins technologies, while it respects the zero defect manufacturing (ZDM) approach. The proposed frameworks is structured on a four layer architecture: (i) the physical layer, which consists of dairy farming, dairy production, and dairy storage and logistics, (ii) the acquisition layer that is responsible for collecting contextual information, (iii) the digital twin layer which uses data from the vision system and the physical system to anticipate future occurrences, and finally (iv) the ZDM layer, which functions as an orchestrator and binding agent for all the processed data.},
booktitle = {Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {528–534},
numpages = {7},
keywords = {Diary 4.0, Digital Twins, Industry 4.0, Machine vision, Zero-Defect-Manufacturing},
location = {Corfu, Greece},
series = {PETRA '23}
}

@article{10.1145/3672453,
author = {Zolduoarrati, Elijah and Licorish, Sherlock A. and Stanger, Nigel},
title = {Harmonising Contributions: Exploring Diversity in Software Engineering through CQA Mining on Stack Overflow},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3672453},
doi = {10.1145/3672453},
abstract = {The need for collective intelligence in technology means that online Q&amp;A platforms, such as Stack Overflow and Reddit, have become invaluable in building the global knowledge ecosystem. Despite literature demonstrating a prevalence of inclusion and contribution disparities in online communities, studies investigating the underlying reasons behind such fluctuations remain scarce. The current study examines Stack Overflow users’ contribution profiles, both in isolation and relative to various diversity metrics, including GDP and access to electricity. This study also examines whether such profiles propagate to the city and state levels, supplemented by granular data such as per capita income and education, before validating quantitative findings using content analysis. We selected 143 countries and compared the profiles of their respective users to assess implicit diversity-related complications that impact how users contribute. Results show that countries with high GDP, prominent R&amp;D presence, less wealth inequality and sufficient access to infrastructure tend to have more users, regardless of their development status. Similarly, cities and states where technology is more prevalent (e.g., San Francisco and New York) have more users who tend to contribute more often. Qualitative analysis reveals distinct communication styles based on users’ locations. Urban users exhibited assertive, solution-oriented behaviour, actively sharing information. Conversely, rural users engaged through inquiries and discussions, incorporating personal anecdotes, gratitude and conciliatory language. Findings from this study may benefit scholars and practitioners, allowing them to develop sustainable mechanisms to bridge the inclusion and diversity gaps.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {179},
numpages = {54},
keywords = {Contribution, diversity, online Q&amp;A, software engineering}
}

@inproceedings{10.1109/SC41406.2024.00047,
author = {Pan, Zaifeng and Zheng, Zhen and Zhang, Feng and Xie, Bing and Wu, Ruofan and Smith, Shaden and Liu, Chuanjie and Ruwase, Olatunji and Du, Xiaoyong and Ding, Yufei},
title = {RecFlex: Enabling Feature Heterogeneity-Aware Optimization for Deep Recommendation Models with Flexible Schedules},
year = {2024},
isbn = {9798350352917},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC41406.2024.00047},
doi = {10.1109/SC41406.2024.00047},
abstract = {Industrial recommendation models typically involve numerous feature fields. The embedding computation workloads are heterogeneous across these fields, thus requiring varied optimal code schedules. While existing solutions apply basic fusion optimization for embedding operations, they inefficiently treat all feature fields with identical schedules, leading to suboptimal performance. In this paper, we introduce RecFlex, which generates fused kernels with distinct schedules for different feature fields. RecFlex employs the interference-aware schedule tuner to tune schedules and the heterogeneous schedule fusion compiler to generate fused kernels, addressing two major challenges. To determine optimal schedules of different feature fields within the fused kernel, RecFlex proposes a two-stage interference-simulated tuning strategy. To handle dynamic workloads that challenge tuning and fusion, RecFlex combines compile-time schedule tuning with runtime kernel thread mapping. RecFlex surpasses state-of-the-art libraries and compilers, achieving average speedups of 2.64 \texttimes{}, 20.77 \texttimes{}, and 11.31 \texttimes{} over TorchRec, HugeCTR, and RECom, respectively. RecFlex is publicly available at https://github.com/PanZaifeng/RecFlex.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {41},
numpages = {15},
keywords = {machine learning compiler, recommender system},
location = {Atlanta, GA, USA},
series = {SC '24}
}

@inproceedings{10.1145/3664646.3664767,
author = {Eskandani, Nafise and Salvaneschi, Guido},
title = {Towards AI for Software Systems},
year = {2024},
isbn = {9798400706851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664646.3664767},
doi = {10.1145/3664646.3664767},
abstract = {Generative Artificial Intelligence (GenAI) is being adopted for a number of Software Engineering activities, mostly centering around coding, such as code generation, code comprehension, code reviews, test generation, and bug fixing. Other phases in the Software Engineering process have been less explored. In this paper, we argue that more investigation is needed on the support that GenAI can provide to the design, and operation of software systems, i.e., a number of crucial activities, beyond coding, that are necessary to successfully deliver and maintain software services. These include reasoning about architectural choices and dealing with third-party platforms.
 
 
 

 
 
 
We discuss crucial aspects of AI for software systems. taking as a use case Function as a Service (FaaS).We present several challenges, including cold start delays, stateless functions, debugging complexities, and vendor lock-in and explore the potential of GenAI tools to mitigate FaaS challenges. Finally, we outline future research into the application of GenAI tools for the development and deployment of software systems.},
booktitle = {Proceedings of the 1st ACM International Conference on AI-Powered Software},
pages = {79–84},
numpages = {6},
keywords = {FaaS, Generative AI, Serverless Computing},
location = {Porto de Galinhas, Brazil},
series = {AIware 2024}
}

@inproceedings{10.1145/3652620.3688201,
author = {Meijer, Willem},
title = {Contract-based Validation of Conceptual Design Bugs for Engineering Complex Machine Learning Software},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688201},
doi = {10.1145/3652620.3688201},
abstract = {Context. Modern software systems increasingly commonly contain one or multiple machine learning (ML) components. Current development practices are generally on a trial-and-error basis, posing a significant risk of introducing bugs. One type of bug is the "conceptual design bug," referring to a misunderstanding between the properties of input data and prerequisites imposed by ML algorithms (e.g., using unscaled data in a scale-sensitive algorithm). These bugs are challenging to test at design time, causing problems at runtime through crashes, noticeably poor model performance, or not at all, threatening the system's robustness and transparency. Objective. In this work, I propose the line of research I intend to pursue during my PhD, addressing conceptual design bugs in complex ML software from a prevention-oriented perspective. I intend to build open-source tooling for ML engineers that can be used to detect conceptual design bugs, enabling them to make quality assurances about their system design's robustness. Approach. We need to understand conceptual bugs beyond the status quo, identifying their types, prevalence, impacts, and structural elements in the code. We operationalize this knowledge into a tool that detects them at design time, allowing ML engineers to resolve them before running their code and wasting resources. We anticipate this tool will leverage contract-based validation applied to partial ML software models. Evaluation. We plan to evaluate the built tool two-fold using professional (industrial) ML software. First, we will study its effectiveness regarding bug detection at design time, identifying whether it fulfills its functional objective. Second, we will study its usability, identifying whether ML engineers benefit when tools like this are introduced into their ML engineering workflow.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {155–161},
numpages = {7},
keywords = {machine learning, software bugs, software design, knowledge mining, software contracts, empirical software engineering},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3721465.3721863,
author = {Awwad, Hanan and Lin, Changyuan and Ward, Rabab and Shahrad, Mohammad},
title = {Estimating the Carbon Footprint of Serverless Functions on a Public Cloud Platform},
year = {2025},
isbn = {9798400715570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3721465.3721863},
doi = {10.1145/3721465.3721863},
abstract = {As the carbon footprint of cloud data centers grows rapidly, sustainability has become an increasing concern for practitioners. Understanding the carbon emissions of cloud workloads and identifying strategies to reduce them is critical. In this paper, we model and extensively analyze the carbon emissions of functions executed on a public serverless platform using available telemetry, offering new insights into the relationship between carbon emissions and traditional metrics of cost and performance. We explore various factors affecting carbon emissions, including host region, architecture, cold starts, application resource composition, and input-sensitivity. Based on our findings, we propose future optimization opportunities and research directions. Our work aims to empower developers to make more sustainable decisions when configuring or optimizing their applications.},
booktitle = {Proceedings of the 3rd Workshop on SErverless Systems, Applications and MEthodologies},
pages = {12–20},
numpages = {9},
keywords = {Carbon Modeling, Serverless Computing, Sustainability},
location = {Rotterdam, Netherlands},
series = {SESAME' 25}
}

@proceedings{10.1145/3643665,
title = {FinanSE '24: Proceedings of the 1st IEEE/ACM Workshop on Software Engineering Challenges in Financial Firms},
year = {2024},
isbn = {9798400705687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Software development has an integral role in every financial organisation; indeed, almost every service provided by a bank utilizes some form of software solution. While SE research has led to solutions and innovations for many popular SE problems, there remain unresolved challenges, particularly, those challenges faced in software development in financial firms. An example of such a challenge is defect prediction, where defects are not equal as some may lead to larger reputational and financial damage than others. Consequently, testing and verification is burdened with a further set of restraints for finance-based SE teams. Financial firms began automating processes as early as the 1960s, and as such, must maintain large legacy systems which may host critical operations. This problem is further exacerbated by the numerous mergers and acquisitions common in the financial sector, which leaves firms with a set of heterogeneous legacy systems that need to communicate with one another effectively and efficiently. Therefore, maintaining these systems while modernizing them leads to intriguing challenges, spanning from model extraction and process optimisation to code translation. Moreover, highly regulated institutions like financial firms require a high degree of transparency and accountability. This requirement facilitates the need for model fairness and explainability for any SE solution, in particular those that rely on AI.The 1st International Workshop on Software Engineering Challenges in Financial Firms (FinanSE 2024) is a forum to bring together academia and industry to share new ideas and results in tackling these challenges.},
location = {Lisbon, Portugal}
}

@article{10.1145/3639564,
author = {Sha, Sai and Li, Chuandong and Wang, Xiaolin and Wang, Zhenlin and Luo, Yingwei},
title = {Hardware-Software Collaborative Tiered-Memory Management Framework for Virtualization},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1–2},
issn = {0734-2071},
url = {https://doi.org/10.1145/3639564},
doi = {10.1145/3639564},
abstract = {The tiered-memory system can effectively expand the memory capacity for virtual machines (VMs). However, virtualization introduces new challenges specifically in enforcing performance isolation, minimizing context switching, and providing resource overcommit. None of the state-of-the-art designs consider virtualization and address these challenges; we observe that a VM with tiered memory incurs up to a 2\texttimes{} slowdown compared to a DRAM-only VM.We propose vTMM, a hardware-software collaborative tiered-memory management framework for virtualization. A key insight in vTMM is to leverage the unique system features in virtualization to meet the above challenges. vTMM automatically determines page hotness and migrates pages between fast and slow memory to achieve better performance. Specially, vTMM optimizes page tracking and migration based on page-modification logging (PML), a hardware-assisted virtualization mechanism, and adaptively distinguishes hot/cold pages through the page “temperature” sorting. vTMM also dynamically adjusts fast memory among multi-VMs on demand by using a memory pool. Further, vTMM tracks huge pages at regular-page granularity in hardware and splits/merges pages in software, realizing hybrid-grained page management and optimization. We implement and evaluate vTMM with single-grained page management on an Intel processor, and the hybrid-grained page management on a Sunway processor with hardware mode supporting hardware/software co-designs. Experiments show that vTMM outperforms existing tiered-memory management designs in virtualization.},
journal = {ACM Trans. Comput. Syst.},
month = feb,
articleno = {4},
numpages = {32},
keywords = {Tiered memory, virtual machine, hardware and software co-design, PML}
}

@article{10.1145/3595178,
author = {Deshmukh, Sameer and Yokota, Rio and Bosilca, George},
title = {Cache Optimization and Performance Modeling of Batched, Small, and Rectangular Matrix Multiplication on Intel, AMD, and Fujitsu Processors},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {3},
issn = {0098-3500},
url = {https://doi.org/10.1145/3595178},
doi = {10.1145/3595178},
abstract = {Factorization and multiplication of dense matrices and tensors are critical, yet extremely expensive pieces of the scientific toolbox. Careful use of low rank approximation can drastically reduce the computation and memory requirements of these operations. In addition to a lower arithmetic complexity, such methods can, by their structure, be designed to efficiently exploit modern hardware architectures. The majority of existing work relies on batched BLAS libraries to handle the computation of many small dense matrices. We show that through careful analysis of the cache utilization, register accumulation using SIMD registers and a redesign of the implementation, one can achieve significantly higher throughput for these types of batched low-rank matrices across a large range of block and batch sizes. We test our algorithm on three CPUs using diverse ISAs – the Fujitsu A64FX using ARM SVE, the Intel Xeon 6148 using AVX-512, and AMD EPYC 7502 using AVX-2, and show that our new batching methodology is able to obtain more than twice the throughput of vendor optimized libraries for all CPU architectures and problem sizes.},
journal = {ACM Trans. Math. Softw.},
month = sep,
articleno = {23},
numpages = {29},
keywords = {Low-rank matrix multiplication, batched matrix multiplication, cache blocking, performance modeling}
}

@inproceedings{10.1145/3661167.3661210,
author = {Kumar, Jahnavi and Chimalakonda, Sridhar},
title = {Code Summarization without Direct Access to Code - Towards Exploring Federated LLMs for Software Engineering},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661210},
doi = {10.1145/3661167.3661210},
abstract = {Software Engineering (SE) researchers are extensively applying Large Language Models (LLMs) to address challenges in SE tasks such as code clone detection, code summarization, and program comprehension. Despite promising results, LLMs have to be fine-tuned and customized with specific datasets for optimal performance. However, the proprietary nature of SE data, and the lack of LLMs trained on non-open source data is an open problem. While there exists work on applying Federated Learning (FL) for SE, integration of FL with LLMs for SE is unexplored. Hence, we propose a FedLLM for “code summarization” as developers spend more time in comprehending code. We setup a federated learning architecture and fine-tune LLM (Llama2 with 6.7B parameters) using Parameter Efficient Fine-Tuning (PEFT) for code summarization. We conducted our experiments on 40GB RAM GPU in an A100 architecture. Results show that FL-trained LLM is as effective as a centrally-trained one. We envision that leveraging non-open source data using FedLLM for SE could be an interesting research direction.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {100–109},
numpages = {10},
keywords = {Code Summarization, Federated Learning, Large Language Model (LLM), Parameter Efficient Fine-Tuning (PEFT)},
location = {Salerno, Italy},
series = {EASE '24}
}

@inproceedings{10.1145/3468264.3468605,
author = {Wu, Xiuheng and Zhu, Chenguang and Li, Yi},
title = {DIFFBASE: a differential factbase for effective software evolution management},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468605},
doi = {10.1145/3468264.3468605},
abstract = {Numerous tools and techniques have been developed to extract and analyze information from software development artifacts. Yet, there is a lack of effective method to process, store, and exchange information among different analyses. In this paper, we propose differential factbase, a uniform exchangeable representation supporting efficient querying and manipulation, based on the existing concept of program facts. We consider program changes as first-class objects, which establish links between intra-version facts of single program snapshots and provide insights on how certain artifacts evolve over time via inter-version facts. We implement a series of differential fact extractors supporting different programming languages and platforms, and demonstrate with usage scenarios the benefits of adopting differential facts in supporting software evolution management.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {503–515},
numpages = {13},
keywords = {Software evolution, program facts, reverse engineering, software maintenance},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3611643.3616367,
author = {Wan, Zhiyuan and Zhang, Yun and Xia, Xin and Jiang, Yi and Lo, David},
title = {Software Architecture in Practice: Challenges and Opportunities},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616367},
doi = {10.1145/3611643.3616367},
abstract = {Software architecture has been an active research field for nearly four decades, in which previous studies make significant progress such as creating methods and techniques and building tools to support software architecture practice. Despite past efforts, we have little understanding of how practitioners perform software architecture related activities, and what challenges they face. Through interviews with 32 practitioners from 21 organizations across three continents, we identified challenges that practitioners face in software architecture practice during software development and maintenance. We reported on common software architecture activities at software requirements, design, construction and testing, and maintenance stages, as well as corresponding challenges. Our study uncovers that most of these challenges center around management, documentation, tooling and process, and collects recommendations to address these challenges.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1457–1469},
numpages = {13},
keywords = {Grounded Theory, Practice, Software Architecture, Software Development and Maintenance},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@article{10.1145/3691629,
author = {Trinkenreich, Bianca and Santos, Fabio and Stol, Klaas-Jan},
title = {Predicting Attrition among Software Professionals: Antecedents and Consequences of Burnout and Engagement},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {8},
issn = {1049-331X},
url = {https://doi.org/10.1145/3691629},
doi = {10.1145/3691629},
abstract = {In this study of burnout and engagement, we address three major themes. First, we offer a review of prior studies of burnout among IT professionals and link these studies to the Job Demands-Resources (JD-R) model. Informed by the JD-R model, we identify three factors that are organizational job resources and posit that these (a) increase engagement and (b) decrease burnout. Second, we extend the JD-R by considering software professionals’ intention to stay as a consequence of these two affective states, burnout and engagement. Third, we focus on the importance of factors for intention to stay, and actual retention behavior. We use a unique dataset of over 13,000 respondents at one global IT organization, enriched with employment status 90 days after the initial survey. Leveraging partial-least squares structural quation modeling and machine learning, we find that the data mostly support our theoretical model, with some variation across different subgroups of respondents. An importance-performance map analysis suggests that managers may wish to focus on interventions regarding burnout as a predictor of intention to leave. The Machine Learning model suggests that engagement and opportunities to learn are the top two most important factors that explain whether software professionals leave an organization.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {218},
numpages = {45},
keywords = {Organizational leadership, Leadership support, Learning, burnout, engagement, culture, attrition}
}

@inproceedings{10.1145/3583133.3590542,
author = {Langdon, W. B. and Petke, Justyna and Blot, Aymeric and Clark, David},
title = {Genetically Improved Software with fewer Data Cache Misses},
year = {2023},
isbn = {9798400701207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583133.3590542},
doi = {10.1145/3583133.3590542},
abstract = {Using MAGPIE (Machine Automated General Performance Improvement via Evolution of software) we show genetic improvement GI can reduce the cache load of existing computer programs. Cache miss reduction is tested on two industrial open source C programs (Google's Open Location Code OLC and Uber's Hexagonal Hierarchical Spatial Index H3) and two C++ 2D photograph image processing tasks, counting pixels and OpenCV's SEEDS segmentation algorithm. Magpie's patches functionally generalise. In one case they reduce data misses on the highest performance L1 cache by 47%.},
booktitle = {Proceedings of the Companion Conference on Genetic and Evolutionary Computation},
pages = {799–802},
numpages = {4},
keywords = {genetic programming, genetic improvement, local search, SBSE, linear representation, software resilience, automatic code optimisation, tabu, nonstationary noise, perf, world wide location, plus codes, zip code, OpenCV, image segmentation},
location = {Lisbon, Portugal},
series = {GECCO '23 Companion}
}

@inproceedings{10.1145/3597503.3639130,
author = {Keim, Jan and Corallo, Sophie and Fuch\ss{}, Dominik and Hey, Tobias and Telge, Tobias and Koziolek, Anne},
title = {Recovering Trace Links Between Software Documentation And Code},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639130},
doi = {10.1145/3597503.3639130},
abstract = {Introduction Software development involves creating various artifacts at different levels of abstraction and establishing relationships between them is essential. Traceability link recovery (TLR) automates this process, enhancing software quality by aiding tasks like maintenance and evolution. However, automating TLR is challenging due to semantic gaps resulting from different levels of abstraction. While automated TLR approaches exist for requirements and code, architecture documentation lacks tailored solutions, hindering the preservation of architecture knowledge and design decisions. Methods This paper presents our approach TransArC for TLR between architecture documentation and code, using component-based architecture models as intermediate artifacts to bridge the semantic gap. We create transitive trace links by combining the existing approach ArDoCo for linking architecture documentation to models with our novel approach ArCoTL for linking architecture models to code.Results We evaluate our approaches with five open-source projects, comparing our results to baseline approaches. The model-to-code TLR approach achieves an average F1-score of 0.98, while the documentation-to-code TLR approach achieves a promising average F1-score of 0.82, significantly outperforming baselines. Conclusion Combining two specialized approaches with an intermediate artifact shows promise for bridging the semantic gap. In future research, we will explore further possibilities for such transitive approaches.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {215},
numpages = {13},
keywords = {software traceability, software architecture, documentation, transitive links, intermediate artifacts, information retrieval},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE-C.2017.36,
author = {Pereira, Juliana Alves},
title = {A collaborative-based recommender system for configuration of extended product lines},
year = {2017},
isbn = {9781538615898},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-C.2017.36},
doi = {10.1109/ICSE-C.2017.36},
abstract = {Product Line (PL) configuration practices have been employed by industries as a mass customization process. However, due to the NP-hard nature of the process, performance concerns start to be an issue when facing large-scale configuration spaces. The aim of my doctoral research is therefore to propose an efficient collaborative-based recommender system that provides accurate and scalable solutions to users. To demonstrate the efficiency of the proposed recommender system, I will conduct series of experiments on real-world extended PLs. In addition, I plan empirically to verify through a user case study the usability of the proposed approach. My expected contribution is to support the adoption of PL configuration practices in industrial scenarios.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering Companion},
pages = {445–448},
numpages = {4},
location = {Buenos Aires, Argentina},
series = {ICSE-C '17}
}

@article{10.1145/3655022,
author = {Perera, Anjana and Turhan, Burak and Aleti, Aldeida and B\"{o}hme, Marcel},
title = {On the Impact of Lower Recall and Precision in Defect Prediction for Guiding Search-based Software Testing},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3655022},
doi = {10.1145/3655022},
abstract = {Defect predictors, static bug detectors, and humans inspecting the code can propose locations in the program that are more likely to be buggy before they are discovered through testing. Automated test generators such as search-based software testing (SBST) techniques can use this information to direct their search for test cases to likely buggy code, thus speeding up the process of detecting existing bugs in those locations. Often the predictions given by these tools or humans are imprecise, which can misguide the SBST technique and may deteriorate its performance. In this article, we study the impact of imprecision in defect prediction on the bug detection effectiveness of SBST.Our study finds that the recall of the defect predictor, i.e., the proportion of correctly identified buggy code, has a significant impact on bug detection effectiveness of SBST with a large effect size. More precisely, the SBST technique detects 7.5 fewer bugs on average (out of 420 bugs) for every 5% decrements of the recall. However, the effect of precision, a measure for false alarms, is not of meaningful practical significance, as indicated by a very small effect size.In the context of combining defect prediction and SBST, our recommendation is to increase the recall of defect predictors as a primary objective and precision as a secondary objective. In our experiments, we find that 75% precision is as good as 100% precision. To account for the imprecision of defect predictors, in particular low recall values, SBST techniques should be designed to search for test cases that also cover the predicted non-buggy parts of the program, while prioritising the parts that have been predicted as buggy.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {144},
numpages = {27},
keywords = {Search-based software testing, automated test generation, defect prediction}
}

@inproceedings{10.1145/3340555.3353729,
author = {Miura, Go and Okada, Shogo},
title = {Task-independent Multimodal Prediction of Group Performance Based on Product Dimensions},
year = {2019},
isbn = {9781450368605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340555.3353729},
doi = {10.1145/3340555.3353729},
abstract = {This paper proposes an approach to develop models for predicting the performance for multiple group meeting tasks, where the model has no clear correct answer. This paper adopts ”product dimensions” [Hackman et al. 1967] (PD) which is proposed as a set of dimensions for describing the general properties of written passages that are generated by a group, as a metric measuring group output. This study enhanced the group discussion corpus called the MATRICS corpus including multiple discussion sessions by annotating the performance metric of PD. We extract group-level linguistic features including vocabulary level features using a word embedding technique, topic segmentation techniques, and functional features with dialog act and parts of speech on the word level. We also extracted nonverbal features from the speech turn, prosody, and head movement. With a corpus including multiple discussion data and an annotation of the group performance, we conduct two types of experiments thorough regression modeling to predict the PD. The first experiment is to evaluate the task-dependent prediction accuracy, in the situation that the samples obtained from the same discussion task are included in both the training and testing. The second experiments is to evaluate the task-independent prediction accuracy, in the situation that the type of discussion task is different between the training samples and testing samples. In this situation, regression models are developed to infer the performance in an unknown discussion task. The experimental results show that a support vector regression model archived a 0.76 correlation in the discussion-task-dependent setting and 0.55 in the task-independent setting.},
booktitle = {2019 International Conference on Multimodal Interaction},
pages = {264–273},
numpages = {10},
keywords = {Group Analysis, Group performance, Multimodal},
location = {Suzhou, China},
series = {ICMI '19}
}

@inproceedings{10.1145/2833312.2849557,
author = {Qu\'{e}va, Caroline and Courouss\'{e}, Damien and Charles, Henri-Pierre},
title = {Self-optimisation using runtime code generation for wireless sensor networks},
year = {2016},
isbn = {9781450340328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2833312.2849557},
doi = {10.1145/2833312.2849557},
abstract = {This paper addresses the use of runtime code specialisation in resource-constrained embedded systems such as nodes of a Wireless Sensor Network (WSN), in order to improve software efficiency, hence the lifetime of WSN nodes. In our approach, runtime code specialisation is achieved with inplace runtime code generation.We present a self-optimising system using runtime code generation. Our system is able to automatically make the decision to generate specialised code and use it each time an improvement is observed in application performance. In the Internet of Things (IoT), devices usually have limited precision; our system adapts to theses devices decreasing precision in order to increase performance. We evaluate our system on floating point multiplication using the WisMote platform, where the specialised code executes more than 7 times faster than generic code, all overheads included. To the best of our knowledge, it is the first time that a runtime code generation system is used to automatically optimise code in such constrained devices as WSN nodes.},
booktitle = {Proceedings of the 17th International Conference on Distributed Computing and Networking},
articleno = {36},
numpages = {6},
keywords = {IoT, compilation, embedded software, runtime code generation, runtime code specialisation, self-optimisation},
location = {Singapore, Singapore},
series = {ICDCN '16}
}

@inproceedings{10.1145/3689937.3695790,
author = {Ortega, Eric and Harris, Scott and Buitrago, Uriel and Nguyen, Tung and Zeng, Hui},
title = {Experiences and Lessons Learned for Late-Stage Software Transformation and Customization},
year = {2024},
isbn = {9798400712333},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689937.3695790},
doi = {10.1145/3689937.3695790},
abstract = {The Office of Naval Research (ONR) Total Platform Cyber Protection (TPCP) program has led to the development of numerous tools for late-stage software customization and debloating. These tools were originally developed by ONR-sponsored academic research to remove unused code and unwanted features from software as well as harden containers. Several performers were funded by this project to generalize and mature selected late-stage software transformation and customization tools. In this paper, we distill the experiences we accumulated as the developers on the project into lessons learned, and present them through a case study on Java debloating. We provide several key recommendations to inform future work and considerations to be made when transitioning academic prototypes of systems security tools to real world use.},
booktitle = {Proceedings of the 2024 Workshop on Forming an Ecosystem Around Software Transformation},
pages = {7–10},
numpages = {4},
keywords = {attack surface reduction, debloat, late-stage customization, security, software, transformation},
location = {Salt Lake City, UT, USA},
series = {FEAST '24}
}

@article{10.1145/3717413.3717420,
author = {Riedel, Tobias and Hauschke, Carl and Schmeck, Hartmut},
title = {Power-Dependent Price Profiles - Defining Grid- and Market-Oriented Incentives for Building Energy Management Systems},
year = {2025},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/3717413.3717420},
doi = {10.1145/3717413.3717420},
abstract = {In order to consider the grid- and market-oriented situation of the energy system and provide corresponding price signals for building energy management systems (BEMS), we propose time- and power-dependent price profiles. Those incentive signals are meant to allow the optimization of BEMS towards dynamic prices while providing a clear incentive to limit power during grid congestion. We evaluate those signals with a generic battery optimizer that is capable of optimizing towards them. Simulations show that those time- and power-dependent grid signals can reduce peak loads in distribution grids while at the same time allowing more market orientation than volumetric dynamic grid fees.},
journal = {SIGENERGY Energy Inform. Rev.},
month = feb,
pages = {78–87},
numpages = {10},
keywords = {BEMS, EMS, dynamic grid fees, energy management, grid-orientation, incentive signals, market-orientation, optimization}
}

@article{10.1145/3715111,
author = {Abrah\~{a}o, Silvia and Grundy, John and Pezz\`{e}, Mauro and Storey, Margaret-Anne and Andrew Tamburri, Damian},
title = {Software Engineering by and for Humans in an AI Era},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715111},
doi = {10.1145/3715111},
abstract = {The landscape of software engineering is undergoing a transformative shift driven by advancements in machine learning, artificial intelligence (AI), and autonomous systems. This roadmap paper explores how these technologies are reshaping the field, positioning humans not only as end users but also as critical components within expansive software ecosystems. We examine the challenges and opportunities arising from this human-centered paradigm, including ethical considerations, fairness, and the intricate interplay between technical and human factors. By recognizing humans at the heart of the software lifecycle —spanning professional engineers, end users, and end-user developers —we emphasize the importance of inclusivity, human-aligned workflows, and the seamless integration of AI-augmented socio-technical systems. As software systems evolve to become more intelligent and human-centric, software engineering practices must adapt to this new reality. This paper provides a comprehensive examination of this transformation, outlining current trends, key challenges, and opportunities that define the emerging research and practice landscape, and envisioning a future where software engineering and AI work synergistically to place humans at the core of the ecosystem.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb
}

@inbook{10.5555/3712729.3712838,
author = {Shashaani, Sara},
title = {Simulation Optimization: An Introductory Tutorial on Methodology},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {With an upward trend for use in real-world problems of high uncertainty, the field of simulation optimization (SO) is evolving to aid in finding near-optimal solutions more rapidly and reliably. A comprehensive overview of the vast and diverse literature in continuous and discrete SO over large spaces is difficult. This short tutorial intends instead to introduce the methodological landscape, stirring a middle ground between statistical analysis of Monte Carlo sampling and mathematical analysis of numerical optimization. Particular attention to sampling and its impact on SO Algorithms highlights open and promising research directions.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1308–1322},
numpages = {15}
}

@inproceedings{10.1145/3559009.3569674,
author = {Xu, Yufan and Yuan, Qiwei and Barton, Erik Curtis and Li, Rui and Sadayappan, P. and Sukumaran-Rajam, Aravind},
title = {Effective Performance Modeling and Domain-Specific Compiler Optimization of CNNs for GPUs},
year = {2023},
isbn = {9781450398688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3559009.3569674},
doi = {10.1145/3559009.3569674},
abstract = {The Convolutional Neural Network (CNN) kernel is a fundamental building block for deep learning, which dominates the computational cost of deep learning pipelines for image analysis. The synthesis of high-performance GPU kernels for CNNs is thus of considerable interest. The current state-of-the-art in optimizing CNN kernels is auto-tuning search using AutoTVM/Ansor, which has been shown to achieve higher performance than vendor libraries as well as polyhedral compilers. A primary reason for the failure of general-purpose optimizing compilers to deliver high-performance code for key kernels like CNN is the challenge of accurate performance modeling to enable effective choice among alternative transformations and/or parameter values such as tile sizes. In this paper we ask if a domain-specific compiler that is customized for the important CNN kernel can be more effective. Our results show that it can be very effective, enabling even higher performance of the generated GPU code for CNNs than auto-tuning with TVM/Ansor. Further, we demonstrate the effectiveness of a performance modeling approach that integrates analytical modeling of data movement volume with machine learning for offline training, enabling much more rapid code optimization than the approach of TVM/Ansor that is based on online construction of a machine learning model to guide auto-tuning search.},
booktitle = {Proceedings of the International Conference on Parallel Architectures and Compilation Techniques},
pages = {252–264},
numpages = {13},
keywords = {CNN, GPU, design space exploration, performance modeling, tile size optimization},
location = {Chicago, Illinois},
series = {PACT '22}
}

@inproceedings{10.5555/3712729.3713010,
author = {Eun, Hyung-Khee and Shashaani, Sara and Barton, Russell R.},
title = {Comparative Analysis of Distance Metrics for Distributionally Robust Optimization in Queuing Systems: Wasserstein vs. Kingman},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {This study examines the effectiveness of different metrics in constructing ambiguity sets for Distributionally Robust Optimization (DRO). Two main approaches for building ambiguity sets are the moment- and the discrepancy-based approaches. The latter is more widely adopted because it incorporates a broader range of distributional information beyond moments. Among discrepancy-based metrics, the Wasserstein distance is often preferred for its advantageous properties over ϕ-divergence. In this study, we propose a moment-based Kingman distance, an approximation of mean waiting time in G/G/1 queues, to determine the ambiguity set. We demonstrate that the Kingman distance provides a straightforward and efficient method for identifying worst-case scenarios for simple queue settings. In contrast, the Wasserstein distance requires exhaustive exploration of the entire ambiguity set to pinpoint the worst-case distributions. These findings suggest that the Kingman distance could offer a practical and effective alternative for DRO applications in some cases.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3368–3379},
numpages = {12},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@inproceedings{10.1109/CGO51591.2021.9370310,
author = {Liu, Ji and Bello, Luciano and Zhou, Huiyang},
title = {Relaxed peephole optimization: a novel compiler optimization for quantum circuits},
year = {2021},
isbn = {9781728186139},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CGO51591.2021.9370310},
doi = {10.1109/CGO51591.2021.9370310},
abstract = {As in classical computing, compilers play an important role in quantum computing. Quantum processors typically support a limited set of primitive operations or quantum gates and have certain hardware-related limitations. A quantum compiler is responsible for adapting a quantum program to these constraint environments and decomposing quantum gates into a sequence of the primitive ones. During the compilation process, it is also critical for the compiler to optimize the quantum circuits in order to reduce the noise in the computation results. Since the noise is introduced by operations and decoherence, reducing the gate count is the key for improving performance.In this paper, we propose a novel quantum compiler optimization, named relaxed peephole optimization (RPO) for quantum computers. RPO leverages the single-qubit state information that can be determined statically by the compiler. We define that a qubit is in a basis state when, at a given point in time, its state is either in the X-, Y-, or Z-basis (|+〉 / |-〉, |L〉 / |R〉 and |0〉 / |1〉). When basis qubits are used as inputs to quantum gates, there exist opportunities for strength reduction, which replaces quantum operations with equivalent but less expensive ones. Compared to the existing peephole optimization for quantum programs, the difference is that our proposed optimization does not require an identical unitary matrix, thereby named 'relaxed' peephole optimization. We also extend our approach to optimize the quantum gates when some input qubits are in known pure states. Both optimizations, namely the Quantum Basis-state Optimization (QBO) and the Quantum Pure-state Optimization (QPO), are implemented in the IBM's Qiskit transpiler. Our experimental results show that our proposed optimization pass is fast and effective. The circuits optimized with our compiler optimizations obtain up to 18.0% (11.7% on average) fewer CNOT gates and up to 8.2% (7.1% on average) lower transpilation time than that of the most aggressive optimization level in the Qiskit compiler. When running on real quantum computers, the success rates of 3-qubit quantum phase estimation algorithm improve by 2.30X due to the reduced gate counts.},
booktitle = {Proceedings of the 2021 IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {301–314},
numpages = {14},
keywords = {peephole optimization, quantum computing},
location = {Virtual Event, Republic of Korea},
series = {CGO '21}
}

@inproceedings{10.1145/3630048.3630187,
author = {Malinovsky, Grigory and Mishchenko, Konstantin and Richt\'{a}rik, Peter},
title = {Server-Side Stepsizes and Sampling Without Replacement Provably Help in Federated Optimization},
year = {2023},
isbn = {9798400704475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630048.3630187},
doi = {10.1145/3630048.3630187},
abstract = {We present a theoretical study of server-side optimization in federated learning. Our results are the first to show that the widely popular heuristic of scaling the client updates with an extra parameter is very useful in the context of Federated Averaging (FedAvg) with local passes over the client data. Each local pass is performed without replacement using Random Reshuffling, which is a key reason we can show improved complexities. In particular, we prove that whenever the local stepsizes are small, and the update direction is given by FedAvg in conjunction with Random Reshuffling over all clients, one can take a big leap in the obtained direction and improve rates for convex, strongly convex, and non-convex objectives. In particular, in non-convex regime we get an enhancement of the rate of convergence from &lt;scr&gt;O&lt;/scr&gt; (ε−3) to &lt;scr&gt;O&lt;/scr&gt; (ε−2). This result is new even for Random Reshuffling performed on a single node. In contrast, if the local stepsizes are large, we prove that the noise of client sampling can be controlled by using a small server-side stepsize. To the best of our knowledge, this is the first time that local steps provably help to overcome the communication bottleneck. Together, our results on the advantage of large and small server-side stepsizes give a formal justification for the practice of adaptive server-side optimization in federated learning. Moreover, we consider a variant of our algorithm that supports partial client participation, which makes the method more practical.},
booktitle = {Proceedings of the 4th International Workshop on Distributed Machine Learning},
pages = {85–104},
numpages = {20},
keywords = {distributed optimization, federated learning},
location = {Paris, France},
series = {DistributedML '23}
}

@inproceedings{10.1145/3698364.3709119,
author = {Kim, Taewhan},
title = {Invited: Physical Design Challenges for Design Technology Co-optimization},
year = {2025},
isbn = {9798400712937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698364.3709119},
doi = {10.1145/3698364.3709119},
abstract = {Design technology co-optimization (DTCO) is the process of optimizing design and process technology together to enhance performance, power efficiency, chip utilization, and manufacturing cost/yield. Through DTCO, we are able to evaluate technologies, design rules, and cell architectures using block-level PPA (performance, power, area) analysis, which greatly helps semiconductor fabs reduce cost and shorten time-to-market in advanced process development with substantial architectural innovation. The parameters that DTCO targets to evaluate include design rules (e.g., gate poly pitch, M1 pitch, side-to-top spacing rule, via-enclosure rule), cell architectures (e.g., single-row or multi-row-height, 2D or 1D M1, preference of metal direction), and technologies (e.g., Fin-FET, Nanosheet-FET, Complementary-FET), which are collectively called DTCO parameters.},
booktitle = {Proceedings of the 2025 International Symposium on Physical Design},
pages = {20–21},
numpages = {2},
keywords = {automatic cell layout generation, design and technology co-optimization, physical design., standard cells},
location = {Austin, TX, USA},
series = {ISPD '25}
}

@proceedings{10.1145/3687997,
title = {SLE '24: Proceedings of the 17th ACM SIGPLAN International Conference on Software Language Engineering},
year = {2024},
isbn = {9798400711800},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 17th ACM SIGPLAN International Conference on Software Language Engineering (SLE), held in Pasadena, California, USA, October 20–21 2024, as part of SPLASH 2024. The SLE conference is devoted to the principles of software languages: their design, their implementation, and their evolution.},
location = {Pasadena, CA, USA}
}

@inproceedings{10.1145/3581784.3607035,
author = {Li, Baolin and Basu Roy, Rohan and Wang, Daniel and Samsi, Siddharth and Gadepally, Vijay and Tiwari, Devesh},
title = {Toward Sustainable HPC: Carbon Footprint Estimation and Environmental Implications of HPC Systems},
year = {2023},
isbn = {9798400701092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581784.3607035},
doi = {10.1145/3581784.3607035},
abstract = {The rapid growth in demand for HPC systems has led to a rise in carbon footprint, which requires urgent intervention. In this work, we present a comprehensive analysis of the carbon footprint of highperformance computing (HPC) systems, considering the carbon footprint during both the hardware manufacturing and system operational stages. Our work employs HPC hardware component carbon footprint modeling, regional carbon intensity analysis, and experimental characterization of the system life cycle to highlight the importance of quantifying the carbon footprint of HPC systems.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {19},
numpages = {15},
keywords = {carbon footprint, sustainability},
location = {Denver, CO, USA},
series = {SC '23}
}

@proceedings{10.1145/3643655,
title = {SESoS '24: Proceedings of the 12th ACM/IEEE International Workshop on Software Engineering for Systems-of-Systems and Software Ecosystems},
year = {2024},
isbn = {9798400705571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {SESoS 2024 will provide a forum for researchers and practitioners with a forum to exchange ideas and experiences, analyze research and development issues, discuss promising solutions, and propose theoretical foundations for the development and evolution of complex software-intensive systems.},
location = {Lisbon, Portugal}
}

@article{10.1145/3664607,
author = {Jiang, Siyu and He, Zhenhang and Chen, Yuwen and Zhang, Mingrong and Ma, Le},
title = {Mobile Application Online Cross-Project Just-in-Time Software Defect Prediction Framework},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3664607},
doi = {10.1145/3664607},
abstract = {As mobile applications evolve rapidly, their fast iterative update nature leads to an increase in software defects. Just-In-Time Software Defect Prediction (JIT-SDP) offers immediate feedback on code changes. For new applications without historical data, researchers have proposed Cross-Project JIT-SDP (CP JIT-SDP). Existing CP JIT-SDP approaches are designed for offline scenarios where target data is available in advance. However, target data in real-world applications usually arrives online in a streaming manner, making online CP JIT-SDP face cross-project distribution differences and target project data concept drift challenges in online scenarios. These challenges often co-exist during application development, and their interactions cause model performance to degrade. To address these issues, we propose an online CP JIT-SDP framework called COTL. Specifically, COTL consists of two stages: offline and online. In the offline stage, the cross-domain structure preserving projection algorithm is used to reduce the cross-project distribution differences. In the online stage, target data arrives sequentially over time. By reducing the differences in marginal and conditional distributions between offline and online data for target project, concept drift is mitigated and classifier weights are updated online. Experimental results on 15 mobile application benchmark datasets show that COTL outperforms 13 benchmark methods on four performance metrics.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {157},
numpages = {31},
keywords = {Online transfer learning, mobile applications bug prediction, cross-project just-in-time software defect prediction, concept drift}
}

@inproceedings{10.1145/3660853.3660917,
author = {Moric, Zlatan and Zak\v{s}ek, Ljiljana and Dakic, Vedran},
title = {Data-Driven PPC Optimization: The Impact of Text Mining on Campaign Performance in Dental Tourism},
year = {2024},
isbn = {9798400716928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660853.3660917},
doi = {10.1145/3660853.3660917},
abstract = {This study examines the impact of utilizing text mining insights on the efficiency of Pay-Per-Click (PPC) campaigns within the dental tourism industry. With the advent of data-driven marketing strategies, the potential of text analysis to enhance PPC campaign performance has garnered significant interest. This research employed a mixed-methods approach, combining qualitative text mining of profitable client inquiries with quantitative analysis of PPC campaign metrics before and after the application of derived insights. Using the open-source software Orange for text mining and Google AdWords for campaign testing, the study revealed that campaigns optimized with text analysis insights significantly outperformed traditional campaigns. Key improvements were observed in click-through rates (CTR) by 25%, conversion rates by 15%, and a 10% reduction in cost-per-click (CPC), highlighting the strategic value of aligning ad content with the identified preferences and language of the target audience. These findings underscore the importance of leveraging deep audience understanding to enhance ad relevance and engagement, suggesting a paradigm shift towards more nuanced, data driven PPC strategies. The study advocates for the integration of text mining into digital advertising efforts, offering a scalable model for improving the effectiveness and efficiency of PPC campaigns across various sectors. Future research directions include exploring the applicability of text mining insights across different industries, assessing long-term impacts on customer lifetime value, and developing sophisticated algorithms for automated campaign optimization.},
booktitle = {Proceedings of the Cognitive Models and Artificial Intelligence Conference},
pages = {214–218},
numpages = {5},
location = {undefinedstanbul, Turkiye},
series = {AICCONF '24}
}

@article{10.1145/3625004,
author = {Ferrari, Victor and Sousa, Rafael and Pereira, Marcio and L. De Carvalho, Jo\~{a}o P. and Amaral, Jos\'{e} Nelson and Moreira, Jos\'{e} and Araujo, Guido},
title = {Advancing Direct Convolution Using Convolution Slicing Optimization and ISA Extensions},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3625004},
doi = {10.1145/3625004},
abstract = {Convolution is one of the most computationally intensive operations that must be performed for machine learning model inference. A traditional approach to computing convolutions is known as the Im2Col + BLAS method. This article proposes SConv: a direct-convolution algorithm based on an MLIR/LLVM code-generation toolchain that can be integrated into machine-learning compilers. This algorithm introduces: (a) Convolution Slicing Analysis (CSA)—a convolution-specific 3D cache-blocking analysis pass that focuses on tile reuse over the cache hierarchy; (b) Convolution Slicing Optimization—a code-generation pass that uses CSA to generate a tiled direct-convolution macro-kernel; and (c) Vector-based Packing—an architecture-specific optimized input-tensor packing solution based on vector-register shift instructions for convolutions with unitary stride. Experiments conducted on 393 convolutions from full ONNX-MLIR machine learning models indicate that the elimination of the Im2Col transformation and the use of fast packing routines result in a total packing time reduction, on full model inference, of 2.3\texttimes{}–4.0\texttimes{} on Intel x86 and 3.3\texttimes{}–5.9\texttimes{} on IBM POWER10. The speed-up over an Im2Col + BLAS method based on current BLAS implementations for end-to-end machine-learning model inference is in the range of 11%–27% for Intel x86 and 11%–34% for IBM POWER10 architectures. The total convolution speedup for model inference is 13%–28% on Intel x86 and 23%–39% on IBM POWER10. SConv&nbsp;also outperforms BLAS GEMM, when computing pointwise convolutions in more than 82% of the 219 tested instances.},
journal = {ACM Trans. Archit. Code Optim.},
month = dec,
articleno = {54},
numpages = {26},
keywords = {Convolution, packing, cache blocking, compilers}
}

@proceedings{10.1145/3650212,
title = {ISSTA 2024: Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 33rd edition of the International Symposium on Software Testing and Analysis, ISSTA 2024, held on September 16--20, 2024 in Vienna, Austria. ISSTA 2024 is co-located with ECOOP and MPLR 2024. ISSTA brings together academics, industrial researchers, and practitioners from all over the world working on testing and analyzing software systems.},
location = {Vienna, Austria}
}

@proceedings{10.1145/3643991,
title = {MSR '24: Proceedings of the 21st International Conference on Mining Software Repositories},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {MSR is a thriving research community that organizes a yearly conference with a solid reputation amongst software engineering researchers.},
location = {Lisbon, Portugal}
}

@article{10.1145/3702986,
author = {Gong, Jingzhi and Chen, Tao},
title = {Deep Configuration Performance Learning: A Systematic Survey and Taxonomy},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3702986},
doi = {10.1145/3702986},
abstract = {Performance is arguably the most crucial attribute that reflects the quality of a configurable software system. However, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance. As such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning. In this article, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 1,206 searched papers spanning six indexing services, based on which 99 primary papers were extracted and analyzed. Our results outline key statistics, taxonomy, strengths, weaknesses, and optimal usage scenarios for techniques related to the preparation of configuration data, the construction of deep learning performance models, the evaluation of these models, and their utilization in various software configuration-related tasks. We also identify the good practices and potentially problematic phenomena from the studies surveyed, together with a comprehensive summary of actionable suggestions and insights into future opportunities within the field. To promote open science, all the raw results of this survey can be accessed at our repository: .},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {25},
numpages = {62},
keywords = {Configuration Performance, Deep Learning, Configurable Software, Performance Modeling, Performance Prediction, Software Engineering}
}

@inproceedings{10.1145/3472674.3473980,
author = {Fortz, Sophie and Temple, Paul and Devroey, Xavier and Heymans, Patrick and Perrouin, Gilles},
title = {VaryMinions: leveraging RNNs to identify variants in event logs},
year = {2021},
isbn = {9781450386258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472674.3473980},
doi = {10.1145/3472674.3473980},
abstract = {Business processes have to manage variability in their execution, e.g., to deliver the correct building permit in different municipalities. This variability is visible in event logs, where sequences of events are shared by the core process (building permit authorisation) but may also be specific to each municipality. To rationalise resources (e.g., derive a configurable business process capturing all municipalities’ permit variants) or to debug anomalous behaviour, it is mandatory to identify to which variant a given trace belongs. This paper supports this task by training Long Short Term Memory (LSTMs) and Gated Recurrent Units (GRUs) algorithms on two datasets: a configurable municipality and a travel expenses workflow. We demonstrate that variability can be identified accurately (&gt;87%) and discuss the challenges of learning highly entangled variants.},
booktitle = {Proceedings of the 5th International Workshop on Machine Learning Techniques for Software Quality Evolution},
pages = {13–18},
numpages = {6},
keywords = {Configurable processes, Recurrent Neural Networks, Variability Mining},
location = {Athens, Greece},
series = {MaLTESQuE 2021}
}

@article{10.1145/3637225,
author = {Leeson, Will and Dwyer, Matthew B.},
title = {Algorithm&nbsp;Selection for Software Verification Using Graph Neural Networks},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3637225},
doi = {10.1145/3637225},
abstract = {The field of software verification has produced a wide array of algorithmic techniques that can prove a variety of properties of a given program. It has been demonstrated that the performance of these techniques can vary up to 4 orders of magnitude on the same verification problem. Even for verification experts, it is difficult to decide which tool will perform best on a given problem. For general users, deciding the best tool for their verification problem is effectively impossible. In this work, we present Graves, a selection strategy based on graph neural networks (GNNs). Graves generates a graph representation of a program from which a GNN predicts a score for a verifier that indicates its performance on the program. We evaluate Graves on a set of 10 verification tools and over 8,000 verification problems and find that it improves the state-of-the-art in verification algorithm selection by 12%, or 8 percentage points. Further, it is able to verify 9% more problems than any existing verifier on our test set. Through a qualitative study on model interpretability, we find strong evidence that the Graves model learns to base its predictions on factors that relate to the unique features of the algorithmic techniques.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {76},
numpages = {36},
keywords = {Algorithm&nbsp;selection, graph neural networks}
}

@article{10.1145/3652154,
author = {Russo, Daniel},
title = {Navigating the Complexity of Generative AI Adoption in Software Engineering},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3652154},
doi = {10.1145/3652154},
abstract = {This article explores the adoption of Generative Artificial Intelligence (AI) tools within the domain of software engineering, focusing on the influencing factors at the individual, technological, and social levels. We applied a convergent mixed-methods approach to offer a comprehensive understanding of AI adoption dynamics. We initially conducted a questionnaire survey with 100 software engineers, drawing upon the Technology Acceptance Model, the Diffusion of Innovation Theory, and the Social Cognitive Theory as guiding theoretical frameworks. Employing the Gioia methodology, we derived a theoretical model of AI adoption in software engineering: the Human-AI Collaboration and Adaptation Framework. This model was then validated using Partial Least Squares–Structural Equation Modeling based on data from 183 software engineers. Findings indicate that at this early stage of AI integration, the compatibility of AI tools within existing development workflows predominantly drives their adoption, challenging conventional technology acceptance theories. The impact of perceived usefulness, social factors, and personal innovativeness seems less pronounced than expected. The study provides crucial insights for future AI tool design and offers a framework for developing effective organizational implementation strategies.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {135},
numpages = {50},
keywords = {Generative AI, large language models, technology adaption, empirical software engineering}
}

@inproceedings{10.1145/3611643.3616357,
author = {Mikek, Benjamin and Zhang, Qirun},
title = {Speeding up SMT Solving via Compiler Optimization},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616357},
doi = {10.1145/3611643.3616357},
abstract = {SMT solvers are fundamental tools for reasoning about constraints in practical problems like symbolic execution and program synthesis. Faster SMT solving can improve the performance and precision of those analysis tools. Existing approaches typically speed up SMT solving by developing new heuristics inside particular solvers, which requires nontrivial engineering efforts. This paper presents a new perspective on speeding up SMT solving. We propose SMT-LLVM Optimizing Translation (SLOT), a solver-agnostic pre-processing approach that utilizes existing compiler optimizations to simplify SMT problem instances. We implement SLOT for the two most application-critical SMT theories, bitvectors, and floating-point numbers. Our extensive evaluation based on the standard SMT-LIB benchmarks shows that SLOT can substantially increase the number of solvable SMT formulas given fixed timeouts and achieve mean speedups of nearly 3\texttimes{} for large benchmarks.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1177–1189},
numpages = {13},
keywords = {SMT Solvers, compiler optimization, simplification},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3676536.3676775,
author = {Yao, Xufeng and Wang, Yiwen and Li, Xing and Lian, Yingzhao and Chen, Ran and Chen, Lei and Yuan, Mingxuan and Xu, Hong and Yu, Bei},
title = {RTLRewriter: Methodologies for Large Models aided RTL Code Optimization},
year = {2025},
isbn = {9798400710773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676536.3676775},
doi = {10.1145/3676536.3676775},
abstract = {Register Transfer Level (RTL) code optimization is crucial for enhancing the efficiency and performance of digital circuits during early synthesis stages. Currently, optimization relies heavily on manual efforts by skilled engineers, often requiring multiple iterations based on synthesis feedback. In contrast, existing compiler-based methods fall short in addressing complex designs. This paper introduces RTLRewriter, an innovative framework that leverages large models to optimize RTL code. A circuit partition pipeline is utilized for fast synthesis and efficient rewriting. A multi-modal program analysis is proposed to incorporate vital visual diagram information as optimization cues. A specialized search engine is designed to identify useful optimization guides, algorithms, and code snippets that enhance the model's ability to generate optimized RTL. Additionally, we introduce a Cost-aware Monte Carlo Tree Search (C-MCTS) algorithm for efficient rewriting, managing diverse retrieved contents and steering the rewriting results. Furthermore, a fast verification pipeline is proposed to reduce verification cost. To cater to the needs of both industry and academia, we propose two benchmarking suites: the long Rewriter benchmark, targeting complex scenarios with extensive circuit partitioning, optimization trade-offs, and verification challenges, and the short Rewriter benchmark, designed for a wider range of scenarios and patterns. Our comparative analysis with established compilers such as Yosys and E-graph demonstrates significant improvements, highlighting the benefits of integrating large models into the early stages of circuit design. We provide our benchmarks at https://github.com/yaoxufeng/RTLRewriter-Bench.},
booktitle = {Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design},
articleno = {98},
numpages = {7},
location = {Newark Liberty International Airport Marriott, New York, NY, USA},
series = {ICCAD '24}
}

@inproceedings{10.1145/3695794.3695818,
author = {Servodio, Salvatore and Li, Xiaoming},
title = {Memory Efficiency Oriented Fine-Grain Representation and Optimization of FFT},
year = {2024},
isbn = {9798400710919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3695794.3695818},
doi = {10.1145/3695794.3695818},
abstract = {Fast Fourier Transform (FFT) is the computation kernel of many scientific and machine learning applications. Its state-of-the-art implementation in current practices normally involves searching for the best performing variant in a pre-defined implementation space. The expressiveness of the space and the capability to manipulate the variant expressions largely determine the final performance. It is known that large size FFT problems are memory bound. However, existing FFT libraries such as FFTW adopt the search space representations that are oriented for tuning operational efficiency. Such representations make it almost impossible to specifically tune FFT for memory performance. In this paper, we introduce REFFT, a software library capable of computing Discrete Fourier Transforms (DFT) by applying a novel approach to exploring the decomposition space of memory accesses in FFT. This is achieved by creating a new model for representing a Fast Fourier Transform (FFT) by means of a binary tree structure to represent its decomposition and by organizing the subspace of valid solutions into two ordered and directed sets: tree rotations and codelet permutations. Furthermore, in this new model, the nodes of a given FFT binary tree provide computationally meaningful attributes which are derived from its edges. This paper demonstrates that it is possible to easily navigate with fine granularity what would initially seem to be an exceptionally vast space of solutions in a practical manner and find best performing plans by tuning specifically for memory efficiency through mathematical structure manipulations. We compare REFFT with FFTW, one of the most broadly used FFT libraries, for problem sizes ranging from 215 to 225 on three recent Intel and AMD processors. REFFT achieves the average speedup of  (14.3%)  across all sizes with max speedup  (43.1%)},
booktitle = {Proceedings of the International Symposium on Memory Systems},
pages = {245–256},
numpages = {12},
location = {
},
series = {MEMSYS '24}
}

@inproceedings{10.1145/3611643.3613873,
author = {Mockus, Audris and Rigby, Peter C. and Abreu, Rui and Suresh, Parth and Chen, Yifen and Nagappan, Nachiappan},
title = {Modeling the Centrality of Developer Output with Software Supply Chains},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613873},
doi = {10.1145/3611643.3613873},
abstract = {Raw developer output, as measured by the number of changes a developer makes to the system, is simplistic and potentially misleading measure of productivity as new developers tend to work on peripheral and experienced developers on more central parts of the system. In this work, we use Software Supply Chain (SSC) networks and Katz centrality and PageRank on these networks to suggest a more nuanced measure of developer productivity. Our SSC is a network that represents the relationships between developers and artifacts that make up a system. We combine author-to-file, co-changing files, call hierarchies, and reporting structure into a single SSC and calculate the centrality of each node. The measures of centrality can be used to better understand variations in the impact of developer output at Meta. We start by partially replicating prior work and show that the raw number of developer commits plateaus over a project-specific period. However, the centrality of developer work grows for the entire period of study, but the growth slows after one year. This implies that while raw output might plateau, more experienced developers work on more central parts of the system. Finally, we investigate the incremental contribution of SSC attributes in modeling developer output. We find that local attributes such as the number of reports and the specific project do not explain much variation (𝑅2 = 5.8%). In contrast, adding Katz centrality or PageRank produces a model with an 𝑅2 above 30%. SSCs and their centrality provide valuable insights into the centrality and importance of a developer’s work.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1809–1819},
numpages = {11},
keywords = {Developer productivity, Software supply chains},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3597503.3639192,
author = {Sa\u{g}lam, Timur and Br\"{o}del, Moritz and Schmid, Larissa and Hahner, Sebastian},
title = {Detecting Automatic Software Plagiarism via Token Sequence Normalization},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639192},
doi = {10.1145/3597503.3639192},
abstract = {While software plagiarism detectors have been used for decades, the assumption that evading detection requires programming proficiency is challenged by the emergence of automated plagiarism generators. These generators enable effortless obfuscation attacks, exploiting vulnerabilities in existing detectors by inserting statements to disrupt the matching of related programs. Thus, we present a novel, language-independent defense mechanism that leverages program dependence graphs, rendering such attacks infeasible. We evaluate our approach with multiple real-world datasets and show that it defeats plagiarism generators by offering resilience against automated obfuscation while maintaining a low rate of false positives.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {113},
numpages = {13},
keywords = {software plagiarism detection, plagiarism obfuscation, obfuscation attacks, code normalization, PDG, tokenization},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3696443.3708927,
author = {Basso, Matteo and Prokopec, Aleksandar and Ros\`{a}, Andrea and Binder, Walter},
title = {Improving Native-Image Startup Performance},
year = {2025},
isbn = {9798400712753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696443.3708927},
doi = {10.1145/3696443.3708927},
abstract = {With the increasing popularity of Serverless computing and Function as a Service---where typical workloads have a short lifetime---the research community is increasingly focusing on startup performance optimization. To reduce the startup time of managed language runtime systems, related work proposes strategies to move runtime environment initialization ahead-of-time. For instance, GraalVM Native Image allows one to create a binary file from a Java application that embeds a snapshot of the pre-initialized heap memory and can run without instantiating a Java Virtual Machine. However, the program startup needs to be further optimized, because the cloud runtime often starts the program while responding to the request. Thus, the program startup time impacts the service-level agreement.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
In this paper, we improve the startup time of Native-Image binaries by changing their layout during compilation, reducing I/O traffic. We propose a profile-guided binary-reordering approach and a profiling methodology to obtain the execution-order profiles of methods and objects. Thanks to these profiles, we first reduce page faults related to the code section. Then, we propose three ordering strategies to reduce page faults related to accessing the objects in the heap snapshot. Since the object identities and the heap-snapshot contents are not persistent across Native-Image builds of the same program, we propose a method of matching objects from the profile against the objects in the profile-guided build. Experimental results show that our ordering strategies lead to an average page-fault reduction factor of 1.61\texttimes{} and an average execution-time speedup of 1.59\texttimes{}.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {689–703},
numpages = {15},
keywords = {Function as a Service, GraalVM, Native Image, Profiling, Serverless Computing, Startup Performance},
location = {Las Vegas, NV, USA},
series = {CGO '25}
}

@inproceedings{10.1145/3639477.3639725,
author = {Groot, Tom and Ochoa Venegas, Lina and Laz\u{a}r, Bogdan and Kr\"{u}ger, Jacob},
title = {A Catalog of Unintended Software Dependencies in Multi-Lingual Systems at ASML},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639725},
doi = {10.1145/3639477.3639725},
abstract = {Multi-lingual software systems build on interconnected components that are implemented in different programming languages. The multi-lingual nature of such systems causes additional complexity, for instance, when developers aim to identify what components of a system use the same data. Organizations and developers typically aim to adhere to a specified system architecture to avoid certain dependencies between multi-lingual components. However, such dependencies may still be introduced and only resolved later on. Thus, we refer to them as unintended dependencies: dependencies that may exist, but are not wanted by the developers or organization. There has been little research on multi-lingual systems so far, and dependencies within such systems have not been studied explicitly. With this paper, we tackle this issue by contributing a catalog of unintended software dependencies in multi-lingual systems. We elicited it by interviewing 17 practitioners at ASML. We report eight types of unintended dependencies, their causes, the resulting problems, and how they can be resolved. Further, we connect our findings to research on software smells and dependencies in monolingual systems. Our contributions serve as recommendations for practitioners on how to deal with unintended dependencies, as supportive evidence for existing research, and as basis for new techniques for managing dependencies in (multi-lingual) systems.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {240–251},
numpages = {12},
keywords = {dependencies, software architecture, multi-lingual systems, software quality, software maintenance},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3544548.3581071,
author = {Gori, Julien and Bellut, Quentin},
title = {Positional Variance Profiles (PVPs): A New Take on the Speed-Accuracy Trade-off},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581071},
doi = {10.1145/3544548.3581071},
abstract = {Fitts’ law is a behavioral model, used to design protocols and analyze data from pointing experiments. These are usually conducted in HCI to evaluate input performance. We recently proposed an alternative method to characterize input performance, called the method of PVPs in 1D, based on 1) a dual-minimization protocol, and 2) an analysis of the variability of entire trajectories. We extend the method in 2D; our contributions include new metrics, a new protocol, and a Python library. We also present the results of a controlled experiment where the new method is validated using three devices (mouse, touchpad, controller): effect sizes in the 2D case replicate those previously found. We also propose a comparison between Fitts’ law and our novel evaluation: the method of PVPs provides more information than Fitts’ law, and can predict its parameters. We discuss how this new method may relieve open problems of Fitts’ law.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {578},
numpages = {16},
keywords = {Fitts’ law, PVP, evaluation, pointing},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3569966.3570010,
author = {Jia, Huihui and Zhang, Cheng and Wu, Sijie},
title = {Multi-objective software test case selection based on density analysis},
year = {2022},
isbn = {9781450397780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3569966.3570010},
doi = {10.1145/3569966.3570010},
abstract = {Software test case selection is committed to select the fewest test cases from test suites to perform a complete test at the least cost. Machine learning and multi-objective optimization techniques have developed rapidly in recent years, and they have been successfully applied to test case selection. In this paper, we present a method called DB-NSGA2, which uses the density clustering algorithm in machine learning combined with the non-dominated ranking algorithm (NSGA2) for test case selection, which can better select the test cases required for testing. In particular, we apply some of the clustering results generated by the clustering algorithm to the crossover and mutation operations of the NSGA2 to improve diversity progeny populations and ensure the transmission of good individuals. Extensive experiments show that the test cases selected by our method can produce a better set of Pareto solutions and can detect more faults at a lower cost than other methods.},
booktitle = {Proceedings of the 5th International Conference on Computer Science and Software Engineering},
pages = {140–147},
numpages = {8},
keywords = {Crossover, DBSCAN, Multi-objective Optimization, Mutation, Test Case Selection},
location = {Guilin, China},
series = {CSSE '22}
}

@inproceedings{10.1145/3149827.3149839,
author = {Wu, Wenqiang and Zhang, Chunliang},
title = {Shearing Process Design and Optimization for NC Transverse Shear Line},
year = {2017},
isbn = {9781450353397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3149827.3149839},
doi = {10.1145/3149827.3149839},
abstract = {Transverse shear line is used to process the silicon steel sheet for transformer. There have some shortcomings of traditional ones, such as programming difficulties, low degree of flexibility, poor scalability, high labor intensity, low efficiency, and so on. In order to solve those problems, the design and optimization method of shearing process for transverse shear line is presented in this paper. Firstly, the mechanical structure and the TPN model of the NC transverse shear line with multi-shears and multi-punches are analyzed and established. Then, the graphical sheet design method and process optimization process with the purpose to increase processing efficiency are introduced. Finally, based on one type of transverse shear line with two-shears and three-punches and one normal sheet edited by the graphical design software, a processing optimization example is implemented to demonstrate the feasibility and effectiveness of the process design and optimization method.},
booktitle = {Proceedings of the 2017 The 5th International Conference on Control, Mechatronics and Automation},
pages = {67–72},
numpages = {6},
keywords = {Transverse shear line, process design, process optimization},
location = {Edmonton, AB, Canada},
series = {ICCMA 2017}
}

@inproceedings{10.1145/3643660.3643941,
author = {Shaw, Mary and Petre, Marian},
title = {Design Spaces and How Software Designers Use Them: a sampler},
year = {2024},
isbn = {9798400705632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643660.3643941},
doi = {10.1145/3643660.3643941},
abstract = {Discussions of software design often refer to using "design spaces" to describe the spectrum of available design alternatives. This supports design thinking in many ways: to capture domain knowledge, to support a wide variety of design activity, to analyze or predict properties of alternatives, to understand interactions and dependencies among design choices. We present a sampling of what designers, especially software designers, mean when they say "design space" and provide examples of the roles their design spaces serve in their design activity. This shows how design spaces can serve designers as lenses to reduce the overall space of possibilities and support systematic design decision making.},
booktitle = {Proceedings of the 1st International Workshop on Designing Software},
pages = {1–8},
numpages = {8},
keywords = {design spaces, software design, design exploration},
location = {Lisbon, Portugal},
series = {Designing '24}
}

@article{10.1145/3659207,
author = {Liu, Qunyou and Huang, Darong and Costero, Luis and Zapater, Marina and Atienza, David},
title = {Intermediate Address Space: virtual memory optimization of heterogeneous architectures for cache-resident workloads},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1544-3566},
url = {https://doi.org/10.1145/3659207},
doi = {10.1145/3659207},
abstract = {The increasing demand for computing power and the emergence of heterogeneous computing architectures have driven the exploration of innovative techniques to address current limitations in both the compute and memory subsystems. One such solution is the use of Accelerated Processing Units (APUs), processors that incorporate both a central processing unit (CPU) and an integrated graphics processing unit (iGPU). However, the performance of both APU and CPU systems can be significantly hampered by address translation overhead, leading to a decline in overall performance, especially for cache-resident workloads. To address this issue, we propose the introduction of a new intermediate address space (IAS) in both APU and CPU systems. IAS serves as a bridge between virtual address (VA) spaces and physical address (PA) spaces, optimizing the address translation process. In the case of APU systems, our research indicates that the iGPU suffers from significant translation look-aside buffer (TLB) misses in certain workload situations. Using an IAS, we can divide the initial address translation into front- and back-end phases, effectively shifting the bottleneck in address translation from the cache side to the memory controller side, a technique that proves to be effective for cache-resident workloads. Our simulations demonstrate that implementing IAS in the CPU system can boost performance by up to 40% compared to conventional CPU systems. Furthermore, we evaluate the effectiveness of APU systems, comparing the performance of IAS-based systems with traditional systems, showing up to a 185% improvement in APU system performance with our proposed IAS implementation. Furthermore, our analysis indicates that over 90% of TLB misses can be filtered by the cache, and employing a larger cache within the system could potentially result in even greater improvements. The proposed IAS offers a promising and practical solution to enhance the performance of both APU and CPU systems, contributing to state-of-the-art research in the field of computer architecture.},
journal = {ACM Trans. Archit. Code Optim.},
month = sep,
articleno = {50},
numpages = {23},
keywords = {Computer architecture, CPU, GPU, virtual memory, TLB, cache}
}

@inproceedings{10.1145/3512353.3512372,
author = {Zheng, Jiahe and Zhang, Xianmin and Li, Hai and Huang, Yanjiang},
title = {High-efficiency Transmission of Industrial Heterogeneous Data in a Typical Mobile Phone Assembly Production Line},
year = {2022},
isbn = {9781450395571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512353.3512372},
doi = {10.1145/3512353.3512372},
abstract = {To address the problems of multiple types of data and large differences in data collection frequency that exist in the data transmission of typical cell phone assembly lines, this research investigated the efficient transmission method of multi-source industrial heterogeneous data based on OPC UA technology. First, the differences in data transmission mechanisms between OPC UA servers and clients in terms of transmission traffic were studied. Then, the production line data are classified according to the update frequency and data volume, and the transmission strategy that should be adopted for each type of data are concluded. Subsequently, a transmission method that can dynamically adjust the information release frequency of the OPC UA server based on the current traffic load is proposed. Finally, a data transmission compression information model is constructed for the SCARA robot (which performs a large number of assembly tasks on the production line), the performance of the proposed method is evaluated through simulation experiments as well.},
booktitle = {Proceedings of the 2022 4th Asia Pacific Information Technology Conference},
pages = {131–137},
numpages = {7},
keywords = {Information Model, OPC UA, RAMI4.0, SCARA robot},
location = {Virtual Event, Thailand},
series = {APIT '22}
}

@inproceedings{10.1145/3664646.3664773,
author = {Coutinho, Mariana and Marques, Lorena and Santos, Anderson and Dahia, Marcio and Fran\c{c}a, Cesar and de Souza Santos, Ronnie},
title = {The Role of Generative AI in Software Development Productivity: A Pilot Case Study},
year = {2024},
isbn = {9798400706851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664646.3664773},
doi = {10.1145/3664646.3664773},
abstract = {With software development increasingly reliant on innovative technologies, there is a growing interest in exploring the potential of generative AI tools to streamline processes and enhance productivity. In this scenario, this paper investigates the integration of generative AI tools within software development, focusing on understanding their uses, benefits, and challenges to software professionals, in particular, looking at aspects of productivity. Through a pilot case study involving software practitioners working in different roles, we gathered valuable experiences on the integration of generative AI tools into their daily work routines. Our findings reveal a generally positive perception of these tools in individual productivity while also highlighting the need to address identified limitations. Overall, our research sets the stage for further exploration into the evolving landscape of software development practices with the integration of generative AI tools.},
booktitle = {Proceedings of the 1st ACM International Conference on AI-Powered Software},
pages = {131–138},
numpages = {8},
keywords = {LLMs, generative AI, productivity, software engineering},
location = {Porto de Galinhas, Brazil},
series = {AIware 2024}
}

@article{10.1145/3204459,
author = {Chen, Tao and Li, Ke and Bahsoon, Rami and Yao, Xin},
title = {FEMOSAA: Feature-Guided and Knee-Driven Multi-Objective Optimization for Self-Adaptive Software},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3204459},
doi = {10.1145/3204459},
abstract = {Self-Adaptive Software (SAS) can reconfigure itself to adapt to the changing environment at runtime, aiming to continually optimize conflicted nonfunctional objectives (e.g., response time, energy consumption, throughput, cost, etc.). In this article, we present Feature-guided and knEe-driven Multi-Objective optimization for Self-Adaptive softwAre (FEMOSAA), a novel framework that automatically synergizes the feature model and Multi-Objective Evolutionary Algorithm (MOEA) to optimize SAS at runtime. FEMOSAA operates in two phases: at design time, FEMOSAA automatically transposes the engineers’ design of SAS, expressed as a feature model, to fit the MOEA, creating new chromosome representation and reproduction operators. At runtime, FEMOSAA utilizes the feature model as domain knowledge to guide the search and further extend the MOEA, providing a larger chance for finding better solutions. In addition, we have designed a new method to search for the knee solutions, which can achieve a balanced tradeoff. We comprehensively evaluated FEMOSAA on two running SAS: One is a highly complex SAS with various adaptable real-world software under the realistic workload trace; another is a service-oriented SAS that can be dynamically composed from services. In particular, we compared the effectiveness and overhead of FEMOSAA against four of its variants and three other search-based frameworks for SAS under various scenarios, including three commonly applied MOEAs, two workload patterns, and diverse conflicting quality objectives. The results reveal the effectiveness of FEMOSAA and its superiority over the others with high statistical significance and nontrivial effect sizes.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {5},
numpages = {50},
keywords = {Feature model, multi-objective evolutionary algorithm, multi-objective optimization, performance engineering, search-based software engineering, self-adaptive system}
}

@inproceedings{10.1109/ICSE48619.2023.00024,
author = {Guan, Hao and Xiao, Ying and Li, Jiaying and Liu, Yepang and Bai, Guangdong},
title = {A Comprehensive Study of Real-World Bugs in Machine Learning Model Optimization},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00024},
doi = {10.1109/ICSE48619.2023.00024},
abstract = {Due to the great advance in machine learning (ML) techniques, numerous ML models are expanding their application domains in recent years. To adapt for resource-constrained platforms such as mobile and Internet of Things (IoT) devices, pre-trained models are often processed to enhance their efficiency and compactness, using optimization techniques such as pruning and quantization. Similar to the optimization process in other complex systems, e.g., program compilers and databases, optimizations for ML models can contain bugs, leading to severe consequences such as system crashes and financial loss. While bugs in training, compiling and deployment stages have been extensively studied, there is still a lack of systematic understanding and characterization of model optimization bugs (MOBs).In this work, we conduct the first empirical study to identify and characterize MOBs. We collect a comprehensive dataset containing 371 MOBs from TensorFlow and PyTorch, the most extensively used open-source ML frameworks, covering the entire development time span of their optimizers (May 2019 to August 2022). We then investigate the collected bugs from various perspectives, including their symptoms, root causes, life cycles, detection and fixes. Our work unveils the status quo of MOBs in the wild, and reveals their features on which future detection techniques can be based. Our findings also serve as a warning to the developers and the users of ML frameworks, and an appeal to our research community to enact dedicated countermeasures.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {147–158},
numpages = {12},
keywords = {machine learning, model optimization, bugs},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3501409.3501693,
author = {Yu, Shengzhao and Lei, Ming and Zhan, Yuqi},
title = {Home Smart Fitness System Integrating Fitness Program and Product Design},
year = {2022},
isbn = {9781450384322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501409.3501693},
doi = {10.1145/3501409.3501693},
abstract = {Under the influence of COVID-19, the time of the public spent at home increased greatly, during which the consumption of home decoration, sports and fitness increased significantly. Under the epidemic, the public can not participate in centralized fitness and weight loss exercises, and mobile-based fitness apps and home fitness equipment have become the focus of attention of the home-based fitness population during the outbreak of COVID-19. The purpose of this study is to propose the product interaction and design direction of intelligent fitness system in home application scenarios under the concept of hybrid design, in order to avoid the blind fitness of individuals based on fitness applications and enhance the intelligent fitness in the home environment to bring more scientific guidance and richer experience. The focus of this paper is to establish a fitness system that integrates scientific fitness guidance, online fitness intelligent experience, fitness social contact and fitness products into an innovative design concept of the industry. This concept is based on a survey of family fitness needs and future market demand, as well as case studies.The system consists of application carrier module, exercise prescription database, fitness APP and interactive experience module. Furniture provides the possibility for the application carrier of the system, and the huge sports prescription database provides suitable exercise prescriptions for the users to choose according to their fitness environment. Mobile app is the carrier of connecting users and intelligent fitness system, and the interactive experience module of mobile APP will bring functions of virtual fitness coach and social fitness game.In the future, with big data, artificial intelligence technologies and the concept of hybrid design, new smart home fitness system products will become possible.},
booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
pages = {1610–1616},
numpages = {7},
keywords = {Design trends, Home Smart fitness system, Hybrid design, Insert Fitness App},
location = {Xiamen, China},
series = {EITCE '21}
}

@inproceedings{10.1145/3563357.3566139,
author = {Chakraborty, Souparna and Arya, Vinay and SS, Siva Shankar and Bakli, Chirodeep},
title = {Optimization of building fa\c{c}ade for passive thermal management: a machine learning based simulation study for Kolkata, India},
year = {2022},
isbn = {9781450398909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563357.3566139},
doi = {10.1145/3563357.3566139},
abstract = {With an ever-increasing population, there is a sharp increase in the demand for residential areas. This has resulted in high-rise residential towers with building fa\c{c}ades having balconies that not only serve utilitarian and aesthetic purposes but also provide air circulation and ventilation. Hence balconies become an important passive component to control the heat gain by the building. This paper investigates the effect of the geometry of the balcony and the material used for construction on the heat gain by the internal space, optimizing the cooling load. This study gauges the effect of various designs of building fa\c{c}ades in terms of balcony geometry and material using Energy Plus and MATLAB-based neural network modeling. We use a surrogate model to predict simulation results and run various material properties to find the optimum material properties and the geometry of the balcony. We assume the balcony area to be fixed and find an optimum design using surrogate modeling. The results of this research can significantly reduce the energy consumption of high-rise buildings by keeping them cool.},
booktitle = {Proceedings of the 9th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {413–418},
numpages = {6},
keywords = {building fa\c{c}ade design, energy efficiency, machine learning, optimization, simulation},
location = {Boston, Massachusetts},
series = {BuildSys '22}
}

@inproceedings{10.1145/3427761.3428344,
author = {Huisman, Marieke and Wijs, Anton},
title = {Towards verified construction of correct and optimised GPU software},
year = {2020},
isbn = {9781450381864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427761.3428344},
doi = {10.1145/3427761.3428344},
abstract = {Techniques are required that support developers to produce GPU software that is both functionally correct and high-performing. We envision an integration of push-button formal verification techniques into a Model Driven Engineering workflow. In this paper, we present our vision on this topic, and how we plan to make steps in that direction in the coming five years.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN International Workshop on Formal Techniques for Java-Like Programs},
pages = {10–14},
numpages = {5},
keywords = {GPU software, code generation, formal verification, model transformation},
location = {Virtual, USA},
series = {FTfJP '20}
}

@inproceedings{10.1145/3519939.3523706,
author = {Pit-Claudel, Cl\'{e}ment and Philipoom, Jade and Jamner, Dustin and Erbsen, Andres and Chlipala, Adam},
title = {Relational compilation for performance-critical applications: extensible proof-producing translation of functional models into low-level code},
year = {2022},
isbn = {9781450392655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3519939.3523706},
doi = {10.1145/3519939.3523706},
abstract = {There are typically two ways to compile and run a purely functional program verified using an interactive theorem prover (ITP): automatically extracting it to a similar language (typically an unverified process, like Coq to OCaml) or manually proving it equivalent to a lower-level reimplementation (like a C program). Traditionally, only the latter produced both excellent performance and end-to-end proofs.  

This paper shows how to recast program extraction as a proof-search problem to automatically derive correct-by-construction, high-performance code from purely functional programs.  
We call this idea relational compilation — it extends recent developments with novel solutions to loop-invariant inference and genericity in kinds of side effects.  

Crucially, relational compilers are incomplete, and unlike traditional compilers, they generate good code not because of a fixed set of clever built-in optimizations but because they allow experts to plug in domain--specific extensions that give them complete control over the compiler's output.  

We demonstrate the benefits of this approach with Rupicola, a new compiler-construction toolkit designed to extract fast, verified, idiomatic low-level code from annotated functional models. Using case studies and performance benchmarks, we show that it is extensible with minimal effort and that it achieves performance on par with that of handwritten C programs.},
booktitle = {Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {918–933},
numpages = {16},
keywords = {compilation, theorem proving, verification},
location = {San Diego, CA, USA},
series = {PLDI 2022}
}

@article{10.1145/3660801,
author = {Xiao, Ying and Zhang, Jie M. and Liu, Yepang and Mousavi, Mohammad Reza and Liu, Sicen and Xue, Dingyuan},
title = {MirrorFair: Fixing Fairness Bugs in Machine Learning Software via Counterfactual Predictions},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660801},
doi = {10.1145/3660801},
abstract = {With the increasing utilization of Machine Learning (ML) software in critical domains such as employee hiring, college admission, and credit evaluation, ensuring fairness in the decision-making processes of underlying models has emerged as a paramount ethical concern. Nonetheless, existing methods for rectifying fairness issues can hardly strike a consistent trade-off between performance and fairness across diverse tasks and algorithms. Informed by the principles of counterfactual inference, this paper introduces MirrorFair, an innovative adaptive ensemble approach designed to mitigate fairness concerns. MirrorFair initially constructs a counterfactual dataset derived from the original data, training two distinct models—one on the original dataset and the other on the counterfactual dataset. Subsequently, MirrorFair adaptively combines these model predictions to generate fairer final decisions.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
We conduct an extensive evaluation of MirrorFair and compare it with 15 existing methods across a diverse range of decision-making scenarios. Our findings reveal that MirrorFair outperforms all the baselines in every measurement (i.e., fairness improvement, performance preservation, and trade-off metrics). Specifically, in 93% of cases, MirrorFair surpasses the fairness and performance trade-off baseline proposed by the benchmarking tool Fairea, whereas the state-of-the-art method achieves this in only 88% of cases. Furthermore, MirrorFair consistently demonstrates its superiority across various tasks and algorithms, ranking first in balancing model performance and fairness in 83% of scenarios. To foster replicability and future research, we have made our code, data, and results openly accessible to the research community.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {94},
numpages = {23},
keywords = {Bias Mitigation, Fairness Bugs, Machine Learning, Software Discrimination}
}

@article{10.5555/3648699.3648817,
author = {Kumar, Manoj and Sharma, Anurag and Kumar, Sandeep},
title = {A unified framework for optimization-based graph coarsening},
year = {2023},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {Graph coarsening is a widely used dimensionality reduction technique for approaching large-scale graph machine-learning problems. Given a large graph, graph coarsening aims to learn a smaller-tractable graph while preserving the properties of the originally given graph. Graph data consist of node features and graph matrix (e.g., adjacency and Laplacian). The existing graph coarsening methods ignore the node features and rely solely on a graph matrix to simplify graphs. In this paper, we introduce a novel optimization-based framework for graph dimensionality reduction. The proposed framework lies in the unification of graph learning and dimensionality reduction. It takes both the graph matrix and the node features as the input and learns the coarsen graph matrix and the coarsen feature matrix jointly while ensuring desired properties. The proposed optimization formulation is a multi-block non-convex optimization problem, which is solved efficiently by leveraging block majorization-minimization, log determinant, Dirichlet energy, and regularization frameworks. The proposed algorithms are provably convergent and practically amenable to numerous tasks. It is also established that the learned coarsened graph is ε ∈ (0,1) similar to the original graph. Extensive experiments elucidate the efficacy of the proposed framework for real-world applications. The code for all the experimental results is available at CODE.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {118},
numpages = {50},
keywords = {graph coarsening, graph learning, optimization, spectral properties, Laplacian matrix, clustering, graph classification, adjacency matrix, spectral similarity}
}

@inproceedings{10.1145/3649329.3658471,
author = {Ferikoglou, Aggelos and Kakolyris, Andreas and Kypriotis, Vasilis and Masouros, Dimosthenis and Soudris, Dimitrios and Xydis, Sotirios},
title = {Data-driven HLS optimization for reconfigurable accelerators},
year = {2024},
isbn = {9798400706011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649329.3658471},
doi = {10.1145/3649329.3658471},
abstract = {High-Level Synthesis (HLS) has played a pivotal role in making FPGAs accessible to a broader audience by facilitating high-level device programming and rapid microarchitecture customization through the use of directives. However, manually selecting the right directives can be a formidable challenge for programmers lacking a hardware background. This paper introduces an ultra-fast, knowledge-based HLS design optimization method that automatically extracts and applies the most promising directive configurations to the original source code. This optimization approach is entirely data-driven, offering a generalized HLS tuning solution without reliance on Quality of Result (QoR) models or meta-heuristics. We design, implement, and evaluate our methodology using over 100 applications sourced from well-established benchmark suites and GitHub repositories, all running on a Xilinx ZCU104 FPGA. The results are promising, including an average geometric mean speedup of \texttimes{}7.2 and \texttimes{}1.35 compared to designer-optimized designs and resource over-provisioning strategies, respectively. Additionally, it demonstrates a high design feasibility score and maintains an average inference latency of 38ms. Comparative analysis with traditional genetic algorithm-based Design Space Exploration (DSE) methods and State-of-the-Art (SoA) approaches reveals that it produces designs of similar quality but at speeds 2-3 orders of magnitude faster. This suggests that it is a highly promising solution for ultra-fast and automated HLS optimization.},
booktitle = {Proceedings of the 61st ACM/IEEE Design Automation Conference},
articleno = {309},
numpages = {6},
keywords = {high-level synthesis (HLS), design space exploration (DSE), FPGA accelerators, auto-tuning, data-driven optimization},
location = {San Francisco, CA, USA},
series = {DAC '24}
}

@inproceedings{10.1145/3324884.3416620,
author = {Dorn, Johannes and Apel, Sven and Siegmund, Norbert},
title = {Mastering uncertainty in performance estimations of configurable software systems},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416620},
doi = {10.1145/3324884.3416620},
abstract = {Understanding the influence of configuration options on performance is key for finding optimal system configurations, system understanding, and performance debugging. In prior research, a number of performance-influence modeling approaches have been proposed, which model a configuration option's influence and a configuration's performance as a scalar value. However, these point estimates falsely imply a certainty regarding an option's influence that neglects several sources of uncertainty within the assessment process, such as (1) measurement bias, (2) model representation and learning process, and (3) incomplete data. This leads to the situation that different approaches and even different learning runs assign different scalar performance values to options and interactions among them. The true influence is uncertain, though. There is no way to quantify this uncertainty with state-of-the-art performance modeling approaches. We propose a novel approach, P4, based on probabilistic programming that explicitly models uncertainty for option influences and consequently provides a confidence interval for each prediction of a configuration's performance alongside a scalar. This way, we can explain, for the first time, why predictions may cause errors and which option's influences may be unreliable. An evaluation on 12 real-world subject systems shows that P4's accuracy is in line with the state of the art while providing reliable confidence intervals, in addition to scalar predictions.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {684–696},
numpages = {13},
keywords = {P4, configurable software systems, performance-influence modeling, probabilistic programming},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.5555/3712729.3712912,
author = {Reimann, Hans and Moka, Sarat and Sofronov, Georgy},
title = {Continuous Optimization for Offline Change Point Detection and Estimation},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {This work explores use of novel advances in best subset selection for regression modelling via continuous optimization for offline change point detection and estimation in univariate Gaussian data sequences. The approach exploits reformulating the normal mean multiple change point model into a regularized statistical inverse problem enforcing sparsity. After introducing the problem statement, criteria and previous investigations via Lasso-regularization, the recently developed framework of continuous optimization for best subset selection (COMBSS) is briefly introduced and related to the problem at hand. Supervised and unsupervised perspectives are explored with the latter testing different approaches for the choice of regularization penalty parameters via the discrepancy principle and a confidence bound. The main result is an adaptation and evaluation of the COMBSS approach for offline normal mean multiple change-point detection via experimental results on simulated data for different choices of regularisation parameters. Results and future directions are discussed.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2190–2201},
numpages = {12},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@article{10.5555/3122009.3176863,
author = {Mahsereci, Maren and Hennig, Philipp},
title = {Probabilistic line searches for stochastic optimization},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization. Our method retains a Gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the Wolfe conditions to monitor the descent. The algorithm has very low computational cost, and no user-controlled parameters. Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {4262–4320},
numpages = {59},
keywords = {Bayesian optimization, Gaussian processes, learning rates, line searches, stochastic optimization}
}

@inproceedings{10.1145/3664476.3670461,
author = {Ricci, Sara and Shapoval, Vladyslav and Dzurenda, Petr and Roenne, Peter and Oupicky, Jan and Malina, Lukas},
title = {Lattice-based Multisignature Optimization for RAM Constrained Devices},
year = {2024},
isbn = {9798400717185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664476.3670461},
doi = {10.1145/3664476.3670461},
abstract = {In the era of growing threats posed by the development of quantum computers, ensuring the security of electronic services has become fundamental. The ongoing standardization process led by the National Institute of Standards and Technology (NIST) emphasizes the necessity for quantum-resistant security measures. However, the implementation of Post-Quantum Cryptographic (PQC) schemes, including advanced schemes such as threshold signatures, faces challenges due to their large key sizes and high computational complexity, particularly on constrained devices. This paper introduces two microcontroller-tailored optimization approaches, focusing on enhancing the DS2 threshold signature scheme. These optimizations aim to reduce memory consumption while maintaining security strength, specifically enabling the implementation of DS2 on microcontrollers with only 192 KB of RAM. Experimental results and security analysis demonstrate the efficacy and practicality of our solution, facilitating the deployment of DS2 threshold signatures on resource-constrained microcontrollers.},
booktitle = {Proceedings of the 19th International Conference on Availability, Reliability and Security},
articleno = {155},
numpages = {10},
keywords = {Dilithium, Lattice-based cryptography, RAM, memory optimization, microcontroller, random access memory, threshold signature},
location = {Vienna, Austria},
series = {ARES '24}
}

@inproceedings{10.1145/3712623.3712649,
author = {Hu, Yang and Yu, Yang and Zhang, Qiang and He, Yujun},
title = {Research on Routing Optimization Algorithm for Power OTN Based on DQN},
year = {2025},
isbn = {9798400712883},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3712623.3712649},
doi = {10.1145/3712623.3712649},
abstract = {Routing optimization is key to improve the performance of power OTN. To solve the unequally distributed traffic load in power OTN, this paper put forward a power OTN routing optimization algorithm based on DQN. The power OTN was logically divided into states according to the technology characteristics of DQN, with corresponding actions, rewards and DQN agent designed for its operation. It generated optimized routing strategies that produce network routing optimization after several rounds of device-level iteration. The experimental method verified good convergence and effectiveness of the algorithm through simulation experiments on a 14 nodes NSFNet network.},
booktitle = {Proceedings of the 2024 2nd International Conference on Advances in Artificial Intelligence and Applications},
pages = {194–198},
numpages = {5},
keywords = {DQN, Load balancing, Power OTN, Routing optimization},
location = {
},
series = {AAIA '24}
}

@article{10.1145/3660816,
author = {Wang, Chengpeng and Zhang, Jipeng and Wu, Rongxin and Zhang, Charles},
title = {DAInfer: Inferring API Aliasing Specifications from Library Documentation via Neurosymbolic Optimization},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660816},
doi = {10.1145/3660816},
abstract = {Modern software systems heavily rely on various libraries, necessitating understanding API semantics in static analysis. However, summarizing API semantics remains challenging due to complex implementations or the unavailability of library code. This paper presents DAInfer, a novel approach for inferring API aliasing specifications from library documentation. Specifically, we employ Natural Language Processing (NLP) models to interpret informal semantic information provided by the documentation, which enables us to reduce the specification inference to an optimization problem. Furthermore, we propose a new technique called neurosymbolic optimization to efficiently solve the optimization problem, yielding the desired API aliasing specifications. We have implemented DAInfer as a tool and evaluated it upon Java classes from several popular libraries. The results indicate that DAInfer infers the API aliasing specifications with a precision of 79.78% and a recall of 82.29%, averagely consuming 5.35 seconds per class. These obtained aliasing specifications further facilitate alias analysis, revealing 80.05% more alias facts for API return values in 15 Java projects. Additionally, the tool supports taint analysis, identifying 85 more taint flows in 23 Android apps. These results demonstrate the practical value of DAInfer in library-aware static analysis.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {109},
numpages = {24},
keywords = {alias analysis, documentation mining, specification inference}
}

@inproceedings{10.1145/2908961.2931711,
author = {Brownlee, Alexander E.I.},
title = {Mining Markov Network Surrogates for Value-Added Optimisation},
year = {2016},
isbn = {9781450343237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2908961.2931711},
doi = {10.1145/2908961.2931711},
abstract = {Surrogate fitness functions are a popular technique for speeding up metaheuristics, replacing calls to a costly fitness function with calls to a cheap model. However, surrogates also represent an explicit model of the fitness function, which can be exploited beyond approximating solution fitness. This paper proposes that mining surrogate fitness models can yield useful additional information on the problem to the decision maker, adding value to the optimisation process. An existing fitness model based on Markov networks is presented and applied to the optimisation of glazing on a building facade. Analysis of the model reveals how its parameters point towards the global optima of the problem after only part of the optimisation run, and reveals useful properties like the relative sensitivities of the problem variables.},
booktitle = {Proceedings of the 2016 on Genetic and Evolutionary Computation Conference Companion},
pages = {1267–1274},
numpages = {8},
keywords = {decision making, fitness approximation, metaheuristics, surrogates},
location = {Denver, Colorado, USA},
series = {GECCO '16 Companion}
}

@inproceedings{10.1145/3689031.3717476,
author = {Ding, Xianzhong and Zhang, Yunkai and Chen, Binbin and Ying, Donghao and Zhang, Tieying and Chen, Jianjun and Zhang, Lei and Cerpa, Alberto and Du, Wan},
title = {Towards VM Rescheduling Optimization Through Deep Reinforcement Learning},
year = {2025},
isbn = {9798400711961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689031.3717476},
doi = {10.1145/3689031.3717476},
abstract = {Modern industry-scale data centers need to manage a large number of virtual machines (VMs). Due to the continual creation and release of VMs, many small resource fragments are scattered across physical machines (PMs). To handle these fragments, data centers periodically reschedule some VMs to alternative PMs, a practice commonly referred to as VM rescheduling. Despite the increasing importance of VM rescheduling as data centers grow in size, the problem remains understudied. We first show that, unlike most combinatorial optimization tasks, the inference time of VM rescheduling algorithms significantly influences their performance, due to dynamic VM state changes during this period. This causes existing methods to scale poorly. Therefore, we develop a reinforcement learning system for VM rescheduling, VMR2L, which incorporates a set of customized techniques, such as a two-stage framework that accommodates diverse constraints and workload conditions, a feature extraction module that captures relational information specific to rescheduling, as well as a risk-seeking evaluation enabling users to optimize the trade-off between latency and accuracy. We conduct extensive experiments with data from an industry-scale data center. Our results show that VMR2L can achieve a performance comparable to the optimal solution but with a running time of seconds. Code12 and datasets3 are open-sourced.},
booktitle = {Proceedings of the Twentieth European Conference on Computer Systems},
pages = {686–701},
numpages = {16},
keywords = {Cloud Computing, Reinforcement Learning, Resource Management, Virtual Machine Rescheduling},
location = {Rotterdam, Netherlands},
series = {EuroSys '25}
}

@article{10.1145/3508362,
author = {Chen, Junjie and Suo, Chenyao},
title = {Boosting Compiler Testing via Compiler Optimization Exploration},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3508362},
doi = {10.1145/3508362},
abstract = {Compilers are a kind of important software, and similar to the quality assurance of other software, compiler testing is one of the most widely-used ways of guaranteeing their quality. Compiler bugs tend to occur in compiler optimizations. Detecting optimization bugs needs to consider two main factors: (1) the optimization flags controlling the accessability of the compiler buggy code should be turned on; and (2) the test program should be able to trigger the buggy code. However, existing compiler testing approaches only consider the latter to generate effective test programs, but just run them under several pre-defined optimization levels (e.g., -O0, -O1, -O2, -O3, -Os in GCC).To better understand the influence of compiler optimizations on compiler testing, we conduct the first empirical study, and find that (1) all the bugs detected under the widely-used optimization levels are also detected under the explored optimization settings (we call a combination of optimization flags turned on for compilation an optimization setting), while 83.54% of bugs are only detected under the latter; (2) there exist both inhibition effect and promotion effect among optimization flags for compiler testing, indicating the necessity and challenges of considering the factor of compiler optimizations in compiler testing.We then propose the first approach, called COTest, by considering both factors to test compilers. Specifically, COTest first adopts machine-learning (the XGBoost algorithm) to model the relationship between test programs and optimization settings, to predict the bug-triggering probability of a test program under an optimization setting. Then, it designs a diversity augmentation strategy to select a set of diverse candidate optimization settings for prediction for a test program. Finally, Top-K optimization settings are selected for compiler testing according to the predicted bug-triggering probabilities. Then, it designs a diversity augmentation strategy to select a set of diverse candidate optimization settings for prediction for a test program. Finally, Top-K optimization settings are selected for compiler testing according to the predicted bug-triggering probabilities. The experiments on GCC and LLVM demonstrate its effectiveness, especially COTest detects 17 previously unknown bugs, 11 of which have been fixed or confirmed by developers.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {72},
numpages = {33},
keywords = {Compiler testing, compiler optimization, machine learning}
}

@inproceedings{10.1145/3141865.3141868,
author = {Roth, Philip C. and Shan, Hongzhang and Riegner, David and Antolin, Nikolas and Sreepathi, Sarat and Oliker, Leonid and Williams, Samuel and Moore, Shirley and Windl, Wolfgang},
title = {Performance analysis and optimization of the RAMPAGE metal alloy potential generation software},
year = {2017},
isbn = {9781450355179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141865.3141868},
doi = {10.1145/3141865.3141868},
abstract = {The Rapid Alloy Method for Producing Accurate, General Empirical potential generation toolkit (RAMPAGE) is a program for fitting multicomponent interatomic potential functions for metal alloys. In this paper, we describe a collaborative effort between domain scientists and performance engineers to improve the parallelism, scalability, and maintainability of the code. We modified RAMPAGE to use the Message Passing Interface (MPI) for communication and synchronization, to use more than one MPI process when evaluating candidate potential functions, and to have its MPI processes execute functionality that was previously executed by external non-MPI processes. We ported RAMPAGE to run on the Eos and Titan Cray systems of the United States Department of Energy (DOE)'s Oak Ridge Leadership Computing Facility (OLCF), and the Cori and Edison systems at the DOE's National Energy Research Scientific Computing Center (NERSC). Our modifications resulted in a 7x speedup on 8 Eos system nodes, and scalability up to 2048 processes on the Cori system with Intel Knights Landing processors. To improve maintainability of the RAMPAGE source code, we introduced several software engineering best practices to the RAMPAGE developers' workflow.},
booktitle = {Proceedings of the 4th ACM SIGPLAN International Workshop on Software Engineering for Parallel Systems},
pages = {11–20},
numpages = {10},
keywords = {Applications, Message Passing Interface, performance engineering},
location = {Vancouver, BC, Canada},
series = {SEPS 2017}
}

@inproceedings{10.1145/3671016.3674814,
author = {Han, Sheng and Han, Qiang and Qiao, Yixin and Xue, Kehan and Shi, Zhichao},
title = {Developing Burr-XII NHPP-based software reliability growth model using Expectation Conditional Maximization Algorithm},
year = {2024},
isbn = {9798400707056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3671016.3674814},
doi = {10.1145/3671016.3674814},
abstract = {With the increasing complexity of software systems, the development of models that can accurately predict and enhance software reliability has become particularly important. This paper introduces an innovative method, applying the Expectation Conditional Maximization (ECM) algorithm to the Burr-XII Software Reliability Growth Model (SRGM) based on the Non-Homogeneous Poisson Process (NHPP). The ECM algorithm, as a variant of the Expectation Maximization (EM) algorithm, is particularly suitable for the estimation of complex model parameters. We utilize the ECM algorithm to estimate the parameters of the Burr XII NHPP SRGM and compare its performance with traditional parameter estimation methods. The results of the experiment demonstrate superior goodness-of-fit and predictive ability on multiple datasets compared to traditional methods, which highlights the efficiency and accuracy of the ECM algorithm in parameter estimation.},
booktitle = {Proceedings of the 15th Asia-Pacific Symposium on Internetware},
pages = {387–396},
numpages = {10},
keywords = {Burr XII distribution, Software reliability growth model, expectation conditional maximization algorithm, non-homogeneous poison process},
location = {Macau, China},
series = {Internetware '24}
}

@inproceedings{10.1145/3629378.3629379,
author = {Yang, Yiweng and Jin, Ruijie},
title = {Research on the Influencing Mechanism of Live Broadcasting of Experience Products},
year = {2023},
isbn = {9798400708824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629378.3629379},
doi = {10.1145/3629378.3629379},
abstract = {As a new form of e-commerce, live broadcasting marketing(LBM) is increasingly favored by enterprises and brands in recent years. The LBM mechanism produced in previous studies can effectively explain about search products, but there is no theoretical mechanism to confirm the LBM of experience products gradually emerging in the current market, such as car test drive, viewing new house online and so on. Therefore, this research introduces the theory of consumer inspiration to discuss the impact of LBM on consumer behavior from the perspectives of content factors and environmental factors. Through the empirical analysis of 331 samples collected in the questionnaire survey, it is found that information satisfaction, interest satisfaction and social presence have significant positive effects on customer inspiration; Interest satisfaction and social presence have significant positive effects on inspiration intention; Inspired-by and Inspired-to play mediating roles and chain mediating roles in some paths. Positive emotional susceptibility plays a positive moderating role in the social presence and consumer inspiration pathways. This research reveals the mechanism of LBM of experience products at the theoretical level, and provides enlightenment for enterprises to optimize the LBM strategies.},
booktitle = {Proceedings of the 2023 9th International Conference on Industrial and Business Engineering},
pages = {1–8},
numpages = {8},
keywords = {Customer Inspiration, Experience Products, Information Registration Behavior, Live Broadcasting Marketing, Positive Emotional Susceptibility},
location = {Beijing, China},
series = {ICIBE '23}
}

@inproceedings{10.5555/3712729.3712996,
author = {Bollapragada, Raghu and Karamanli, Cem and Wild, Stefan M.},
title = {Central Finite-Difference Based Gradient Estimation Methods for Stochastic Optimization},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {This paper presents an algorithmic framework for solving unconstrained stochastic optimization problems using only stochastic function evaluations. We employ central finite-difference based gradient estimation methods to approximate the gradients and dynamically control the accuracy of these approximations by adjusting the sample sizes used in stochastic realizations. We analyze the theoretical properties of the proposed framework on nonconvex functions. Our analysis yields sublinear convergence results to the neighborhood of the solution, and establishes the optimal worst-case iteration complexity (O(ε-1)) and sample complexity (O(ε-2)) for each gradient estimation method to achieve an ε-accurate solution. Finally, we demonstrate the performance of the proposed framework and the quality of the gradient estimation methods through numerical experiments on nonlinear least squares problems.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3205–3216},
numpages = {12},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@inproceedings{10.1145/3427921.3450246,
author = {Avritzer, Alberto and Britto, Ricardo and Trubiani, Catia and Russo, Barbara and Janes, Andrea and Camilli, Matteo and van Hoorn, Andr\'{e} and Heinrich, Robert and Rapp, Martina and Hen\ss{}, J\"{o}rg},
title = {A Multivariate Characterization and Detection of Software Performance Antipatterns},
year = {2021},
isbn = {9781450381949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427921.3450246},
doi = {10.1145/3427921.3450246},
abstract = {Context. Software Performance Antipatterns (SPAs) research has focused on algorithms for the characterization, detection, and solution of antipatterns. However, existing algorithms are based on the analysis of runtime behavior to detect trends on several monitored variables (e.g., response time, CPU utilization, and number of threads) using pre-defined thresholds. Objective. In this paper, we introduce a new approach for SPA characterization and detection designed to support continuous integration/delivery/deployment (CI/CDD) pipelines, with the goal of addressing the lack of computationally efficient algorithms.Method. Our approach includes SPA statistical characterization using a multivariate analysis approach of load testing experimental results to identify the services that have the largest impact on system scalability.More specifically, we introduce a layered decomposition approach that implements statistical analysis based on response time to characterize load testing experimental results. A distance function is used to match experimental results to SPAs.Results. We have instantiated the introduced methodology by applying it to a large complex telecom system. We were able to automatically identify the top five services that are scalability choke points. In addition, we were able to automatically identify one SPA. We have validated the engineering aspects of our methodology and the expected benefits by means of a domain experts' survey.Conclusion. We contribute to the state-of-the-art by introducing a novel approach to support computationally efficient SPA characterization and detection in large complex systems using performance testing results. We have compared the computational efficiency of the proposed approach with state-of-the-art heuristics. We have found that the approach introduced in this paper grows linearly, which is a significant improvement over existing techniques.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {61–72},
numpages = {12},
keywords = {multivariate analysis, software performance antipattern characterization, software performance antipattern detection},
location = {Virtual Event, France},
series = {ICPE '21}
}

@article{10.1145/3649815,
author = {Zhang, Chi and Wang, Linzhang and Rigger, Manuel},
title = {Finding Cross-Rule Optimization Bugs in Datalog Engines},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649815},
doi = {10.1145/3649815},
abstract = {Datalog is a popular and widely-used declarative logic programming language. Datalog engines apply many cross-rule optimizations; bugs in them can cause incorrect results. To detect such optimization bugs, we propose an automated testing approach called Incremental Rule Evaluation (IRE), which synergistically tackles the test oracle and test case generation problem. The core idea behind the test oracle is to compare the results of an optimized program and a program without cross-rule optimization; any difference indicates a bug in the Datalog engine. Our core insight is that, for an optimized, incrementally-generated Datalog program, we can evaluate all rules individually by constructing a reference program to disable the optimizations that are performed among multiple rules. Incrementally generating test cases not only allows us to apply the test oracle for every new rule generated—we also can ensure that every newly added rule generates a non-empty result with a given probability and eschew recomputing already-known facts. We implemented IRE as a tool named Deopt, and evaluated Deopt on four mature Datalog engines, namely Souffl\'{e}, CozoDB, μZ, and DDlog, and discovered a total of 30 bugs. Of these, 13 were logic bugs, while the remaining were crash and error bugs. Deopt can detect all bugs found by queryFuzz, a state-of-the-art approach. Out of the bugs identified by Deopt, queryFuzz might be unable to detect 5. Our incremental test case generation approach is efficient; for example, for test cases containing 60 rules, our incremental approach can produce 1.17\texttimes{} (for DDlog) to 31.02\texttimes{} (for Souffl\'{e}) as many valid test cases with non-empty results as the naive random method. We believe that the simplicity and the generality of the approach will lead to its wide adoption in practice.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {98},
numpages = {27},
keywords = {Datalog engine testing, cross-rule optimization bugs, test oracle}
}

@inproceedings{10.1145/3643690.3648236,
author = {Hamza, Muhammad and Siemon, Dominik and Akbar, Muhammad Azeem and Rahman, Tahsinur},
title = {Human-AI Collaboration in Software Engineering: Lessons Learned from a Hands-On Workshop},
year = {2024},
isbn = {9798400705717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643690.3648236},
doi = {10.1145/3643690.3648236},
abstract = {This paper investigates the dynamics of human-AI collaboration in software engineering, focusing on the use of ChatGPT. Through a thematic analysis of a hands-on workshop in which 22 professional software engineers collaborated for three hours with ChatGPT, we explore the transition of AI from a mere tool to a collaborative partner. The study identifies key themes such as the evolving nature of human-AI interaction, the capabilities of AI in software engineering tasks, and the challenges and limitations of integrating AI in this domain. The findings show that while AI, particularly ChatGPT, improves the efficiency of code generation and optimization, human oversight remains crucial, especially in areas requiring complex problem-solving and security considerations. This research contributes to the theoretical understanding of human-AI collaboration in software engineering and provides practical insights for effectively integrating AI tools into development processes. It highlights the need for clear role allocation, effective communication, and balanced AI-human collaboration to realize the full potential of AI in software engineering.},
booktitle = {Proceedings of the 7th ACM/IEEE International Workshop on Software-Intensive Business},
pages = {7–14},
numpages = {8},
keywords = {generative AI, ChatGPT, software engineering, workshop, empirical investigation},
location = {Lisbon, Portugal},
series = {IWSiB '24}
}

@inproceedings{10.1145/3637528.3671634,
author = {Abdulaal, Ahmed and Polat, Ali and Narayan, Hari and Zeng, Wenrong and Yi, Yimin},
title = {Dynamic Pricing for Multi-Retailer Delivery Platforms with Additive Deep Learning and Evolutionary Optimization},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671634},
doi = {10.1145/3637528.3671634},
abstract = {Dynamic Pricing for online retail has been discussed extensively in literature. However, past solutions fell short of addressing the unique challenges of independent multi-retailer platforms for grocery delivery. From limited visibility of retailers' inventories to diverse demand-side dynamics across retail brands and locations, the highly decentralized nature of multi-retailer platforms deviates from the classical framework of modeling price elasticity and cross-elasticity of demand. In this paper, we present a novel scheme to scalable and practical price adjustment in the highly dynamic multi-retailer context. First, we present a deep learning framework to distinctly model complex cross-elasticity relationships via additive neural networks augmented with adversarial data. Second, we present evolutionary optimization agents for adjusting itemized prices in a location-decentralized manner, while adhering to custom business constraints and objectives. The optimization utilizes the genetic algorithm structure, where we introduce a potential mechanism, inspired by bandit algorithms, in order to improve convergence speed by managing exploitation and exploration trade-offs. Our solution is deployed at Shipt and is extendable to other types of multi-retailer platforms, such as restaurant delivery. Finally, we empirically demonstrate performance using public and industry datasets of hundreds and thousands of diverse products across tens of stores, offering an optimization targets coverage scale in the tens of thousands, far larger than experimental setups in past research.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4741–4751},
numpages = {11},
keywords = {deep learning, dynamic pricing, evolutionary optimization, high-dimensional time series, neural networks},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3702191.3703645,
author = {Yu, Chuanwei and Yang, Junshi and Ding, Jinyong and Tian, Zhiduo and Li, Yuchao and Yan, Zhenlei},
title = {Research on Application of Intelligent Control System in Electric Automatic Spraying Device of Anti-corrosion Paint on Steel Pipe Rod of Transmission Line},
year = {2025},
isbn = {9798400718229},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702191.3703645},
doi = {10.1145/3702191.3703645},
abstract = {This paper presents an intelligent control system designed for an electric automatic spraying device used in applying anti-corrosion coatings on steel pipe rods in transmission lines. The proposed system integrates a sensor feedback mechanism, motion control algorithm, and paint flow optimization module, enabling precise control over paint thickness, improved material efficiency, and enhanced coating uniformity. The intelligent system utilizes real-time feedback from multiple sensors to dynamically adjust spray parameters, maintaining consistent coating quality across various pipe dimensions. Experimental results demonstrate the system's effectiveness in reducing paint wastage by 15% and achieving a uniform coating with a mean deviation of less than 5 microns, meeting anti-corrosion standards for transmission infrastructure. This study provides a scalable, adaptive approach to automated coating processes, with potential applications in other industrial contexts where precision and material efficiency are essential.},
booktitle = {Proceedings of the 2024 International Symposium on Integrated Circuit Design and Integrated Systems},
pages = {160–165},
numpages = {6},
keywords = {Anti-corrosion Coating, Automatic Spraying Device, Intelligent Control System, Sensor Feedback, Transmission Lines},
location = {
},
series = {ICDIS '24}
}

@proceedings{10.1145/3664646,
title = {AIware 2024: Proceedings of the 1st ACM International Conference on AI-Powered Software},
year = {2024},
isbn = {9798400706851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 1st ACM International Conference on AI-Powered Software (AIware), held on 15th and 16th July 2024 in Porto de Galinhas, Brazil co-located with the ACM International Conference on the Foundations of Software Engineering (FSE 2024). AIware aims to be an annual conference that brings the software engineering community together in anticipation of the upcoming changes driven by Foundation Models (FMs) and looks at them from the perspective of AI-powered software and their evolution.
 
 
 

 
 
 
AIware 2024 prioritizes fostering discussions about the latest developments in the interdisciplinary field of AIware rather than solely focusing on the presentation of papers. The emphasis is on engaging conversations from diverse backgrounds to identify emerging research challenges and establish a new research agenda for the community in the Foundation Model era. To present papers and for discussions, the two-day conference will have five sessions themed around AIware Vision, SE for AIware, Human - AI Conversation, Security &amp; Safety and AIware for Software Lifecycle Activities. Furthermore, the conference program will include two keynotes and five industry talks. The final session in the conference program will be dedicated to presenting accepted papers of the AIware challenge track.},
location = {Porto de Galinhas, Brazil}
}

@inproceedings{10.1145/3641554.3701832,
author = {Koitz-Hristov, Roxane and Mandl, Franz and Wotawa, Franz},
title = {VisOpt - Visualization of Compiler Optimizations for Computer Science Education},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701832},
doi = {10.1145/3641554.3701832},
abstract = {Visualizations in teaching have become a common practice as they effectively convey theoretical concepts. Compiler construction, a heavily theory-based subject in computer science education, is particularly challenging for students to understand. While many tools simulate a compiler's front end, or analysis phase, applications that focus on the back end, or synthesis phase, are scarce. This paper describes VisOpt, a web-based visualization tool designed for a master's level Compiler Construction course. VisOptfocuses on the synthesis phase, i.e., code optimization and code generation. Its primary objective is to help students comprehend various local compiler optimizations, which can be visualized on the original code, an intermediate representation, or an assembler-like target code. A quasi-experiment with a pre-test-post-test design revealed that students who used VisOpt reported higher self-efficacy compared to those who did not. Although no significant improvement in learning outcomes was observed overall, we propose VisOpt as an engaging pedagogical tool that effectively complements traditional methods for teaching the synthesis phase of compilers.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {603–609},
numpages = {7},
keywords = {compiler optimization, compiler visualization, computer science education, simulation software, visualization},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.5555/3643142.3643397,
author = {Montevechi, Jos\'{e} Arnaldo B. and Amaral, Jo\~{a}o Victor S. and Miranda, Rafael C. and dos Santos, Carlos H. and Brito, Fl\'{a}vio O. D. and Machado, Michael E. F. H. S.},
title = {Ensemble-Based Infill Search Simulation Optimization Framework},
year = {2024},
isbn = {9798350369663},
publisher = {IEEE Press},
abstract = {Simulation is widely used in several areas of knowledge, from engineering to biology, including physics and finance. It allows the evaluation of the model's results under different conditions, enabling performance analysis and more assertive decision-making. However, simulation can be computationally intensive, especially when we consider complex models. To deal with this problem, metamodeling has been increasingly used as a simulation optimization technique. In this article, we propose a new adaptive metamodeling method for simulation optimization, which aims to achieve better results using fewer experiments. This method combines machine learning and metaheuristic techniques, allowing the identification of the most important regions of the search space, which can be explored more efficiently to obtain optimal solutions. The results achieved in a manufacturing problem show that the proposed method presents a significant improvement in the achieved objective function value, in comparison with the conventional benchmark method, without compromising the simulation execution time.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3059–3070},
numpages = {12},
location = {San Antonio, Texas, USA},
series = {WSC '23}
}

@inproceedings{10.1145/3472749.3474811,
author = {Wang, Michael and ZHAO, HANG and Zhou, Xiaolei and Ren, Xiangshi and Bi, Xiaojun},
title = {Variance and Distribution Models for Steering Tasks},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474811},
doi = {10.1145/3472749.3474811},
abstract = {Steering law reveals a linear relationship between the movement time (MT) and the index of difficulty (ID) in trajectory-based steering tasks. However, it does not relate the variance or distribution of MT to ID. In this paper, we propose and evaluate models that predict the variance and distribution of MT based on ID for steering tasks. We first propose a quadratic variance model which reveals that the variance of MT is quadratically related to ID with the linear coefficient being 0. Empirical evaluation on a new and a previously collected dataset show that the quadratic variance model accounts for between 78% and 97% of variance of observed MT variances; it outperforms other model candidates such as linear and constant models; adding the linear coefficient leads to no improvement on the model fitness. The variance model enables predicting the distribution of MT given ID: we can use the variance model to predict the variance (or scale) parameter and Steering law to predict the mean (or location) parameter of a distribution. We have evaluated six types of distributions for predicting the distribution of MT. Our investigation also shows that positively skewed distribution such as Gamma, Lognormal, Exponentially Modified Gaussian (ExGaussian), and Extreme value distributions outperformed the symmetric distribution such as Gaussian and truncated Gaussian distribution in predicting the MT distribution, and Gamma distribution performed slightly better than other positively skewed distributions. Overall, our research advances the MT prediction of steering tasks from a point estimate to variance and distribution estimates, which provides a more complete understanding of steering behavior and quantifies the uncertainty of MT prediction.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {1122–1143},
numpages = {22},
keywords = {Steering law, probabilistic modeling},
location = {Virtual Event, USA},
series = {UIST '21}
}

@article{10.1145/3617169,
author = {Rolland, Knut H. and Fitzgerald, Brian and Dings\o{}yr, Torgeir and Stol, Klaas-Jan},
title = {Acrobats and Safety Nets: Problematizing Large-Scale Agile Software Development},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3617169},
doi = {10.1145/3617169},
abstract = {Agile development methods have become a standard in the software industry, including in large-scale projects. These methods share a set of underlying assumptions that distinguish them from more traditional plan-driven approaches. In this article, we adopt Alvesson and Sandberg's problematization approach to challenge three key assumptions that are prevalent in the large-scale agile literature: (1) agile and plan-driven methods are mutually exclusive; (2) self-managing and hierarchically organized teams are mutually exclusive; and (3) agile methods can scale through simple linear composition. Using a longitudinal case study of large-scale agile development, we describe a series of trigger events and episodes whereby the agile approach was tailored to address the needs of the large-scale development context, which was very much at odds with these fundamental assumptions. We develop a set of new underlying assumptions which suggest that agile and plan-driven practices are mutually enabling and necessary for coordination and scaling in large-scale agile projects. We develop nine propositions for large-scale agile projects based on these new alternative underlying assumptions. Finally, we summarize our theoretical contribution in a generic process model of continuously adjusting agile and plan-driven practices in order to accommodate process challenges in large-scale agile projects.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {33},
numpages = {45},
keywords = {Large-scale agile, problematization, assumptions, literature review, case study, software architecture, requirements engineering, multiteam project management}
}

@inproceedings{10.1145/3565387.3565427,
author = {Chang, Chunguang and Wang, Manqing and Wang, Shen},
title = {Production Scheduling Optimization of Prefabricated Components Based on Improved Artificial Fish Swarm Algorithm},
year = {2022},
isbn = {9781450396004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565387.3565427},
doi = {10.1145/3565387.3565427},
abstract = {In this paper, the production scheduling problem of prefabricated hybrid flow shop is studied. Resource constraints such as limited labor, limited buffer and steam maintenance parallel machine are considered in the actual production process, and an optimization model of production scheduling is established to minimize production cost, storage cost and delay cost. The maximum permutation method is used to arrange the discrete data, and then the artificial fish swarm algorithm is improved by combining the modified adjustment operation and mutation operation of genetic algorithm. Finally, by analyzing the actual production case of a prefabricated component factory, it is verified that the improved artificial fish swarm algorithm has good optimization and convergence.},
booktitle = {Proceedings of the 6th International Conference on Computer Science and Application Engineering},
articleno = {40},
numpages = {5},
keywords = {Flow shop scheduling, Improved artificial fish swarm algorithm, Prefabricated component, Resource constraints},
location = {Virtual Event, China},
series = {CSAE '22}
}

@inproceedings{10.1145/3636534.3649383,
author = {Liu, Yuxin (Myles) and Yao, Zhihao and Chen, Mingyi and Amiri Sani, Ardalan and Agarwal, Sharad and Tsudik, Gene},
title = {ProvCam: A Camera Module with Self-Contained TCB for Producing Verifiable Videos},
year = {2024},
isbn = {9798400704895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636534.3649383},
doi = {10.1145/3636534.3649383},
abstract = {Our perception of reality is under constant threat from ever-improving video manipulation techniques, including deep-fakes and generative AI. Therefore, proving authenticity of videos is increasingly important, especially in legal and news contexts. However, it is very challenging to prove it based on post-factum video content analysis.In this work, we take a preventative stance and construct ProvCam, a novel camera module that generates a cryptographic proof of video authenticity. Our solution greatly reduces the size of Trusted Computing Base (TCB) to include the module itself. Moreover, it mitigates tampering during the numerous processing steps between video capture by the camera sensor and generation of the digital video output. To confirm its practicality, we present a complete prototype of ProvCam on a Xilinx FPGA evaluation board. As experiments show, ProvCam incurs a negligible performance overhead (latency and throughput) and small energy consumption overhead when recording a video. It imposes a moderate hardware cost but is relatively small compared to other major components such as SoC. Moreover, it does not change the existing camera software stack and thus can be easily integrated with various camera-bearing devices, such as smartphones.},
booktitle = {Proceedings of the 30th Annual International Conference on Mobile Computing and Networking},
pages = {588–602},
numpages = {15},
keywords = {video provenance, deepfakes, secure camera},
location = {Washington D.C., DC, USA},
series = {ACM MobiCom '24}
}

@article{10.1145/3702984,
author = {Sun, Yongqian and Liang, Minghan and Zhang, Shenglin and Che, Zeyu and Luo, Zhiyao and Li, Dongwen and Zhang, Yuzhi and Pei, Dan and Pan, Lemeng and Hou, Liping},
title = {Efficient Multivariate Time Series Anomaly Detection Through Transfer Learning for Large-Scale Software Systems},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3702984},
doi = {10.1145/3702984},
abstract = {Timely anomaly detection of multivariate time series (MTS) is of vital importance for managing large-scale software systems. However, many deep learning-based MTS anomaly detection models require long-term MTS training data to achieve optimal performance, which often conflicts with the frequent pattern changes observed in software systems. Moreover, the training overhead of vast MTS in large-scale software systems is unacceptably high. To address these issues, we design OmniTransfer, a model-agnostic framework that combines weighted hierarchical agglomerative clustering with an adaptive transfer learning strategy, making many state-of-the-art (SOTA) MTS anomaly detection models efficient and effective. Extensive experiments using real-world data from a large web content service provider and a network operator show that OmniTransfer significantly reduces the model initialization time by 46.49% and the training cost by 74.51%, while maintaining high accuracy in detecting anomalies.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
keywords = {Transfer Learning, Multivariate Time Series, Multivariate Time Series Clustering, Anomaly Detection}
}

@inproceedings{10.1109/ASE56229.2023.00032,
author = {Qian, Shangshu and Fan, Wen and Tan, Lin and Zhang, Yongle},
title = {Vicious Cycles in Distributed Software Systems},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00032},
doi = {10.1109/ASE56229.2023.00032},
abstract = {A major threat to distributed software systems' reliability is vicious cycles, which are observed when an event in the distributed software system's execution causes a system degradation, and the degradation, in turn, causes more of such events. Vicious cycles often result in large-scale cloud outages that are hard to recover from due to their self-reinforcing nature.This paper formally defines Vicious Cycle, and conducts the first in-depth study of 33 real-world vicious cycles in 13 widely-used open-source distributed software systems, shedding light on the root causes, triggering conditions, and fixing strategies of vicious cycles, with over a dozen concrete implications to combat them. Our findings show that the majority of the vicious cycles are caused by incorrect error handlers, where the handlers do not obtain enough information to distinguish between 1) an error induced by incoming requests and 2) an error induced by an unexpected interference from another error handler.This paper further performs a feasibility study by 1) building a monitoring tool that prevents one type of vicious cycle by collecting information to make a more informed decision in error handling, and 2) investigating the effectiveness of one commonly suggested practice---injecting exponential backoff---to prevent vicious cycles induced by unconstrained retry.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {91–103},
numpages = {13},
keywords = {distributed software systems, vicious cycles},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@article{10.1145/3652949,
author = {Li, Nianyu and Zhang, Mingyue and Li, Jialong and Adepu, Sridhar and Kang, Eunsuk and Jin, Zhi},
title = {A Game-Theoretical Self-Adaptation Framework for Securing Software-Intensive Systems},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/3652949},
doi = {10.1145/3652949},
abstract = {Security attacks present unique challenges to the design of self-adaptation mechanism for software-intensive systems due to the adversarial nature of the environment. Game-theoretical approaches have been explored in security to model malicious behaviors and design reliable defense for the system in a mathematically grounded manner. However, modeling the system as a single player, as done in prior works, is insufficient for the system under partial compromise and for the design of fine-grained defensive policies where the rest of the system with autonomy can cooperate to mitigate the impact of attacks. To address such issues, we propose a new self-adaptation framework incorporating Bayesian game theory and model the defender (i.e., the system) at the granularity of components. Under security attacks, the architecture model of the system is automatically translated, by the proposed translation process with designed algorithms, into a multi-player Bayesian game. This representation allows each component to be modeled as an independent player, while security attacks are encoded as variant types for the components. By solving for pure equilibrium (i.e., adaptation response), the system’s optimal defensive strategy is dynamically computed, enhancing system resilience against security attacks by maximizing system utility. We validate the effectiveness of our framework through two sets of experiments using generic benchmark tasks tailored for the security domain. Additionally, we exemplify the practical application of our approach through a real-world implementation in the Secure Water Treatment System to demonstrate the applicability and potency in mitigating security risks.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = apr,
articleno = {12},
numpages = {49},
keywords = {Software-intensive systems, game theory, self-adaptation, software security}
}

@article{10.1145/3653458,
author = {Du, Linfeng and Liang, Tingyuan and Zhou, Xiaofeng and Ge, Jinming and Li, Shangkun and Sinha, Sharad and Zhao, Jieru and Xie, Zhiyao and Zhang, Wei},
title = {FADO: Floorplan-Aware Directive Optimization Based on Synthesis and Analytical Models for High-Level Synthesis Designs on Multi-Die FPGAs},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1936-7406},
url = {https://doi.org/10.1145/3653458},
doi = {10.1145/3653458},
abstract = {Multi-die FPGAs are widely adopted for large-scale accelerators, but optimizing high-level synthesis designs on these FPGAs faces two challenges. First, the delay caused by die-crossing nets creates an NP-hard floorplanning problem. Second, traditional directive optimization cannot consider resource constraints on each die or the timing issue incurred by the die-crossings. Furthermore, the high algorithmic complexity and the large scale lead to extended runtime for legalizing the floorplan of HLS designs under different directive configurations. To co-optimize the directives and floorplan of HLS designs on multi-die FPGAs, we formulate the co-search based on bin-packing variants and present two iterative optimization flows. The first (FADO 1.0) relies on a pre-built QoR library. It involves a greedy, latency-bottleneck-guided directive search, and an incremental floorplan legalization. Compared with a global floorplanning solution, it takes 693X~4925X shorter search time and achieves 1.16X~8.78X better design performance, measured in workload execution time. To remove the time-consuming QoR library generation, the second flow (FADO 2.0) integrates an analytical QoR model and redesigns the directive search to accelerate convergence. Through experiments on mixed dataflow and non-dataflow designs, compared with 1.0, FADO 2.0 further yields a 1.40X better design performance on average after implementation on the Alveo U250 FPGA.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = sep,
articleno = {47},
numpages = {33},
keywords = {High-level synthesis, analytical model, design space exploration, multi-die FPGA, directive optimization, floorplanning}
}

@article{10.1145/3720503,
author = {Gupta, Ahan and Yuan, Yueming and Jain, Devansh and Ge, Yuhao and Aponte, David and Zhou, Yanqi and Mendis, Charith},
title = {SPLAT: A Framework for Optimised GPU Code-Generation for SParse reguLar ATtention},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3720503},
doi = {10.1145/3720503},
abstract = {Multi-head-self-attention (MHSA) mechanisms achieve state-of-the-art (SOTA) performance across natural language processing and vision tasks. However, their quadratic dependence on sequence lengths has bottlenecked inference speeds. To circumvent this bottleneck, researchers have proposed various sparse-MHSA models, where a subset of full attention is computed. Despite their promise, current sparse libraries and compilers do not support high-performance implementations for diverse sparse-MHSA patterns due to the underlying sparse formats they operate on. On one end, sparse libraries operate on general sparse formats which target extreme amounts of random sparsity (&lt;10% non-zero values) and have high metadata in O(nnzs). On the other end, hand-written kernels operate on custom sparse formats which target specific sparse-MHSA patterns. However, the sparsity patterns in sparse-MHSA are moderately sparse (10-50% non-zero values) and varied, resulting in general sparse formats incurring high metadata overhead and custom sparse formats covering few sparse-MSHA patterns, trading off generality for performance.   We bridge this gap, achieving both generality and performance, by proposing a novel sparse format: affine-compressed-sparse-row (ACSR) and supporting code-generation scheme, SPLAT, that generates high-performance implementations for diverse sparse-MHSA patterns on GPUs. Core to our proposed format and code generation algorithm is the observation that common sparse-MHSA patterns have uniquely regular geometric properties. These properties, which can be analyzed just-in-time, expose novel optimizations and tiling strategies that SPLAT exploits to generate high-performance implementations for diverse patterns. To demonstrate SPLAT’s efficacy, we use it to generate code for various sparse-MHSA models, achieving speedups of up-to 2.05x and 4.05x over hand-written kernels written in triton and TVM respectively on A100 GPUs in single-precision.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {138},
numpages = {29},
keywords = {Code-generation, Deep Learning, Large Language Models}
}

@article{10.1145/3712005,
author = {Gao, Cuiyun and Hu, Xing and Gao, Shan and Xia, Xin and Jin, Zhi},
title = {The Current Challenges of Software Engineering in the Era of Large Language Models},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3712005},
doi = {10.1145/3712005},
abstract = {With the advent of large language models (LLMs) in the artificial intelligence (AI) area, the field of software engineering (SE) has also witnessed a paradigm shift. These models, by leveraging the power of deep learning and massive amounts of data, have demonstrated an unprecedented capacity to understand, generate, and operate programming languages. They can assist developers in completing a broad spectrum of software development activities, encompassing software design, automated programming, and maintenance, which potentially reduces huge human efforts. Integrating LLMs within the SE landscape (LLM4SE) has become a burgeoning trend, necessitating exploring this emergent landscape’s challenges and opportunities.The paper aims at revisiting the software development life cycle (SDLC) under LLMs, and highlighting challenges and opportunities of the new paradigm. The paper first summarizes the overall process of LLM4SE, and then elaborates on the current challenges based on a through discussion. The discussion was held among more than 20 participants from academia and industry, specializing in fields such as software engineering and artificial intelligence. Specifically, we achieve 26 key challenges from seven aspects, including software requirement &amp; design, coding assistance, testing code generation, code review, code maintenance, software vulnerability management, and data, training, and evaluation. We hope the achieved challenges would benefit future research in the LLM4SE field.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Large Language Models, Challenges, LLM4SE}
}

@inproceedings{10.1145/3604915.3608840,
author = {Spillo, Giuseppe and De Filippo, Allegra and Musto, Cataldo and Milano, Michela and Semeraro, Giovanni},
title = {Towards Sustainability-aware Recommender Systems: Analyzing the Trade-off Between Algorithms Performance and Carbon Footprint},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604915.3608840},
doi = {10.1145/3604915.3608840},
abstract = {In this paper, we present a comparative analysis of the trade-off between the performance of state-of-the-art recommendation algorithms and their environmental impact. In particular, we compared 18 popular recommendation algorithms in terms of both performance metrics (i.e., accuracy and diversity of the recommendations) as well as in terms of energy consumption and carbon footprint on three different datasets. In order to obtain a fair comparison, all the algorithms were run based on the implementations available in a popular recommendation library, i.e., RecBole, and used the same experimental settings. The outcomes of the experiments showed that the choice of the optimal recommendation algorithm requires a thorough analysis, since more sophisticated algorithms often led to tiny improvements at the cost of an exponential increase of carbon emissions. Through this paper, we aim to shed light on the problem of carbon footprint and energy consumption of recommender systems, and we make the first step towards the development of sustainability-aware recommendation algorithms.},
booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
pages = {856–862},
numpages = {7},
keywords = {carbon footprint, evaluation, non-accuracy metrics, recommender systems, sustainability},
location = {Singapore, Singapore},
series = {RecSys '23}
}

@inproceedings{10.1145/3639476.3639770,
author = {Maninger, Daniel and Narasimhan, Krishna and Mezini, Mira},
title = {Towards Trustworthy AI Software Development Assistance},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639770},
doi = {10.1145/3639476.3639770},
abstract = {It is expected that in the near future, AI software development assistants will play an important role in the software industry. However, current software development assistants tend to be unreliable, often producing incorrect, unsafe, or low-quality code. We seek to resolve these issues by introducing a holistic architecture for constructing, training, and using trustworthy AI software development assistants. In the center of the architecture, there is a foundational LLM trained on datasets representative of real-world coding scenarios and complex software architectures, and fine-tuned on code quality criteria beyond correctness. The LLM will make use of graph-based code representations for advanced semantic comprehension. We envision a knowledge graph integrated into the system to provide up-to-date background knowledge and to enable the assistant to provide appropriate explanations. Finally, a modular framework for constrained decoding will ensure that certain guarantees (e.g., for correctness and security) hold for the generated code.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {112–116},
numpages = {5},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@inproceedings{10.1145/3678722.3685533,
author = {Schwarcz, Florian and Berlakovich, Felix and Barany, Gerg\"{o} and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {LOOL: Low-Overhead, Optimization-Log-Guided Compiler Fuzzing (Registered Report)},
year = {2024},
isbn = {9798400711121},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678722.3685533},
doi = {10.1145/3678722.3685533},
abstract = {Compiler fuzzing with randomly generated input programs is a powerful technique for finding compiler crashes and miscompilation bugs. Existing fuzzers for compilers are often unguided and must be manually parameterized to cover different parts of the compiler under test.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
In this work we present LOOL, an approach for fuzzing a compiler with low overhead, guided by optimization log information produced by the compiler. The optimization log tracks program transformations performed by the compiler on the level of individual methods compiled. We argue that using the optimization log has less overhead than off-the-shelf code coverage tools. At the same time, the optimization log's per-method data gives more information than code coverage collected over a number of distinct compilations. The level of detail of the optimization log is also easy to tune for the use case of guiding a fuzzer.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
We are integrating the LOOL approach in an existing fuzzer for the GraalVM compiler. A genetic optimization algorithm uses optimization log information for tuning code generation parameters with the goal of covering optimizations that were previously rarely exercised. Initial experiments confirm that varying the generator's parameters is effective at finding new bugs. The genetic algorithm will automate the exploration of the parameter space to improve testing of currently insufficiently fuzzed parts of the compiler.},
booktitle = {Proceedings of the 3rd ACM International Fuzzing Workshop},
pages = {42–51},
numpages = {10},
keywords = {GraalVM, JIT compiler, fuzzing, genetic algorithm},
location = {Vienna, Austria},
series = {FUZZING 2024}
}

@inproceedings{10.1145/3620665.3640363,
author = {Wang, Meng and Fang, Bo and Li, Ang and Nair, Prashant J.},
title = {Red-QAOA: Efficient Variational Optimization through Circuit Reduction},
year = {2024},
isbn = {9798400703850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620665.3640363},
doi = {10.1145/3620665.3640363},
abstract = {The Quantum Approximate Optimization Algorithm (QAOA) addresses combinatorial optimization challenges by converting inputs to graphs. However, the optimal parameter searching process of QAOA is greatly affected by noise. Larger problems yield bigger graphs, requiring more qubits and making their outcomes highly noise-sensitive. This paper introduces Red-QAOA, leveraging energy landscape concentration via a simulated annealing-based graph reduction.Red-QAOA creates a smaller (distilled) graph with nearly identical parameters to the original graph. The distilled graph produces a smaller quantum circuit and thus reduces noise impact. At the end of the optimization, Red-QAOA employs the parameters from the distilled graph on the original graph and continues the parameter search on the original graph. Red-QAOA outperforms state-of-the-art Graph Neural Network (GNN)-based pooling techniques on 3200 real-world problems. Red-QAOA reduced node and edge counts by 28% and 37%, respectively, with a mean square error of only 2%.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {980–998},
numpages = {19},
location = {La Jolla, CA, USA},
series = {ASPLOS '24}
}

@article{10.1145/3631967,
author = {Li, Wen and Marino, Austin and Yang, Haoran and Meng, Na and Li, Li and Cai, Haipeng},
title = {How Are Multilingual Systems Constructed: Characterizing Language Use and Selection in Open-Source Multilingual Software},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3631967},
doi = {10.1145/3631967},
abstract = {For many years now, modern software is known to be developed in multiple languages (hence termed as multilingual or multi-language software). Yet, to date, we still only have very limited knowledge about how multilingual software systems are constructed. For instance, it is not yet really clear how different languages are used, selected together, and why they have been so in multilingual software development. Given the fact that using multiple languages in a single software project has become a norm, understanding language use and selection (i.e., language profile) as a basic element of the multilingual construction in contemporary software engineering is an essential first step.In this article, we set out to fill this gap with a large-scale characterization study on language use and selection in open-source multilingual software. We start with presenting an updated overview of language use in 7,113 GitHub projects spanning the 5 past years by characterizing overall statistics of language profiles, followed by a deeper look into the functionality relevance/justification of language selection in these projects through association rule mining. We proceed with an evolutionary characterization of 1,000 GitHub projects for each of the 10 past years to provide a longitudinal view of how language use and selection have changed over the years, as well as how the association between functionality and language selection has been evolving.Among many other findings, our study revealed a growing trend of using three to five languages in one multilingual software project and the noticeable stableness of top language selections. We found a non-trivial association between language selection and certain functionality domains, which was less stable than that with individual languages over time. In a historical context, we also have observed major shifts in these characteristics of multilingual systems both in contrast to earlier peer studies and along the evolutionary timeline. Our findings offer essential knowledge on the multilingual construction in modern software development. Based on our results, we also provide insights and actionable suggestions for both researchers and developers of multilingual systems.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {63},
numpages = {46},
keywords = {Multilingual software, language use, language selection, language profile, functionality relevance, evolutionary characterization, association mining}
}

@inproceedings{10.1145/3624062.3624251,
author = {Zubair, Mohammad and Walden, Aaron and Nastac, Gabriel and Nielsen, Eric and Bauinger, Christoph and Zhu, Xiao},
title = {Optimization of Ported CFD Kernels on Intel Data Center GPU Max 1550 using oneAPI ESIMD},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624251},
doi = {10.1145/3624062.3624251},
abstract = {We describe our experience porting FUN3D’s CUDA-optimized kernels to Intel oneAPI SYCL. We faced several challenges, including foremost the suboptimal performance of the oneAPI code on Intel’s new data center GPU. Suboptimal performance of the oneAPI code was due primarily to high register spills, memory latency, and poor vectorization. We addressed these issues by implementing the kernels using Intel oneAPI’s Explicit SIMD SYCL extension (ESIMD) API. The ESIMD API enables the writing of explicitly vectorized kernel code, gives more precise control over register usage and prefetching, and better handles thread divergence compared to SYCL. The ESIMD code outperforms the optimized SYCL code by up to a factor of 3.6, depending on the kernel. We also compared the performance of three ESIMD kernels on the Intel Data Center Max 1550 GPU with the CUDA-optimized versions on NVIDIA V100 and A100 GPUs. We found the performance of a single tile of the Intel GPU using ESIMD greater than NVIDIA V100 and similar to NVIDIA A100.},
booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {1705–1712},
numpages = {8},
keywords = {CFD, ESIMD, GPU, Performance},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@inproceedings{10.1145/3444370.3444575,
author = {Zhang, Bo and Kong, Dehua},
title = {Dynamic estimation model of insurance product recommendation based on Naive Bayesian model},
year = {2021},
isbn = {9781450387828},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3444370.3444575},
doi = {10.1145/3444370.3444575},
abstract = {Aiming at the dynamic estimation of insurance product recommendation, considering the particularity and complexity of purchasing insurance product and the uncertainty of influencing factors, a dynamic estimation model of insurance product recommendation based on Naive Bayes is proposed. The model combines customer insurance information with machine learning. The results show that the naive Bayesian classification algorithm can be compared with the decision tree and neural network classification algorithm, showing high accuracy and high speed.},
booktitle = {Proceedings of the 2020 International Conference on Cyberspace Innovation of Advanced Technologies},
pages = {219–224},
numpages = {6},
keywords = {Naive Bayes, dynamic estimation, insurance products, recommendation},
location = {Guangzhou, China},
series = {CIAT 2020}
}

@article{10.1109/TNET.2024.3423673,
author = {Wang, Su and Morabito, Roberto and Hosseinalipour, Seyyedali and Chiang, Mung and Brinton, Christopher G.},
title = {Device Sampling and Resource Optimization for Federated Learning in Cooperative Edge Networks},
year = {2024},
issue_date = {Oct. 2024},
publisher = {IEEE Press},
volume = {32},
number = {5},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2024.3423673},
doi = {10.1109/TNET.2024.3423673},
abstract = {The conventional federated learning (FedL) architecture distributes machine learning (ML) across worker devices by having them train local models that are periodically aggregated by a server. FedL ignores two important characteristics of contemporary wireless networks, however: (i) the network may contain heterogeneous communication/computation resources, and (ii) there may be significant overlaps in devices’ local data distributions. In this work, we develop a novel optimization methodology that jointly accounts for these factors via intelligent device sampling complemented by device-to-device (D2D) offloading. Our optimization methodology aims to select the best combination of sampled nodes and data offloading configuration to maximize FedL training accuracy while minimizing data processing and D2D communication resource consumption subject to realistic constraints on the network topology and device capabilities. Theoretical analysis of the D2D offloading subproblem leads to new FedL convergence bounds and an efficient sequential convex optimizer. Using these results, we develop a sampling methodology based on graph convolutional networks (GCNs) which learns the relationship between network attributes, sampled nodes, and D2D data offloading to maximize FedL accuracy. Through evaluation on popular datasets and real-world network measurements from our edge testbed, we find that our methodology outperforms popular device sampling methodologies from literature in terms of ML model performance, data processing overhead, and energy consumption.},
journal = {IEEE/ACM Trans. Netw.},
month = jul,
pages = {4365–4381},
numpages = {17}
}

@inproceedings{10.1145/2110147.2110158,
author = {Lopez-Herrejon, Roberto E. and Egyed, Alexander},
title = {Towards fixing inconsistencies in models with variability},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110158},
doi = {10.1145/2110147.2110158},
abstract = {Recent years have witnessed a convergence between research in SPL and Model-Driven Engineering (MDE) that leverages the complementary capabilities that both paradigms can offer. A crucial factor for the success of MDE is the availability of effective support for detecting and fixing inconsistencies among model elements. The importance of such support is attested by the extensive literature devoted to the topic. However, when coupled with variability, the research focus has been devoted to inconsistency detection, while leaving the important issue of fixing the inconsistency largely unaddressed. In this research-in-progress paper, we explore one of the issues that variability raises for inconsistency fixing. Namely, in which features to locate the fixes. We compute what is the minimal number of fixes and use it as a baseline to compare fixes obtained with a heuristic based on feature model analysis and random approaches. Our work highlights the pros and cons of both approaches and suggests how they could be addressed.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {93–100},
numpages = {8},
keywords = {consistency, consistency checking, feature oriented software development, model, safe composition, software product line, variability},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@article{10.1145/3643746,
author = {Y\i{}ld\i{}ran, Necip Faz\i{}l and Oh, Jeho and Lawall, Julia and Gazzillo, Paul},
title = {Maximizing Patch Coverage for Testing of Highly-Configurable Software without Exploding Build Times},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643746},
doi = {10.1145/3643746},
abstract = {The Linux kernel is highly-configurable, with a build system that takes a configuration file as input and automatically tailors the source code accordingly. Configurability, however, complicates testing, because different configuration options lead to the inclusion of different code fragments. With thousands of patches received per month, Linux kernel maintainers employ extensive automated continuous integration testing. To attempt patch coverage, i.e., taking all changed lines into account, current approaches either use configuration files that maximize total statement coverage or use multiple randomly-generated configuration files, both of which incur high build times without guaranteeing patch coverage. To achieve patch coverage without exploding build times, we propose krepair, which automatically repairs configuration files that are fast-building but have poor patch coverage to achieve high patch coverage with little effect on build times. krepair works by discovering a small set of changes to a configuration file that will ensure patch coverage, preserving most of the original configuration file's settings. Our evaluation shows that, when applied to configuration files with poor patch coverage on a statistically-significant sample of recent Linux kernel patches, krepair achieves nearly complete patch coverage, 98.5% on average, while changing less than 1.53% of the original default configuration file in 99% of patches, which keeps build times 10.5x faster than maximal configuration files.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {20},
numpages = {23},
keywords = {build systems, software configuration, static analysis}
}

@inproceedings{10.1145/3196398.3196442,
author = {Nair, Vivek and Agrawal, Amritanshu and Chen, Jianfeng and Fu, Wei and Mathew, George and Menzies, Tim and Minku, Leandro and Wagner, Markus and Yu, Zhe},
title = {Data-driven search-based software engineering},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196442},
doi = {10.1145/3196398.3196442},
abstract = {This paper introduces Data-Driven Search-based Software Engineering (DSE), which combines insights from Mining Software Repositories (MSR) and Search-based Software Engineering (SBSE). While MSR formulates software engineering problems as data mining problems, SBSE reformulate Software Engineering (SE) problems as optimization problems and use meta-heuristic algorithms to solve them. Both MSR and SBSE share the common goal of providing insights to improve software engineering. The algorithms used in these two areas also have intrinsic relationships. We, therefore, argue that combining these two fields is useful for situations (a) which require learning from a large data source or (b) when optimizers need to know the lay of the land to find better solutions, faster.This paper aims to answer the following three questions: (1) What are the various topics addressed by DSE?, (2) What types of data are used by the researchers in this area?, and (3) What research approaches do researchers use? The paper briefly sets out to act as a practical guide to develop new DSE techniques and also to serve as a teaching resource.This paper also presents a resource (tiny.cc/data-se) for exploring DSE. The resource contains 89 artifacts which are related to DSE, divided into 13 groups such as requirements engineering, software product lines, software processes. All the materials in this repository have been used in recent software engineering papers; i.e., for all this material, there exist baseline results against which researchers can comparatively assess their new ideas.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {341–352},
numpages = {12},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@article{10.1145/3594539,
author = {Lumpp, Francesco and Panato, Marco and Bombieri, Nicola and Fummi, Franco},
title = {A Design Flow Based on Docker and Kubernetes for ROS-based Robotic Software Applications},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {5},
issn = {1539-9087},
url = {https://doi.org/10.1145/3594539},
doi = {10.1145/3594539},
abstract = {Human-centered robotic applications are becoming pervasive in the context of robotics and smart manufacturing, and such a pervasiveness is even more expected with the shift to Industry 5.0. The always increasing level of autonomy of modern robotic platforms requires the integration of software applications from different domains to implement artificial intelligence, cognition, and human-robot/robot-robot interaction. Developing and (re)configuring such a multi-domain software to meet functional constraints is a challenging task. Even more challenging is customizing the software to satisfy non-functional requirements such as real-time, reliability, and energy efficiency. In this context, the concept of Edge-Cloud continuum is gaining consensus as a solution to address functional and non-functional constraints in a seamless way. Containerization and orchestration are becoming a standard practice, as they allow for better information flow among different network levels as well as increased modularity in the use of multi-domain software components. Nevertheless, the adoption of such a practice along the design flow, from simulation to the deployment of complex robotic applications by addressing the de facto development standards (e.g., ROS - Robotic Operating System) is still an open problem. We present a design methodology based on Docker and Kubernetes that enables containerization and orchestration of ROS-based robotic SW applications for heterogeneous and hierarchical HW architectures. The methodology aims at (i) integrating and verifying multi-domain components since early in the design flow, (ii) mapping software tasks to containers to minimize the performance and memory footprint overhead, (iii) clustering containers to efficiently distribute load across the edge-cloud architecture by minimizing resource utilization, and (iv) enabling multi-domain verification of functional and non-functional constraints before deployment. The article presents the results obtained with a real case of study, in which the design methodology has been applied to program the mission of a Robotnik RB-Kairos mobile robot in an industrial agile production chain. We have obtained reduced load on the robot’s HW with minimal performance and network overhead, thanks to the optimized distributed system.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = aug,
articleno = {74},
numpages = {24},
keywords = {ROS, Docker, Kubernetes, K3S, robotic applications, Edge-Cloud computing}
}

@article{10.1145/3708533,
author = {B\"{o}hme, Marcel and Bodden, Eric and Bultan, Tevfik and Cadar, Cristian and Liu, Yang and Scanniello, Giuseppe},
title = {Software Security Analysis in 2030 and Beyond: A Research Roadmap},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708533},
doi = {10.1145/3708533},
abstract = {As our lives, our businesses, and indeed our world economy become increasingly reliant on the secure operation of many interconnected software systems, the software engineering research community is faced with unprecedented research challenges, but also with exciting new opportunities. In this roadmap paper, we outline our vision of Software Security Analysis for the systems of the future. Given the recent advances in generative AI, we need new methods to assess and maximize the security of code co-written by machines. As our systems become increasingly heterogeneous, we need practical approaches that work even if some functions are automatically generated, e.g., by deep neural networks. As software systems depend evermore on the software supply chain, we need tools that scale to an entire ecosystem. What kind of vulnerabilities exist in future systems and how do we detect them? When all the shallow bugs are found, how do we discover vulnerabilities hidden deeply in the system? Assuming we cannot find all security flaws, how can we nevertheless protect our system? To answer these questions, we start our roadmap with a survey of recent advances in software security, then discuss open challenges and opportunities, and conclude with a long-term perspective for the field.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec
}

@proceedings{10.1145/3643787,
title = {NLBSE '24: Proceedings of the Third ACM/IEEE International Workshop on NL-based Software Engineering},
year = {2024},
isbn = {9798400705762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Natural Language Processing (NLP) refers to the automated elaboration of human language, including both algorithms that take human-produced text as input and algorithms that produce natural-looking text as outputs. NLP is widely used to optimize many aspects of the software development process. Since natural language artifacts are used and reused during the software development life-cycle, the availability of natural language-based approaches and tools has led to improvements in the software process and product efficiency. Indeed, NLP approaches (including LLMs) have proven useful for retrieving key information from a wide range of structured or unstructured sources. Besides, they show promise for the automated generation of fine-grained source code documentation to ease program comprehension and maintenance activities. Literature has shown that many software engineering (SE)-related tasks can benefit from adopting NLP techniques. The main objective of the Natural Language-Based Software Engineering Workshop (NLBSE) is to bring together researchers and industrial practitioners from the NLP and SE communities to share experiences. Our workshop aims to provide directions for future research and encourage the development of increasingly effective NLP solutions for addressing SE-specific challenges.},
location = {Lisbon, Portugal}
}

@article{10.1145/3720500,
author = {Cho, Minsung and Gouwar, John and Holtzen, Steven},
title = {Scaling Optimization over Uncertainty via Compilation},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3720500},
doi = {10.1145/3720500},
abstract = {Probabilistic inference is fundamentally hard, yet many tasks require optimization on top of inference, which is even harder. We present a new optimization-via-compilation strategy to scalably solve a certain class of such problems. In particular, we introduce a new intermediate representation (IR), binary decision diagrams weighted by a novel notion of branch-and-bound semiring, that enables a scalable branch-and-bound based optimization procedure. This IR automatically factorizes problems through program structure and prunes suboptimal values via a straightforward branch-and-bound style algorithm to find optima. Additionally, the IR is naturally amenable to staged compilation, allowing the programmer to query for optima mid-compilation to inform further executions of the program. We showcase the effectiveness and flexibility of the IR by implementing two performant languages that both compile to it: dappl and pineappl. dappl is a functional language that solves maximum expected utility problems with first-class support for rewards, decision making, and conditioning. pineappl is an imperative language that performs exact probabilistic inference with support for nested marginal maximum a posteriori (MMAP) optimization via staging.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {135},
numpages = {29},
keywords = {maximum expected utility, maximum marginal a posteriori, probabilistic programming languages}
}

@article{10.1145/3657633,
author = {Belussi, Alberto and Migliorini, Sara and Eldawy, Ahmed},
title = {A Generic Machine Learning Model for Spatial Query Optimization based on Spatial Embeddings},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2374-0353},
url = {https://doi.org/10.1145/3657633},
doi = {10.1145/3657633},
abstract = {Machine learning (ML) and deep learning (DL) techniques are increasingly applied to produce efficient query optimizers, in particular in regards to big data systems. The optimization of spatial operations is even more challenging due to the inherent complexity of such kind of operations, like spatial join or range query, and the peculiarities of spatial data. Although a few ML-based spatial query optimizers have been proposed in literature, their design limits their use, since each one is tailored for a specific collection of datasets, a specific operation, or a specific hardware setting. Changes to any of these will require building and training a completely new model which entails collecting a new very large training dataset to obtain a good model.This article proposes a different approach which exploits the use of the novel notion of spatial embedding to overcome these limitations. In particular, a preliminary model is defined which captures the relevant features of spatial datasets, independently from the operation to be optimized and in an unsupervised manner. This model is trained with a large amount of both synthetic and real-world data, with the aim to produce meaningful spatial embeddings. The construction of an embedding model could be intended as a preliminary step for the optimization of many different spatial operations, so the cost of its building can be compensated during the subsequent construction of specific models. Indeed, for each considered spatial operation, a specific tailored model will be trained but by using spatial embeddings as input, so a very little amount of training data points is required for them. Three peculiar operations are considered as proof of concept in this article: range query, self-join, and binary spatial join. Finally, a comparison with an alternative technique, known as transfer learning, is provided and the advantages of the proposed technique over it are highlighted.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = oct,
articleno = {36},
numpages = {33},
keywords = {Query optimizer, machine learning, big data, range query, spatial join, spatial embedding}
}

@inproceedings{10.1145/3634713.3634735,
author = {H\"{o}nig, Michael Franziskus and Eichhorn, Domenik},
title = {Grammars for Feature Models},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634735},
doi = {10.1145/3634713.3634735},
abstract = {Applications for feature models, such as sampling, usually involve exploring a decision structure to systematically generate product configurations. This decision structure is often learned implicitly, using SAT solvers, or explicitly by describing it in the form of a binary decision diagram. Another structure, context-free grammars, have only been discussed for constraint-free feature models. We outline two algorithms that allow the transformation of feature models into context-free grammars and argue that, though those initial algorithms do not perform well, context-free grammars show promising potential for optimizations.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {155–157},
numpages = {3},
keywords = {context-free grammars, feature models, software product lines},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.1109/CGO57630.2024.10444795,
author = {Jayaweera, Malith and Kong, Martin and Wang, Yanzhi and Kaeli, David},
title = {Energy-Aware Tile Size Selection for Affine Programs on GPUs},
year = {2024},
isbn = {9798350395099},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CGO57630.2024.10444795},
doi = {10.1109/CGO57630.2024.10444795},
abstract = {Loop tiling is a high-order transformation used to increase data locality and performance. While previous work has considered its application to several domains and architectures, its potential impact on energy efficiency has been largely ignored. In this work, we present an Energy-Aware Tile Size Selection Scheme (EATSS) for affine programs targeting GPUs. We automatically derive non-linear integer formulations for affine programs and use the Z3 solver to find effective tile sizes that meet architectural resource constraints, while maximizing performance and minimizing energy consumption. Our approach builds on the insight that reducing the liveness of in-cache data, together with exploiting automatic power scaling, can lead to substantial gains in performance and energy efficiency. We evaluate EATSS on NVIDIA Xavier and GA100 GPUs, and report median performance-per-Watt improvement relative to PPCG on several affine kernels. On Polybench kernels, we achieve 1.5\texttimes{} and 1.2\texttimes{} improvement and obtain up to 6.3\texttimes{} improvement on non-Polybench high-dimensional affine kernels.},
booktitle = {Proceedings of the 2024 IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {13–27},
numpages = {15},
keywords = {loop tiling, energy optimization, affine transformations, GPUs},
location = {Edinburgh, United Kingdom},
series = {CGO '24}
}

@article{10.1145/3468504,
author = {Sundelin, Anders and Gonzalez-huerta, Javier and Wnuk, Krzysztof and Gorschek, Tony},
title = {Towards an Anatomy of Software Craftsmanship},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3468504},
doi = {10.1145/3468504},
abstract = {Context: The concept of software craftsmanship has early roots in computing, and in 2009, the Manifesto for Software Craftsmanship was formulated as a reaction to how the Agile methods were practiced and taught. But software craftsmanship has seldom been studied from a software engineering perspective.Objective: The objective of this article is to systematize an anatomy of software craftsmanship through literature studies and a longitudinal case study.Method: We performed a snowballing literature review based on an initial set of nine papers, resulting in&nbsp;18 papers and 11 books. We also performed a case study following seven years of software development of a product for the financial market, eliciting qualitative, and quantitative results. We used thematic coding to synthesize the results into categories.Results: The resulting anatomy is centered around four themes, containing 17 principles and 47 hierarchical practices connected to the principles. We present the identified practices based on the experiences gathered from the case study, triangulating with the literature results.Conclusion: We provide our systematically derived anatomy of software craftsmanship with the goal of inspiring more research into the principles and practices of software craftsmanship and how these relate to other principles within software engineering in general.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {6},
numpages = {49},
keywords = {Software craftsmanship, principles of software development, deliberate practice}
}

@inproceedings{10.1145/3624062.3624140,
author = {Barakhshan, Parinaz and Eigenmann, Rudolf},
title = {CaRV -- Accelerating Program Optimization through Capture, Replay, Validate},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624140},
doi = {10.1145/3624062.3624140},
abstract = {This paper presents a new methodology and tool that speeds up the process of optimizing science and engineering programs. The tool, called CaRV (Capture, Replay, and Validate), enables users to experiment quickly with large applications, comparing individual program sections before and after optimizations in terms of efficiency and accuracy. Using language-level checkpointing techniques, CaRV captures the necessary data for replaying the experimental section as a separate execution unit after the code optimization and validating the optimization against the original program. The tool reduces the amount of time and resources spent on experimentation with long-running programs by up to two orders of magnitude, making program optimization more efficient and cost-effective.},
booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {654–662},
numpages = {9},
keywords = {CaRV Tool, Enhanced Liveness Analysis, Language-level checkpointing, Program Optimization Tool, Reduce Experimentation Time},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@article{10.1145/3680282,
author = {Shashaani, Sara and Eckman, David and Sanchez, Susan},
title = {Data Farming the Parameters of Simulation-Optimization Solvers},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {1049-3301},
url = {https://doi.org/10.1145/3680282},
doi = {10.1145/3680282},
abstract = {The performance of a simulation-optimization algorithm, a.k.a. a solver, depends on its parameter settings. Much of the research to date has focused on how a solver’s parameters affect its convergence and other asymptotic behavior. While these results are important for providing a theoretical understanding of a solver, they can be of limited utility to a user who must set up and run the solver on a particular problem. When running a solver in practice, good finite-time performance is paramount. In this article, we explore the relationship between a solver’s parameter settings and its finite-time performance by adopting a data farming approach. The approach involves conducting and analyzing the outputs of a designed experiment wherein the factors are the solver’s parameters and the responses are assorted performance metrics measuring the solver’s speed and solution quality over time. We demonstrate this approach with a study of the ASTRO-DF solver when solving a stochastic activity network problem and an inventory control problem. Through these examples, we show that how some of the solver’s parameters are set greatly affects its ability to achieve rapid, reliable progress and gain insights into the solver’s inner workings. We discuss the implications of using this framework for tuning solver parameters, as well as for addressing related questions of interest to solver specialists and generalists.},
journal = {ACM Trans. Model. Comput. Simul.},
month = aug,
articleno = {24},
numpages = {29},
keywords = {Data farming, simulation optimization, design of experiments, parameter tuning, ASTRO-DF}
}

@inproceedings{10.5555/3049877.3049903,
author = {Bergen, Andreas and Taherimakhsousi, Nina},
title = {Software energy optimization in the cloud},
year = {2016},
publisher = {IBM Corp.},
address = {USA},
abstract = {A promising avenue to control energy-related costs in enterprise data centers is to investigate power-aware resource management strategies. Mechanisms to accurately capture energy consumption in data centers include source and machine code instruction analysis, kernel sensors, system call monitors and per-VM metering techniques. Though very accurate, these approaches are highly invasive, requiring modifications to software or hardware, and introduce an observer effect that can adversely impact performance. Perhaps most important, results obtained from these approaches require refinement before they can actually be used for management decisions that must strike a balance between costs, SLOs and SLAs. Using existing instrumentation at a rack's PDU provides sufficient granularity to determine the true energy consumption of servers in a non-intrusive way. We show that by leveraging existing instrumentation at a rack's PDU, profiling the type of resource (e.g., CPU, memory, disk, network) a process is using on a given server is not only possible, but highly accurate despite the anticipated signal noise from other servers on a rack's power circuit. This provides a better foundation and allows us to forecast and manage energy demands in data centers.},
booktitle = {Proceedings of the 26th Annual International Conference on Computer Science and Software Engineering},
pages = {243–249},
numpages = {7},
keywords = {ACM proceedings, LaTeX, text tagging},
location = {Toronto, Ontario, Canada},
series = {CASCON '16}
}

@article{10.1145/3718361,
author = {Liu, Tianen and Wang, Shuai and Dong, Zheng and Li, Borui and He, Tian},
title = {From Perception to Computation: Revisiting Delay Optimization for Connected Autonomous Vehicles},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3718361},
doi = {10.1145/3718361},
abstract = {With the development of sensing, wireless communication, and real-time computing technologies, vehicles are gradually becoming more and more intelligent. To provide safe autonomous mobility services, connected autonomous vehicles (CAVs) need to obtain complete information about their environment and process it in real-time to make driving decisions. However, the rapid increase in data volume puts pressure on CAVs to process tasks in real time. This survey analyzes CAVs delay optimization from the perception layer, communication layer, computation layer, and cross-layer. According to different coordination modes, each layer of CAVs is divided, and the problem of delay optimization is classified in fine granularity. This survey will help researchers gain insight into the mechanism of delay optimization on CAVs and highlight the key role of optimized delay in autonomous driving.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {200},
numpages = {45},
keywords = {Connected autonomous vehicle, vehicle edge computing, autonomous driving, delay optimization}
}

@proceedings{10.1145/3674805,
title = {ESEM '24: Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
year = {2024},
isbn = {9798400710476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Barcelona, Spain}
}

@inproceedings{10.1145/3641525.3663624,
author = {Gilbertson, Christian and Mundt, Miranda and Teves, Joshua and Toribio, Simone and Milewicz, Reed},
title = {Towards Evidence-Based Software Quality Practices for Reproducibility: Practices and Aligned Software Qualities},
year = {2024},
isbn = {9798400705304},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641525.3663624},
doi = {10.1145/3641525.3663624},
abstract = {Among computational science and engineering (CSE) software development projects, reproducibility of results is widely understood to be essential for the software to be useful and meaningful. At present, there is a lack of empirical support for how best to design for, implement, or maintain reproducibility over the course of the CSE software lifecycle, and the relationship between reproducibility and other qualities of interest is not well-understood. In this study, we consider the role of software engineering practices and product qualities in enabling reproducibility. To build a foundation for future studies, we conducted case studies of the software engineering practices of 9 teams at Sandia National Laboratories (Sandia), and a first-in-kind online survey of 219&nbsp; developers and users of CSE software to assess their beliefs about quality and reproducibility. Among our results, we identified three practices that were universally attributed to enhancing reproducibility of research and software at Sandia: automated testing, documentation, and version control. We found that the majority of survey respondents believed that high-quality software was more likely to be reproducible than low-quality software, and that maintainability, reliability, and usability were generally seen as positively contributing to reproducibility. These preliminary findings merit follow-up studies to better understand how software quality is valued and enacted in CSE software development practice.},
booktitle = {Proceedings of the 2nd ACM Conference on Reproducibility and Replicability},
pages = {52–63},
numpages = {12},
location = {Rennes, France},
series = {ACM REP '24}
}

@inproceedings{10.1145/3686490.3686495,
author = {Zhu, Yan and Liang, Quanquan and Cui, Kaile},
title = {Fresnel Zone Theory Based Non-Line-of-Sight Respiration Behavior Detection Using USRP},
year = {2024},
isbn = {9798400717192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686490.3686495},
doi = {10.1145/3686490.3686495},
abstract = {Respiration is one of the most important metabolic activities of the living system, and the detection of respiratory status is an important method for the prevention and diagnosis of human diseases. In this paper, a real-time breath detection system based on a universal software radio peripheral (USRP) is designed. With the support of Fresnel zone theory, the amplitude information of OFDM generated multi-subcarriers is collected to detect breathing. We built a respiratory detection system in a real environment, using the hampel function to process the outliers of the collected channel state information (CSI) data after wavelet filtering for noise reduction and smoothing of CSI data, and importing the above pre-processed data into the feature extraction and machine learning algorithms for amplitude map classification. It was found that the accuracy of SVM classification using a combination of both HOG and GLCM features could reach more than 97.5% when judging a respiratory condition alone. However, the accuracy of the method is not satisfactory for classifying more than two cases. In this regard, we can modify the feature extraction method and the classification algorithm. The KNN accuracy is up to 91.67% and Tree accuracy to 95% based on LBP features; for HOG features, the KNN accuracy reaches up to 91.67% and NB accuracy reaches up to 96.67%; under Gabor features, the KNN accuracy comes to 94.17% and NB accuracy to 95%. Our work shows that real-time sensing of normal breathing as well as interval breathing by measuring CSI is feasible in real-world settings.},
booktitle = {Proceedings of the 2024 7th International Conference on Signal Processing and Machine Learning},
pages = {34–41},
numpages = {8},
keywords = {Fresnel zone theory, USRP, amplitude information, machine learning, respiration detection},
location = {Qingdao, China},
series = {SPML '24}
}

@article{10.14778/3696435.3696436,
author = {Arch, Samuel and Liu, Yuchen and Mowry, Todd C. and Patel, Jignesh M. and Pavlo, Andrew},
title = {The Key to Effective UDF Optimization: Before Inlining, First Perform Outlining},
year = {2024},
issue_date = {September 2024},
publisher = {VLDB Endowment},
volume = {18},
number = {1},
issn = {2150-8097},
url = {https://doi.org/10.14778/3696435.3696436},
doi = {10.14778/3696435.3696436},
abstract = {Although user-defined functions (UDFs) are a popular way to augment SQL's declarative approach with procedural code, the mismatch between programming paradigms creates a fundamental optimization challenge. UDF inlining automatically removes all UDF calls by replacing them with equivalent SQL subqueries. Although inlining leaves queries entirely in SQL (resulting in large performance gains), we observe that inlining the entire UDF often leads to sub-optimal performance. A better approach is to analyze the UDF, deconstruct it into smaller pieces, and inline only the pieces that help query optimization. To achieve this, we propose UDF outlining, a technique to intentionally hide pieces of a UDF from the optimizer, resulting in simpler UDFs and significantly faster query plans. Our implementation (PRISM) demonstrates that UDF outlining improves performance over conventional inlining (on average 1.29\texttimes{} speedup for DuckDB and 298.73\texttimes{} for SQL Server) through a combination of more effective unnesting, improved data skipping, and by avoiding unnecessary joins.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {1–13},
numpages = {13}
}

@inproceedings{10.5555/2486788.2486852,
author = {Apel, Sven and Rhein, Alexander von and Wendler, Philipp and Gr\"{o}\ss{}linger, Armin and Beyer, Dirk},
title = {Strategies for product-line verification: case studies and experiments},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Product-line technology is increasingly used in mission-critical and safety-critical applications. Hence, researchers are developing verification approaches that follow different strategies to cope with the specific properties of product lines. While the research community is discussing the mutual strengths and weaknesses of the different strategies—mostly at a conceptual level—there is a lack of evidence in terms of case studies, tool implementations, and experiments. We have collected and prepared six product lines as subject systems for experimentation. Furthermore, we have developed a model-checking tool chain for C-based and Java-based product lines, called SPLVERIFIER, which we use to compare sample-based and family-based strategies with regard to verification performance and the ability to find defects. Based on the experimental results and an analytical model, we revisit the discussion of the strengths and weaknesses of product-line–verification strategies.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {482–491},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/2591028.2600810,
author = {Santos, Jadson and Lima, Gleydson and Kulesza, Uir\'{a} and Sena, Demostenes and Pinto, Felipe and Lima, Jalerson and Vianna, Alexandre and Pereira, David and Fernandes, Victor},
title = {Conditional execution: a pattern for the implementation of fine-grained variabilities in software product lines},
year = {2012},
isbn = {9781450327879},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591028.2600810},
doi = {10.1145/2591028.2600810},
abstract = {This paper presents the Conditional Execution design pattern that aims to help the implementation of fine-grained variabilities in the context of software product lines of information systems. The pattern has been used successfully in three product lines of web information systems developed by Informatics Superintendence (SINFO) at Federal University of Rio Grande do Norte.},
booktitle = {Proceedings of the 9th Latin-American Conference on Pattern Languages of Programming},
articleno = {1},
numpages = {17},
keywords = {design pattern, fine-grained variabilities, software product lines},
location = {Natal, Rio Grande do Norte, Brazil},
series = {SugarLoafPLoP '12}
}

@inproceedings{10.1145/3634713.3634726,
author = {Martou, Pierre and Duhoux, Beno\^{\i}t and Mens, Kim and Legay, Axel},
title = {Combinatorial Transition Testing in Dynamically Adaptive Systems},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634726},
doi = {10.1145/3634713.3634726},
abstract = {Due to the large number of possible interactions and transitions among features in dynamically adaptive systems, testing such systems poses significant challenges. To verify that such systems behave correctly, the technique of combinatorial interaction testing (CIT) can be used to create concise test suites covering all valid pairs of features of such systems. However, while CIT claims to find all errors caused by two features, we show that it does not cover certain errors occurring only for specific transitions between two features. To address this issue we study in depth the complementary technique of Combinatorial Transition Testing (CTT). From an initial generation algorithm that combines both interaction and transition coverage, we propose an optimised version that reduces the size of generated test suites by ∼ 30%, reconfiguration cost by ∼ 27% and drastically stabilises these results. After multiple generations, the standard deviation on the sizes of generated test suites is reduced by ∼ 81%. Based on a comprehensive analysis over a large number of feature models, we also conclude that the size of CTT-generated test suites is linearly correlated to CIT-generated ones and that combinatorial transition testing also grows logarithmically in the number of features.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {1–10},
numpages = {10},
keywords = {combinatorial testing, dynamic software product lines, dynamically adaptive software systems, feature modelling, software testing, transition testing},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.1145/3639477.3639747,
author = {Butler, Jenna L. and Zimmermann, Thomas and Bird, Christian},
title = {Objectives and Key Results in Software Teams: Challenges, Opportunities and Impact on Development},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639747},
doi = {10.1145/3639477.3639747},
abstract = {Building software, like building almost anything, requires people to understand a common goal and work together towards it. In large software companies, a VP or Director will have an idea or goal and it is often the job of middle management to distill that lofty, general idea into manageable, finite units of work. How do organizations do this hard work of setting and measuring progress towards goals? To understand this question, we undertook a mixed methods approach to studying goal setting, management dissemination of goals, goal tracking and ultimately software delivery at a large multi-national software company.Semi-structured interviews with 47 participants were analyzed and used to develop a survey which was deployed to a multinational organization of over 4,000 engineers. The 512 responses were analyzed using thematic analysis, linear regressions and hypothesis testing, and found that tracking, measuring and setting goals is hard work, regardless of tools used. Middle management seems to be a critical component of the translation of lofty goals to actionable work items. In addition, attitudes and beliefs of engineers are critical to the success of any goal setting framework. Based on this research, we make recommendations on how to improve the goal setting and OKR process in software organizations: invest in the data pipeline, increase transparency, improve communication, promote learning communities, and a structured roll out of OKRs.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {358–368},
numpages = {11},
keywords = {objectives and key results, software development, organizational behavior, goal setting, mixed methods},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3691620.3695555,
author = {Zhang, Jian and Wang, Chong and Li, Anran and Wang, Wenhan and Li, Tianlin and Liu, Yang},
title = {VulAdvisor: Natural Language Suggestion Generation for Software Vulnerability Repair},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695555},
doi = {10.1145/3691620.3695555},
abstract = {Software vulnerabilities pose serious threats to the security of modern software systems. Deep Learning-based Automated Vulnerability Repair (AVR) has gained attention as a potential solution to accelerate the remediation of vulnerabilities. However, recent studies indicate that existing AVR approaches often only generate patches, which may not align with developers' current repair practices or expectations. In this paper, we introduce VulAdvisor, an automated approach that generates natural language suggestions to guide developers or AVR tools in repairing vulnerabilities. VulAdvisor comprises two main components: oracle extraction and suggestion learning. To address the challenge of limited historical data, we propose an oracle extraction method facilitating ChatGPT to construct a comprehensive and high-quality dataset. For suggestion learning, we take the supervised fine-tuning CodeT5 model as the basis, integrating local context into Multi-Head Attention and introducing a repair action loss, to improve the relevance and meaningfulness of the generated suggestions. Extensive experiments on a large-scale dataset from real-world C/C++ projects demonstrate the effectiveness of VulAdvisor, surpassing several alternatives in terms of both lexical and semantic metrics. Moreover, we show that the generated suggestions enhance the patch generation capabilities of existing AVR tools. Human evaluations further validate the quality and utility of VulAdvisor's suggestions, confirming their potential to improve software vulnerability repair practices.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1932–1944},
numpages = {13},
keywords = {vulnerability repair, large language models, suggestion generation, program repair},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@article{10.5555/3722577.3722730,
author = {H\"{u}ttenrauch, Maximilian and Neumann, Gerhard},
title = {Robust black-box optimization for stochastic search and episodic reinforcement learning},
year = {2024},
issue_date = {January 2024},
publisher = {JMLR.org},
volume = {25},
number = {1},
issn = {1532-4435},
abstract = {Black-box optimization is a versatile approach to solve complex problems where the objective function is not explicitly known and no higher order information is available. Due to its general nature, it finds widespread applications in function optimization as well as machine learning, especially episodic reinforcement learning tasks. While traditional black-box optimizers like CMA-ES may falter in noisy scenarios due to their reliance on ranking-based transformations, a promising alternative emerges in the form of the Model-based Relative Entropy Stochastic Search (MORE) algorithm. MORE can be derived from natural policy gradients and compatible function approximation and directly optimizes the expected fitness without resorting to rankings. However, in its original formulation, MORE often cannot achieve state of the art performance. In this paper, we improve MORE by decoupling the update of the search distribution's mean and covariance and an improved entropy scheduling technique based on an evolution path resulting in faster convergence, and a simplified model learning approach in comparison to the original paper. We show that our algorithm performs comparable to state-of-the-art black-box optimizers on standard benchmark functions. Further, it clearly outperforms ranking-based methods and other policy-gradient based black-box algorithms as well as state of the art deep reinforcement learning algorithms when used for episodic reinforcement learning tasks.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {153},
numpages = {44},
keywords = {black-box optimization, stochastic search, derivative-free optimization, evolution strategies, episodic reinforcement learning}
}

@article{10.1145/3622797,
author = {Yi, Pu (Luke) and Achour, Sara},
title = {Hardware-Aware Static Optimization of Hyperdimensional Computations},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622797},
doi = {10.1145/3622797},
abstract = {Binary spatter code (BSC)-based hyperdimensional computing (HDC) is a highly error-resilient approximate computational paradigm suited for error-prone, emerging hardware platforms. In BSC HDC, the basic datatype is a hypervector, a typically large binary vector, where the size of the hypervector has a significant impact on the fidelity and resource usage of the computation. Typically, the hypervector size is dynamically tuned to deliver the desired accuracy; this process is time-consuming and often produces hypervector sizes that lack accuracy guarantees and produce poor results when reused for very similar workloads. We present Heim, a hardware-aware static analysis and optimization framework for BSC HD computations. Heim analytically derives the minimum hypervector size that minimizes resource usage and meets the target accuracy requirement. Heim guarantees the optimized computation converges to the user-provided accuracy target on expectation, even in the presence of hardware error. Heim deploys a novel static analysis procedure that unifies theoretical results from the neuroscience community to systematically optimize HD computations. We evaluate Heim against dynamic tuning-based optimization on 25 benchmark data structures. Given a 99% accuracy requirement, Heim-optimized computations achieve a 99.2%-100.0% median accuracy, up to 49.5% higher than dynamic tuning-based optimization, while achieving 1.15x-7.14x reductions in hypervector size compared to HD computations that achieve comparable query accuracy and finding parametrizations 30.0x-100167.4x faster than dynamic tuning-based approaches. We also use Heim to systematically evaluate the performance benefits of using analog CAMs and multiple-bit-per-cell ReRAM over conventional hardware, while maintaining iso-accuracy – for both emerging technologies, we find usages where the emerging hardware imparts significant benefits.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {222},
numpages = {30},
keywords = {emerging hardware technologies, program optimization, unconventional computing}
}

@article{10.1145/3640333,
author = {Shao, Changjie and Li, Gaolei and Wu, Jun and Zheng, Xi},
title = {Exploring Semantic Redundancy using Backdoor Triggers: A Complementary Insight into the Challenges Facing DNN-based Software Vulnerability Detection},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3640333},
doi = {10.1145/3640333},
abstract = {To detect software vulnerabilities with better performance, deep neural networks (DNNs) have received extensive attention recently. However, these vulnerability detection DNN models trained with code representations are vulnerable to specific perturbations on code representations. This motivates us to rethink the bane of software vulnerability detection and find function-agnostic features during code representation which we name as semantic redundant features. This paper first identifies a tight correlation between function-agnostic triggers and semantic redundant feature space (where the redundant features reside) in these DNN models. For correlation identification, we propose a novel Backdoor-based Semantic Redundancy Exploration (BSemRE) framework. In BSemRE, the sensitivity of the trained models to function-agnostic triggers is observed to verify the existence of semantic redundancy in various code representations. Specifically, acting as the typical manifestations of semantic redundancy, naming conventions, ternary operators and identically-true conditions are exploited to generate function-agnostic triggers. Extensive comparative experiments on 1,613,823 samples of eight representative vulnerability datasets and state-of-the-art code representation techniques and vulnerability detection models demonstrate that the existence of semantic redundancy determines the upper trustworthiness limit of DNN-based software vulnerability detection. To the best of our knowledge, this is the first work exploring the bane of software vulnerability detection using backdoor triggers.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {92},
numpages = {28},
keywords = {Software vulnerability detection, deep neural networks, backdoor triggers, semantic redundancy, function-agnostic}
}

@inproceedings{10.1145/3650212.3680378,
author = {Xu, Jinchen and Cui, Mengqi and Li, Fei and Zhang, Zuoyan and Yang, Hongru and Zhou, Bei and Zhao, Jie},
title = {Arfa: An Agile Regime-Based Floating-Point Optimization Approach for Rounding Errors},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680378},
doi = {10.1145/3650212.3680378},
abstract = {We introduce a floating-point (FP) error optimization approach called Arfa that partitions the domain D of an FP expression fe into regimes and rewrites fe in each regime where fe shows larger errors. First, Arfa seeks a rewrite substitution fo with lower errors across D, whose error distribution is plotted for effective regime inference. Next, Arfa generates an incomplete set of ordered rewrite candidates within each regime of interest, so that searching for the best rewrite substitutions is performed efficiently. Finally, Arfa selects the best rewrite substitution by inspecting the errors of top ranked rewrite candidates, with enhancing precision also considered. Experiments on 56 FPbench examples and four real-life programs show that Arfa not only reduces the maximum and average errors of fe by 4.73 and 2.08 bits on average (and up to 33 and 16 bits), but also exhibits lower errors, sometimes to a significant degree, than Herbie and NumOpt.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1516–1528},
numpages = {13},
keywords = {FPbench, dynamic analysis, e-graph, floating-point errors, numerical analysis, rewrite},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3642921.3642923,
author = {Fu, Vincent and Zaourar, Lilia and Munier-Kordon, Alix and Duranton, Marc},
title = {Design Space Exploration of HPC Systems with Random Forest-based Bayesian Optimization},
year = {2024},
isbn = {9798400717918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3642921.3642923},
doi = {10.1145/3642921.3642923},
abstract = {Nowadays, High-Performance Computing (HPC) systems need to deliver computational performance by processing complex applications and workloads at high speeds in parallel. To provide computing power, Multiprocessor System-on-Chip, the main design paradigm, is scaled with advanced technology nodes and even with heterogeneity. These improvements open up more design possibilities, leading to an increase in its complexity. Therefore, chip designers are facing unprecedented challenges to find the best Power, Performance, and Area architectural configurations, inducing a Design Space Exploration problem. This work proposes a complete framework to ease the next generation of HPC processor designs. By combining competitive simulators VPSim and McPAT for a realistic estimation of Key Performance Indicators, with a time-consuming simulation-adapted exploration algorithm such as Bayesian Optimization, we leveraged an Automated Design Space Exploration for efficient HPC processor designs based on ARMv8 architecture. We have also demonstrated the potential of Bayesian Optimization to reach a similar or even larger Pareto front than Genetic Algorithm while being around 2 \texttimes{} to 5 \texttimes{} sample-efficient. Furthermore, the diversity of the obtained Pareto-front enables deep analysis of relevant architectural parameters that significantly impact design performances and thus, empowering architects’ knowledge for further targeted design exploration and design choices.},
booktitle = {Proceedings of the 16th Workshop on Rapid Simulation and Performance Evaluation for Design},
pages = {9–15},
numpages = {7},
keywords = {Automated Design Space Exploration, Bayesian Optimization, High-Performance Computing, Multiprocessor System-on-Chip, Random Forest},
location = {Munich, Germany},
series = {RAPIDO '24}
}

@inproceedings{10.1145/3705374.3705376,
author = {Wu, Chenyang and Zhang, Zheng and Su, Jiang and Li, Zhonghao and Gao, Qi},
title = {Research Hotspot and Frontier Analysis of Vehicle Routing Optimization Based on Knowledge Graph},
year = {2025},
isbn = {9798400710445},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3705374.3705376},
doi = {10.1145/3705374.3705376},
abstract = {Vehicle routing problem is a NP problem. With the continuous in-depth study of VRP model and its solving methods, many branch problems with more constraints have been developed, and their solving methods have become more and more diverse. In order to further review the research status of VRP at home and abroad. This paper employs bibliometric methods and knowledge graph analysis to examine publishing institutions, authors, and research hotspots in vehicle routing optimization studies both domestically and internationally from 2008 to 2023, aiming to provide insights for the field of vehicle routing optimization in China. The main research content includes the use of CiteSpace visualization software to generate the keyword co-occurrence and clustering knowledge map of both domestic and international literature, as well as keyword burst detection maps, so as to interpret the research status in the field. The results show that in the field of vehicle routing optimization, the cooperation relationship between foreign institutions is closer than that of domestic institutions, and research hotspots differ significantly between domestic and foreign studies. Foreign scholars pay more attention to the combination of theory and practice, while domestic scholars focus more on deepening algorithm research. Meanwhile, the research processes at home and abroad show different characteristics. Cold chain logistics and urban transportation have become areas of focus for domestic scholars, who have begun to shift from traditional vehicle route optimization problems to in-depth studies of green transportation. Integrating big data analysis and artificial intelligence technologies into vehicle route optimization is a future research direction.},
booktitle = {Proceedings of the 2024 8th International Conference on Computing and Data Analysis},
pages = {9–16},
numpages = {8},
keywords = {Bibliometrics, CiteSpace, Knowledge graph, Vehicle routing optimization},
location = {
},
series = {ICCDA '24}
}

@article{10.5555/3722577.3722750,
author = {Bodnar, Taras and Parolya, Nestor and Thors\'{e}n, Erik},
title = {Two is better than one: regularized shrinkage of large minimum variance portfolios},
year = {2024},
issue_date = {January 2024},
publisher = {JMLR.org},
volume = {25},
number = {1},
issn = {1532-4435},
abstract = {In this paper, we construct a shrinkage estimator of the global minimum variance (GMV) portfolio by combining two techniques: Tikhonov regularization and direct shrinkage of portfolio weights. More specifically, we employ a double shrinkage approach, where the covariance matrix and portfolio weights are shrunk simultaneously. The ridge parameter controls the stability of the covariance matrix, while the portfolio shrinkage intensity shrinks the regularized portfolio weights to a predefined target. Both parameters simultaneously minimize, with probability one, the out-of-sample variance as the number of assets p and the sample size n tend to infinity, while their ratio p/n tends to a constant c &gt; 0. This method can also be seen as the optimal combination of the well-established linear shrinkage approach of Ledoit and Wolf (2004) and the shrinkage of the portfolio weights by Bodnar et al. (2018). No specific distribution is assumed for the asset returns, except for the assumption of finite moments of order 4 + ε for ε &gt; 0. The performance of the double shrinkage estimator is investigated via extensive simulation and empirical studies. The suggested method significantly outperforms its predecessor (without regularization) and the nonlinear shrinkage approach in terms of the out-of-sample variance, Sharpe ratio, and other empirical measures in the majority of scenarios. Moreover, it maintains the most stable portfolio weights with uniformly smallest turnover.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {173},
numpages = {32},
keywords = {shrinkage estimator, high dimensional covariance matrix, random matrix theory, minimum variance portfolio, parameter uncertainty, ridge regularization}
}

@article{10.1145/3528100,
author = {Cheng, Jiezhu and Gao, Cuiyun and Zheng, Zibin},
title = {HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3528100},
doi = {10.1145/3528100},
abstract = {Modern software systems are usually highly configurable, providing users with customized functionality through various configuration options. Understanding how system performance varies with different option combinations is important to determine optimal configurations that meet specific requirements. Due to the complex interactions among multiple options and the high cost of performance measurement under a huge configuration space, it is challenging to study how different configurations influence the system performance. To address these challenges, we propose HINNPerf, a novel hierarchical interaction neural network for performance prediction of configurable systems. HINNPerf employs the embedding method and hierarchic network blocks to model the complicated interplay between configuration options, which improves the prediction accuracy of the method. In addition, we devise a hierarchical regularization strategy to enhance the model robustness. Empirical results on 10 real-world configurable systems show that our method statistically significantly outperforms state-of-the-art approaches by achieving average 22.67% improvement in prediction accuracy. In addition, combined with the Integrated Gradients method, the designed hierarchical architecture provides some insights about the interaction complexity and the significance of configuration options, which might help users and developers better understand how the configurable system works and efficiently identify significant options affecting the performance.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {46},
numpages = {30},
keywords = {Software performance prediction, highly configurable systems, deep neural network, machine learning}
}

@inproceedings{10.1145/3638530.3664124,
author = {Halsema, Martijn and Vermetten, Diederick and B\"{a}ck, Thomas and Van Stein, Niki},
title = {A Critical Analysis of Raven Roost Optimization},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638530.3664124},
doi = {10.1145/3638530.3664124},
abstract = {This study critically examines the Raven Roost Optimization (RRO) algorithm within the broader context of nature-inspired metaheuristics, challenging its novelty and efficacy in the field of black-box optimization. Many similar methods use ideas from nature, but it's important to see if they really bring something new to the table. We compared RRO with another well-known method called Particle Swarm Optimization (PSO) to see how well RRO works and if it's truly a new idea. Through comprehensive analysis and benchmarking, we reveal that RRO's purported novelty largely recapitulates existing strategies under a new metaphorical guise. The algorithm's performance is systematically evaluated across various dimensions, revealing inherent limitations and biases introduced by its metaphorical foundation. Our findings advocate for a critical reassessment of metaphor-based heuristics, urging the computational intelligence community to prioritize substantive algorithmic advancements over superficial novelty. The call for rigorous evaluation and validation of new optimization methods is underscored, emphasizing the need for transparency, reproducibility, and genuine innovation in algorithmic design. This work contributes to the ongoing discourse on the validation and merit of bio-inspired algorithms, providing insights that may guide future research towards more meaningful and empirically justified contributions.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1993–2001},
numpages = {9},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@inproceedings{10.1145/3609703.3609715,
author = {Wu, Linfeng and Yang, Xiaowei and Yang, Hao and Zhu, Zhenhui and Chen, Shunli},
title = {A study on the line loss index of a substation area based on cooperative games with multiple influencing factors},
year = {2023},
isbn = {9781450399968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609703.3609715},
doi = {10.1145/3609703.3609715},
abstract = {The line loss rate varies significantly among different substation areas due to diverse influencing factors. Consequently, a study is conducted to investigate the line loss index of a substation area by employing a cooperative game approach that considers multiple influencing factors. Firstly, utilizing the available fundamental data of the substation area, construct a substation area factor suitable for the calculation of "one substation area, one index". Subsequently, an initial low-voltage substation area line loss prediction model was constructed using Bi-LSTM. Finally, the weights of each influencing factor are calculated using a cooperative game strategy, and the attention mechanism is applied to Bi-LSTM. After the model is trained and optimized, the predicted value for the line loss index for each substation area is output. Experiments indicate that the algorithm can effectively enhances the accuracy of predicting the line loss index value in the substation area, and assist in customized and refined management of loss reduction in the low-voltage distribution substation area.},
booktitle = {Proceedings of the 2023 5th International Conference on Pattern Recognition and Intelligent Systems},
pages = {71–76},
numpages = {6},
keywords = {Attention mechanism, Bi-LSTM, Characteristic indicator factor, Cooperative game, Line loss rate},
location = {Shenyang, China},
series = {PRIS '23}
}

@proceedings{10.1145/3617694,
title = {EAAMO '23: Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization},
year = {2023},
isbn = {9798400703812},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Boston, MA, USA}
}

@proceedings{10.5555/3623290,
title = {ICSE-SEIS '23: Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Society},
year = {2023},
isbn = {9798350322613},
publisher = {IEEE Press},
abstract = {We are delighted to introduce the Software Engineering in Society (SEIS) track program as part of the 45th IEEE/ACM International Conference on Software Engineering, to be held in Melbourne, Australia, on May 14-20, 2023. The aim of the track is to bring together researchers studying various roles that software engineering plays in society.},
location = {Melbourne, Australia}
}

@inproceedings{10.1109/CGO53902.2022.9741277,
author = {Sommer, Lukas and Axenie, Cristian and Koch, Andreas},
title = {SPNC: an open-source MLIR-based compiler for fast sum-product network inference on CPUs and GPUs},
year = {2022},
isbn = {9781665405843},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CGO53902.2022.9741277},
doi = {10.1109/CGO53902.2022.9741277},
abstract = {Sum-Product Networks (SPNs) are an alternative to the widely used Neural Networks (NNs) for machine learning. SPNs can not only reason about (un)certainty by qualifying their output with a probability, they also allow fast (tractable) inference by having run-times that are just linear w.r.t. the network size.We present SPNC, the first tool flow for generating fast native code for SPN inference on both CPUs and GPUs, including the use of vectorized/SIMD execution. To this end, we add two SPN-specific dialects to the MLIR framework and discuss their lowering towards the execution targets.We evaluate our approach on two applications, for which we consider performance, scaling to very large SPNs, and compile vs execution-time trade-offs. In this manner, we achieve multiple orders of magnitude in speed-ups over existing SPN support libraries.},
booktitle = {Proceedings of the 20th IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {290–300},
numpages = {11},
keywords = {CPU, GPU, LLVM, MLIR, machine learning, sum-product networks},
location = {Virtual Event, Republic of Korea},
series = {CGO '22}
}

@inproceedings{10.1145/3677779.3677815,
author = {Guan, Jing},
title = {Research on Human-Computer Interaction Design Standards in Artificial Intelligence Products},
year = {2024},
isbn = {9798400709760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677779.3677815},
doi = {10.1145/3677779.3677815},
abstract = {This article first analyzes the concept and development process of artificial intelligence products, then delves into the application and development of human-computer interaction technology in artificial intelligence products, and then analyzes the standardization research of human-computer interaction design in artificial intelligence products; Finally, the two-dimensional architecture of human-computer interaction design for artificial intelligence products was elaborated, with detailed discussions and planning in terms of technical and value dimensions. The rapid development of artificial intelligence has driven the emergence of a large number of artificial intelligence products, resulting in a fundamental change in the human-computer interaction mode of products and higher requirements for human-computer interaction design. Therefore, how to construct standardized human-computer interaction design patterns in artificial intelligence products in the new era is a key research topic in the development of artificial intelligence products.},
booktitle = {Proceedings of the International Conference on Modeling, Natural Language Processing and Machine Learning},
pages = {220–225},
numpages = {6},
location = {Xi'an, China},
series = {CMNM '24}
}

@proceedings{10.1145/3629479,
title = {SBQS '23: Proceedings of the XXII Brazilian Symposium on Software Quality},
year = {2023},
isbn = {9798400707865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bras\'{\i}lia, Brazil}
}

@inproceedings{10.1145/3631991.3631995,
author = {I, Rinish Sam and V S, Anirudh and P, Indresh and S, Mary Saira Bhanu},
title = {ROTI-MARK: Real-world-impact-driven Optimization Target Identification - Metrics Assessment and Reporting Kit},
year = {2023},
isbn = {9798400708053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631991.3631995},
doi = {10.1145/3631991.3631995},
abstract = {The work introduces a new approach to profiling web applications in production environments that is focused on measuring the impact on user experience. While traditional profilers are used to identify performance bottlenecks by measuring the execution time of different functions or methods, they do not take into account other factors that impact performance, such as memory usage, I/O operations, and context switches. The proposed approach collects traces that contain all the necessary information to compute execution blocks and metrics for performance analysis. This allows developers to pinpoint performance bottlenecks to a few lines of code and optimize accordingly. Additionally, the proposed approach calculates the Data Driven Impact Heuristic (DDIH) which takes into account various factors such as CPU time, response time, memory usage, I/O read and write, voluntary context switches, and the number of times an execution block is called in the real world. The DDIH provides a comprehensive metric that enables developers to assess the impact of their code on user experience and prioritize optimization efforts accordingly. Furthermore, the proposed approach includes a feature for distributed trace collection which helps developers to identify potential issues that may not be apparent in a local environment. This enables one to gain a more holistic understanding of how the code affects other parts of the system and make better-informed optimization decisions.},
booktitle = {Proceedings of the 2023 5th World Symposium on Software Engineering},
pages = {27–32},
numpages = {6},
keywords = {Call graph, Dynamic analysis, Function call traces, Large-scale applications, Performance bottlenecks, Performance profiling, Prioritisation, Resource-intensive, User experience, Web server},
location = {Tokyo, Japan},
series = {WSSE '23}
}

@inproceedings{10.1109/PACT58117.2023.00015,
author = {Elnawawy, Hussein and Tuck, James and Byrd, Gregory T.},
title = {PreFlush: Lightweight Hardware Prediction Mechanism for Cache Line Flush and Writeback},
year = {2024},
isbn = {9798350342543},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/PACT58117.2023.00015},
doi = {10.1109/PACT58117.2023.00015},
abstract = {Non-Volatile Main Memory (NVMM) technologies make it possible for applications to permanently store data in memory. To do so, they need to make sure that updates to persistent data comply with the crash consistency model, which often involves explicitly flushing a dirty cache line after a store and then waiting for the flush operation to complete using a store fence. While cache line flush and write back instructions can complete in the background, fence instructions expose the latency of flushing to the critical path of the program's execution, incurring significant overheads. If flush operations are started earlier, the penalty of fences can be significantly reduced.We propose PreFlush, a lightweight and transparent hardware mechanism that predicts when a cache line flush or write back is needed and speculatively performs the operation early. Since we speculatively perform the flush, we add hardware to handle flush misspeculation to ensure correct execution of the code without the need for any complex recovery mechanisms. Our PreFlush design is transparent to the programmer (i.e. it requires no modification on existing NVMM-enabled code). Our results show that PreFlush can improve performance by up to 25% (15.7% average) for the WHISPER NVM benchmark suite and loop-based matrix microbenchmarks.},
booktitle = {Proceedings of the 32nd International Conference on Parallel Architectures and Compilation Techniques},
pages = {74–85},
numpages = {12},
keywords = {Cache Architecture, Non-Volatile Memory},
location = {Vienna, AE, Austria},
series = {PACT '23}
}

@inproceedings{10.1145/3623263.3623356,
author = {Sun, Xingyuan and Roeder, Geoffrey and Xue, Tianju and Adams, Ryan P. and Rusinkiewicz, Szymon},
title = {More Stiffness with Less Fiber: End-to-End Fiber Path Optimization for 3D-Printed Composites},
year = {2023},
isbn = {9798400703195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623263.3623356},
doi = {10.1145/3623263.3623356},
abstract = {In 3D printing, stiff fibers (e.g., carbon fiber) can reinforce thermoplastic polymers with limited stiffness. However, existing commercial digital manufacturing software only provides a few simple fiber layout algorithms, which solely use the geometry of the shape. In this work, we build an automated fiber path planning algorithm that maximizes the stiffness of a 3D print given specified external loads. We formalize this as an optimization problem: an objective function is designed to measure the stiffness of the object while regularizing certain properties of fiber paths (e.g., smoothness). To initialize each fiber path, we use finite element analysis to calculate the stress field on the object and greedily “walk” in the direction of the stress field. We then apply a gradient-based optimization algorithm that uses the adjoint method to calculate the gradient of stiffness with respect to fiber layout. We compare our approach, in both simulation and real-world experiments, to three baselines: (1) concentric fiber rings generated by Eiger, a leading digital manufacturing software package developed by Markforged, (2) greedy extraction on the simulated stress field (i.e., our method without optimization), and (3) the greedy algorithm on a fiber orientation field calculated by smoothing the simulated stress fields. The results show that objects with fiber paths generated by our algorithm achieve greater stiffness while using less fiber than the baselines—our algorithm improves the Pareto frontier of object stiffness as a function of fiber usage. Ablation studies show that the smoothing regularizer is needed for feasible fiber paths and stability of optimization, and multi-resolution optimization helps reduce the running time compared to single-resolution optimization.},
booktitle = {Proceedings of the 8th ACM Symposium on Computational Fabrication},
articleno = {8},
numpages = {14},
keywords = {3D Printing, Continuous Fiber, End-to-End Differentiable, Optimization, Path Planning},
location = {New York City, NY, USA},
series = {SCF '23}
}

@article{10.1145/3626750,
author = {Song, Haoze and Zhou, Wenchao and Li, Feifei and Peng, Xiang and Cui, Heming},
title = {Rethink Query Optimization in HTAP Databases},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
url = {https://doi.org/10.1145/3626750},
doi = {10.1145/3626750},
abstract = {The advent of data-intensive applications has fueled the evolution of hybrid transactional and analytical processing (HTAP). To support mixed workloads, distributed HTAP databases typically maintain two data copies that are specially tailored for data freshness and performance isolation. In particular, a copy in a row-oriented format is well-suited for OLTP workloads, and a second copy in a column-oriented format is optimized for OLAP workloads. Such a hybrid design opens up a new design space for query optimization: plans can be optimized over different data formats and can be executed over isolated resources, which we term hybrid plans. In this paper, we demonstrate that hybrid plans can largely benefit query execution (e.g., up to 11x speedups in our evaluation). However, we also found these benefits will potentially be at the cost of sacrificing data freshness or performance isolation since traditional optimizers may not precisely model and schedule the execution of hybrid plans on real-time updated HTAP databases. Therefore, we propose Metis, an HTAP-aware optimizer. We show, both theoretically and experimentally, that using the proposed optimizations, a system can largely benefit from hybrid plans while preserving isolated performance for OLTP and OLAP, and these optimizations are robust to the changes in workloads.},
journal = {Proc. ACM Manag. Data},
month = dec,
articleno = {256},
numpages = {27},
keywords = {HTAP database, hybrid physical format, query optimization}
}

@inproceedings{10.1145/3691620.3695366,
author = {Lemberger, Thomas and Wachowitz, Henrik},
title = {CoVeriTeam GUI: A No-Code Approach to Cooperative Software Verification},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695366},
doi = {10.1145/3691620.3695366},
abstract = {We present CoVeriTeam GUI, a No-Code web frontend to compose new software-verification workflows from existing analysis techniques. Verification approaches stopped relying on single techniques years ago, and instead combine selections that complement each other well. So far, such combinations were---under high implementation and maintenance cost---glued together with proprietary code. Now, CoVeriTeam GUI enables users to build new verification workflows without programming. Verification techniques can be combined through various composition operators in a drag-and-drop fashion directly in the browser, and an integration with a remote service allows to execute the built workflows with the click of a button. CoVeriTeam GUI is available open source under Apache 2.0: https://gitlab.com/sosy-lab/software/coveriteam-guiDemonstration video: https://youtu.be/oZoOARuIOuA},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {2419–2422},
numpages = {4},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3694812.3699928,
author = {Verninas, Hippolyte and Linguaglossa, Leonardo},
title = {Optimizing Energy Consumption through Scheduling in Low-resource Edge Clusters using Multi-agent PPO},
year = {2024},
isbn = {9798400712555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3694812.3699928},
doi = {10.1145/3694812.3699928},
abstract = {With the growing demand for computing resources, data centers must optimize energy consumption while maintaining performance. This paper focuses on optimizing job scheduling in low-resource edge clusters using Multi-Agent Proximal Policy Optimization (MAPPO). Cloud computing offers scalability and flexibility but faces challenges in energy efficiency due to the high consumption of traditional data centers. By leveraging low-resource computational clusters at the edge, we aim at reducing energy costs while meeting performance needs. A MAPPO-based scheduling policy is proposed to dynamically allocate jobs between the cloud and machines in a low-power cluster, balancing energy efficiency and scalability. The policy was designed for real-world deployment, ensuring fast decision-making and effective resource management. We evaluate the model's effectiveness in minimizing energy usage.},
booktitle = {Proceedings of the CoNEXT on Student Workshop 2024},
pages = {17–18},
numpages = {2},
keywords = {PPO, SBC, experimental evaluation, high-speed, low-energy, networking, reinforcement learning, system design},
location = {Los Angeles, CA, USA},
series = {CoNEXT-SW '24}
}

@inproceedings{10.1145/1868688.1868692,
author = {Hofer, Wanja and Elsner, Christoph and Blendinger, Frank and Schr\"{o}der-Preikschat, Wolfgang and Lohmann, Daniel},
title = {Toolchain-independent variant management with the Leviathan filesystem},
year = {2010},
isbn = {9781450302081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868688.1868692},
doi = {10.1145/1868688.1868692},
abstract = {Preprocessor-configured software needs tool support for the developer to be able to cope with the complexity introduced by optional and alternative code blocks in the source. Current approaches, which assist the software developer by providing preprocessed views, are all bound to a special integrated development environment. This eliminates them from being used both in industry settings (where domain-specific toolchains are often mandated) and in open-source projects (where diverse sets of editors and tools are being used and freedom of tool choice is crucial for the project success).We therefore propose to tackle the problem at a lower level by implementing variant views at the filesystem level. By mounting one or more variants using our Leviathan filesystem, we enable the use of standard tools such as syntax validators, code metric analysis tools, or arbitrary editors to view or modify a variant. The major benefit (and challenge) is the support for automatically writing back to the configurable code base when editing one of the mounted variant views.},
booktitle = {Proceedings of the 2nd International Workshop on Feature-Oriented Software Development},
pages = {18–24},
numpages = {7},
keywords = {Leviathan, filesystem views, preprocessor-based configuration, software product lines, toolchain-independent variability support, variability implementation},
location = {Eindhoven, The Netherlands},
series = {FOSD '10}
}

@inproceedings{10.1145/2814270.2814309,
author = {Oh, Hakjoo and Yang, Hongseok and Yi, Kwangkeun},
title = {Learning a strategy for adapting a program analysis via bayesian optimisation},
year = {2015},
isbn = {9781450336895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814270.2814309},
doi = {10.1145/2814270.2814309},
abstract = {Building a cost-effective static analyser for real-world programs is still regarded an art. One key contributor to this grim reputation is the difficulty in balancing the cost and the precision of an analyser. An ideal analyser should be adaptive to a given analysis task, and avoid using techniques that unnecessarily improve precision and increase analysis cost. However, achieving this ideal is highly nontrivial, and it requires a large amount of engineering efforts. In this paper we present a new approach for building an adaptive static analyser. In our approach, the analyser includes a sophisticated parameterised strategy that decides, for each part of a given program, whether to apply a precision-improving technique to that part or not. We present a method for learning a good parameter for such a strategy from an existing codebase via Bayesian optimisation. The learnt strategy is then used for new, unseen programs. Using our approach, we developed partially flow- and context-sensitive variants of a realistic C static analyser. The experimental results demonstrate that using Bayesian optimisation is crucial for learning from an existing codebase. Also, they show that among all program queries that require flow- or context-sensitivity, our partially flow- and context-sensitive analysis answers the 75% of them, while increasing the analysis cost only by 3.3x of the baseline flow- and context-insensitive analysis, rather than 40x or more of the fully sensitive version.},
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {572–588},
numpages = {17},
keywords = {Bayesian Optimization, rogram Analysis},
location = {Pittsburgh, PA, USA},
series = {OOPSLA 2015}
}

@article{10.1145/3674729,
author = {Zhang, Tiehua and Xu, Rui and Zhang, Jianping and Liu, Yuze and Chen, Xin and Yin, Jun and Zheng, Xi},
title = {DSHGT: Dual-Supervisors Heterogeneous Graph Transformer—A Pioneer Study of Using Heterogeneous Graph Learning for Detecting Software Vulnerabilities},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {8},
issn = {1049-331X},
url = {https://doi.org/10.1145/3674729},
doi = {10.1145/3674729},
abstract = {Vulnerability detection is a critical problem in software security and attracts growing attention both from academia and industry. Traditionally, software security is safeguarded by designated rule-based detectors that heavily rely on empirical expertise, requiring tremendous effort from software experts to generate rule repositories for large code corpus. Recent advances in deep learning, especially Graph Neural Networks (GNN), have uncovered the feasibility of automatic detection of a wide range of software vulnerabilities. However, prior learning-based works only break programs down into a sequence of word tokens for extracting contextual features of codes, or apply GNN largely on homogeneous graph representation (e.g., AST) without discerning complex types of underlying program entities (e.g., methods, variables). In this work, we are one of the first to explore heterogeneous graph representation in the form of Code Property Graph and adapt a well-known heterogeneous graph network with a dual-supervisor structure for the corresponding graph learning task. Using the prototype built, we have conducted extensive experiments on both synthetic datasets and real-world projects. Compared with the state-of-the-art baselines, the results demonstrate superior performance in vulnerability detection (average F1 improvements over 10% in real-world projects) and language-agnostic transferability from C/C ({+}{+})  to other programming languages (average F1 improvements over 11%).},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {202},
numpages = {31},
keywords = {Vulnerability detection, heterogeneous graph learning, code property graph (CPG)}
}

@inproceedings{10.1145/3627915.3628092,
author = {Zhang, Caixia and Wang, Chenyu and Xu, Qingyang},
title = {MobileNet-DeepLabV3+ based Robot Passable Path Segmentation and Navigation Line Extraction},
year = {2023},
isbn = {9798400700590},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627915.3628092},
doi = {10.1145/3627915.3628092},
abstract = {Traditional vision-based navigation methods for mobile robot can only be applied to some specific simple scenes due to the limitations of complex road conditions and light variations. However, the research achievements of deep learning and the substantial improvement in computer data processing performance have made it practical and applicable to guide robots in navigation using visual semantic information. For the robot visual semantic navigation problem, a lightweight DeepLabV3+ image semantic segmentation model based on MobileNetV2 is constructed as the backbone network. The images acquired by the robot vision sensor can be sensed and segmented into targets and feasible paths within the scene. Navigation lines for navigable paths can be established using the obtained feasible areas. In this way, the robot's motion decisions can be guided. Therefore, the mobile robot is capable of pure vision-based navigation. The segmentation algorithm is validated using the CamVid dataset and the constructed experimental environment dataset with mPA 82.73 and mIoU 71.8, and the navigation line can be generated according to the drivable path, which guides the robot's navigation.},
booktitle = {Proceedings of the 7th International Conference on Computer Science and Application Engineering},
articleno = {6},
numpages = {6},
keywords = {DeepLabV3+, Image Segmentation, MobileNetV2, Robot Navigation},
location = {Virtual Event, China},
series = {CSAE '23}
}

@inproceedings{10.1145/3663529.3663830,
author = {Muqeet, Asmar and Ali, Shaukat and Yue, Tao and Arcaini, Paolo},
title = {A Machine Learning-Based Error Mitigation Approach for Reliable Software Development on IBM’s Quantum Computers},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663830},
doi = {10.1145/3663529.3663830},
abstract = {Quantum computers have the potential to outperform classical computers for some complex computational problems. However, current quantum computers (e.g., from IBM and Google) have inherent noise that results in errors in the outputs of quantum software executing on the quantum computers, affecting the reliability of quantum software development. The industry is increasingly interested in machine learning (ML)-based error mitigation techniques, given their scalability and practicality. However, existing ML-based techniques have limitations, such as only targeting specific noise types or specific quantum circuits. This paper proposes a practical ML-based approach, called Q-LEAR, with a novel feature set, to mitigate noise errors in quantum software outputs. We evaluated Q-LEAR on eight quantum computers and their corresponding noisy simulators, all from IBM, and compared Q-LEAR with a state-of-the-art ML-based approach taken as baseline. Results show that, compared to the baseline, Q-LEAR achieved a 25% average improvement in error mitigation on both real quantum computers and simulators. We also discuss the implications and practicality of Q-LEAR, which, we believe, is valuable for practitioners.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {80–91},
numpages = {12},
keywords = {Error Mitigation, Machine learning, Quantum Computing, Quantum noise, Software Engineering},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/3676536.3697126,
author = {Liu, Yiwen and Jiao, Qingyue and Shi, Yiyu and Wan, Ke and Guo, Shangjie},
title = {A comparison on constrain encoding methods for quantum approximate optimization algorithm},
year = {2025},
isbn = {9798400710773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676536.3697126},
doi = {10.1145/3676536.3697126},
abstract = {The Quantum Approximate Optimization Algorithm (QAOA) represents a significant opportunity for practical quantum computing applications, particularly in the era before error correction is fully realized. This algorithm is especially relevant for addressing constraint satisfaction problems (CSPs), which are critical in various fields such as supply chain management, energy distribution, and financial modeling. In our study, we conduct a numerical comparison of three different strategies for incorporating linear constraints into QAOA: transforming them into an unconstrained format, introducing penalty dephasing, and utilizing the quantum Zeno effect. We assess the efficiency and effectiveness of these methods using the knapsack problem as a case study. Our findings provide insights into the potential applicability of different encoding methods for various use cases.},
booktitle = {Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design},
articleno = {64},
numpages = {7},
location = {Newark Liberty International Airport Marriott, New York, NY, USA},
series = {ICCAD '24}
}

@inproceedings{10.5555/3712729.3712744,
author = {Saroj, Abhilasha and Xu, Guanhao and Shao, Yunli and Wang, Chieh (Ross)},
title = {A Systematic Comparison for Consistent Scenario Development Using Microscopic Simulation Software},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {This study aims to explore a methodology that enables the development of consistent traffic microsimulation for emerging traffic and vehicle control technologies for improved mobility and energy efficiency across different modeling platforms. Researchers might study the same application on different platforms and have the need to benchmark across platforms. However, there lacks a systematic study on simulation software comparison, especially for emerging mobility and energy efficiency applications. For this, a systematic scenario development and evaluation approach is presented and demonstrated to compare scenarios generated in different traffic microsimulation platforms. Network-level and vehicle-level trip performance results of the traffic scenario are evaluated in three microscopic simulation platforms --- VISSIM, AIMSUN, and SUMO. The results indicate that the network-level performance is consistent among the three software suites except when the demand is high, where the energy consumption performance varies.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {194–205},
numpages = {12},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@inproceedings{10.1145/3605573.3605587,
author = {Lu, Qinglin and Wang, Xinyu and Ma, Wenjing and Zhao, Yuwen and Chen, Daokun and Liu, Fangfang},
title = {GFFT: a Task Graph Based Fast Fourier Transform Optimization Framework},
year = {2023},
isbn = {9798400708435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605573.3605587},
doi = {10.1145/3605573.3605587},
abstract = {Fast Fourier Transform (FFT) is a widely used mathematical tool in scientific and engineering applications, and optimizing its performance remains a challenging problem. This paper introduces GFFT, a novel task-graph-based FFT optimization framework that leverages modern hardware and software techniques to achieve high-performance computation. GFFT features a tuning model that uses hardware parameters to optimize FFT decomposition, a bi-directional recursive FFT algorithm that avoids strided load in SIMD implementation, and several graph optimizers inspired by deep learning frameworks to enhance performance. In addition, GFFT utilizes task-based parallelism to exploit performance on multi-core processors and provide potential compatibility with heterogeneous systems. Experimental results demonstrate that GFFT outperforms popular FFT frameworks, achieving an average speedup of 1.17x to FFTW and 1.27x to oneMKL on the Intel Xeon processor, 1.18x to AOCL-FFTW on the AMD EPYC processor, and 2.11x to FFTW on the Sunway multi-core processor with a single thread. Additionally, GFFT achieves an average speedup of 11.48x to FFTW and 1.41x to oneMKL on the Intel Xeon processor, 9.87x to AOCL-FFTW on the AMD EPYC processor with 16-threads.},
booktitle = {Proceedings of the 52nd International Conference on Parallel Processing},
pages = {513–523},
numpages = {11},
keywords = {FFT, High-Performance Computing, Task Graph},
location = {Salt Lake City, UT, USA},
series = {ICPP '23}
}

@proceedings{10.1145/3644032,
title = {AST '24: Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {AST continues to be a venue for researchers and practitioners where they can discuss high quality research contributions on methods for software test automation, and various case studies reporting practices in this field. Indeed, software test automation is a discipline that has produced noteworthy research in the last decade.The special theme of AST 2024 is "Test automation for and with Generative AI". This innovative and promising research direction deals with the application of test automation technologies to the testing of Generative AI applications, as well as the adoption of generative AI to facilitate test automation.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3396851.3402121,
author = {Geth, Frederik and Coffrin, Carleton and Fobes, David},
title = {A Flexible Storage Model for Power Network Optimization},
year = {2020},
isbn = {9781450380096},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396851.3402121},
doi = {10.1145/3396851.3402121},
abstract = {This paper develops a simple but flexible storage model for use in a variety of multi-period optimal power flow problems. The proposed model is designed for research use in a broad assortment of contexts enabled by the following key features: (i) the model can represent the dynamics of an energy buffer at a wide range of scales, from residential battery storage to grid-scale pumped hydro; (ii) it is compatible with both balanced and unbalanced formulations of the power flow equations; (iii) convex relaxations and linear approximations allow seamless integration of the proposed model into applications where convexity or linearity is required are developed; (iv) a minimalist and standardized data model is presented, to facilitate ease of use by the research community. The proposed model is validated using a proof-of-concept 24h storage scheduling task that demonstrates the value of the model's key features. An open-source implementation of the model is provided as part of the PowerModels and PowerModelsDistribution optimization toolboxes.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Future Energy Systems},
pages = {503–508},
numpages = {6},
keywords = {energy storage, mathematical optimization},
location = {Virtual Event, Australia},
series = {e-Energy '20}
}

@article{10.1145/3440757,
author = {Zhao, Guoliang and Hassan, Safwat and Zou, Ying and Truong, Derek and Corbin, Toby},
title = {Predicting Performance Anomalies in Software Systems at Run-time},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3440757},
doi = {10.1145/3440757},
abstract = {High performance is a critical factor to achieve and maintain the success of a software system. Performance anomalies represent the performance degradation issues (e.g., slowing down in system response times) of software systems at run-time. Performance anomalies can cause a dramatically negative impact on users’ satisfaction. Prior studies propose different approaches to detect anomalies by analyzing execution logs and resource utilization metrics after the anomalies have happened. However, the prior detection approaches cannot predict the anomalies ahead of time; such limitation causes an inevitable delay in taking corrective actions to prevent performance anomalies from happening. We propose an approach that can predict performance anomalies in software systems and raise anomaly warnings in advance. Our approach uses a Long-Short Term Memory neural network to capture the normal behaviors of a software system. Then, our approach predicts performance anomalies by identifying the early deviations from the captured normal system behaviors. We conduct extensive experiments to evaluate our approach using two real-world software systems (i.e., Elasticsearch and Hadoop). We compare the performance of our approach with two baselines. The first baseline is one state-to-the-art baseline called Unsupervised Behavior Learning. The second baseline predicts performance anomalies by checking if the resource utilization exceeds pre-defined thresholds. Our results show that our approach can predict various performance anomalies with high precision (i.e., 97–100%) and recall (i.e., 80–100%), while the baselines achieve 25–97% precision and 93–100% recall. For a range of performance anomalies, our approach can achieve sufficient lead times that vary from 20 to 1,403 s (i.e., 23.4 min). We also demonstrate the ability of our approach to predict the performance anomalies that are caused by real-world performance bugs. For predicting performance anomalies that are caused by real-world performance bugs, our approach achieves 95–100% precision and 87–100% recall, while the baselines achieve 49–83% precision and 100% recall. The obtained results show that our approach outperforms the existing anomaly prediction approaches and is able to predict performance anomalies in real-world systems.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {33},
numpages = {33},
keywords = {LSTM neural network, Performance anomaly prediction, software systems}
}

@inbook{10.1145/3658617.3697730,
author = {Chiang, Cheng-Yu and Chiang, Yi-Hsien and Lan, Chao-Chi and Hsu, Yang and Chang, Che-Ming and Huang, Shao-Chi and Wang, Sheng-Hua and Chang, Yao-Wen and Chen, Hung-Ming},
title = {Mixed-Size Placement Prototyping Based on Reinforcement Learning with Semi-Concurrent Optimization},
year = {2025},
isbn = {9798400706356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658617.3697730},
abstract = {Placement plays a crucial role in modern chip design, aiming to determine the positions of circuit blocks (macros and standard cells). Traditional data structure-centric heuristics often yield suboptimal placement prototypes, ineffectively guiding downstream mixed-size analytical placement to find the desired results for modern large-scale designs. Recent works have showcased the potential of reinforcement learning (RL) to enhance chip placement by training a policy to place macros as a board game. However, placing macros and fixing them in the earlier stages without sufficient information often incurs undesired solutions. This paper proposes a novel RL-based mixed-size placer with iteratively moving the blocks to characterize dense rewards and comprehensive layout information in each step. We further introduce a semi-concurrent moving mechanism to learn the collaborative dynamics among actions on a subset of blocks at each step. We integrate continuous action spaces to develop a deep Q network-based model for learning the semi-concurrent moving policy to derive the proposed moving strategy. Compared with the state-of-the-art methods, experimental results show that our RL-based placer achieves the best placement quality based on commonly used mixed-size placement benchmarks.},
booktitle = {Proceedings of the 30th Asia and South Pacific Design Automation Conference},
pages = {893–899},
numpages = {7}
}

@article{10.1145/3643742,
author = {Bouzenia, Islem and Krishan, Bajaj Piyush and Pradel, Michael},
title = {DyPyBench: A Benchmark of Executable Python Software},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643742},
doi = {10.1145/3643742},
abstract = {Python has emerged as one of the most popular programming languages, extensively utilized in domains such 
 
as machine learning, data analysis, and web applications. Python’s dynamic nature and extensive usage make 
 
it an attractive candidate for dynamic program analysis. However, unlike for other popular languages, there 
 
currently is no comprehensive benchmark suite of executable Python projects, which hinders the development 
 
of dynamic analyses. This work addresses this gap by presenting DyPyBench, the first benchmark of Python 
 
projects that is large-scale, diverse, ready-to-run (i.e., with fully configured and prepared test suites), and ready- 
 
to-analyze (by integrating with the DynaPyt dynamic analysis framework). The benchmark encompasses 50 
 
popular open-source projects from various application domains, with a total of 681k lines of Python code, 
 
and 30k test cases. DyPyBench enables various applications in testing and dynamic analysis, of which we 
 
explore three in this work: (i) Gathering dynamic call graphs and empirically comparing them to statically 
 
computed call graphs, which exposes and quantifies limitations of existing call graph construction techniques 
 
for Python. (ii) Using DyPyBench to build a training data set for LExecutor, a neural model that learns to 
 
predict values that otherwise would be missing at runtime. (iii) Using dynamically gathered execution traces 
 
to mine API usage specifications, which establishes a baseline for future work on specification mining for 
 
Python. We envision DyPyBench to provide a basis for other dynamic analyses and for studying the runtime 
 
behavior of Python code.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {16},
numpages = {21},
keywords = {Python benchmark, dynamic analysis, executable}
}

@inproceedings{10.1145/3589250.3596148,
author = {Liblit, Ben and Lyu, Yingjun and Mukherjee, Rajdeep and Tripp, Omer and Wang, Yanjun},
title = {User-Assisted Code Query Optimization},
year = {2023},
isbn = {9798400701702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589250.3596148},
doi = {10.1145/3589250.3596148},
abstract = {Running static analysis rules in the wild, as part of a commercial service, demands special consideration of time limits and scalability given the large and diverse real-world workloads that the rules are evaluated on. Furthermore, these rules do not run in isolation, which exposes opportunities for reuse of partial evaluation results across rules. In our work on Amazon CodeGuru Reviewer, and its underlying rule-authoring toolkit known as the Guru Query Language (GQL), we have encountered performance and scalability challenges, and identified corresponding optimization opportunities such as, caching, indexing, and customization of analysis scope, which rule authors can take advantage of as built-in GQL constructs. Our experimental evaluation on a dataset of open-source GitHub repositories shows 3X speedup and perfect recall using indexing-based configurations, and 2X speedup and 51% increase on the number of findings for caching-based optimization.},
booktitle = {Proceedings of the 12th ACM SIGPLAN International Workshop on the State Of the Art in Program Analysis},
pages = {40–46},
numpages = {7},
keywords = {AWS, GitHub, Guru Query Language (GQL), caching, performance optimization, static analysis},
location = {Orlando, FL, USA},
series = {SOAP 2023}
}

@article{10.1145/3561651,
author = {Chen, Zhangyu and Hua, Yu and Ding, Luochangqi and Ding, Bo and Zuo, Pengfei and Liu, Xue},
title = {Lock-Free High-performance Hashing for Persistent Memory via PM-aware Holistic Optimization},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3561651},
doi = {10.1145/3561651},
abstract = {Persistent memory (PM) provides large-scale non-volatile memory (NVM) with DRAM-comparable performance. The non-volatility and other unique characteristics of PM architecture bring new opportunities and challenges for the efficient storage system design. For example, some recent crash-consistent and write-friendly hashing schemes are proposed to provide fast queries for PM systems. However, existing PM hashing indexes suffer from the concurrency bottleneck due to the blocking resizing and expensive lock-based concurrency control for queries. Moreover, the lack of PM awareness and systematical design further increases the query latency. To address the concurrency bottleneck of lock contention in PM hashing, we propose clevel hashing, a lock-free concurrent level hashing scheme that provides non-blocking resizing via background threads and lock-free search/insertion/update/deletion using atomic primitives to enable high concurrency for PM hashing. By exploiting the PM characteristics, we present a holistic approach to building clevel hashing for high throughput and low tail latency via the PM-aware index/allocator co-design. The proposed volatile announcement array with a helping mechanism coordinates lock-free insertions and guarantees a strong consistency model. Our experiments using real-world YCSB workloads on Intel Optane DC PMM show that clevel hashing, respectively, achieves up to 5.7\texttimes{} and 1.6\texttimes{} higher throughput than state-of-the-art P-CLHT and Dash while guaranteeing low tail latency, e.g., 1.9\texttimes{}–7.2\texttimes{} speedup for the p99 latency with the insert-only workload.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {5},
numpages = {26},
keywords = {Lock-free index, hashing, persistent memory, correctness}
}

@proceedings{10.1145/3701625,
title = {SBQS '24: Proceedings of the XXIII Brazilian Symposium on Software Quality},
year = {2024},
isbn = {9798400717772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@article{10.1145/3632951,
author = {Kang, Seokwon and Kim, Jongbin and Lee, Gyeongyong and Lee, Jeongmyung and Seo, Jiwon and Jung, Hyungsoo and Song, Yong Ho and Park, Yongjun},
title = {ISP Agent: A Generalized In-storage-processing Workload Offloading Framework by Providing Multiple Optimization Opportunities},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3632951},
doi = {10.1145/3632951},
abstract = {As solid-state drives (SSDs) with sufficient computing power have recently become the dominant devices in modern computer systems, in-storage processing (ISP), which processes data within the storage without transferring it to the host memory, is being utilized in various emerging applications. The main challenge of ISP is to deliver storage data to the offloaded workload. This is difficult because of the information gap between the host and storage, the data consistency problem between the host and offloaded workloads, and SSD-specific hardware limitations. Moreover, because the offloaded workloads use internal SSD resources, host I/O performance might be degraded due to resource conflicts. Although several ISP frameworks have been proposed, existing ISP approaches that do not deeply consider the internal SSD behavior are often insufficient to support efficient ISP workload offloading with high programmability.In this article, we propose an ISP agent, a lightweight ISP workload offloading framework for SSD devices. The ISP agent provides I/O and memory interfaces that allow users to run existing function codes on SSDs without major code modifications, and separates the resources for the offloaded workloads from the existing SSD firmware to minimize interference with host I/O processing. The ISP agent also provides further optimization opportunities for the offloaded workload by considering SSD architectures. We have implemented the ISP agent on the OpenSSD Cosmos+ board and evaluated its performance using synthetic benchmarks and a real-world ISP-assisted database checkpointing application. The experimental results demonstrate that the ISP agent enhances host application performance while increasing ISP programmability, and that the optimization opportunities provided by the ISP agent can significantly improve ISP-side performance without compromising host I/O processing.},
journal = {ACM Trans. Archit. Code Optim.},
month = jan,
articleno = {11},
numpages = {24},
keywords = {In-storage processing, solid state drive, firmware, programming model}
}

@inproceedings{10.1145/3632775.3661940,
author = {Reihs, David and Kupzog, Friederich and De Meer, Hermann},
title = {Regional Differences in Configurations of Energy Communities: Future Scenarios and Policy Implications},
year = {2024},
isbn = {9798400704802},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632775.3661940},
doi = {10.1145/3632775.3661940},
abstract = {Local energy communities can enable end customers to increase the use of renewable energy through sharing, trading and joint investments. These energy communities will have an impact on load flows and grid utilization. However, it is challenging to predict the configurations of energy communities that will be present in future power systems. This paper presents simulation models for distributed generation, storage and consumption technologies in energy communities. An agent-based energy market is implemented to consider effects of local energy sharing. Different heuristic control scenarios are considered to investigates the effects of demand response in energy communities. The simulation is coupled with a Mixed Integer Linear Programming optimization of the energy resource sizing problem. The presented sizing approach is applied in a case study of six realistic energy communities in Austria. Policy implications on local generation and storage capacities are investigated in a multitude of scenarios.},
booktitle = {Proceedings of the 15th ACM International Conference on Future and Sustainable Energy Systems},
pages = {1–13},
numpages = {13},
keywords = {Distributed Energy Generation, Energy Communities, Energy Storage Sizing},
location = {Singapore, Singapore},
series = {e-Energy '24}
}

@inproceedings{10.1145/3634848.3634859,
author = {Viana, Rhuan and Souza, Rebecca and Silva, Wilson and Oliveira, Flavia and Tiago, Leonardo and Chaves, Lennon},
title = {Assertive Wiki: An Experience Report In The Industry on the Redesign of Software Requirements Documentation},
year = {2024},
isbn = {9798400708107},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634848.3634859},
doi = {10.1145/3634848.3634859},
abstract = {The development of requirements documentation in the software life cycle process is a crucial step in requirements engineering. However, due to excessive documentation, multiple databases for reference, and difficulties in understanding requirements, the testing process can become exhaustive. Existing literature already demonstrates that well-written documentation results in higher-quality product delivery. In summary, the industry focuses on clarity and objectivity in business rules, but does not address the issues of excessive documentation and multiple requirement databases. The aim of this study is to highlight the value of developing usable and useful documentation within the context of SIDIA, an Institute of Science and Technology. To achieve this, the Assertive Wiki was developed, comprising 45 wikis that document requirements based on the application of UX/UI Design and A/B Testing techniques. Through qualitative research conducted with a software testing team at SIDIA, it is demonstrated that 90% of the testers agree that the utilities and databases built in this manner are useful. Thus, it can be concluded that the applied requirements documentation process, through the use of Assertive Wiki, led to testers satisfaction with testing activities.},
booktitle = {Proceedings of the 2023 12th International Conference on Software and Information Engineering},
pages = {22–30},
numpages = {9},
keywords = {A/B Test, Software Requirements, Software Testing, UX/UI Design},
location = {Sharm El-Sheikh, Egypt},
series = {ICSIE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00104,
author = {Stradowski, Szymon and Madeyski, Lech},
title = {Can We Knapsack Software Defect Prediction? Nokia 5G Case},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00104},
doi = {10.1109/ICSE-Companion58688.2023.00104},
abstract = {As software products become larger and more complex, the test infrastructure needed for quality assurance grows similarly, causing a constant increase in operational and maintenance costs. Although rising in popularity, most Artificial Intelligence (AI) and Machine Learning (ML) Software Defect Prediction (SDP) solutions address singular test phases. In contrast, the need to address the whole Software Development Life Cycle (SDLC) is rarely explored. Therefore in this paper, we define the problem of extending the SDP concept to the entire SDLC, as this may be one of the significant next steps for the field. Furthermore, we explore the similarity between the defined challenge and the widely known Multidimensional Knapsack Problem (MKP). We use Nokia's 5G wireless technology test process to illustrate the proposed concept. Resulting comparison validates the applicability of MKP to optimize the overall test cycle, which can be similarly relevant to any large-scale industrial software development process.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {365–369},
numpages = {5},
keywords = {artificial intelligence, software defect prediction, software testing, continuous integration, software development life cycle, Nokia 5G},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/2814204.2814220,
author = {Selgrad, Kai and Lier, Alexander and K\"{o}ferl, Franz and Stamminger, Marc and Lohmann, Daniel},
title = {Lightweight, generative variant exploration for high-performance graphics Applications},
year = {2015},
isbn = {9781450336871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814204.2814220},
doi = {10.1145/2814204.2814220},
abstract = {Rendering performance is an everlasting goal of computer graphics and significant driver for advances in both, hardware architecture and algorithms. Thereby, it has become possible to apply advanced computer graphics technology even in low-cost embedded appliances, such as car instruments. Yet, to come up with an efficient implementation, developers have to put enormous efforts into hardware/problem-specific tailoring, fine-tuning, and domain exploration, which requires profound expert knowledge. If a good solution has been found, there is a high probability that it does not work as well with other architectures or even the next hardware generation. Generative DSL-based approaches could mitigate these efforts and provide for an efficient exploration of algorithmic variants and hardware-specific tuning ideas. However, in vertically organized industries, such as automotive, suppliers are reluctant to introduce these techniques as they fear loss of control, high introduction costs, and additional constraints imposed by the OEM with respect to software and tool-chain certification. Moreover, suppliers do not want to share their generic solutions with the OEM, but only concrete instances. To this end, we propose a light-weight and incremental approach for meta programming of graphics applications. Our approach relies on an existing formulation of C-like languages that is amenable to meta programming, which we extend to become a lightweight language to combine algorithmic features. Our method provides a concise notation for meta programs and generates easily sharable output in the appropriate C-style target language.},
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {141–150},
numpages = {10},
keywords = {exploratory programming, general purpose code generation, prototyping, ray tracing},
location = {Pittsburgh, PA, USA},
series = {GPCE 2015}
}

@article{10.5555/3722577.3722778,
author = {Keisler, Julien and Talbi, El-Ghazali and Claudel, Sandra and Cabriel, Gilles},
title = {An algorithmic framework for the optimization of deep neural networks architectures and hyperparameters},
year = {2024},
issue_date = {January 2024},
publisher = {JMLR.org},
volume = {25},
number = {1},
issn = {1532-4435},
abstract = {In this paper, we propose DRAGON (for DiRected Acyclic Graph OptimizatioN), an algorithmic framework to automatically generate efficient deep neural networks architectures and optimize their associated hyperparameters. The framework is based on evolving Directed Acyclic Graphs (DAGs), defining a more exible search space than the existing ones in the literature. It allows mixtures of different classical operations: convolutions, recurrences and dense layers, but also more newfangled operations such as self-attention. Based on this search space we propose neighbourhood and evolution search operators to optimize both the architecture and hyper-parameters of our networks. These search operators can be used with any metaheuristic capable of handling mixed search spaces. We tested our algorithmic framework with an asynchronous evolutionary algorithm on a time series forecasting benchmark. The results demonstrate that DRAGON outperforms state-of-the-art handcrafted models and AutoML techniques for time series forecasting on numerous datasets. DRAGON has been implemented as a python open-source package1.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {201},
numpages = {33},
keywords = {neural architecture search, hyperparameters optimization, metaheuristics, evolutionary algorithm, time series forecasting}
}

@inproceedings{10.1145/3659914.3659917,
author = {Wijnja, Steven and Alachiotis, Nikolaos},
title = {SoftCache: A Software Cache for PCIe-Attached Hardware Accelerators},
year = {2024},
isbn = {9798400706394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3659914.3659917},
doi = {10.1145/3659914.3659917},
abstract = {Hardware accelerators are used to speed up computationally expensive applications in many scientific fields. However, offloading tasks to accelerator cards requires data to be transferred between the memory of the host and the external memory of the accelerator card; this data movement frequently becomes the bottleneck for increasing accelerator performance. In this work, we explore the use of a software cache to optimize communication and alleviate the data-movement bottleneck by transparently exploiting locality and data reuse. We present a generic, application-agnostic framework, dubbed SoftCache, that can be used with both GPU and FPGA accelerator cards. SoftCache exploits locality to optimize data movement in a non-intrusive manner (i.e., no changes to the algorithm are necessary) and allows the programmer to tune the cache size, cache organization, and replacement policy toward the application needs. Each cache line can store data of any size, thereby eliminating the need for separate caches for different data types. We used a phylogenetic application to showcase SoftCache. Phylogenetics study the evolutionary history and relationships among different species or groups of organisms. The phylogenetic application implements a tree-search algorithm to create and evaluate phylogenetic trees, while hardware accelerators are used to reduce the computation time of probability vectors at every tree node. Using SoftCache, we observed that the total number of bytes transferred during a complete run of the application was reduced by as much as 89%, resulting in up to 1.7x (81% of the theoretical peak) and 3.5x (75% of the theoretical peak) higher accelerator performance (as seen by the application) for a GPU and an FPGA accelerator, respectively.},
booktitle = {Proceedings of the Platform for Advanced Scientific Computing Conference},
articleno = {3},
numpages = {11},
keywords = {data movement, software cache, GPU, FPGA, OpenCL, phylogenetic likelihood function (PLF), RAxML},
location = {Zurich, Switzerland},
series = {PASC '24}
}

@inproceedings{10.1145/3581784.3613214,
author = {Schaad, Philipp and Schneider, Timo and Ben-Nun, Tal and Calotoiu, Alexandru and Ziogas, Alexandros Nikolaos and Hoefler, Torsten},
title = {FuzzyFlow: Leveraging Dataflow To Find and Squash Program Optimization Bugs},
year = {2023},
isbn = {9798400701092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581784.3613214},
doi = {10.1145/3581784.3613214},
abstract = {The current hardware landscape and application scale is driving performance engineers towards writing bespoke optimizations. Verifying such optimizations, and generating minimal failing cases, is important for robustness in the face of changing program conditions, such as inputs and sizes. However, isolation of minimal test-cases from existing applications and generating new configurations are often difficult due to side effects on the system state, mostly related to dataflow. This paper introduces FuzzyFlow: a fault localization and test case extraction framework designed to test program optimizations. We leverage dataflow program representations to capture a fully reproducible system state and area-of-effect for optimizations to enable fast checking for semantic equivalence. To reduce testing time, we design an algorithm for minimizing test inputs, trading off memory for recomputation. We demonstrate FuzzyFlow on example use cases in real-world applications where the approach provides up to 528 times faster optimization testing and debugging compared to traditional approaches.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {88},
numpages = {15},
keywords = {software testing, fuzzing, translation verification, test generation},
location = {Denver, CO, USA},
series = {SC '23}
}

@article{10.1145/3508391,
author = {Jeong, Eunjin and Kim, Jangryul and Ha, Soonhoi},
title = {TensorRT-Based Framework and Optimization Methodology for Deep Learning Inference on Jetson Boards},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {5},
issn = {1539-9087},
url = {https://doi.org/10.1145/3508391},
doi = {10.1145/3508391},
abstract = {As deep learning inference applications are increasing in embedded devices, an embedded device tends to equip neural processing units (NPUs) in addition to a multi-core CPU and a GPU. NVIDIA Jetson AGX Xavier is an example. For fast and efficient development of deep learning applications, TensorRT is provided as the SDK for high-performance inference, including an optimizer and runtime that delivers low latency and high throughput for deep learning inference applications. Like most deep learning frameworks, TensorRT assumes that the inference is executed on a single processing element, GPU or NPU, not both. In this article, we present a TensorRT-based framework supporting various optimization parameters to accelerate a deep learning application targeted on an NVIDIA Jetson embedded platform with heterogeneous processors, including multi-threading, pipelining, buffer assignment, and network duplication. Since the design space of allocating layers to diverse processing elements and optimizing other parameters is huge, we devise a parameter optimization methodology that consists of a heuristic for balancing pipeline stages among heterogeneous processors and fine-tuning the process for optimizing parameters. With nine real-life benchmarks, we could achieve 101%~680% performance improvement and up to 55% energy reduction over the baseline inference using a GPU only.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = oct,
articleno = {51},
numpages = {26},
keywords = {Deep learning, optimization, framework, acceleration}
}

@article{10.1145/3704923,
author = {Nagrecha, Kabir and Liu, Lingyi and Delgado, Pablo},
title = {Reinforcement Learning for Intra- &amp; Inter-Node Recommender Data Pipeline Optimization},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
url = {https://doi.org/10.1145/3704923},
doi = {10.1145/3704923},
abstract = {Deep learning-based recommender models (DLRMs) have become an essential component of many modern recommender systems. Several companies are now building large compute clusters reserved for DLRM training, driving new interest in cost- and time-saving optimizations. The systems challenges faced in this setting are unique; while typical deep learning (DL) training jobs are dominated by model execution times, the most important factor in DLRM training performance is often online data ingestion.In this article, we study real-world DLRM data processing pipelines taken from our compute cluster at Netflix to observe the performance impacts of online ingestion and identify shortfalls in existing data pipeline optimizers. Our studies lead us to design a new solution for data pipeline optimization, InTuneX.InTuneX&nbsp;is designed for production-scale multi-node recommender data pipelines. It unifies and tackles the challenges of both intra- and inter-node pipeline optimization. We achieve this with a multi-agent reinforcement learning (RL) design, simultaneously optimizing node assignments at the cluster level and CPU assignments within nodes.Our experiments show that InTuneX&nbsp;can build optimized data pipeline configurations within minutes. We apply InTuneX&nbsp;to our cluster and find that it increases single-node data ingestion throughput by as much as 2.29 \texttimes{} versus state-of-the-art optimizers, while improving the cost-efficiency of multi-node pipelines by 15% to 25%.},
journal = {ACM Trans. Recomm. Syst.},
month = apr,
articleno = {45},
numpages = {29},
keywords = {Data processing, recommendation systems, deep learning, parallel computing, resource allocation}
}

@inproceedings{10.1109/MICRO56248.2022.00064,
author = {Li, Xueliang and Shi, Zhuobin and Chen, Junyang and Liu, Yepang},
title = {Realizing Emotional Interactions to Learn User Experience and Guide Energy Optimization for Mobile Architectures},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00064},
doi = {10.1109/MICRO56248.2022.00064},
abstract = {In the age of AI, mobile architectures such as smartphones are still "cold machines"; machines do not feel. If the architecture is able to feel users' feelings and runtime user experience (UX), it will accordingly adapt performance/energy to find the optimal system-operating state that consumes the least energy to satisfy users. In this paper, we will utilize users' facial expressions (FEs) to learn their runtime UX. We know that FEs are the natural and direct way for humans to convey their emotions and feelings. Our study reveals that FEs also reflect UX. Our research for the first time quantifies the link between FEs and UX. Leveraging this link, the architecture will be able to use the front camera to see FEs and feel users' UX. Based on UX, the architecture can appropriately provision computing resources. We propose Vi-energy system to realize the above idea. Our evaluation shows that Vi-energy reduces energy consumption by 52.9% at maximum and secures UX.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {868–884},
numpages = {17},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1145/3599733.3600256,
author = {Adamski, Jakub and G\'{o}rze\'{n}ski, Rados\l{}aw and Kargin, Turhan Can and Malewski, \L{}ukasz and Oleksiak, Ariel and Sidorski, Franciszek},
title = {Planning data center waste heat re-use in a university campus - a case study and software tools},
year = {2023},
isbn = {9798400702273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3599733.3600256},
doi = {10.1145/3599733.3600256},
abstract = {Data centers are huge energy consumers but also a source of significant amounts of waste heat. Within this paper we present approach to re-use the data center waste heat both locally and in connection to a district heating network. The heat re-use design and analysis process is supported by the methodology and software tools developed by the RENergetic project. The analysis is performed based on a case study of the university campus and a data center located in a close neighbourhood.},
booktitle = {Companion Proceedings of the 14th ACM International Conference on Future Energy Systems},
pages = {98–105},
numpages = {8},
keywords = {data center, forecasting, heat demand, heat supply, modeling and simulation, waste heat re-use},
location = {Orlando, FL, USA},
series = {e-Energy '23 Companion}
}

@article{10.1613/jair.1.14972,
author = {Cant\"{u}rk, Furkan and Varol, Taha and Aydo\u{g}an, Reyhan and \"{O}zener, Okan \"{O}rsan},
title = {Scalable Primal Heuristics Using Graph Neural Networks for Combinatorial Optimization},
year = {2024},
issue_date = {Sep 2024},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {80},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.14972},
doi = {10.1613/jair.1.14972},
abstract = {By examining the patterns of solutions obtained for various instances, one can gain insights into the structure and behavior of combinatorial optimization (CO) problems and develop efficient algorithms for solving them. Machine learning techniques, especially Graph Neural Networks (GNNs), have shown promise in parametrizing and automating this laborious design process. The inductive bias of GNNs allows for learning solutions to mixed-integer programming (MIP) formulations of constrained CO problems with a relational representation of decision variables and constraints. The trained GNNs can be leveraged with primal heuristics to construct high-quality feasible solutions to CO problems quickly. However, current GNN-based end-to-end learning approaches have limitations for scalable training and generalization on larger-scale instances; therefore, they have been mostly evaluated over small-scale instances. Addressing this issue, our study builds on supervised learning of optimal solutions to the downscaled instances of given large-scale CO problems. We introduce several improvements on a recent GNN model for CO to generalize on instances of a larger scale than those used in training. We also propose a two-stage primal heuristic strategy based on uncertainty-quantification to automatically configure how solution search relies on the predicted decision values. Our models can generalize on 16x upscaled instances of commonly benchmarked five CO problems. Unlike the regressive performance of existing GNN-based CO approaches as the scale of problems increases, the CO pipelines using our models offer an incremental performance improvement relative to CPLEX. The proposed uncertainty-based primal heuristics provide 6-75% better optimality gap values and 45-99% better primal gap values for the 16x upscaled instances and brings immense speedup to obtain high-quality solutions. All these gains are achieved through a computationally efficient modeling approach without sacrificing solution quality.},
journal = {J. Artif. Int. Res.},
month = sep,
numpages = {50}
}

@inproceedings{10.1145/3719330.3721227,
author = {Nicolas, Louis-Marie and Mimouni, Salim and Couv\'{e}e, Philippe and Boukhobza, Jalil},
title = {Characterizing the use of DVFS for HPC I/O optimization: a Microbenchmarking Approach},
year = {2025},
isbn = {9798400715297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719330.3721227},
doi = {10.1145/3719330.3721227},
abstract = {The scale of HPC clusters has increased over the past decade, eventually achieving exascale in 2022. While the shift to exascale computing meets the rising demand for processing power, it introduces challenges in energy consumption and power efficiency. Dynamic Voltage and Frequency Scaling (DVFS), is a common tool that can be used to reduce the processor's power usage. However, while the effect of DVFS during classical compute tasks has already been well-documented, its impact on I/Os has not been covered by the literature. In this paper, we offer a comprehensive investigation of the impact of DVFS on a compute node comprising two AMD EPYC Rome 7282 CPU in an HPC cluster. Our contributions include (1) a methodology to microbenchmark the performance and energy consumption impact of DVFS in an HPC system through the use of carefully selected synthetic application, (2) a set of results and analysis related to the behavior of DVFS on the tested HPC nodes, and (3) some guidelines for using DVFS on HPC applications.},
booktitle = {Proceedings of the 5th Workshop on Challenges and Opportunities of Efficient and Performant Storage Systems},
pages = {1–7},
numpages = {7},
keywords = {Dynamic Voltage and Frequency Scaling, Energy-Aware Computing, Energy-Aware I/Os, High-Performance Computing},
location = {Rotterdam, Netherlands},
series = {CHEOPS '25}
}

@inproceedings{10.1145/3650200.3656631,
author = {Huang, Hanxian and Chen, Xin and Zhao, Jishen},
title = {Fasor: A Fast Tensor Program Optimization Framework for Efficient DNN Deployment},
year = {2024},
isbn = {9798400706103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650200.3656631},
doi = {10.1145/3650200.3656631},
abstract = {With the growing importance of deploying deep neural networks (DNNs), there are increasing demands to improve both the efficiency and quality of tensor program optimization&nbsp;(TPO). TPO involves searching for possible program transformations for a given tensor program on target hardware to optimize its execution. TPO is challenging and expensive due to the exponential combinations of transformations and time-consuming on-device measurement of transformations. While prior research has primarily focused on the quality of TPO, i.e., generating high-performance tensor programs, there has been less emphasis on the efficiency of TPO, i.e., optimizing tensor programs with low optimization time overhead. In this paper, we address the primary inefficiencies in current TPO approaches, especially the extensive time required for on-device measurement and the inefficiency in the search process, and aim to reduce the optimization time for DNNs. To this end, we propose a machine learning-based, end-to-end TPO framework named Fasor. Fasor includes three key design components: 1):&nbsp;a transferable cost model with high transferring efficiency to reduce the on-device measurement time significantly, 2):&nbsp;a search space shrinking module to prune program transformations with low optimization potential, and 3):&nbsp;a two-stage fast exploration module to enhance searching efficiency substantially. Experimental results show that Fasor achieves the best of both worlds in TPO quality and efficiency compared to state-of-the-art TPO frameworks for CPUs and GPUs, contributing to efficient and scalable DNN deployment.},
booktitle = {Proceedings of the 38th ACM International Conference on Supercomputing},
pages = {498–510},
numpages = {13},
keywords = {TVM, auto-tuning, compute schedules, deep neural networks, performance models, tensor compilers, tensor program optimization},
location = {Kyoto, Japan},
series = {ICS '24}
}

@inproceedings{10.1109/ICSE48619.2023.00035,
author = {Kim, Taeyoung and Jang, Yunhee and Lee, Chanjong and Koo, Hyungjoon and Kim, Hyoungshick},
title = {SmartMark: Software Watermarking Scheme for Smart Contracts},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00035},
doi = {10.1109/ICSE48619.2023.00035},
abstract = {A smart contract is a self-executing program on a blockchain to ensure an immutable and transparent agreement without the involvement of intermediaries. Despite its growing popularity for many blockchain platforms like Ethereum, no technical means is available even when a smart contract requires to be protected from being copied. One promising direction to claim a software ownership is software watermarking. However, applying existing software watermarking techniques is challenging because of the unique properties of a smart contract, such as a code size constraint, non-free execution cost, and no support for dynamic allocation under a virtual machine environment. This paper introduces a novel software watermarking scheme, dubbed SmartMark, aiming to protect the ownership of a smart contract against a pirate activity. SmartMark builds the control flow graph of a target contract runtime bytecode, and locates a collection of bytes that are randomly elected for representing a watermark. We implement a full-fledged prototype for Ethereum, applying SmartMark to 27,824 unique smart contract bytecodes. Our empirical results demonstrate that SmartMark can effectively embed a watermark into a smart contract and verify its presence, meeting the requirements of credibility and imperceptibility while incurring an acceptable performance degradation. Besides, our security analysis shows that SmartMark is resilient against viable watermarking corruption attacks; e.g., a large number of dummy opcodes are needed to disable a watermark effectively, resulting in producing an illegitimate smart contract clone that is not economical.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {283–294},
numpages = {12},
keywords = {smart contract, software watermarking, blockchain, software copyrights},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@proceedings{10.1145/3689944,
title = {SCORED '24: Proceedings of the 2024 Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses},
year = {2024},
isbn = {9798400712401},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to ACM SCORED '24, the third edition of the ACM Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses. This edition is held in Salt Lake City, Utah, United States with extensive support for in-person and virtual attendance. This year's program includes exciting work along many different dimensions of research on supply chain security: the development of security policies for software supply chains, the use of artificial intelligence and large language models, approaches on software bills of materials, and the proposals of risk mitigation techniques. Consistent with its focus, SCORED brings researchers, legislators and practitioners in both open- and closed-source ecosystems to the center of current and emerging challenges and opportunities in software supply chain security.},
location = {Salt Lake City, UT, USA}
}

@inproceedings{10.1145/3689488.3689991,
author = {Boksem, Max and van Binsbergen, L. Thomas},
title = {Bridging Incremental Programming and Complex Software Development Environments},
year = {2024},
isbn = {9798400712128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689488.3689991},
doi = {10.1145/3689488.3689991},
abstract = {In modern software development, programmers typically choose between two main types of coding environments: Incremental Programming Environments (IPEs), such as the Read-Eval-Print-Loop (REPL) interpreter IPython and the Jupyter Computational Notebook, and Integrated (text-based) Development Environments (IDEs), such as Visual Studio Code. IPEs excel in providing immediate feedback for iterative development, testing, and debugging, making them ideal for fields like data science and AI. However, their typically linear and isolated interface struggles with managing the complexity of larger software projects. Conversely, traditional IDEs support extensive project management and debugging tools suited for complex applications but lack the interactive and incremental nature of IPEs.
 

 
This paper reports on an ongoing investigation and design of a hybrid environment that combines benefits of IPEs and IDEs and the programming styles they naturally support. 
 
Central to our design is a graph structure representing code fragments as nodes and code structure as edges. By considering various types of nodes and relationships (e.g. for representing class membership, execution order, documentation, and dependencies) we can facilitate aspects of both incremental programming (in IPEs) and complexity management (in IDEs). We demonstrate our approach with a prototype, called Incremental Graph Code (IGC), by presenting its architecture and a showcase. We demonstrate IGC's functionality and discuss its potential advantages over existing environments. Key features include advanced code visualization, modular and incremental development, and complexity management. IGC aims to provide a unified, extensible, and flexible development environment that bridges the gap between different styles of programming.},
booktitle = {Proceedings of the 3rd ACM SIGPLAN International Workshop on Programming Abstractions and Interactive Notations, Tools, and Environments},
pages = {29–40},
numpages = {12},
keywords = {exploratory programming, incremental programming, read-eval-print loop, software complexity management, visual general-purpose language development environment},
location = {Pasadena, CA, USA},
series = {PAINT '24}
}

@article{10.5555/3722577.3722906,
author = {Liang, Enming and Chen, Minghua and Low, Steven H.},
title = {Homeomorphic projection to ensure neural-network solution feasibility for constrained optimization},
year = {2024},
issue_date = {January 2024},
publisher = {JMLR.org},
volume = {25},
number = {1},
issn = {1532-4435},
abstract = {There has been growing interest in employing neural networks (NNs) to directly solve constrained optimization problems with low run-time complexity. However, it is non-trivial to ensure NN solutions strictly satisfy problem constraints due to inherent NN prediction errors. Existing feasibility-ensuring methods are either computationally expensive or lack performance guarantee. In this paper, we propose Homeomorphic Projection as a lowcomplexity scheme to guarantee NN solution feasibility for optimization over a general set homeomorphic to a unit ball, covering all compact convex sets and certain classes of nonconvex sets. The idea is to (i) learn a minimum distortion homeomorphic mapping between the constraint set and a unit ball using a bi-Lipschitz invertible NN (INN), and then (ii) perform a simple bisection operation concerning the unit ball such that the INN-mapped _nal solution is feasible with respect to the constraint set with minor distortion-induced optimality loss. We prove the feasibility guarantee and bounded optimality loss under mild conditions. Simulation results, including those for non-convex AC-OPF problems in power grid operation, show that homeomorphic projection outperforms existing methods in solution feasibility and run-time complexity while achieving similar optimality loss.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {329},
numpages = {55},
keywords = {constrained optimization, feasibility, homeomorphism, distortion, projection}
}

@article{10.1145/3605147,
author = {Terway, Prerit and Jha, Niraj K.},
title = {REPAIRS: Gaussian Mixture Model-based Completion and Optimization of Partially Specified Systems},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {1539-9087},
url = {https://doi.org/10.1145/3605147},
doi = {10.1145/3605147},
abstract = {Most system optimization techniques focus on finding the values of the system components to achieve the best performance. Searching over all component values gives the search methodology the freedom to explore the entire design space to determine the best system configuration. However, real-world systems often require searching in a restricted space over only a subset of component values while freezing some of the components to fixed values. Rather than optimizing from scratch to search over the subset of components, incorporating the past simulation logs (search performed when all components were allowed to vary) enables the optimization mechanism to utilize knowledge from past system behavior. In addition, when the system gives the same response over different combinations of input values, the designer may prefer one combination over another. Furthermore, real-world data often contain errors. To avoid catastrophic consequences of making decisions based on incorrect data points, we need a mechanism to identify and correct the resulting error. We propose REPAIRS, a methodology to complete/optimize partially specified systems. It also performs data integrity checks and identifies/corrects errors after detecting an anomaly in the data. We use a Gaussian mixture model to learn the joint distribution of the system inputs and the corresponding output response (objectives/constraints). We use the learned model to complete a partially specified system where only a subset of the component values and/or the system response is specified. When the system response exhibits multiple modes (e.g., same response for different combinations of input values), REPAIRS determines the combinations of input values that correspond to the several modes. Using past simulation logs, it searches over various subsets of system inputs to improve the performance of the reference solution. We also present a framework for verifying the integrity of a given data instance. When the integrity check fails, we provide a mechanism to identify the error location and correct the error. REPAIRS provides an explanation for the decision it makes for the different use cases described in this article. We provide results of REPAIRS in the context of completion, partial optimization, and data integrity check of real-world systems. REPAIRS achieves a hypervolume that is better than that obtained using a baseline method by up to 50%. It successfully identifies the error location and predicts the correct value of the erroneous feature with an error less than 0.2%. It detects error locations with a mean accuracy of up to 95% even when three feature values have an error.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = jul,
articleno = {69},
numpages = {36},
keywords = {Active learning, constrained multi-objective optimization, Gaussian mixture model, inverse design, data integrity}
}

@inproceedings{10.1145/3698038.3698531,
author = {Sharma, Prateek and Fuerst, Alexander},
title = {Accountable Carbon Footprints and Energy Profiling For Serverless Functions},
year = {2024},
isbn = {9798400712869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698038.3698531},
doi = {10.1145/3698038.3698531},
abstract = {Cloud computing is a significant and growing cause of carbon emissions. Understanding the energy consumption and carbon footprints of cloud applications is a fundamental prerequisite to raising awareness, designing sustainability metrics, and creating targeted system optimizations. In this paper, we address the challenges of providing accurate and full-system (not just CPU) carbon footprints for serverless (FaaS) functions. To the best of our knowledge, this is the first work which develops an energy and carbon metrology framework for FaaS.Carbon footprints require a new approach to energy profiling. We use FaaS workload properties such as locality to develop a simple and practical online statistical disaggregation approach. Our fine-grained per-invocation carbon footprints also include shared hardware and software emissions, and use insights from Shapley values to fairly account for both operational and embodied emissions. Owing to the growing importance of carbon measurement, we develop a new rigorous marginal energy based validation methodology which results in accountable, complete, and fair footprints. Over a wide range of FaaS workloads and hardware platforms, our energy footprints have an accuracy of &gt; 99%.},
booktitle = {Proceedings of the 2024 ACM Symposium on Cloud Computing},
pages = {522–541},
numpages = {20},
keywords = {Carbon footprint, Cloud computing, Energy measurement, Functions as a Service, Sustainable computing},
location = {Redmond, WA, USA},
series = {SoCC '24}
}

@article{10.1145/3567550,
author = {Zhao, Yunhua and Damevski, Kostadin and Chen, Hui},
title = {A Systematic Survey of Just-in-Time Software Defect Prediction},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {10},
issn = {0360-0300},
url = {https://doi.org/10.1145/3567550},
doi = {10.1145/3567550},
abstract = {Recent years have experienced sustained focus in research on software defect prediction that aims to predict the likelihood of software defects. Moreover, with the increased interest in continuous deployment, a variant of software defect prediction called Just-in-Time Software Defect Prediction (JIT-SDP) focuses on predicting whether each incremental software change is defective. JIT-SDP is unique in that it consists of two interconnected data streams, one consisting of the arrivals of software changes stemming from design and implementation, and the other the (defective or clean) labels of software changes resulting from quality assurance processes.We present a systematic survey of 67 JIT-SDP studies with the objective to help researchers advance the state of the art in JIT-SDP and to help practitioners become familiar with recent progress. We summarize best practices in each phase of the JIT-SDP workflow, carry out a meta-analysis of prior studies, and suggest future research directions. Our meta-analysis of JIT-SDP studies indicates, among other findings, that the predictive performance correlates with change defect ratio, suggesting that JIT-SDP is most performant in projects that experience relatively high defect ratios. Future research directions for JIT-SDP include situating each technique into its application domain, reliability-aware JIT-SDP, and user-centered JIT-SDP.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {201},
numpages = {35},
keywords = {Software defect prediction, release software defect prediction, just-in-time software defect prediction, change-level software defect prediction, machine learning, searching-based algorithms, software change metrics, change defect density}
}

@inproceedings{10.1145/1449913.1449917,
author = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Saake, Gunter and Apel, Sven},
title = {Code generation to support static and dynamic composition of software product lines},
year = {2008},
isbn = {9781605582672},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1449913.1449917},
doi = {10.1145/1449913.1449917},
abstract = {Software product lines (SPLs) are used to create tailor-made software products by managing and composing reusable assets. Generating a software product from the assets of an SPL is possible statically before runtime or dynamically at load-time or runtime. Both approaches have benefits and drawbacks with respect to composition flexibility, performance, and resource consumption. Which type of composition is preferable should be decided by taking the application scenario into account. Current tools and languages, however, force a programmer to decide between static and dynamic composition during development. In this paper, we present an approach that employs code generation to support static and dynamic composition of features of a single code base. We offer an implementation on top of FeatureC++, an extension of the C++ programming language that supports software composition based on features. To simplify dynamic composition and to avoid creation of invalid products we furthermore provide means to (1) validate the correctness of a composition at runtime, (2) automatically instantiate SPLs in case of stand-alone applications, and (3) automatically apply interaction code of crosscutting concerns.},
booktitle = {Proceedings of the 7th International Conference on Generative Programming and Component Engineering},
pages = {3–12},
numpages = {10},
keywords = {dynamic feature binding, feature-oriented programming, software product lines, static feature binding},
location = {Nashville, TN, USA},
series = {GPCE '08}
}

@proceedings{10.1145/3689492,
title = {Onward! '24: Proceedings of the 2024 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
year = {2024},
isbn = {9798400712159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 2024 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (Onward! 2024), the premier multidisciplinary conference focused on everything to do with programming and software, including processes, methods, languages, communities and applications. Onward! is more radical, more visionary and more open than other conferences to ideas that are well-argued but not yet proven. We welcome different ways of thinking about, approaching, and reporting on programming language and software engineering research.},
location = {Pasadena, CA, USA}
}

@inproceedings{10.1145/3658644.3690283,
author = {Klemmer, Jan H. and Horstmann, Stefan Albert and Patnaik, Nikhil and Ludden, Cordelia and Burton, Cordell and Powers, Carson and Massacci, Fabio and Rahman, Akond and Votipka, Daniel and Lipford, Heather Richter and Rashid, Awais and Naiakshina, Alena and Fahl, Sascha},
title = {Using AI Assistants in Software Development: A Qualitative Study on Security Practices and Concerns},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3690283},
doi = {10.1145/3658644.3690283},
abstract = {Following the recent release of AI assistants, such as OpenAI's ChatGPT and GitHub Copilot, the software industry quickly utilized these tools for software development tasks, e.g., generating code or consulting AI for advice. While recent research has demonstrated that AI-generated code can contain security issues, how software professionals balance AI assistant usage and security remains unclear. This paper investigates how software professionals use AI assistants in secure software development, what security implications and considerations arise, and what impact they foresee on security in software development. We conducted 27 semi-structured interviews with software professionals, including software engineers, team leads, and security testers. We also reviewed 190 relevant Reddit posts and comments to gain insights into the current discourse surrounding AI assistants for software development. Our analysis of the interviews and Reddit posts finds that, despite many security and quality concerns, participants widely use AI assistants for security-critical tasks, e.g., code generation, threat modeling, and vulnerability detection. Participants' overall mistrust leads to checking AI suggestions in similar ways to human code. However, they expect improvements and, therefore, a heavier use of AI for security tasks in the future. We conclude with recommendations for software professionals to critically check AI suggestions, for AI creators to improve suggestion security and capabilities for ethical security tasks, and for academic researchers to consider general-purpose AI in software development.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {2726–2740},
numpages = {15},
keywords = {ai assistants, generative ai, interviews, large language models, llm, software development, software security},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@inproceedings{10.1109/SC41406.2024.00089,
author = {An, Wei and Bi, Xiao and Chen, Guanting and Chen, Shanhuang and Deng, Chengqi and Ding, Honghui and Dong, Kai and Du, Qiushi and Gao, Wenjun and Guan, Kang and Guo, Jianzhong and Guo, Yongqiang and Fu, Zhe and He, Ying and Huang, Panpan and Li, Jiashi and Liang, Wenfeng and Liu, Xiaodong and Liu, Xin and Liu, Yiyuan and Liu, Yuxuan and Lu, Shanghao and Lu, Xuan and Nie, Xiaotao and Pei, Tian and Qiu, Junjie and Qu, Hui and Ren, Zehui and Sha, Zhangli and Su, Xuecheng and Sun, Xiaowen and Tan, Yixuan and Tang, Minghui and Wang, Shiyu and Wang, Yaohui and Wang, Yongji and Xie, Ziwei and Xiong, Yiliang and Xu, Yanhong and Ye, Shengfeng and Yu, Shuiping and Zha, Yukun and Zhang, Liyue and Zhang, Haowei and Zhang, Mingchuan and Zhang, Wentao and Zhang, Yichao and Zhao, Chenggang and Zhao, Yao and Zhou, Shangyan and Zhou, Shunfeng and Zou, Yuheng},
title = {Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning},
year = {2024},
isbn = {9798350352917},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC41406.2024.00089},
doi = {10.1109/SC41406.2024.00089},
abstract = {The rapid progress in Deep Learning (DL) and Large Language Models (LLMs) has exponentially increased demands of computational power and bandwidth. This, combined with the high costs of faster computing chips and interconnects, has significantly inflated High Performance Computing (HPC) construction costs. To address these challenges, we introduce the Fire-Flyer AI-HPC architecture, a synergistic hardware-software co-design framework and its best practices. For DL training, we deployed the Fire-Flyer 2 with 10,000 PCIe A100 GPUs, achieved performance approximating the DGX-A100 while reducing costs by half and energy consumption by 40%. We specifically engineered HFReduce to accelerate allreduce communication and implemented numerous measures to keep our Computation-Storage Integrated Network congestion-free. Through our software stack, including HaiScale, 3FS, and HAI-Platform, we achieved substantial scalability by overlapping computation and communication. Our system-oriented experience from DL training provides valuable insights to drive future advancements in AI-HPC.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {83},
numpages = {23},
keywords = {All-Reduce, Artificial Intelligence Infrastructure, Best Practices, Cost-Effective, Deep Learning, High Performance Computing, Large Language Models, Machine Learning},
location = {Atlanta, GA, USA},
series = {SC '24}
}

@inproceedings{10.1145/3661638.3661690,
author = {Liu, Li and Liu, Zijin and Qian, Xuefei},
title = {Construction and Application of Digital Twin Cloud Platform for Rebar Straight Thread Processing Line},
year = {2024},
isbn = {9798400716966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661638.3661690},
doi = {10.1145/3661638.3661690},
abstract = {Faced with the problem of the "black box", which is prevalent in the traditional workshop manufacturing industry, the digital and intelligent transformation and upgrading of the manufacturing industry have become a focus area of research with the continuous development of a new generation of information technology. This study focuses on the rebar straight thread processing line, and explores the construction and application of the production line cloud platform with digital twin. Firstly, the overall framework of the production line digital twin cloud platform was constructed. Then, the digital twin modelling technology was explored from four perspectives: geometry, physics, behavior and rules. Additionally, an Internet of Things (IoT) platform was constructed to enable bidirectional mapping between the physical system and the virtual model using data as the driving force. Finally, a digital twin prototyping system for the rebar straight thread processing line with integrated hardware and software was developed, relying on the sawing and cutting head line of a one-dragger. Through actual engineering cases, the proposed theory and technology are tested, verifying their effectiveness and practicability. The study demonstrates the application value of digital twin technology in remote monitoring, health operation and maintenance, quality inspection, and intelligent production, providing a certain reference for the realization of intelligent manufacturing of rebar processing.},
booktitle = {Proceedings of the 2023 International Conference on Artificial Intelligence, Systems and Network Security},
pages = {263–276},
numpages = {14},
location = {Mianyang, China},
series = {AISNS '23}
}

@article{10.1145/3474373,
author = {Lin, Xuelian and Ma, Shuai and Jiang, Jiahao and Hou, Yanchen and Wo, Tianyu},
title = {Error Bounded Line Simplification Algorithms for Trajectory Compression: An Experimental Evaluation},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {3},
issn = {0362-5915},
url = {https://doi.org/10.1145/3474373},
doi = {10.1145/3474373},
abstract = {Nowadays, various sensors are collecting, storing, and transmitting tremendous trajectory data, and it is well known that the storage, network bandwidth, and computing resources could be heavily wasted if raw trajectory data is directly adopted. Line simplification algorithms are effective approaches to attacking this issue by compressing a trajectory to a set of continuous line segments, and are commonly used in practice. In this article, we first classify the error bounded line simplification algorithms into different categories and review each category of algorithms. We then study the data aging problem of line simplification algorithms and distance metrics from the views of aging friendliness and aging errors. Finally, we present a systematic experimental evaluation of representative error bounded line simplification algorithms, including both compression optimal and sub-optimal methods, in terms of commonly adopted perpendicular Euclidean, synchronous Euclidean, and direction-aware distances. Using real-life trajectory datasets, we systematically evaluate and analyze the performance (compression ratio, average error, running time, aging friendliness, and query friendliness) of error bounded line simplification algorithms with respect to distance metrics, trajectory sizes, and error bounds. Our study provides a full picture of error bounded line simplification algorithms, which leads to guidelines on how to choose appropriate algorithms and distance metrics for practical applications.},
journal = {ACM Trans. Database Syst.},
month = sep,
articleno = {11},
numpages = {44},
keywords = {Trajectory compression, line simplification, batch algorithms, online algorithms, one-pass algorithms}
}

@inproceedings{10.1145/3605098.3635917,
author = {Alt\i{}n\i{}\c{s}\i{}k, Metin and S\"{o}zer, Hasan and G\"{u}rsun, Gonca},
title = {Software Architecture Recovery from Multiple Dependency Models},
year = {2024},
isbn = {9798400702433},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605098.3635917},
doi = {10.1145/3605098.3635917},
abstract = {Software architecture recovery tools cluster software modules based on their inter-dependencies to suggest a gross-level decomposition with high modularity. However, there are usually multiple types of dependencies among software modules, each of which can be captured with a separate dependency model. In this paper, we evaluate the effectiveness of a software architecture recovery approach that utilizes multiple such dependency models. First, we perform clustering based on various models separately. These models reflect different types of dependencies, including call dependencies, evolutionary coupling and dependencies of modules on commonly used database tables. Then, we perform cluster aggregation on the obtained clustering results to propose a packaging structure to the designer. We present an industrial case study on a large-scale legacy system from the telecommunications domain. The highest accuracy is achieved with the aggregation of clustering results obtained based on 3 dependency models. The accuracy in terms of matching between the proposed packaging and the existing one is above 65%. The non-matching results mostly pinpoint architectural anomalies as confirmed by domain experts.},
booktitle = {Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing},
pages = {1185–1192},
numpages = {8},
keywords = {reverse engineering, software architecture recovery, software module clustering, software modularity, cluster aggregation, industrial case study},
location = {Avila, Spain},
series = {SAC '24}
}

@proceedings{10.1145/3643663,
title = {RoSE '24: Proceedings of the 2024 ACM/IEEE 6th International Workshop on Robotics Software Engineering},
year = {2024},
isbn = {9798400705663},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Software engineering is a crucial enabler for successful deployment of robotic applications. However, much of the research that is advancing the state of the art in robotics software engineering is dispersed across numerous conferences that are either primarily attended by robotics researchers and practitioners (e.g., ICRA, IROS, SIMPAR) or attended mostly by software engineering researchers and practitioners (e.g., ICSE, FSE, MODELS). At robotics conferences, software engineering lacks visibility and vice versa.RoSE brings together researchers and practitioners from both domains at a prominent conference to foster crossfertilization between the two domains. Through a combination of presentations, papers, and discussions, RoSE helps researchers within the budding field of robotics software engineering to learn more about the challenges faced by robotics practitioners that (i) require further research from the software engineering community or (ii) are already solved but solutions have not yet been widely adopted by practitioners.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/2602576.2602585,
author = {Etxeberria, Leire and Trubiani, Catia and Cortellessa, Vittorio and Sagardui, Goiuria},
title = {Performance-based selection of software and hardware features under parameter uncertainty},
year = {2014},
isbn = {9781450325769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602576.2602585},
doi = {10.1145/2602576.2602585},
abstract = {Configurable software systems allow stakeholders to derive variants by selecting software and/or hardware features. Performance analysis of feature-based systems has been of large interest in the last few years, however a major research challenge is still to conduct such analysis before achieving full knowledge of the system, namely under a certain degree of uncertainty. In this paper we present an approach to analyze the correlation between selection of features embedding uncertain parameters and system performance. In particular, we provide best and worst case performance bounds on the basis of selected features and, in cases of wide gaps among these bounds, we carry on a sensitivity analysis process aimed at taming the uncertainty of parameters. The application of our approach to a case study in the e-health domain demonstrates how to support stakeholders in the identification of system variants that meet performance requirements.},
booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {23–32},
numpages = {10},
keywords = {feature selection, performance analysis, software architectures, uncertainty},
location = {Marcq-en-Bareul, France},
series = {QoSA '14}
}

@inproceedings{10.1145/3597503.3639212,
author = {Wang, Huanting and Tang, Zhanyong and Tan, Shin Hwei and Wang, Jie and Liu, Yuzhe and Fang, Hejun and Xia, Chunwei and Wang, Zheng},
title = {Combining Structured Static Code Information and Dynamic Symbolic Traces for Software Vulnerability Prediction},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639212},
doi = {10.1145/3597503.3639212},
abstract = {Deep learning (DL) has emerged as a viable means for identifying software bugs and vulnerabilities. The success of DL relies on having a suitable representation of the problem domain. However, existing DL-based solutions for learning program representations have limitations - they either cannot capture the deep, precise program semantics or suffer from poor scalability. We present Concoction, the first DL system to learn program presentations by combining static source code information and dynamic program execution traces. Concoction employs unsupervised active learning techniques to determine a subset of important paths to collect dynamic symbolic execution traces. By implementing a focused symbolic execution solution, Concoction brings the benefits of static and dynamic code features while reducing the expensive symbolic execution overhead. We integrate Concoction with fuzzing techniques to detect function-level code vulnerabilities in C programs from 20 open-source projects. In 200 hours of automated concurrent test runs, Concoction has successfully uncovered vulnerabilities in all tested projects, identifying 54 unique vulnerabilities and yielding 37 new, unique CVE IDs. Concoction also significantly outperforms 16 prior methods by providing higher accuracy and lower false positive rates.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {169},
numpages = {13},
keywords = {software vulnerability detection, deep learning, symbolic execution},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@article{10.1145/3320277,
author = {Zhang, Mingyue and Wei, Xuan and Guo, Xunhua and Chen, Guoqing and Wei, Qiang},
title = {Identifying Complements and Substitutes of Products: A Neural Network Framework Based on Product Embedding},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3320277},
doi = {10.1145/3320277},
abstract = {Complements and substitutes are two typical product relationships that deserve consideration in online product recommendation. One of the key objectives of recommender systems is to promote cross-selling, which heavily relies on recommending the appropriate type of products in specific scenarios. Research on consumer behavior has shown that consumers usually prefer substitutes in the browsing stage whereas complements in the purchasing stage. Thus, it is of great importance to identify the complementary and substitutable relationships between products. In this article, we design a neural network based framework that integrates the textual content and non-textual information of online reviews to mine product relationships. For the textual content, we utilize methods such as LDA topic modeling to represent products in a succinct form called “embedding.” To capture the semantics of complementary and substitutable relationships, we design a modeling process that transfers the product embeddings into semantic features and incorporates additional non-textual factors of product reviews. Extensive experiments are conducted to verify the effectiveness of the proposed product relationship mining model. The advantages and robustness of our model are discussed from various perspectives.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {34},
numpages = {29},
keywords = {Complements, online reviews, product embedding, product recommendation, product relationship, substitutes}
}

@inproceedings{10.1145/3679006.3685070,
author = {Sun, Chang-Ai and Xing, Jiayu and Li, Xiaobei and Zhang, Xiaoyi and Fu, An},
title = {Metamorphic Testing of Image Processing Applications: A General Framework and Optimization Strategies},
year = {2024},
isbn = {9798400711176},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3679006.3685070},
doi = {10.1145/3679006.3685070},
abstract = {Metamorphic testing (MT) is widely adopted for testing image processing applications. Although a variety of metamorphic relations (MRs) have been proposed, using all of them for testing will cost a large amount of computational resources. In addition, complex transformation operations are not well supported when generating follow-up test images based on MRs. To overcome these limitations, this study proposes a general MT framework for image processing applications, which employs CycleGAN to generate images that are very close to the realistic scenarios and leverages MRs for various categories of image processing applications. Two optimization strategies called EquivalentMR and SSampling are further proposed to reduce MRs and test images, respectively. A prototype tool called MT4I was developed. The experimental results showed that the proposed framework was capable of effectively testing various categories of image processing applications, while optimization strategies can reduce the amounts of MRs and test images without significantly jeopardizing the fault detection effectiveness.},
booktitle = {Proceedings of the 9th ACM International Workshop on Metamorphic Testing},
pages = {26–33},
numpages = {8},
keywords = {Fault Detection Effectiveness, Image Processing Applications, Metamorphic Testing, Software Testing},
location = {Vienna, Austria},
series = {MET 2024}
}

@proceedings{10.1145/3643661,
title = {InteNSE '24: Proceedings of the ACM/IEEE 2nd International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering},
year = {2024},
isbn = {9798400705649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {InteNSE is an interdisciplinary workshop for research at the intersection of Artificial Intelligence (AI) and Software Engineering (SE) and would be a pioneer in emphasizing the implicit properties and applications of neural software engineering and analysis. Due to recent computational advancements, AI has become an inseparable part of the SE research community, with Large Language Models (LLMs) showing a promising performance to automate SE tasks. However, most research in the AI and SE communities consider machine learning (ML) components as closed-box, i.e., only considering the final performance of the developed models as an evaluation metric. Ignoring the implicit properties of neural models, such as interpretability, robustness, and fairness, one cannot validate its actual performance, generalizability, and whether it is learning what it should do. Specifically, in the domain of SE, where the result of AI4SE tools is code synthesis, bug finding, or repair; interpretability and robustness are crucial to ensure the reliability of the products.},
location = {Lisbon, Portugal}
}

