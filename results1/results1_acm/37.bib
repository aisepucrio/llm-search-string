@inproceedings{10.1145/3590837.3590918,
author = {Gautam, Shikha and Khunteta, Ajay and Ghosh, Debolina},
title = {A Review on Software Defect Prediction Using Machine Learning},
year = {2023},
isbn = {9781450399937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3590837.3590918},
doi = {10.1145/3590837.3590918},
abstract = {Software plays an important role in many of the systems and devices that make up our modern societies. In order to provide their customers with software of a higher quality in a shorter amount of time, numerous software companies are developing software systems of varying sizes for various purposes. It is too challenging to produce high-quality software in a shorter amount of time due to the constraints of software development and the growing size of software data. Therefore, prior to delivering the software product, defect prediction can significantly contribute to a project's success in terms of; cost and quality to evaluate the quality of their software. The goal of the literature review is to investigate about the current trends of software defect prediction approaches. Conclusion of the literature review introduce that many machine learning algorithms are implemented named with Random forest, Logistic regression, Na\"{\i}ve Bayes and Artificial neutral Network etc. with different software metrics like CK metrics, Source code metric etc. The performance measurement of the model done by various methods like accuracy, precision etc.},
booktitle = {Proceedings of the 4th International Conference on Information Management &amp; Machine Intelligence},
articleno = {81},
numpages = {10},
keywords = {Statement Level, Software Metrics, Software Defect Prediction, Machine Learning, Datasets},
location = {Jaipur, India},
series = {ICIMMI '22}
}

@proceedings{10.1145/3617572,
title = {SDD 2023: Proceedings of the 1st International Workshop on Software Defect Datasets},
year = {2023},
isbn = {9798400703775},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the First International Workshop on Software Defect Datasets (SDD), co-located with ESEC/FSE 2023 and to take place in San Francisco, CA on December 8th, 2023.},
location = {San Francisco, CA, USA}
}

@article{10.1145/3567550,
author = {Zhao, Yunhua and Damevski, Kostadin and Chen, Hui},
title = {A Systematic Survey of Just-in-Time Software Defect Prediction},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {10},
issn = {0360-0300},
url = {https://doi.org/10.1145/3567550},
doi = {10.1145/3567550},
abstract = {Recent years have experienced sustained focus in research on software defect prediction that aims to predict the likelihood of software defects. Moreover, with the increased interest in continuous deployment, a variant of software defect prediction called Just-in-Time Software Defect Prediction (JIT-SDP) focuses on predicting whether each incremental software change is defective. JIT-SDP is unique in that it consists of two interconnected data streams, one consisting of the arrivals of software changes stemming from design and implementation, and the other the (defective or clean) labels of software changes resulting from quality assurance processes.We present a systematic survey of 67 JIT-SDP studies with the objective to help researchers advance the state of the art in JIT-SDP and to help practitioners become familiar with recent progress. We summarize best practices in each phase of the JIT-SDP workflow, carry out a meta-analysis of prior studies, and suggest future research directions. Our meta-analysis of JIT-SDP studies indicates, among other findings, that the predictive performance correlates with change defect ratio, suggesting that JIT-SDP is most performant in projects that experience relatively high defect ratios. Future research directions for JIT-SDP include situating each technique into its application domain, reliability-aware JIT-SDP, and user-centered JIT-SDP.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {201},
numpages = {35},
keywords = {change defect density, software change metrics, searching-based algorithms, machine learning, change-level software defect prediction, just-in-time software defect prediction, release software defect prediction, Software defect prediction}
}

@article{10.1145/3589342,
author = {Gangwar, Arvind Kumar and Kumar, Sandeep},
title = {Concept Drift in Software Defect Prediction: A Method for Detecting and Handling the Drift},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3589342},
doi = {10.1145/3589342},
abstract = {Software Defect Prediction (SDP) is crucial towards software quality assurance in software engineering. SDP analyzes the software metrics data for timely prediction of defect prone software modules. Prediction process is automated by constructing defect prediction classification models using machine learning techniques. These models are trained using metrics data from historical projects of similar types. Based on the learned experience, models are used to predict defect prone modules in currently tested software. These models perform well if the concept is stationary in a dynamic software development environment. But their performance degrades unexpectedly in the presence of change in concept (Concept Drift). Therefore, concept drift (CD) detection is an important activity for improving the overall accuracy of the prediction model. Previous studies on SDP have shown that CD may occur in software defect data and the used defect prediction model may require to be updated to deal with CD. This phenomenon of handling the CD is known as CD adaptation. It is observed that still efforts need to be done in this direction in the SDP domain. In this article, we have proposed a pair of paired learners (PoPL) approach for handling CD in SDP. We combined the drift detection capabilities of two independent paired learners and used the paired learner (PL) with the best performance in recent time for next prediction. We experimented on various publicly available software defect datasets garnered from public data repositories. Experimentation results showed that our proposed approach performed better than the existing similar works and the base PL model based on various performance measures.},
journal = {ACM Trans. Internet Technol.},
month = may,
articleno = {31},
numpages = {28},
keywords = {software quality assurance, software defect prediction, paired learning, Concept drift}
}

@inproceedings{10.1145/3416508.3417114,
author = {Aljamaan, Hamoud and Alazba, Amal},
title = {Software defect prediction using tree-based ensembles},
year = {2020},
isbn = {9781450381277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416508.3417114},
doi = {10.1145/3416508.3417114},
abstract = {Software defect prediction is an active research area in software engineering. Accurate prediction of software defects assists software engineers in guiding software quality assurance activities. In machine learning, ensemble learning has been proven to improve the prediction performance over individual machine learning models. Recently, many Tree-based ensembles have been proposed in the literature, and their prediction capabilities were not investigated in defect prediction. In this paper, we will empirically investigate the prediction performance of seven Tree-based ensembles in defect prediction. Two ensembles are classified as bagging ensembles: Random Forest and Extra Trees, while the other five ensembles are boosting ensembles: Ada boost, Gradient Boosting, Hist Gradient Boosting, XGBoost and CatBoost. The study utilized 11 publicly available MDP NASA software defect datasets. Empirical results indicate the superiority of Tree-based bagging ensembles: Random Forest and Extra Trees ensembles over other Tree-based boosting ensembles. However, none of the investigated Tree-based ensembles was significantly lower than individual decision trees in prediction performance. Finally, Adaboost ensemble was the worst performing ensemble among all Tree-based ensembles.},
booktitle = {Proceedings of the 16th ACM International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {1–10},
numpages = {10},
keywords = {Bagging, Boosting, Classification, Ensemble Learning, Machine Learning, Prediction, Software Defect},
location = {Virtual, USA},
series = {PROMISE 2020}
}

@inproceedings{10.1145/3628797.3628963,
author = {Thi-Mai-Anh, Bui and Nhat-Hai, Nguyen},
title = {On the Value of Code Embedding and Imbalanced Learning Approaches for Software Defect Prediction},
year = {2023},
isbn = {9798400708916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628797.3628963},
doi = {10.1145/3628797.3628963},
abstract = {Automated software defect prediction aims to identify and estimate the likelihood of defects in software source code elements, seeking to enhance software quality while reducing testing costs. Previous research on software defect prediction primarily concentrated on investigating design-related features such as source code complexity and object-oriented design metrics for the purpose of classifying program elements into two categories: (i) defective and (ii) non-detective. Nevertheless, the majority of these studies have relied solely on hand-crafted software metrics, neglecting the valuable asset of source code instruction, which can play a pivotal role in detecting bugs. This study leverages the use of source code embedding techniques to extract essential information from program elements through a convolutional neural network. The likelihood of a source file element (e.g., class or method) being defective is established through the utilization of a fully connected network that incorporates both source code features and design-related attributes. Additionally, we explore specific imbalanced learning strategies to address the skewed defect data distribution issue. To assess the effectiveness of our proposed approach, we conducted experiments on the publicly available dataset, namely PROMISE. The empirical results consistently showcase the superior performance of our method, as it effectively predicts defective source files, outperforming other state-of-the-art models.},
booktitle = {Proceedings of the 12th International Symposium on Information and Communication Technology},
pages = {510–516},
numpages = {7},
keywords = {code embedding, convolutional neural network, cost sensitive learning, sampling data},
location = {Ho Chi Minh, Vietnam},
series = {SOICT '23}
}

@inproceedings{10.1145/3530019.3531330,
author = {sadaf, saadia and Iqbal, Danish and Buhnova, Barbora},
title = {AI-Based Software Defect Prediction for Trustworthy Android Apps},
year = {2022},
isbn = {9781450396134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3530019.3531330},
doi = {10.1145/3530019.3531330},
abstract = {The present time in the industry is a time where Android Applications are in a wide range with its widespread of the users also. With the increased use of Android applications, the defects in the Android context have also been increasing. The malware of defective software can be any pernicious program with malignant effects. Many techniques based on static, dynamic, and hybrid approaches have been proposed with the combination of Machine learning (ML) or Artificial Intelligence (AI) techniques. In this regard. Scientifically, it is complicated to examine the malignant effects. A single approach cannot predict defects alone, so multiple approaches must be used simultaneously. However, the proposed techniques do not describe the types of defects they address. The paper aims to propose a framework that classifies the defects. The Artificial Intelligence (AI) techniques are described, and the different defects are mapped to them. The mapping of defects to AI techniques is based on the types of defects found in the Android Context. The accuracy of the techniques and the working criteria has been set as the mapping metrics. This will significantly improve the quality and testing of the product. However, the appropriate technique for a particular type of defect could be easily selected. This will reduce the cost and time efforts put into predicting defects.},
booktitle = {Proceedings of the 26th International Conference on Evaluation and Assessment in Software Engineering},
pages = {393–398},
numpages = {6},
keywords = {Software Defect prevention technique, Machine Learning, Defect Prediction Technique, Artificial Intelligence},
location = {Gothenburg, Sweden},
series = {EASE '22}
}

@inproceedings{10.1145/3474198.3478215,
author = {Du, Xiaozhi and Yue, Hehe and Dong, Honglei},
title = {Software Defect Prediction Method based on Hybrid Sampling},
year = {2022},
isbn = {9781450390149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474198.3478215},
doi = {10.1145/3474198.3478215},
abstract = {Software defect prediction is an essential technology to provide guidance and assistance for software testers and developers. However, the problem of imbalanced data sets limits the effect and application of the software defect prediction. To address this issue, this paper proposes a software defect prediction method based on hybrid sampling, which combines the strategies of over-sampling with under-sampling. For minority class, over-sampling uses k-means to cluster samples, then adopts SMOTE to generate artificial data based on safe areas of the clustering outcome. For majority class, under-sampling uses logistic regression classifier to get the misclassification probability of each sample and its instance hardness value. Then the samples, whose instance hardness values are lower than the threshold, are removed from the datasets. The experimental results show that our method is superior to the previous methods. Compared with SMOTE-kNN, SMOTE-Tomek, SMOTE and DBSMOTE, the accuracy of our method is improved by 17.60%, 6.99%, 8.66% and 26.18% on average respectively.},
booktitle = {International Conference on Frontiers of Electronics, Information and Computation Technologies},
articleno = {93},
numpages = {9},
keywords = {Software defect prediction, Hybrid sampling, Data imbalance},
location = {Changsha, China},
series = {ICFEICT 2021}
}

@inproceedings{10.1145/3568562.3568587,
author = {Ho, Anh and Nhat Hai, Nguyen and Thi-Mai-Anh, Bui},
title = {Combining Deep Learning and Kernel PCA for Software Defect Prediction},
year = {2022},
isbn = {9781450397254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568562.3568587},
doi = {10.1145/3568562.3568587},
abstract = {Software defect prediction aims to automatically determine the most likely location of defective program elements (i.e., statement, method, class, module etc.). Previous studies for software defect prediction mainly focus on exploring designing features such as source code complexity, object oriented design metrics etc. to classify program elements into two categories: (i) defective and (ii) non-defective. Although these approaches have obtained promising results, there exists two significant challenges in this research field: (i) removing irrelevant and redundant information from designing structures ; (ii) reducing the impact of skewed data distribution on learning models. In this paper, we aim to address these two issues by firstly applying kernel PCA to extract essential information from designing features and secondly proposing a deep neural network model which investigates the non-linear relationship among features. In order to mitigate the class imbalance, we apply a weighted loss function combined with a bootstrapping method to handle batch training mechanism of our model. We conducted some experiments to assess the performance of our proposed approach over NASA (with 10 projects) and PROMISE (with 34 projects) datasets. In order to leverage the efficiency of kernel PCA technique in software defect prediction, we compared it to some traditional feature selection approaches over a high-dimensional dataset ECLIPSE. The empirical results showed that our proposed method has outperformed these other state-of-the-art models by effectively predicting defective source files.},
booktitle = {Proceedings of the 11th International Symposium on Information and Communication Technology},
pages = {360–367},
numpages = {8},
keywords = {kernel PCA, feature reduction, deep neural network},
location = {Hanoi, Vietnam},
series = {SoICT '22}
}

@inproceedings{10.1145/3368926.3369711,
author = {Ha, Duy-An and Chen, Ting-Hsuan and Yuan, Shyan-Ming},
title = {Unsupervised methods for Software Defect Prediction},
year = {2019},
isbn = {9781450372459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368926.3369711},
doi = {10.1145/3368926.3369711},
abstract = {Software Defect Prediction (SDP) aims to assess software quality by using machine learning techniques. Recently, by proposing the connectivity-based unsupervised learning method, Zhang et al. have been proven that unsupervised classification has great potential to apply to this problem. Inspiring by this idea, in our work we try to replicate the results of Zhang et al.'s experiment and attempt to improve the performance by examining different techniques at each step of the approach using unsupervised learning methods to solve the SDP problem. Specifically, we try to follow the steps of the experiment described in their work strictly and examine three other clustering methods with four other ways for feature selection besides using all. To the best of our knowledge, these methods are first applied in SDP to evaluate their predictive power. For replicating the results, generally results in our experiments are not as good as the previous work. It may be due to we do not know which features are used in their experiment exactly. Fluid clustering and spectral clustering give better results than Newman clustering and CNM clustering in our experiments. Additionally, the experiments also show that using Kernel Principal Component Analysis (KPCA) or Non-Negative Matrix Factorization (NMF) for feature selection step gives better performance than using all features in the case of unlabeled data. Lastly, to make replicating our work easy, a lightweight framework is created and released on Github.},
booktitle = {Proceedings of the 10th International Symposium on Information and Communication Technology},
pages = {49–55},
numpages = {7},
keywords = {Unsupervised Learning, Software Engineering, Software Defect Prediction, Machine Learning, Community Structure Detection},
location = {Hanoi, Ha Long Bay, Viet Nam},
series = {SoICT '19}
}

@inproceedings{10.1145/3387940.3391463,
author = {Omri, Safa and Sinz, Carsten},
title = {Deep Learning for Software Defect Prediction: A Survey},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391463},
doi = {10.1145/3387940.3391463},
abstract = {Software fault prediction is an important and beneficial practice for improving software quality and reliability. The ability to predict which components in a large software system are most likely to contain the largest numbers of faults in the next release helps to better manage projects, including early estimation of possible release delays, and affordably guide corrective actions to improve the quality of the software. However, developing robust fault prediction models is a challenging task and many techniques have been proposed in the literature. Traditional software fault prediction studies mainly focus on manually designing features (e.g. complexity metrics), which are input into machine learning classifiers to identify defective code. However, these features often fail to capture the semantic and structural information of programs. Such information is needed for building accurate fault prediction models. In this survey, we discuss various approaches in fault prediction, also explaining how in recent studies deep learning algorithms for fault prediction help to bridge the gap between programs' semantics and fault prediction features and make accurate predictions.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {209–214},
numpages = {6},
keywords = {software testing, software quality assurance, software defect prediction, machine learning, deep learning},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3584871.3584885,
author = {Malhotra, Ruchika and Chawla, Sonali and Sharma, Anjali},
title = {An Artificial Neural Network Model based on Binary Particle Swarm Optimization for enhancing the efficiency of Software Defect Prediction},
year = {2023},
isbn = {9781450398237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584871.3584885},
doi = {10.1145/3584871.3584885},
abstract = {With the rise in the growth of the software industry, it is essential to identify software defects in earlier stages to save costs and improve the efficiency of the software development lifecycle process. We have devised a hybrid software defect prediction (SDP) model that integrates Binary Particle Swarm Optimization (Binary PSO), Synthetic Minority Oversampling Technique (SMOTE), and Artificial Neural Network (ANN). BPSO is applied as a wrapper feature selection process utilizing AUC as a fitness function, SMOTE handles the dataset imbalance, and ANN is used as a classification algorithm for predicting software defects. We analyze the proposed BPSO-SMOTE-ANN model's predictive capability using the AUC and G-mean performance metrics. The proposed hybrid model is found helpful in predicting software defects. The statistical results suggest the enhanced performance of the proposed hybrid model concerning AUC and G-mean values. Also, the hybrid model was found to be competitive with other machine learning(ML) algorithms in determining software defects.},
booktitle = {Proceedings of the 2023 6th International Conference on Software Engineering and Information Management},
pages = {92–100},
numpages = {9},
keywords = {Software Defect Prediction, Search-based Techniques, SMOTE, Particle Swarm Optimization, Artificial Neural Networks},
location = {Palmerston North, New Zealand},
series = {ICSIM '23}
}

@article{10.1145/3572905,
author = {Kotti, Zoe and Galanopoulou, Rafaila and Spinellis, Diomidis},
title = {Machine Learning for Software Engineering: A Tertiary Study},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3572905},
doi = {10.1145/3572905},
abstract = {Machine learning (ML) techniques increase the effectiveness of software engineering (SE) lifecycle activities. We systematically collected, quality-assessed, summarized, and categorized 83 reviews in ML for SE published between 2009 and 2022, covering 6,117 primary studies. The SE areas most tackled with ML are software quality and testing, while human-centered areas appear more challenging for ML. We propose a number of ML for SE research challenges and actions, including conducting further empirical validation and industrial studies on ML, reconsidering deficient SE methods, documenting and automating data collection and pipeline processes, reexamining how industrial practitioners distribute their proprietary data, and implementing incremental ML approaches.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {256},
numpages = {39},
keywords = {systematic literature review, software engineering, machine learning, Tertiary study}
}

@inproceedings{10.1145/3238147.3240469,
author = {Qu, Yu and Liu, Ting and Chi, Jianlei and Jin, Yangxu and Cui, Di and He, Ancheng and Zheng, Qinghua},
title = {node2defect: using network embedding to improve software defect prediction},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240469},
doi = {10.1145/3238147.3240469},
abstract = {Network measures have been proved to be useful in predicting software defects. Leveraging the dependency relationships between software modules, network measures can capture various structural features of software systems. However, existing studies have relied on user-defined network measures (e.g., degree statistics or centrality metrics), which are inflexible and require high computation cost, to describe the structural features. In this paper, we propose a new method called node2defect which uses a newly proposed network embedding technique, node2vec, to automatically learn to encode dependency network structure into low-dimensional vector spaces to improve software defect prediction. Specifically, we firstly construct a program's Class Dependency Network. Then node2vec is used to automatically learn structural features of the network. After that, we combine the learned features with traditional software engineering features, for accurate defect prediction. We evaluate our method on 15 open source programs. The experimental results show that in average, node2defect improves the state-of-the-art approach by 9.15% in terms of F-measure.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {844–849},
numpages = {6},
keywords = {software metrics, network embedding, defect prediction, Software defect},
location = {Montpellier, France},
series = {ASE '18}
}

@proceedings{10.1145/3549034,
title = {MaLTeSQuE 2022: Proceedings of the 6th International Workshop on Machine Learning Techniques for Software Quality Evaluation},
year = {2022},
isbn = {9781450394567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 6th edition of the workshop on Machine Learning Techniques for Software Quality Evaluation (MaLTeSQuE 2022), held in Singapore, on November 18th, 2022, co-located with the 30th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2022). MaLTeSQuE received a total of six submissions from all over the world, from which five papers were included in the program. The program also features two keynotes, by Yuriy Brun and Mike Papadakis, on the promises, dangers, and best practices of working at the intersection of machine learning and software engineering.},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3538969.3543809,
author = {Sotgiu, Angelo and Pintor, Maura and Biggio, Battista},
title = {Explainability-based Debugging of Machine Learning for Vulnerability Discovery},
year = {2022},
isbn = {9781450396707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538969.3543809},
doi = {10.1145/3538969.3543809},
abstract = {Machine learning has been successfully used for increasingly complex and critical tasks, achieving high performance and efficiency that would not be possible for human operators. Unfortunately, recent studies have shown that, despite its power, this technology tends to learn spurious correlations from data, making it weak and susceptible to manipulation. Explainability techniques are often used to identify the most relevant features contributing to the decision. However, this is often done by taking examples one by one and trying to show the problem locally. To mitigate this issue, we propose in this paper a systematic method to leverage explainability techniques and build on their results to highlight problems in the model design and training. With an empirical analysis on the Devign dataset, we validate the proposed methodology with a CodeBERT model trained for vulnerability discovery, showing that, despite its impressive performances, spurious correlations consistently steer its decision.},
booktitle = {Proceedings of the 17th International Conference on Availability, Reliability and Security},
articleno = {113},
numpages = {8},
keywords = {code vulnerability detection, datasets, machine learning, neural networks},
location = {Vienna, Austria},
series = {ARES '22}
}

@inproceedings{10.1145/3474124.3474193,
author = {Agnihotri, Mansi and Chug, Anuradha},
title = {Analyzing the Relationship between Software Metrics and Bad Smells Using Critical Metric Value (CMV)},
year = {2021},
isbn = {9781450389204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474124.3474193},
doi = {10.1145/3474124.3474193},
abstract = {User requirements for a software system frequently evolve with time, and developers sometimes make incorrect implementation choices while meeting such requirements. These choices tend to introduce system flaws, i.e., bad smells in the program's source code. Bad smells do not disturb the normal functioning of a system, but they might worsen the software quality by increasing its complexity. Software metrics play a significant role in the analysis of object-oriented properties of a system. Also, change in software metrics’ values are often used to predict bad smells in a code. In the current study, an investigation has been conducted on four open-source projects. The concept of Critical Metric Value (CMV) has been introduced in the current study, and its impact on the occurrence of five selected bad smells has been examined to establish a relationship between software metrics and bad smells. CMV is that value of any software metric that is considered an outlier when compared to the rest of the metric values. The selected bad smells have been categorized as Null, Only, and Multiple based on the presence of CMV in a class. This study shows that 37.3% of the total classes have been affected by bad smells. Long statement bad smell pre-dominantly affects the selected systems as it is present in 25.74% classes. The experiment's findings show that 82.5% of the total bad smells occur in a class that consists of at least one CMV. The current study helps to establish a relationship between the bad smells and critical software metrics.},
booktitle = {Proceedings of the 2021 Thirteenth International Conference on Contemporary Computing},
pages = {450–456},
numpages = {7},
keywords = {Software Quality, Software Metric, Critical Software Metrics, Bad Smells},
location = {Noida, India},
series = {IC3-2021}
}

@article{10.1145/3511805,
author = {Ram\'{\i}rez, Aurora and Feldt, Robert and Romero, Jos\'{e} Ra\'{u}l},
title = {A Taxonomy of Information Attributes for Test Case Prioritisation: Applicability, Machine Learning},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3511805},
doi = {10.1145/3511805},
abstract = {Most software companies have extensive test suites and re-run parts of them continuously to ensure that recent changes have no adverse effects. Since test suites are costly to execute, industry needs methods for test case prioritisation (TCP). Recently, TCP methods use machine learning (ML) to exploit the information known about the system under test and its test cases. However, the value added by ML-based TCP methods should be critically assessed with respect to the cost of collecting the information. This article analyses two decades of TCP research and presents a taxonomy of 91 information attributes that have been used. The attributes are classified with respect to their information sources and the characteristics of their extraction process. Based on this taxonomy, TCP methods validated with industrial data and those applying ML are analysed in terms of information availability, attribute combination and definition of data features suitable for ML. Relying on a high number of information attributes, assuming easy access to system under test code and simplified testing environments are identified as factors that might hamper industrial applicability of ML-based TCP. The TePIA taxonomy provides a reference framework to unify terminology and evaluate alternatives considering the cost-benefit of the information attributes.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {21},
numpages = {42},
keywords = {industry, test case prioritisation, machine learning, taxonomy, Regression testing}
}

@inproceedings{10.1145/3342999.3343010,
author = {Cui, Mengtian and Sun, Yue and Lu, Yang and Jiang, Yue},
title = {Study on the Influence of the Number of Features on the Performance of Software Defect Prediction Model},
year = {2019},
isbn = {9781450371605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342999.3343010},
doi = {10.1145/3342999.3343010},
abstract = {The software defect prediction model based on machine learning technology is the key to improve the reliability of software. The influence of the number of features on the performance of different software defect prediction models was proposed in this paper. First, a new data sets was built, which is increasing by the number of features based on the NASA public data sets. Then, the eight predictive models are experimented based on these data sets. Next, the influence of the number of features on the performance of different prediction models was analyzed based on the experimental results. Next, the AUC values obtained from the experiment were used to evaluate the performance of different prediction models, and the coefficient of variation C·V values was used to evaluate the performance stability of different prediction models while the number of features changed. In the end, the experiments show that the performance of the predictive model C4.5 is highly susceptible to changes in the number of features, while the performance of the predictive model SMO is relatively stable.},
booktitle = {Proceedings of the 2019 3rd International Conference on Deep Learning Technologies},
pages = {32–37},
numpages = {6},
keywords = {software defect prediction, number of features, machine learning, feature selection},
location = {Xiamen, China},
series = {ICDLT '19}
}

@proceedings{10.1145/3616901,
title = {FAIML '23: Proceedings of the 2023 International Conference on Frontiers of Artificial Intelligence and Machine Learning},
year = {2023},
isbn = {9798400707544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@inproceedings{10.1145/3028842.3028859,
author = {Gao, Yan and Yang, Chunhui},
title = {Software defect prediction based on manifold learning in subspace selection},
year = {2016},
isbn = {9781450347990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3028842.3028859},
doi = {10.1145/3028842.3028859},
abstract = {Software defects will lead to software running error and system crashes. In order to detect software defect as early as possible at early stage of software development, a series of machine learning approaches have been studied and applied to predict defects in software modules. Unfortunately, the imbalanceof software defect datasets brings great challenge to software defect prediction model training. In this paper, a new manifold learning based subspace learning algorithm, Discriminative Locality Alignment(DLA), is introduced into software defects prediction. Experimental results demonstrate that DLA is consistently superior to LDA (Linear Discriminant Analysis) and PCA (Principal Component Analysis) in terms of discriminate information extraction and prediction performance. In addition, DLA reveals some attractive intrinsic properties for numeric calculation, e.g. it can overcome the matrix singular problem and small sample size problem in software defect prediction.},
booktitle = {Proceedings of the 1st International Conference on Intelligent Information Processing},
articleno = {17},
numpages = {6},
keywords = {support vector machine, software defect prediction, manifold learning, discriminative locality alignment},
location = {Wuhan, China},
series = {ICIIP '16}
}

@inproceedings{10.1145/3239576.3239622,
author = {Yang, Zhao and Qian, Hongbing},
title = {Automated Parameter Tuning of Artificial Neural Networks for Software Defect Prediction},
year = {2018},
isbn = {9781450364607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239576.3239622},
doi = {10.1145/3239576.3239622},
abstract = {Defect prediction can help predict defect-prone software modules and improve the efficiency and accuracy of defect location and repair, which plays an extremely important role in software quality assurance. Artificial Neural Networks (ANNs), a family of powerful machine learning regression or classification models, have been widely applied for defect prediction. However, the performance of these models will be degraded if they use suboptimal default parameter settings (e.g., the number of units in the hidden layer). This paper utilizes an automated parameter tuning technique-Caret to optimize parameter settings. In our study, 30 datasets are downloaded from the Tera-PROMISE Repository. According to the characteristics of the datasets, we select key features (metrics) as predictors to train defect prediction models. The experiment applies feed-forward, single hidden layer artificial neural network as classifier to build different defect prediction models respectively with optimized parameter settings and with default parameter settings. Confusion matrix and ROC curve are used for evaluating the quality of the models above. The results show that the models trained with optimized parameter settings outperform the models trained with default parameter settings. Hence, we suggest that researchers should pay attention to tuning parameter settings by Caret for ANNs instead of using suboptimal default settings if they select ANNs for training models in the future defect prediction studies.},
booktitle = {Proceedings of the 2nd International Conference on Advances in Image Processing},
pages = {203–209},
numpages = {7},
keywords = {Software defect prediction, Metrics, Automated Parameter Tuning, Artificial Neural Networks},
location = {Chengdu, China},
series = {ICAIP '18}
}

@inproceedings{10.1145/3180374.3181331,
author = {Li, Yuting and Su, Jianmin and Yang, Xiaoxing},
title = {Multi-Objective vs. Single-Objective Approaches for Software Defect Prediction},
year = {2018},
isbn = {9781450354318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180374.3181331},
doi = {10.1145/3180374.3181331},
abstract = {Software defect prediction employs attributes of software modules to identify defect-prone modules and thus improves software reliability by allocating testing resources more efficiently. Realizing that single-objective methods might be insufficient for solving defect prediction problems, some researchers have proposed multi-objective learning approaches, and proved better performance of multi-objective than single-objective methods. However, existing compared single-objective methods optimize a completely different goal from goals of multi-objective approaches, which might lead to bias. In this paper, we compare a multi-objective approach that optimizes two objectives and a single-objective approach that directly optimizes a trade-off of the two objectives, in order to further investigate the comparison of multi-objective and single-objective approaches. The conclusion will help to appropriately choose multi-objective or single-objective learning approaches for defect prediction.},
booktitle = {Proceedings of the 2018 2nd International Conference on Management Engineering, Software Engineering and Service Sciences},
pages = {122–127},
numpages = {6},
keywords = {software defect prediction, single-objective learning, effectiveness, cost, Multi-objective learning},
location = {Wuhan, China},
series = {ICMSS 2018}
}

@inproceedings{10.1145/3439961.3439979,
author = {Santos, Geanderson and Figueiredo, Eduardo and Veloso, Adriano and Viggiato, Markos and Ziviani, Nivio},
title = {Predicting Software Defects with Explainable Machine Learning},
year = {2021},
isbn = {9781450389235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439961.3439979},
doi = {10.1145/3439961.3439979},
abstract = {Most software systems must evolve to cope with stakeholders’ requirements and fix existing defects. Hence, software defect prediction represents an area of interest in both academia and the software industry. As a result, predicting software defects can help the development team to maintain substantial levels of software quality. For this reason, machine learning models have increased in popularity for software defect prediction and have demonstrated effectiveness in many scenarios. In this paper, we evaluate a machine learning approach for selecting features to predict software module defects. We use a tree boosting algorithm that receives as input a training set comprising records of software features encoding characteristics of each module and outputs whether the corresponding module is defective prone. For nine projects within the widely known NASA data program, we build prediction models from a set of easy-to-compute module features. We then sample this sizable model space by randomly selecting software features to compose each model. This significant number of models allows us to structure our work along model understandability and predictive accuracy. We argue that explaining model predictions is meaningful to provide information to developers on features related to each module defective-prone. We show that (i) features that contribute most to finding the best models may vary depending on the project, and (ii) effective models are highly understandable based on a survey with 40 developers.},
booktitle = {Proceedings of the XIX Brazilian Symposium on Software Quality},
articleno = {18},
numpages = {10},
keywords = {software defects, explainable models, SHAP values, NASA datasets},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {SBQS '20}
}

@inproceedings{10.1145/2896387.2900324,
author = {Rahman, Md. Habibur and Sharmin, Sadia and Sarwar, Sheikh Muhammad and Shoyaib, Mohammad},
title = {Software Defect Prediction Using Feature Space Transformation},
year = {2016},
isbn = {9781450340632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896387.2900324},
doi = {10.1145/2896387.2900324},
abstract = {In software quality estimation research, software defect prediction is a key topic. A defect prediction model is generally constructed using a variety of software attributes and each attribute may have positive, negative or neutral effect on a specific model. Selection of an optimal set of attributes for model development remains a vital yet unexplored issue. In this paper, we have introduced a new feature space transformation process with a normalization technique to improve the defect prediction accuracy. We proposed a feature space transformation technique and classify the instances using Support Vector Machine (SVM) with its histogram intersection kernel. The proposed method is evaluated using the data sets from NASA metric data repository and its application demonstrates acceptable accuracy.},
booktitle = {Proceedings of the International Conference on Internet of Things and Cloud Computing},
articleno = {72},
numpages = {6},
keywords = {Software defect prediction, Feature space transformation, Attribute selection},
location = {Cambridge, United Kingdom},
series = {ICC '16}
}

@inproceedings{10.1145/3166094.3166114,
author = {Chistyakov, Alexander and Pripadchev, Artem and Radchenko, Irina},
title = {On development of a framework for massive source code analysis using static code analyzers},
year = {2017},
isbn = {9781450363969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3166094.3166114},
doi = {10.1145/3166094.3166114},
abstract = {Authors describe architecture and implementation of an automated source code analyzing system which uses pluggable static code analyzers. The paper presents a module for gathering and analyzing the source code massively in a detailed manner. Authors also compare existing static code analyzers for Python programming language. A common format of storing results of code analysis for subsequent processing is introduced. Also, authors discuss methods of statistical processing and visualizing of raw analysis data.},
booktitle = {Proceedings of the 13th Central &amp; Eastern European Software Engineering Conference in Russia},
articleno = {20},
numpages = {3},
keywords = {static analyzers, open source, code analysis},
location = {St. Petersburg, Russia},
series = {CEE-SECR '17}
}

@inproceedings{10.1145/3318299.3318345,
author = {Li, ZhanJun and Shao, Yan},
title = {A Survey of Feature Selection for Vulnerability Prediction Using Feature-based Machine Learning},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318345},
doi = {10.1145/3318299.3318345},
abstract = {This paper summarized the basic process of software vulnerability prediction using feature-based machine learning for the first time. In addition to sorting out the related types and basis of vulnerability features definition, the advantages and disadvantages of different methods are compared. Finally, this paper analyzed the difficulties and challenges in this research field, and put forward some suggestions for future work.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {36–42},
numpages = {7},
keywords = {machine learning, feature, Software vulnerability prediction},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@inproceedings{10.5555/3432601.3432618,
author = {Grigoriou, Marios-Stavros and Kontogiannis, Kostas and Giammaria, Alberto and Brealey, Chris},
title = {Report on evaluation experiments using different machine learning techniques for defect prediction},
year = {2020},
publisher = {IBM Corp.},
address = {USA},
abstract = {With the emergence of AI, it is of no surprise that the application of Machine Learning techniques has attracted the attention of numerous software maintenance groups around the world. For defect proneness classification in particular, the use of Machine Learning classifiers has been touted as a promising approach. As a consequence, a large volume of research works has been published in the related research literature, utilizing either proprietary data sets or the PROMISE data repository which, for the purposes of this study, focuses only on the use of source code metrics as defect prediction training features. It has been argued though by several researchers, that process metrics may provide a better option as training features than source code metrics. For this paper, we have conducted a detailed extraction of GitHub process metrics from 148 open source systems, and we report on the findings of experiments conducted by using different Machine Learning classification algorithms for defect proneness classification. The main purpose of the paper is not to propose yet another Machine Learning technique for defect proneness classification, but to present to the community a very large data set using process metrics as opposed to source code metrics, and draw some initial interesting conclusions from this statistically significant data set.},
booktitle = {Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering},
pages = {123–132},
numpages = {10},
location = {Toronto, Ontario, Canada},
series = {CASCON '20}
}

@inproceedings{10.1145/3555776.3577809,
author = {Rebro, Dominik Arne and Chren, Stanislav and Rossi, Bruno},
title = {Source Code Metrics for Software Defects Prediction},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577809},
doi = {10.1145/3555776.3577809},
abstract = {In current research, there are contrasting results about the applicability of software source code metrics as features for defect prediction models. The goal of the paper is to evaluate the adoption of software metrics in models for software defect prediction, identifying the impact of individual source code metrics. With an empirical study on 275 release versions of 39 Java projects mined from GitHub, we compute 12 software metrics and collect software defect information. We train and compare three defect classification models. The results across all projects indicate that Decision Tree (DT) and Random Forest (RF) classifiers show the best results. Among the highest-performing individual metrics are NOC, NPA, DIT, and LCOM5. While other metrics, such as CBO, do not bring significant improvements to the models.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1469–1472},
numpages = {4},
keywords = {software quality, mining software repositories, software metrics, software defect prediction},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/3410352.3410747,
author = {Almaghairbe, Rafig and Roper, Marc and Almabruk, Tahani},
title = {Machine Learning Techniques for Automated Software Fault Detection via Dynamic Execution Data: Empirical Evaluation Study},
year = {2020},
isbn = {9781450377362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410352.3410747},
doi = {10.1145/3410352.3410747},
abstract = {The biggest obstacle of automated software testing is the construction of test oracles. Today, it is possible to generate enormous amount of test cases for an arbitrary system that reach a remarkably high level of coverage, but the effectiveness of test cases is limited by the availability of test oracles that can distinguish failing executions. Previous work by the authors has explored the use of unsupervised and semi-supervised learning techniques to develop test oracles so that the correctness of software outputs and behaviours on new test cases can be predicated [1], [2], [10], and experimental results demonstrate the promise of this approach. In this paper, we present an evaluation study for test oracles based on machine-learning approaches via dynamic execution data (firstly, input/output pairs and secondly, amalgamations of input/output pairs and execution traces) by comparing their effectiveness with existing techniques from the specification mining domain (the data invariant detector Daikon [5]). The two approaches are evaluated on a range of mid-sized systems and compared in terms of their fault detection ability and false positive rate. The empirical study also discuss the major limitations and the most important properties related to the application of machine learning techniques as test oracles in practice. The study also gives a road map for further research direction in order to tackle some of discussed limitations such as accuracy and scalability. The results show that in most cases semi-supervised learning techniques performed far better as an automated test classifier than Daikon (especially in the case that input/output pairs were augmented with their execution traces). However, there is one system for which our strategy struggles and Daikon performed far better. Furthermore, unsupervised learning techniques performed on a par when compared with Daikon in several cases particularly when input/output pairs were used together with execution traces.},
booktitle = {Proceedings of the 6th International Conference on Engineering &amp; MIS 2020},
articleno = {15},
numpages = {12},
keywords = {Automated Testing Oracles, Empirical Study, Machine Learning Techniques, Specification Mining},
location = {Almaty, Kazakhstan},
series = {ICEMIS'20}
}

@inproceedings{10.5555/3507788.3507810,
author = {Korlepara, Piyush and Grigoriou, Marios and Kontogiannis, Kostas and Brealey, Chris and Giammaria, Alberto},
title = {Combining domain expert knowledge and machine learning for the identification of error prone files},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {Identifying as early as possible fault prone modules in order to facilitate continuous delivery in large software systems, has been an area where significant attention has been paid over the past few years. Recent efforts consider source code metrics and process metrics for training machine learning models to predict whether a software source code file is fault prone or not. In such prediction frameworks the accuracy of the trained model relies heavily on the features selected and the profiles of the metrics used for training the model which are unique to each system. Furthermore, these models act as black-boxes, where the end-user does not know how a specific prediction was reached. In this paper, we propose an approach which allows for domain expert knowledge to be combined with machine learning in order to yield fault-proneness prediction models that both exhibit high levels of recall and at the same time are able to provide explanations to the developers as to how and why these predictions were reached. For this paper we apply two rule-based inferencing techniques namely, Fuzzy reasoning, and Markov Logic Networks. The main contribution of this work is that it allows for expert developers to identify in the form of if-then rules domain logic that pertains to the fault-proneness of a source code file in the specific system being analysed. Results obtained from 19 open source systens indicate that MLNs perform better than Fuzzy Logic models and that project-customized rules achieve better results than generic rules. Furthermore, results indicate that its possible to compile a common set of rules that yields consistently acceptable results across different projects.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {153–162},
numpages = {10},
keywords = {software repositories, process metrics, fault-proneness prediction, continuous software engineering},
location = {Toronto, Canada},
series = {CASCON '21}
}

@inproceedings{10.1109/MSR.2019.00017,
author = {Dam, Hoa Khanh and Pham, Trang and Ng, Shien Wee and Tran, Truyen and Grundy, John and Ghose, Aditya and Kim, Taeksu and Kim, Chul-Joo},
title = {Lessons learned from using a deep tree-based model for software defect prediction in practice},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00017},
doi = {10.1109/MSR.2019.00017},
abstract = {Defects are common in software systems and cause many problems for software users. Different methods have been developed to make early prediction about the most likely defective modules in large codebases. Most focus on designing features (e.g. complexity metrics) that correlate with potentially defective code. Those approaches however do not sufficiently capture the syntax and multiple levels of semantics of source code, a potentially important capability for building accurate prediction models. In this paper, we report on our experience of deploying a new deep learning tree-based defect prediction model in practice. This model is built upon the tree-structured Long Short Term Memory network which directly matches with the Abstract Syntax Tree representation of source code. We discuss a number of lessons learned from developing the model and evaluating it on two datasets, one from open source projects contributed by our industry partner Samsung and the other from the public PROMISE repository.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {46–57},
numpages = {12},
keywords = {defect prediction, deep learning},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3617572.3617879,
author = {Pei, Yulong and Alamir, Salwa and Dolga, Rares and Shah, Sameena},
title = {Code Revert Prediction with Graph Neural Networks: A Case Study at J.P. Morgan Chase},
year = {2023},
isbn = {9798400703775},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617572.3617879},
doi = {10.1145/3617572.3617879},
abstract = {Code revert prediction, a specialized form of software defect detection, aims to forecast or predict the likelihood of code changes being reverted or rolled back in software development. This task is very important in practice because by identifying code changes that are more prone to being reverted, developers and project managers can proactively take measures to prevent issues, improve code quality, and optimize development processes. However, compared to code defect detection, code revert prediction has been rarely studied in previous research. Additionally, many previous methods for code defect detection relied on independent features but ignored relationships between code scripts. Moreover, new challenges are introduced due to constraints in an industry setting such as company regulation, limited features and large-scale codebase. To overcome these limitations, this paper presents a systematic empirical study for code revert prediction that integrates the code import graph with code features. Different strategies to address anomalies and data imbalance have been implemented including graph neural networks with imbalance classification and anomaly detection. We conduct the experiments on real-world code commit data within J.P. Morgan Chase which is extremely imbalanced in order to make a comprehensive comparison of these different approaches for the code revert prediction problem.},
booktitle = {Proceedings of the 1st International Workshop on Software Defect Datasets},
pages = {1–5},
numpages = {5},
keywords = {imbalanced classification, graph neural networks, anomaly detection, Code revert prediction},
location = {San Francisco, CA, USA},
series = {SDD 2023}
}

@inproceedings{10.1145/2351676.2351734,
author = {Lu, Huihua and Cukic, Bojan and Culp, Mark},
title = {Software defect prediction using semi-supervised learning with dimension reduction},
year = {2012},
isbn = {9781450312042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351676.2351734},
doi = {10.1145/2351676.2351734},
abstract = {Accurate detection of fault prone modules offers the path to high quality software products while minimizing non essential assurance expenditures. This type of quality modeling requires the availability of software modules with known fault content developed in similar environment. Establishing whether a module contains a fault or not can be expensive. The basic idea behind semi-supervised learning is to learn from a small number of software modules with known fault content and supplement model training with modules for which the fault information is not available. In this study, we investigate the performance of semi-supervised learning for software fault prediction. A preprocessing strategy, multidimensional scaling, is embedded in the approach to reduce the dimensional complexity of software metrics. Our results show that the semi-supervised learning algorithm with dimension-reduction preforms significantly better than one of the best performing supervised learning algorithms, random forest, in situations when few modules with known fault content are available for training.},
booktitle = {Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering},
pages = {314–317},
numpages = {4},
keywords = {software metrics, semi-supervised learning, dimension reduction, Software fault prediction},
location = {Essen, Germany},
series = {ASE '12}
}

@article{10.1145/3092566,
author = {Ghaffarian, Seyed Mohammad and Shahriari, Hamid Reza},
title = {Software Vulnerability Analysis and Discovery Using Machine-Learning and Data-Mining Techniques: A Survey},
year = {2017},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3092566},
doi = {10.1145/3092566},
abstract = {Software security vulnerabilities are one of the critical issues in the realm of computer security. Due to their potential high severity impacts, many different approaches have been proposed in the past decades to mitigate the damages of software vulnerabilities. Machine-learning and data-mining techniques are also among the many approaches to address this issue. In this article, we provide an extensive review of the many different works in the field of software vulnerability analysis and discovery that utilize machine-learning and data-mining techniques. We review different categories of works in this domain, discuss both advantages and shortcomings, and point out challenges and some uncharted territories in the field.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {56},
numpages = {36},
keywords = {survey, software vulnerability discovery, software security, review, machine-learning, data-mining, Software vulnerability analysis}
}

@article{10.1145/3716857,
author = {Mangal, Akshat and Rathore, Santosh Singh},
title = {ATE-FS: An Average Treatment Effect-based Feature Selection Technique for Software Fault Prediction},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3716857},
doi = {10.1145/3716857},
abstract = {In software development, software fault prediction (SFP) models aim to identify code sections with a high likelihood of faults before the testing process. SFP models achieve this by analyzing data about the structural properties of the software’s previous versions. Consequently, the accuracy and interpretation of SFP models depend heavily on the chosen software metrics and how well they correlate with patterns of fault occurrence. Previous research has explored improving SFP model performance through feature selection (metric selection). Yet inconsistencies in conclusions arose due to the presence of inconsistent and correlated software metrics. Relying solely on correlations between metrics and faults makes it difficult for developers to take actionable steps, as the causal relationships remain unclear. To address this challenge, this work investigates the use of Causal Inference (CI) methods to understand the causal relationships between software project characteristics, development practices, and the fault-proneness of code sections. We propose a CI-based technique called Average Treatment Effect for Feature Selection (ATE-FS). This technique leverages the causal inference concept to quantify the cause-and-effect relationships between software metrics and fault-proneness. ATE-FS utilizes Average Treatment Effect (ATE) features to identify code metrics that are most suitable for building SFP models. These ATE features capture the causal impact of a metric on fault-proneness. Through an experimental analysis involving twenty-seven SFP datasets, we validate the performance of ATE-FS. We further compare its performance with other state-of-the-art feature selection techniques. The results demonstrate that ATE-FS achieves a significant performance for fault prediction. Additionally, ATE-FS improved consistency in feature selection across diverse SFP datasets.},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
keywords = {Software Fault Prediction, Causal Inference, Average Treatment Effect, Empirical Analysis}
}

@inproceedings{10.5555/2486788.2487006,
author = {Jonsson, Leif},
title = {Increasing anomaly handling efficiency in large organizations using applied machine learning},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Maintenance costs can be substantial for large organizations (several hundreds of programmers) with very large and complex software systems. By large we mean lines of code in the range of hundreds of thousands or millions. Our research objective is to improve the process of handling anomaly reports for large organizations. Specifically, we are addressing the problem of the manual, laborious and time consuming process of assigning anomaly reports to the correct design teams and the related issue of localizing faults in the system architecture. In large organizations, with complex systems, this is particularly problematic because the receiver of an anomaly report may not have detailed knowledge of the whole system. As a consequence, anomaly reports may be assigned to the wrong team in the organization, causing delays and unnecessary work. We have so far developed two machine learning prototypes to validate our approach. The latest, a re-implementation and extension, of the first is being evaluated on four large systems at Ericsson AB. Our main goal is to investigate how large software development organizations can significantly improve development efficiency by replacing manual anomaly report assignment and fault localization with machine learning techniques. Our approach focuses on training machine learning systems on anomaly report databases; this is in contrast to many other approaches that are based on test case execution combined with program sampling and/or source code analysis.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1361–1364},
numpages = {4},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/1370750.1370759,
author = {Ratzinger, Jacek and Sigmund, Thomas and Gall, Harald C.},
title = {On the relation of refactorings and software defect prediction},
year = {2008},
isbn = {9781605580241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370750.1370759},
doi = {10.1145/1370750.1370759},
abstract = {This paper analyzes the influence of evolution activities such as refactoring on software defects. In a case study of five open source projects we used attributes of software evolution to predict defects in time periods of six months. We use versioning and issue tracking systems to extract 110 data mining features, which are separated into refactoring and non-refactoring related features. These features are used as input into classification algorithms that create prediction models for software defects. We found out that refactoring related features as well as non-refactoring related features lead to high quality prediction models. Additionally, we discovered that refactorings and defects have an inverse correlation: The number of software defects decreases, if the number of refactorings increased in the preceding time period. As a result, refactoring should be a significant part of both bug fixes and other evolutionary changes to reduce software defects.},
booktitle = {Proceedings of the 2008 International Working Conference on Mining Software Repositories},
pages = {35–38},
numpages = {4},
keywords = {software evolution, software analysis, mining},
location = {Leipzig, Germany},
series = {MSR '08}
}

@inproceedings{10.1145/3548660.3561332,
author = {Cernau, Laura Diana and Dio\c{s}an, Laura Silvia and undefinederban, Camelia},
title = {A pedagogical approach in interleaving software quality concerns at an artificial intelligence course},
year = {2022},
isbn = {9781450394536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548660.3561332},
doi = {10.1145/3548660.3561332},
abstract = {The software engineering industry is an everchanging domain requiring professionals to have a good knowledge base and adaptability skills.Artificial Intelligence (AI) has achieved substantial success in enhancing program analysis techniques and applications, including bug prediction. It is a promising direction by applying advanced Machine Learning techniques into suitable software engineering tasks.  

The main goal of this paper is to propose a pedagogical interdisciplinary approach that pave the path for developing an e-learning platform serving to check the quality of the source code that students wrote by means of Artificial Intelligence techniques. By putting into practice this proposal, we are planning to show the students how to combine concepts learned from two different courses. The first step of this approach would be part of the Advanced Programming Methods, a Software Engineering related course, where students learn about the importance of writing good quality code and use software metrics as a mean of software quality assessment. Then, the following steps will be integrated into the Artificial Intelligence course, where students learn about different Machine Learning algorithms and how to apply them to solve practical problems. Thus, as an applicability in this respect, students use the metric values calculated for their projects developed at Advanced Programming Methods course as lab assignments and also to train (at Artificial Intelligence class) a bug detection model able to estimate the quality of new codebases.  

The proposed approach is helpful for both students and teachers. On one side, it helps the students understand the importance of writing clean, high-quality code. And on the other side, it helps teachers in their evaluation process by giving them time to focus on different aspects of homework than the code quality.},
booktitle = {Proceedings of the 4th International Workshop on Education through Advanced Software Engineering and Artificial Intelligence},
pages = {18–24},
numpages = {7},
keywords = {software metrics, software engineering, code quality},
location = {Singapore, Singapore},
series = {EASEAI 2022}
}

@inproceedings{10.1145/3578527.3578530,
author = {Jain, Ridhi and Gervasoni, Nicole and Ndhlovu, Mthandazo and Rawat, Sanjay},
title = {A Code Centric Evaluation of C/C++ Vulnerability Datasets for Deep Learning Based Vulnerability Detection Techniques},
year = {2023},
isbn = {9798400700644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578527.3578530},
doi = {10.1145/3578527.3578530},
abstract = {Recent years have witnessed tremendous progress in NLP-based code comprehension via deep neural networks (DNN) learning, especially Large Language Models (LLMs). While the original application of LLMs is focused on code generation, there have been attempts to extend the application to more specialized tasks, like code similarity, author attribution, code repairs, and so on. As data plays an important role in the success of any machine learning approach, researchers have also proposed several benchmarks which are coupled with a specific task at hand. It is well known in the machine learning (ML) community that the presence of biases in the dataset affects the quality of the ML algorithm in a real-world scenario. This paper evaluates several existing datasets from DNN’s application perspective. We specifically focus on training datasets of C/C++ language code. Our choice of language stems from the fact that while LLM-based techniques have been applied and evaluated on programming languages like Python, JavaScript, and Ruby, there is not much LLM research for C/C++. As a result, datasets generated synthetically or from real-world codes are in individual research work. Consequently, in the absence of a uniform dataset, such works are hard to compare with each other. In this work, we aim to achieve two main objectives– 1. propose code-centric features that are relevant to security program analysis tasks like vulnerability detection; 2. a thorough (qualitative and quantitative) examination of the existing code datasets that demonstrate the main characteristics of the individual datasets to have a clear comparison. Our evaluation finds exciting facts about existing datasets highlighting gaps that need to be addressed.},
booktitle = {Proceedings of the 16th Innovations in Software Engineering Conference},
articleno = {6},
numpages = {10},
keywords = {software vulnerability, software metrics, program graphs, datasets},
location = {Allahabad, India},
series = {ISEC '23}
}

@inproceedings{10.1145/1868328.1868350,
author = {Zhang, Hongyu and Nelson, Adam and Menzies, Tim},
title = {On the value of learning from defect dense components for software defect prediction},
year = {2010},
isbn = {9781450304047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868328.1868350},
doi = {10.1145/1868328.1868350},
abstract = {BACKGROUND: Defect predictors learned from static code measures can isolate code modules with a higher than usual probability of defects.AIMS: To improve those learners by focusing on the defect-rich portions of the training sets.METHOD: Defect data CM1, KC1, MC1, PC1, PC3 was separated into components. A subset of the projects (selected at random) were set aside for testing. Training sets were generated for a NaiveBayes classifier in two ways. In sample the dense treatment, the components with higher than the median number of defective modules were used for training. In the standard treatment, modules from any component were used for training. Both samples were run against the test set and evaluated using recall, probability of false alarm, and precision. In addition, under sampling and over sampling was performed on the defect data. Each method was repeated in a 10-by-10 cross-validation experiment.RESULTS: Prediction models learned from defect dense components out-performed standard method, under sampling, as well as over sampling. In statistical rankings based on recall, probability of false alarm, and precision, models learned from dense components won 4--5 times more often than any other method, and also lost the least amount of times.CONCLUSIONS: Given training data where most of the defects exist in small numbers of components, better defect predictors can be trained from the defect dense components.},
booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
articleno = {14},
numpages = {9},
keywords = {ceiling effect, defect dense components, defect prediction, sampling},
location = {Timi\c{s}oara, Romania},
series = {PROMISE '10}
}

@inproceedings{10.1145/3555228.3555269,
author = {Santos, Geanderson and Veloso, Adriano and Figueiredo, Eduardo},
title = {Understanding Thresholds of Software Features for Defect Prediction},
year = {2022},
isbn = {9781450397353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555228.3555269},
doi = {10.1145/3555228.3555269},
abstract = {Software defect prediction is a subject of study involving the interplay of the software engineering and machine learning areas. The current literature proposed numerous machine learning models to predict software defects from software data, such as commits and code metrics. However, existing machine learning models are more valuable when we can understand the prediction. Otherwise, software developers cannot reason why a machine learning model made such predictions, generating many questions about the model’s applicability in software projects. As explainable machine learning models for the defect prediction problem remain a recent research topic, it leaves room for exploration. In this paper, we propose a preliminary analysis of an extensive dataset to predict software defects. The dataset includes 47,618 classes from 53 open-source projects and covers 66 software features related to numerous features of the code. Therefore, we offer contributions on explaining how each selected software feature favors the prediction of software defects in Java projects. Our initial results suggest that developers should keep the values of some specific software features small to avoid software defects. We hope our approach can guide more discussions about explainable machine learning for defect prediction and its impact on software development.},
booktitle = {Proceedings of the XXXVI Brazilian Symposium on Software Engineering},
pages = {305–310},
numpages = {6},
keywords = {software features for defect prediction, explainable machine learning, defect prediction},
location = {Virtual Event, Brazil},
series = {SBES '22}
}

@inproceedings{10.1145/3616901.3616929,
author = {Yuan, Yuan},
title = {Software Technology Project Defect Prediction Based on AI},
year = {2024},
isbn = {9798400707544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616901.3616929},
doi = {10.1145/3616901.3616929},
abstract = {Abstract—In the rapidly developing information age, software development has become an important component of modern society. However, due to the complexity of the development process, project defects would inevitably occur in the software development process, leading to the extension of the software development cycle and the increase of costs. Therefore, this article explores and studies the issue of defect prediction in software technology projects based on artificial intelligence (AI). By analyzing and mining relevant data on project defects, a defect prediction model based on machine learning was established, and the effectiveness of the model was evaluated and analyzed. The results show that the average defect detection rate of strategy 3 is 0.88. The AI based software technology project defect prediction method proposed in this article can effectively improve the quality and efficiency of software development.},
booktitle = {Proceedings of the 2023 International Conference on Frontiers of Artificial Intelligence and Machine Learning},
pages = {126–130},
numpages = {5},
keywords = {Keywords: Artificial Intelligence, Prediction Systems, Project Defects, Software Technology},
location = {Beijing, China},
series = {FAIML '23}
}

@article{10.1145/3708532,
author = {H\"{a}m\"{a}l\"{a}inen, Joonas and Das, Teerath and Mikkonen, Tommi},
title = {A Systematic Literature Review of Multi-Label Learning in Software Engineering},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708532},
doi = {10.1145/3708532},
abstract = {In this paper, we provide the first systematic literature review of the intersection of two research areas, Multi-Label Learning (MLL) and Software Engineering (SE). We refer to this intersection as MLL4SE. In recent years, MLL problems have increased in many applications and research areas because real-world datasets often have a multi-label nature. For multi-label data, simplifying the assumption of traditional classification approaches that an instance can only be associated with one class only leads to worse accuracy. Thus, a better match of methods and assumptions about the data is required. We identified 50 primary studies in our systematic literature review in the MLL4SE domain. Based on this review, we identified six main SE application domains where MLL has been applied. These domains include Software Requirement Engineering, Issue Tracking and Management, Community and Knowledge Management, API Usage and Management, Code Quality and Maintenance, and Mobile Application Development. We summarized the methods used and the data nature of the MLL4SE applications. Moreover, we separately provide taxonomies of future work directions from machine learning and software engineering perspectives. In general, we highlight current trends, research gaps, and shortcomings.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {Machine Learning, Multi-Label Learning, Software Engineering, Systematic Literature Review, Software Development Life Cycle (SDLC) Activities}
}

@inproceedings{10.1145/1985374.1985386,
author = {M\i{}s\i{}rl\i{}, Ayse Tosun and \c{C}a\u{g}layan, Bora and Miranskyy, Andriy V. and Bener, Ay\c{s}e and Ruffolo, Nuzio},
title = {Different strokes for different folks: a case study on software metrics for different defect categories},
year = {2011},
isbn = {9781450305938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985374.1985386},
doi = {10.1145/1985374.1985386},
abstract = {Defect prediction has been evolved with variety of metric sets, and defect types. Researchers found code, churn, and network metrics as significant indicators of defects. However, all metric sets may not be informative for all defect categories such that only one metric type may represent majority of a defect category. Our previous study showed that defect category sensitive prediction models are more successful than general models, since each category has different characteristics in terms of metrics. We extend our previous work, and propose specialized prediction models using churn, code, and network metrics with respect to three defect categories. Results show that churn metrics are the best for predicting all defects. The strength of correlation for code and network metrics varies with defect category: Network metrics have higher correlations than code metrics for defects reported during functional testing and in the field, and vice versa for defects reported during system testing.},
booktitle = {Proceedings of the 2nd International Workshop on Emerging Trends in Software Metrics},
pages = {45–51},
numpages = {7},
keywords = {static code metrics, software defect prediction, network metrics, churn metrics},
location = {Waikiki, Honolulu, HI, USA},
series = {WETSoM '11}
}

@article{10.1145/3597202,
author = {Suneja, Sahil and Zhuang, Yufan and Zheng, Yunhui and Laredo, Jim and Morari, Alessandro and Khurana, Udayan},
title = {Incorporating Signal Awareness in Source Code Modeling: An Application to Vulnerability Detection},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3597202},
doi = {10.1145/3597202},
abstract = {AI models of code have made significant progress over the past few years. However, many models are actually not learning task-relevant source code features. Instead, they often fit non-relevant but correlated data, leading to a lack of robustness and generalizability, and limiting the subsequent practical use of such models. In this work, we focus on improving the model quality through signal awareness, i.e., learning the relevant signals in the input for making predictions. We do so by leveraging the heterogeneity of code samples in terms of their signal-to-noise content. We perform an end-to-end exploration of model signal awareness, comprising: (i) uncovering the reliance of AI models of code on task-irrelevant signals, via prediction-preserving input minimization; (ii) improving models’ signal awareness by incorporating the notion of code complexity during model training, via curriculum learning; (iii) improving models’ signal awareness by generating simplified signal-preserving programs and augmenting them to the training dataset; and (iv) presenting a novel interpretation of the model learning behavior from the perspective of the dataset, using its code complexity distribution. We propose a new metric to measure model signal awareness, Signal-aware Recall, which captures how much of the model’s performance is attributable to task-relevant signal learning. Using a software vulnerability detection use-case, our model probing approach uncovers a significant lack of signal awareness in the models, across three different neural network architectures and three datasets. Signal-aware Recall is observed to be in the sub-50s for models with traditional Recall in the high 90s, suggesting that the models are presumably picking up a lot of noise or dataset nuances while learning their logic. With our code-complexity-aware model learning enhancement techniques, we are able to assist the models toward more task-relevant learning, recording up-to 4.8\texttimes{} improvement in model signal awareness. Finally, we employ our model learning introspection approach to uncover the aspects of source code where the model is facing difficulty, and we analyze how our learning enhancement techniques alleviate it.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {145},
numpages = {40},
keywords = {explainability, data augmentation, curriculum learning, signal awareness, reliability, neural networks, Machine learning}
}

@inproceedings{10.1145/3472674.3473981,
author = {Pontillo, Valeria and Palomba, Fabio and Ferrucci, Filomena},
title = {Toward static test flakiness prediction: a feasibility study},
year = {2021},
isbn = {9781450386258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472674.3473981},
doi = {10.1145/3472674.3473981},
abstract = {Flaky tests are tests that exhibit both a passing and failing behavior when run against the same code. While the research community has attempted to define automated approaches for detecting and addressing test flakiness, most of them suffer from scalability issues and uncertainty as they require test cases to be run multiple times. This limitation has been recently targeted by means of machine learning solutions that could predict the flakiness of tests using a set of both static and dynamic metrics that would avoid the re-execution of tests. Recognizing the effort spent so far, this paper poses the first steps toward an orthogonal view of the problem, namely the classification of flaky tests using only statically computable software metrics. We propose a feasibility study on 72 projects of the iDFlakies dataset, and investigate the differences between flaky and non-flaky tests in terms of 25 test and production code metrics and smells. First, we statistically assess those differences. Second, we build a logistic regression model to verify the extent to which the differences observed are still significant when the metrics are considered together. The results show a relation between test flakiness and a number of test and production code factors, indicating the possibility to build classification approaches that exploit those factors to predict test flakiness.},
booktitle = {Proceedings of the 5th International Workshop on Machine Learning Techniques for Software Quality Evolution},
pages = {19–24},
numpages = {6},
keywords = {Software Quality Evaluation, Flaky Tests, Empirical Studies},
location = {Athens, Greece},
series = {MaLTESQuE 2021}
}

@inproceedings{10.1145/2856636.2856637,
author = {Lal, Sangeeta and Sureka, Ashish},
title = {LogOpt: Static Feature Extraction from Source Code for Automated Catch Block Logging Prediction},
year = {2016},
isbn = {9781450340182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2856636.2856637},
doi = {10.1145/2856636.2856637},
abstract = {Software logging is an important software development practice which is used to trace important software execution points. This execution information can provide important insight to developer while software debugging. Inspite of many benefits logging is often done in an ad-hoc manner based only on knowledge and experience of software developer because of lack of formal guidelines and training required for making strategic logging decision. It is known that appropriate logging is beneficial for developers but inappropriate logging can have adverse effect on the system. Excessive logging can not only cause performance and cost overhead, it can also lessen the benefit of logging by producing tons of useless logs. Sparse logging can make logging ineffective by leaving out important information. In order to lessen the load of software developers and to improve the quality of software logging, in this work we propose 'LogOpt' tool to help developers in making informed logging decision. LogOpt uses static features from source code to make catch block logging decision. LogOpt is a machine learning based framework which learns the characteristics of logged and unlogged training instance to make informed logging decision. We manually analyze snippets of logged and unlogged source code and extracted 46 distinguishing features important in making logging decision. We evaluated LogOpt on two large open source projects Apache Tomcat and CloudStack (nearly 1.41M LOC). Results show that LogOpt is effective for automated logging task.},
booktitle = {Proceedings of the 9th India Software Engineering Conference},
pages = {151–155},
numpages = {5},
keywords = {Tracing, Source Code Analysis, Machine Learning, Logging, Debugging},
location = {Goa, India},
series = {ISEC '16}
}

@inproceedings{10.1145/3474624.3477070,
author = {Martins, Luana and Bezerra, Carla and Costa, Heitor and Machado, Ivan},
title = {Smart prediction for refactorings in the software test code},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3477070},
doi = {10.1145/3474624.3477070},
abstract = {Test smells are bad practices to either design or implement a test code. Their presence may reduce the test code quality, harming the software testing activities, primarily from a maintenance perspective. Therefore, defining strategies and tools to handle test smells and improve the test code quality is necessary. State-of-the-art strategies encompass automated support mainly based on hard thresholds of rules, static and dynamic metrics to identify the test smells. Such thresholds are subjective to interpretation and may not consider the complexity of the software projects. Moreover, they are limited as they do not automate test refactoring but only count on developers’ expertise and intuition. In this context, a technique that uses historical implicit or tacit data to generate knowledge could assist the identification and refactoring of test smells. This study aims to establish a novel approach based on machine learning techniques to suggest developers refactoring strategies for test smells. As an expected result, we could understand the applicability of the machine learning techniques to handle test smells and a framework proposal that helps developers in decision-making regarding the refactoring of test smells.},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {115–120},
numpages = {6},
keywords = {Test Smells, Software Quality, Machine Learning},
location = {Joinville, Brazil},
series = {SBES '21}
}

@article{10.1145/3384517,
author = {Kapur, Ritu and Sodhi, Balwinder},
title = {A Defect Estimator for Source Code: Linking Defect Reports with Programming Constructs Usage Metrics},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3384517},
doi = {10.1145/3384517},
abstract = {An important issue faced during software development is to identify defects and the properties of those defects, if found, in a given source file. Determining defectiveness of source code assumes significance due to its implications on software development and maintenance cost.We present a novel system to estimate the presence of defects in source code and detect attributes of the possible defects, such as the severity of defects. The salient elements of our system are: (i) a dataset of newly introduced source code metrics, called PROgramming CONstruct (PROCON) metrics, and (ii) a novel Machine-Learning (ML)-based system, called Defect Estimator for Source Code (DESCo), that makes use of PROCON dataset for predicting defectiveness in a given scenario. The dataset was created by processing 30,400+ source files written in four popular programming languages, viz., C, C++, Java, and Python.The results of our experiments show that DESCo system outperforms one of the state-of-the-art methods with an improvement of 44.9%. To verify the correctness of our system, we compared the performance of 12 different ML algorithms with 50+ different combinations of their key parameters. Our system achieves the best results with SVM technique with a mean accuracy measure of 80.8%.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {12},
numpages = {35},
keywords = {source code mining, software metrics, software faults and failures, software defect prediction, automated software engineering, Maintaining software, AI in software engineering}
}

@inproceedings{10.1145/3178212.3178221,
author = {Rizwan, Syed and Tiantian, Wang and Xiaohong, Su and Salahuddin},
title = {Empirical Study on Software Bug Prediction},
year = {2017},
isbn = {9781450354882},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178212.3178221},
doi = {10.1145/3178212.3178221},
abstract = {Software defect prediction is a vital research direction in software engineering field. Software defect prediction predicts whether software errors are present in the software by using machine learning analysis on software metrics. It can help software developers to improve the quality of the software. Software defect prediction is usually a binary classification problem, which relies on software metrics and the use of classifiers. There have been many research efforts to improve accuracy in software defect prediction using a variety of classifiers and data preprocessing techniques. However, the "classic classifier validity" and "data preprocessing techniques can enhance the functionality of software defect prediction" has not yet been answered explicitly. Therefore, it is necessary to conduct an empirical analysis to compare these studies. In software defect prediction, the category of interest is a defective module, and the number of defective modules is much less than that of a non-defective module in data. This leads to a category of imbalance problem that reduces the accuracy of the prediction. Therefore, the problem of imbalance is a key problem that needs to be solved in software defect prediction. In this paper, we proposed an experimental model and used the NASA MDP data set to analyze the software defect prediction. Five research questions were defined and analyzed experimentally. In addition to experimental analysis, this paper focuses on the improvement of SMOTE. SMOTE ASMO algorithm has been proposed to overcome the shortcomings of SMOTE.},
booktitle = {Proceedings of the 2017 International Conference on Software and E-Business},
pages = {55–59},
numpages = {5},
keywords = {SMOTE, Defect prediction, Data preprocessing, Classification},
location = {Hong Kong, Hong Kong},
series = {ICSEB '17}
}

@article{10.1145/3643729,
author = {Song, Zirui and Chen, Jiongyi and Zhang, Kehuan},
title = {Bin2Summary: Beyond Function Name Prediction in Stripped Binaries with Functionality-Specific Code Embeddings},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643729},
doi = {10.1145/3643729},
abstract = {Nowadays, closed-source software only with stripped binaries still dominates the ecosystem, which brings obstacles to understanding the functionalities of the software and further conducting the security analysis. With such an urgent need, research has traditionally focused on predicting function names, which can only provide fragmented and abbreviated information about functionality.  To advance the state-of-the-art, this paper presents Bin2Summary to automatically summarize the functionality of the function in stripped binaries with natural language sentences. Specifically, the proposed framework includes a functionality-specific code embedding module to facilitate fine-grained similarity detection and an attention-based seq2seq model to generate summaries in natural language. Based on 16 widely-used projects (e.g., Coreutils), we have evaluated Bin2Summary with 38,167 functions, which are filtered from 162,406 functions, and all of them have a high-quality comment. Bin2Summary achieves 0.728 in precision and 0.729 in recall on our datasets, and the functionality-specific embedding module can improve the existing assembly language model by up to 109.5% and 109.9% in precision and recall. Meanwhile, the experiments demonstrated that Bin2Summary has outstanding transferability in analyzing the cross-architecture (i.e., in x64 and x86) and cross-environment (i.e., in Cygwin and MSYS2) binaries. Finally, the case study illustrates how Bin2Summary outperforms the existing works in providing functionality summaries with abundant semantics beyond function names.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {3},
numpages = {23},
keywords = {Code Summarization, Machine Learning for Program Analysis, Reverse Engineering, Transfer Learning}
}

@inproceedings{10.1145/3475716.3475781,
author = {Croft, Roland and Newlands, Dominic and Chen, Ziyu and Babar, M. Ali},
title = {An Empirical Study of Rule-Based and Learning-Based Approaches for Static Application Security Testing},
year = {2021},
isbn = {9781450386654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475716.3475781},
doi = {10.1145/3475716.3475781},
abstract = {Background: Static Application Security Testing (SAST) tools purport to assist developers in detecting security issues in source code. These tools typically use rule-based approaches to scan source code for security vulnerabilities. However, due to the significant shortcomings of these tools (i.e., high false positive rates), learning-based approaches for Software Vulnerability Prediction (SVP) are becoming a popular approach. Aims: Despite the similar objectives of these two approaches, their comparative value is unexplored. We provide an empirical analysis of SAST tools and SVP models, to identify their relative capabilities for source code security analysis. Method: We evaluate the detection and assessment performance of several common SAST tools and SVP models on a variety of vulnerability datasets. We further assess the viability and potential benefits of combining the two approaches. Results: SAST tools and SVP models provide similar detection capabilities, but SVP models exhibit better overall performance for both detection and assessment. Unification of the two approaches is difficult due to lacking synergies. Conclusions: Our study generates 12 main findings which provide insights into the capabilities and synergy of these two approaches. Through these observations we provide recommendations for use and improvement.},
booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {8},
numpages = {12},
keywords = {Static Application Security Testing, Security, Machine Learning},
location = {Bari, Italy},
series = {ESEM '21}
}

@inproceedings{10.1145/3611643.3613880,
author = {Feng, Sidong and Lu, Haochuan and Xiong, Ting and Deng, Yuetang and Chen, Chunyang},
title = {Towards Efficient Record and Replay: A Case Study in WeChat},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613880},
doi = {10.1145/3611643.3613880},
abstract = {WeChat, a widely-used messenger app boasting over 1 billion monthly active users, requires effective app quality assurance for its complex features. Record-and-replay tools are crucial in achieving this goal. Despite the extensive development of these tools, the impact of waiting time between replay events has been largely overlooked. On one hand, a long waiting time for executing replay events on fully-rendered GUIs slows down the process. On the other hand, a short waiting time can lead to events executing on partially-rendered GUIs, negatively affecting replay effectiveness. An optimal waiting time should strike a balance between effectiveness and efficiency. We introduce WeReplay, a lightweight image-based approach that dynamically adjusts inter-event time based on the GUI rendering state. Given the real-time streaming on the GUI, WeReplay employs a deep learning model to infer the rendering state and synchronize with the replaying tool, scheduling the next event when the GUI is fully rendered. Our evaluation shows that our model achieves 92.1% precision and 93.3% recall in discerning GUI rendering states in the WeChat app. Through assessing the performance in replaying 23 common WeChat usage scenarios, WeReplay successfully replays all scenarios on the same and different devices more efficiently than the state-of-the-practice baselines.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1681–1692},
numpages = {12},
keywords = {Efficient record and replay, GUI rendering, Machine Learning},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3475716.3475790,
author = {Wang, Song and Wang, Junjie and Nam, Jaechang and Nagappan, Nachiappan},
title = {Continuous Software Bug Prediction},
year = {2021},
isbn = {9781450386654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475716.3475790},
doi = {10.1145/3475716.3475790},
abstract = {Background: Many software bug prediction models have been proposed and evaluated on a set of well-known benchmark datasets. We conducted pilot studies on the widely used benchmark datasets and observed common issues among them. Specifically, most of existing benchmark datasets consist of randomly selected historical versions of software projects, which poses non-trivial threats to the validity of existing bug prediction studies since the real-world software projects often evolve continuously. Yet how to conduct software bug prediction in the real-world continuous software development scenarios is not well studied.Aims: In this paper, to bridge the gap between current software bug prediction practice and real-world continuous software development, we propose new approaches to conduct bug prediction in real-world continuous software development regarding model building, updating, and evaluation.Method: For model building, we propose ConBuild, which leverages distributional characteristics of bug prediction data to guide the training version selection. For model updating, we propose ConUpdate, which leverages the evolution of distributional characteristics of bug prediction data between versions to guide the reuse or update of bug prediction models in continuous software development. For model evaluation, we propose ConEA, which leverages the evolution of buggy probability of files between versions to conduct effort-aware evaluation.Results: Experiments on 120 continuously release versions that span across six large-scale open-source software systems show the practical value of our approaches.Conclusions: This paper provides new insights and guidelines for conducting software bug prediction in the context of continuous software development.},
booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {14},
numpages = {12},
keywords = {software quality, software defect prediction, continuous software development, Empirical software engineering},
location = {Bari, Italy},
series = {ESEM '21}
}

@inproceedings{10.1145/2915970.2915979,
author = {Petri\'{c}, Jean},
title = {Using different characteristics of machine learners to identify different defect families},
year = {2016},
isbn = {9781450336918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2915970.2915979},
doi = {10.1145/2915970.2915979},
abstract = {Background: Software defect prediction has been an active area of research for the last few decades. Many models have been developed with aim to find locations in code likely to contain defects. As of yet, these prediction models are of limited use and rarely used in the software industry.Problem: Current modelling techniques are too coarse grained and fail in finding some defects. Most of the prediction models do not look for targeted defect characteristics, but rather treat them as a black box and homogeneous. No study has investigated in greater detail how well certain defect characteristics work with different prediction modelling techniques.Methodology: This PhD will address three major tasks. First, the relation among software defects, prediction models and static code metrics will be analysed. Second, the possibility of a mapping function between prediction models and defect characteristics shall be investigated. Third, an optimised ensemble model that searches for targeted defects will be developed.Contribution: A few contributions will yield from this work. Characteristics of defects will be identified, allowing other researchers to build on this work to produce more efficient prediction models in future. New modelling techniques that better suit state-of-the-art knowledge in defect prediction shall be designed. Such prediction models should be transformed in a tool that can be used by our industrial collaborator in the real industry environment.},
booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {5},
numpages = {4},
keywords = {software defect prediction, prediction modeling, machine learning},
location = {Limerick, Ireland},
series = {EASE '16}
}

@inproceedings{10.1145/3651640.3651644,
author = {Sch\"{u}tz, Martin and Pl\"{o}sch, Reinhold},
title = {A Practical Failure Prediction Model based on Code Smells and Software Development Metrics},
year = {2024},
isbn = {9798400708817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3651640.3651644},
doi = {10.1145/3651640.3651644},
abstract = {Making errors during software development is unavoidable. Developers inevitably make errors that take additional time to fix later. Consequently, efforts for bug fixing compete with implementing new features. Typically, the later bugs are found, the higher the cost for remediation. To address this concern, software testing should start as early as possible in software development lifecycle. For this purpose, static analysis is proposed, but typically shows too many findings and hence do not support development teams appropriately. So, it would be a benefit to premature detect those findings in static analysis that will result in failures to reduce subsequent efforts notably. The purpose of the paper is to analyze failure data from issue tracking systems that are correlated to findings from static analysis. Thereupon an artificial intelligence-based approach is used to train practicable models for business environment that enables effective prediction of software faults. The results from static analysis show that predefined complexity measures encompassed the most defects. While there are commonalities in relevant defect findings in static analysis reports, meaningful prediction models cannot be expected based solely on this data. In addition to the findings of the static analysis, metrics like code changes in a time period or number of authors involved in code changes were considered for building the prediction models. Two of the developed prediction models have a high accuracy and excellent utility rate. These resulting prediction models are currently used at Raiffeisen Software GmbH for a long-term study on failure prediction based on code smells.},
booktitle = {Proceedings of the 4th European Symposium on Software Engineering},
pages = {14–22},
numpages = {9},
keywords = {change metrics and failure prediction, failure prediction, machine learning for failure prediction, static analysis, technical debt},
location = {Napoli, Italy},
series = {ESSE '23}
}

@inproceedings{10.1145/3373477.3373486,
author = {Aggarwal, Simran},
title = {Software code analysis using ensemble learning techniques},
year = {2020},
isbn = {9781450372916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373477.3373486},
doi = {10.1145/3373477.3373486},
abstract = {Ensuing the advent of advancements in software systems, the probability of them containing high severity defects is exponentially on the rise. With each technological addition, the complexity of software is increasing. Reproduction and rectification of a defect requires time and effort. Current state of the art analysis tools cater to the investigation of static aspects of a production level code. However, it is imperative to assess the dynamic development process of a system so as to be able to timely detect erroneous components early on in the development life cycle of a software. A novel automated defect prediction feature enhancement is proposed that analyses the static structure of the current code and state of the software in past releases to extract relevant static and dynamic feature sets. Data generated is modelled for defect trends in the future release of the software by four ensemble classifiers. Results demonstrate the superiority of Voting algorithm for the problem of defect prediction.},
booktitle = {Proceedings of the 1st International Conference on Advanced Information Science and System},
articleno = {9},
numpages = {7},
keywords = {software quality, object-oriented metrics, machine learning, ensemble learning, empirical validation, defect prediction},
location = {Singapore, Singapore},
series = {AISS '19}
}

@inproceedings{10.1145/3643991.3644928,
author = {Shahini, Xhulja and Metzger, Andreas and Pohl, Klaus},
title = {An Empirical Study on Just-in-time Conformal Defect Prediction},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644928},
doi = {10.1145/3643991.3644928},
abstract = {Code changes can introduce defects that affect software quality and reliability. Just-in-time (JIT) defect prediction techniques provide feedback at check-in time on whether a code change is likely to contain defects. This immediate feedback allows practitioners to make timely decisions regarding potential defects. However, a prediction model may deliver false predictions, that may negatively affect practitioners' decisions. False positive predictions lead to unnecessarily spending resources on investigating clean code changes, while false negative predictions may result in overlooking defective changes. Knowing how uncertain a defect prediction is, would help practitioners to avoid wrong decisions. Previous research in defect prediction explored different approaches to quantify prediction uncertainty for supporting decision-making activities. However, these approaches only offer a heuristic quantification of uncertainty and do not provide guarantees.In this study, we use conformal prediction (CP) as a rigorous uncertainty quantification approach on top of JIT defect predictors. We assess how often CP can provide guarantees for JIT defect predictions. We also assess how many false JIT defect predictions CP can filter out. We experiment with two state-of-the-art JIT defect prediction techniques (DeepJIT and CC2Vec) and two widely used datasets (Qt and OpenStack).Our experiments show that CP can ensure correctness with a 95% probability, for only 27% (for DeepJIT) and 9% (for CC2Vec) of the JIT defect predictions. Additionally, our experiments indicate that CP might be a valuable technique for filtering out the false predictions of JIT defect predictors. CP can filter out up to 100% of false negative predictions and 90% of false positives generated by CC2Vec, and up to 86% of false negative predictions and 83% of false positives generated by DeepJIT.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {88–99},
numpages = {12},
keywords = {defect prediction, quality assurance, conformal prediction, machine learning, deep learning, correctness guarantees, uncertainty},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/2931037.2931039,
author = {Bowes, David and Hall, Tracy and Harman, Mark and Jia, Yue and Sarro, Federica and Wu, Fan},
title = {Mutation-aware fault prediction},
year = {2016},
isbn = {9781450343909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2931037.2931039},
doi = {10.1145/2931037.2931039},
abstract = {We introduce mutation-aware fault prediction, which leverages additional guidance from metrics constructed in terms of mutants and the test cases that cover and detect them. We report the results of 12 sets of experiments, applying 4 different predictive modelling techniques to 3 large real-world systems (both open and closed source). The results show that our proposal can significantly (p ≤ 0.05) improve fault prediction performance. Moreover, mutation-based metrics lie in the top 5% most frequently relied upon fault predictors in 10 of the 12 sets of experiments, and provide the majority of the top ten fault predictors in 9 of the 12 sets of experiments.},
booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
pages = {330–341},
numpages = {12},
keywords = {Software Metrics, Software Fault Prediction, Software Defect Prediction, Mutation Testing, Empirical Study},
location = {Saarbr\"{u}cken, Germany},
series = {ISSTA 2016}
}

@inproceedings{10.1145/3387904.3389273,
author = {Terragni, Valerio and Salza, Pasquale and Pezz\`{e}, Mauro},
title = {Measuring Software Testability Modulo Test Quality},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389273},
doi = {10.1145/3387904.3389273},
abstract = {Comprehending the degree to which software components support testing is important to accurately schedule testing activities, train developers, and plan effective refactoring actions. Software testability estimates such property by relating code characteristics to the test effort. The main studies of testability reported in the literature investigate the relation between class metrics and test effort in terms of the size and complexity of the associated test suites. They report a moderate correlation of some class metrics to test-effort metrics, but suffer from two main limitations: (i) the results hardly generalize due to the small empirical evidence (datasets with no more than eight software projects); and (ii) mostly ignore the quality of the tests. However, considering the quality of the tests is important. Indeed, a class may have a low test effort because the associated tests are of poor quality, and not because the class is easier to test. In this paper, we propose an approach to measure testability that normalizes the test effort with respect to the test quality, which we quantify in terms of code coverage and mutation score. We present the results of a set of experiments on a dataset of 9,861 Java classes, belonging to 1,186 open source projects, with around 1.5 million of lines of code overall. The results confirm that normalizing the test effort with respect to the test quality largely improves the correlation between class metrics and the test effort. Better correlations result in better prediction power and thus better prediction of the test effort.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {241–251},
numpages = {11},
keywords = {Test Quality, Test Effort, Software Testability, Software Metrics},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@inproceedings{10.1145/3643991.3644920,
author = {Zhao, Guoliang and Georgiou, Stefanos and Hassan, Safwat and Zou, Ying and Truong, Derek and Corbin, Toby},
title = {Enhancing Performance Bug Prediction Using Performance Code Metrics},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644920},
doi = {10.1145/3643991.3644920},
abstract = {Performance bugs are non-functional defects that can significantly reduce the performance of an application (e.g., software hanging or freezing) and lead to poor user experience. Prior studies found that each type of performance bugs follows a unique code-based performance anti-pattern and proposed different approaches to detect such anti-patterns by analyzing the source code of a program. However, each approach can only recognize one performance anti-pattern. Different approaches need to be applied separately to identify different performance anti-patterns. To predict a large variety of performance bug types using a unified approach, we propose an approach that predicts performance bugs by leveraging various historical data (e.g., source code and code change history). We collect performance bugs from 80 popular Java projects. Next, we propose performance code metrics to capture the code characteristics of performance bugs. We build performance bug predictors using machine learning models, such as Random Forest, eXtreme Gradient Boosting, and Linear Regressions. We observe that: (1) Random Forest and eXtreme Gradient Boosting are the best algorithms for predicting performance bugs at a file level with a median of 0.84 AUC, 0.21 PR-AUC, and 0.38 MCC; (2) The proposed performance code metrics have the most significant impact on the performance of our models compared to code and process metrics. In particular, the median AUC, PR-AUC, and MCC of the studied machine learning models drop by 7.7%, 25.4%, and 20.2% without using the proposed performance code metrics; and (3) Our approach can predict additional performance bugs that are not covered by the anti-patterns proposed in the prior studies.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {50–62},
numpages = {13},
keywords = {performance bugs, performance anti-patterns, performance code metrics, performance bug prediction},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@article{10.1145/3660809,
author = {Oueslati, Khouloud and Laberge, Gabriel and Lamothe, Maxime and Khomh, Foutse},
title = {Mining Action Rules for Defect Reduction Planning},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660809},
doi = {10.1145/3660809},
abstract = {Defect reduction planning plays a vital role in enhancing software quality and minimizing software maintenance costs. By training a black box machine learning model and “explaining” its predictions, explainable AI for software engineering aims to identify the code characteristics that impact maintenance risks. However, post-hoc explanations do not always faithfully reflect what the original model computes. In this paper, we introduce CounterACT, a Counterfactual ACTion rule mining approach that can generate defect reduction plans without black-box models. By leveraging action rules, CounterACT provides a course of action that can be considered as a counterfactual explanation for the class (e.g., buggy or not buggy) assigned to a piece of code. We compare the effectiveness of CounterACT with the original action rule mining algorithm and six established defect reduction approaches on 9 software projects. Our evaluation is based on (a) overlap scores between proposed code changes and actual developer modifications; (b) improvement scores in future releases; and (c) the precision, recall, and F1-score of the plans. Our results show that, compared to competing approaches, CounterACT’s explainable plans achieve higher overlap scores at the release level (median 95%) and commit level (median 85.97%), and they offer better trade-off between precision and recall (median F1-score 88.12%). Finally, we venture beyond planning and explore leveraging Large Language models (LLM) for generating code edits from our generated plans. Our results show that suggested LLM code edits supported by our plans are actionable and are more likely to pass relevant test cases than vanilla LLM code recommendations.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {102},
numpages = {23},
keywords = {Action rule mining, Counterfactual explanations, Defect reduction planning, Explainability, Software analytics}
}

@inproceedings{10.1145/3172871.3172872,
author = {Kumar, Lov and Sureka, Ashish},
title = {Feature Selection Techniques to Counter Class Imbalance Problem for Aging Related Bug Prediction: Aging Related Bug Prediction},
year = {2018},
isbn = {9781450363983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172871.3172872},
doi = {10.1145/3172871.3172872},
abstract = {Aging-Related Bugs (ARBs) occur in long running systems due to error conditions caused because of accumulation of problems such as memory leakage or unreleased files and locks. Aging-Related Bugs are hard to discover during software testing and also challenging to replicate. Automatic identification and prediction of aging related fault-prone files and classes in an object oriented system can help the software quality assurance team to optimize their testing efforts. In this paper, we present a study on the application of static source code metrics and machine learning techniques to predict aging related bugs. We conduct a series of experiments on publicly available dataset from two large open-source software systems: Linux and MySQL. Class imbalance and high dimensionality are the two main technical challenges in building effective predictors for aging related bugs.We investigate the application of five different feature selection techniques (OneR, Information Gain, Gain Ratio, RELEIF and Symmetric Uncertainty) for dimensionality reduction and five different strategies (Random Under-sampling, Random Oversampling, SMOTE, SMOTEBoost and RUSBoost) to counter the effect of class imbalance in our proposed machine learning based solution approach. Experimental results reveal that the random under-sampling approach performs best followed by RUSBoost in-terms of the mean AUC metric. Statistical significance test demonstrates that there is a significant difference between the performance of the various feature selection techniques. Experimental results shows that Gain Ratio and RELEIF performs best in comparison to other strategies to address the class imbalance problem. We infer from the statistical significance test that there is no difference between the performances of the five different learning algorithms.},
booktitle = {Proceedings of the 11th Innovations in Software Engineering Conference},
articleno = {2},
numpages = {11},
keywords = {Source Code Metrics, Software Maintenance, Predictive Modeling, Machine Learning, Imbalance Learning, Feature Selection Techniques, Empirical Software Engineering, Aging Related Bugs},
location = {Hyderabad, India},
series = {ISEC '18}
}

@inproceedings{10.1145/3568364.3568380,
author = {Matcha, Wyao and Toure, Fadel and Badri, Mourad and Badri, Linda},
title = {Identifying Candidate Classes for Unit Testing Using Deep Learning Classifiers: An Empirical Validation},
year = {2022},
isbn = {9781450396950},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568364.3568380},
doi = {10.1145/3568364.3568380},
abstract = {This paper aims at investigating the use of deep learning to suggest (prioritize) classes to be tested rigorously during unit testing of object-oriented systems. We relied on software unit testing information history and source code metrics. We conducted an empirical study using data collected from two Apache open-source Java software systems (POI and ANT). For each software system, we extracted the source code of five different versions. For each version, we collected various metrics from the source code of the Java classes. Then, for all software classes, we extracted testing coverage measures at instruction and method levels of granularity. We used the existing JUnit test cases developed for these systems. Based on the different datasets we collected, we trained several deep neural network models. We validated the obtained classifiers using four validation techniques: (1) CV: Cross Version validation, (2) CPV: Combined Previous Version validation, (3) CSPV: Combined System and Previous Version validation, and (4) LOSO: Leave One System Out validation. The obtained results in terms of classifiers’ performance vary between 70% and 80% of accuracy and strongly support the viability of our approach.},
booktitle = {Proceedings of the 4th World Symposium on Software Engineering},
pages = {98–107},
numpages = {10},
keywords = {Unit Testing, Tests Prioritization, Testing Coverage Measures, Source Code, Metrics, Machine Learning, Deep Learning},
location = {Xiamen, China},
series = {WSSE '22}
}

@inproceedings{10.1145/3558489.3559066,
author = {Al Debeyan, Fahad and Hall, Tracy and Bowes, David},
title = {Improving the performance of code vulnerability prediction using abstract syntax tree information},
year = {2022},
isbn = {9781450398602},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3558489.3559066},
doi = {10.1145/3558489.3559066},
abstract = {The recent emergence of the Log4jshell vulnerability demonstrates the importance of detecting code vulnerabilities in software systems. Software Vulnerability Prediction Models (VPMs) are a promising tool for vulnerability detection. Recent studies have focused on improving the performance of models to predict whether a piece of code is vulnerable or not (binary classification). However, such approaches are limited because they do not provide developers with information on the type of vulnerability that needs to be patched. We present our multiclass classification approach to improve the performance of vulnerability prediction models. Our approach uses abstract syntax tree n-grams to identify code clusters related to specific vulnerabilities. We evaluated our approach using real-world Java software vulnerability data. We report increased predictive performance compared to a variety of other models, for example, F-measure increases from 55% to 75% and MCC increases from 48% to 74%. Our results suggest that clustering software vulnerabilities using AST n-gram information is a promising approach to improve vulnerability prediction and enable specific information about the vulnerability type to be provided.},
booktitle = {Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {2–11},
numpages = {10},
keywords = {Software Vulnerability, Software Security, Machine learning},
location = {Singapore, Singapore},
series = {PROMISE 2022}
}

@inproceedings{10.1145/3382025.3414960,
author = {Str\"{u}der, Stefan and Mukelabai, Mukelabai and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Feature-oriented defect prediction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414960},
doi = {10.1145/3382025.3414960},
abstract = {Software errors are a major nuisance in software development and can lead not only to reputation damages, but also to considerable financial losses for companies. Therefore, numerous techniques for predicting software defects, largely based on machine learning methods, have been developed over the past decades. These techniques usually rely on code and process metrics in order to predict defects at the granularity of typical software assets, such as subsystems, components, and files. In this paper, we present the first systematic investigation of feature-oriented defect prediction: the prediction of defects at the granularity of features---domain-oriented entities abstractly representing (and often cross-cutting) typical software assets. Feature-oriented prediction can be beneficial, since: (i) particular features might be more error-prone than others, (ii) characteristics of features known as defective might be useful to predict other error-prone features, (iii) feature-specific code might be especially prone to faults arising from feature interactions. We present a dataset derived from 12 software projects and introduce two metric sets for feature-oriented defect prediction. We evaluated seven machine learning classifiers with three different attribute sets each, using our two new metric sets as well as an existing metric set from the literature. We observe precision and recall values of around 85% and better robustness when more diverse metrics sets with richer feature information are used.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {21},
numpages = {12},
keywords = {prediction, feature, defect, classification},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3330204.3330230,
author = {de Macedo, Charles Mendes and Ruela, Andr\'{e} Siqueira and Delgado, Karina Valdivia},
title = {Application of Clustering Algorithms for Discovering Bug Patterns in JavaScript Software},
year = {2019},
isbn = {9781450372374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330204.3330230},
doi = {10.1145/3330204.3330230},
abstract = {Applications developed with JavaScript language are increasing every day, not only for client-side, but also for server-side and for mobile devices. In this context, the existence of tools to identify faults is fundamental in order to assist developers during the evolution of their applications. Different tools and approaches have been proposed over the years, however they have limitations to evolve over time, becoming obsolete quickly. The reason for this is the use of a fixed list of pre-defined faults that are searched in the code. The BugAID tool implements a semiautomatic strategy for discovering bug patterns by grouping the changes made during the project development. The objective of this work is to contribute to the BugAID tool, extending this tool with improvements in the extraction of characteristics to be used by the clustering algorithm. The extended module of the BugAID extraction module (BE) that extracts the characteristics is called BE+. Additionally, an evaluation of the clustering algorithms used for discovering fault patterns in JavaScript software is performed. The results show that the DBScan and Optics algorithms with BE+ presented the best results for the Rand, Jaccard and Adjusted Rand indexes, while HDBScan with BE and BE+ presented the worst result.},
booktitle = {Proceedings of the XV Brazilian Symposium on Information Systems},
articleno = {21},
numpages = {8},
keywords = {Software Quality, Pattern Recognition, Machine Learning, Data Mining, Bug Discovery},
location = {Aracaju, Brazil},
series = {SBSI '19}
}

@inproceedings{10.1145/2365324.2365326,
author = {Shepperd, Martin},
title = {The scientific basis for prediction research},
year = {2012},
isbn = {9781450312417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2365324.2365326},
doi = {10.1145/2365324.2365326},
abstract = {In recent years there has been a huge growth in using statistical and machine learning methods to find useful prediction systems for software engineers. Of particular interest is predicting project effort and duration and defect behaviour. Unfortunately though results are often promising no single technique dominates and there are clearly complex interactions between technique, training methods and the problem domain. Since we lack deep theory our research is of necessity experimental. Minimally, as scientists, we need reproducible studies. We also need comparable studies. I will show through a meta-analysis of many primary studies that we are not presently in that situation and so the scientific basis for our collective research remains in doubt. By way of remedy I will argue that we need to address these issues of reporting protocols and expertise plus ensure blind analysis is routine.},
booktitle = {Proceedings of the 8th International Conference on Predictive Models in Software Engineering},
pages = {1–2},
numpages = {2},
keywords = {defect prediction, empirical research, machine learning, software metrics},
location = {Lund, Sweden},
series = {PROMISE '12}
}

@inproceedings{10.1145/3524304.3524310,
author = {Abaei, Golnoush and Tah, Wen Zhong and Toh, Jason Zhern Wee and Hor, Ethan Sheng Jian},
title = {Improving software fault prediction in imbalanced datasets using the under-sampling approach},
year = {2022},
isbn = {9781450385770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524304.3524310},
doi = {10.1145/3524304.3524310},
abstract = {To make most software defect-free, a considerable amount of budget needs to be allocated to the software testing phase. As each day goes by, this budget slowly rises, as most software grows in size and complexity, which causes an issue for specific companies that cannot allocate sufficient resources towards testing. To tackle this, many researchers use machine learning methods to create software fault prediction models that can help detect defect-prone modules so that resources can be allocated more efficiently during testing. Although this is a feasible plan, the effectiveness of these machine learning models also depends on a few factors, such as the issue of data imbalance. There are many known techniques in class imbalance research that can potentially improve the performance of prediction models through processing the dataset before providing it as input. However, not all methods are compatible with one another. Before building a prediction model, the dataset undergoes the preprocessing step, the under-sampling, and the feature selection process. This study uses an under-sampling process by employing the Instance Hardness Threshold (IHT), which reduces the number of data present in the majority class. The performance of the proposed approach is evaluated based on eight machine learning algorithms by applying it to eight moderate and highly imbalanced NASA datasets. The results of our proposed approach show improvement in AUC and F1-Score by 33% and 26%, respectively, compared to other research work in some datasets.},
booktitle = {Proceedings of the 2022 11th International Conference on Software and Computer Applications},
pages = {41–47},
numpages = {7},
keywords = {Under-sampling, Testing, Software Fault Prediction, Imbalanced Dataset},
location = {Melaka, Malaysia},
series = {ICSCA '22}
}

@inproceedings{10.1145/3575879.3575964,
author = {Katsadouros, Evangelos and Patrikakis, Charalampos},
title = {A Survey on Vulnerability Prediction using GNNs},
year = {2023},
isbn = {9781450398541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575879.3575964},
doi = {10.1145/3575879.3575964},
abstract = {The massive release of software products has led to critical incidents in the software industry due to low-quality software. Software engineers lack security knowledge which causes the development of insecure software. Traditional solutions for analysing code for vulnerabilities suffer from high false positives and negative rates. Researchers over the last decade have proposed mechanisms for analysing code for vulnerabilities using machine learning. The results are promising and could replace traditional static analysis tools or accompany them in the foreseeable future to produce more reliable results. This survey presents the work done so far in vulnerability detection using Graph Neural Networks (GNNs). Presents the GNNs architectures, the graph representations, the datasets, and the results of these studies.},
booktitle = {Proceedings of the 26th Pan-Hellenic Conference on Informatics},
pages = {38–43},
numpages = {6},
keywords = {Software Quality, Software, Privacy},
location = {Athens, Greece},
series = {PCI '22}
}

@inproceedings{10.1145/2499393.2499395,
author = {Herbold, Steffen},
title = {Training data selection for cross-project defect prediction},
year = {2013},
isbn = {9781450320160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499393.2499395},
doi = {10.1145/2499393.2499395},
abstract = {Software defect prediction has been a popular research topic in recent years and is considered as a means for the optimization of quality assurance activities. Defect prediction can be done in a within-project or a cross-project scenario. The within-project scenario produces results with a very high quality, but requires historic data of the project, which is often not available. For the cross-project prediction, the data availability is not an issue as data from other projects is readily available, e.g., in repositories like PROMISE. However, the quality of the defect prediction results is too low for practical use. Recent research showed that the selection of appropriate training data can improve the quality of cross-project defect predictions. In this paper, we propose distance-based strategies for the selection of training data based on distributional characteristics of the available data. We evaluate the proposed strategies in a large case study with 44 data sets obtained from 14 open source projects. Our results show that our training data selection strategy improves the achieved success rate of cross-project defect predictions significantly. However, the quality of the results still cannot compete with within-project defect prediction.},
booktitle = {Proceedings of the 9th International Conference on Predictive Models in Software Engineering},
articleno = {6},
numpages = {10},
keywords = {cross-project prediction, defect-prediction, machine learning},
location = {Baltimore, Maryland, USA},
series = {PROMISE '13}
}

@inproceedings{10.1145/3530019.3531333,
author = {Hussain, Shahid and Ibrahim, Naseem},
title = {Empirical Investigation of role of Meta-learning approaches for the Improvement of Software Development Process via Software Fault Prediction},
year = {2022},
isbn = {9781450396134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3530019.3531333},
doi = {10.1145/3530019.3531333},
abstract = {Context: Software Engineering (SE) community has empirically investigated software defect prediction as a proxy to benchmark it as a process improvement activity to assure software quality. In the domain of software fault prediction, the performance of classification algorithms is highly provoked with the residual effects attributed to feature irrelevance and data redundancy issues. Problem: The meta-learning-based ensemble methods are usually carried out to mitigate these noise effects and boost the software fault prediction performance. However, there is a need to benchmark the performance of meta-learning ensemble methods (as fault predictor) to assure software quality control and aid developers in their decision making. Method: We conduct an empirical and comparative study to evaluate and benchmark the improvement in the fault prediction performance via meta-learning ensemble methods as compared to their component base-level fault predictors. In this study, we perform a series of experiments with four well-known meta-level ensemble methods Vote, StackingC (i.e., Stacking), MultiScheme, and Grading. We also use five high-performance fault predictors Logistic (i.e., Logistic Regression), J48 (i.e., Decision Tree), IBK (i.e. k-nearest neighbor), NaiveBayes, and Decision Table (DT). Subsequently, we performed these experiments on public defect datasets with k-fold (k=10) cross-validation. We used F-measure and ROC-AUC (Receiver Operating Characteristic-Area Under Curve) performance measures and applied the four non-parametric tests to benchmark the fault prediction performance results of meta-learning ensemble methods. Results and Conclusion: we conclude that meta-learning ensemble methods, especially Vote could outperform the base-level fault predictors to tackle the feature irrelevance and redundancy issues in the domain of software fault prediction. Having said that, their performance is highly related to the number of base-level classifiers and the set of software fault prediction metrics.},
booktitle = {Proceedings of the 26th International Conference on Evaluation and Assessment in Software Engineering},
pages = {413–420},
numpages = {8},
keywords = {Performance, Metrics, Fault Prediction, Ensemble method, Classification},
location = {Gothenburg, Sweden},
series = {EASE '22}
}

@inproceedings{10.1145/3510003.3510214,
author = {Kang, Hong Jin and Aw, Khai Loong and Lo, David},
title = {Detecting false alarms from automatic static analysis tools: how far are we?},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510214},
doi = {10.1145/3510003.3510214},
abstract = {Automatic static analysis tools (ASATs), such as Findbugs, have a high false alarm rate. The large number of false alarms produced poses a barrier to adoption. Researchers have proposed the use of machine learning to prune false alarms and present only actionable warnings to developers. The state-of-the-art study has identified a set of "Golden Features" based on metrics computed over the characteristics and history of the file, code, and warning. Recent studies show that machine learning using these features is extremely effective and that they achieve almost perfect performance.We perform a detailed analysis to better understand the strong performance of the "Golden Features". We found that several studies used an experimental procedure that results in data leakage and data duplication, which are subtle issues with significant implications. Firstly, the ground-truth labels have leaked into features that measure the proportion of actionable warnings in a given context. Secondly, many warnings in the testing dataset appear in the training dataset. Next, we demonstrate limitations in the warning oracle that determines the ground-truth labels, a heuristic comparing warnings in a given revision to a reference revision in the future. We show the choice of reference revision influences the warning distribution. Moreover, the heuristic produces labels that do not agree with human oracles. Hence, the strong performance of these techniques previously seen is overoptimistic of their true performance if adopted in practice. Our results convey several lessons and provide guidelines for evaluating false alarm detectors.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {698–709},
numpages = {12},
keywords = {data duplication, data leakage, false alarms, static analysis},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@article{10.1145/3708521,
author = {Li, Rui and Liu, Huai and Poon, Pak-Lok and Towey, Dave and Sun, Chang-Ai and Zheng, Zheng and Zhou, Zhi Quan and Chen, Tsong Yueh},
title = {Metamorphic Relation Generation: State of the Art and Research Directions},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708521},
doi = {10.1145/3708521},
abstract = {Metamorphic testing has become one mainstream technique to address the notorious oracle problem in software testing, thanks to its great successes in revealing real-life bugs in a wide variety of software systems. Metamorphic relations, the core component of metamorphic testing, have continuously attracted research interests from both academia and industry. In the last decade, a rapidly increasing number of studies have been conducted to systematically generate metamorphic relations from various sources and for different application domains. In this article, based on the systematic review on the state of the art for metamorphic relations’ generation, we summarize and highlight visions for further advancing the theory and techniques for identifying and constructing metamorphic relations, and discuss promising research directions in related areas.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {Metamorphic testing, Metamorphic relation, Metamorphic relation generation}
}

@inproceedings{10.1145/2897695.2897696,
author = {Ortu, Marco and Destefanis, Giuseppe and Swift, Stephen and Marchesi, Michele},
title = {Measuring high and low priority defects on traditional and mobile open source software},
year = {2016},
isbn = {9781450341776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897695.2897696},
doi = {10.1145/2897695.2897696},
abstract = {Software defects are the major cause for system failures. To effectively design tools and provide support for detecting and recovering from software failures, requires a deep understanding of defect features. In this paper we present an analysis of defect characteristics in two different open source software development domains: Mobile and Traditional. Our attention is focused on measuring the differences between High-Priority and Low-Priority defects. High or Low priority of a given defect is decided by a developer when creating a bug report for an issue tracking system. We sampled hundreds of real world bugs in hundreds of large and representative open-source projects. We used natural language text classification techniques to automatically analyse roughly 700,000 bug reports from the Bugzilla, Jira and Google Issues issue tracking systems. Results show that there are differences between High-Priority and Low-Priority defects classification in Mobile and Traditional development domains.},
booktitle = {Proceedings of the 7th International Workshop on Emerging Trends in Software Metrics},
pages = {1–7},
numpages = {7},
keywords = {data mining, bug reports, bug categorisation},
location = {Austin, Texas},
series = {WETSoM '16}
}

@inproceedings{10.1145/3631991.3631996,
author = {Irie, Shinnosuke and Aman, Hirohisa and Amasaki, Sousuke and Yokogawa, Tomoyuki and Kawahara, Minoru},
title = {A Comparative Study of Hybrid Fault-Prone Module Prediction Models Using Association Rule and Random Forest},
year = {2023},
isbn = {9798400708053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631991.3631996},
doi = {10.1145/3631991.3631996},
abstract = {Many fault-prone module prediction methods are implemented using machine learning algorithms, and the random forest is well known as the simple and powerful one. However, since the random forest uses an ensemble of decision trees, it is hard to explain why the module is predicted as “fault-prone.” In order to compensate for such a weakness, there have been studies of hybrid prediction methods combining the association rule mining technique with the random forest. In the hybrid method, a module’s fault-proneness is first assessed by the association rules. Then, when the module’s feature does not match any rules, its fault-proneness is evaluated by the random forest model. This paper focuses on how to combine the two techniques and conducts a comparative study to explore a better hybrid prediction method. The empirical results show: (1) it is better to use both association rules of “faulty” and “non-faulty” rather than using only “faulty” rules; (2) it is better to train the random forest classifiers using all data regardless of whether or not they matched association rules.},
booktitle = {Proceedings of the 2023 5th World Symposium on Software Engineering},
pages = {33–38},
numpages = {6},
keywords = {association rule, fault-prone module prediction, random forest},
location = {Tokyo, Japan},
series = {WSSE '23}
}

@inproceedings{10.1145/3540250.3549175,
author = {Le-Cong, Thanh and Kang, Hong Jin and Nguyen, Truong Giang and Haryono, Stefanus Agus and Lo, David and Le, Xuan-Bach D. and Huynh, Quyet Thang},
title = {AutoPruner: transformer-based call graph pruning},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549175},
doi = {10.1145/3540250.3549175},
abstract = {Constructing a static call graph requires trade-offs between soundness and precision.  
Program analysis techniques for constructing call graphs are unfortunately usually imprecise.  
To address this problem, researchers have recently proposed call graph pruning empowered by machine learning to post-process call graphs constructed by static analysis. A machine learning model is built to capture information from the call graph by extracting structural features for use in a random forest classifier. It then removes edges that are predicted to be false positives. Despite the improvements shown by machine learning models, they are still limited as they do not consider the source code semantics and thus often are not able to effectively distinguish true and false positives.  

In this paper, we present a novel call graph pruning technique, AutoPruner, for eliminating false positives in call graphs via both statistical semantic and structural analysis.  
Given a call graph constructed by traditional static analysis tools, AutoPruner takes a Transformer-based approach to capture the semantic relationships between the caller and callee functions associated with each edge in the call graph. To do so, AutoPruner fine-tunes a model of code that was pre-trained on a large corpus to represent source code based on descriptions of its semantics.  
Next, the model is used to extract semantic features from the functions related to each edge in the call graph. AutoPruner uses these semantic features together with the structural features extracted from the call graph to classify each edge via a feed-forward neural network. Our empirical evaluation on a benchmark dataset of real-world programs shows that AutoPruner outperforms the state-of-the-art baselines, improving on F-measure by up to 13% in identifying false-positive edges in a static call graph. Moreover, AutoPruner achieves improvements on two client analyses, including halving the false alarm rate on null pointer analysis and over 10% improvements on monomorphic call-site detection. Additionally, our ablation study and qualitative analysis show that the semantic features extracted by AutoPruner capture a remarkable amount of information for distinguishing between true and false positives.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {520–532},
numpages = {13},
keywords = {Transformer, Static Analysis, Pretrained Language Model, Call Graph Pruning},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3558489.3559069,
author = {Coskun, Tugce and Halepmollasi, Rusen and Hanifi, Khadija and Fouladi, Ramin Fadaei and De Cnudde, Pinar Comak and Tosun, Ayse},
title = {Profiling developers to predict vulnerable code changes},
year = {2022},
isbn = {9781450398602},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3558489.3559069},
doi = {10.1145/3558489.3559069},
abstract = {Software vulnerability prediction and management have caught the interest of researchers and practitioners, recently. Various techniques that are usually based on characteristics of the code artefacts are also offered to predict software vulnerabilities. While other studies achieve promising results, the role of developers in inducing vulnerabilities has not been studied yet. We aim to profile the vulnerability inducing and vulnerability fixing behaviors of developers in software projects using Heterogeneous Information Network (HIN) analysis. We also investigate the impact of developer profiles in predicting vulnerability inducing commits, and compare the findings against the approach based on the code metrics. We adopt Random Walk with Restart (RWR) algorithm on HIN and the aggregation of code metrics for extracting all the input features. We utilize traditional machine learning algorithms namely, Naive Bayes (NB), Support Vector Machine (SVM), Random Forest (RF) and eXtreme Gradient Boosting (XGBoost) to build the prediction models.We report our empirical analysis to predict vulnerability inducing commits of four Apache projects. The technique based on code metrics achieves 90% success for the recall measure, whereas the technique based on profiling developer behavior achieves 71% success. When we use the feature sets obtained with the two techniques together, we achieve 89% success.},
booktitle = {Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {32–41},
numpages = {10},
keywords = {vulnerability prediction, vulnerability, technical debt, profiling developers},
location = {Singapore, Singapore},
series = {PROMISE 2022}
}

@article{10.1145/3503509,
author = {Yang, Yanming and Xia, Xin and Lo, David and Bi, Tingting and Grundy, John and Yang, Xiaohu},
title = {Predictive Models in Software Engineering: Challenges and Opportunities},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3503509},
doi = {10.1145/3503509},
abstract = {Predictive models are one of the most important techniques that are widely applied in many areas of software engineering. There have been a large number of primary studies that apply predictive models and that present well-performed studies in various research domains, including software requirements, software design and development, testing and debugging, and software maintenance. This article is a first attempt to systematically organize knowledge in this area by surveying a body of 421 papers on predictive models published between 2009 and 2020. We describe the key models and approaches used, classify the different models, summarize the range of key application areas, and analyze research results. Based on our findings, we also propose a set of current challenges that still need to be addressed in future work and provide a proposed research road map for these opportunities.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {56},
numpages = {72},
keywords = {survey, software engineering, deep learning, machine learning, Predictive models}
}

@inproceedings{10.1145/2593868.2593870,
author = {Malhotra, Ruchika and Khanna, Megha},
title = {A new metric for predicting software change using gene expression programming},
year = {2014},
isbn = {9781450328548},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593868.2593870},
doi = {10.1145/2593868.2593870},
abstract = {Software metrics help in determining the quality of a software product. They can be used for continuous inspection of a software to assist software developers in improving its quality. We can also use metrics to develop quality models which predict important quality attributes like change proneness. Determination of change prone classes in an Object-Oriented software will help software developers to focus their time and resources on the weak portions of the software. In this paper, we validate the Chidamber and Kemerer metric suite for building an efficient software quality model which predict change prone classes with the help of Gene Expression Programming. The model is developed using an open source software. We further propose a new metric which can be used for identifying change prone classes in the early phases of software development life cycle. The proposed metric is validated on another open source software and the results show that it can be effectively used by the software industry to classify change prone classes. Identification of change prone classes may help in efficient refactoring and rigorous testing of these classes in the forthcoming releases of the software product.},
booktitle = {Proceedings of the 5th International Workshop on Emerging Trends in Software Metrics},
pages = {8–14},
numpages = {7},
keywords = {Software Quality, Object-Oriented Metrics, Gene Expression Programming, Empirical Validation, Change Proneness},
location = {Hyderabad, India},
series = {WETSoM 2014}
}

@inproceedings{10.5555/3507788.3507804,
author = {Ria and Grigoriou, Marios-Stavros and Kontogiannis, Kostas and Giammaria, Alberto and Brealey, Chris},
title = {Process-metrics trends analysis for evaluating file-level error proneness},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {Assessing the likelihood of a source code file being buggy or healthy in upcoming commits given its past behavior and its interaction with other files, has been an area where the software engineering community has paid significant attention over the years. Early efforts aimed on associating software metrics with maintainability indexes, while more recent efforts focused on the use of machine learning for classifying a software module as error prone or not. In most approaches to date, this analysis is primarily based on source code metrics or on information extracted from the system's source code, and to a lesser extend on information that relates to process metrics. In this paper, we propose a process-metrics based method for predicting the behavior of a file, based both on its GitHub commits and its interdependences with other co-committed files. More specifically, for each file and for each commit a file participates in, we compute a dependency score this file has with its other co-committed files. This score is appropriately amplified if the file is participating in a bug-fixing commit, or decayed over time if it does not. By examining, over several open source systems, the trend of that dependency score for every file as a product of time, for files whose outcome is known and that are used as gold standard, we report statistics which shed light on estimating the likelihood of whether these trends can predict the future behavior of a file or not.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {113–122},
numpages = {10},
location = {Toronto, Canada},
series = {CASCON '21}
}

@inproceedings{10.1145/1774088.1774612,
author = {Sami, Ashkan and Fakhrahmad, Seyed Mostafa},
title = {Design-level metrics estimation based on code metrics},
year = {2010},
isbn = {9781605586397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1774088.1774612},
doi = {10.1145/1774088.1774612},
abstract = {Fault detection based on mining code and design metrics has been an active research area for many years. Basically "module"-based metrics for source code and design level are calculated or obtained and data mining is used to build predictor models. However, in many projects due to organizational or software process models, design level metrics are not available and/or accurate. It has been shown that performance of these classifiers or predictors decline if only source code features are used for training them. Based on best of our know knowledge no set of rule to estimate design level metrics based on code level metrics has been presented since it is believed that design level metrics have additional information and cannot be estimated without access to design artifacts. In this study we present a fuzzy modeling system to find and present these relationships for projects presented in NASA Metrics Data Repository (MDP) datasets. Interestingly, we could find a set of empirical rules that govern all the projects regardless of size, programming language and software development methodology. Comparison of fault detectors built based on estimated design metrics with actual design metrics on various projects showed a very small difference in accuracy of classifiers and validated our hypothesis that estimation of design metrics based on source code attributes can become a practical exercise.},
booktitle = {Proceedings of the 2010 ACM Symposium on Applied Computing},
pages = {2531–2535},
numpages = {5},
keywords = {software metrics, software defect prediction, parameter estimation, fuzzy classification, approximate dependencies},
location = {Sierre, Switzerland},
series = {SAC '10}
}

@inproceedings{10.1145/1629575.1629587,
author = {Xu, Wei and Huang, Ling and Fox, Armando and Patterson, David and Jordan, Michael I.},
title = {Detecting large-scale system problems by mining console logs},
year = {2009},
isbn = {9781605587523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629575.1629587},
doi = {10.1145/1629575.1629587},
abstract = {Surprisingly, console logs rarely help operators detect problems in large-scale datacenter services, for they often consist of the voluminous intermixing of messages from many software components written by independent developers. We propose a general methodology to mine this rich source of information to automatically detect system runtime problems. We first parse console logs by combining source code analysis with information retrieval to create composite features. We then analyze these features using machine learning to detect operational problems. We show that our method enables analyses that are impossible with previous methods because of its superior ability to create sophisticated features. We also show how to distill the results of our analysis to an operator-friendly one-page decision tree showing the critical messages associated with the detected problems. We validate our approach using the Darkstar online game server and the Hadoop File System, where we detect numerous real problems with high accuracy and few false positives. In the Hadoop case, we are able to analyze 24 million lines of console logs in 3 minutes. Our methodology works on textual console logs of any size and requires no changes to the service software, no human input, and no knowledge of the software's internals.},
booktitle = {Proceedings of the ACM SIGOPS 22nd Symposium on Operating Systems Principles},
pages = {117–132},
numpages = {16},
keywords = {tracing, statistical learning, source code analysis, problem detection, pca, monitoring, console log analysis},
location = {Big Sky, Montana, USA},
series = {SOSP '09}
}

@article{10.1145/3678167,
author = {Liu, Yue and Tantithamthavorn, Chakkrit and Liu, Yonghui and Thongtanunam, Patanamon and Li, Li},
title = {Automatically Recommend Code Updates: Are We There Yet?},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {8},
issn = {1049-331X},
url = {https://doi.org/10.1145/3678167},
doi = {10.1145/3678167},
abstract = {In recent years, large pre-trained Language Models of Code (CodeLMs) have shown promising results on various software engineering tasks. One such task is automatic code update recommendation, which transforms outdated code snippets into their approved and revised counterparts. Although many CodeLM-based approaches have been proposed, claiming high accuracy, their effectiveness and reliability on real-world code update tasks remain questionable. In this article, we present the first extensive evaluation of state-of-the-art CodeLMs for automatically recommending code updates. We assess their performance on two diverse datasets of paired updated methods, considering factors such as temporal evolution, project specificity, method size, and update complexity. Our results reveal that while CodeLMs exhibit higher performance in settings that ignore temporal information, they struggle in more realistic time-wise scenarios and generalize poorly to new projects. Furthermore, CodeLM performance decreases significantly for larger methods and more complex updates. Furthermore, we observe that many CodeLM-generated “updates” are actually null, especially in time-wise settings, and meaningful edits remain challenging. Our findings highlight the significant gap between the perceived and actual effectiveness of CodeLMs for real-world code update recommendation and emphasize the need for more research on improving their practicality, robustness, and generalizability.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {217},
numpages = {27},
keywords = {Code updates, neural machine translation}
}

@article{10.1145/2557833.2560586,
author = {Peiris, Manjula and Hill, James H.},
title = {Towards detecting software performance anti-patterns using classification techniques},
year = {2014},
issue_date = {January 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2557833.2560586},
doi = {10.1145/2557833.2560586},
abstract = {This paper presents a non-intrusive machine learning approach called Non-intrusive Performance Anti-pattern Detecter (NiPAD) for identifying and classifying software performance anti-patterns. NiPAD uses only system performance metrics-as opposed to analyzing application level performance metrics or source code and the design of a software application to identify and classify software performance anti-patterns within an application. The results of applying NiPAD to an example application show that NiPAD is able to predict the One Lane Bridge software performance anti-pattern within a software application with 0.94 accuracy.},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–4},
numpages = {4},
keywords = {software performance anti-patterns, machine learning, dynamic software analysis, classification}
}

@inproceedings{10.1145/3395363.3397355,
author = {Liu, Hui and Shen, Mingzhu and Jin, Jiahao and Jiang, Yanjie},
title = {Automated classification of actions in bug reports of mobile apps},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397355},
doi = {10.1145/3395363.3397355},
abstract = {When users encounter problems with mobile apps, they may commit such problems to developers as bug reports. To facilitate the processing of bug reports, researchers proposed approaches to validate the reported issues automatically according to the steps to reproduce specified in bug reports. Although such approaches have achieved high success rate in reproducing the reported issues, they often rely on a predefined vocabulary to identify and classify actions in bug reports. However, such manually constructed vocabulary and classification have significant limitations. It is challenging for the vocabulary to cover all potential action words because users may describe the same action with different words. Besides that, classification of actions solely based on the action words could be inaccurate because the same action word, appearing in different contexts, may have different meaning and thus belongs to different action categories. To this end, in this paper we propose an automated approach, called MaCa, to identify and classify action words in Mobile apps’ bug reports. For a given bug report, it first identifies action words based on natural language processing. For each of the resulting action words, MaCa extracts its contexts, i.e., its enclosing segment, the associated UI target, and the type of its target element by both natural language processing and static analysis of the associated app. The action word and its contexts are then fed into a machine learning based classifier that predicts the category of the given action word in the given context. To train the classifier, we manually labelled 1,202 actions words from 525 bug reports that are associated with 207 apps. Our evaluation results on manually labelled data suggested that MaCa was accurate with high accuracy varying from 95% to 96.7%. We also investigated to what extent MaCa could further improve existing approaches (i.e., Yakusu and ReCDroid) in reproducing bug reports. Our evaluation results suggested that integrating MaCa into existing approaches significantly improved the success rates of ReCDroid and Yakusu by 22.7% = (69.2%-56.4%)/56.4% and 22.9%= (62.7%-51%)/51%, respectively.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {128–140},
numpages = {13},
keywords = {Test Case Generation, Mobile Testing, Classification, Bug report},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/3520312.3534869,
author = {Rabin, Md Rafiqul Islam and Hussain, Aftab and Alipour, Mohammad Amin},
title = {Syntax-guided program reduction for understanding neural code intelligence models},
year = {2022},
isbn = {9781450392730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520312.3534869},
doi = {10.1145/3520312.3534869},
abstract = {Neural code intelligence (CI) models are opaque black-boxes and offer little insight on the features they use in making predictions. This opacity may lead to distrust in their prediction and hamper their wider adoption in safety-critical applications. Recently, input program reduction techniques have been proposed to identify key features in the input programs to improve the transparency of CI models. However, this approach is syntax-unaware and does not consider the grammar of the programming language.  

In this paper, we apply a syntax-guided program reduction technique that considers the grammar of the input programs during reduction. Our experiments on multiple models across different types of input programs show that the syntax-guided program reduction technique is faster and provides smaller sets of key tokens in reduced programs. We also show that the key tokens could be used in generating adversarial examples for up to 65% of the input programs.},
booktitle = {Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming},
pages = {70–79},
numpages = {10},
keywords = {Transparency, Program Reduction, Neural Models of Source Code, Interpretability, Feature Engineering},
location = {San Diego, CA, USA},
series = {MAPS 2022}
}

@article{10.1145/3705302,
author = {Zhang, Lehuan and Guo, Shikai and Guo, Yi and Li, Hui and Chai, Yu and Chen, Rong and Li, Xiaochen and Jiang, He},
title = {Context-based Transfer Learning for Structuring Fault Localization and Program Repair Automation},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3705302},
doi = {10.1145/3705302},
abstract = {Automated software debugging plays a crucial role in aiding software developers to swiftly identify and attempt to rectify faults, thereby significantly reducing developers’ workload. Previous researches have predominantly relied on simplistic semantic deep learning or statistical analysis methods to locate faulty statements in diverse projects. However, code repositories often consist of lengthy sequences with long-distance dependencies, posing challenges for accurately modeling fault localization using these methods. In addition, the lack of joint reasoning among various faults prevents existing models from deeply capturing fault information. To address these challenges, we propose a method named CodeHealer to achieve accurate fault localization and program repair. CodeHealer comprises three components: a Deep Semantic Information Extraction Component that effectively extracts deep semantic features from suspicious code statements using classifiers based on Joint-attention mechanisms; a Suspicious Statement Ranking Component that combines various fault localization features and employs multilayer perceptrons to derive multidimensional vectors of suspicion values; and a Fault Repair Component that, based on ranked suspicious statements generated by fault localization, adopts a top-down approach using multiple classifiers based on Co-teaching mechanisms to select repair templates and generate patches. The experimental results indicate that when applied to fault localization, CodeHealer outperforms the best baseline method with improvements of 11.4%, 2.7%, and 1.6% on Top-1/3/5 metrics, respectively. It also reduces the MFR and MAR by 9.8% and 2.1%, where lower values denote better fault localization effectiveness. Additionally, in automated software debugging, CodeHealer fixes an additional 6 faults compared to the current best method, totaling 53 faults repaired.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
keywords = {Software debugging, Fault Localization, Transfer learning}
}

@inproceedings{10.1145/3230467.3230477,
author = {Shatnawi, Raed},
title = {Identifying Threshold Values of Change-Prone Modules},
year = {2018},
isbn = {9781450364300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230467.3230477},
doi = {10.1145/3230467.3230477},
abstract = {Software changes frequently during the lifetime of a project. These changes increase the total cost of software development. Knowing where changes are more likely help in allocating appropriate resources in different activities of software lifecycle. The change-proneness of modules is defined as likelihood of change in a module. Change-proneness can be predicted using software metrics such as Chidamber and Kemerer metrics. However, the use of prediction models requires knowledge in advanced techniques such as data mining and regression analysis. Hence, there is a need to a more direct and simple technique to have more information about change-prone modules. Threshold values are proposed as such technique to identify the modules that are more change-prone. We propose the ROC analysis to identify thresholds. The ROC analysis suggests that large values of software metrics are indicators of change proneness. The research could identify practical thresholds for four metrics, CBO, RFC, WMC and the LCOM. The inheritance metrics do not have practical thresholds.In this paper, we describe the formatting guidelines for ACM SIG Proceedings.},
booktitle = {Proceedings of the 2018 International Conference on E-Business and Mobile Commerce},
pages = {39–43},
numpages = {5},
keywords = {product metrics, change-prone metrics, ROC curves, CK metrics},
location = {Chengdu, China},
series = {ICEMC '18}
}

@inproceedings{10.1145/3702138.3702146,
author = {Yuan, Zhidan and Wang, Zhuangzhuang and Wu, Enze and Huang, Tao and Chen, Yingying},
title = {Empirical Studies on Failure Prediction for Distributed Systems Based on Feature Selection},
year = {2025},
isbn = {9798400717543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702138.3702146},
doi = {10.1145/3702138.3702146},
abstract = {Distributed system failure prediction identifies potential failures by constructing machine learning models based on Key Performance Indicator (KPI) data. However, due to the diversity of KPI metrics, not all metrics are relevant to specific failures. This lead to redundancy in the KPI metrics used to construct the models. To address this issue, we investigate the impact of filter-based ranking feature selection methods on distributed system failure prediction. In our empirical study, we conduct experiments based on the ZTE dataset by using six feature selection methods and four tree-based models. We explore the effects of different feature selection methods and selection ratios on model performance. Additionally, we study whether feature selection methods can effectively identify important KPI metrics from the perspective of model interpretability. The results indicate that compared to not using feature selection methods, employing such methods allows for the construction of effective failure prediction models with only a subset of KPI metrics. Model performance improves with an increase in feature selection ratio until reaching a point of stabilization. Moreover, feature selection methods effectively identify important KPI metrics within the dataset.},
booktitle = {Proceeding of the 2024 5th Asia Service Sciences and Software Engineering Conference},
pages = {43–52},
numpages = {10},
keywords = {Empirical Studies, Feature Selection, KPI Metrics Data, Model Interpretability},
location = {
},
series = {ASSE '24}
}

@inproceedings{10.1145/3512353.3512379,
author = {Chopra, Rahul and Roy, Shreoshi and Malhotra, Ruchika},
title = {Transductive Instance Transfer Learning for Cross-Language Defect Prediction},
year = {2022},
isbn = {9781450395571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512353.3512379},
doi = {10.1145/3512353.3512379},
abstract = {Predicting defects (bugs) is critical to increasing software quality. Many software defect prediction algorithms have been presented, and many of them have shown to be effective in practice. However, because existing works are largely limited to a single project, their effectiveness in predicting cross-project defects is usually poor. This is primarily due to the issue of class imbalance and discrepancies in feature distribution between the source and destination projects. However, because of the disparities in distribution amongst datasets from different studies, developing high-quality Cross Project Defect Prediction (CPDP) models remains a difficulty. In our study, instead of collecting data from a single project, we have collected source code from multiple code submissions on a programming contest website and employed Natural Language Processing (NLP) models to detect software defects in them.},
booktitle = {Proceedings of the 2022 4th Asia Pacific Information Technology Conference},
pages = {176–182},
numpages = {7},
keywords = {Transfer Learning, Natural Language Processing, Long Short Term Memory, Doc2Vec Embedding, Defect Prediction, Artificial Neural Network},
location = {Virtual Event, Thailand},
series = {APIT '22}
}

@inproceedings{10.1145/259526.259548,
author = {Cheatham, Thomas J. and Yoo, Jungsoon P. and Wahl, Nancy J.},
title = {Software testing: a machine learning experiment},
year = {1995},
isbn = {0897917375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/259526.259548},
doi = {10.1145/259526.259548},
booktitle = {Proceedings of the 1995 ACM 23rd Annual Conference on Computer Science},
pages = {135–141},
numpages = {7},
location = {Nashville, Tennessee, USA},
series = {CSC '95}
}

@inproceedings{10.1145/3691620.3695539,
author = {Zhao, Yu and Gong, Lina and Huang, Zhiqiu and Wang, Yongwei and Wei, Mingqiang and Wu, Fei},
title = {Coding-PTMs: How to Find Optimal Code Pre-trained Models for Code Embedding in Vulnerability Detection?},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695539},
doi = {10.1145/3691620.3695539},
abstract = {Vulnerability detection is garnering increasing attention in software engineering, since code vulnerabilities possibly pose significant security. Recently, reusing various code pre-trained models (e.g., CodeBERT, CodeT5, and CodeGen) has become common for code embedding without providing reasonable justifications in vulnerability detection. The premise for casually utilizing pre-trained models (PTMs) is that the code embeddings generated by different PTMs would generate a similar impact on the performance. Is that TRUE? To answer this important question, we systematically investigate the effects of code embedding generated by ten different code PTMs on the performance of vulnerability detection, and get the answer, i.e., that is NOT true. We observe that code embedding generated by various code PTMs can indeed influence the performance and selecting an embedding technique based on parameter scales and embedding dimension is not reliable. Our findings highlight the necessity of quantifying and evaluating the characteristics of code embedding generated by various code PTMs to understand the effects. To achieve this goal, we analyze the numerical representation and data distribution of code embedding generated by different PTMs to evaluate differences and characteristics. Based on these insights, we propose Coding-PTMs, a recommendation framework to assist engineers in selecting optimal code PTMs for their specific vulnerability detection tasks. Specifically, we define thirteen code embedding metrics across three dimensions (i.e., statistics, norm, and distribution) for constructing a specialized code PTM recommendation dataset. We then employ a Random Forest classifier to train a recommendation model and identify the optimal code PTMs from the candidate model zoo. We encourage engineers to use our Coding-PTMs to evaluate the characteristics of code embeddings generated by candidate code PTMs on the performance and recommend optimal code PTMs for code embedding in their vulnerability detection tasks.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1732–1744},
numpages = {13},
keywords = {coding-PTMs, code embedding, pre-trained models, vulnerability detection, embedding metrics, recommendation framework},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3338906.3338941,
author = {Jimenez, Matthieu and Rwemalika, Renaud and Papadakis, Mike and Sarro, Federica and Le Traon, Yves and Harman, Mark},
title = {The importance of accounting for real-world labelling when predicting software vulnerabilities},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338941},
doi = {10.1145/3338906.3338941},
abstract = {Previous work on vulnerability prediction assume that predictive models are trained with respect to perfect labelling information (includes labels from future, as yet undiscovered vulnerabilities). In this paper we present results from a comprehensive empirical study of 1,898 real-world vulnerabilities reported in 74 releases of three security-critical open source systems (Linux Kernel, OpenSSL and Wiresark). Our study investigates the effectiveness of three previously proposed vulnerability prediction approaches, in two settings: with and without the unrealistic labelling assumption. The results reveal that the unrealistic labelling assumption can profoundly mis- lead the scientific conclusions drawn; suggesting highly effective and deployable prediction results vanish when we fully account for realistically available labelling in the experimental methodology. More precisely, MCC mean values of predictive effectiveness drop from 0.77, 0.65 and 0.43 to 0.08, 0.22, 0.10 for Linux Kernel, OpenSSL and Wiresark, respectively. Similar results are also obtained for precision, recall and other assessments of predictive efficacy. The community therefore needs to upgrade experimental and empirical methodology for vulnerability prediction evaluation and development to ensure robust and actionable scientific findings.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {695–705},
numpages = {11},
keywords = {Software Vulnerabilities, Prediction Modelling, Machine Learning},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3639478.3639781,
author = {Duque-Torres, Alejandra},
title = {Selecting and Constraining Metamorphic Relations},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639781},
doi = {10.1145/3639478.3639781},
abstract = {Software testing is a critical aspect of ensuring the reliability and quality of software systems. However, it often poses challenges, particularly in determining the expected output of a System Under Test (SUT) for a given set of inputs, a problem commonly referred to as the test oracle problem. Metamorphic Testing (MT) offers a promising solution to the test oracle problem by examining the relations between input-output pairs in consecutive executions of the SUT. These relations, referred to as Metamorphic Relations (MRs), define the expected changes in the output when specific changes are made to the input. Our research is focused on developing methods and tools to assist testers in the selection of MRs, the definition of constraints, and providing explanations for MR outcomes. The research is divided in three parts. The first part focuses on MR collection and description, entailing the creation of a comprehensive repository of MRs from various sources. A standardised MR representation is devised to promote machine-readability and wide-ranging applicability. The second part introduces MetraTrimmer, a test-data-driven approach for systematically selecting and constraining MRs. This approach acknowledges that MRs may not be universally applicable to all test data space. The final part, evaluation and validation, encompasses empirical studies aimed at assessing the effectiveness of the developed methods and validating their suitability for real-world regression testing scenarios. Through this research, we aim to advance the automation of MR generation, enhance the understanding of MR violations, and facilitate their effective application in regression testing.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {212–216},
numpages = {5},
keywords = {test oracle, metamorphic testing, metamorphic relations, test data, pattern mining},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3324884.3415292,
author = {Yu, Runze and Zhang, Youzhe and Xuan, Jifeng},
title = {MetPurity: a learning-based tool of pure method identification for automatic test generation},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3415292},
doi = {10.1145/3324884.3415292},
abstract = {In object-oriented programming, a method is pure if calling the method does not change object states that exist in the pre-states of the method call. Pure methods are widely-used in automatic techniques, including test generation, compiler optimization, and program repair. Due to the source code dependency, it is infeasible to completely and accurately identify all pure methods. Instead, existing techniques such as ReImInfer are designed to identify a subset of accurate results of pure method and mark the other methods as unknown ones. In this paper, we designed and implemented MetPurity, a learning-based tool of pure method identification. Given all methods in a project, MetPurity labels a training set via automatic program analysis and builds a binary classifier (implemented with the random forest classifier) based on the training set. This classifier is used to predict the purity of all the other methods (i.e., unknown ones) in the same project. Preliminary evaluation on four open-source Java projects shows that MetPurity can provide a list of identified pure methods with a low error rate. Applying MetPurity to EvoSuite can increase the number of generated assertions for regression testing in test generation by EvoSuite.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1326–1330},
numpages = {5},
keywords = {debugging, machine learning, method purity, regression testing, static analysis, test generation},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1145/3544792,
author = {Zohdinasab, Tahereh and Riccio, Vincenzo and Gambi, Alessio and Tonella, Paolo},
title = {Efficient and Effective Feature Space Exploration for Testing Deep Learning Systems},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3544792},
doi = {10.1145/3544792},
abstract = {Assessing the quality of Deep Learning (DL) systems is crucial, as they are increasingly adopted in safety-critical domains. Researchers have proposed several input generation techniques for DL systems. While such techniques can expose failures, they do not explain which features of the test inputs influenced the system’s (mis-) behaviour. DeepHyperion was the first test generator to overcome this limitation by exploring the DL systems’ feature space at large. In this article, we propose DeepHyperion-CS, a test generator for DL systems that enhances DeepHyperion by promoting the inputs that contributed more to feature space exploration during the previous search iterations. We performed an empirical study involving two different test subjects (i.e., a digit classifier and a lane-keeping system for self-driving cars). Our results proved that the contribution-based guidance implemented within DeepHyperion-CS outperforms state-of-the-art tools and significantly improves the efficiency and the effectiveness of DeepHyperion. DeepHyperion-CS exposed significantly more misbehaviours for five out of six feature combinations and was up to 65% more efficient than DeepHyperion in finding misbehaviour-inducing inputs and exploring the feature space. DeepHyperion-CS was useful for expanding the datasets used to train the DL systems, populating up to 200% more feature map cells than the original training set.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {49},
numpages = {38},
keywords = {self-driving cars, search based software engineering, Deep Learning, Software testing}
}

@inproceedings{10.1109/ASE56229.2023.00133,
author = {Huo, Yintong and Li, Yichen and Su, Yuxin and He, Pinjia and Xie, Zifan and Lyu, Michael R.},
title = {AutoLog: A Log Sequence Synthesis Framework for Anomaly Detection},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00133},
doi = {10.1109/ASE56229.2023.00133},
abstract = {The rapid progress of modern computing systems has led to a growing interest in informative run-time logs. Various log-based anomaly detection techniques have been proposed to ensure software reliability. However, their implementation in the industry has been limited due to the lack of high-quality public log resources as training datasets.While some log datasets are available for anomaly detection, they suffer from limitations in (1) comprehensiveness of log events; (2) scalability over diverse systems; and (3) flexibility of log utility. To address these limitations, we propose AutoLog, the first automated log generation methodology for anomaly detection. AutoLog uses program analysis to generate runtime log sequences without actually running the system. AutoLog starts with probing comprehensive logging statements associated with the call graphs of an application. Then, it constructs execution graphs for each method after pruning the call graphs to find log-related execution paths in a scalable manner. Finally, AutoLog propagates the anomaly label to each acquired execution path based on human knowledge. It generates flexible log sequences by walking along the log execution paths with controllable parameters. Experiments on 50 popular Java projects show that AutoLog acquires significantly more (9x-58x) log events than existing log datasets from the same system, and generates log messages much faster (15x) with a single machine than existing passive data collection approaches. AutoLog also provides hyper-parameters to adjust the data size, anomaly rate, and component indicator for simulating different real-world scenarios. We further demonstrate AutoLog's practicality by showing that AutoLog enables log-based anomaly detectors to achieve better performance (1.93%) compared to existing log datasets. We hope AutoLog can facilitate the benchmarking and adoption of automated log analysis techniques.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {497–509},
numpages = {13},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/2905055.2905123,
author = {Singh, Satwinder and Singla, Rozy},
title = {Comparative Performance of Fault-Prone Prediction Classes with K-means Clustering and MLP},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905123},
doi = {10.1145/2905055.2905123},
abstract = {Software defect in today's era is most important in the field of software engineering. Most of the organizations used various techniques to predict defects in their products before they are delivered. Defect prediction techniques help the organizations to use their resources effectively which results in lower cost and time requirements. There are various techniques that are used for predicting defects in software before it has to be delivered. For example clustering, neural networks, support vector machine (SVM) etc. In this paper two defect prediction techniques: - K-means Clustering and Multilayer Perceptron model (MLP), are compared. Both the techniques are implemented on different platforms. K-means clustering is implemented using WEKA tool and MLP is implemented using SPSS. The results are compared to find which algorithm produces better results. In this paper Object-Oriented metrics are used for predicting defects in the software.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {65},
numpages = {7},
keywords = {Weka, Object-Oriented Metrics, Neural Network, K-means Clustering, Defect prediction},
location = {Udaipur, India},
series = {ICTCS '16}
}

@proceedings{10.1145/3558489,
title = {PROMISE 2022: Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering},
year = {2022},
isbn = {9781450398602},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our pleasure to welcome you to the 18th ACM International Conference on Predictive Models and Data Analytics in Software Engineering (PROMISE 2022), to be held in hybrid mode (physically and virtually) on November 18th, 2022, co-located with the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2022). PROMISE is an annual forum for researchers and practitioners to present, discuss and exchange ideas, results, expertise and experiences in the construction and/or application of predictive models and data analytics in software engineering. Such models and analyses could be targeted at planning, design, implementation, testing, maintenance, quality assurance, evaluation, process improvement, management, decision making, and risk assessment in software and systems development. This year PROMISE received a total of 18 paper submissions. The review process was double blind and each paper was reviewed by at least three members of the program committee. An online discussion was also held for 8 days. Based on this procedure, we accepted a total of 10 full papers, which will be presented in 3 technical sessions. The acceptance criteria were entirely based on the quality of the papers, without imposing any constraint on the number of papers to be accepted.  

We are delighted to announce an outstanding keynote: Release Engineering in the AI World: How can Analytics Help? By Prof. Bram Adams, Queen’s University, Canada  

We would like to thank all authors for submitting high quality papers, and program committee members for their timely and accurate reviewing activity. Last, but not least, we would like to thank the FSE 2022 organizers for hosting PROMISE 2022 as a co-located event and for their logistic support in the organization of the conference.  

We hope you will enjoy PROMISE 2022.  
We certainly will!  

Many thanks from  
Shane McIntosh (General Chair),  
Gema Rodriguez-Perez and Weiyi Shang (Program Chairs).},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3196398.3196435,
author = {de P\'{a}dua, Guilherme B. and Shang, Weiyi},
title = {Studying the relationship between exception handling practices and post-release defects},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196435},
doi = {10.1145/3196398.3196435},
abstract = {Modern programming languages, such as Java and C#, typically provide features that handle exceptions. These features separate error-handling code from regular source code and aim to assist in the practice of software comprehension and maintenance. Nevertheless, their misuse can still cause reliability degradation or even catastrophic software failures. Prior studies on exception handling revealed the suboptimal practices of the exception handling flows and the prevalence of their anti-patterns. However, little is known about the relationship between exception handling practices and software quality. In this work, we investigate the relationship between software quality (measured by the probability of having post-release defects) and: (i) exception flow characteristics and (ii) 17 exception handling anti-patterns. We perform a case study on three Java and C# open-source projects. By building statistical models of the probability of post-release defects using traditional software metrics and metrics that are associated with exception handling practice, we study whether exception flow characteristics and exception handling anti-patterns have a statistically significant relationship with post-release defects. We find that exception flow characteristics in Java projects have a significant relationship with post-release defects. In addition, although the majority of the exception handing anti-patterns are not significant in the models, there exist anti-patterns that can provide significant explanatory power to the probability of post-release defects. Therefore, development teams should consider allocating more resources to improving their exception handling practices and avoid the anti-patterns that are found to have a relationship with post-release defects. Our findings also highlight the need for techniques that assist in handling exceptions in the software development practice.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {564–575},
numpages = {12},
keywords = {empirical software engineering, exception handling, software quality},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@article{10.1145/3433928,
author = {Vandehei, Bailey and Costa, Daniel Alencar Da and Falessi, Davide},
title = {Leveraging the Defects Life Cycle to Label Affected Versions and Defective Classes},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3433928},
doi = {10.1145/3433928},
abstract = {Two recent studies explicitly recommend labeling defective classes in releases using the affected versions (AV) available in issue trackers (e.g., Jira). This practice is coined as the realistic approach. However, no study has investigated whether it is feasible to rely on AVs. For example, how available and consistent is the AV information on existing issue trackers? Additionally, no study has attempted to retrieve AVs when they are unavailable. The aim of our study is threefold: (1) to measure the proportion of defects for which the realistic method is usable, (2) to propose a method for retrieving the AVs of a defect, thus making the realistic approach usable when AVs are unavailable, (3) to compare the accuracy of the proposed method versus three SZZ implementations. The assumption of our proposed method is that defects have a stable life cycle in terms of the proportion of the number of versions affected by the defects before discovering and fixing these defects. Results related to 212 open-source projects from the Apache ecosystem, featuring a total of about 125,000 defects, reveal that the realistic method cannot be used in the majority (51%) of defects. Therefore, it is important to develop automated methods to retrieve AVs. Results related to 76 open-source projects from the Apache ecosystem, featuring a total of about 6,250,000 classes, affected by 60,000 defects, and spread over 4,000 versions and 760,000 commits, reveal that the proportion of the number of versions between defect discovery and fix is pretty stable (standard deviation &lt;2)—across the defects of the same project. Moreover, the proposed method resulted significantly more accurate than all three SZZ implementations in (i) retrieving AVs, (ii) labeling classes as defective, and (iii) in developing defects repositories to perform feature selection. Thus, when the realistic method is unusable, the proposed method is a valid automated alternative to SZZ for retrieving the origin of a defect. Finally, given the low accuracy of SZZ, researchers should consider re-executing the studies that have used SZZ as an oracle and, in general, should prefer selecting projects with a high proportion of available and consistent AVs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {24},
numpages = {35},
keywords = {developing defects repository, defect origin, SZZ, Affected version}
}

@inproceedings{10.1145/1368088.1368114,
author = {Moser, Raimund and Pedrycz, Witold and Succi, Giancarlo},
title = {A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1368088.1368114},
doi = {10.1145/1368088.1368114},
abstract = {In this paper we present a comparative analysis of the predictive power of two different sets of metrics for defect prediction. We choose one set of product related and one set of process related software metrics and use them for classifying Java files of the Eclipse project as defective respective defect-free. Classification models are built using three common machine learners: logistic regression, Na\"{\i}ve Bayes, and decision trees. To allow different costs for prediction errors we perform cost-sensitive classification, which proves to be very successful: &gt;75% percentage of correctly classified files, a recall of &gt;80%, and a false positive rate &lt;30%. Results indicate that for the Eclipse data, process metrics are more efficient defect predictors than code metrics.},
booktitle = {Proceedings of the 30th International Conference on Software Engineering},
pages = {181–190},
numpages = {10},
keywords = {software metrics, defect prediction, cost-sensitive classification},
location = {Leipzig, Germany},
series = {ICSE '08}
}

@article{10.1145/3483424,
author = {Notaro, Paolo and Cardoso, Jorge and Gerndt, Michael},
title = {A Survey of AIOps Methods for Failure Management},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3483424},
doi = {10.1145/3483424},
abstract = {Modern society is increasingly moving toward complex and distributed computing systems. The increase in scale and complexity of these systems challenges O&amp;M teams that perform daily monitoring and repair operations, in contrast with the increasing demand for reliability and scalability of modern applications. For this reason, the study of automated and intelligent monitoring systems has recently sparked much interest across applied IT industry and academia. Artificial Intelligence for IT Operations (AIOps) has been proposed to tackle modern IT administration challenges thanks to Machine Learning, AI, and Big Data. However, AIOps as a research topic is still largely unstructured and unexplored, due to missing conventions in categorizing contributions for their data requirements, target goals, and components. In this work, we focus on AIOps for Failure Management (FM), characterizing and describing 5 different categories and 14 subcategories of contributions, based on their time intervention window and the target problem being solved. We review 100 FM solutions, focusing on applicability requirements and the quantitative results achieved, to facilitate an effective application of AIOps solutions. Finally, we discuss current development problems in the areas covered by AIOps and delineate possible future trends for AI-based failure management.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {81},
numpages = {45},
keywords = {artificial intelligence, failure management, IT operations and maintenance, AIOps}
}

@article{10.1145/1457516.1457529,
author = {Singh, Yogesh and Kaur, Arvinder and Malhotra, Ruchika},
title = {Application of support vector machine to predict fault prone classes},
year = {2009},
issue_date = {January 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/1457516.1457529},
doi = {10.1145/1457516.1457529},
abstract = {Empirical validation of software metrics to predict quality using machine learning methods is important to ensure their practical relevance in the software organizations. It would also be interesting to know the relationship between object-oriented metrics and fault proneness. In this paper, we build a Support Vector Machine (SVM) model to find the relation-ship between object-oriented metrics given by Chidamber and Kemerer and fault proneness. The proposed model is empirically evaluated using open source software. The performance of the SVM method was evaluated by Receiver Operating Characteristic (ROC) analysis. Based on these results, it is reasonable to claim that such models could help for planning and performing testing by focusing resources on fault- prone parts of the design and code. Thus, the study shows that SVM method may also be used in constructing software quality models. However, similar types of studies are required to be carried out in order to establish the acceptability of the model.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {1–6},
numpages = {6}
}

@inproceedings{10.1109/MSR.2017.4,
author = {Rajbahadur, Gopi Krishnan and Wang, Shaowei and Kamei, Yasutaka and Hassan, Ahmed E.},
title = {The impact of using regression models to build defect classifiers},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.4},
doi = {10.1109/MSR.2017.4},
abstract = {It is common practice to discretize continuous defect counts into defective and non-defective classes and use them as a target variable when building defect classifiers (discretized classifiers). However, this discretization of continuous defect counts leads to information loss that might affect the performance and interpretation of defect classifiers. Another possible approach to build defect classifiers is through the use of regression models then discretizing the predicted defect counts into defective and non-defective classes (regression-based classifiers).In this paper, we compare the performance and interpretation of defect classifiers that are built using both approaches (i.e., discretized classifiers and regression-based classifiers) across six commonly used machine learning classifiers (i.e., linear/logistic regression, random forest, KNN, SVM, CART, and neural networks) and 17 datasets. We find that: i) Random forest based classifiers outperform other classifiers (best AUC) for both classifier building approaches; ii) In contrast to common practice, building a defect classifier using discretized defect counts (i.e., discretized classifiers) does not always lead to better performance.Hence we suggest that future defect classification studies should consider building regression-based classifiers (in particular when the defective ratio of the modeled dataset is low). Moreover, we suggest that both approaches for building defect classifiers should be explored, so the best-performing classifier can be used when determining the most influential features.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {135–145},
numpages = {11},
keywords = {random forest, non-discretization, model interpretation, discretization, classification via regression, bug prediction},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@inproceedings{10.1145/3540250.3558950,
author = {Kim, Hyungjin and Kwon, Yonghwi and Joh, Sangwoo and Kwon, Hyukin and Ryou, Yeonhee and Kim, Taeksu},
title = {Understanding automated code review process and developer experience in industry},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558950},
doi = {10.1145/3540250.3558950},
abstract = {Code Review Automation can reduce human efforts during code review by automatically providing valuable information to reviewers. Nevertheless, it is a challenge to automate the process for large-scale companies, such as Samsung Electronics, due to their complexity: various development environments, frequent review requests, huge size of software, and diverse process among the teams. In this work, we show how we automated the code review process for those intricate environments, and share some lessons learned during two years of operation. Our unified code review automation system, Code Review Bot, is designed to process review requests holistically regardless of such environments, and checks various quality-assurance items such as potential defects in the code, coding style, test coverage, and open source license violations. Some key findings include: 1) about 60% of issues found by Code Review Bot were reviewed and fixed in advance of product releases, 2) more than 70% of developers gave positive feedback about the system, 3) developers rapidly and actively responded to reviews, and 4) the automation did not much affect the amount or the frequency of human code reviews compared to the internal policy to encourage code review activities. Our findings provide practical evidence that automating code review helps assure software quality.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1398–1407},
numpages = {10},
keywords = {static analysis, review bot, code review automation, code review},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3196321.3196331,
author = {Xu, Zhou and Li, Shuai and Tang, Yutian and Luo, Xiapu and Zhang, Tao and Liu, Jin and Xu, Jun},
title = {Cross version defect prediction with representative data via sparse subset selection},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196331},
doi = {10.1145/3196321.3196331},
abstract = {Software defect prediction aims at detecting the defect-prone software modules by mining historical development data from software repositories. If such modules are identified at the early stage of the development, it can save large amounts of resources. Cross Version Defect Prediction (CVDP) is a practical scenario by training the classification model on the historical data of the prior version and then predicting the defect labels of modules of the current version. However, software development is a constantly-evolving process which leads to the data distribution differences across versions within the same project. The distribution differences will degrade the performance of the classification model. In this paper, we approach this issue by leveraging a state-of-the-art Dissimilarity-based Sparse Subset Selection (DS3) method. This method selects a representative module subset from the prior version based on the pairwise dissimilarities between the modules of two versions and assigns each module of the current version to one of the representative modules. These selected modules can well represent the modules of the current version, thus mitigating the distribution differences. We evaluate the effectiveness of DS3 for CVDP performance on total 40 cross-version pairs from 56 versions of 15 projects with three traditional and two effort-aware indicators. The extensive experiments show that DS3 outperforms three baseline methods, especially in terms of two effort-aware indicators.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {132–143},
numpages = {12},
keywords = {cross version defect prediction, pairwise dissimilarities, representative data, sparse subset selection},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@proceedings{10.1145/3663529,
title = {FSE 2024: Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to FSE 2024, the ACM International Conference on the Foundations of Software Engineering (FSE) 2024. The conference now has a shorter name! FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {Porto de Galinhas, Brazil}
}

@inproceedings{10.1145/3540250.3549165,
author = {Ni, Chao and Wang, Wei and Yang, Kaiwen and Xia, Xin and Liu, Kui and Lo, David},
title = {The best of both worlds: integrating semantic features with expert features for defect prediction and localization},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549165},
doi = {10.1145/3540250.3549165},
abstract = {To improve software quality, just-in-time defect prediction (JIT-DP) (identifying defect-inducing commits) and just-in-time defect localization (JIT-DL) (identifying defect-inducing code lines in commits) have been widely studied by learning semantic features or expert features respectively, and indeed achieved promising performance. Semantic features and expert features describe code change commits from different aspects, however, the best of the two features have not been fully explored together to boost the just-in-time  
defect prediction and localization in the literature yet. Additional, JIT-DP identifies defects at the coarse commit level, while as the  
consequent task of JIT-DP, JIT-DL cannot achieve the accurate localization of defect-inducing code lines in a commit without JIT-DP.  
We hypothesize that the two JIT tasks can be combined together to boost the accurate prediction and localization of defect-inducing  
commits by integrating semantic features with expert features. Therefore, we propose to build a unified model, JIT-Fine, for the  
just-in-time defect prediction and localization by leveraging the best of semantic features and expert features. To assess the feasibility  
of JIT-Fine, we first build a large-scale line-level manually labeled dataset, JIT-Defects4J. Then, we make a comprehensive comparison  
with six state-of-the-art baselines under various settings using ten performance measures grouped into two types: effort-agnostic  
and effort-aware. The experimental results indicate that JIT-Fine can outperform all state-of-the-art baselines on both JIT-DP and JITDL  
tasks in terms of ten performance measures with a substantial improvement (i.e., 10%-629% in terms of effort-agnostic measures on JIT-DP, 5%-54% in terms of effort-aware measures on JIT-DP, and 4%-117% in terms of effort-aware measures on JIT-DL).},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {672–683},
numpages = {12},
keywords = {Just-In-Time, Defect Prediction, Defect Localization, Deep Learning},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3475716.3475791,
author = {Gesi, Jiri and Li, Jiawei and Ahmed, Iftekhar},
title = {An Empirical Examination of the Impact of Bias on Just-in-time Defect Prediction},
year = {2021},
isbn = {9781450386654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475716.3475791},
doi = {10.1145/3475716.3475791},
abstract = {Background: Just-In-Time (JIT) defect prediction models predict if a commit will introduce defects in the future. DeepJIT and CC2Vec are two state-of-the-art JIT Deep Learning (DL) techniques. Usually, defect prediction techniques are evaluated, treating all training data equally. However, data is usually imbalanced not only in terms of the overall class label (e.g., defect and non-defect) but also in terms of characteristics such as File Count, Edit Count, Multiline Comments, Inward Dependency Sum etc. Prior research has investigated the impact of class imbalance on prediction technique's performance but not the impact of imbalance of other characteristics. Aims: We aim to explore the impact of different commit related characteristic's imbalance on DL defect prediction. Method: We investigated different characteristic's impact on the overall performance of DeepJIT and CC2Vec. We also propose a Siamese network based few-shot learning framework for JIT defect prediction (SifterJIT) combining Siamese network and DeepJIT. Results: Our results show that DeepJIT and CC2Vec lose out on the performance by around 20% when trained and tested on imbalanced data. However, SifterJIT can outperform state-of-the-art DL techniques with an average of 8.65% AUC score, 11% precision, and 6% F1-score improvement. Conclusions: Our results highlight that dataset imbalanced in terms of commit characteristics can significantly impact prediction performance, and few-shot learning based techniques can help alleviate the situation.},
booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {7},
numpages = {12},
keywords = {software engineering, few-shot learning, defect prediction, Deep learning},
location = {Bari, Italy},
series = {ESEM '21}
}

@inproceedings{10.1145/2393596.2393619,
author = {Caglayan, Bora and Misirli, Ayse Tosun and Calikli, Gul and Bener, Ayse and Aytac, Turgay and Turhan, Burak},
title = {Dione: an integrated measurement and defect prediction solution},
year = {2012},
isbn = {9781450316149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393596.2393619},
doi = {10.1145/2393596.2393619},
abstract = {We present an integrated measurement and defect prediction tool: Dione. Our tool enables organizations to measure, monitor, and control product quality through learning based defect prediction. Similar existing tools either provide data collection and analytics, or work just as a prediction engine. Therefore, companies need to deal with multiple tools with incompatible interfaces in order to deploy a complete measurement and prediction solution. Dione provides a fully integrated solution where data extraction, defect prediction and reporting steps fit seamlessly. In this paper, we present the major functionality and architectural elements of Dione followed by an overview of our demonstration.},
booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
articleno = {20},
numpages = {2},
keywords = {software tool, software defect prediction, measurement},
location = {Cary, North Carolina},
series = {FSE '12}
}

@inproceedings{10.1109/ASE56229.2023.00163,
author = {He, Ye and Chen, Zimin and Goues, Claire Le},
title = {PreciseBugCollector: Extensible, Executable and Precise Bug-Fix Collection},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00163},
doi = {10.1109/ASE56229.2023.00163},
abstract = {Bug datasets are vital for enabling deep learning techniques to address software maintenance tasks related to bugs. However, existing bug datasets suffer from precise and scale limitations: they are either small-scale but precise with manual validation or large-scale but imprecise with simple commit message processing. In this paper, we introduce Precise-BugCollector, a precise, multi-language bug collection approach that overcomes these two limitations. PreciseBugCollector is based on two novel components: a) A bug tracker to map the codebase repositories with external bug repositories to trace bug type information, and b) A bug injector to generate project-specific bugs by injecting noise into the correct codebases and then executing them against their test suites to obtain test failure messages.We implement PreciseBugCollector against three sources: 1) A bug tracker that links to the national vulnerability data set (NVD) to collect general-wise vulnerabilities, 2) A bug tracker that links to OSS-Fuzz to collect general-wise bugs, and 3) A bug injector based on 16 injection rules to generate project-wise bugs. To date, PreciseBugCollector comprises 1 057 818 bugs extracted from 2 968 open-source projects. Of these, 12 602 bugs are sourced from bug repositories (NVD and OSS-Fuzz), while the remaining 1 045 216 project-specific bugs are generated by the bug injector. Considering the challenge objectives, we argue that a bug injection approach is highly valuable for the industrial setting, since project-specific bugs align with domain knowledge, share the same codebase, and adhere to the coding style employed in industrial projects.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1899–1910},
numpages = {12},
keywords = {bug datasets, program repair, software testing and debugging},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.5555/2486788.2486808,
author = {Cotroneo, Domenico and Pietrantuono, Roberto and Russo, Stefano},
title = {A learning-based method for combining testing techniques},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {This work presents a method to combine testing techniques adaptively during the testing process. It intends to mitigate the sources of uncertainty of software testing processes, by learning from past experience and, at the same time, adapting the technique selection to the current testing session. The method is based on machine learning strategies. It uses offline strategies to take historical information into account about the techniques performance collected in past testing sessions; then, online strategies are used to adapt the selection of test cases to the data observed as the testing proceeds. Experimental results show that techniques performance can be accurately characterized from features of the past testing sessions, by means of machine learning algorithms, and that integrating this result into the online algorithm allows improving the fault detection effectiveness with respect to single testing techniques, as well as to their random combination.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {142–151},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@proceedings{10.1145/3650212,
title = {ISSTA 2024: Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 33rd edition of the International Symposium on Software Testing and Analysis, ISSTA 2024, held on September 16--20, 2024 in Vienna, Austria. ISSTA 2024 is co-located with ECOOP and MPLR 2024. ISSTA brings together academics, industrial researchers, and practitioners from all over the world working on testing and analyzing software systems.},
location = {Vienna, Austria}
}

@article{10.1145/3447332.3447334,
author = {Islam, Md Rakibul and Zibran, Minhaz F.},
title = {What changes in where? an empirical study of bug-fixing change patterns},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1559-6915},
url = {https://doi.org/10.1145/3447332.3447334},
doi = {10.1145/3447332.3447334},
abstract = {A deep understanding of the common patterns of bug-fixing changes is useful in several ways: (a) such knowledge can help developers in proactively avoiding coding patterns that lead to bugs and (b) bug-fixing patterns are exploited in devising techniques for automatic bug localization and program repair.This work includes an in-depth quantitative and qualitative analysis over 4,653 buggy revisions of five software systems. Our study identifies 38 bug-fixing edit patterns and discovers 37 new patterns of nested code structures, which frequently host the bug-fixing edits. While some of the edit patterns were reported in earlier studies, these nesting patterns are new and were never targeted before.},
journal = {SIGAPP Appl. Comput. Rev.},
month = jan,
pages = {18–34},
numpages = {17},
keywords = {vulnerability, source code, software, pattern, nesting, fault, error, empirical study, edits, defect, bug, analysis}
}

@inproceedings{10.1145/3383219.3383243,
author = {Pham, Van and Lokan, Chris and Kasmarik, Kathryn},
title = {A Better Set of Object-Oriented Design Metrics for Within-Project Defect Prediction},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383243},
doi = {10.1145/3383219.3383243},
abstract = {Background: Using design metrics to predict fault-prone elements of a software design can help to focus attention on classes that need redesign and more extensive testing. However, some design metrics have been pointed out to be theoretically invalid, and the usefulness of some metrics is questioned.Aim: To identify a set of object-oriented metrics that are theoretically valid, and useful for identifying fault-prone classes in a design.Method: Drawing on four well-known sets of design metrics (CK, LK, MOOD and QMOOD), we propose a consolidated set of metrics that covers many aspects of object-oriented software design. We conduct two experiments, first using a single large system and then considering successive releases of that system, to compare the usefulness of the consolidated set with the other four sets for within-project prediction of fault-prone classes.Results: Both experiments suggest the consolidated set is effective at identifying fault-prone classes, outperforming the other metric sets (though at a cost of more false alarms).Conclusion: This paper adds to knowledge about the usefulness of existing sets of design metrics for within-project defect prediction, and identifies a consolidated set of metrics that is more effective than the existing sets at identifying fault-prone classes.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {230–239},
numpages = {10},
keywords = {fault-proneness prediction, design metrics, Object-oriented software design},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1145/3524842.3527949,
author = {Hin, David and Kan, Andrey and Chen, Huaming and Babar, M. Ali},
title = {LineVD: statement-level vulnerability detection using graph neural networks},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3527949},
doi = {10.1145/3524842.3527949},
abstract = {Current machine-learning based software vulnerability detection methods are primarily conducted at the function-level. However, a key limitation of these methods is that they do not indicate the specific lines of code contributing to vulnerabilities. This limits the ability of developers to efficiently inspect and interpret the predictions from a learnt model, which is crucial for integrating machine-learning based tools into the software development work-flow. Graph-based models have shown promising performance in function-level vulnerability detection, but their capability for statement-level vulnerability detection has not been extensively explored. While interpreting function-level predictions through explainable AI is one promising direction, we herein consider the statement-level software vulnerability detection task from a fully supervised learning perspective. We propose a novel deep learning framework, LineVD, which formulates statement-level vulnerability detection as a node classification task. LineVD leverages control and data dependencies between statements using graph neural networks, and a transformer-based model to encode the raw source code tokens. In particular, by addressing the conflicting outputs between function-level and statement-level information, LineVD significantly improve the prediction performance without vulnerability status for function code. We have conducted extensive experiments against a large-scale collection of real-world C/C++ vulnerabilities obtained from multiple real-world projects, and demonstrate an increase of 105% in F1-score over the current state-of-the-art.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {596–607},
numpages = {12},
keywords = {software vulnerability detection, program representation, deep learning},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1145/3297156.3297257,
author = {Ma, Zhen-Yu and Zhang, Wei and Wang, Jian-Ping and Liu, Fu-Sheng and Han, Kun and Gao, Fei},
title = {Research on a Method of Software Reliability Prediction Model},
year = {2018},
isbn = {9781450366069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297156.3297257},
doi = {10.1145/3297156.3297257},
abstract = {Software defects seriously affect software reliability. Software reliability is accurately predicted, which can guarantee normal use of software. For the prediction of software reliability, a software reliability prediction model based on support vector regression is constructed. Firstly, software metrics are extracted. Then data is preprocessed by the method of normalization and random sequence, and it is divided into training sets and testing sets for regression prediction; Secondly, the grid search method is introduced to optimize parameters in the support vector regression model; Finally, through experimental comparison and analysis, it is proved that this prediction method can indeed greatly improve the prediction accuracy of software reliability.},
booktitle = {Proceedings of the 2018 2nd International Conference on Computer Science and Artificial Intelligence},
pages = {163–167},
numpages = {5},
keywords = {support vector regression, software metric, reliability prediction, reliability index, grid search, defect density},
location = {Shenzhen, China},
series = {CSAI '18}
}

@inproceedings{10.1145/3579856.3582814,
author = {K\"{u}chler, Alexander and Wenning, Leon and Wendland, Florian},
title = {AbsIntIO: Towards Showing the Absence of Integer Overflows in Binaries using Abstract Interpretation},
year = {2023},
isbn = {9798400700989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579856.3582814},
doi = {10.1145/3579856.3582814},
abstract = {In the past years, the CWE-190 integer overflow led to many vulnerabilities. Program verification techniques such as Abstract Interpretation can show that no such bug is present in a given program. To date, such techniques often aim to verify the correctness of source code. However, as the source code is not always available or might not have been subject to such an analysis, it is advisable to apply abstract integer range analysis to the binary. However, analyzing binaries imposes other challenges which are not always addressed accurately by existing analysis tools. As an example, some tools fail to model bitwise operators, recover type information or do not account for compiler optimizations. We propose techniques to address these limitations and illustrate their effects in our configurable reference implementation AbsIntIO. AbsIntIO applies abstract integer range analysis to binaries with the goal to show that no integer overflow is possible. We evaluate the effects of the improvements and observed a reduction of the error rates. Hence, the improvements provide a step towards verifying the correctness of binaries.},
booktitle = {Proceedings of the 2023 ACM Asia Conference on Computer and Communications Security},
pages = {247–258},
numpages = {12},
keywords = {abstract interpretation, binary analysis, integer overflows},
location = {Melbourne, VIC, Australia},
series = {ASIA CCS '23}
}

@inproceedings{10.1145/3475960.3475985,
author = {Bhandari, Guru and Naseer, Amara and Moonen, Leon},
title = {CVEfixes: automated collection of vulnerabilities and their fixes from open-source software},
year = {2021},
isbn = {9781450386807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475960.3475985},
doi = {10.1145/3475960.3475985},
abstract = {Data-driven research on the automated discovery and repair of security vulnerabilities in source code requires comprehensive datasets of real-life vulnerable code and their fixes. To assist in such research, we propose a method to automatically collect and curate a comprehensive vulnerability dataset from Common Vulnerabilities and Exposures (CVE) records in the National Vulnerability Database (NVD). We implement our approach in a fully automated dataset collection tool and share an initial release of the resulting vulnerability dataset named CVEfixes. The CVEfixes collection tool automatically fetches all available CVE records from the NVD, gathers the vulnerable code and corresponding fixes from associated open-source repositories, and organizes the collected information in a relational database. Moreover, the dataset is enriched with meta-data such as programming language, and detailed code and security metrics at five levels of abstraction. The collection can easily be repeated to keep up-to-date with newly discovered or patched vulnerabilities. The initial release of CVEfixes spans all published CVEs up to 9 June 2021, covering 5365 CVE records for 1754 open-source projects that were addressed in a total of 5495 vulnerability fixing commits. CVEfixes supports various types of data-driven software security research, such as vulnerability prediction, vulnerability classification, vulnerability severity prediction, analysis of vulnerability-related code changes, and automated vulnerability repair.},
booktitle = {Proceedings of the 17th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {30–39},
numpages = {10},
keywords = {Security vulnerabilities, dataset, software repository mining, source code repair, vulnerability classification, vulnerability prediction},
location = {Athens, Greece},
series = {PROMISE 2021}
}

@inproceedings{10.1145/3452383.3452391,
author = {Kuri, Mohit and Karre, Sai Anirudh and Reddy, Y. Raghu},
title = {Understanding Software Quality Metrics for Virtual Reality Products - A Mapping Study},
year = {2021},
isbn = {9781450390460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452383.3452391},
doi = {10.1145/3452383.3452391},
abstract = {Virtual Reality (VR) Software is becoming more mainstream in recent years. It has provided an opportunity for VR practitioners to explore new domains and deliver cutting edge products. The success of the VR products depends primarily on the product contextual relevance and qualities exhibited. However, it is unclear how VR practitioners curb software quality challenges and improve the essence of the VR product over every release. In this paper, we present a Systematic Mapping Study of the software quality metrics adopted by VR practitioners for assessing the quality of their VR products. The study showed that practitioners used unique metrics to measure the quality of their VR products in addition to adopting some of existing enterprise software metrics. Further, we consolidate these metrics into different themes that future practitioners may use for developing VR products.},
booktitle = {Proceedings of the 14th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {8},
numpages = {11},
keywords = {Industrial Practices, Metrics, Software Quality, Virtual Reality},
location = {Bhubaneswar, Odisha, India},
series = {ISEC '21}
}

@article{10.1145/3442694,
author = {Bluemke, Ilona and Malanowska, Agnieszka},
title = {Software Testing Effort Estimation and Related Problems: A Systematic Literature Review},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3442694},
doi = {10.1145/3442694},
abstract = {Although testing effort estimation is a very important task in software project management, it is rarely described in the literature. There are many difficulties in finding any useful methods or tools for this purpose. Solutions to many other problems related to testing effort calculation are published much more often. There is also no research focusing on both testing effort estimation and all related areas of software engineering. To fill this gap, we performed a systematic literature review on both questions. Although our primary objective was to find some tools or implementable metods for test effort estimation, we have quickly discovered many other interesting topics related to the main one. The main contribution of this work is the presentation of the testing effort estimation task in a very wide context, indicating the relations with other research fields. This systematic literature review presents a detailed overview of testing effort estimation task, including challenges and approaches to automating it and the solutions proposed in the literature. It also exhaustively investigates related research topics, classifying publications that can be found in connection to the testing effort according to seven criteria formulated on the basis of our research questions. We present here both synthesis of our finding and the deep analysis of the stated research problems.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {53},
numpages = {38},
keywords = {testing effort estimation-related problems, testing effort estimation, systematic literature review, Testing effort}
}

@proceedings{10.1145/3590837,
title = {ICIMMI '22: Proceedings of the 4th International Conference on Information Management &amp; Machine Intelligence},
year = {2022},
isbn = {9781450399937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Jaipur, India}
}

@proceedings{10.1145/3597926,
title = {ISSTA 2023: Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to ISSTA 2023, the 32nd edition of the International Symposium on Software Testing and Analysis, to be held on July 18–20, 2023 in Seattle, USA. The symposium has become a premier scientific event in the expanding area of software testing and analysis, with a strong appeal to researchers from all continents.},
location = {Seattle, WA, USA}
}

@inproceedings{10.1145/2786805.2786813,
author = {Jing, Xiaoyuan and Wu, Fei and Dong, Xiwei and Qi, Fumin and Xu, Baowen},
title = {Heterogeneous cross-company defect prediction by unified metric representation and CCA-based transfer learning},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786813},
doi = {10.1145/2786805.2786813},
abstract = {Cross-company defect prediction (CCDP) learns a prediction model by using training data from one or multiple projects of a source company and then applies the model to the target company data. Existing CCDP methods are based on the assumption that the data of source and target companies should have the same software metrics. However, for CCDP, the source and target company data is usually heterogeneous, namely the metrics used and the size of metric set are different in the data of two companies. We call CCDP in this scenario as heterogeneous CCDP (HCCDP) task. In this paper, we aim to provide an effective solution for HCCDP. We propose a unified metric representation (UMR) for the data of source and target companies. The UMR consists of three types of metrics, i.e., the common metrics of the source and target companies, source-company specific metrics and target-company specific metrics. To construct UMR for source company data, the target-company specific metrics are set as zeros, while for UMR of the target company data, the source-company specific metrics are set as zeros. Based on the unified metric representation, we for the first time introduce canonical correlation analysis (CCA), an effective transfer learning method, into CCDP to make the data distributions of source and target companies similar. Experiments on 14 public heterogeneous datasets from four companies indicate that: 1) for HCCDP with partially different metrics, our approach significantly outperforms state-of-the-art CCDP methods; 2) for HCCDP with totally different metrics, our approach obtains comparable prediction performances in contrast with within-project prediction results. The proposed approach is effective for HCCDP.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {496–507},
numpages = {12},
keywords = {unified metric representation, company-specific metrics, common metrics, canonical correlation analysis (CCA), Heterogeneous cross-company defect prediction (HCCDP)},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/1985793.1986028,
author = {Kidwell, Billy},
title = {A decision support system for the classification of software coding faults: a research abstract},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1986028},
doi = {10.1145/1985793.1986028},
abstract = {A decision support system for fault classification is presented. The fault classification scheme is developed to provide guidance in process improvement and fault-based testing. The research integrates results in fault classification, source code analysis, and fault-based testing research. Initial results indicate that existing change type and fault classification schemes are insufficient for this purpose. Development of sufficient schemes and their evaluation are discussed.},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {1158–1160},
numpages = {3},
keywords = {software fault taxonomy, software evolution, object-oriented software, fault-based testing, fault model, decision support system},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@inproceedings{10.1145/3094243.3094245,
author = {Pang, Yulei and Xue, Xiaozhen and Wang, Huaying},
title = {Predicting Vulnerable Software Components through Deep Neural Network},
year = {2017},
isbn = {9781450352321},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3094243.3094245},
doi = {10.1145/3094243.3094245},
abstract = {Vulnerabilities need to be detected and removed from software. Although previous studies demonstrated the usefulness of employing prediction techniques in deciding about vulnerabilities of software components, the improvement of effectiveness of these prediction techniques is still a grand challenging research question. This paper employed a technique based on a deep neural network with rectifier linear units trained with stochastic gradient descent method and batch normalization, for predicting vulnerable software components. The features are defined as continuous sequences of tokens in source code files. Besides, a statistical feature selection algorithm is then employed to reduce the feature and search space. We evaluated the proposed technique based on some Java Android applications, and the results demonstrated that the proposed technique could predict vulnerable classes, i.e., software components, with high precision, accuracy and recall.},
booktitle = {Proceedings of the 2017 International Conference on Deep Learning Technologies},
pages = {6–10},
numpages = {5},
keywords = {vulnerability prediction, neural network, deep learning, Android},
location = {Chengdu, China},
series = {ICDLT '17}
}

@inproceedings{10.5555/2818754.2818850,
author = {Ghotra, Baljinder and McIntosh, Shane and Hassan, Ahmed E.},
title = {Revisiting the impact of classification techniques on the performance of defect prediction models},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Defect prediction models help software quality assurance teams to effectively allocate their limited resources to the most defect-prone software modules. A variety of classification techniques have been used to build defect prediction models ranging from simple (e.g., logistic regression) to advanced techniques (e.g., Multivariate Adaptive Regression Splines (MARS)). Surprisingly, recent research on the NASA dataset suggests that the performance of a defect prediction model is not significantly impacted by the classification technique that is used to train it. However, the dataset that is used in the prior study is both: (a) noisy, i.e., contains erroneous entries and (b) biased, i.e., only contains software developed in one setting. Hence, we set out to replicate this prior study in two experimental settings. First, we apply the replicated procedure to the same (known-to-be noisy) NASA dataset, where we derive similar results to the prior study, i.e., the impact that classification techniques have appear to be minimal. Next, we apply the replicated procedure to two new datasets: (a) the cleaned version of the NASA dataset and (b) the PROMISE dataset, which contains open source software developed in a variety of settings (e.g., Apache, GNU). The results in these new datasets show a clear, statistically distinct separation of groups of techniques, i.e., the choice of classification technique has an impact on the performance of defect prediction models. Indeed, contrary to earlier research, our results suggest that some classification techniques tend to produce defect prediction models that outperform others.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {789–800},
numpages = {12},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/3524610.3527902,
author = {Widyasari, Ratnadira and Prana, Gede Artha Azriadi and Haryono, Stefanus A. and Tian, Yuan and Zachiary, Hafil Noer and Lo, David},
title = {XAI4FL: enhancing spectrum-based fault localization with explainable artificial intelligence},
year = {2022},
isbn = {9781450392983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524610.3527902},
doi = {10.1145/3524610.3527902},
abstract = {Manually finding the program unit (e.g., class, method, or statement) responsible for a fault is tedious and time-consuming. To mitigate this problem, many fault localization techniques have been proposed. A popular family of such techniques is spectrum-based fault localization (SBFL), which takes program execution traces (spectra) of failed and passed test cases as input and applies a ranking formula to compute a suspiciousness score for each program unit. However, most existing SBFL techniques fail to consider two facts: 1) not all failed test cases contribute equally to a considered fault(s), and 2) program units collaboratively contribute to the failure/pass of each test case in different ways.In this study, we propose a novel idea that first models the SBFL task as a classification problem of predicting whether a test case will fail or pass based on spectra information on program units. We subsequently apply eXplainable Artificial Intelligence (XAI) techniques to infer the local importance of each program unit to the prediction of each executed test case. Applying XAI to the failed test case, we retrieve information about which program statements within the test case that are considered the most important (i.e., have the biggest effect in making the test case failed). Such a design can automatically learn the unique contributions of failed test cases to the suspiciousness of a program unit by learning the different and collaborative contributions of program units to each test case's executed result. As far as we know, this is the first XAI-supported SBFL approach. We evaluate the new approach on the Defects4J benchmark dataset.We compare the performance of our approach against five popular SBFL techniques: DStar, Tarantula, Barinel, Ochiai, and OP. We measure their performance using the Top-K and EXAM scores. In particular, we focus on the result of the Top-1, which importance has been highlighted in automated program repair domain, where the proposed methods often assume perfect fault localization (i.e., the fault must be found at the first rank of the suspiciousness list). Our results show that our approach, named XAI4FL, has a statistically significant and substantially better performance in terms of Top-1 than the SBFL approaches. We also compare our approach with a simpler approach to get feature importance in a tree-based model (i.e., using the Mean Decrease in Impurity method). Our results show that XAI4FL statistically significantly outperforms the MDI method in Top-K and EXAM score. Our results and findings highlight that the utilization of XAI for fault localization can improve the overall results of fault localization techniques.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension},
pages = {499–510},
numpages = {12},
keywords = {testing and debugging, spectrum-based fault localization, model-agnostic explanation technique, fault localization, explainable artificial intelligence (XAI)},
location = {Virtual Event},
series = {ICPC '22}
}

@inproceedings{10.1145/3464298.3493400,
author = {Tak, Byungchul and Han, Wook-Shin},
title = {Lognroll: discovering accurate log templates by iterative filtering},
year = {2021},
isbn = {9781450385343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3464298.3493400},
doi = {10.1145/3464298.3493400},
abstract = {Modern IT systems rely heavily on log analytics for critical operational tasks. Since the volume of logs produced from numerous distributed components is overwhelming, it requires us to employ automated processing. The first step of automated log processing is to convert streams of log lines into the sequence of log format IDs, called log templates. A log template serves as a base string with unfilled parts from which logs are generated during runtime by substitution of contextual information. The problem of log template discovery from the volume of collected logs poses a great challenge due to the semi-structured nature of the logs and the computational overheads. Our investigation reveals that existing techniques show various limitations. We approach the log template discovery problem as search-based learning by applying the ILP (Inductive Logic Programming) framework. The algorithm core consists of narrowing down the logs into smaller sets by analyzing value compositions on selected log column positions. Our evaluation shows that it produces accurate log templates from diverse application logs with small computational costs compared to existing methods. With the quality metric we defined, we obtained about 21%-51% improvements of log template quality.},
booktitle = {Proceedings of the 22nd International Middleware Conference},
pages = {273–285},
numpages = {13},
keywords = {sequential covering, log template, log analysis},
location = {Qu\'{e}bec city, Canada},
series = {Middleware '21}
}

@inproceedings{10.1109/SC41406.2024.00023,
author = {Devarajan, Hariharan and Pottier, Lo\"{\i}c and Velusamy, Kaushik and Zheng, Huihuo and Yildirim, Izzet and Kogiou, Olga and Yu, Weikuan and Kougkas, Anthony and Sun, Xian-He and Yeom, Jae Seung and Mohror, Kathryn},
title = {DFTracer: An Analysis-Friendly Data Flow Tracer for AI-Driven Workflows},
year = {2024},
isbn = {9798350352917},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC41406.2024.00023},
doi = {10.1109/SC41406.2024.00023},
abstract = {Modern HPC workflows involve intricate coupling of simulation, data analytics, and artificial intelligence (AI) applications to improve time to scientific insight. These workflows require a cohesive set of performance analysis tools to provide a comprehensive understanding of data exchange patterns in HPC systems. However, current tools are not designed to work with an AI-based I/O software stack that requires tracing at multiple levels of the application. To this end, we developed a data flow tracer called DFTracer to capture data-centric events from workflows and the I/O stack to build a detailed understanding of the data exchange within AI-driven workflows. DFTracer has the following three novel features, including a unified interface to capture trace data from different layers in the software stack, a trace format that is analysis-friendly and optimized to support efficiently loading multi-million events in a few seconds, and the capability to tag events with workflow-specific context to perform domain-centric data flow analysis for workflows. Additionally, we demonstrate that DFTracer has a 1.44x smaller runtime overhead and 1.3-7.1x smaller trace size than state-of-the-art tracing tools such as Score-P, Recorder, and Darshan. Moreover, with AI-driven workflows, Score-P, Recorder, and Darshan cannot find I/O accesses from dynamically spawned processes, and their load performance of 100M events is three orders of magnitude slower than DFTracer. In conclusion, we demonstrate that DFTracer can capture multi-level performance data, including contextual event tagging with a low overhead of 1-5% from AI-driven workflows such as MuMMI and Microsoft's Megatron Deepspeed running on large-scale HPC systems.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {17},
numpages = {24},
keywords = {I/O, application apis, deep learning, interception, multilevel, system calls, tracer, transparent, workflows},
location = {Atlanta, GA, USA},
series = {SC '24}
}

@inproceedings{10.1145/3368089.3409764,
author = {Mahajan, Sonal and Abolhassani, Negarsadat and Prasad, Mukul R.},
title = {Recommending stack overflow posts for fixing runtime exceptions using failure scenario matching},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409764},
doi = {10.1145/3368089.3409764},
abstract = {Using online Q&amp;A forums, such as Stack Overflow (SO), for guidance to resolve program bugs, among other development issues, is commonplace in modern software development practice. Runtime exceptions (RE) is one such important class of bugs that is actively discussed on SO. In this work we present a technique and prototype tool called MAESTRO that can automatically recommend an SO post that is most relevant to a given Java RE in a developer's code. MAESTRO compares the exception-generating program scenario in the developer's code with that discussed in an SO post and returns the post with the closest match. To extract and compare the exception scenario effectively, MAESTRO first uses the answer code snippets in a post to implicate a subset of lines in the post's question code snippet as responsible for the exception and then compares these lines with the developer's code in terms of their respective Abstract Program Graph (APG) representations. The APG is a simplified and abstracted derivative of an abstract syntax tree, proposed in this work, that allows an effective comparison of the functionality embodied in the high-level program structure, while discarding many of the low-level syntactic or semantic differences. We evaluate MAESTRO on a benchmark of 78 instances of Java REs extracted from the top 500 Java projects on GitHub and show that MAESTRO can return either a highly relevant or somewhat relevant SO post corresponding to the exception instance in 71% of the cases, compared to relevant posts returned in only 8% - 44% instances, by four competitor tools based on state-of-the-art techniques. We also conduct a user experience study of MAESTRO with 10 Java developers, where the participants judge MAESTRO reporting a highly relevant or somewhat relevant post in 80% of the instances. In some cases the post is judged to be even better than the one manually found by the participant.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1052–1064},
numpages = {13},
keywords = {static analysis, runtime exceptions, crowd intelligence, code search},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3319008.3319716,
author = {Alsolai, Hadeel and Roper, Marc},
title = {Application of Ensemble Techniques in Predicting Object-Oriented Software Maintainability},
year = {2019},
isbn = {9781450371452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319008.3319716},
doi = {10.1145/3319008.3319716},
abstract = {While prior object-oriented software maintainability literature acknowledges the role of machine learning techniques as valuable predictors of potential change, the most suitable technique that achieves consistently high accuracy remains undetermined. With the objective of obtaining more consistent results, an ensemble technique is investigated to advance the performance of the individual models and increase their accuracy in predicting software maintainability of the object-oriented system. This paper describes the research plan for predicting object-oriented software maintainability using ensemble techniques. First, we present a brief overview of the main research background and its different components. Second, we explain the research methodology. Third, we provide expected results. Finally, we conclude summary of the current status.},
booktitle = {Proceedings of the 23rd International Conference on Evaluation and Assessment in Software Engineering},
pages = {370–373},
numpages = {4},
keywords = {software maintainability, prediction, individual model, ensemble model, Object-oriented system},
location = {Copenhagen, Denmark},
series = {EASE '19}
}

@inproceedings{10.1145/2639490.2639504,
author = {Russo, Barbara},
title = {A proposed method to evaluate and compare fault predictions across studies},
year = {2014},
isbn = {9781450328982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2639490.2639504},
doi = {10.1145/2639490.2639504},
abstract = {Studies on fault prediction often pay little attention to empirical rigor and presentation. Researchers might not have full command over the statistical method they use, full understanding of the data they have, or tend not to report key details about their work. What does it happen when we want to compare such studies for building a theory on fault prediction? There are two issues that if not addressed, we believe, prevent building such theory. The first concerns how to compare and report prediction performance across studies on different data sets. The second regards fitting performance of prediction models. Studies tend not to control and report the performance of predictors on historical data underestimating the risk that good predictors may poorly perform on past data. The degree of both fitting and prediction performance determines the risk managers are requested to take when they use such predictors. In this work, we propose a framework to compare studies on categorical fault prediction that aims at addressing the two issues. We propose three algorithms that automate our framework. We finally review baseline studies on fault prediction to discuss the application of the framework.},
booktitle = {Proceedings of the 10th International Conference on Predictive Models in Software Engineering},
pages = {2–11},
numpages = {10},
keywords = {confusion matrix, fault, machine learning, model comparison},
location = {Turin, Italy},
series = {PROMISE '14}
}

@inproceedings{10.1145/2889160.2889256,
author = {Tantithamthavorn, Chakkrit},
title = {Towards a better understanding of the impact of experimental components on defect prediction modelling},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889256},
doi = {10.1145/2889160.2889256},
abstract = {Defect prediction models are used to pinpoint risky software modules and understand past pitfalls that lead to defective modules. The predictions and insights that are derived from defect prediction models may not be accurate and reliable if researchers do not consider the impact of experimental components (e.g., datasets, metrics, and classifiers) of defect prediction modelling. Therefore, a lack of awareness and practical guidelines from previous research can lead to invalid predictions and unreliable insights. In this thesis, we investigate the impact that experimental components have on the predictions and insights of defect prediction models. Through case studies of systems that span both proprietary and open-source domains, we find that (1) noise in defect datasets; (2) parameter settings of classification techniques; and (3) model validation techniques have a large impact on the predictions and insights of defect prediction models, suggesting that researchers should carefully select experimental components in order to produce more accurate and reliable defect prediction models.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {867–870},
numpages = {4},
keywords = {experimental components, defect prediction modelling},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/3273934.3273938,
author = {Amasaki, Sousuke},
title = {Cross-Version Defect Prediction using Cross-Project Defect Prediction Approaches: Does it work?},
year = {2018},
isbn = {9781450365932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3273934.3273938},
doi = {10.1145/3273934.3273938},
abstract = {Background: Specifying and removing defects before release deserve extra cost for the success of software projects. Long-running projects experience multiple releases, and it is a natural choice to adopt cross-version defect prediction (CVDP) that uses information from older versions. A past study shows that feeding multi older versions data may have a positive influence on the performance. The study also suggests that cross-project defect prediction (CPDP) may fit the situation but one CPDP approach was only examined.Aims: To investigate whether feeding multiple older versions data is effective for CVDP using CPDP approaches. The investigation also involves performance comparisons of the CPDP approaches under CVDP situation. Method: We chose a style of replication of the comparative study on CPDP approaches by Herbold et al. under CVDP situation. Results: Feeding multiple older versions had a positive effect for more than a half CPDP approaches. However, almost all of the CPDP approaches did not perform significantly better than a simple rule-based prediction. Although the best CPDP approach could work better than it and with-in project defect prediction, we found no effect of feeding multiple older versions for it. Conclusions: Feeding multiple older versions could improve CPDP approaches under CVDP situation. However, it did not work for the best CPDP approach in the study.},
booktitle = {Proceedings of the 14th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {32–41},
numpages = {10},
keywords = {Comparative Study, Cross-Project Defect Prediction, Cross-Version Defect Prediction},
location = {Oulu, Finland},
series = {PROMISE'18}
}

@inproceedings{10.1145/3106237.3106258,
author = {Wang, Song and Nam, Jaechang and Tan, Lin},
title = {QTEP: quality-aware test case prioritization},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106258},
doi = {10.1145/3106237.3106258},
abstract = {Test case prioritization (TCP) is a practical activity in software testing for exposing faults earlier. Researchers have proposed many TCP techniques to reorder test cases. Among them, coverage-based TCPs have been widely investigated. Specifically, coverage-based TCP approaches leverage coverage information between source code and test cases, i.e., static code coverage and dynamic code coverage, to schedule test cases. Existing coverage-based TCP techniques mainly focus on maximizing coverage while often do not consider the likely distribution of faults in source code. However, software faults are not often equally distributed in source code, e.g., around 80% faults are located in about 20% source code. Intuitively, test cases that cover the faulty source code should have higher priorities, since they are more likely to find faults.  In this paper, we present a quality-aware test case prioritization technique, QTEP, to address the limitation of existing coverage-based TCP algorithms. In QTEP, we leverage code inspection techniques, i.e., a typical statistic defect prediction model and a typical static bug finder, to detect fault-prone source code and then adapt existing coverage-based TCP algorithms by considering the weighted source code in terms of fault-proneness. Our evaluation with 16 variant QTEP techniques on 33 different versions of 7 open source Java projects shows that QTEP could improve existing coverage-based TCP techniques for both regression and new test cases. Specifically, the improvement of the best variant of QTEP for regression test cases could be up to 15.0% and on average 7.6%, and for all test cases (both regression and new test cases), the improvement could be up to 10.0% and on average 5.0%.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {523–534},
numpages = {12},
keywords = {static bug finder, defect prediction, Test case prioritization},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1109/ESEM.2017.48,
author = {Yan, Meng and Fang, Yicheng and Lo, David and Xia, Xin and Zhang, Xiaohong},
title = {File-level defect prediction: unsupervised vs. supervised models},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.48},
doi = {10.1109/ESEM.2017.48},
abstract = {Background: Software defect models can help software quality assurance teams to allocate testing or code review resources. A variety of techniques have been used to build defect prediction models, including supervised and unsupervised methods. Recently, Yang et al. [1] surprisingly find that unsupervised models can perform statistically significantly better than supervised models in effort-aware change-level defect prediction. However, little is known about relative performance of unsupervised and supervised models for effort-aware file-level defect prediction. Goal: Inspired by their work, we aim to investigate whether a similar finding holds in effort-aware file-level defect prediction. Method: We replicate Yang et al.'s study on PROMISE dataset with totally ten projects. We compare the effectiveness of unsupervised and supervised prediction models for effort-aware file-level defect prediction. Results: We find that the conclusion of Yang et al. [1] does not hold under within-project but holds under cross-project setting for file-level defect prediction. In addition, following the recommendations given by the best unsupervised model, developers needs to inspect statistically significantly more files than that of supervised models considering the same inspection effort (i.e., LOC). Conclusions: (a) Unsupervised models do not perform statistically significantly better than state-of-art supervised model under within-project setting, (b) Unsupervised models can perform statistically significantly better than state-of-art supervised model under cross-project setting, (c) We suggest that not only LOC but also number of files needed to be inspected should be considered when evaluating effort-aware file-level defect prediction models.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {344–353},
numpages = {10},
keywords = {replication study, inspection effort, effort-aware defect prediction},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@inproceedings{10.1145/2961111.2962601,
author = {Soltanifar, Behjat and Erdem, Atakan and Bener, Ayse},
title = {Predicting Defectiveness of Software Patches},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962601},
doi = {10.1145/2961111.2962601},
abstract = {Context: Software code review, as an engineering best practice, refers to the inspection of the code change in order to find possible defects and ensure change quality. Code reviews, however, may not guarantee finding the defects. Thus, there is a risk for a defective code change in a given patch, to pass the review process and be submitted.Goal: In this research, we aim to apply different machine learning algorithms in order to predict the defectiveness of a patch after being reviewed, at the time of its submission.Method: We built three models using three different machine learning algorithms: Logistic Regression, Na\~{A}undefinedve Bayes, and Bayesian Network model. To build the models, we consider different factors involved in review process in terms of Product, Process and People (3P).Results: Our empirical results show that, Bayesian Networks is able to better predict the defectiveness of the changed code with 76% accuracy.Conclusions: Predicting defectiveness of change code is beneficial in making patch release decisions. The Bayesian Network model outperforms the others since it capturs the relationship among the factors in the review process.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {22},
numpages = {10},
keywords = {Software Patch Defectiveness, Defect Prediction, Code review, Code Review Quality},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@inproceedings{10.1145/3379597.3387484,
author = {Cor\`{o}, Federico and Verdecchia, Roberto and Cruciani, Emilio and Miranda†, Breno and Bertolino, Antonia},
title = {JTeC: A Large Collection of Java Test Classes for Test Code Analysis and Processing},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387484},
doi = {10.1145/3379597.3387484},
abstract = {The recent push towards test automation and test-driven development continues to scale up the dimensions of test code that needs to be maintained, analysed, and processed side-by-side with production code. As a consequence, on the one side regression testing techniques, e.g., for test suite prioritization or test case selection, capable to handle such large-scale test suites become indispensable; on the other side, as test code exposes own characteristics, specific techniques for its analysis and refactoring are actively sought. We present JTeC, a large-scale dataset of test cases that researchers can use for benchmarking the above techniques or any other type of tool expressly targeting test code. JTeC collects more than 2.5M test classes belonging to 31K+ GitHub projects and summing up to more than 430 Million SLOCs of ready-to-use real-world test code.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {578–582},
numpages = {5},
keywords = {Test Suite, Software Testing, Large Scale, Java, GitHub},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/581339.581371,
author = {Denaro, Giovanni and Pezz\`{e}, Mauro},
title = {An empirical evaluation of fault-proneness models},
year = {2002},
isbn = {158113472X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/581339.581371},
doi = {10.1145/581339.581371},
abstract = {Planning and allocating resources for testing is difficult and it is usually done on empirical basis, often leading to unsatisfactory results. The possibility of early estimating the potential faultiness of software could be of great help for planning and executing testing activities. Most research concentrates on the study of different techniques for computing multivariate models and evaluating their statistical validity, but we still lack experimental data about the validity of such models across different software applications.This paper reports an empirical study of the validity of multivariate models for predicting software fault-proneness across different applications. It shows that suitably selected multivariate models can predict fault-proneness of modules of different software packages.},
booktitle = {Proceedings of the 24th International Conference on Software Engineering},
pages = {241–251},
numpages = {11},
keywords = {testing process, software process, software metrics, software faultiness, principal component analysis, logistic regression, fault-proneness models, cross-validation},
location = {Orlando, Florida},
series = {ICSE '02}
}

@inproceedings{10.1145/2811411.2811544,
author = {Siebra, Clauirton A. and Mello, Michael A. B.},
title = {The importance of replications in software engineering: a case study in defect prediction},
year = {2015},
isbn = {9781450337380},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811411.2811544},
doi = {10.1145/2811411.2811544},
abstract = {Prediction of defects in software is an important investigation area in software engineering, since such technique is able to return indications of parts of the code that are prone to contain problems. Thus, test teams can optimize the allocation of their resources by directing them to modules that are more defect-prone. The use of supervised learning is one of the approaches to support the design of prediction models. However, the erroneous use of training datasets can lead to poor models and, consequently, false results regarding accuracy. This work replicates important experiments of the area and shows how they could provide reliable results via the use of simple techniques of pre-processing. Based on the results, we discuss the importance of replications as method to find problems in current results and how this method is being motivated inside the software engineering area.},
booktitle = {Proceedings of the 2015 Conference on Research in Adaptive and Convergent Systems},
pages = {376–381},
numpages = {6},
keywords = {supervised learning, replication, defect prediction},
location = {Prague, Czech Republic},
series = {RACS '15}
}

@inproceedings{10.1145/3167132.3167299,
author = {Azizi, Maral and Do, Hyunsook},
title = {A collaborative filtering recommender system for test case prioritization in web applications},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167299},
doi = {10.1145/3167132.3167299},
abstract = {The use of relevant metrics of software systems could improve various software engineering tasks, but identifying relationships among metrics is not simple and can be very time consuming. Recommender systems can help with this decision-making process; many applications have utilized these systems to improve the performance of their applications. To investigate the potential benefits of recommender systems in regression testing, we implemented an item-based collaborative filtering recommender system that uses user interaction data and application change history information to develop a test case prioritization technique. To evaluate our approach, we performed an empirical study using three web applications with multiple versions and compared four control techniques. Our results indicate that our recommender system can help improve the effectiveness of test prioritization.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1560–1567},
numpages = {8},
keywords = {test case prioritization, risk measurement, regression testing, recommender system},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/2786805.2786814,
author = {Nam, Jaechang and Kim, Sunghun},
title = {Heterogeneous defect prediction},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786814},
doi = {10.1145/2786805.2786814},
abstract = {Software defect prediction is one of the most active research areas in software engineering. We can build a prediction model with defect data collected from a software project and predict defects in the same project, i.e. within-project defect prediction (WPDP). Researchers also proposed cross-project defect prediction (CPDP) to predict defects for new projects lacking in defect data by using prediction models built by other projects. In recent studies, CPDP is proved to be feasible. However, CPDP requires projects that have the same metric set, meaning the metric sets should be identical between projects. As a result, current techniques for CPDP are difficult to apply across projects with heterogeneous metric sets. To address the limitation, we propose heterogeneous defect prediction (HDP) to predict defects across projects with heterogeneous metric sets. Our HDP approach conducts metric selection and metric matching to build a prediction model between projects with heterogeneous metric sets. Our empirical study on 28 subjects shows that about 68% of predictions using our approach outperform or are comparable to WPDP with statistical significance.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {508–519},
numpages = {12},
keywords = {quality assurance, heterogeneous metrics, Defect prediction},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/2245276.2231967,
author = {Sarro, F. and Di Martino, S. and Ferrucci, F. and Gravino, C.},
title = {A further analysis on the use of Genetic Algorithm to configure Support Vector Machines for inter-release fault prediction},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2231967},
doi = {10.1145/2245276.2231967},
abstract = {Some studies have reported promising results on the use of Support Vector Machines (SVMs) for predicting fault-prone software components. Nevertheless, the performance of the method heavily depends on the setting of some parameters. To address this issue, we investigated the use of a Genetic Algorithm (GA) to search for a suitable configuration of SVMs to be used for inter-release fault prediction. In particular, we report on an assessment of the method on five software systems. As benchmarks we exploited SVMs with random and Grid-search configuration strategies and several other machine learning techniques. The results show that the combined use of GA and SVMs is effective for inter-release fault prediction.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1215–1220},
numpages = {6},
keywords = {support vector machines, genetic algorithm, fault prediction},
location = {Trento, Italy},
series = {SAC '12}
}

@inproceedings{10.1145/2610384.2610419,
author = {Marinescu, Paul and Hosek, Petr and Cadar, Cristian},
title = {Covrig: a framework for the analysis of code, test, and coverage evolution in real software},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610419},
doi = {10.1145/2610384.2610419},
abstract = {Software repositories provide rich information about the construction and evolution of software systems. While static data that can be mined directly from version control systems has been extensively studied, dynamic metrics concerning the execution of the software have received much less attention, due to the inherent difficulty of running and monitoring a large number of software versions. In this paper, we present Covrig, a flexible infrastructure that can be used to run each version of a system in isolation and collect static and dynamic software metrics, using a lightweight virtual machine environment that can be deployed on a cluster of local or cloud machines. We use Covrig to conduct an empirical study examining how code and tests co-evolve in six popular open-source systems. We report the main characteristics of software patches, analyse the evolution of program and patch coverage, assess the impact of nondeterminism on the execution of test suites, and investigate whether the coverage of code containing bugs and bug fixes is higher than average.},
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {93–104},
numpages = {12},
keywords = {nondeterministic coverage, latent patch cover- age, coverage evolution, bugs and fixes, Patch characteristics},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@proceedings{10.1145/3611643,
title = {ESEC/FSE 2023: Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to ESEC/FSE 2023, the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ESEC/FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. ESEC/FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {San Francisco, CA, USA}
}

@inproceedings{10.1145/2896839.2896843,
author = {Koroglu, Yavuz and Sen, Alper and Kutluay, Doruk and Bayraktar, Akin and Tosun, Yalcin and Cinar, Murat and Kaya, Hasan},
title = {Defect prediction on a legacy industrial software: a case study on software with few defects},
year = {2016},
isbn = {9781450341547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896839.2896843},
doi = {10.1145/2896839.2896843},
abstract = {Context: Building defect prediction models for software projects is helpful for reducing the effort in locating defects. In this paper, we share our experiences in building a defect prediction model for a large industrial software project. We extract product and process metrics to build models and show that we can build an accurate defect prediction model even when 4% of the software is defective.Objective: Our goal in this project is to integrate a defect predictor into the continuous integration (CI) cycle of a large software project and decrease the effort in testing.Method: We present our approach in the form of an experience report. Specifically, we collected data from seven older versions of the software project and used additional features to predict defects of current versions. We compared several classification techniques including Naive Bayes, Decision Trees, and Random Forest and resampled our training data to present the company with the most accurate defect predictor.Results: Our results indicate that we can focus testing efforts by guiding the test team to only 8% of the software where 53% of actual defects can be found. Our model has 90% accuracy.Conclusion: We produce a defect prediction model with high accuracy for a software with defect rate of 4%. Our model uses Random Forest, that which we show has more predictive power than Naive Bayes, Logistic Regression and Decision Trees in our case.},
booktitle = {Proceedings of the 4th International Workshop on Conducting Empirical Studies in Industry},
pages = {14–20},
numpages = {7},
keywords = {random forest, process metrics, feature selection, experience report, defect prediction},
location = {Austin, Texas},
series = {CESI '16}
}

@inproceedings{10.1145/3324884.3416622,
author = {Roy, Devjeet and Zhang, Ziyi and Ma, Maggie and Arnaoudova, Venera and Panichella, Annibale and Panichella, Sebastiano and Gonzalez, Danielle and Mirakhorli, Mehdi},
title = {DeepTC-enhancer: improving the readability of automatically generated tests},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416622},
doi = {10.1145/3324884.3416622},
abstract = {Automated test case generation tools have been successfully proposed to reduce the amount of human and infrastructure resources required to write and run test cases. However, recent studies demonstrate that the readability of generated tests is very limited due to (i) uninformative identifiers and (ii) lack of proper documentation. Prior studies proposed techniques to improve test readability by either generating natural language summaries or meaningful methods names. While these approaches are shown to improve test readability, they are also affected by two limitations: (1) generated summaries are often perceived as too verbose and redundant by developers, and (2) readable tests require both proper method names but also meaningful identifiers (within-method readability).In this work, we combine template based methods and Deep Learning (DL) approaches to automatically generate test case scenarios (elicited from natural language patterns of test case statements) as well as to train DL models on path-based representations of source code to generate meaningful identifier names. Our approach, called DeepTC-Enhancer, recommends documentation and identifier names with the ultimate goal of enhancing readability of automatically generated test cases.An empirical evaluation with 36 external and internal developers shows that (1) DeepTC-Enhancer outperforms significantly the baseline approach for generating summaries and performs equally with the baseline approach for test case renaming, (2) the transformation proposed by DeepTC-Enhancer results in a significant increase in readability of automatically generated test cases, and (3) there is a significant difference in the feature preferences between external and internal developers.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {287–298},
numpages = {12},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1145/3461731,
author = {Weninger, Markus and Gander, Elias and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {Guided Exploration: A Method for Guiding Novice Users in Interactive Memory Monitoring Tools},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {EICS},
url = {https://doi.org/10.1145/3461731},
doi = {10.1145/3461731},
abstract = {Many monitoring tools that help developers in analyzing the run-time behavior of their applications share a common shortcoming: they require their users to have a fair amount of experience in monitoring applications to understand the used terminology and the available analysis features. Consequently, novice users who lack this knowledge often struggle to use these tools efficiently. In this paper, we introduce the guided exploration (GE) method that aims to make interactive monitoring tools easier to use and learn. In general, tools that implement GE should provide four support operations on each analysis step: they should automatically (1) detect and (2) highlight the most important information on the screen, (3) explain why it is important, and (4) suggest which next steps are appropriate. This way, tools guide users through their analysis processes, helping them to explore the root cause of a problem. At the same time, users learn the capabilities of the tool and how to use them efficiently. We show how GE can be implemented in new monitoring tools as well as how it can be integrated into existing ones. To demonstrate GE's feasibility and usefulness, we present how we extended the memory monitoring tool AntTracks to provided guided exploration support during memory leak analysis and memory churn analysis. We use these guidances in two user scenarios to inspect and improve the memory behavior of the monitored applications. We hope that our contribution will help usability researchers and developers in making monitoring tools more novice-friendly by improving their usability and learnability.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {209},
numpages = {34},
keywords = {user guidance, usability, program comprehension, onboarding, monitoring tools, memory monitoring, memory leak analysis, memory comprehension, memory churn analysis, learnability, intelligent assistant, guided exploration, advisor, context-sensitive help}
}

@article{10.1145/3554976,
author = {Baldassarre, Maria Teresa and Ernst, Neil and Hermann, Ben and Menzies, Tim and Yedida, Rahul},
title = {(Re)Use of Research Results (Is Rampant)},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/3554976},
doi = {10.1145/3554976},
abstract = {Prior pessimism about reuse in software engineering research may have been a result of using the wrong methods to measure the wrong things.},
journal = {Commun. ACM},
month = jan,
pages = {75–81},
numpages = {7}
}

@inproceedings{10.1145/2771783.2784776,
author = {Rolfsnes, Thomas},
title = {Mining change history for test-plan generation (doctoral symposium)},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2784776},
doi = {10.1145/2771783.2784776},
abstract = {Regression testing is an essential step in safeguarding the evolution of a system, yet there is often not enough time to exercise all available tests. Identifying the subset of tests that can reveal potential issues introduced by a change is a challenge. It requires identifying the tests that test the chan- ged part of the software. Furthermore, and more challeng- ing, it requires identifying the parts of the system that are potentially affected by that change, a task typically done by means of static program analysis. In this doctoral research, we investigate an alternative approach, using software repos- itory mining. We propose a method that mines the change history of a system to uncover dependencies, and uses these for test-selection and test-prioritization. By reducing the amount of test to exercise, and limiting time spend on test- plan creation (i.e., selecting and prioritizing tests), the aim of our approach is to increase cost-effectiveness of software regression testing.},
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {444–447},
numpages = {4},
keywords = {test case selection, test case prioritiza- tion, targeted association rule mining, regression testing, recommender system, evolutionary couplings, dependency analysis},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@inproceedings{10.1145/2991079.2991103,
author = {Pewny, Jannik and Holz, Thorsten},
title = {EvilCoder: automated bug insertion},
year = {2016},
isbn = {9781450347716},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2991079.2991103},
doi = {10.1145/2991079.2991103},
abstract = {The art of finding software vulnerabilities has been covered extensively in the literature and there is a huge body of work on this topic. In contrast, the intentional insertion of exploitable, security-critical bugs has received little (public) attention yet. Wanting more bugs seems to be counterproductive at first sight, but the comprehensive evaluation of bug-finding techniques suffers from a lack of ground truth and the scarcity of bugs.In this paper, we propose EvilCoder, a system to automatically find potentially vulnerable source code locations and modify the source code to be actually vulnerable. More specifically, we leverage automated program analysis techniques to find sensitive sinks which match typical bug patterns (e.g., a sensitive API function with a preceding sanity check), and try to find data-flow connections to user-controlled sources. We then transform the source code such that exploitation becomes possible, for example by removing or modifying input sanitization or other types of security checks. Our tool is designed to randomly pick vulnerable locations and possible modifications, such that it can generate numerous different vulnerabilities on the same software corpus. We evaluated our tool on several open-source projects such as for example libpng and vsftpd, where we found between 22 and 158 unique connected source-sink pairs per project. This translates to hundreds of potentially vulnerable data-flow paths and hundreds of bugs we can insert. We hope to support future bug-finding techniques by supplying freshly generated, bug-ridden test corpora so that such techniques can (finally) be evaluated and compared in a comprehensive and statistically meaningful way.},
booktitle = {Proceedings of the 32nd Annual Conference on Computer Security Applications},
pages = {214–225},
numpages = {12},
location = {Los Angeles, California, USA},
series = {ACSAC '16}
}

@inproceedings{10.1145/3605770.3625216,
author = {Wyss, Elizabeth and De Carli, Lorenzo and Davidson, Drew},
title = {(Nothing But) Many Eyes Make All Bugs Shallow},
year = {2023},
isbn = {9798400702631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605770.3625216},
doi = {10.1145/3605770.3625216},
abstract = {Open source package repositories have become a crucial component of the modern software supply chain since they enable developers to easily and rapidly import code written by others. However, low quality, poorly vetted code residing in such repositories exposes developers and end-users to dangerous bugs and vulnerabilities at a large scale. Such issues have recently led to the creation of government-backed verification standards pertaining to packages, as well as a significant body of developer folklore regarding what constitutes a reliable package. However, there exists little academic research assessing the relationships between recommended development practices and known package issues in this domain. Motivated by this gap in understanding, we conduct a large-scale study that formally evaluates whether adherence to these guidelines meaningfully impacts reported issues and bug maintenance activity across the most widely utilized npm packages (encompassing 7,162 packages with over 100K weekly downloads each), which unveiled wide disparities across package-level metrics. We find that it is only recommendations pertaining to a broad notion of scrutiny that provide strong and reliable insights into the reporting and resolving of package issues. These findings pose significant implications for developers, who seek to identify well-maintained packages for use, as well as security researchers, who seek to identify suspicious packages for critical observation.},
booktitle = {Proceedings of the 2023 Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses},
pages = {53–63},
numpages = {11},
keywords = {software supply chain, package repositories, open-source},
location = {Copenhagen, Denmark},
series = {SCORED '23}
}

@article{10.1145/2579281.2579292,
author = {Bhasin, Harsh and Khanna, Esha},
title = {Neural network based black box testing},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/2579281.2579292},
doi = {10.1145/2579281.2579292},
abstract = {Black Box Testing is immensely important because the source code of a module is not always available. Enterprise Resource Planning systems are also tested using Black Box Testing wherein all the test cases are not equally important. The prioritization of these test cases would be helpful in case of premature termination of testing, due to lack of resources. This paper proposes a Neural Network based method to prioritize test cases. The paper also presents guidelines for prioritizing test cases. The technique has been tested using a financial management system and the results are encouraging. This paper paves way for applying Neural Network in Black Box Testing and presents a framework, which would help both researchers and practitioners.},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {1–6},
numpages = {6},
keywords = {test case design, prioritizing test cases, neural network, black box testing}
}

@inproceedings{10.1145/3350768.3352571,
author = {Santos, Italo and Melo, Silvana M. and de Souza, Paulo Sergio Lopes and Souza, Simone R. S.},
title = {Testing Techniques Selection: A Systematic Mapping Study},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3352571},
doi = {10.1145/3350768.3352571},
abstract = {[Context] Software projects must consider the selection of testing techniques and criteria during their life cycles. This practice increases the chances of testing activity to be appropriately performed. In a previous work, an infrastructure to support the selection of testing techniques was proposed for the context of concurrent software. This infrastructure considers information (attributes) of the project to make the selection closer to the testers need. [Objective] This paper extends the previous work by identifying new studies concerning testing techniques selection, project attributes that can be used for this selection and which approaches can be employed to support the combined selection. [Method] A mapping study was conducted and a total of 15 primary studies, published in the last 20 years were selected. Information about approaches to testing techniques selection was analyzed and classified. [Results] The following results were obtained: (i) existing approaches for selection of testing techniques; (ii) proposition of a taxonomy of selection approaches; (iii) characterization of attributes to offer the support for the selection of a testing technique; and (iv) identification of approaches that perform combined selection of testing techniques. [Conclusion] Combining testing techniques is essential to improve the testing activity quality by finding different failure categories and supplementing other techniques limitations. This paper describes an initiative to offer support for the construction of new or combined strategies for the selection of testing techniques capable of being used in practice.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {347–356},
numpages = {10},
keywords = {testing techniques selection, combined selection},
location = {Salvador, Brazil},
series = {SBES '19}
}

@inproceedings{10.1145/3236024.3236071,
author = {Lu, Jie and Li, Feng and Li, Lian and Feng, Xiaobing},
title = {CloudRaid: hunting concurrency bugs in the cloud via log-mining},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236071},
doi = {10.1145/3236024.3236071},
abstract = {Cloud systems suffer from distributed concurrency bugs, which are notoriously difficult to detect and often lead to data loss and service outage. This paper presents CloudRaid, a new effective tool to battle distributed concurrency bugs. CloudRaid automatically detects concurrency bugs in cloud systems, by analyzing and testing those message orderings that are likely to expose errors. We observe that large-scale online cloud applications process millions of user requests per second, exercising many permutations of message orderings extensively. Those already sufficiently-tested message orderings are unlikely to expose errors. Hence, CloudRaid mines logs from previous executions to uncover those message orderings which are feasible, but not sufficiently tested. Specifically, CloudRaid tries to flip the order of a pair of messages &lt;S,P&gt; if they may happen in parallel, but S always arrives before P from existing logs, i.e., excercising the order P ↣ S. The log-based approach makes it suitable to live systems. We have applied CloudRaid to automatically test four representative distributed systems: Apache Hadoop2/Yarn, HBase, HDFS and Cassandra. CloudRaid can automatically test 40 different versions of the 4 systems (10 versions per system) in 35 hours, and can successfully trigger 28 concurrency bugs, including 8 new bugs that have never been found before. The 8 new bugs have all been confirmed by their original developers, and 3 of them are considered as critical bugs that have already been fixed.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {3–14},
numpages = {12},
keywords = {Distributed Systems, Concurrency Bugs, Cloud Computing, Bug Detection},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@article{10.1145/3649593,
author = {Chen, Zhifei and Chen, Lin and Yang, Yibiao and Feng, Qiong and Li, Xuansong and Song, Wei},
title = {Risky Dynamic Typing-related Practices in Python: An Empirical Study},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3649593},
doi = {10.1145/3649593},
abstract = {Python’s dynamic typing nature provides developers with powerful programming abstractions. However, many type-related bugs are accumulated in code bases of Python due to the misuse of dynamic typing. The goal of this article is to aid in the understanding of developers’ high-risk practices toward dynamic typing and the early detection of type-related bugs. We first formulate the rules of six types of risky dynamic typing-related practices (type smells for short) in Python. We then develop a rule-based tool named RUPOR, which builds an accurate type base to detect type smells. Our evaluation shows that RUPOR outperforms the existing type smell detection techniques (including the Large Language Models–based approaches, Mypy, and PYDYPE) on a benchmark of 900 Python methods. Based on RUPOR, we conduct an empirical study on 25 real-world projects. We find that type smells are significantly related to the occurrence of post-release faults. The fault-proneness prediction model built with type smell features slightly outperforms the model built without them. We also summarize the common patterns, including inserting type check to fix type smell bugs. These findings provide valuable insights for preventing and fixing type-related bugs in the programs written in dynamic-typed languages.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {140},
numpages = {35},
keywords = {Dynamic typing, Python, empirical study, bug fixing}
}

@inproceedings{10.1145/1370750.1370772,
author = {Hata, Hideaki and Mizuno, Osamu and Kikuno, Tohru},
title = {An extension of fault-prone filtering using precise training and a dynamic threshold},
year = {2008},
isbn = {9781605580241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370750.1370772},
doi = {10.1145/1370750.1370772},
abstract = {Fault-prone module detection in source code is important for assurance of software quality. Most previous fault-prone detection approaches have been based on software metrics. Such approaches, however, have difficulties in collecting the metrics and in constructing mathematical models based on the metrics. To mitigate such difficulties, we have proposed a novel approach for detecting fault-prone modules using a spam-filtering technique, named Fault-Prone Filtering. In our approach, fault-prone modules are detected in such a way that the source code modules are considered as text files and are applied to the spam filter directly. In practice, we use the training only errors procedure and apply this procedure to fault-prone. Since no pre-training is required, this procedure can be applied to an actual development field immediately. This paper describes an extension of the training only errors procedures. We introduce a precise unit of training, "modified lines of code," instead of methods. In addition, we introduce the dynamic threshold for classification. The result of the experiment shows that our extension leads to twice the precision with about the same recall, and improves 15% on the best F1 measurement.},
booktitle = {Proceedings of the 2008 International Working Conference on Mining Software Repositories},
pages = {89–98},
numpages = {10},
keywords = {text mining, spam filter, fault-prone modules},
location = {Leipzig, Germany},
series = {MSR '08}
}

@inproceedings{10.1145/1985793.1985859,
author = {Kim, Sunghun and Zhang, Hongyu and Wu, Rongxin and Gong, Liang},
title = {Dealing with noise in defect prediction},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1985859},
doi = {10.1145/1985793.1985859},
abstract = {Many software defect prediction models have been built using historical defect data obtained by mining software repositories (MSR). Recent studies have discovered that data so collected contain noises because current defect collection practices are based on optional bug fix keywords or bug report links in change logs. Automatically collected defect data based on the change logs could include noises.This paper proposes approaches to deal with the noise in defect data. First, we measure the impact of noise on defect prediction models and provide guidelines for acceptable noise level. We measure noise resistant ability of two well-known defect prediction algorithms and find that in general, for large defect datasets, adding FP (false positive) or FN (false negative) noises alone does not lead to substantial performance differences. However, the prediction performance decreases significantly when the dataset contains 20%-35% of both FP and FN noises. Second, we propose a noise detection and elimination algorithm to address this problem. Our empirical study shows that our algorithm can identify noisy instances with reasonable accuracy. In addition, after eliminating the noises using our algorithm, defect prediction accuracy is improved.},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {481–490},
numpages = {10},
keywords = {noise resistance, defect prediction, data quality, buggy files, buggy changes},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@inproceedings{10.1145/3341105.3373914,
author = {Muslija, Adnan and Enoiu, Eduard},
title = {On the measurement of software complexity for plc industrial control systems using TIQVA},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373914},
doi = {10.1145/3341105.3373914},
abstract = {In the safety-critical domain (e.g. transportation, nuclear, aerospace and automotive), large-scale embedded systems implemented using Programmable Logic Controllers (PLCs) are widely used to provide supervisory control. Software complexity metrics, such as code size and cyclomatic complexity, have been used in the software engineering community for predicting quality metrics such as maintainability, bug proneness and robustness. However, since there is no available approach and tool support for measuring software complexity of PLC programs, we developed a tool called TIQVA in an effort to measure complexity for this type of software. We show how to measure different software complexity metrics such as lines of code, cyclomatic complexity, and information flow for a popular PLC programming language named Function Block Diagram (FBD). We evaluate the tool using data provided by Bombardier Transportation from a Train Control Management System (TCMS). In addition, we report some empirical and industrial evidence showing how TIQVA can be used to provide some experimental evidence to support the use of these metrics to estimate testing effort for an industrial control software. The results from this evaluation indicate that other specific dimensions of PLC programs (e.g., function block relationships, block coupling and timing) could be used to improve the measurement of complexity for industrial embedded software.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1556–1565},
numpages = {10},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@proceedings{10.1145/3540250,
title = {ESEC/FSE 2022: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of all members of the organizing committee, we are delighted to welcome everyone to the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2022. The event continues the long, distinguished ESEC/FSE tradition of presenting the most innovative research, and facilitating interactions between scientists and engineers who are passionate about advancing the theory and practice of software engineering.},
location = {Singapore, Singapore}
}

@article{10.1145/2491509.2491513,
author = {Yoo, Shin and Harman, Mark and Clark, David},
title = {Fault localization prioritization: Comparing information-theoretic and coverage-based approaches},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2491509.2491513},
doi = {10.1145/2491509.2491513},
abstract = {Test case prioritization techniques seek to maximize early fault detection. Fault localization seeks to use test cases already executed to help find the fault location. There is a natural interplay between the two techniques; once a fault is detected, we often switch focus to fault fixing, for which localization may be a first step. In this article we introduce the Fault Localization Prioritization (FLP) problem, which combines prioritization and localization. We evaluate three techniques: a novel FLP technique based on information theory, FLINT (Fault Localization using INformation Theory), that we introduce in this article, a standard Test Case Prioritization (TCP) technique, and a “test similarity technique” used in previous work. Our evaluation uses five different releases of four software systems. The results indicate that FLP and TCP can statistically significantly reduce fault localization costs for 73% and 76% of cases, respectively, and that FLINT significantly outperforms similarity-based localization techniques in 52% of the cases considered in the study.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {19},
numpages = {29},
keywords = {information theory, fault localization, Test case prioritization}
}

@inproceedings{10.1145/1723028.1723031,
author = {Reidemeister, Thomas and Munawar, Mohammad Ahmad and Jiang, Miao and Ward, Paul A. S.},
title = {Diagnosis of recurrent faults using log files},
year = {2009},
publisher = {IBM Corp.},
address = {USA},
url = {https://doi.org/10.1145/1723028.1723031},
doi = {10.1145/1723028.1723031},
abstract = {Enterprise software systems (ESS) are becoming larger and increasingly complex. Failure in business-critical systems is expensive, leading to consequences such as loss of critical data, loss of sales, customer dissatisfaction, even law suits. Therefore, detecting failures and diagnosing their root-cause in a timely manner is essential. Many studies suggest that a large fraction of failures encountered in practice are recurrent (i.e., they have been seen before). Fast and accurate detection of these failures can accelerate problem determination, and thereby improve system reliability. To this effect, we explore machine learning techniques, including the Na\"{\i}ve Bayes classifier, partially-supervised learning, and decision trees (using C4.5), to automatically recognize symptoms of recurrent faults and to derive detection rules from samples of log data. This work focuses on log files, since they are readily available and they do not put any additional computational burden on the component generating the data.The methods explored in this work can aid the development of tools to assist support personnel in problem determination tasks. Instead of requiring the operators to manually define patterns for identifying recurrent problems, such tools can be trained using prior, solved and unsolved cases from existing support databases.},
booktitle = {Proceedings of the 2009 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {12–23},
numpages = {12},
location = {Ontario, Canada},
series = {CASCON '09}
}

@proceedings{10.1145/3700706,
title = {ICISS '24: Proceedings of the 2024 7th International Conference on Information Science and Systems},
year = {2024},
isbn = {9798400717567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/2806777.2806937,
author = {Huang, Jian and Zhang, Xuechen and Schwan, Karsten},
title = {Understanding issue correlations: a case study of the Hadoop system},
year = {2015},
isbn = {9781450336512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2806777.2806937},
doi = {10.1145/2806777.2806937},
abstract = {Over the last decade, Hadoop has evolved into a widely used platform for Big Data applications. Acknowledging its wide-spread use, we present a comprehensive analysis of the solved issues with applied patches in the Hadoop ecosystem. The analysis is conducted with a focus on Hadoop's two essential components: HDFS (storage) and MapReduce (computation), it involves a total of 4218 solved issues over the last six years, covering 2180 issues from HDFS and 2038 issues from MapReduce. Insights derived from the study concern system design and development, particularly with respect to correlated issues and correlations between root causes of issues and characteristics of the Hadoop subsystems. These findings shed light on the future development of Big Data systems, on their testing, and on bug-finding tools.},
booktitle = {Proceedings of the Sixth ACM Symposium on Cloud Computing},
pages = {2–15},
numpages = {14},
keywords = {issue correlation, bug study, big data, Hadoop},
location = {Kohala Coast, Hawaii},
series = {SoCC '15}
}

@inproceedings{10.1145/3501774.3501775,
author = {Jaccheri, Letizia and Kholmatova, Zamira and Succi, Giancarlo},
title = {Systematizing the Meta-Analytical Process in Software Engineering},
year = {2022},
isbn = {9781450385060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501774.3501775},
doi = {10.1145/3501774.3501775},
abstract = {The generalization of knowledge is a necessary part of every scientific field. Meta-analysis is already advocated as a tool for generalization in different areas such as medicine, psychology, business, and this process is already standardized for them. Software engineering started using meta-analysis as a tool for aggregating results from families of experiments, but not so long for generalization of results coming from different studies, and for this purpose, the meta-analytical approach is not yet clarified. In this paper, we attempt to systematize the application of meta-analysis as a secondary study to the software engineering field suggesting our preliminary protocol. To see the reliability of the proposed protocol we conducted several studies using it. Following even uniform protocol with these studies, we identified the issues preventing the wide usage of meta-analysis in software engineering and proposed our solutions for them.},
booktitle = {Proceedings of the 2021 European Symposium on Software Engineering},
pages = {1–5},
numpages = {5},
keywords = {Research synthesis in software engineering, Meta-analysis in software engineering, Evidence-based software engineering},
location = {Larissa, Greece},
series = {ESSE '21}
}

@inproceedings{10.5555/2819009.2819019,
author = {Anderson, Jeff and Salem, Saeed and Do, Hyunsook},
title = {Striving for failure: an industrial case study about test failure prediction},
year = {2015},
publisher = {IEEE Press},
abstract = {Software regression testing is an important, yet very costly, part of most major software projects. When regression tests run, any failures that are found help catch bugs early and smooth the future development work. The act of executing large numbers of tests takes significant resources that could, otherwise, be applied elsewhere. If tests could be accurately classified as likely to pass or fail prior to the run, it could save significant time while maintaining the benefits of early bug detection. In this paper, we present a case study to build a classifier for regression tests based on industrial software, Microsoft Dynamics AX. In this study, we examine the effectiveness of this classification as well as which aspects of the software are the most important in predicting regression test failures.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {49–58},
numpages = {10},
keywords = {test failure prediction, regression testing, data-mining software repositories, case study},
location = {Florence, Italy},
series = {ICSE '15}
}

@proceedings{10.5555/3606013,
title = {ICSE '23: Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Victoria, Australia}
}

@inproceedings{10.1145/3194095.3194100,
author = {Flynn, Lori and Snavely, William and Svoboda, David and VanHoudnos, Nathan and Qin, Richard and Burns, Jennifer and Zubrow, David and Stoddard, Robert and Marce-Santurio, Guillermo},
title = {Prioritizing alerts from multiple static analysis tools, using classification models},
year = {2018},
isbn = {9781450357371},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194095.3194100},
doi = {10.1145/3194095.3194100},
abstract = {Static analysis (SA) tools examine code for flaws without executing the code, and produce warnings ("alerts") about possible flaws. A human auditor then evaluates the validity of the purported code flaws. The effort required to manually audit all alerts and repair all confirmed code flaws is often too much for a project's budget and schedule. An alert triaging tool enables strategically prioritizing alerts for examination, and could use classifier confidence. We developed and tested classification models that predict if static analysis alerts are true or false positives, using a novel combination of multiple static analysis tools, features from the alerts, alert fusion, code base metrics, and archived audit determinations. We developed classifiers using a partition of the data, then evaluated the performance of the classifier using standard measurements, including specificity, sensitivity, and accuracy. Test results and overall data analysis show accurate classifiers were developed, and specifically using multiple SA tools increased classifier accuracy, but labeled data for many types of flaws were inadequately represented (if at all) in the archive data, resulting in poor predictive accuracy for many of those flaws.},
booktitle = {Proceedings of the 1st International Workshop on Software Qualities and Their Dependencies},
pages = {13–20},
numpages = {8},
keywords = {static analysis, rapid, classification, alert, accurate},
location = {Gothenburg, Sweden},
series = {SQUADE '18}
}

@inproceedings{10.1145/3524842.3528452,
author = {Fu, Michael and Tantithamthavorn, Chakkrit},
title = {LineVul: a transformer-based line-level vulnerability prediction},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528452},
doi = {10.1145/3524842.3528452},
abstract = {Software vulnerabilities are prevalent in software systems, causing a variety of problems including deadlock, information loss, or system failures. Thus, early predictions of software vulnerabilities are critically important in safety-critical software systems. Various ML/DL-based approaches have been proposed to predict vulnerabilities at the file/function/method level. Recently, IVDetect (a graph-based neural network) is proposed to predict vulnerabilities at the function level. Yet, the IVDetect approach is still inaccurate and coarse-grained. In this paper, we propose LineVul, a Transformer-based line-level vulnerability prediction approach in order to address several limitations of the state-of-the-art IVDetect approach. Through an empirical evaluation of a large-scale real-world dataset with 188k+ C/C++ functions, we show that LineVul achieves (1) 160%-379% higher F1-measure for function-level predictions; (2) 12%-25% higher Top-10 Accuracy for line-level predictions; and (3) 29%-53% less Effort@20%Recall than the baseline approaches, highlighting the significant advancement of LineVul towards more accurate and more cost-effective line-level vulnerability predictions. Our additional analysis also shows that our LineVul is also very accurate (75%-100%) for predicting vulnerable functions affected by the Top-25 most dangerous CWEs, highlighting the potential impact of our LineVul in real-world usage scenarios.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {608–620},
numpages = {13},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@proceedings{10.1145/3605770,
title = {SCORED '23: Proceedings of the 2023 Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses},
year = {2023},
isbn = {9798400702631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to ACM SCORED '23, the second edition of the ACM Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses. This edition is held in Copenhagen, Denmark with extensive support for in-person and virtual attendance.This year's program includes exciting work along many different dimensions of research on supply chain security: the development of security policies for software supply chains, the use of artificial intelligence and large language models, approaches on software bills of materials, and the proposals of risk mitigation techniques. Consistent with its focus, SCORED brings researchers, legislators and practitioners in both open- and closed-source ecosystems to the center of current and emerging challenges and opportunities in software supply chain security.},
location = {Copenhagen, Denmark}
}

@inproceedings{10.1109/ICSSP.2019.00011,
author = {Kapur, Ritu and Sodhi, Balwinder},
title = {Towards a knowledge warehouse and expert system for the automation of SDLC tasks},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSSP.2019.00011},
doi = {10.1109/ICSSP.2019.00011},
abstract = {Cost of a skilled and competent software developer is high, and it is desirable to minimize dependency on such costly human resources. One of the ways to minimize such costs is via automation of various software development tasks.Recent advances in Artificial Intelligence (AI) and the availability of a large volume of knowledge bearing data at various software development related venues present a ripe opportunity for building tools that can automate software development tasks. For instance, there is significant latent knowledge present in raw or unstructured data associated with items such as source files, code commit logs, defect reports, comments, and so on, available in the Open Source Software (OSS) repositories.We aim to leverage such knowledge-bearing data, the latest advances in AI and hardware to create knowledge warehouses and expert systems for the software development domain. Such tools can help in building applications for performing various software development tasks such as defect prediction, effort estimation, code review, etc.},
booktitle = {Proceedings of the International Conference on Software and System Processes},
pages = {5–8},
numpages = {4},
keywords = {supervised learning, software maintenance, data mining, automated software engineering},
location = {Montreal, Quebec, Canada},
series = {ICSSP '19}
}

@article{10.1145/2685612,
author = {Fraser, Gordon and Arcuri, Andrea},
title = {A Large-Scale Evaluation of Automated Unit Test Generation Using EvoSuite},
year = {2014},
issue_date = {December 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/2685612},
doi = {10.1145/2685612},
abstract = {Research on software testing produces many innovative automated techniques, but because software testing is by necessity incomplete and approximate, any new technique faces the challenge of an empirical assessment. In the past, we have demonstrated scientific advance in automated unit test generation with the EVOSUITE tool by evaluating it on manually selected open-source projects or examples that represent a particular problem addressed by the underlying technique. However, demonstrating scientific advance is not necessarily the same as demonstrating practical value; even if VOSUITE worked well on the software projects we selected for evaluation, it might not scale up to the complexity of real systems. Ideally, one would use large “real-world” software systems to minimize the threats to external validity when evaluating research tools. However, neither choosing such software systems nor applying research prototypes to them are trivial tasks.In this article we present the results of a large experiment in unit test generation using the VOSUITE tool on 100 randomly chosen open-source projects, the 10 most popular open-source projects according to the SourceForge Web site, seven industrial projects, and 11 automatically generated software projects. The study confirms that VOSUITE can achieve good levels of branch coverage (on average, 71% per class) in practice. However, the study also exemplifies how the choice of software systems for an empirical study can influence the results of the experiments, which can serve to inform researchers to make more conscious choices in the selection of software system subjects. Furthermore, our experiments demonstrate how practical limitations interfere with scientific advances, branch coverage on an unbiased sample is affected by predominant environmental dependencies. The surprisingly large effect of such practical engineering problems in unit testing will hopefully lead to a larger appreciation of work in this area, thus supporting transfer of knowledge from software testing research to practice.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {8},
numpages = {42},
keywords = {empirical software engineering, branch coverage, benchmark, automated test generation, Unit testing, Java, JUnit}
}

@proceedings{10.1145/3702138,
title = {ASSE '24: Proceeding of the 2024 5th Asia Service Sciences and Software Engineering Conference},
year = {2024},
isbn = {9798400717543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3356317.3356323,
author = {Dallilo, Lucas Diniz and Pizzoleto, Alessandro Viola and Ferrari, Fabiano Cutigi},
title = {An Evaluation of Internal Program Metrics as Predictors of Mutation Operator Score},
year = {2019},
isbn = {9781450376488},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356317.3356323},
doi = {10.1145/3356317.3356323},
abstract = {Context: Mutation testing is effective in producing high quality test sets. On the downside, it is expensive due to factors like the large number of mutants and the need for manual analysis tasks. Over time, researchers devised several ways of reducing its costs and achieved noticeable results. However, results are little generalizable due to characteristics of the programs and of the applied mutation operators. Objective: This paper reports on an exploratory study that aimed to evaluate internal program characteristics as predictors of mutation testing scores. Method: By applying a clustering algorithm, the core idea consists in identifying a group R of tested programs that can be used as a baseline for testing a new program u using mutation at reduced cost. The same cost reduction results obtained for R is expected to produce relevant results for u. Results: We experimented our approach with 38 programs used in previous experiments. The programs have mutation-adequate test sets. For each program, we analyzed if the results achieved for the cluster that contained the program were superior to results achieved for the remaining clusters, as well as to results achieved for the whole set of programs. Results indicate that the best mutation operators for the cluster to which the program is associated, in the majority of the cases, are also the best mutation operators for the program itself. Some variations in results occurred with reduced and increased sets of programs. Conclusion: Overall, results are promising and motivate further experiments with varying settings. We hypothesize that program similarity can be useful predictors of consistent results of mutation testing applied at low cost.},
booktitle = {Proceedings of the IV Brazilian Symposium on Systematic and Automated Software Testing},
pages = {12–21},
numpages = {10},
keywords = {program similarity, mutation testing, mutation operators, cost reduction},
location = {Salvador, Brazil},
series = {SAST '19}
}

@article{10.1145/571681.571683,
author = {Li, Bixin},
title = {Analyzing information-flow in java program based on slicing technique},
year = {2002},
issue_date = {September 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/571681.571683},
doi = {10.1145/571681.571683},
abstract = {Traditional information-flow analysis is mainly based on dataflow and control-flow analysis. In object-oriented program, because of pointer aliasing, inheritance, and polymorphism, information-flow analysis become very complicated. Especially, it is difficult to rely only on normal data and control-flow analysis techniques. some new approaches are required to analyze the information-flow between components in object-oriented program. In this paper, object-oriented program slicing technique is introduced. By this technique, the amount of information-flow, the width of information-flow and correlation coefficient between components can be computed. Some applications of the information-flow are also discussed and analyzed in this paper.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {98–103},
numpages = {6}
}

@article{10.1145/3708476,
author = {Fu, Xiaoqin and Zaman, Asif and Cai, Haipeng},
title = {DistMeasure: A Framework for Runtime Characterization and Quality Assessment of Distributed Software via Interprocess Communications},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708476},
doi = {10.1145/3708476},
abstract = {A defining, unique aspect of distributed systems lies in interprocess communication (IPC) through which distributed components interact and collaborate toward the holistic system behaviors. This highly decoupled construction intuitively contributes to the scalability, performance, and resiliency advantages of distributed software, but also adds largely to their greater complexity, compared to centralized software. Yet despite the importance of IPC in distributed systems, little is known about how to quantify IPC-induced behaviors in these systems through IPC measurement and how such behaviors may be related to the quality of distributed software. To answer these questions, in this article, we present DistMeasure, a framework for measuring distributed software systems via the lens of IPC hence enabling the study of its correlation with distributed system quality. Underlying DistMeasure is a novel set of IPC metrics that focus on gauging the coupling and cohesion of distributed processes. Through these metrics, DistMeasure quantifies relevant runtime characteristics of distributed systems and their quality relevance, covering a range of quality aspects each via respective direct quality metrics. Further, DistMeasure enables predictive assessment of distributed system quality in those aspects via learning-based anomaly detection with respect to the corresponding quality metrics based on their significant correlations with related IPC metrics. Using DistMeasure, we demonstrated the practicality and usefulness of IPC measurement against 11 real-world distributed systems and their diverse execution scenarios. Among other findings, our results revealed that IPC has a strong correlation with distributed system complexity, performance efficiency, and security. Higher IPC coupling between distributed processes tended to be negatively indicative of distributed software quality, while more cohesive processes have positive quality implications. Yet overall IPC-induced behaviors are largely independent of the system scale, and higher (lower) process coupling does not necessarily come with lower (higher) process cohesion. We also show promising merits (with 98% precision/recall/F1) of IPC measurement (e.g., class-level coupling and process-level cohesion) for predictive anomaly assessment of various aspects (e.g., attack surface and performance efficiency) of distributed system quality.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {74},
numpages = {53},
keywords = {distributed system, interprocess communication, dynamic metrics, software quality}
}

@proceedings{10.1145/3661167,
title = {EASE '24: Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Salerno, Italy}
}

@proceedings{10.1145/3584871,
title = {ICSIM '23: Proceedings of the 2023 6th International Conference on Software Engineering and Information Management},
year = {2023},
isbn = {9781450398237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Palmerston North, New Zealand}
}

@proceedings{10.1145/3568562,
title = {SoICT '22: Proceedings of the 11th International Symposium on Information and Communication Technology},
year = {2022},
isbn = {9781450397254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hanoi, Vietnam}
}

@proceedings{10.1145/3651640,
title = {ESSE '23: Proceedings of the 4th European Symposium on Software Engineering},
year = {2023},
isbn = {9798400708817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Napoli, Italy}
}

@inproceedings{10.1145/2593882.2593892,
author = {Chandra, Satish and Sinha, Vibha Singhal and Sinha, Saurabh and Ratakonda, Krishna},
title = {Software services: a research roadmap},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593892},
doi = {10.1145/2593882.2593892},
abstract = {Software services companies offer software development, testing and maintenance as a “service” to other organizations. As a thriv- ing industry in its own right, software services offers certain unique research problems as well as different takes on research problems typically considered in software engineering research. In this paper, we highlight some of these research problems, drawing heavily upon our involvement with IBM Global Business Services organization over the past several years. We focus on four selected topics: how to organize people and the flow of work through people, how to manage knowledge at an organizational level, how to estimate and manage risk in a services engagement, and finally, testing services. These topics by no means cover all areas pertinent to soft- ware services; rather, they reflect ones in which we have personal perspectives to offer. We also share our experience in deployment of research innovations in a large service delivery organization.},
booktitle = {Future of Software Engineering Proceedings},
pages = {40–54},
numpages = {15},
keywords = {testing, risk management, knowledge management, distributed development, Software services},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@inproceedings{10.1145/2901739.2901753,
author = {Trautsch, Fabian and Herbold, Steffen and Makedonski, Philip and Grabowski, Jens},
title = {Adressing problems with external validity of repository mining studies through a smart data platform},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2901753},
doi = {10.1145/2901739.2901753},
abstract = {Research in software repository mining has grown considerably the last decade. Due to the data-driven nature of this venue of investigation, we identified several problems within the current state-of-the-art that pose a threat to the external validity of results. The heavy re-use of data sets in many studies may invalidate the results in case problems with the data itself are identified. Moreover, for many studies data and/or the implementations are not available, which hinders a replication of the results and, thereby, decreases the comparability between studies. Even if all information about the studies is available, the diversity of the used tooling can make their replication even then very hard. Within this paper, we discuss a potential solution to these problems through a cloud-based platform that integrates data collection and analytics. We created the prototype SmartSHARK that implements our approach. Using SmartSHARK, we collected data from several projects and created different analytic examples. Within this article, we present SmartSHARK and discuss our experiences regarding the use of SmartSHARK and the mentioned problems.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {97–108},
numpages = {12},
keywords = {software mining, software analytics, smart data},
location = {Austin, Texas},
series = {MSR '16}
}

@article{10.1145/3328746,
author = {Bosu, Michael F. and Macdonell, Stephen G.},
title = {Experience: Quality Benchmarking of Datasets Used in Software Effort Estimation},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3328746},
doi = {10.1145/3328746},
abstract = {Data is a cornerstone of empirical software engineering (ESE) research and practice. Data underpin numerous process and project management activities, including the estimation of development effort and the prediction of the likely location and severity of defects in code. Serious questions have been raised, however, over the quality of the data used in ESE. Data quality problems caused by noise, outliers, and incompleteness have been noted as being especially prevalent. Other quality issues, although also potentially important, have received less attention. In this study, we assess the quality of 13 datasets that have been used extensively in research on software effort estimation. The quality issues considered in this article draw on a taxonomy that we published previously based on a systematic mapping of data quality issues in ESE. Our contributions are as follows: (1) an evaluation of the “fitness for purpose” of these commonly used datasets and (2) an assessment of the utility of the taxonomy in terms of dataset benchmarking. We also propose a template that could be used to both improve the ESE data collection/submission process and to evaluate other such datasets, contributing to enhanced awareness of data quality issues in the ESE community and, in time, the availability and use of higher-quality datasets.},
journal = {J. Data and Information Quality},
month = aug,
articleno = {19},
numpages = {38},
keywords = {software effort estimation, noise, missing data, empirical software engineering, benchmarking, Data quality}
}

@article{10.1145/1317471.1317478,
author = {Ploski, Jan and Rohr, Matthias and Schwenkenberg, Peter and Hasselbring, Wilhelm},
title = {Research issues in software fault categorization},
year = {2007},
issue_date = {November 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/1317471.1317478},
doi = {10.1145/1317471.1317478},
abstract = {Software faults are a major threat for the dependability of software systems. When we intend to study the impact of software faults on software behavior, examine the quality of fault tolerance mechanisms, or evaluate diagnostic techniques, the issue of distinguishing fault categories and their frequency distribution arises immediately. This article surveys the literature that provides quantitative data on categories of software faults and discusses the applicability of these software fault category distributions to fault injection case studies.},
journal = {SIGSOFT Softw. Eng. Notes},
month = nov,
pages = {6–es},
numpages = {8},
keywords = {software reliability, software faults, software fault categorization, injection of software faults, bugs}
}

@proceedings{10.1145/3628797,
title = {SOICT '23: Proceedings of the 12th International Symposium on Information and Communication Technology},
year = {2023},
isbn = {9798400708916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Ho Chi Minh, Vietnam}
}

@inproceedings{10.1109/PROMISE.2007.1,
author = {Ma, Yan and Cukic, Bojan},
title = {Adequate and Precise Evaluation of Quality Models in Software Engineering Studies},
year = {2007},
isbn = {0769529542},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/PROMISE.2007.1},
doi = {10.1109/PROMISE.2007.1},
abstract = {Many statistical techniques have been proposed and introduced to predict fault-proneness of program modules in software engineering. Choosing the "best" candidate among many available models involves performance assessment and detailed comparison. But these comparisons are not simple due to varying performance measures and the related verification and validation cost implications. Therefore, a methodology for precise definition and evaluation of the predictive models is still needed. We believe the procedure we outline here, if followed, has a potential to enhance the statistical validity of future experiments.},
booktitle = {Proceedings of the Third International Workshop on Predictor Models in Software Engineering},
pages = {1},
series = {PROMISE '07}
}

@inproceedings{10.1145/2723742.2723754,
author = {Muthukumaran, K. and Rallapalli, Akhila and Murthy, N. L. Bhanu},
title = {Impact of Feature Selection Techniques on Bug Prediction Models},
year = {2015},
isbn = {9781450334327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723742.2723754},
doi = {10.1145/2723742.2723754},
abstract = {Several change metrics and source code metrics have been introduced and proved to be effective features in building bug prediction models. Researchers performed comparative studies of bug prediction models built using the individual metrics as well as combination of these metrics. In this paper, we investigate whether the prediction accuracy of bug prediction models is improved by applying feature selection techniques. We explore if there is one algorithm amongst ten popular feature selection algorithms that consistently fares better than others across sixteen bench marked open source projects. We also study whether the metrics in best feature subset are consistent across projects.},
booktitle = {Proceedings of the 8th India Software Engineering Conference},
pages = {120–129},
numpages = {10},
keywords = {Software Quality, Feature selection, Bug prediction},
location = {Bangalore, India},
series = {ISEC '15}
}

@proceedings{10.1145/3643991,
title = {MSR '24: Proceedings of the 21st International Conference on Mining Software Repositories},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {MSR is a thriving research community that organizes a yearly conference with a solid reputation amongst software engineering researchers.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/2393596.2393636,
author = {Park, Sangmin and Hossain, B. M. Mainul and Hussain, Ishtiaque and Csallner, Christoph and Grechanik, Mark and Taneja, Kunal and Fu, Chen and Xie, Qing},
title = {CarFast: achieving higher statement coverage faster},
year = {2012},
isbn = {9781450316149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393596.2393636},
doi = {10.1145/2393596.2393636},
abstract = {Test coverage is an important metric of software quality, since it indicates thoroughness of testing. In industry, test coverage is often measured as statement coverage. A fundamental problem of software testing is how to achieve higher statement coverage faster, and it is a difficult problem since it requires testers to cleverly find input data that can steer execution sooner toward sections of application code that contain more statements.We created a novel fully automatic approach for aChieving higher stAtement coveRage FASTer (CarFast), which we implemented and evaluated on twelve generated Java applications whose sizes range from 300 LOC to one million LOC. We compared CarFast with several popular test case generation techniques, including pure random, adaptive random, and Directed Automated Random Testing (DART). Our results indicate with strong statistical significance that when execution time is measured in terms of the number of runs of the application on different input test data, CarFast outperforms the evaluated competitive approaches on most subject applications.},
booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
articleno = {35},
numpages = {11},
keywords = {testing, statement coverage, experimentation},
location = {Cary, North Carolina},
series = {FSE '12}
}

@article{10.1145/1543405.1543413,
author = {Biswas, Swarnendu and Mall, Rajib and Satpathy, Manoranjan and Sukumaran, Srihari},
title = {A model-based regression test selection approach for embedded applications},
year = {2009},
issue_date = {July 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1543405.1543413},
doi = {10.1145/1543405.1543413},
abstract = {Regression test selection techniques for embedded programs have scarcely been reported in the literature. In this paper, we propose a model-based regression test selection technique for embedded programs. Our proposed model, in addition to capturing the data and control dependence aspects, also represents several additional program features that are important for regression test case selection of embedded programs. These features include control flow, exception handling, message paths, task priorities, state information and object relations. We select a regression test suite based on slicing our proposed graph model. We also propose a genetic algorithm-based technique to select an optimal subset of test cases from the set of regression test cases selected after slicing our proposed model.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jul,
pages = {1–9},
numpages = {9},
keywords = {safety critical, regression testing, regression test selection, regression test optimization, real-time, embedded software}
}

@inproceedings{10.1145/3121264.3121265,
author = {Sadeghi, Alireza and Esfahani, Naeem and Malek, Sam},
title = {Mining mobile app markets for prioritization of security assessment effort},
year = {2017},
isbn = {9781450351584},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3121264.3121265},
doi = {10.1145/3121264.3121265},
abstract = {Like any other software engineering activity, assessing the security of a software system entails prioritizing the resources and minimizing the risks. Techniques ranging from the manual inspection to automated static and dynamic analyses are commonly employed to identify security vulnerabilities prior to the release of the software. However, none of these techniques is perfect, as static analysis is prone to producing lots of false positives and negatives, while dynamic analysis and manual inspection are unwieldy, both in terms of required time and cost. This research aims to improve these techniques by mining relevant information from vulnerabilities found in the app markets. The approach relies on the fact that many modern software systems, in particular mobile software, are developed using rich application development frameworks (ADF), allowing us to raise the level of abstraction for detecting vulnerabilities and thereby making it possible to classify the types of vulnerabilities that are encountered in a given category of application. By coupling this type of information with severity of the vulnerabilities, we are able to improve the efficiency of static and dynamic analyses, and target the manual effort on the riskiest vulnerabilities.},
booktitle = {Proceedings of the 2nd ACM SIGSOFT International Workshop on App Market Analytics},
pages = {1–7},
numpages = {7},
keywords = {Software Analysis, Security Vulnerability, Mining App Market},
location = {Paderborn, Germany},
series = {WAMA 2017}
}

@proceedings{10.1145/3643916,
title = {ICPC '24: Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ICPC is the premier (CORE A) venue for research on program comprehension. Research on program comprehension encompasses both human activities for comprehending the software and technologies for supporting such comprehension.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3607947,
title = {IC3-2023: Proceedings of the 2023 Fifteenth International Conference on Contemporary Computing},
year = {2023},
isbn = {9798400700224},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Noida, India}
}

@proceedings{10.1145/3643794,
title = {SERP4IoT '24: Proceedings of the ACM/IEEE 6th International Workshop on Software Engineering Research &amp; Practices for the Internet of Things},
year = {2024},
isbn = {9798400705786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {SERP4IoT begins to be recognized as an annual venue gathering researchers, industrials, and practitioners to share their vision, experience, and opinion on how to address the challenges of, find solutions for, and share experiences with the development, release, and testing of robust software for IoT systems.Even today, there is no precise definition of what is software engineering for the IoT, because it encompasses many different aspects of software design, development, evolution, deployment, and operation, with varying and conflicting criteria such as success, longevity, growth, resilience, survival, diversity, sustainability, transparency, privacy, security, etc.Yet, software engineering is vital for IoT to design systems that are secure, interoperable, modifiable, and scalable. It is crucial to bring good practices for developing projects for IoT, to devise and study the best architectures, to understand and secure communication protocols, and, generally, to overcome the many challenges faced by practitioners and researchers.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/1134285.1134474,
author = {Zimmermann, Thomas},
title = {Taking lessons from history},
year = {2006},
isbn = {1595933751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1134285.1134474},
doi = {10.1145/1134285.1134474},
abstract = {Mining of software repositories has become an active research area. However, most past research considered any change to software as beneficial. This thesis will show how we can benefit from a classification into good and bad changes. The knowledge of bad changes will improve defect prediction and localization. Furthermore, we will describe how to learn project-specific error patterns that will help reducing future errors.},
booktitle = {Proceedings of the 28th International Conference on Software Engineering},
pages = {1001–1005},
numpages = {5},
location = {Shanghai, China},
series = {ICSE '06}
}

@proceedings{10.1145/3718491,
title = {AIBDF '24: Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum},
year = {2024},
isbn = {9798400710865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/2884781.2884813,
author = {Gulzar, Muhammad Ali and Interlandi, Matteo and Yoo, Seunghyun and Tetali, Sai Deep and Condie, Tyson and Millstein, Todd and Kim, Miryung},
title = {BigDebug: debugging primitives for interactive big data processing in spark},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884813},
doi = {10.1145/2884781.2884813},
abstract = {Developers use cloud computing platforms to process a large quantity of data in parallel when developing big data analytics. Debugging the massive parallel computations that run in today's datacenters is time consuming and error-prone. To address this challenge, we design a set of interactive, real-time debugging primitives for big data processing in Apache Spark, the next generation data-intensive scalable cloud computing platform. This requires rethinking the notion of step-through debugging in a traditional debugger such as gdb, because pausing the entire computation across distributed worker nodes causes significant delay and naively inspecting millions of records using a watchpoint is too time consuming for an end user.First, BigDebug's simulated breakpoints and on-demand watchpoints allow users to selectively examine distributed, intermediate data on the cloud with little overhead. Second, a user can also pinpoint a crash-inducing record and selectively resume relevant sub-computations after a quick fix. Third, a user can determine the root causes of errors (or delays) at the level of individual records through a fine-grained data provenance capability. Our evaluation shows that BigDebug scales to terabytes and its record-level tracing incurs less than 25% overhead on average. It determines crash culprits orders of magnitude more accurately and provides up to 100% time saving compared to the baseline replay debugger. The results show that BigDebug supports debugging at interactive speeds with minimal performance impact.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {784–795},
numpages = {12},
keywords = {interactive tools, fault localization and recovery, debugging, data-intensive scalable computing (DISC), big data analytics},
location = {Austin, Texas},
series = {ICSE '16}
}

@proceedings{10.1145/3609437,
title = {Internetware '23: Proceedings of the 14th Asia-Pacific Symposium on Internetware},
year = {2023},
isbn = {9798400708947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hangzhou, China}
}

@proceedings{10.1145/3555228,
title = {SBES '22: Proceedings of the XXXVI Brazilian Symposium on Software Engineering},
year = {2022},
isbn = {9781450397353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Brazil}
}

@inproceedings{10.1145/2245276.2231975,
author = {Banthia, Deepak and Gupta, Atul},
title = {Investigating fault prediction capabilities of five prediction models for software quality},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2231975},
doi = {10.1145/2245276.2231975},
abstract = {Predicting faults in software modules can lead to a high quality and more effective software development process to follow. However, the results of a fault prediction model have to be properly interpreted before incorporating them into any decision making. Most of the earlier studies have used the prediction accuracy as the main criteria to compare amongst competing fault prediction models. However, we show that besides accuracy, other criteria like number of false positives and false negatives can equally be important to choose a candidate model for fault prediction. We have used five NASA software data sets in our experiment. Our results suggest that the performance of Simple Logistic is better than the others on raw data sets whereas the performance of Neural Network was found to be better when we applied dimensionality reduction method on raw data sets. When we used data pre-processing techniques, the prediction accuracy of Random Forest was found to be better in both cases i.e. with and without dimensionality reduction but reliability of Simple Logistic was better than Random Forest because it had less number of fault negatives.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1259–1261},
numpages = {3},
keywords = {quality assurance, fault prediction models, fault prediction, effort estimation, attribute selection},
location = {Trento, Italy},
series = {SAC '12}
}

@proceedings{10.1145/3644815,
title = {CAIN '24: Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI},
year = {2024},
isbn = {9798400705915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The goal of the CAIN Conference Series is to bring together researchers and practitioners in software engineering, data science, and artificial intelligence (AI) as part of a growing community that is targeting the challenges of Software Engineering for AI-enabled systems.},
location = {Lisbon, Portugal}
}

@article{10.1145/129852.129861,
author = {Silverman, Barry G.},
title = {Survey of expert critiquing systems: practical and theoretical frontiers},
year = {1992},
issue_date = {April 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/129852.129861},
doi = {10.1145/129852.129861},
journal = {Commun. ACM},
month = apr,
pages = {106–127},
numpages = {22},
keywords = {expert critiquing systems, critics}
}

@proceedings{10.1145/3698062,
title = {WSSE '24: Proceedings of the 2024 The 6th World Symposium on Software Engineering (WSSE)},
year = {2024},
isbn = {9798400717086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/2745802.2745816,
author = {Liu, Gaoxuan and Rong, Guoping and Zhang, He and Shan, Qi},
title = {The adoption of capture-recapture in software engineering: a systematic literature review},
year = {2015},
isbn = {9781450333504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2745802.2745816},
doi = {10.1145/2745802.2745816},
abstract = {Context: Capture-recapture method has long been adopted in software engineering as a relatively objective way for defect estimation. While many relevant studies have been carried out to evaluate various capture-recapture models and estimators, there still lacks common understanding on the adoption status of the method in software engineering. It is necessary to systematically collect empirical evidence of Capture-recapture adoption hence form necessary understanding on the method.Objective: This study aims to synthesize relevant primary studies on the adoption of capture-recapture method in software engineering, and try to identify possible gaps between the state-of-practice and the state-of-art so as to provide clues for future research.Method: By following the guidelines of Kitchenham, we conducted a Systematic Literature Review(SLR) on studies of the adoption of capture-recapture method in software engineering.Results: From 5 common digital libraries, we retrieved 506 published articles, among them 44 were identified as relevant primary studies. We identified 18 capture-recapture estimators under 4 basic models. Types of the currently existing studies as well as the relevant influencing factors to adoption of the capture-recapture method are also discussed.Conclusion: Results show that there are no conclusive decisions on the best capture-recapture models and estimators. Besides, the number of inspectors and their capability to detect defects as well as the difficulty to detect defects are most critical influencing factors. In addition, lacking of industrial application may be the major issue of current adoption status of capture-recapture method in software engineering.},
booktitle = {Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {15},
numpages = {13},
keywords = {systematic literature review, software inspection, defect estimation, capture-recapture method},
location = {Nanjing, China},
series = {EASE '15}
}

@proceedings{10.1145/3605098,
title = {SAC '24: Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing},
year = {2024},
isbn = {9798400702433},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of the Organizing Committee, I extend a warm welcome to you at the 39th Annual ACM Symposium on Applied Computing (SAC 2024), taking place in \'{A}vila, Spain, and hosted by the University of Salamanca. For more than three decades, this international forum has been dedicated to computer scientists, engineers, and practitioners, providing a platform for presenting their research findings and results in various areas of applied computing. The organizing committee sincerely appreciates your participation in this exciting international event, and we hope that the conference proves interesting and beneficial for all attendees.},
location = {Avila, Spain}
}

@proceedings{10.1145/3593434,
title = {EASE '23: Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Oulu, Finland}
}

@proceedings{10.1145/3544902,
title = {ESEM '22: Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement},
year = {2022},
isbn = {9781450394277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Helsinki, Finland}
}

@proceedings{10.5555/3715674,
title = {SC-W '24: Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
year = {2024},
isbn = {9798350355543},
publisher = {IEEE Press},
location = {Atlanta, GA, USA}
}

@proceedings{10.1145/3629527,
title = {ICPE '24 Companion: Companion of the 15th ACM/SPEC International Conference on Performance Engineering},
year = {2024},
isbn = {9798400704451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to present the ICPE 2024 workshops program. ICPE workshops extend the main conference by providing a forum to foster discussion on hot and emerging topics from the broad field of performance engineering. They offer a highly dynamic venue to exchange ideas, establish new collaborations, and bootstrap debates on novel techniques, methodologies, and their associated early research results. Workshops feature various presentation formats, including research paper presentations, panel discussions, and keynote talks. Through these presentations and discussions with peer researchers, ICPE workshops help shape future research and identify promising research directions for performance engineering.},
location = {London, United Kingdom}
}

@proceedings{10.1145/3639478,
title = {ICSE-Companion '24: Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ICSE is the leading and, by far, the largest conference in Software Engineering, attracting researchers, practitioners, and students worldwide. ICSE2024 is co-located with 11 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3641032,
title = {ICISE '23: Proceedings of the 2023 8th International Conference on Information Systems Engineering},
year = {2023},
isbn = {9798400709173},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bangkok, Thailand}
}

@inproceedings{10.1145/99412.99487,
author = {Sheppard, John and Simpson, William R.},
title = {Using a competitive learning neural network to evaluate software complexity},
year = {1990},
isbn = {0897913477},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/99412.99487},
doi = {10.1145/99412.99487},
abstract = {With recent advances in neural networks, an increasing number of application areas are being explored for this technology. Also, as software takes a more prominent role in systems engineering, ensuring the quality of software is becoming a critical issue. This paper explores the application of one neural network paradigm—the competitive learning network—to the problem of evaluating software complexity. The network was developed by ARINC Research Corporation for its SofTest software analysis system, developed on a Sun workstation. In this paper, we discuss the network used in SofTest and the approach taken to train the network. We conclude with a discussion of the implications of the approach and areas for further research.},
booktitle = {Proceedings of the 1990 ACM SIGSMALL/PC Symposium on Small Systems},
pages = {262–267},
numpages = {6},
location = {Crystal City, Virginia, USA},
series = {SIGSMALL '90}
}

@proceedings{10.1145/3638530,
title = {GECCO '24 Companion: Proceedings of the Genetic and Evolutionary Computation Conference Companion},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, VIC, Australia}
}

@proceedings{10.1145/3592813,
title = {SBSI '23: Proceedings of the XIX Brazilian Symposium on Information Systems},
year = {2023},
isbn = {9798400707599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Macei\'{o}, Brazil}
}

@proceedings{10.1145/3575879,
title = {PCI '22: Proceedings of the 26th Pan-Hellenic Conference on Informatics},
year = {2022},
isbn = {9781450398541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Athens, Greece}
}

@proceedings{10.1145/3578527,
title = {ISEC '23: Proceedings of the 16th Innovations in Software Engineering Conference},
year = {2023},
isbn = {9798400700644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Allahabad, India}
}

@proceedings{10.1145/3664476,
title = {ARES '24: Proceedings of the 19th International Conference on Availability, Reliability and Security},
year = {2024},
isbn = {9798400717185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vienna, Austria}
}

@proceedings{10.1145/3613372,
title = {SBES '23: Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Campo Grande, Brazil}
}

@proceedings{10.1145/3697090,
title = {LADC '24: Proceedings of the 13th Latin-American Symposium on Dependable and Secure Computing},
year = {2024},
isbn = {9798400717406},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3555776,
title = {SAC '23: Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tallinn, Estonia}
}

@proceedings{10.1145/3643658,
title = {GAS '24: Proceedings of the ACM/IEEE 8th International Workshop on Games and Software Engineering},
year = {2024},
isbn = {9798400705618},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {GAS is an annual workshop that brings together researchers and practitioners who are keen on exchanging ideas and progressing techniques in the intersection of game engineering and software engineering.GAS explores how advanced technologies can be used to benefit the engineering of gameful systems, including entertainment games, serious games, and gamified applications. The goal of this one-day workshop is to bring together the greater community of software engineers and game engineers to encourage discussions from an interdisciplinary perspective, on the emerging research challenges around game and software engineering.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3594441,
title = {ICIEI '23: Proceedings of the 2023 8th International Conference on Information and Education Innovations},
year = {2023},
isbn = {9798400700613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Manchester, United Kingdom}
}

@inproceedings{10.1145/2020390.2020406,
author = {Paikari, Elham and Sun, Bo and Ruhe, Guenther and Livani, Emadoddin},
title = {Customization support for CBR-based defect prediction},
year = {2011},
isbn = {9781450307093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020390.2020406},
doi = {10.1145/2020390.2020406},
abstract = {Background: The prediction performance of a case-based reasoning (CBR) model is influenced by the combination of the following parameters: (i) similarity function, (ii) number of nearest neighbor cases, (iii) weighting technique used for attributes, and (iv) solution algorithm. Each combination of the above parameters is considered as an instantiation of the general CBR-based prediction method. The selection of an instantiation for a new data set with specific characteristics (such as size, defect density and language) is called customization of the general CBR method.Aims: For the purpose of defect prediction, we approach the question which combinations of parameters works best at which situation. Three more specific questions were studied:(RQ1) Does one size fit all? Is one instantiation always the best?(RQ2) If not, which individual and combined parameter settings occur most frequently in generating the best prediction results?(RQ3) Are there context-specific rules to support the customization?Method: In total, 120 different CBR instantiations were created and applied to 11 data sets from the PROMISE repository. Predictions were evaluated in terms of their mean magnitude of relative error (MMRE) and percentage Pred(α) of objects fulfilling a prediction quality level α. For the third research question, dependency network analysis was performed.Results: Most frequent parameter options for CBR instantiations were neural network based sensitivity analysis (as the weighting technique), un-weighted average (as the solution algorithm), and maximum number of nearest neighbors (as the number of nearest neighbors). Using dependency network analysis, a set of recommendations for customization was provided.Conclusion: An approach to support customization is provided. It was confirmed that application of context-specific rules across groups of similar data sets is risky and produces poor results.},
booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
articleno = {16},
numpages = {10},
keywords = {case-based reasoning, customization, defect prediction, dependency network analysis, instantiation},
location = {Banff, Alberta, Canada},
series = {Promise '11}
}

@proceedings{10.1145/3653081,
title = {IoTAAI '23: Proceedings of the 2023 5th International Conference on Internet of Things, Automation and Artificial Intelligence},
year = {2023},
isbn = {9798400716485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nanchang, China}
}

@proceedings{10.1145/3590777,
title = {EICC '23: Proceedings of the 2023 European Interdisciplinary Cybersecurity Conference},
year = {2023},
isbn = {9781450398299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Stavanger, Norway}
}

@proceedings{10.5555/3606010,
title = {ICSE '23: Proceedings of the 45th International Conference on Software Engineering},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Victoria, Australia}
}

@proceedings{10.1145/3576914,
title = {CPS-IoT Week '23: Proceedings of Cyber-Physical Systems and Internet of Things Week 2023},
year = {2023},
isbn = {9798400700491},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Antonio, TX, USA}
}

@proceedings{10.1145/3639477,
title = {ICSE-SEIP '24: Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@article{10.1145/2110356.2110360,
author = {Yuan, Ding and Zheng, Jing and Park, Soyeon and Zhou, Yuanyuan and Savage, Stefan},
title = {Improving Software Diagnosability via Log Enhancement},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {0734-2071},
url = {https://doi.org/10.1145/2110356.2110360},
doi = {10.1145/2110356.2110360},
abstract = {Diagnosing software failures in the field is notoriously difficult, in part due to the fundamental complexity of troubleshooting any complex software system, but further exacerbated by the paucity of information that is typically available in the production setting. Indeed, for reasons of both overhead and privacy, it is common that only the run-time log generated by a system (e.g., syslog) can be shared with the developers. Unfortunately, the ad-hoc nature of such reports are frequently insufficient for detailed failure diagnosis. This paper seeks to improve this situation within the rubric of existing practice. We describe a tool, LogEnhancer that automatically “enhances” existing logging code to aid in future post-failure debugging. We evaluate LogEnhancer on eight large, real-world applications and demonstrate that it can dramatically reduce the set of potential root failure causes that must be considered while imposing negligible overheads.},
journal = {ACM Trans. Comput. Syst.},
month = feb,
articleno = {4},
numpages = {28},
keywords = {software diagnosability, program analysis, failure diagnostics, debugging, Log}
}

@proceedings{10.1145/3652628,
title = {ICAICE '23: Proceedings of the 4th International Conference on Artificial Intelligence and Computer Engineering},
year = {2023},
isbn = {9798400708831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Dalian, China}
}

@proceedings{10.1145/3631204,
title = {CSCS '23: Proceedings of the 7th ACM Computer Science in Cars Symposium},
year = {2023},
isbn = {9798400704543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Darmstadt, Germany}
}

@proceedings{10.1145/3657604,
title = {L@S '24: Proceedings of the Eleventh ACM Conference on Learning @ Scale},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to present the Proceedings of the Eleventh Annual ACM Conference on Learning at Scale, L@S 2024, held July 18-20, 2024 at Georgia Tech in Atlanta, Georgia, USA.The Learning at Scale conference was created by the Association for Computing Machinery (ACM), inspired by the emergence of Massive Open Online Courses (MOOCs) and the accompanying shift in thinking about education. During the last few years, new opportunities for scaling up learning have emerged, like hybrid learning environments combining online and face-to-face, and informal learning enabled by all sorts of platforms (e.g., gamified language learning, citizen science communities, and collaborative programming communities). In the recent two years, the unprecedented development of generative AI has brought profound opportunities to scale the teaching and learning experiences, with the goal of enhancing learning for the increasingly diverse group of learners in both formal and informal contexts. L@S has evolved along with these emergent massive learning scenarios and opportunities and is today one of the most prominent venues for discussion of the highest quality of research on how learning and teaching can be transformed at scale, in diverse learning environments.The theme of L@S 2024 is Scaling Learning in the Age of AI. Rapid advances in AI have created new opportunities but also challenges for the Learning@Scale community. The advances in generative AI show potential to enhance pedagogical practices and the efficacy of learning at scale. This has led to an unprecedented level of interest in employing generative AI for scaling tutoring and feedback. The prevalence of such tools calls for new practices and understanding on how AI-based methods should be designed and developed to enhance the experiences and outcomes of teachers and learners.Learning@Scale 2024 solicits empirical and theoretical papers on, but not limited to, the following topics (in no particular order): 1) Instruction at scale: studies that examine how teachers and educators scale their instructions, what aspects of instruction could be scaled effectively, and which of these instructional strategies are the most effective for learning. 2) Interventions at scale: studies that examine the effects of interventions on student learning and performance when implemented at scale. We welcome studies that use both qualitative and quantitative methods. 3) The use of generative AI to scale learning: studies that investigate stakeholders' experiences with generative AI, students' and teachers' interactions with generative AI, and the potentials and limitations of using generative AI in education. 4) Systems and tools to support learning at scale: research that designs and develops systems and tools to support learning at scale. For example, this involves scaling learning through web-based systems, MOOCs, visualization, intelligent tutoring systems, gamification, immersive techniques (AR/VR/MR), mobile technologies, tangible interfaces, and various other technologies. 5) The evaluation of existing learning at scale systems and online learning environments using but not limited to the above-mentioned technologies. 6) Methods and algorithms that model learner behavior: research that contributes methods, algorithms, and pipelines that process large student data to enhance learning at scale. 7) Scaling learning in informal contexts: studies that explore how people take advantage of online environments to pursue their interests informally. 8) Review and synthesis of existing literature related to learning at scale. 9) Empirical studies and interventions that address equity, trust, algorithmic transparency and explainability, fairness and bias when using AI in education. 10) Research that addresses accessibility in learning at scale contexts. 11) Design and deployment of learning at scale systems for learners from underrepresented groups.},
location = {Atlanta, GA, USA}
}

@inproceedings{10.1109/ASE.2003.1240314,
author = {Guo, Lan and Cukic, Bojan and Singh, Harshinder},
title = {Predicting fault prone modules by the Dempster-Shafer belief networks},
year = {2003},
isbn = {0769520359},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2003.1240314},
doi = {10.1109/ASE.2003.1240314},
abstract = {This paper describes a novel methodology for predicting fault prone modules. The methodology is based on Dempster-Shafer (D-S) belief networks. Our approach consists of three steps: First, building the Dempster-Shafer network by the induction algorithm; Second, selecting the predictors (attributes) by the logistic procedure; Third, feeding the predictors describing the modules of the current project into the inducted Dempster-Shafer network and identifying fault prone modules. We applied this methodology to a NASA dataset. The prediction accuracy of our methodology is higher than that achieved by logistic regression or discriminant analysis on the same dataset.},
booktitle = {Proceedings of the 18th IEEE International Conference on Automated Software Engineering},
pages = {249–252},
numpages = {4},
location = {Montreal, Quebec, Canada},
series = {ASE'03}
}

@proceedings{10.1145/3600160,
title = {ARES '23: Proceedings of the 18th International Conference on Availability, Reliability and Security},
year = {2023},
isbn = {9798400707728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Benevento, Italy}
}

@proceedings{10.1145/3538969,
title = {ARES '22: Proceedings of the 17th International Conference on Availability, Reliability and Security},
year = {2022},
isbn = {9781450396707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vienna, Austria}
}

@proceedings{10.1145/3569966,
title = {CSSE '22: Proceedings of the 5th International Conference on Computer Science and Software Engineering},
year = {2022},
isbn = {9781450397780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guilin, China}
}

@proceedings{10.1145/3701625,
title = {SBQS '24: Proceedings of the XXIII Brazilian Symposium on Software Quality},
year = {2024},
isbn = {9798400717772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3551349,
title = {ASE '22: Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
year = {2022},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rochester, MI, USA}
}

@proceedings{10.1145/3671016,
title = {Internetware '24: Proceedings of the 15th Asia-Pacific Symposium on Internetware},
year = {2024},
isbn = {9798400707056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Macau, China}
}

@proceedings{10.1145/3597503,
title = {ICSE '24: Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3571788,
title = {VaMoS '23: Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Odense, Denmark}
}

@proceedings{10.1145/3629479,
title = {SBQS '23: Proceedings of the XXII Brazilian Symposium on Software Quality},
year = {2023},
isbn = {9798400707865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bras\'{\i}lia, Brazil}
}

@proceedings{10.1145/3658644,
title = {CCS '24: Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is with great enthusiasm that we, on behalf of the Organizing Committee, invite you to join us for the 31st ACM SIGSAC Conference on Computer and Communications Security (CCS), a premier security and privacy conference where researchers, practitioners, and educators come together to present, learn, and debate research, innovation, and trends in the field of Computer and Communications Security and Privacy.This year, we are proud to introduce our conference theme to be "Inclusion, Mentorship, Community." These three pillars reflect our collective commitment to fostering a vibrant, supportive, and forwardthinking environment within the CCS community. Particularly, we host our inaugural Doctoral Symposium, which offers PhD students a unique platform to receive timely, constructive feedback on their dissertation research from leading experts in our community. Additionally, our first-ever Diversity, Equity, and Inclusion (DEI) Workshop is designed to cultivate a culture that embraces diversity and champions equity in our field. Moreover, understanding the importance of guidance and support, we have organized panels focusing on Student Mentoring, Faculty Mentoring, and Public Service. These panels are designed to facilitate mentorship connections, share valuable experiences, and encourage service that extends the impact of our work beyond academia. These new initiatives are also opportunities to strengthen the bonds within our CCS community.Regarding the main conference, this year's main conference is our largest ever, featuring 328 paper presentations that showcase the latest research and developments in our field. We are also honored to have two distinguished keynote speakers: Dr. Dan Boneh and Dr. Gene Tsudik, who will share their invaluable insights and perspectives on pressing topics in security and privacy. Additionally, 18 specialized workshops will take place on the pre-conference and post-conference days, providing platforms for focused discussions and collaborations on numerous specialized topics.},
location = {Salt Lake City, UT, USA}
}

@proceedings{10.1145/3689031,
title = {EuroSys '25: Proceedings of the Twentieth European Conference on Computer Systems},
year = {2025},
isbn = {9798400711961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are delighted to welcome you to EuroSys 2025, the 20th edition of the European Conference on Computer Systems! We are excited to host EuroSys 2025 in the modern and dynamic city of Rotterdam, Netherlands. This year's EuroSys is very special as it is co-located (for the first time) with the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 2025). We hope you will enjoy an excellent technical program, engaging discussions, and networking opportunities in this vibrant city known for its innovative architecture, bustling port, and rich cultural scene.},
location = {Rotterdam, Netherlands}
}

@proceedings{10.1145/3564625,
title = {ACSAC '22: Proceedings of the 38th Annual Computer Security Applications Conference},
year = {2022},
isbn = {9781450397599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Austin, TX, USA}
}

@proceedings{10.1145/3579856,
title = {ASIA CCS '23: Proceedings of the 2023 ACM Asia Conference on Computer and Communications Security},
year = {2023},
isbn = {9798400700989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, VIC, Australia}
}

@proceedings{10.1145/3581784,
title = {SC '23: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
year = {2023},
isbn = {9798400701092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Started in 1988, the SC Conference has become the annual nexus for researchers and practitioners from academia, industry and government to share information and foster collaborations to advance the state of the art in High Performance Computing (HPC), Networking, Storage, and Analysis.},
location = {Denver, CO, USA}
}

@proceedings{10.1145/3600006,
title = {SOSP '23: Proceedings of the 29th Symposium on Operating Systems Principles},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the Proceedings of the 29th ACM Symposium on Operating Systems Principles (SOSP 2023). This year's program includes 43 papers that reflect today's broad range of topics that comprise modern computer systems research. The program committee carefully reviewed submitted papers and worked closely with the authors of selected papers to produce the collection of high-quality, readable papers presented here. We hope that you enjoy the program!},
location = {Koblenz, Germany}
}

@proceedings{10.1145/3578837,
title = {ICEEL '22: Proceedings of the 2022 6th International Conference on Education and E-Learning},
year = {2022},
isbn = {9781450398428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yamanashi, Japan}
}

@proceedings{10.1145/2970276,
title = {ASE '16: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@proceedings{10.1145/2950290,
title = {FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

