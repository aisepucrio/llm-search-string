@inproceedings{10.1145/2701319.2701326,
author = {Soares, Larissa Rocha and do Carmo Machado, Ivan and de Almeida, Eduardo Santana},
title = {Non-Functional Properties in Software Product Lines: A Reuse Approach},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701326},
doi = {10.1145/2701319.2701326},
abstract = {Software Product Line Engineering (SPLE) emerges for software organizations interested in customized products at reasonable costs. Based on the selection of features, stakeholders can derive programs satisfying a range of functional properties and non-functional ones. The explicit definition of Non-Functional Properties (NFP) during software configuration has been considered a challenging task. Dealing with them is not well established yet, neither in theory nor in practice. In this sense, we present a framework to specify NFP for SPLE and we also propose a reuse approach that promotes the reuse of NFP values during the product configuration. We discuss the results of a case study aimed to evaluate the applicability of the proposed work.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {67–74},
numpages = {8},
keywords = {Empirical Software Engineering, Quality Attributes, Software Product Line},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@inproceedings{10.1145/3382025.3414945,
author = {G\"{o}ttmann, Hendrik and Luthmann, Lars and Lochau, Malte and Sch\"{u}rr, Andy},
title = {Real-time-aware reconfiguration decisions for dynamic software product lines},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414945},
doi = {10.1145/3382025.3414945},
abstract = {Dynamic Software Product Lines (DSPL) have recently shown promising potentials as integrated engineering methodology for (self-)adaptive software systems. Based on the software-configuration principles of software product lines, DSPL additionally foster reconfiguration capabilities to continuously adapt software products to ever-changing environmental contexts. However, in most recent works concerned with finding near-optimal reconfiguration decisions, real-time aspects of reconfiguration processes are usually out of scope. In this paper, we present a model-based methodology for specifying and automatically analyzing real-time constraints of reconfiguration decisions in a feature-oriented and compositional way. Those real-time aware DSPL specifications are internally translated into timed automata, a well-founded formalism for real-time behaviors. This representation allows for formally reasoning about consistency and worst-case/best-case execution-time behaviors of sequences of reconfiguration decisions. The technique is implemented in a prototype tool and experimentally evaluated with respect to a set of case studies1.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {13},
numpages = {11},
keywords = {dynamic software product lines, reconfiguration decisions, timed automata},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3503229.3547026,
author = {Friesel, Birte and Elmenhorst, Kathrin and Kaiser, Lennart and M\"{u}ller, Michael and Spinczyk, Olaf},
title = {kconfig-webconf: retrofitting performance models onto kconfig-based software product lines},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547026},
doi = {10.1145/3503229.3547026},
abstract = {Despite decades of research and clear advantages, performance-aware configuration of real-world software product lines is still an exception rather than the norm. One reason for this may be tooling: configuration software with support for non-functional property models is generally not compatible with the configuration and build process of existing product lines. Specifically, the Kconfig language is popular in open source software projects, but neither language nor configuration frontends support performance models. To address this, we present kconfig-webconf: a performance-aware, Kconfig-compatible software product line configuration frontend. It is part of a toolchain that can automatically generate performance models with a minimal amount of changes to a software product line's build process. With such a performance model, kconfig-webconf can serve as a performance-aware drop-in replacement for existing Kconfig frontends. We evaluate its usage in five examples, including the busybox multi-call binary and the resKIL agricultural AI product line.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {58–61},
numpages = {4},
keywords = {kconfig, performance prediction, product lines, regression trees},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3503229.3547055,
author = {Friesel, Birte and M\"{u}ller, Michael and Ferraz, Matheus and Spinczyk, Olaf},
title = {On the relation of variability modeling languages and non-functional properties},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547055},
doi = {10.1145/3503229.3547055},
abstract = {Non-functional properties (NFPs) such as code size (RAM, ROM), performance, and energy consumption are at least as important as functional properties in many software development domains. When configuring a software product line - especially in the area of resource-constrained embedded systems - developers must be aware of the NFPs of the configured product instance. Several NFP-aware variability modeling languages have been proposed to address this in the past. However, it is not clear whether a variability modeling language is the best place for handling NFP-related concerns, or whether separate NFP prediction models should be preferred. We shine light onto this question by discussing limitations of state-of-the-art NFP-aware variability modeling languages, and find that both in terms of the development process and model accuracy a separate NFP model is favorable. Our quantitative analysis is based on six different software product lines, including the widely used busybox multi-call binary and the x264 video encoder. We use classification and regression trees (CART) and our recently proposed Regression Model Trees [8] as separate NFP models. These tree-based models can cover the effects of arbitrary feature interactions and thus easily outperform variability models with static, feature-wise NFP annotations. For example, when estimating the throughput of an embedded AI product line, static annotations come with a mean generalization error of 114.5% while the error of CART is only 9.4 %.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {140–144},
numpages = {5},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3377024.3377031,
author = {El-Sharkawy, Sascha and Krafczyk, Adam and Schmid, Klaus},
title = {Fast static analyses of software product lines: an example with more than 42,000 metrics},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377031},
doi = {10.1145/3377024.3377031},
abstract = {Context: Software metrics, as one form of static analyses, is a commonly used approach in software engineering in order to understand the state of a software system, in particular to identify potential areas prone to defects. Family-based techniques extract variability information from code artifacts in Software Product Lines (SPLs) to perform static analysis for all available variants. Many different types of metrics with numerous variants have been defined in literature. When counting all metrics including such variants, easily thousands of metrics can be defined. Computing all of them for large product lines can be an extremely expensive process in terms of performance and resource consumption.Objective: We address these performance and resource challenges while supporting customizable metric suites, which allow running both, single system and variability-aware code metrics.Method: In this paper, we introduce a partial parsing approach used for the efficient measurement of more than 42,000 code metric variations. The approach covers variability information and restricts parsing to the relevant parts of the Abstract Syntax Tree (AST).Conclusions: This partial parsing approach is designed to cover all relevant information to compute a broad variety of variability-aware code metrics on code artifacts containing annotation-based variability, e.g., realized with C-preprocessor statements. It allows for the flexible combination of single system and variability-aware metrics, which is not supported by existing tools. This is achieved by a novel representation of partially parsed product line code artifacts, which is tailored to the computation of the metrics. Our approach consumes considerably less resources, especially when computing many metric variants in parallel.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {8},
numpages = {9},
keywords = {AST, SPL, abstract syntax trees, feature models, implementation, metrics, software product lines, variability models},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@article{10.1145/3628158,
author = {Xiang, Yi and Huang, Han and Li, Sizhe and Li, Miqing and Luo, Chuan and Yang, Xiaowei},
title = {Automated Test Suite Generation for Software Product Lines Based on Quality-Diversity Optimization},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3628158},
doi = {10.1145/3628158},
abstract = {A Software Product Line (SPL) is a set of software products that are built from a variability model. Real-world SPLs typically involve a vast number of valid products, making it impossible to individually test each of them. This arises the need for automated test suite generation, which was previously modeled as either a single-objective or a multi-objective optimization problem considering only objective functions. This article provides a completely different mathematical model by exploiting the benefits of Quality-Diversity (QD) optimization that is composed of not only an objective function (e.g., t-wise coverage or test suite diversity) but also a user-defined behavior space (e.g., the space with test suite size as its dimension). We argue that the new model is more suitable and generic than the two alternatives because it provides at a time a large set of diverse (measured in the behavior space) and high-performing solutions that can ease the decision-making process. We apply MAP-Elites, one of the most popular QD algorithms, to solve the model. The results of the evaluation, on both realistic and artificial SPLs, are promising, with MAP-Elites significantly and substantially outperforming both single- and multi-objective approaches, and also several state-of-the-art SPL testing tools. In summary, this article provides a new and promising perspective on the test suite generation for SPLs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {46},
numpages = {52},
keywords = {Software Product Line, automated test suite generation, Quality-Diversity (QD) optimization}
}

@inproceedings{10.1145/3167132.3167353,
author = {Pereira, Juliana Alves and Martinez, Jabier and Gurudu, Hari Kumar and Krieter, Sebastian and Saake, Gunter},
title = {Visual guidance for product line configuration using recommendations and non-functional properties},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167353},
doi = {10.1145/3167132.3167353},
abstract = {Software Product Lines (SPLs) are a mature approach for the derivation of a family of products using systematic reuse. Different combinations of predefined features enable tailoring the product to fit the needs of each customer. These needs are related to functional properties of the system (optional features) as well as non-functional properties (e.g., performance or cost of the final product). In industrial scenarios, the configuration process of a final product is complex and the tool support is usually limited to check functional properties interdependencies. In addition, the importance of nonfunctional properties as relevant drivers during configuration has been overlooked. Thus, there is a lack of holistic paradigms integrating recommendation systems and visualizations that can help the decision makers. In this paper, we propose and evaluate an interrelated set of visualizations for the configuration process filling these gaps. We integrate them as part of the FeatureIDE tool and we evaluate its effectiveness, scalability, and performance.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {2058–2065},
numpages = {8},
keywords = {configuration, feature model, recommendation systems, software product lines, visualization},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3233027.3233039,
author = {Pereira, Juliana Alves and Schulze, Sandro and Figueiredo, Eduardo and Saake, Gunter},
title = {N-dimensional tensor factorization for self-configuration of software product lines at runtime},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233039},
doi = {10.1145/3233027.3233039},
abstract = {Dynamic software product lines demand self-adaptation of their behavior to deal with runtime contextual changes in their environment and offer a personalized product to the user. However, taking user preferences and context into account impedes the manual configuration process, and thus, an efficient and automated procedure is required. To automate the configuration process, context-aware recommendation techniques have been acknowledged as an effective mean to provide suggestions to a user based on their recognized context. In this work, we propose a collaborative filtering method based on tensor factorization that allows an integration of contextual data by modeling an N-dimensional tensor User-Feature-Context instead of the traditional two-dimensional User-Feature matrix. In the proposed approach, different types of non-functional properties are considered as additional contextual dimensions. Moreover, we show how to self-configure software product lines by applying our N-dimensional tensor factorization recommendation approach. We evaluate our approach by means of an empirical study using two datasets of configurations derived for medium-sized product lines. Our results reveal significant improvements in the predictive accuracy of the configuration over a state-of-the-art non-contextual matrix factorization approach. Moreover, it can scale up to a 7-dimensional tensor containing hundred of configurations in a couple of milliseconds.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {87–97},
numpages = {11},
keywords = {recommender systems, runtime decision-making, self-configuration, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3522664.3528602,
author = {Friesel, Birte and Spinczyk, Olaf},
title = {Black-box models for non-functional properties of AI software systems},
year = {2022},
isbn = {9781450392754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3522664.3528602},
doi = {10.1145/3522664.3528602},
abstract = {Non-functional properties (NFPs) such as latency, memory requirements, or hardware cost are an important characteristic of AI software systems, especially in the domain of resource-constrained embedded devices. Embedded AI products require sufficient resources for satisfactory latency and accuracy, but should also be cost-efficient and therefore not use more powerful hardware than strictly necessary. Traditionally, modeling and optimization efforts focus on the AI architecture, utilizing methods such as neural architecture search (NAS). However, before developers can start optimizing, they need to know which architectures are suitable candidates for their use case. To this end, architectures must be viewed in context: model post-processing (e.g. quantization), hardware platform, and run-time configuration such as batching all have significant effects on NFPs and therefore on AI architecture performance. Moreover, scalar parameters such as batch size cannot be benchmarked exhaustively. We argue that it is worthwhile to address this issue by means of black-box models before deciding on AI architectures for optimization and hardware/software platforms for inference. To support our claim, we present an AI product line with variable hardware and software components, perform benchmarks, and present notable results. Additionally, we evaluate both compactness and generalization capabilities of regression tree-based modeling approaches from the machine learning and product line engineering communities. We find that linear model trees perform best: they can capture NFPs of known AI configurations with a mean error of up to 13 %, and can predict unseen configurations with a mean error of 10 to 26 %. We find linear model trees to be more compact and interpretable than other tree-based approaches.},
booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
pages = {170–180},
numpages = {11},
keywords = {AI, performance prediction, product lines, regression trees},
location = {Pittsburgh, Pennsylvania},
series = {CAIN '22}
}

@inproceedings{10.1145/3233027.3236395,
author = {Pereira, Juliana Alves and Maciel, Lucas and Noronha, Thiago F. and Figueiredo, Eduardo},
title = {Heuristic and exact algorithms for product configuration in software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236395},
doi = {10.1145/3233027.3236395},
abstract = {The Software Product Line (SPL) configuration field is an active area of research and has attracted both practitioners and researchers attention in the last years. A key part of an SPL configuration is a feature model that represents features and their dependencies (i.e., SPL configuration rules). This model can be extended by adding Non-Functional Properties (NFPs) as feature attributes resulting in Extended Feature Models (EFMs). Configuring products from an EFM requires considering the configuration rules of the model and satisfying the product functional and non-functional requirements. Although the configuration of a product arising from EFMs may reduce the space of valid configurations, selecting the most appropriate set of features is still an overwhelming task due to many factors including technical limitations and diversity of contexts. Consequently, configuring large and complex SPLs by using configurators is often beyond the users' capabilities of identifying valid combinations of features that match their (non-functional) requirements. To overcome this limitation, several approaches have modeled the product configuration task as a combinatorial optimization problem and proposed constraint programming algorithms to automatically derive a configuration. Although these approaches do not require any user intervention to guarantee the optimality of the generated configuration, due to the NP-hard computational complexity of finding an optimal variant, exact approaches have inefficient exponential time. Thus, to improve scalability and performance issues, we introduced the adoption of a greedy heuristic algorithm and a biased random-key genetic algorithm (BRKGA). Our experiment results show that our proposed heuristics found optimal solutions for all instances where those are known. For the instances where optimal solutions are not known, the greedy heuristic outperformed the best solution obtained by a one-hour run of the exact algorithm by up to 67.89%. Although the BRKGA heuristic slightly outperformed the greedy heuristic, it has shown larger running times (especially on the largest instances). Therefore, to ensure a good user experience and enable a very fast configuration task, we extended a state-of-the-art configurator with the proposed greedy heuristic approach.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {247},
numpages = {1},
keywords = {configuration optimization, search-based software engineering, software product line configuration, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3233027.3233035,
author = {Varshosaz, Mahsa and Al-Hajjaji, Mustafa and Th\"{u}m, Thomas and Runge, Tobias and Mousavi, Mohammad Reza and Schaefer, Ina},
title = {A classification of product sampling for software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233035},
doi = {10.1145/3233027.3233035},
abstract = {The analysis of software product lines is challenging due to the potentially large number of products, which grow exponentially in terms of the number of features. Product sampling is a technique used to avoid exhaustive testing, which is often infeasible. In this paper, we propose a classification for product sampling techniques and classify the existing literature accordingly. We distinguish the important characteristics of such approaches based on the information used for sampling, the kind of algorithm, and the achieved coverage criteria. Furthermore, we give an overview on existing tools and evaluations of product sampling techniques. We share our insights on the state-of-the-art of product sampling and discuss potential future work.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {1–13},
numpages = {13},
keywords = {domain models, feature interaction, sampling algorithms, software product lines, testing},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3336294.3336309,
author = {Temple, Paul and Acher, Mathieu and Perrouin, Gilles and Biggio, Battista and Jezequel, Jean-Marc and Roli, Fabio},
title = {Towards Quality Assurance of Software Product Lines with Adversarial Configurations},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336309},
doi = {10.1145/3336294.3336309},
abstract = {Software product line (SPL) engineers put a lot of effort to ensure that, through the setting of a large number of possible configuration options, products are acceptable and well-tailored to customers' needs. Unfortunately, options and their mutual interactions create a huge configuration space which is intractable to exhaustively explore. Instead of testing all products, machine learning is increasingly employed to approximate the set of acceptable products out of a small training sample of configurations. Machine learning (ML) techniques can refine a software product line through learned constraints and a priori prevent non-acceptable products to be derived. In this paper, we use adversarial ML techniques to generate adversarial configurations fooling ML classifiers and pinpoint incorrect classifications of products (videos) derived from an industrial video generator. Our attacks yield (up to) a 100% misclassification rate and a drop in accuracy of 5%. We discuss the implications these results have on SPL quality assurance.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {277–288},
numpages = {12},
keywords = {machine learning, quality assurance, software product line, software testing, software variability},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1145/3088440,
author = {Acher, Mathieu and Lopez-Herrejon, Roberto E. and Rabiser, Rick},
title = {Teaching Software Product Lines: A Snapshot of Current Practices and Challenges},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
url = {https://doi.org/10.1145/3088440},
doi = {10.1145/3088440},
abstract = {Software Product Line (SPL) engineering has emerged to provide the means to efficiently model, produce, and maintain multiple similar software variants, exploiting their common properties, and managing their variabilities (differences). With over two decades of existence, the community of SPL researchers and practitioners is thriving, as can be attested by the extensive research output and the numerous successful industrial projects. Education has a key role to support the next generation of practitioners to build highly complex, variability-intensive systems. Yet, it is unclear how the concepts of variability and SPLs are taught, what are the possible missing gaps and difficulties faced, what are the benefits, and what is the material available. Also, it remains unclear whether scholars teach what is actually needed by industry. In this article, we report on three initiatives we have conducted with scholars, educators, industry practitioners, and students to further understand the connection between SPLs and education, that is, an online survey on teaching SPLs we performed with 35 scholars, another survey on learning SPLs we conducted with 25 students, as well as two workshops held at the International Software Product Line Conference in 2014 and 2015 with both researchers and industry practitioners participating. We build upon the two surveys and the workshops to derive recommendations for educators to continue improving the state of practice of teaching SPLs, aimed at both individual educators as well as the wider community.},
journal = {ACM Trans. Comput. Educ.},
month = oct,
articleno = {2},
numpages = {31},
keywords = {Software product lines, software engineering teaching, software product line teaching, variability modeling}
}

@inproceedings{10.1145/3526071.3527518,
author = {Blender, Timo and Schlegel, Christian},
title = {Dynamic allocation of service robot resources to an order picking task considering functional and non-functional properties},
year = {2023},
isbn = {9781450393171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526071.3527518},
doi = {10.1145/3526071.3527518},
abstract = {Industry 4.0 processes have often varying requirements. A service robot and a team of service robots respectively represent a flexible resource. That means, it possesses variability that can possibly be configured in such a way that it is able to fulfill the requirements of industry 4.0 processes. Determining whether that is the case and how that has to happen is an important part of variability management. Based on a model-driven general method for variability management in a robotics software ecosystem, we present here a concrete use case (model) in which we allocate for an order picking task with specific time requirements either a single fitting service robot or a collaboration of two fitting service robots. Relevant properties of the service robots considered are both functional (are the capabilities to execute the tasks available?) as well as non-functional (the desired velocity parameterization while executing the individual navigation sub tasks limited by the respective maximum speed of a service robot).},
booktitle = {Proceedings of the 4th International Workshop on Robotics Software Engineering},
pages = {25–32},
numpages = {8},
keywords = {model-driven software development, non-functional properties, robotics software ecosystem, service robot collaboration, variability management},
location = {Pittsburgh, Pennsylvania},
series = {RoSE '22}
}

@article{10.1145/3361146,
author = {Hierons, Robert M. and Li, Miqing and Liu, Xiaohui and Parejo, Jose Antonio and Segura, Sergio and Yao, Xin},
title = {Many-Objective Test Suite Generation for Software Product Lines},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3361146},
doi = {10.1145/3361146},
abstract = {A Software Product Line (SPL) is a set of products built from a number of features, the set of valid products being defined by a feature model. Typically, it does not make sense to test all products defined by an SPL and one instead chooses a set of products to test (test selection) and, ideally, derives a good order in which to test them (test prioritisation). Since one cannot know in advance which products will reveal faults, test selection and prioritisation are normally based on objective functions that are known to relate to likely effectiveness or cost. This article introduces a new technique, the grid-based evolution strategy (GrES), which considers several objective functions that assess a selection or prioritisation and aims to optimise on all of these. The problem is thus a many-objective optimisation problem. We use a new approach, in which all of the objective functions are considered but one (pairwise coverage) is seen as the most important. We also derive a novel evolution strategy based on domain knowledge. The results of the evaluation, on randomly generated and realistic feature models, were promising, with GrES outperforming previously proposed techniques and a range of many-objective optimisation algorithms.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {2},
numpages = {46},
keywords = {Software product line, multi-objective optimisation, test prioritisation, test selection}
}

@inproceedings{10.1145/3483899.3483907,
author = {Uch\^{o}a, Anderson and Assun\c{c}\~{a}o, Wesley Klewerton Guez and Garcia, Alessandro},
title = {Do Critical Components Smell Bad? An Empirical Study with Component-based Software Product Lines},
year = {2021},
isbn = {9781450384193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483899.3483907},
doi = {10.1145/3483899.3483907},
abstract = {Component-based software product line (SPL) consists of a set of software products that share common components. For a proper SPL product composition, each component has to follow three principles: encapsulating a single feature, restricting data access, and be replaceable. However, it is known that developers usually introduce anomalous structures, i.e., code smells, along the implementation of components. These code smells might violate one or more component principles and hinder the SPL product composition. Thus, developers should identify code smells in component-based SPLs, especially those affecting highly interconnected components, which are called critical components. Nevertheless, there is limited evidence of how smelly these critical components tend to be in component-based SPLs. To address this limitation, this paper presents a survey with developers of three SPLs. We inquire these developers about their perceptions of a critical component. Then, we characterize critical components per SPL, and identify nine recurring types of code smells. Finally, we quantitatively assess the smelliness of the critical components. Our results suggest that: (i) critical components are ten times more prone to have code smells than non-critical ones; (ii) the most frequent code smell types affecting critical components violate several component principles together; and (iii) these smell types affect multiple SPL components.},
booktitle = {Proceedings of the 15th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {21–30},
numpages = {10},
keywords = {Component-based software product line, empirical study, smell},
location = {Joinville, Brazil},
series = {SBCARS '21}
}

@inproceedings{10.1145/2739482.2768422,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Assun\c{c}\~{a}o, Wesley K.G. and Fischer, Stefan and Vergilio, Silvia R. and Egyed, Alexander},
title = {Genetic Improvement for Software Product Lines: An Overview and a Roadmap},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2768422},
doi = {10.1145/2739482.2768422},
abstract = {Software Product Lines (SPLs) are families of related software systems that provide different combinations of features. Extensive research and application attest to the significant economical and technological benefits of employing SPL practices. However, there are still several challenges that remain open. Salient among them is reverse engineering SPLs from existing variants of software systems and their subsequent evolution. In this paper, we aim at sketching connections between research on these open SPL challenges and ongoing work on Genetic Improvement. Our hope is that by drawing such connections we can spark the interest of both research communities on the exciting synergies at the intersection of these subject areas.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {823–830},
numpages = {8},
keywords = {evolutionary algorithms, genetic improvement, genetic programming, software product lines, variability},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@inproceedings{10.1145/3106195.3106212,
author = {Marimuthu, C. and Chandrasekaran, K.},
title = {Systematic Studies in Software Product Lines: A Tertiary Study},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106212},
doi = {10.1145/3106195.3106212},
abstract = {Software product lines are widely used in the software industries to increase the re-usability and to decrease maintenance cost. On the other hand, systematic reviews are widely used in the software engineering research community to provide the overview of the research field and practitioners guidelines. Researchers have conducted many systematic studies on the different aspects of SPLs. To the best of our knowledge, till now there is no tertiary study conducted on systematic studies of SPL related research topics. In this paper, we aim at conducting a systematic mapping study of existing systematic studies to report the overview of the findings for researchers and practitioners. We performed snowballing and automated search to find out the relevant systematic studies. As a result, we analyzed 60 relevant studies to answer 5 research questions. The main focus of this tertiary study is to highlight the research topics, type of published reviews, active researchers and publication forums. Additionally, we highlight some of the limitations of the systematic studies. The important finding of this study is that the research field is well matured as the systematic studies covered a wide range of research topics. Another important finding is that many studies provided information for practitioners as well as researchers which is a notable improvement in the systematic reviews. However, many studies failed to assess the quality of the primary studies which is the major limitation of the existing systematic studies.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {143–152},
numpages = {10},
keywords = {software product line, systematic review, tertiary study},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2791060.2791078,
author = {Vale, Gustavo and Albuquerque, Danyllo and Figueiredo, Eduardo and Garcia, Alessandro},
title = {Defining metric thresholds for software product lines: a comparative study},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791078},
doi = {10.1145/2791060.2791078},
abstract = {A software product line (SPL) is a set of software systems that share a common and variable set of features. Software metrics provide basic means to quantify several modularity aspects of SPLs. However, the effectiveness of the SPL measurement process is directly dependent on the definition of reliable thresholds. If thresholds are not properly defined, it is difficult to actually know whether a given metric value indicates a potential problem in the feature implementation. There are several methods to derive thresholds for software metrics. However, there is little understanding about their appropriateness for the SPL context. This paper aims at comparing three methods to derive thresholds based on a benchmark of 33 SPLs. We assess to what extent these methods derive appropriate values for four metrics used in product-line engineering. These thresholds were used for guiding the identification of a typical anomaly found in features' implementation, named God Class. We also discuss the lessons learned on using such methods to derive thresholds for SPLs.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {176–185},
numpages = {10},
keywords = {metrics, software product lines, thresholds},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3646548.3676545,
author = {Azonhoumon, Vidjinnangni Alphonse Ignace},
title = {Design of a Meta-Factory for Product Lines Model-Driven Software},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676545},
doi = {10.1145/3646548.3676545},
abstract = {Software Product Lines (SPLs) offer a robust method for reusing code in software development. Despite various approaches to managing variability in SPLs, existing tools fall short of fully meeting industry needs, such as capturing non-functional requirements, modelling variability mechanisms, combining multiple tactics, maintaining traceability across features, tactics and assets, and ultimately, deriving products. The Bespoke project addresses these gaps by introducing a novel model-driven approach that transitions from Domain-Specific Modelling Language (DSML) to Feature Model (FM), allowing the integration of non-functional properties, and guiding products derivation with comprehensive tactics management. This solution is later validated through evaluation on a real-world SPL project focused on ageing. By leveraging existing tools and focusing on product derivation, this project aims to significantly improve code reuse and product derivation in SPLs, offering practical solutions for industry adoption. This article presents research questions related to the implementation of this methodology.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {6–10},
numpages = {5},
keywords = {DSML, Product derivation, Quality attributes, SPL, Tactics},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/2647908.2655972,
author = {Meinicke, Jens and Th\"{u}m, Thomas and Schr\"{o}ter, Reimar and Benduhn, Fabian and Saake, Gunter},
title = {An overview on analysis tools for software product lines},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655972},
doi = {10.1145/2647908.2655972},
abstract = {A software product line is a set of different software products that share commonalities. For a selection of features, specialized products of one domain can be generated automatically from domain artifacts. However, analyses of software product lines need to handle a large number of products that can be exponential in the number of features. In the last decade, many approaches have been proposed to analyze software product lines efficiently. For some of these approaches tool support is available. Based on a recent survey on analysis for software product lines, we provide a first overview on such tools. While our discussion is limited to analysis tools, we provide an accompanying website covering further tools for product-line development. We compare tools according to their analysis and implementation strategy to identify underrepresented areas. In addition, we want to ease the reuse of existing tools for researchers and students, and to simplify research transfer to practice.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {94–101},
numpages = {8},
keywords = {code metrics, model checking, non-functional properties, sampling, software product lines, static analysis, testing, theorem proving, tool support, type checking},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1145/3034827,
author = {Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz},
title = {CASE Tool Support for Variability Management in Software Product Lines},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3034827},
doi = {10.1145/3034827},
abstract = {Software product lines (SPL) aim at reducing time-to-market and increasing software quality through extensive, planned reuse of artifacts. An essential activity in SPL is variability management, i.e., defining and managing commonality and variability among member products. Due to the large scale and complexity of today's software-intensive systems, variability management has become increasingly complex to conduct. Accordingly, tool support for variability management has been gathering increasing momentum over the last few years and can be considered a key success factor for developing and maintaining SPLs. While several studies have already been conducted on variability management, none of these analyzed the available tool support in detail. In this work, we report on a survey in which we analyzed 37 existing variability management tools identified using a systematic literature review to understand the tools’ characteristics, maturity, and the challenges in the field. We conclude that while most studies on variability management tools provide a good motivation and description of the research context and challenges, they often lack empirical data to support their claims and findings. It was also found that quality attributes important for the practical use of tools such as usability, integration, scalability, and performance were out of scope for most studies.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {14},
numpages = {45},
keywords = {Software engineering, computer-aided software engineering, software variability}
}

@inproceedings{10.1145/2866614.2866628,
author = {Th\"{u}m, Thomas and Winkelmann, Tim and Schr\"{o}ter, Reimar and Hentschel, Martin and Kr\"{u}ger, Stefan},
title = {Variability Hiding in Contracts for Dependent Software Product Lines},
year = {2016},
isbn = {9781450340199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2866614.2866628},
doi = {10.1145/2866614.2866628},
abstract = {Software product lines are used to efficiently develop and verify similar software products. While they focus on reuse of artifacts between products, a product line may also be reused itself in other product lines. A challenge with such dependent product lines is evolution; every change in a product line may influence all dependent product lines. With variability hiding, we aim to hide certain features and their artifacts in dependent product lines. In prior work, we focused on feature models and implementation artifacts. We build on this by discussing how variability hiding can be extended to specifications in terms of method contracts. We illustrate variability hiding in contracts by means of a running example and share our insights with preliminary experiments on the benefits for formal verification. In particular, we find that not every change in a certain product line requires a re-verification of other dependent product lines.},
booktitle = {Proceedings of the 10th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Multi product line, deductive verification, method contracts},
location = {Salvador, Brazil},
series = {VaMoS '16}
}

@inproceedings{10.1145/2658761.2658767,
author = {Ruprecht, Andreas and Heinloth, Bernhard and Lohmann, Daniel},
title = {Automatic feature selection in large-scale system-software product lines},
year = {2014},
isbn = {9781450331616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658761.2658767},
doi = {10.1145/2658761.2658767},
abstract = {System software can typically be configured at compile time via a comfortable feature-based interface to tailor its functionality towards a specific use case. However, with the growing number of features, this tailoring process becomes increasingly difficult: As a prominent example, the Linux kernel in v3.14 provides nearly 14 000 configuration options to choose from. Even developers of embedded systems refrain from trying to build a minimized distinctive kernel configuration for their device – and thereby waste memory and money for unneeded functionality. In this paper, we present an approach for the automatic use-case specific tailoring of system software for special-purpose embedded systems. We evaluate the effectiveness of our approach on the example of Linux by generating tailored kernels for well-known applications of the Rasperry Pi and a Google Nexus 4 smartphone. Compared to the original configurations, our approach leads to memory savings of 15–70 percent and requires only very little manual intervention.},
booktitle = {Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences},
pages = {39–48},
numpages = {10},
keywords = {Feature Selection, Linux, Software Product Lines, Software Tailoring},
location = {V\"{a}ster\r{a}s, Sweden},
series = {GPCE 2014}
}

@inproceedings{10.1145/2576768.2598305,
author = {Lopez-Herrejon, Roberto Erick and Javier Ferrer, Javier and Chicano, Francisco and Haslinger, Evelyn Nicole and Egyed, Alexander and Alba, Enrique},
title = {A parallel evolutionary algorithm for prioritized pairwise testing of software product lines},
year = {2014},
isbn = {9781450326629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2576768.2598305},
doi = {10.1145/2576768.2598305},
abstract = {Software Product Lines (SPLs) are families of related software systems, which provide different feature combinations. Different SPL testing approaches have been proposed. However, despite the extensive and successful use of evolutionary computation techniques for software testing, their application to SPL testing remains largely unexplored. In this paper we present the Parallel Prioritized product line Genetic Solver (PPGS), a parallel genetic algorithm for the generation of prioritized pairwise testing suites for SPLs. We perform an extensive and comprehensive analysis of PPGS with 235 feature models from a wide range of number of features and products, using 3 different priority assignment schemes and 5 product prioritization selection strategies. We also compare PPGS with the greedy algorithm prioritized-ICPL. Our study reveals that overall PPGS obtains smaller covering arrays with an acceptable performance difference with prioritized-ICPL.},
booktitle = {Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {1255–1262},
numpages = {8},
keywords = {combinatorial interaction testing, feature models, pairwise testing, software product lines},
location = {Vancouver, BC, Canada},
series = {GECCO '14}
}

@inproceedings{10.1145/2577080.2577095,
author = {Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha and Baier, Christel},
title = {Probabilistic model checking for energy analysis in software product lines},
year = {2014},
isbn = {9781450327725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2577080.2577095},
doi = {10.1145/2577080.2577095},
abstract = {In a software product line (SPL), a collection of software products is defined by their commonalities in terms of features rather than explicitly specifying all products one-by-one. Several verification techniques were adapted to establish temporal properties of SPLs. Symbolic and family-based model checking have been proven to be successful for tackling the combinatorial blow-up arising when reasoning about several feature combinations. However, most formal verification approaches for SPLs presented in the literature focus on the static SPLs, where the features of a product are fixed and cannot be changed during runtime. This is in contrast to dynamic SPLs, allowing to adapt feature combinations of a product dynamically after deployment.The main contribution of the paper is a compositional modeling framework for dynamic SPLs, which supports probabilistic and nondeterministic choices and allows for quantitative analysis. We specify the feature changes during runtime within an automata-based coordination component, enabling to reason over strategies how to trigger dynamic feature changes for optimizing various quantitative objectives, e.g., energy or monetary costs and reliability. For our framework there is a natural and conceptually simple translation into the input language of the prominent probabilistic model checker PRISM. This facilitates the application of PRISM's powerful symbolic engine to the operational behavior of dynamic SPLs and their family-based analysis against various quantitative queries. We demonstrate feasibility of our approach by a case study issuing an energy-aware bonding network device.},
booktitle = {Proceedings of the 13th International Conference on Modularity},
pages = {169–180},
numpages = {12},
keywords = {dynamic features, energy analysis, probabilistic model checking, software product lines},
location = {Lugano, Switzerland},
series = {MODULARITY '14}
}

@inproceedings{10.1145/2491627.2491631,
author = {Myll\"{a}rniemi, Varvana and Savolainen, Juha and M\"{a}nnist\"{o}, Tomi},
title = {Performance variability in software product lines: a case study in the telecommunication domain},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491631},
doi = {10.1145/2491627.2491631},
abstract = {In the research on software product lines, product variants typically differ by their functionality, and quality attributes are more or less similar across products. To accumulate empirical evidence, this paper presents a descriptive case study of performance variability in a software product line of mobile network base stations. The goal is to study the motivation to vary performance, and the strategy for realizing performance variability in the product line architecture. The results highlight that the evolution of customer needs motivates performance variability; performance variability can be realized either with software or hardware variability strategy, with the latter often being prevailing; and the software strategy can be kept focused by downgrading performance.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {32–41},
numpages = {10},
keywords = {architecture, case study, software product line, variability},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.5555/2821357.2821367,
author = {Baresi, Luciano and Quinton, Cl\'{e}ment},
title = {Dynamically evolving the structural variability of dynamic software product lines},
year = {2015},
publisher = {IEEE Press},
abstract = {A Dynamic Software Product Line (dspl) is a widely used approach to handle variability at runtime, e.g., by activating or deactivating features to adapt the running configuration. With the emergence of highly configurable and evolvable systems, dspls have to cope with the evolution of their structural variability, i.e., the Feature Model (fm) used to derive the configuration. So far, little is known about the evolution of the fm while a configuration derived from this fm is running. In particular, such a dynamic evolution changes the dspl configuration space, which is thus unsynchronized with the running configuration and its adaptation capabilities. In this position paper, we propose and describe an initial architecture to manage the dynamic evolution of dspls and their synchronization. In particular, we explain how this architecture supports the evolution of dspls based on fms extended with cardinality and attributes, which, to the best of our knowledge, has never been addressed yet.},
booktitle = {Proceedings of the 10th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {57–63},
numpages = {7},
location = {Florence, Italy},
series = {SEAMS '15}
}

@inproceedings{10.1145/2897010.2897011,
author = {Fischer, Stefan and Lopez-Herrejon, Roberto E. and Ramler, Rudolf and Egyed, Alexander},
title = {A preliminary empirical assessment of similarity for combinatorial interaction testing of software product lines},
year = {2016},
isbn = {9781450341660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897010.2897011},
doi = {10.1145/2897010.2897011},
abstract = {Extensive work on Search-Based Software Testing for Software Product Lines has been published in the last few years. Salient among them is the use of similarity as a surrogate metric for t-wise coverage whenever higher strengths are needed or whenever the size of the test suites is infeasible because of technological or budget limitations. Though promising, this metric has not been assessed with real fault data. In this paper, we address this limitation by using Drupal, a widely used open source web content management system, as an industry-strength case study for which both variability information and fault data have been recently made available. Our preliminary assessment corroborates some of the previous findings but also raises issues on some assumptions and claims made. We hope our work encourages further empirical evaluations of Combinatorial Interaction Testing approaches for Software Product Lines.},
booktitle = {Proceedings of the 9th International Workshop on Search-Based Software Testing},
pages = {15–18},
numpages = {4},
location = {Austin, Texas},
series = {SBST '16}
}

@inproceedings{10.1145/1368088.1368124,
author = {Figueiredo, Eduardo and Cacho, Nelio and Sant'Anna, Claudio and Monteiro, Mario and Kulesza, Uira and Garcia, Alessandro and Soares, S\'{e}rgio and Ferrari, Fabiano and Khan, Safoora and Castor Filho, Fernando and Dantas, Francisco},
title = {Evolving software product lines with aspects: an empirical study on design stability},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1368088.1368124},
doi = {10.1145/1368088.1368124},
abstract = {Software product lines (SPLs) enable modular, large-scale reuse through a software architecture addressing multiple core and varying features. To reap the benefits of SPLs, their designs need to be stable. Design stability encompasses the sustenance of the product line's modularity properties in the presence of changes to both the core and varying features. It is usually assumed that aspect-oriented programming promotes better modularity and changeability of product lines than conventional variability mechanisms, such as conditional compilation. However, there is no empirical evidence on its efficacy to prolong design stability of SPLs through realistic development scenarios. This paper reports a quantitative study that evolves two SPLs to assess various design stability facets of their aspect-oriented implementations. Our investigation focused upon a multi-perspective analysis of the evolving product lines in terms of modularity, change propagation, and feature dependency. We have identified a number of scenarios which positively or negatively affect the architecture stability of aspectual SPLs.},
booktitle = {Proceedings of the 30th International Conference on Software Engineering},
pages = {261–270},
numpages = {10},
keywords = {aspect-oriented programming, empirical evaluation, software product lines},
location = {Leipzig, Germany},
series = {ICSE '08}
}

@inproceedings{10.1145/3646548.3676546,
author = {G\"{u}thing, Lukas and Pett, Tobias and Schaefer, Ina},
title = {Out-of-the-Box Prediction of Non-Functional Variant Properties Using Automated Machine Learning},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676546},
doi = {10.1145/3646548.3676546},
abstract = {A configurable system is characterized by the configuration options present or absent in its variants. Selecting and deselecting those configuration options directly influences the functional properties of the system. Apart from functional properties, there are system characteristics that influence the performance (e.g., power demand), safety (e.g., fault probabilities), and security (e.g., susceptibility to attacks) of the system, called Non-Functional Properties (NFPs). Knowledge of NFPs is crucial for evaluating a system’s feasibility, usability, and resource demands. Although variability influences these characteristics, NFPs do not compose linearly for every selected feature. Feature interactions can increase the overall NFP values through (potentially exponential) amplification or decrease them through mitigation effects. In this paper, we propose an automated machine learning (AutoML) approach to predict NFP values for new configurations based on previously measured configuration values. Using AutoML, we leverage the advantages of machine learning for predicting NFPs without having to parameterize and fine-tune machine learning models. This approach and the resulting pipeline aim to reduce the complexity of performance prediction for configurable systems. We test the feasibility of our pipeline in a first evaluation on 4 real-world subject systems and discuss cases where AutoML may improve the prediction of NFPs.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {82–87},
numpages = {6},
keywords = {AutoML, Cyber-physical systems, Machine learning, Software product lines},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/1562860.1562864,
author = {Siegmund, Norbert and Pukall, Mario and Soffner, Michael and K\"{o}ppen, Veit and Saake, Gunter},
title = {Using software product lines for runtime interoperability},
year = {2009},
isbn = {9781605585482},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1562860.1562864},
doi = {10.1145/1562860.1562864},
abstract = {Today, often small, heterogeneous systems have to cooperate in order to fulfill a certain task. Interoperability between these systems is needed for their collaboration. However, achieving this interoperability raises several problems. For example, embedded systems might induce a higher probability for a system failure due to constrained power supply. Nevertheless, interoperability must be guaranteed even in scenarios where embedded systems are used. To overcome this problem, we use services to abstract the functionality from the system which realizes it. We outline how services can be generated using software product line techniques to bridge the heterogeneity of cooperating systems. Additionally, we address runtime changes of already deployed services to overcome system failures. In this paper, we show the runtime adaption process of these changes which includes the following two points. First, we outline why feature-oriented programming is appropriate in such scenarios. Second, we describe the runtime adaption process of services with feature-oriented programming.},
booktitle = {Proceedings of the Workshop on AOP and Meta-Data for Software Evolution},
articleno = {4},
numpages = {7},
keywords = {interoperability, runtime adaption, software product lines},
location = {Genova, Italy},
series = {RAM-SE '09}
}

@inproceedings{10.1145/2362536.2362567,
author = {Savolainen, Juha and Mannion, Mike and Kuusela, Juha},
title = {Developing platforms for multiple software product lines},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362567},
doi = {10.1145/2362536.2362567},
abstract = {Many approaches to software product line engineering have been founded on the development of a single product line platform. However as customer requirements change and new products are added to the product line, software producers recognize that the platform cannot be "stretched" indefinitely and a significant problem is striking a balance between development efficiency by increasing platform commonality and customer dissatisfaction from products with additional undesirable features and properties.One alternative is to develop multiple product lines (MPLs). However the challenge remains about what to include in a multiple product line platform. Drawing upon industrial experience of working with 4 companies, this paper explores the characteristics of the contexts in which MPLs are a viable alternative development strategy and then proposes a framework of approaches to platform development.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {220–228},
numpages = {9},
keywords = {industrial experience, multiple product lines, software reuse},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1145/3699839.3699841,
author = {Awad, Hiba and Alidra, Abdelghani and Bruneliere, Hugo and Ledoux, Thomas and Rivalan, Jonathan},
title = {VeriFog: A Generic Model-based Approach for Verifying Fog Systems at Design Time and Generating Deployment Configurations},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {3},
issn = {1559-6915},
url = {https://doi.org/10.1145/3699839.3699841},
doi = {10.1145/3699839.3699841},
abstract = {Fog Computing is a paradigm decentralizing the Cloud by geographically distributing computation, storage, network resources and related services. It provides benefits such as reducing the number of bottlenecks, limiting unwanted data movements, etc. However, managing the size, complexity and heterogeneity of the Fog systems to be engineered is challenging and can quickly become costly. According to best practices in software engineering, verification tasks could be performed on a system design prior to its implementation and deployment. We propose a generic model-based approach for verifying Fog systems at design time, also enabling the automatic generation of corresponding deployment configuration files. Named VeriFog, this approach is notably based on a customizable Fog Modeling Language (FML). We experimented in practice by modeling three use cases, from three different application domains, and by considering three main types of non-functional properties to be verified. From this modeling and verification effort, we show that we are able to automatically generate usable deployment configuration files for different deployment tools. In direct collaboration with our industrial partner Smile, the approach and underlying language presented in this paper are necessary steps towards a more global model-based support for the complete life cycle of Fog systems.},
journal = {SIGAPP Appl. Comput. Rev.},
month = oct,
pages = {18–36},
numpages = {19},
keywords = {deployment configuration, design time, fog computing, generation, model-based engineering, modeling language, non-functional properties, verification}
}

@inproceedings{10.1145/2019136.2019158,
author = {Guana, Victor and Correal, Dario},
title = {Variability quality evaluation on component-based software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019158},
doi = {10.1145/2019136.2019158},
abstract = {Quality assurance and evaluation in Model Driven Software Product Lines (MD-SPLs) are pivotal points for the growing and solidification of the generative software factories. They are framed as one of the future fact methodologies for the construction of software systems. Although several approximations address the problem of generative environments, software product line scope expression, and core asset definition, not many of them try to solve, as a fundamental step, the automation of the quality attribute evaluation in the MD-SPL development cycle. This paper presents a model-driven engineering method and a tool for the quality evaluation of product line configurations through a cross architectural view analysis.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {19},
numpages = {8},
keywords = {domain specific modeling, model composition, model-driven software product line, quality attribute, sensitivity point},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, M\'{a}rcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Configuration, Feature Model, Non-Functional Properties, Recommender Systems, Software Product Lines},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/2695664.2695749,
author = {Perkusich, Mirko and Medeiros, Amaury and e Silva, Lenardo Chaves and Gorg\^{o}nio, Kyller Costa and de Almeida, Hyggo Oliveira and Perkusich, Angelo},
title = {A Bayesian network approach to assist on the interpretation of software metrics},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695749},
doi = {10.1145/2695664.2695749},
abstract = {Despite the quantity of software metrics that has been proposed, their adoption and application by practitioners has been limited. A challenge to their use is to interpret them to perform assessments and predictions. The existing approaches to assist with their interpretation consists of defining thresholds to determine whether the value of a metric is acceptable. These approaches are not enough to ensure a correct metrics' interpretation, because they ignore risks and other subjective factors that influence the measurement process. This might affect the metrics' interpretation, and consequently, the manager's decision. To minimize wrong decisions based on software metrics, we present a method to construct Bayesian networks to assist on metric interpretation considering these risks. We successfully validated the method with a case study performed in three software development projects. We concluded that it is a promising approach to assist practitioners to interpret metrics and support software projects managerial decision-making.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1498–1503},
numpages = {6},
keywords = {bayesian networks, managerial decision-making, software development projects, software metrics, software metrics semantic},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.5555/2555523.2555556,
author = {Bagheri, Ebrahim and Ensan, Faezeh},
title = {Light-weight software product lines for small and medium-sized enterprises (SMEs)},
year = {2013},
publisher = {IBM Corp.},
address = {USA},
abstract = {Product line engineering practices promote the idea of systematic reuse of core assets and have been reported to decrease time-to-market and development costs for new products. However, our recent efforts to transfer our product line engineering knowledge to several of our small and medium-size enterprise industrial partner showed that there are challenges that need to be addressed before core product line engineering ideas can be deployed in SME context. These challenges include upfront investment costs, business traceability, levels of abstraction of functional features and semantic distinction between functional and non-functional software aspects. In order to address these challenges within the context of SMEs, we adopt and extend the behavior-driven development methodology in a way to not only offer agility in practice but also to equip software developers with the means to capture and manage software variability within the behavior-driven development process. We introduce the details of the extended methodology and discuss its advantages and disadvantages in detail.},
booktitle = {Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {311–324},
numpages = {14},
location = {Ontario, Canada},
series = {CASCON '13}
}

@inproceedings{10.5555/1753235.1753249,
author = {Montagud, Sonia and Abrah\~{a}o, Silvia},
title = {Gathering current knowledge about quality evaluation in software product lines},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Recently, a number of methods and techniques for assessing the quality of software product lines have been proposed. However, to the best of our knowledge, there is no study which summarizes all the existing evidence about them. This paper presents a systematic review that investigates what methods and techniques have been employed (in the last 10 years) to evaluate the quality of software product lines and how they were employed. A total of 39 research papers have been reviewed from an initial set of 1388 papers. The results show that 25% of the papers reported evaluations at the Design phase of the Domain Engineering phase. The most widely used mechanism for modeling quality attributes was extended feature models and the most evaluated artifact was the base architecture. In addition, the results of the review have identified several research gaps. Specifically, 77% of the papers employed case studies as a "proof of concept" whereas 23% of the papers did not perform any type of validation. Our results are particularly relevant in positioning new research activities and in the selection of quality evaluation methods or techniques that best fit a given purpose.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {91–100},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2897695.2897701,
author = {Abilio, Ramon and Vale, Gustavo and Figueiredo, Eduardo and Costa, Heitor},
title = {Metrics for feature-oriented programming},
year = {2016},
isbn = {9781450341776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897695.2897701},
doi = {10.1145/2897695.2897701},
abstract = {Feature-oriented programming (FOP) is a programming technique to implement software product lines based on composition mechanisms called refinements. A software product line is a set of software systems that share a common, managed set of features satisfying the specific needs of a particular market segment. The literature reports various software metrics for software product lines developed using object-oriented and aspect-oriented programming. However, after a literature review, we observed that we lack the definition of FOP-specific metrics. Based on this observation, this paper proposes a set of eight novel metrics for feature-oriented programming. These metrics were derived both from our experience in FOP and from existing software metrics. We demonstrate the applicability of the proposed metrics by applying them to a software product line.},
booktitle = {Proceedings of the 7th International Workshop on Emerging Trends in Software Metrics},
pages = {36–42},
numpages = {7},
keywords = {feature-oriented programming, software metrics, software product lines, software quality},
location = {Austin, Texas},
series = {WETSoM '16}
}

@inproceedings{10.1145/3546932.3546989,
author = {Bertolotti, Francesco and Cazzola, Walter and Favalli, Luca},
title = {Features, believe it or not! a design pattern for first-class citizen features on stock JVM},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3546989},
doi = {10.1145/3546932.3546989},
abstract = {Modern software systems must fulfill the needs of an ever-growing customer base. Due to the innate diversity of human needs, software should be highly customizable and reconfigurable. Researchers and practitioners gained interest in software product lines (SPL), mimicking aspects of product lines in industrial production for the engineering of highly-variable systems. There are two main approaches towards the engineering of SPLs. The first uses macros---such as the #ifdef macro in C. The second---called feature-oriented programming (FOP)---uses variability-aware preprocessors called composers to generate a program variant from a set of features and a configuration. Both approaches have disadvantages. Most notably, these approaches are usually not supported by the base language; for instance Java is one of the most commonly used FOP languages among researchers, but it does not support macros rather it relies on the C preprocessor or a custom one to translate macros into actual Java code. As a result, developers must struggle to keep up with the evolution of the base language, hindering the general applicability of SPL engineering. Moreover, to effectively evolve a software configuration and its features, their location must be known. The problem of recording and maintaining traceability information is considered expensive and error-prone and it is once again handled externally through dedicated modeling languages and tools. Instead, to properly convey the FOP paradigm, software features should be treated as first-class citizens using concepts that are proper to the host language, so that the variability can be expressed and analyzed with the same tools used to develop any other software in the same language. In this paper, we present a simple and flexible design pattern for JVM-based languages---dubbed devise pattern---that can be used to express feature dependencies and behaviors with a light-weight syntax both at domain analysis and at domain implementation level. To showcase the qualities and feasibility of our approach, we present several variability-aware implementations of a MNIST-encoder---including one using the devise pattern---and compare strengths and weaknesses of each approach.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {32–42},
numpages = {11},
keywords = {design patterns, software product lines, variability modeling},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3646548.3672586,
author = {Fernandez-Amoros, David and Heradio, Ruben and Horcas Aguilera, Jose Miguel and Galindo, Jos\'{e} A. and Benavides, David and Fuentes, Lidia},
title = {Pragmatic Random Sampling of the Linux Kernel: Enhancing the Randomness and Correctness of the conf Tool},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672586},
doi = {10.1145/3646548.3672586},
abstract = {The configuration space of some systems is so large that it cannot be computed. This is the case with the Linux Kernel, which provides almost 19,000 configurable options described across more than 1,600 files in the Kconfig language. As a result, many analyses of the Kernel rely on sampling its configuration space (e.g., debugging compilation errors, predicting configuration performance, finding the configuration that optimizes specific performance metrics, etc.). The Kernel can be sampled pragmatically, with its built-in tool conf, or idealistically, translating the Kconfig files into logic formulas. The pros of the idealistic approach are that it provides statistical guarantees for the sampled configurations, but the cons are that it sets out many challenging problems that have not been solved yet, such as scalability issues. This paper introduces a new version of conf called randconfig+, which incorporates a series of improvements that increase the randomness and correctness of pragmatic sampling and also help validate the Boolean translation required for the idealistic approach. randconfig+ has been tested on 20,000 configurations generated for 10 different Kernel versions from 2003 to the present day. The experimental results show that randconfig+ is compatible with all tested Kernel versions, guarantees the correctness of the generated configurations, and increases conf’s randomness for numeric and string options.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {24–35},
numpages = {12},
keywords = {Kconfig, SAT, configurable systems, randconfig, random sampling, software product lines, variability modeling},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/1390630.1390648,
author = {Lincke, R\"{u}diger and Lundberg, Jonas and L\"{o}we, Welf},
title = {Comparing software metrics tools},
year = {2008},
isbn = {9781605580500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390630.1390648},
doi = {10.1145/1390630.1390648},
abstract = {This paper shows that existing software metric tools interpret and implement the definitions of object-oriented software metrics differently. This delivers tool-dependent metrics results and has even implications on the results of analyses based on these metrics results. In short, the metrics-based assessment of a software system and measures taken to improve its design differ considerably from tool to tool. To support our case, we conducted an experiment with a number of commercial and free metrics tools. We calculated metrics values using the same set of standard metrics for three software systems of different sizes. Measurements show that, for the same software system and metrics, the metrics values are tool depended. We also defined a (simple) software quality model for "maintainability" based on the metrics selected. It defines a ranking of the classes that are most critical wrt. maintainability. Measurements show that even the ranking of classes in a software system is metrics tool dependent.},
booktitle = {Proceedings of the 2008 International Symposium on Software Testing and Analysis},
pages = {131–142},
numpages = {12},
keywords = {comparing tools, software quality metrics},
location = {Seattle, WA, USA},
series = {ISSTA '08}
}

@inproceedings{10.1145/3634713.3634715,
author = {B\"{o}hm, Sabrina and Krieter, Sebastian and He\ss{}, Tobias and Th\"{u}m, Thomas and Lochau, Malte},
title = {Incremental Identification of T-Wise Feature Interactions},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634715},
doi = {10.1145/3634713.3634715},
abstract = {Developers of configurable software use the concept of selecting and deselecting features to create different variants of a software product. In this context, one of the most challenging aspects is to identify unwanted interactions between those features. Due to the combinatorial explosion of the number of potentially interacting features, it is currently an open question how to systematically identify a particular feature interaction that causes a specific fault in a set of software products. In this paper, we propose an incremental approach to identify such t-wise feature interactions based on testing additional configurations in a black-box setting. We present the algorithm Inciident, which generates and selects new configurations based on a divide-and-conquer strategy to efficiently identify the feature interaction with a preferably minimal number of configurations. We evaluate our approach by considering simulated and real interactions of different sizes for 48 real-world feature models. Our results show that on average, Inciident requires 80&nbsp;% less configurations to identify an interaction than using randomly selected configurations.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {27–36},
numpages = {10},
keywords = {Configurable Systems, Feature Interaction, Feature-Model Analysis, Software Product Lines},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.1145/3307630.3342705,
author = {Krieter, Sebastian},
title = {Enabling Efficient Automated Configuration Generation and Management},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342705},
doi = {10.1145/3307630.3342705},
abstract = {Creating and managing valid configurations is one of the main tasks in software product line engineering. Due to the often complex constraints from a feature model, some kind of automated configuration generation is required to facilitate the configuration process for users and developers. For instance, decision propagation can be applied to support users in configuring a product from a software product line (SPL) with less manual effort and error potential, leading to a semi-automatic configuration process. Furthermore, fully-automatic configuration processes, such as random sampling or t-wise interaction sampling can be employed to test or to optimize an SPL. However, current techniques for automated configuration generation still do not scale well to SPLs with large and complex feature models. Within our thesis, we identify current challenges regarding the efficiency and effectiveness of the semi- and fully-automatic configuration process and aim to address these challenges by introducing novel techniques and improving current ones. Our preliminary results show already show promising progress for both, the semi- and fully-automatic configuration process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {215–221},
numpages = {7},
keywords = {configurable system, decision propagation, software product lines, t-wise sampling, uniform random sampling},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3382025.3414965,
author = {Young, Jeffrey M. and Walkingshaw, Eric and Th\"{u}m, Thomas},
title = {Variational satisfiability solving},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414965},
doi = {10.1145/3382025.3414965},
abstract = {Incremental satisfiability (SAT) solving is an extension of classic SAT solving that allows users to efficiently solve a set of related SAT problems by identifying and exploiting shared terms. However, using incremental solvers effectively is hard since performance is sensitive to a problem's structure and the order sub-terms are fed to the solver, and the burden to track results is placed on the end user. For analyses that generate sets of related SAT problems, such as those in software product lines, incremental SAT solvers are either not used at all, used but not explicitly stated so in the literature, or used but suffer from the aforementioned usability problems. This paper translates the ordering problem to an encoding problem and automates the use of incremental SAT solving. We introduce variational SAT solving, which differs from incremental SAT solving by accepting all related problems as a single variational input and returning all results as a single variational output. Our central idea is to make explicit the operations of incremental SAT solving, thereby encoding differences between related SAT problems as local points of variation. Our approach automates the interaction with the incremental solver and enables methods to automatically optimize sharing of the input. To evaluate our methods we construct a prototype variational SAT solver and perform an empirical analysis on two real-world datasets that applied incremental solvers to software evolution scenarios. We show, assuming a variational input, that the prototype solver scales better for these problems than naive incremental solving while also removing the need to track individual results.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {18},
numpages = {12},
keywords = {choice calculus, satisfiability solving, software product lines, variation},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1145/3672555,
author = {Mahdavi-Hezaveh, Rezvan and Fatima, Sameeha and Williams, Laurie},
title = {Paving a Path for a Combined Family of Feature Toggle and Configuration Option Research},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3672555},
doi = {10.1145/3672555},
abstract = {Feature toggles and configuration options are techniques to include or exclude functionality in software. The research contributions to these two techniques have most often been focused on either one of them. However, focusing on the similarities of these two techniques and the use of a common terminology may enable a combined family of research on software configuration (a term we use to encompass both techniques) and prevent duplication of effort. The goal of this study is to aid researchers in conducting a family of research on software configuration by extending an existing model of software configuration that provides a common terminology for feature toggles and configuration options in research studies. We started with Siegmund et al.’s Model of Software Configuration (MSC), which was developed based on configuration option-related resources. We extend the MSC by qualitative analysis of feature toggle-related resources. From our analysis, we proposed MSCv2 and evaluated it through its application on publications and an industrial system. Our results indicate researchers studying the same system may provide different definitions of software configuration in publications, similar research questions may be answered repeatedly because of a lack of a clear definition of software configuration, and having an MSC may enable generalized research on this family of research.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {172},
numpages = {27},
keywords = {Feature toggle, configuration option, software configuration, software engineering}
}

@inproceedings{10.1145/3377024.3377039,
author = {Ludwig, Kai and Kr\"{u}ger, Jacob and Leich, Thomas},
title = {FeatureCoPP: unfolding preprocessor variability},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377039},
doi = {10.1145/3377024.3377039},
abstract = {Annotation-based and composition-based variability mechanisms have complementary strengths regarding software maintenance and evolution. Consequently, several proposals have been made to combine, integrate, and substitute both mechanisms. An open challenge is to provide a unified, automatic, and practical technique to adopt such proposals. In this paper, we present a technique to convert variable feature code that is enclosed in the C preprocessor's conditional compilation into compositional feature modules and vice versa. We facilitate the usability of our technique by keeping the annotation-based representation of the C preprocessor. Besides contributing a practicable implementation, we describe the core principles of our technique and demonstrate its functionality based on previous empirical studies and by analyzing the Linux kernel. While our technique is fast in transforming projects, we also illustrate the challenges of maintaining fine-grained feature modules.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {24},
numpages = {9},
keywords = {empirical study, preprocessor, software metrics, software product lines, variability analysis},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1145/3336294.3336296,
author = {Ludwig, Kai and Kr\"{u}ger, Jacob and Leich, Thomas},
title = {Covert and Phantom Features in Annotations: Do They Impact Variability Analysis?},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336296},
doi = {10.1145/3336294.3336296},
abstract = {The annotation-based variability of the C preprocessor (CPP) has a bad reputation regarding comprehensibility and maintainability of software systems, but is widely adopted in practice. To assess the complexity of such systems' variability, several analysis techniques and metrics have been proposed in scientific communities. While most metrics seem reasonable at first glance, they do not generalize over all possible usages of C preprocessor variability that appear in practice. Consequently, some analyses may neglect the actual complexity of variability in these systems and may not properly reflect the real situation. In this paper, we investigate two types of variation points, namely negating and #else directives, to which we refer to as corner cases, as they are seldom explicitly considered in research. To investigate these directives, we rely on three commonly used metrics: lines of feature code, scattering degree, and tangling degree. We (1) describe how the considered directives impact these metrics, (2) unveil the resulting differences within 19 systems, and (3) propose how to address the arising issues. The results show that the corner cases appear regularly in variable feature code and can heavily change the results obtained with established metrics. We argue that we need to refine metrics and improve variability analysis techniques to provide more precise results, but we also need to reason about the meaning of corner cases and metrics.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {218–230},
numpages = {13},
keywords = {empirical study, preprocessor, software metrics, software product lines, variability analysis},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3233027.3233028,
author = {Rabiser, Rick and Schmid, Klaus and Becker, Martin and Botterweck, Goetz and Galster, Matthias and Groher, Iris and Weyns, Danny},
title = {A study and comparison of industrial vs. academic software product line research published at SPLC},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233028},
doi = {10.1145/3233027.3233028},
abstract = {The study presented in this paper aims to provide evidence for the hypothesis that software product line research has been changing and that the works in industry and academia have diverged over time. We analysed a subset (140) of all (593) papers published at the Software Product Line Conference (SPLC) until 2017. The subset was randomly selected to cover all years as well as types of papers. We assessed the research type of the papers (academic or industry), the kind of evaluation (application example, empirical, etc.), and the application domain. Also, we assessed which product line life-cycle phases, development practices, and topics the papers address. We present an analysis of the topics covered by academic vs. industry research and discuss the evolution of these topics and their relation over the years. We also discuss implications for researchers and practitioners. We conclude that even though several topics have received more attention than others, academic and industry research on software product lines are actually rather in line with each other.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {14–24},
numpages = {11},
keywords = {SPLC, academia, industry, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3236405.3236425,
author = {Hinterreiter, Daniel},
title = {Supporting feature-oriented development and evolution in industrial software ecosystems},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236425},
doi = {10.1145/3236405.3236425},
abstract = {Companies nowadays need to serve a mass market while at the same time customers request highly individual solutions. To handle this problem, development is frequently organized in software ecosystems (SECOs), i.e., interrelated software product lines involving internal and external developers. Individual products for customers are derived and adapted by adding new features or creating new versions of existing features to meet the customer-specific requirements. Development teams typically use version control systems to track fine-grained, implementation-level changes to product lines and products. However, it is difficult to relate such low-level changes to features and their evolution in the SECO. State-of-the-art approaches addressing this issue are variation control systems, which allow tracking of changes at the level of features. However, these systems have not found their way into mainstream development so far. In this thesis we will describe which workflows and additions to variation control systems are required to support feature-oriented development in an industrial SECO environment. We will further investigate mechanisms that support feature-based monitoring to guide the evolution in SECOs.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {79–86},
numpages = {8},
keywords = {configuration management, software evolution, software product lines, variation control systems},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3358960.3379137,
author = {Alves Pereira, Juliana and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc},
title = {Sampling Effect on Performance Prediction of Configurable Systems: A Case Study},
year = {2020},
isbn = {9781450369916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358960.3379137},
doi = {10.1145/3358960.3379137},
abstract = {Numerous software systems are highly configurable and provide a myriad of configuration options that users can tune to fit their functional and performance requirements (e.g., execution time). Measuring all configurations of a system is the most obvious way to understand the effect of options and their interactions, but is too costly or infeasible in practice. Numerous works thus propose to measure only a few configurations (a sample) to learn and predict the performance of any combination of options' values. A challenging issue is to sample a small and representative set of configurations that leads to a good accuracy of performance prediction models. A recent study devised a new algorithm, called distance-based sampling, that obtains state-of-the-art accurate performance predictions on different subject systems. In this paper, we replicate this study through an in-depth analysis of x264, a popular and configurable video encoder. We systematically measure all 1,152 configurations of x264 with 17 input videos and two quantitative properties (encoding time and encoding size). Our goal is to understand whether there is a dominant sampling strategy over the very same subject system (x264), i.e., whatever the workload and targeted performance properties. The findings from this study show that random sampling leads to more accurate performance models. However, without considering random, there is no single "dominant" sampling, instead different strategies perform best on different inputs and non-functional properties, further challenging practitioners and researchers.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {277–288},
numpages = {12},
keywords = {configurable systems, machine learning, performance prediction, software product lines},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@inproceedings{10.1145/2499777.2500725,
author = {Varshosaz, Mahsa and Khosravi, Ramtin},
title = {Discrete time Markov chain families: modeling and verification of probabilistic software product lines},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500725},
doi = {10.1145/2499777.2500725},
abstract = {Software product line engineering (SPLE) enables systematic reuse in development of a family of related software systems by explicitly defining commonalities and variabilities among the individual products in the family. Nowadays, SPLE is used in a variety of complex domains such as avionics and automotive. As such domains include safety critical systems which exhibit probabilistic behavior, there is a major need for modeling and verification approaches dealing with probabilistic aspects of systems in the presence of variabilities. In this paper, we introduce a mathematical model, Discrete Time Markov Chain Family (DTMCF), which compactly represents the probabilistic behavior of all the products in the product line. We also provide a probabilistic model checking method to verify DTMCFs against Probabilistic Computation Tree Logic (PCTL) properties. This way, instead of verifying each product individually, the whole family is model checked at once, resulting in the set of products satisfying the desired property. This reduces the required cost for model checking by eliminating redundant processing caused by the commonalities among the products.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {34–41},
numpages = {8},
keywords = {probabilistic model checking, software product line, variable discrete time Markov chains},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@article{10.1145/2853073.2853082,
author = {Soujanya, K. L.S. and AnandaRao, A.},
title = {A Generic Framework for Configuration Management of SPL and Controlling Evolution of Complex Software Products},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853082},
doi = {10.1145/2853073.2853082},
abstract = {Efficient configuration management system is crucial for the success of any software product line (SPL). Due to ever changing needs of customers, SPL undergoes constant changes that are to be tracked in real time. In the context of customer-driven development, anticipation and change management are to be given paramount importance. It demands implementation of software variability that drives home changed, extended and customized configurations besides economy at scale. Moreover, the emergence of distributed technologies, the unprecedented growth of component based, serviceoriented systems throw ever increasing challenges to software product line configuration management. Derivation of a new product is a dynamic process in software product line that should consider functionality and quality attributes. Very few approaches are found on configuration management (CM) of SPL though CM is enough matured for traditional products. They are tailor made and inadequate to provide a general solution. Stated differently, a comprehensive approach for SPL configuration management and product derivation is still to be desired. In this paper, we proposed a framework that guides in doing so besides helping in SPL definitions in generic way. Our framework facilitates SPL configuration management and product derivation based on critical path analysis, weight computation and feedback. We proposed two algorithms namely Quality Driven Product Derivation (QDPD) and Composition Analysis algorithm for generating satisfied compositions and to find best possible composition respectively. The usage of weights and critical path analysis improves quality of product derivation. The framework is extensible and flexible thus it can be leveraged with variability-aware design patterns and ontology. We built a prototype that demonstrates the proof of concept. We tested our approach with Dr. School product line. The results reveal that the framework supports configuration management of SPL and derivation of high quality product in the product line. We evaluated results with ground truth to establish significance of our implementation},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–10},
numpages = {10},
keywords = {Software product line, configuration management, critical path analysis, product derivation, weighted approach}
}

@inproceedings{10.1145/3510003.3510190,
author = {Randrianaina, Georges Aaron and T\"{e}rnava, Xhevahire and Khelladi, Djamel Eddine and Acher, Mathieu},
title = {On the benefits and limits of incremental build of software configurations: an exploratory study},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510190},
doi = {10.1145/3510003.3510190},
abstract = {Software projects use build systems to automate the compilation, testing, and continuous deployment of their software products. As software becomes increasingly configurable, the build of multiple configurations is a pressing need, but expensive and challenging to implement. The current state of practice is to build independently (a.k.a., clean build) a software for a subset of configurations. While incremental build has been studied for software evolution and relatively small changes of the source code, it has surprisingly not been considered for software configurations. In this exploratory study, we examine the benefits and limits of building software configurations incrementally, rather than always building them cleanly. By using five real-life configurable systems as subjects, we explore whether incremental build works, outperforms a sequence of clean builds, is correct w.r.t. clean build, and can be used to find an optimal ordering for building configurations. Our results show that incremental build is feasible in 100% of the times in four subjects and in 78% of the times in one subject. In average, 88.5% of the configurations could be built faster with incremental build while also finding several alternatives faster incremental builds. However, only 60% of faster incremental builds are correct. Still, when considering those correct incremental builds with clean builds, we could always find an optimal order that is faster than just a collection of clean builds with a gain up to 11.76%.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1584–1596},
numpages = {13},
keywords = {build systems, configurable software systems, configuration build},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3461001.3471146,
author = {Horcas, Jose-Miguel and Galindo, Jos\'{e} A. and Heradio, Ruben and Fernandez-Amoros, David and Benavides, David},
title = {Monte Carlo tree search for feature model analyses: a general framework for decision-making},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471146},
doi = {10.1145/3461001.3471146},
abstract = {The colossal solution spaces of most configurable systems make intractable their exhaustive exploration. Accordingly, relevant analyses remain open research problems. There exist analyses alternatives such as SAT solving or constraint programming. However, none of them have explored simulation-based methods. Monte Carlo-based decision making is a simulation-based method for dealing with colossal solution spaces using randomness. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo methods, which have proven to succeed in vast search spaces (e.g., game theory). Our general framework is described formally, and its flexibility to cope with a diversity of analysis problems is discussed (e.g., finding defective configurations, feature model reverse engineering or getting optimal performance configurations). Additionally, we present a Python implementation of the framework that shows the feasibility of our proposal. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state of the art a step forward.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {190–201},
numpages = {12},
keywords = {configurable systems, feature models, monte carlo tree search, software product lines, variability modeling},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3338906.3338974,
author = {Ne\v{s}i\'{c}, Damir and Kr\"{u}ger, Jacob and St\u{a}nciulescu, undefinedtefan and Berger, Thorsten},
title = {Principles of feature modeling},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338974},
doi = {10.1145/3338906.3338974},
abstract = {Feature models are arguably one of the most intuitive and successful notations for modeling the features of a variant-rich software system. Feature models help developers to keep an overall understanding of the system, and also support scoping, planning, development, variant derivation, configuration, and maintenance activities that sustain the system's long-term success. Unfortunately, feature models are difficult to build and evolve. Features need to be identified, grouped, organized in a hierarchy, and mapped to software assets. Also, dependencies between features need to be declared. While feature models have been the subject of three decades of research, resulting in many feature-modeling notations together with automated analysis and configuration techniques, a generic set of principles for engineering feature models is still missing. It is not even clear whether feature models could be engineered using recurrent principles. Our work shows that such principles in fact exist. We analyzed feature-modeling practices elicited from ten interviews conducted with industrial practitioners and from 31 relevant papers. We synthesized a set of 34 principles covering eight different phases of feature modeling, from planning over model construction, to model maintenance and evolution. Grounded in empirical evidence, these principles provide practical, context-specific advice on how to perform feature modeling, describe what information sources to consider, and highlight common characteristics of feature models. We believe that our principles can support researchers and practitioners enhancing feature-modeling tooling, synthesis, and analyses techniques, as well as scope future research.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {62–73},
numpages = {12},
keywords = {Feature models, modeling principles, software product lines},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@article{10.1145/3442389,
author = {Castro, Thiago and Teixeira, Leopoldo and Alves, Vander and Apel, Sven and Cordy, Maxime and Gheyi, Rohit},
title = {A Formal Framework of Software Product Line Analyses},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3442389},
doi = {10.1145/3442389},
abstract = {A number of product-line analysis approaches lift analyses such as type checking, model checking, and theorem proving from the level of single programs to the level of product lines. These approaches share concepts and mechanisms that suggest an unexplored potential for reuse of key analysis steps and properties, implementation, and verification efforts. Despite the availability of taxonomies synthesizing such approaches, there still remains the underlying problem of not being able to describe product-line analyses and their properties precisely and uniformly. We propose a formal framework that models product-line analyses in a compositional manner, providing an overall understanding of the space of family-based, feature-based, and product-based analysis strategies. It defines precisely how the different types of product-line analyses compose and inter-relate. To ensure soundness, we formalize the framework, providing mechanized specification and proofs of key concepts and properties of the individual analyses. The formalization provides unambiguous definitions of domain terminology and assumptions as well as solid evidence of key properties based on rigorous formal proofs. To qualitatively assess the generality of the framework, we discuss to what extent it describes five representative product-line analyses targeting the following properties: safety, performance, dataflow facts, security, and functional program properties.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {34},
numpages = {37},
keywords = {Software product lines, product-line analysis}
}

@inproceedings{10.1145/3307630.3342403,
author = {Berger, Thorsten and Collet, Philippe},
title = {Usage Scenarios for a Common Feature Modeling Language},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342403},
doi = {10.1145/3307630.3342403},
abstract = {Feature models are recognized as a de facto standard for variability modeling. Presented almost three decades ago, dozens of different variations and extensions to the original feature-modeling notation have been proposed, together with hundreds of variability management techniques building upon feature models. Unfortunately, despite several attempts to establish a unified language, there is still no emerging consensus on a feature-modeling language that is both intuitive and simple, but also expressive enough to cover a range of important usage scenarios. There is not even a documented and commonly agreed set of such scenarios.Following an initiative among product-line engineering researchers in September 2018, we present 14 usage scenarios together with examples and requirements detailing each scenario. The scenario descriptions are the result of a systematic process, where members of the initiative authored original descriptions, which received feedback via a survey, and which we then refined and extended based on the survey results, reviewers' comments, and our own expertise. We also report the relevance of supporting each usage scenario for the language, as perceived by the initiative's members, prioritizing each scenario. We present a roadmap to build and implement a first version of the envisaged common language.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {174–181},
numpages = {8},
keywords = {feature models, software product lines, unified language},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3571788.3571793,
author = {Schulthei\ss{}, Alexander and Bittner, Paul Maximilian and Greiner, Sandra and Kehrer, Timo},
title = {Benchmark Generation with VEVOS: A Coverage Analysis of Evolution Scenarios in Variant-Rich Systems},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571788.3571793},
doi = {10.1145/3571788.3571793},
abstract = {Clone-and-own development is a simple and flexible approach to realize multi-variant software systems in practice but typically provokes costly challenges in maintaining a continuously evolving set of variants. Therefore, managed clone-and-own development is key to efficiently mitigate these problems. While supporting techniques have been proposed in the literature, hardly any of them have been evaluated in a realistic setting due to a substantial lack of publicly available clone-and-own projects which could be used as experimental subjects. Recently, we presented the benchmark generation framework VEVOS for simulating clone-and-own development. However, it is yet unclear to which extent VEVOS can cover key scenarios for evaluating evolving variant-rich systems. This paper examines to what extent benchmarks created by VEVOS satisfy evaluation requirements for evolution scenarios demanded by the community. In addition, we report on our own experiences when employing VEVOS within six studies and elaborate on necessary extensions we implemented into VEVOS.},
booktitle = {Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {13–22},
numpages = {10},
keywords = {Clone-and-own, empirical evaluation, software product lines},
location = {Odense, Denmark},
series = {VaMoS '23}
}

@inproceedings{10.1109/ICSE43902.2021.00081,
author = {Fang, Xinwei and Calinescu, Radu and Gerasimou, Simos and Alhwikem, Faisal},
title = {Fast Parametric Model Checking through Model Fragmentation},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00081},
doi = {10.1109/ICSE43902.2021.00081},
abstract = {Parametric model checking (PMC) computes algebraic formulae that express key non-functional properties of a system (reliability, performance, etc.) as rational functions of the system and environment parameters. In software engineering, PMC formulae can be used during design, e.g., to analyse the sensitivity of different system architectures to parametric variability, or to find optimal system configurations. They can also be used at runtime, e.g., to check if non-functional requirements are still satisfied after environmental changes, or to select new configurations after such changes. However, current PMC techniques do not scale well to systems with complex behaviour and more than a few parameters. Our paper introduces a fast PMC (fPMC) approach that overcomes this limitation, extending the applicability of PMC to a broader class of systems than previously possible. To this end, fPMC partitions the Markov models that PMC operates with into fragments whose reachability properties are analysed independently, and obtains PMC reachability formulae by combining the results of these fragment analyses. To demonstrate the effectiveness of fPMC, we show how our fPMC tool can analyse three systems (taken from the research literature, and belonging to different application domains) with which current PMC techniques and tools struggle.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {835–846},
numpages = {12},
keywords = {Parametric model checking, discrete-time Markov chains, non-functional properties},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3571788.3571801,
author = {Acher, Mathieu and Lesoil, Luc and Randrianaina, Georges Aaron and T\"{e}rnava, Xhevahire and Zendra, Olivier},
title = {A Call for Removing Variability},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571788.3571801},
doi = {10.1145/3571788.3571801},
abstract = {Software variability is largely accepted and explored in software engineering and seems to have become a norm and a must, if only in the context of product lines. Yet, the removal of superfluous or unneeded software artefacts and functionalities is an inevitable trend. It is frequently investigated in relation to software bloat. This paper is essentially a call to the community on software variability to devise methods and tools that will facilitate the removal of unneeded variability from software systems. The advantages are expected to be numerous in terms of functional and non-functional properties, such as maintainability (lower complexity), security (smaller attack surface), reliability, and performance (smaller binaries).},
booktitle = {Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {82–84},
numpages = {3},
keywords = {removing variability, software bloat, software variability},
location = {Odense, Denmark},
series = {VaMoS '23}
}

@inproceedings{10.1145/3368089.3409675,
author = {Siegmund, Norbert and Ruckel, Nicolai and Siegmund, Janet},
title = {Dimensions of software configuration: on the configuration context in modern software development},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409675},
doi = {10.1145/3368089.3409675},
abstract = {With the rise of containerization, cloud development, and continuous integration and delivery, configuration has become an essential aspect not only to tailor software to user requirements, but also to configure a software system’s environment and infrastructure. This heterogeneity of activities, domains, and processes blurs the term configuration, as it is not clear anymore what tasks, artifacts, or stakeholders are involved and intertwined. However, each re- search study and each paper involving configuration places their contributions and findings in a certain context without making the context explicit. This makes it difficult to compare findings, translate them to practice, and to generalize the results. Thus, we set out to evaluate whether these different views on configuration are really distinct or can be summarized under a common umbrella. By interviewing practitioners from different domains and in different roles about the aspects of configuration and by analyzing two qualitative studies in similar areas, we derive a model of configuration that provides terminology and context for research studies, identifies new research opportunities, and allows practitioners to spot possible challenges in their current tasks. Although our interviewees have a clear view about configuration, it substantially differs due to their personal experience and role. This indicates that the term configuration might be overloaded. However, when taking a closer look, we see the interconnections and dependencies among all views, arriving at the conclusion that we need to start considering the entire spectrum of dimensions of configuration.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {338–349},
numpages = {12},
keywords = {Dimensions of software configuration, configuration management and life cycle, developer study, variability},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/2993236.2993249,
author = {Pereira, Juliana Alves and Matuszyk, Pawel and Krieter, Sebastian and Spiliopoulou, Myra and Saake, Gunter},
title = {A feature-based personalized recommender system for product-line configuration},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993249},
doi = {10.1145/2993236.2993249},
abstract = {Today’s competitive marketplace requires the industry to understand unique and particular needs of their customers. Product line practices enable companies to create individual products for every customer by providing an interdependent set of features. Users configure personalized products by consecutively selecting desired features based on their individual needs. However, as most features are interdependent, users must understand the impact of their gradual selections in order to make valid decisions. Thus, especially when dealing with large feature models, specialized assistance is needed to guide the users in configuring their product. Recently, recommender systems have proved to be an appropriate mean to assist users in finding information and making decisions. In this paper, we propose an advanced feature recommender system that provides personalized recommendations to users. In detail, we offer four main contributions: (i) We provide a recommender system that suggests relevant features to ease the decision-making process. (ii) Based on this system, we provide visual support to users that guides them through the decision-making process and allows them to focus on valid and relevant parts of the configuration space. (iii) We provide an interactive open-source configurator tool encompassing all those features. (iv) In order to demonstrate the performance of our approach, we compare three different recommender algorithms in two real case studies derived from business experience.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {120–131},
numpages = {12},
keywords = {Personalized Recommendations, Product-Line Configuration, Recommenders, Software Product Lines},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@inproceedings{10.1145/3180155.3180159,
author = {Krieter, Sebastian and Th\"{u}m, Thomas and Schulze, Sandro and Schr\"{o}ter, Reimar and Saake, Gunter},
title = {Propagating configuration decisions with modal implication graphs},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180159},
doi = {10.1145/3180155.3180159},
abstract = {Highly-configurable systems encompass thousands of interdependent configuration options, which require a non-trivial configuration process. Decision propagation enables a backtracking-free configuration process by computing values implied by user decisions. However, employing decision propagation for large-scale systems is a time-consuming task and, thus, can be a bottleneck in interactive configuration processes and analyses alike. We propose modal implication graphs to improve the performance of decision propagation by precomputing intermediate values used in the process. Our evaluation results show a significant improvement over state-of-the-art algorithms for 120 real-world systems.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {898–909},
numpages = {12},
keywords = {configuration, decision propagation, software product line},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3023956.3023959,
author = {Ochoa, Lina and Pereira, Juliana Alves and Gonz\'{a}lez-Rojas, Oscar and Castro, Harold and Saake, Gunter},
title = {A survey on scalability and performance concerns in extended product lines configuration},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023959},
doi = {10.1145/3023956.3023959},
abstract = {Product lines have been employed as a mass customisation method that reduces production costs and time-to-market. Multiple product variants are represented in a product line, however the selection of a particular configuration depends on stakeholders' functional and non-functional requirements. Methods like constraint programming and evolutionary algorithms have been used to support the configuration process. They consider a set of product requirements like resource constraints, stakeholders' preferences, and optimization objectives. Nevertheless, scalability and performance concerns start to be an issue when facing large-scale product lines and runtime environments. Thus, this paper presents a survey that analyses strengths and drawbacks of 21 approaches that support product line configuration. This survey aims to: i) evidence which product requirements are currently supported by studied methods; ii) how scalability and performance is considered in existing approaches; and iii) point out some challenges to be addressed in future research.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {5–12},
numpages = {8},
keywords = {configuration, literature review, performance, product line, product requirements, scalability, survey},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/3442391.3442409,
author = {G\"{o}ttmann, Hendrik and Bacher, Isabelle and Gottwald, Nicolas and Lochau, Malte},
title = {Static Analysis Techniques for Efficient Consistency Checking of Real-Time-Aware DSPL Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442409},
doi = {10.1145/3442391.3442409},
abstract = {Dynamic Software Product Lines (DSPL) have recently gained momentum as integrated engineering methodology for (self-)adaptive software. DSPL enhance statically configurable software by enabling run-time reconfiguration to facilitate continuous adaptations to changing environmental contexts. In a previous work, we presented a model-based methodology for specifying and automatically analyzing real-time constraints of reconfiguration decisions in a feature-oriented and compositional way. Internally, we translate real-time-aware DSPL specifications into timed automata serving as input for off-the-shelf model checkers like Uppaal for automatically checking semantic consistency properties. However, due to the very high computational complexity of model checking timed automata, those consistency checks suffer from scalability problems thus obstructing practical applications of the proposed approach. In this paper, we tackle this issue by investigating various kinds of static-analysis techniques that (1) aim to avoid expensive model checker calls by statically detecting certain classes of inconsistencies beforehand and otherwise (2) perform model reduction by detecting and merging equivalence states prior to model checker calls. The results of our experimental evaluation show very promising performance improvements achievable by those techniques, especially by the model-reduction approach.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {17},
numpages = {9},
keywords = {Dynamic Software Product Lines, Reconfiguration Decisions, Timed Automata},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.1109/ICSE43902.2021.00099,
author = {Weber, Max and Apel, Sven and Siegmund, Norbert},
title = {White-Box Performance-Influence Models: A Profiling and Learning Approach},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00099},
doi = {10.1109/ICSE43902.2021.00099},
abstract = {Many modern software systems are highly configurable, allowing the user to tune them for performance and more. Current performance modeling approaches aim at finding performance-optimal configurations by building performance models in a black-box manner. While these models provide accurate estimates, they cannot pinpoint causes of observed performance behavior to specific code regions. This does not only hinder system understanding, but it also complicates tracing the influence of configuration options to individual methods.We propose a white-box approach that models configuration-dependent performance behavior at the method level. This allows us to predict the influence of configuration decisions on individual methods, supporting system understanding and performance debugging. The approach consists of two steps: First, we use a coarse-grained profiler and learn performance-influence models for all methods, potentially identifying some methods that are highly configuration- and performance-sensitive, causing inaccurate predictions. Second, we re-measure these methods with a fine-grained profiler and learn more accurate models, at higher cost, though. By means of 9 real-world Java software systems, we demonstrate that our approach can efficiently identify configuration-relevant methods and learn accurate performance-influence models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1059–1071},
numpages = {13},
keywords = {Configuration management, performance, software product lines, software variability},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1145/3591335.3591343,
author = {Bordis, Tabea and Runge, Tobias and Kittelmann, Alexander and Schaefer, Ina},
title = {Correctness-by-Construction: An Overview of the CorC Ecosystem},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {2},
issn = {1094-3641},
url = {https://doi.org/10.1145/3591335.3591343},
doi = {10.1145/3591335.3591343},
abstract = {Correctness-by-Construction (CbC) is an incremental software development technique in the field of formal methods to create functionally correct programs guided by a specification. In contrast to post-hoc verification, where the specification and verification take part after implementing a program, with CbC the specification is defined first, and then the program is successively created using a small set of refinement rules that define side conditions preserving the correctness of the program. This specification-first, refinement-based approach as pursued by CbC has the advantage that errors are likely to be detected earlier in the design process and can be tracked more easily. Even though the idea of CbC emerged over 40 years ago, CbC is not widespread and is mostly used to create small algorithms.We believe in the idea of CbC and envision a scaled CbC approach that contributes to solving problems of modern software verification. In this short paper, we give an overview of our research regarding CbC in four different lines of research. For all of them, we provide tool support for building the CorC ecosystem that even further enables CbC-based development for different fields of application and size of software systems. Furthermore, we give an outlook on future work that extends on our concepts for CbC.},
journal = {Ada Lett.},
month = apr,
pages = {75–78},
numpages = {4},
keywords = {architecture, correctness-by-construction, information flow control, program verification, software product lines}
}

@inproceedings{10.1145/3646548.3676539,
author = {M\"{u}ller, Robert and Wei\ss{}, Mathis and Lochau, Malte},
title = {Mapping Cardinality-based Feature Models to Weighted Automata over Featured Multiset Semirings},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676539},
doi = {10.1145/3646548.3676539},
abstract = {Cardinality-based feature models permit to select multiple copies of the same feature, thus generalizing the notion of product configurations from subsets of Boolean features to multisets of feature instances. This increased expressiveness shapes a-priori infinite and non-convex configuration spaces, which renders established solution-space mappings based on Boolean presence conditions insufficient for cardinality-based feature models. To address this issue, we propose weighted automata over featured multiset semirings as a novel behavioral variability modeling formalism for cardinality-based feature models. The formalism uses multisets over features as a predefined semantic domain for transition weights. It permits to use any algebraic structure forming a proper semiring on multisets to aggregate the weights traversed along paths to map accepted words to multiset configurations. In particular, tropical semirings constitute a promising sub-class with a reasonable trade-off between expressiveness and computational tractability of canonical analysis problems. The formalism is strictly more expressive than featured transition systems, as it enables upper-bound multiplicity constraints depending on the length of words. We provide a tool implementation of the behavioral variability model and present preliminary experimental results showing applicability and computational feasibility of the proposed approach.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {1–11},
numpages = {11},
keywords = {Behavioral Variability Modeling, Cardinality-Based Feature Models, Weighted Automata},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/2362536.2362548,
author = {Soltani, Samaneh and Asadi, Mohsen and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Bagheri, Ebrahim},
title = {Automated planning for feature model configuration based on functional and non-functional requirements},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362548},
doi = {10.1145/2362536.2362548},
abstract = {Feature modeling is one of the main techniques used in Software Product Line Engineering to manage the variability within the products of a family. Concrete products of the family can be generated through a configuration process. The configuration process selects and/or removes features from the feature model according to the stakeholders' requirements. Selecting the right set of features for one product from amongst all of the available features in the feature model is a complex task because: 1) the multiplicity of stakeholders' functional requirements; 2) the positive or negative impact of features on non-functional properties; and 3) the stakeholders' preferences w.r.t. the desirable non-functional properties of the final product. Many configurations techniques have already been proposed to facilitate automated product derivation. However, most of the current proposals are not designed to consider stakeholders' preferences and constraints especially with regard to non-functional properties. We address the software product line configuration problem and propose a framework, which employs an artificial intelligence planning technique to automatically select suitable features that satisfy both the stakeholders' functional and non-functional preferences and constraints. We also provide tooling support to facilitate the use of our framework. Our experiments show that despite the complexity involved with the simultaneous consideration of both functional and non-functional properties our configuration technique is scalable.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {56–65},
numpages = {10},
keywords = {artificial intelligence, configuration, feature model, planning techniques, software product line engineering},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3510003.3510094,
author = {He, Haochen and Jia, Zhouyang and Li, Shanshan and Yu, Yue and Zhou, Chenglong and Liao, Qing and Wang, Ji and Liao, Xiangke},
title = {Multi-intention-aware configuration selection for performance tuning},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510094},
doi = {10.1145/3510003.3510094},
abstract = {Automatic configuration tuning helps users who intend to improve software performance. However, the auto-tuners are limited by the huge configuration search space. More importantly, they focus only on performance improvement while being unaware of other important user intentions (e.g., reliability, security). To reduce the search space, researchers mainly focus on pre-selecting performance-related parameters which requires a heavy stage of dynamically running under different configurations to build performance models. Given that other important user intentions are not paid attention to, we focus on guiding users in pre-selecting performance-related parameters in general while warning about side-effects on non-performance intentions. We find that the configuration document often, if it does not always, contains rich information about the parameters' relationship with diverse user intentions, but documents might also be long and domain-specific.In this paper, we first conduct a comprehensive study on 13 representative software containing 7,349 configuration parameters, and derive six types of ways in which configuration parameters may affect non-performance intentions. Guided by this study, we design SafeTune, a multi-intention-aware method that preselects important performance-related parameters and warns about their side-effects on non-performance intentions. Evaluation on target software shows that SafeTune correctly identifies 22--26 performance-related parameters that are missed by state-of-the-art tools but have significant performance impact (up to 14.7x). Furthermore, we illustrate eight representative cases to show that SafeTune can effectively prevent real-world and critical side-effects on other user intentions.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1431–1442},
numpages = {12},
keywords = {non-performance property, performance tuning, user intention},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3109729.3109751,
author = {Krieter, Sebastian and Pinnecke, Marcus and Kr\"{u}ger, Jacob and Sprey, Joshua and Sontag, Christopher and Th\"{u}m, Thomas and Leich, Thomas and Saake, Gunter},
title = {FeatureIDE: Empowering Third-Party Developers},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109751},
doi = {10.1145/3109729.3109751},
abstract = {FeatureIDE is a popular open-source tool for modeling, implementing, configuring, and analyzing software product lines. However, FeatureIDE's initial design was lacking mechanisms that facilitate extension and reuse of core implementations. In current releases, we improve these traits by providing a modular concept for core data structures and functionalities. As a result, we are facilitating the usage of external implementations for feature models and file formats within FeatureIDE. Additionally, we provide a Java library containing FeatureIDE's core functionalities, including feature modeling and configuration. This allows developers to use these functionalities in their own tools without relying on external dependencies, such as the Eclipse framework.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {42–45},
numpages = {4},
keywords = {Software product line, configuration, feature modeling, feature-oriented software development},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3307630.3342414,
author = {Th\"{u}m, Thomas and Teixeira, Leopoldo and Schmid, Klaus and Walkingshaw, Eric and Mukelabai, Mukelabai and Varshosaz, Mahsa and Botterweck, Goetz and Schaefer, Ina and Kehrer, Timo},
title = {Towards Efficient Analysis of Variation in Time and Space},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342414},
doi = {10.1145/3307630.3342414},
abstract = {Variation is central to today's software development. There are two fundamental dimensions to variation: Variation in time refers to the fact that software exists in numerous revisions that typically replace each other (i.e., a newer version supersedes an older one). Variation in space refers to differences among variants that are designed to coexist in parallel. There are numerous analyses to cope with variation in space (i.e., product-line analyses) and others that cope with variation in time (i.e., regression analyses). The goal of this work is to discuss to which extent product-line analyses can be applied to revisions and, conversely, where regression analyses can be applied to variants. In addition, we discuss challenges related to the combination of product-line and regression analyses. The overall goal is to increase the efficiency of analyses by exploiting the inherent commonality between variants and revisions.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {57–64},
numpages = {8},
keywords = {product-line analysis, regression analysis, software configuration management, software evolution, software product lines, software variation, variability management, variability-aware analysis},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3106237.3106252,
author = {Kn\"{u}ppel, Alexander and Th\"{u}m, Thomas and Mennicke, Stephan and Meinicke, Jens and Schaefer, Ina},
title = {Is there a mismatch between real-world feature models and product-line research?},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106252},
doi = {10.1145/3106237.3106252},
abstract = {Feature modeling has emerged as the de-facto standard to compactly capture the variability of a software product line. Multiple feature modeling languages have been proposed that evolved over the last decades to manage industrial-size product lines. However, less expressive languages, solely permitting require and exclude constraints, are permanently and carelessly used in product-line research. We address the problem whether those less expressive languages are sufficient for industrial product lines. We developed an algorithm to eliminate complex cross-tree constraints in a feature model, enabling the combination of tools and algorithms working with different feature model dialects in a plug-and-play manner. However, the scope of our algorithm is limited. Our evaluation on large feature models, including the Linux kernel, gives evidence that require and exclude constraints are not sufficient to express real-world feature models. Hence, we promote that research on feature models needs to consider arbitrary propositional formulas as cross-tree constraints prospectively.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {291–302},
numpages = {12},
keywords = {Software product lines, cross-tree constraints, exclude constraints, expressiveness, feature modeling, model transformation, require constraints},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/1985374.1985387,
author = {da Silva, Bruno C. and Sant'Anna, Cl\'{a}udio and Chavez, Christina},
title = {Concern-based cohesion as change proneness indicator: an initial empirical study},
year = {2011},
isbn = {9781450305938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985374.1985387},
doi = {10.1145/1985374.1985387},
abstract = {Structure-based cohesion metrics, such as the well-known Chidamber and Kemerer's Lack of Cohesion in Methods (LCOM), fail to capture the semantic notion of a software component's cohesion. Some researchers claim that it is one of the reasons they are not good indicators of change proneness. The Lack of Concern-based Cohesion metric (LCC) is an alternative cohesion metric which is centered on counting the number of concerns a component implements. A concern is any important concept, feature, property or area of interest of a system that we want to treat in a modular way. In this way, LCC focus on what really matters for assessing a component's cohesion - the amount of responsibilities placed on them. Our aim in this paper is to present an initial investigation about the applicability of this concern-based cohesion metric as a change proneness indicator. We also checked if this metric has a correlation with efferent coupling. An initial empirical assessment work was done with two small to medium-sized systems. Our results indicated a moderate to strong correlation between LCC and change proneness, and also a strong correlation between LCC and efferent coupling.},
booktitle = {Proceedings of the 2nd International Workshop on Emerging Trends in Software Metrics},
pages = {52–58},
numpages = {7},
keywords = {cohesion, empirical software engineering, software metrics},
location = {Waikiki, Honolulu, HI, USA},
series = {WETSoM '11}
}

@inproceedings{10.1145/3106195.3106205,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green Configurations of Functional Quality Attributes},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106205},
doi = {10.1145/3106195.3106205},
abstract = {Functional quality attributes (FQAs) are those quality attributes that, to be satisfied, require the incorporation of additional functionality into the application architecture. By adding an FQA (e.g., security) we can improve the quality of the final product, but there is also an increase in energy consumption. This paper proposes a solution to help the software architect to generate configurations of FQAs whilst keeping the energy consumed by the application as low as possible. For this, a usage model is defined for each FQA, taking into account the variables that affect the energy consumption, and that the values of these variables change according to the part of the application where the FQA is required. We extend a Software Product Line that models a family of FQAs to incorporate the variability of the usage model and the existing frameworks that implement FQAs. We generate the most eco-efficient configuration of FQAs by selecting the framework with the most suitable characteristics according to the requirements of the application.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {79–83},
numpages = {5},
keywords = {Energy Consumption, FQA, Quality Attributes, SPL, Variability},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3483899.3483905,
author = {Freire, Willian and Tonh\~{a}o, Simone and Bonetti, Tiago and Shigenaga, Marcelo and Cadette, William and Felizardo, Fernando and Amaral, Aline and OliveiraJr, Edson and Colanzi, Thelma},
title = {On the configuration of multi-objective evolutionary algorithms for PLA design optimization},
year = {2021},
isbn = {9781450384193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483899.3483905},
doi = {10.1145/3483899.3483905},
abstract = {Search-based algorithms have been successfully applied in the Product Line Architecture (PLA) optimization using the seminal approach called Multi-Objective Approach for Product-Line Architecture Design (MOA4PLA). This approach produces a set of alternative PLA designs intending to improve the different factors being optimized. Currently, the MOA4PLA uses the NSGA-II algorithm, a multi-objective evolutionary algorithm (MOEA) that can optimize several architectural properties simultaneously. Despite the promising results, studying the best values for the algorithm parameters is essential to obtain even better results. This is also crucial to ease the adoption of MOA4PLA by newcomers or non-expert companies willing to start using search-based software engineering to PLA design. Three crossover operators for the PLA design optimization were proposed recently. However, reference values for parameters have not been defined for PLA design optimization using crossover operators. In this context, the objective of this work is conducting an experimental study to discover which are the most effective crossover operators and the best values to configure the MOEA parameters, such as population size, number of generations, and mutation and crossover rates. A quantitative analysis based on quality indicators and statistical tests was performed using four PLA designs to determine the most suitable parameter values to the search-based algorithm. Empirical results pointed out the best combination of crossover operators and the most suitable values to configure MOA4PLA.},
booktitle = {Proceedings of the 15th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Multi-objective evolutionary algorithm, recombination operators, software architecture, software product line},
location = {Joinville, Brazil},
series = {SBCARS '21}
}

@proceedings{10.1145/3643664,
title = {WSESE '24: Proceedings of the 1st IEEE/ACM International Workshop on Methodological Issues with Empirical Studies in Software Engineering},
year = {2024},
isbn = {9798400705670},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {WSESE 2024 was a one-day event held on April 16, 2024, in Lisbon, Portugal. The theme of the workshop was "Methodological Issues with Empirical Studies in Software Engineering". The primary goal was to gain a better understanding of the adoption of the empirical paradigm in SE. Specifically, our focus was on identifying, discussing and finding solutions for the issues in the empirical methods currently employed. The workshop provided an opportunity for researchers and practitioners to discuss current methodological challenges and explore ways to address them.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3382025.3414962,
author = {Chrszon, Philipp and Baier, Christel and Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha},
title = {From features to roles},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414962},
doi = {10.1145/3382025.3414962},
abstract = {The detection of interactions is a challenging task present in almost all stages of software development. In feature-oriented system design, this task is mainly investigated for interactions of features within a single system, detected by their emergent behaviors. We propose a formalism to describe interactions in hierarchies of feature-oriented systems (hierarchical interactions) and the actual situations where features interact (active interplays). Based on the observation that such interactions are also crucial in role-based systems, we introduce a compositional modeling framework based on concepts and notions of roles, comprising role-based automata (RBAs). To describe RBAs, we present a modeling language that is close to the input language of the probabilistic model checker Prism. To exemplify the use of RBAs, we implemented a tool that translates RBA models into Prism and thus enables the formal analysis of functional and non-functional properties including system dynamics, contextual changes, and interactions. We carry out two case studies as a proof of concept of such analyses: First, a peer-to-peer protocol case study illustrates how undesired hierarchical interactions can be discovered automatically. Second, a case study on a self-adaptive production cell demonstrates how undesired interactions influence quality-of-service measures such as reliability and throughput.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {19},
numpages = {11},
keywords = {feature-oriented systems, formal methods, roles, verification},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3564719.3568695,
author = {Hentze, Marc and Pett, Tobias and Sundermann, Chico and Krieter, Sebastian and Th\"{u}m, Thomas and Schaefer, Ina},
title = {Generic Solution-Space Sampling for Multi-domain Product Lines},
year = {2022},
isbn = {9781450399203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564719.3568695},
doi = {10.1145/3564719.3568695},
abstract = {Validating a configurable software system is challenging, as there are potentially millions of configurations, which makes testing each configuration individually infeasible. Thus, existing sampling algorithms allow to compute a representative subset of configurations, called sample, that can be tested instead. However, sampling on the set of configurations may miss potential error sources on implementation level. In this paper, we present solution-space sampling, a concept that mitigates this problem by allowing to sample directly on the implementation level. We apply solution-space sampling to six real-word, automotive product lines and show that it produces up to 56 % smaller samples, while also covering all potential error sources missed by problem-space sampling.},
booktitle = {Proceedings of the 21st ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {135–147},
numpages = {13},
keywords = {Multi-domain product lines, feature-model sampling, solution-space analysis},
location = {Auckland, New Zealand},
series = {GPCE 2022}
}

@inproceedings{10.1145/2420942.2420944,
author = {Olaechea, Rafael and Stewart, Steven and Czarnecki, Krzysztof and Rayside, Derek},
title = {Modelling and multi-objective optimization of quality attributes in variability-rich software},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420944},
doi = {10.1145/2420942.2420944},
abstract = {Variability-rich software, such as software product lines, offers optional and alternative features to accommodate varying needs of users. Designers of variability-rich software face the challenge of reasoning about the impact of selecting such features on the quality attributes of the resulting software variant. Attributed feature models have been proposed to model such features and their impact on quality attributes, but existing variability modelling languages and tools have limited or no support for such models and the complex multi-objective optimization problem that arises. This paper presents ClaferMoo, a language and tool that addresses these shortcomings. ClaferMoo uses type inheritance to modularize the attribution of features in feature models and allows specifying multiple optimization goals. We evaluate an implementation of the language on a set of attributed feature models from the literature, showing that the optimization infrastructure can handle small-scale feature models with about a dozen features within seconds.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {2},
numpages = {6},
keywords = {multi-objective optimization, software product lines},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@inproceedings{10.1145/3350768.3351993,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Towards the support of user preferences in search-based product line architecture design: an exploratory study},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3351993},
doi = {10.1145/3350768.3351993},
abstract = {Software Product Lines (SPLs) is a reuse approach in which a family of products is generalized in a common architecture that can be adapted to different clients. The Product Line Architecture (PLA) is one of the most important artifacts of a SPL. PLA design requires great human effort as it involves several factors that are usually in conflict. To ease this task, PLA design can be formulated as an optimization problem with many factors, i.e, as a multi-objective optimization problem. In this context, the MOA4PLA approach was proposed to optimize PLA design using search algorithms and metrics specific to the context. This approach supported by OPLA-Tool has already been used in several works demonstrating its applicability. However, MOA4PLA does not take into account aspects that are subjective, such as the preferences of a particular Decision Maker (DM). To do so, this paper presents a proposal to incorporate the user preferences in the optimization process performed by MOA4PLA, through an interactive process in which the DM subjectively evaluates the solutions in processing time. Thus, the solutions generated can be better suited to the DM's needs or preferences. In order to allow the user interaction, modifications were made in MOA4PLA and implemented in the OPLA-Tool. Aiming at an initial validation of the proposal, an exploratory study was carried out, composed of two experiments: a qualitative and a quantitative. These experiments were realized with the participation of a software architect. Empirical results pointed out that the proposed interactive process enables the generation of PLAs that are in accordance with the architect's preferences. Another significant contribution are the lessons learned on how to improve the interactive process.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {387–396},
numpages = {10},
keywords = {Human-computer interaction, Multi-Objective Optimization, Product Line Architecture},
location = {Salvador, Brazil},
series = {SBES '19}
}

@inproceedings{10.1145/3023956.3023963,
author = {Halin, Axel and Nuttinck, Alexandre and Acher, Mathieu and Devroey, Xavier and Perrouin, Gilles and Heymans, Patrick},
title = {Yo variability! JHipster: a playground for web-apps analyses},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023963},
doi = {10.1145/3023956.3023963},
abstract = {Though variability is everywhere, there has always been a shortage of publicly available cases for assessing variability-aware tools and techniques as well as supports for teaching variability-related concepts. Historical software product lines contains industrial secrets their owners do not want to disclose to a wide audience. The open source community contributed to large-scale cases such as Eclipse, Linux kernels, or web-based plugin systems (Drupal, WordPress). To assess accuracy of sampling and prediction approaches (bugs, performance), a case where all products can be enumerated is desirable. As configuration issues do not lie within only one place but are scattered across technologies and assets, a case exposing such diversity is an additional asset. To this end, we present in this paper our efforts in building an explicit product line on top of JHipster, an industrial open-source Web-app configurator that is both manageable in terms of configurations (≈ 163,000) and diverse in terms of technologies used. We present our efforts in building a variability-aware chain on top of JHipster's configurator and lessons learned using it as a teaching case at the University of Rennes. We also sketch the diversity of analyses that can be performed with our infrastructure as well as early issues found using it. Our long term goal is both to support students and researchers studying variability analysis and JHipster developers in the maintenance and evolution of their tools.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {44–51},
numpages = {8},
keywords = {case study, variability-related analyses, web-apps},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.5555/2662572.2662584,
author = {Sanchez, Luis Emiliano and Moisan, Sabine and Rigault, Jean-Paul},
title = {Metrics on feature models to optimize configuration adaptation at run time},
year = {2013},
isbn = {9781467362849},
publisher = {IEEE Press},
abstract = {Feature models are widely used to capture variability, commonalities and configuration rules of software systems. We apply this technique to model component-based systems with many variants during specification, implementation, or run time. This representation allows us to determine the set of valid configurations befitting a given context, especially at run time. A key challenge is to determine the configuration most suitable, especially with respect to non-functional aspects: quality of service, performance, reconfiguration time... We propose an algorithm for selecting the configuration that optimizes a given quality metrics. This algorithm is a variant of the Best-First Search algorithm, a heuristic technique suitable for feature model optimization. The algorithm is parameterized with several strategies and heuristics on feature models leading to different optimality and efficiency properties. We discuss the algorithm, its strategies and heuristics, and we present experimental results showing that the algorithm meets the requirements for our real time systems.},
booktitle = {Proceedings of the 1st International Workshop on Combining Modelling and Search-Based Software Engineering},
pages = {39–44},
numpages = {6},
keywords = {feature model optimization, heuristic search in graphs, real time adaptation, search-based software engineering, software metrics},
location = {San Francisco, California},
series = {CMSBSE '13}
}

@inproceedings{10.1145/2934466.2934481,
author = {Sion, Laurens and Van Landuyt, Dimitri and Joosen, Wouter and de Jong, Gjalt},
title = {Systematic quality trade-off support in the software product-line configuration process},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934481},
doi = {10.1145/2934466.2934481},
abstract = {Software product line engineering is a compelling methodology that accomplishes systematic reuse in families of systems by relying on two key principles: (i) the decomposition of complex systems into composable and reusable building blocks (often logical units called features), and (ii) on-demand construction of products and product variants by composing these building blocks.However, unless the stakeholder responsible for product configuration has detailed knowledge of the technical ins and outs of the software product line (e.g., the architectural impact of a specific feature, or potential feature interactions), he is in many cases flying in the dark. Although many initial approaches and techniques have been proposed that take into account quality considerations and involve trade-off decisions during product configuration, no systematic support exists.In this paper, we present a reference architecture for product configuration tooling, providing support for (i) up-front generation of variants, and (ii) quality analysis of these variants. This allows pro-actively assessing and predicting architectural quality properties for each product variant and in turn, product configuration tools can take into account architectural considerations. In addition, we provide an in-depth discussion of techniques and tactics for dealing with the problem of variant explosion, and as such to maintain practical feasibility of such approaches.We validated and implemented our reference architecture in the context of a real-world industrial application, a product-line for the firmware of an automotive sensor. Our prototype, based on FeatureIDE, is open for extension and readily available.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {164–173},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3307630.3342411,
author = {Meixner, Kristof and Rabiser, Rick and Biffl, Stefan},
title = {Towards Modeling Variability of Products, Processes and Resources in Cyber-Physical Production Systems Engineering},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342411},
doi = {10.1145/3307630.3342411},
abstract = {Planning and developing Cyber-Physical Production Systems (CPPS) are multi-disciplinary engineering activities that rely on effective and efficient knowledge exchange for better collaboration between engineers of different disciplines. The Product-Process-Resource (PPR) approach allows modeling products produced by industrial processes using specific production resources. In practice, a CPPS manufactures a portfolio of product type variants, i.e., a product line. Therefore, engineers need to create and maintain several PPR models to cover PPR variants and their evolving versions. In this paper, we detail a representative use case, identify challenges for using Variability Modeling (VM) methods to describe and manage PPR variants, and present a first solution approach based on cooperation with domain experts at an industry partner, a system integrator of automation for high-performance CPPS. We conclude that integrating basic variability concepts into PPR models is a promising first step and describe our further research plans to support PPR VM in CPPS.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {49–56},
numpages = {8},
keywords = {cyber-physical production system, product-process-resource, variability modelling},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3461001.3471149,
author = {Lesoil, Luc and Acher, Mathieu and T\'{e}rnava, Xhevahire and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {The interplay of compile-time and run-time options for performance prediction},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471149},
doi = {10.1145/3461001.3471149},
abstract = {Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {100–111},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2851613.2851959,
author = {Noorian, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Quality-centric feature model configuration using goal models},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851959},
doi = {10.1145/2851613.2851959},
abstract = {In software product line engineering, a feature model represents the possible configuration space and can be customized based on the stakeholders' needs. Considering the complexity of feature models in addition to the diversity of the stake-holders' expectations, the configuration process is viewed as a complex optimization problem. In this paper, we propose a holistic approach for the configuration process that seeks to satisfy the stakeholders' requirements as well as the feature models' structural and integrity constraints. Here, we model stakeholders' functional and non-functional needs and their preferences using requirement engineering goal models. We formalize the structure of the feature model, the stake-holders' objectives, and their preferences in the form of an integer linear program to automatically perform feature selection.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1296–1299},
numpages = {4},
keywords = {configuration process, feature model, goal model},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/3336294.3336302,
author = {Str\"{u}ber, Daniel and Mukelabai, Mukelabai and Kr\"{u}ger, Jacob and Fischer, Stefan and Linsbauer, Lukas and Martinez, Jabier and Berger, Thorsten},
title = {Facing the Truth: Benchmarking the Techniques for the Evolution of Variant-Rich Systems},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336302},
doi = {10.1145/3336294.3336302},
abstract = {The evolution of variant-rich systems is a challenging task. To support developers, the research community has proposed a range of different techniques over the last decades. However, many techniques have not been adopted in practice so far. To advance such techniques and to support their adoption, it is crucial to evaluate them against realistic baselines, ideally in the form of generally accessible benchmarks. To this end, we need to improve our empirical understanding of typical evolution scenarios for variant-rich systems and their relevance for benchmarking. In this paper, we establish eleven evolution scenarios in which benchmarks would be beneficial. Our scenarios cover typical lifecycles of variant-rich system, ranging from clone &amp; own to adopting and evolving a configurable product-line platform. For each scenario, we formulate benchmarking requirements and assess its clarity and relevance via a survey with experts in variant-rich systems and software evolution. We also surveyed the existing benchmarking landscape, identifying synergies and gaps. We observed that most scenarios, despite being perceived as important by experts, are only partially or not at all supported by existing benchmarks-a call to arms for building community benchmarks upon our requirements. We hope that our work raises awareness for benchmarking as a means to advance techniques for evolving variant-rich systems, and that it will lead to a benchmarking initiative in our community.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {177–188},
numpages = {12},
keywords = {benchmark, product lines, software evolution, software variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3307630.3342404,
author = {Th\"{u}m, Thomas and Seidl, Christoph and Schaefer, Ina},
title = {On Language Levels for Feature Modeling Notations},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342404},
doi = {10.1145/3307630.3342404},
abstract = {Configuration is a key enabling technology for the engineering of systems and software as wells as physical goods. A selection of configuration options (aka. features) is often enough to automatically generate a product tailored to the needs of a customer. It is common that not all combinations of features are possible in a given domain. Feature modeling is the de-facto standard for specifying features and their valid combinations. However, a pivotal hurdle for practitioners, researchers, and teachers in applying feature modeling is that there are hundreds of tools and languages available. While there have been first attempts to define a standard feature modeling language, they still struggle with finding an appropriate level of expressiveness. If the expressiveness is too high, the language will not be adopted, as it is too much effort to support all language constructs. If the expressiveness is too low, the language will not be adopted, as many interesting domains cannot be modeled in such a language. Towards a standard feature modeling notation, we propose the use of language levels with different expressiveness each and discuss criteria to be used to define such language levels. We aim to raise the awareness on the expressiveness and eventually contribute to a standard feature modeling notation.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {158–161},
numpages = {4},
keywords = {automated analysis, expressiveness, feature model, language design, product lines, variability modeling},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3336294.3336311,
author = {T\"{e}rnava, Xhevahire and Mortara, Johann and Collet, Philippe},
title = {Identifying and Visualizing Variability in Object-Oriented Variability-Rich Systems},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336311},
doi = {10.1145/3336294.3336311},
abstract = {In many variability-intensive systems, variability is implemented in code units provided by a host language, such as classes or functions, which do not align well with the domain features. Annotating or creating an orthogonal decomposition of code in terms of features implies extra effort, as well as massive and cumbersome refactoring activities. In this paper, we introduce an approach for identifying and visualizing the variability implementation places within the main decomposition structure of object-oriented code assets in a single variability-rich system. First, we propose to use symmetry, as a common property of some main implementation techniques, such as inheritance or overloading, to identify uniformly these places. We study symmetry in different constructs (e.g., classes), techniques (e.g., subtyping, overloading) and design patterns (e.g., strategy, factory), and we also show how we can use such symmetries to find variation points with variants. We then report on the implementation and application of a toolchain, symfinder, which automatically identifies and visualizes places with symmetry. The publicly available application to several large open-source systems shows that symfinder can help in characterizing code bases that are variability-rich or not, as well as in discerning zones of interest w.r.t. variability.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {231–243},
numpages = {13},
keywords = {identifying software variability, object-oriented variability-rich systems, software product line engineering, tool support for understanding software variability, visualizing software variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3491204.3527489,
author = {Lesoil, Luc and Acher, Mathieu and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {Beware of the Interactions of Variability Layers When Reasoning about Evolution of MongoDB},
year = {2022},
isbn = {9781450391597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491204.3527489},
doi = {10.1145/3491204.3527489},
abstract = {With commits and releases, hundreds of tests are run on varying conditions (e.g., over different hardware and workload) that can help to understand evolution and ensure non-regression of software performance. We hypothesize that performance is not only sensitive to evolution of software, but also to different variability layers of its execution environment, spanning the hardware, the operating system, the build, or the workload processed by the software. Leveraging the MongoDB dataset, our results show that changes in hardware and workload can drastically impact performance evolution and thus should be taken into account when reasoning about performance. An open problem resulting from this study is how to manage the variability layers in order to efficiently test the performance evolution of a software.},
booktitle = {Companion of the 2022 ACM/SPEC International Conference on Performance Engineering},
pages = {39–43},
numpages = {5},
keywords = {deep software variability, software evolution},
location = {Bejing, China},
series = {ICPE '22}
}

@inproceedings{10.1145/3382026.3431246,
author = {Kenner, Andy},
title = {Model-Based Evaluation of Vulnerabilities in Software Systems},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431246},
doi = {10.1145/3382026.3431246},
abstract = {Vulnerabilities in software systems result from faults, which occur at different stages in a software's life cycle, for example, in the design (i.e., undesired feature-interactions), the development (i.e., buffer overflows), or the operation (i.e., configuration errors). Various databases provide detailed information about vulnerabilities in software systems or the way to exploit it, but face severe limitations. The information is scattered across these databases, fluctuates in quality and granularity, and provides only an insight into a single vulnerability per entry. Even for a single software system it is challenging for any security-related stakeholder to determine the threat level, which consists of all vulnerabilities of the software system and its environment (i.e., operating system). Manual vulnerability management is feasible only to a limited extend if we want to identify all configurations that are affected by vulnerabilities, or determine a system's threat level and the resulting risk we have to deal with. For variant-rich systems, we also have to deal with variability, allowing different stakeholders to understand the threats to their particular setup. To deal with this variability, we propose vulnerability feature models, which offer a homogeneous view on all vulnerabilities of a software system. These models and the resulting analyses offer advantages in many disciplines of the vulnerability management process. In this paper, we report the research plan for our project, in which we focus on the model-based evaluation of vulnerabilities. This includes research objectives that take into account the design of vulnerability feature models, their application in the process of vulnerability management, and the impact of evolution, discovery, and verification of vulnerabilities.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {112–119},
numpages = {8},
keywords = {Exploit, Feature Model, Variability Model, Vulnerability, Vulnerability Analysis and Management},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2420942.2420948,
author = {Gonz\'{a}lez-Huerta, Javier and Insfran, Emilio and Abrah\~{a}o, Silvia and McGregor, John D.},
title = {Non-functional requirements in model-driven software product line engineering},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420948},
doi = {10.1145/2420942.2420948},
abstract = {Developing variant-rich software systems through the application of the software product line approach requires the management of a wide set of requirements. However, in most cases, the focus of those requirements is limited to the functional requirements. The non-functional requirements are often informally defined and their management does not provide traceability mechanisms for their validation. In this paper, we present a multimodel approach that allows the explicit representation of non-functional requirements for software product lines both at domain engineering, and application engineering levels. The multimodel allows the representation of different viewpoints of a software product line, including the non-functional requirements and the relationships that these non-functional requirements might have with features and functionalities. The feasibility of this approach is illustrated through a specific example from the automotive domain.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {6},
numpages = {6},
keywords = {model driven engineering, non-functional requirements, software product lines},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and J\'{e}z\'{e}quel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/1985374.1985392,
author = {Bowes, David and Hall, Tracy and Kerr, Andrew},
title = {Program slicing-based cohesion measurement: the challenges of replicating studies using metrics},
year = {2011},
isbn = {9781450305938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985374.1985392},
doi = {10.1145/1985374.1985392},
abstract = {It is important to develop corpuses of data to test out the efficacy of using metrics. Replicated studies are an important contribution to corpuses of metrics data. There are few replicated studies using metrics reported in software engineering.To contribute more data to the body of evidence on the use of novel program slicing-based cohesion metrics.We replicate a very well regarded study by Meyers and Binkley [15, 16] which analyses the cohesion of open source projects using program slicing-based metrics.Our results are very different from Meyers and Binkley's original results. This suggests that there are a variety of opportunities for inconsistently to creep into the collection and analysis of metrics data during replicated studies.We conclude that researchers using metrics data must present their work with sufficient detail for replication to be possible. Without this detail it is difficult for subsequent researchers to accurately replicate a study such that consistent and reliable data can be added to a body of evidence.},
booktitle = {Proceedings of the 2nd International Workshop on Emerging Trends in Software Metrics},
pages = {75–80},
numpages = {6},
keywords = {data quality, empirical software engineering, slicing metrics},
location = {Waikiki, Honolulu, HI, USA},
series = {WETSoM '11}
}

@inproceedings{10.1145/2791060.2791066,
author = {Dhungana, Deepak and Falkner, Andreas and Haselb\"{o}ck, Alois and Schreiner, Herwig},
title = {Smart factory product lines: a configuration perspective on smart production ecosystems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791066},
doi = {10.1145/2791060.2791066},
abstract = {Smart production aims to increase the flexibility of the production processes and be more efficient in the use of resources. Two important pillars of this initiative are "smart products" and "smart factories". From the perspective of product line engineering, these can be seen as two product lines (product line of factories and product line of goods) that need to be integrated for a common systems engineering approach. In this paper, we look at this problem from the perspective of configuration technologies, outline the research challenges in this area and illustrate our vision using an industrial example. The factory product line goes hand-in-hand with the product line of the products to be manufactured. Future research in product line engineering needs to consider an ecosystem of a multitude of stakeholders - e.g., factory component vendors, product designers, factory owners/operators and end-consumers.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {201–210},
numpages = {10},
keywords = {product and production configuration, product line of factories, smart factory, smart product, smart production},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2351676.2351678,
author = {Harman, Mark and Langdon, William B. and Jia, Yue and White, David R. and Arcuri, Andrea and Clark, John A.},
title = {The GISMOE challenge: constructing the pareto program surface using genetic programming to find better programs (keynote paper)},
year = {2012},
isbn = {9781450312042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351676.2351678},
doi = {10.1145/2351676.2351678},
abstract = {Optimising programs for non-functional properties such as speed, size, throughput, power consumption and bandwidth can be demanding; pity the poor programmer who is asked to cater for them all at once! We set out an alternate vision for a new kind of software development environment inspired by recent results from Search Based Software Engineering (SBSE). Given an input program that satisfies the functional requirements, the proposed programming environment will automatically generate a set of candidate program implementations, all of which share functionality, but each of which differ in their non-functional trade offs. The software designer navigates this diverse Pareto surface of candidate implementations, gaining insight into the trade offs and selecting solutions for different platforms and environments, thereby stretching beyond the reach of current compiler technologies. Rather than having to focus on the details required to manage complex, inter-related and conflicting, non-functional trade offs, the designer is thus freed to explore, to understand, to control and to decide rather than to construct.},
booktitle = {Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1–14},
numpages = {14},
keywords = {Compilation, Genetic Programming, Non-functional Properties, Pareto Surface, SBSE, Search Based Optimization},
location = {Essen, Germany},
series = {ASE '12}
}

@inproceedings{10.1145/3236405.3236410,
author = {Kr\"{o}her, Christian and El-Sharkawy, Sascha and Schmid, Klaus},
title = {KernelHaven: an open infrastructure for product line analysis},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236410},
doi = {10.1145/3236405.3236410},
abstract = {KernelHaven is an open infrastructure for Software Product Line (SPL) analysis. It is intended both as a production-quality analysis tool set as well as a research support tool, e.g., to support researchers in systematically exploring research hypothesis. For flexibility and ease of experimentation KernelHaven components are plug-ins for extracting certain information from SPL artifacts and processing this information, e.g., to check the correctness and consistency of variability information or to apply metrics. A configuration-based setup along with automatic documentation functionality allows different experiments and supports their easy reproduction.Here, we describe KernelHaven as a product line analysis research tool and highlight its basic approach as well as its fundamental capabilities. In particular, we describe available information extraction and processing plug-ins and how to combine them. On this basis, researchers and interested professional users can rapidly conduct a first set of experiments. Further, we describe the concepts for extending KernelHaven by new plug-ins, which reduces development effort when realizing new experiments.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {5–10},
numpages = {6},
keywords = {empirical software engineering, software product line analysis, static analysis, variability extraction},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2372251.2372269,
author = {Sj\o{}berg, Dag I.K. and Anda, Bente and Mockus, Audris},
title = {Questioning software maintenance metrics: a comparative case study},
year = {2012},
isbn = {9781450310567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2372251.2372269},
doi = {10.1145/2372251.2372269},
abstract = {Context: Many metrics are used in software engineering research as surrogates for maintainability of software systems. Aim: Our aim was to investigate whether such metrics are consistent among themselves and the extent to which they predict maintenance effort at the entire system level. Method: The Maintainability Index, a set of structural measures, two code smells (Feature Envy and God Class) and size were applied to a set of four functionally equivalent systems. The metrics were compared with each other and with the outcome of a study in which six developers were hired to perform three maintenance tasks on the same systems. Results: The metrics were not mutually consistent. Only system size and low cohesion were strongly associated with increased maintenance effort. Conclusion: Apart from size, surrogate maintainability measures may not reflect future maintenance effort. Surrogates need to be evaluated in the contexts for which they will be used. While traditional metrics are used to identify problematic areas in the code, the improvements of the worst areas may, inadvertently, lead to more problems for the entire system. Our results suggest that local improvements should be accompanied by an evaluation at the system level.},
booktitle = {Proceedings of the ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {107–110},
numpages = {4},
keywords = {software maintenance, software metrics},
location = {Lund, Sweden},
series = {ESEM '12}
}

@inproceedings{10.1145/1527033.1527046,
author = {Schackmann, Holger and Lichter, Horst},
title = {Process assessment by evaluating configuration and change request management systems},
year = {2009},
isbn = {9781605585659},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1527033.1527046},
doi = {10.1145/1527033.1527046},
abstract = {This paper presents an approach for assessing process qualities based on evaluating metrics on change request and configuration management systems. It is based on user-defined quality models to enable quality evaluations customized to the information needs of an organization. Further on the concept of declarative metric specifications is introduced, which enables a precise definition of metrics on an appropriate abstraction level. With the corresponding tool support given in the QMetric tool suite, this concept simplifies development and validation of the metrics needed for quality evaluations.},
booktitle = {Proceedings of the Warm Up Workshop for ACM/IEEE ICSE 2010},
pages = {37–40},
numpages = {4},
keywords = {declarative metric specification, mining software repositories, quality modeling, software product management},
location = {Cape Town, South Africa},
series = {WUP '09}
}

@inproceedings{10.1145/1739230.1739240,
author = {Farias, Kleinner and Garcia, Alessandro and Whittle, Jon},
title = {Assessing the impact of aspects on model composition effort},
year = {2010},
isbn = {9781605589589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1739230.1739240},
doi = {10.1145/1739230.1739240},
abstract = {Model composition is a common operation used in many software development activities---for example, reconciling models developed in parallel by different development teams, or merging models of new features with existing model artifacts. Unfortunately, both commercial and academic model composition tools suffer from the composition conflict problem. That is, models to-be-composed may conflict with each other and these conflicts must be resolved. In practice, detecting and resolving conflicts is a highly-intensive manual activity. In this paper, we investigate whether aspect-orientation reduces conflict resolution effort as improved modularization may better localize conflicts. The main goal of the paper is to conduct an exploratory study to analyze the impact of aspects on conflict resolution. In particular, model compositions are used to express the evolution of architectural models along six releases of a software product line. Well-known composition algorithms, such as override, merge and union, are applied and compared on both AO and non-AO models in terms of their conflict rate and effort to solve the identified conflicts. Our findings identify specific scenarios where aspect-orientation properties, such as obliviousness and quantification, result in a lower (or higher) composition effort.},
booktitle = {Proceedings of the 9th International Conference on Aspect-Oriented Software Development},
pages = {73–84},
numpages = {12},
keywords = {empirical studies, model composition, software architecture, software metrics, software product lines},
location = {Rennes and Saint-Malo, France},
series = {AOSD '10}
}

@inproceedings{10.5555/2008503.2008518,
author = {Bo\v{s}kovi\'{c}, Marko and Mussbacher, Gunter and Bagheri, Ebrahim and Amyot, Daniel and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek},
title = {Aspect-oriented feature models},
year = {2010},
isbn = {9783642212093},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software Product Lines (SPLs) have emerged as a prominent approach for software reuse. SPLs are sets of software systems called families that are usually developed as a whole and share many common features. Feature models are most typically used as a means for capturing commonality and managing variability of the family. A particular product from the family is configured by selecting the desired features of that product. Typically, feature models are considered monolithic entities that do not support modularization well. As industrial feature models tend to be large, their modularization has become an important research topic lately. However, existing modularization approaches do not support modularization of crosscutting concerns. In this paper, we introduce Aspect-oriented Feature Models (AoFM) and argue that using aspect-oriented techniques improves the manageability and reduces the maintainability effort of feature models. Particularly, we advocate an asymmetric approach that allows for the modularization of basic and crosscutting concerns in feature models.},
booktitle = {Proceedings of the 2010 International Conference on Models in Software Engineering},
pages = {110–124},
numpages = {15},
keywords = {aspect-oriented modeling, feature models, software product lines},
location = {Oslo, Norway},
series = {MODELS'10}
}

@inproceedings{10.1145/1982185.1982522,
author = {Mohabbati, Bardia and Hatala, Marek and Ga\v{s}evi\'{c}, Dragan and Asadi, Mohsen and Bo\v{s}kovi\'{c}, Marko},
title = {Development and configuration of service-oriented systems families},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982522},
doi = {10.1145/1982185.1982522},
abstract = {Software Product Lines (SPLs) are families of software systems which share a common sets of feature and are developed through common set of core assets in order to promotes software reusability, mass customization, reducing cost, time-to-market and improving the quality of the product. SPLs are sets (i.e., families) of software applications developed as a whole for a specific business domain. Particular applications are derived from software families by selecting the desired features through configuration process. Traditionally, SPLs are implemented with systematically developed components, shared by members of the SPLs and reused every time a new application is derived. In this paper, we propose an approach to the development and configuration of Service-Oriented SPLs in which services are used as reusable assets and building blocks of implementation. Our proposed approach also suggests prioritization of family features according to stakeholder's non-functional requirements (NFRs) and preferences. Priorities of NFRs are used to filter the most important features of the family, which is performed by Stratified Analytic Hierarchical Process (S-AHP). The priorities also are used further for the selection of appropriate services implementation for business processes realizing features. We apply Mixed Integer Linear Programming to find the optimal service selection within the constraints boundaries specified by stakeholders.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {1606–1613},
numpages = {8},
keywords = {feature-oriented development, optimization, service selection, service-oriented architecture, software product line},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@inproceedings{10.1145/3461001.3461660,
author = {Michelon, Gabriela Karoline and Obermann, David and Assun\c{c}\~{a}o, Wesley K. G. and Linsbauer, Lukas and Gr\"{u}nbacher, Paul and Egyed, Alexander},
title = {Managing systems evolving in space and time: four challenges for maintenance, evolution and composition of variants},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3461660},
doi = {10.1145/3461001.3461660},
abstract = {Software companies need to provide a large set of features satisfying functional and non-functional requirements of diverse customers, thereby leading to variability in space. Feature location techniques have been proposed to support software maintenance and evolution in space. However, so far only one feature location technique also analyses the evolution in time of system variants, which is required for feature enhancements and bug fixing. Specifically, existing tools for managing a set of systems over time do not offer proper support for keeping track of feature revisions, updating existing variants, and creating new product configurations based on feature revisions. This paper presents four challenges concerning such capabilities for feature (revision) location and composition of new product configurations based on feature/s (revisions). We also provide a benchmark containing a ground truth and support for computing metrics. We hope that this will motivate researchers to provide and evaluate tool-supported approaches aiming at managing systems evolving in space and time. Further, we do not limit the evaluation of techniques to only this benchmark: we introduce and provide instructions on how to use a benchmark extractor for generating ground truth data for other systems. We expect that the feature (revision) location techniques maximize information retrieval in terms of precision, recall, and F-score, while keeping execution time and memory consumption low.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {75–80},
numpages = {6},
keywords = {benchmark extractor, feature location, feature revision, repository mining, software product line},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2019136.2019155,
author = {Ribeiro, Heberth Braga G. and de Almeida, Eduardo Santana and de Lemos Meira, Silvio R.},
title = {An approach for implementing core assets in service-oriented product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019155},
doi = {10.1145/2019136.2019155},
abstract = {In the combination of Software Product Lines (SPL) and Service-Oriented Architectures (SOA), a key aspect is the understanding about the use of variability mechanisms for realizing variabilities in different levels of granularity, e.g., components, services, and service-compositions, addressed in a specific service technology and systematic fashion. In this paper, we propose an approach for implementing core assets in service-oriented product lines based on a well defined set of activities, tasks, inputs, outputs and roles. In order to assess the quality of the proposed approach, an initial case study conducted at the university was performed.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {17},
numpages = {4},
keywords = {service-oriented architecture, service-oriented product lines, software development process, software product lines},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2695664.2695875,
author = {Almeida, Andr\'{e} and Bencomo, Nelly and Batista, Thais and Cavalcante, Everton and Dantas, Francisco},
title = {Dynamic decision-making based on NFR for managing software variability and configuration selection},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695875},
doi = {10.1145/2695664.2695875},
abstract = {Due to dynamic variability, identifying the specific conditions under which non-functional requirements (NFRs) are satisfied may be only possible at runtime. Therefore, it is necessary to consider the dynamic treatment of relevant information during the requirements specifications. The associated data can be gathered by monitoring the execution of the application and its underlying environment to support reasoning about how the current application configuration is fulfilling the established requirements. This paper presents a dynamic decision-making infrastructure to support both NFRs representation and monitoring, and to reason about the degree of satisfaction of NFRs during runtime. The infrastructure is composed of: (i) an extended feature model aligned with a domain-specific language for representing NFRs to be monitored at runtime; (ii) a monitoring infrastructure to continuously assess NFRs at runtime; and (iii) a flexible decision-making process to select the best available configuration based on the satisfaction degree of the NRFs. The evaluation of the approach has shown that it is able to choose application configurations that well fit user NFRs based on runtime information. The evaluation also revealed that the proposed infrastructure provided consistent indicators regarding the best application configurations that fit user NFRs. Finally, a benefit of our approach is that it allows us to quantify the level of satisfaction with respect to NFRs specification.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1376–1382},
numpages = {7},
keywords = {SPLs, monitoring, non-functional requirements, variability},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/3336294.3336304,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Software Product Line Engineering: A Practical Experience},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336304},
doi = {10.1145/3336294.3336304},
abstract = {The lack of mature tool support is one of the main reasons that make the industry to be reluctant to adopt Software Product Line (SPL) approaches. A number of systematic literature reviews exist that identify the main characteristics offered by existing tools and the SPL phases in which they can be applied. However, these reviews do not really help to understand if those tools are offering what is really needed to apply SPLs to complex projects. These studies are mainly based on information extracted from the tool documentation or published papers. In this paper, we follow a different approach, in which we firstly identify those characteristics that are currently essential for the development of an SPL, and secondly analyze whether the tools provide or not support for those characteristics. We focus on those tools that satisfy certain selection criteria (e.g., they can be downloaded and are ready to be used). The paper presents a state of practice with the availability and usability of the existing tools for SPL, and defines different roadmaps that allow carrying out a complete SPL process with the existing tool support.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {164–176},
numpages = {13},
keywords = {spl in practice, state of practice, tool support, tooling roadmap},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2491627.2499880,
author = {Clarke, Dave and Schaefer, Ina and ter Beek, Maurice H. and Apel, Sven and Atlee, Joanne M.},
title = {Formal methods and analysis in software product line engineering: 4th edition of FMSPLE workshop series},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2499880},
doi = {10.1145/2491627.2499880},
abstract = {FMSPLE 2013 is the fourth edition of the FMSPLE workshop series aimed at connecting researchers and practitioners interested in raising the efficiency and the effectiveness of software product line engineering through the application of innovative analysis approaches and formal methods.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {266–267},
numpages = {2},
keywords = {evolution, formal methods, semantics, software product lines, testing, variability, verification},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@article{10.1145/3229048,
author = {Zheng, Yongjie and Cu, Cuong and Taylor, Richard N.},
title = {Maintaining Architecture-Implementation Conformance to Support Architecture Centrality: From Single System to Product Line Development},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3229048},
doi = {10.1145/3229048},
abstract = {Architecture-centric development addresses the increasing complexity and variability of software systems by focusing on architectural models, which are generally easier to understand and manipulate than source code. It requires a mechanism that can maintain architecture-implementation conformance during architectural development and evolution. The challenge is twofold. There is an abstraction gap between software architecture and implementation, and both may evolve. Existing approaches are deficient in support for both change mapping and product line architecture. This article presents a novel approach named 1.x-way mapping and its extension, 1.x-line mapping to support architecture-implementation mapping in single system development and in product line development, respectively. They specifically address mapping architecture changes to code, maintaining variability conformance between product line architecture and code, and tracing architectural implementation. We built software tools named xMapper and xLineMapper to realize the two approaches, and conducted case studies with two existing open-source systems to evaluate the approaches. The result shows that our approaches are applicable to the implementation of a real software system and are capable of maintaining architecture-implementation conformance during system evolution.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {8},
numpages = {52},
keywords = {Architecture-implementation mapping, architectural evolution, architecture-centric development, architecture-centric feature traceability, variability conformance}
}

@inproceedings{10.1145/1809223.1809230,
author = {Lind, Kenneth and Heldal, Rogardt},
title = {On the relationship between functional size and software code size},
year = {2010},
isbn = {9781605589763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1809223.1809230},
doi = {10.1145/1809223.1809230},
abstract = {SLOC (Source Lines-Of-Code) has been used extensively as a Code Size Measure, and as input to parametric software cost and effort estimation tools. SLOC is obtained by measuring FP (Function Points) on the requirements and multiplying by the SLOC/FP ratio from similar projects. This is done even though several studies show large variations in this ratio, due to weak correlation between FP and SLOC. However, in our previous experiments we have obtained strong correlation between CFP (COSMIC Function Points) and Bytes compiled code as Code Size Measure. The experiments were conducted in the automotive industry using software components developed by GM (General Motors). In this paper we explain the reasons behind the strong correlation. The main reasons are that we apply the COSMIC method on software components of similar type, with a 1-to-1 mapping to COSMIC. A strong correlation between the Functional Size Measure and the Code Size Measure is required to obtain accurate Code Size estimation results. To estimate the Code Size before the software is available, is important both for Cost/Effort estimation and design of electronic hardware.},
booktitle = {Proceedings of the 2010 ICSE Workshop on Emerging Trends in Software Metrics},
pages = {47–52},
numpages = {6},
keywords = {COSMIC function points, UML components, bytes, functional size measurement, software code size, source lines-of-code, system architecture},
location = {Cape Town, South Africa},
series = {WETSoM '10}
}

@inproceedings{10.1145/3382025.3414960,
author = {Str\"{u}der, Stefan and Mukelabai, Mukelabai and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Feature-oriented defect prediction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414960},
doi = {10.1145/3382025.3414960},
abstract = {Software errors are a major nuisance in software development and can lead not only to reputation damages, but also to considerable financial losses for companies. Therefore, numerous techniques for predicting software defects, largely based on machine learning methods, have been developed over the past decades. These techniques usually rely on code and process metrics in order to predict defects at the granularity of typical software assets, such as subsystems, components, and files. In this paper, we present the first systematic investigation of feature-oriented defect prediction: the prediction of defects at the granularity of features---domain-oriented entities abstractly representing (and often cross-cutting) typical software assets. Feature-oriented prediction can be beneficial, since: (i) particular features might be more error-prone than others, (ii) characteristics of features known as defective might be useful to predict other error-prone features, (iii) feature-specific code might be especially prone to faults arising from feature interactions. We present a dataset derived from 12 software projects and introduce two metric sets for feature-oriented defect prediction. We evaluated seven machine learning classifiers with three different attribute sets each, using our two new metric sets as well as an existing metric set from the literature. We observe precision and recall values of around 85% and better robustness when more diverse metrics sets with richer feature information are used.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {21},
numpages = {12},
keywords = {classification, defect, feature, prediction},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3622748.3622750,
author = {Arasaki, Caio and Wolschick, Lucas and Freire, Willian and Amaral, Aline},
title = {Feature selection in an interactive search-based PLA design approach},
year = {2023},
isbn = {9798400709524},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622748.3622750},
doi = {10.1145/3622748.3622750},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line (SPL). PLA design can be formulated as an interactive optimization problem with many conflicting factors. Incorporating Decision Makers’ (DM) preferences during the search process may help the algorithms find more adequate solutions for their profiles. Interactive approaches allow the DM to evaluate solutions, guiding the optimization according to their preferences. However, this brings up human fatigue problems caused by the excessive amount of interactions and solutions to evaluate. A common strategy to prevent this problem is limiting the number of interactions and solutions evaluated by the DM. Machine Learning (ML) models were also used to learn how to evaluate solutions according to the DM profile and replace them after some interactions. Feature selection performs an essential task as non-relevant and/or redundant features used to train the ML model can reduce the accuracy and comprehensibility of the hypotheses induced by ML algorithms. This work aims to select features of an ML model used to prevent human fatigue in an interactive search-based PLA design approach. We applied four selectors and through results we were able to reduce 30% of features, obtaining an accuracy of 99%.},
booktitle = {Proceedings of the 17th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Feature Selection, Interactive search-based Software Engineering, Machine Learning},
location = {Campo Grande, Brazil},
series = {SBCARS '23}
}

@inproceedings{10.1145/2362536.2362560,
author = {Lettner, Daniela and Vierhauser, Michael and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {Supporting end users with business calculations in product configuration},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362560},
doi = {10.1145/2362536.2362560},
abstract = {Business calculations like break-even, return on investment, or cost are essential in many domains to support decision making while configuring products. For instance, customers and sales people need to estimate and compare the business value of different product variants. Some product line approaches provide initial support, e.g., by defining quality attributes in relation to features. However, an approach that allows domain engineers to easily define business calculations together with variability models is still lacking. In product configuration, calculation results need to be instantly presented to end users after making configuration choices. Further, due to the often high number of calculations, the presentation of calculation results to end users can be challenging. These challenges cannot be addressed by integrating off-the-shelf applications performing the calculations with product line tools. We thus present an approach based on dedicated calculation models that are related to variability models. Our approach seamlessly integrates business calculations with product configuration and provides support for formatting calculations and calculation results. We use the DOPLER tool suite to deploy calculations together with variability models to end users in product configuration. We evaluate the expressiveness and practical relevance of the approach by investigating the development of business calculations for 15 product lines from the domain of industrial automation.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {171–180},
numpages = {10},
keywords = {business calculations, product configuration, variability models},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2362536.2362576,
author = {ter Beek, Maurice H. and Becker, Martin and Classen, Andreas and Roos-Frantz, Fabricia and Schaefer, Ina and Wong, Peter Y. H.},
title = {Formal methods and analysis in software product line engineering: 3rd edition of FMSPLE workshop series},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362576},
doi = {10.1145/2362536.2362576},
abstract = {FMSPLE 2012 is the third edition of the FMSPLE workshop series, traditionally affiliated with SPLC, which aims to connect researchers and practitioners interested in raising the efficiency and the effectiveness of SPLE through the application of innovative analysis approaches and formal methods.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {286–287},
numpages = {2},
keywords = {evolution, formal methods, semantics, software product lines, testing, variability, verification},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/1774088.1774504,
author = {Chowdhury, Istehad and Zulkernine, Mohammad},
title = {Can complexity, coupling, and cohesion metrics be used as early indicators of vulnerabilities?},
year = {2010},
isbn = {9781605586397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1774088.1774504},
doi = {10.1145/1774088.1774504},
abstract = {It is difficult to detect vulnerabilities until they manifest themselves as security failures in the operational stage of software, because the security concerns are not addressed or known sufficiently early during software development. Complexity, coupling, and cohesion (CCC) related software metrics can be measured during the earlier phases of software development. If empirical relationships can be discovered between CCC metrics and vulnerabilities, these metrics could aid software developers to take proactive actions against potential vulnerabilities in software. In this paper, we conduct an extensive case study on Mozilla Firefox to provide empirical evidence on how vulnerabilities are related to complexity, coupling, and cohesion. We find that CCC metrics are correlated to vulnerabilities at a statistically significant level. We further examine the correlations to determine which level (design or code) of CCC metrics are better indicators of vulnerabilities. We also observe that the correlation patterns are stable across multiple releases of the software. These observations show that CCC metrics can be dependably used as early indicators of vulnerabilities in software.},
booktitle = {Proceedings of the 2010 ACM Symposium on Applied Computing},
pages = {1963–1969},
numpages = {7},
keywords = {cohesion, complexity, coupling, empirical study, security, software metrics, software security, vulnerability},
location = {Sierre, Switzerland},
series = {SAC '10}
}

@inproceedings{10.1109/ASE.2013.6693115,
author = {Garc\'{\i}a-Gal\'{a}n, Jes\'{u}s and Trinidad, Pablo and Ruiz-Cort\'{e}s, Antonio},
title = {Multi-user variability configuration: a game theoretic approach},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693115},
doi = {10.1109/ASE.2013.6693115},
abstract = {Multi-user configuration is a neglected problem in variability-intensive systems area. The appearance of conflicts among user configurations is a main concern. Current approaches focus on avoiding such conflicts, applying the mutual exclusion principle. However, this perspective has a negative impact on users satisfaction, who cannot make any decision fairly. In this work, we propose an interpretation of multi-user configuration as a game theoretic problem. Game theory is a well-known discipline which analyzes conflicts and cooperation among intelligent rational decision-makers. We present a taxonomy of multi-user configuration approaches, and how they can be interpreted as different problems of game theory. We focus on cooperative game theory to propose and automate a tradeoff-based bargaining approach, as a way to solve the conflicts and maximize user satisfaction at the same time.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {574–579},
numpages = {6},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@inproceedings{10.1145/3422392.3422498,
author = {Freire, Willian Marques and Massago, Mamoru and Zavadski, Arthur Cattaneo and Malachini, Aline Maria and Amaral, Miotto and Colanzi, Thelma Elita},
title = {OPLA-Tool v2.0: a Tool for Product Line Architecture Design Optimization},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422498},
doi = {10.1145/3422392.3422498},
abstract = {The Multi-objective Optimization Approach for Product Line Architecture Design (MOA4PLA) is the seminal approach that successfully optimizes Product Line Architecture (PLA) design using search algorithms. The tool named OPLA-Tool was developed in order to automate the use of MOA4PLA. Over time, the customization of the tool to suit the needs of new research and application scenarios led to several problems. The main problems identified in the original version of OPLA-Tool are environment configuration, maintainability and usability problems, and PLA design modeling and visualization. Such problems motivated the development of a new version of this tool: OPLA-Tool v2.0, presented in this work. In this version, those problems were solved by the source code refactoring, migration to a web-based graphical user interface (GUI) and inclusion of a new support tool for PLA modeling and visualization. Furthermore, OPLA-Tool v2.0 has new functionalities, such as new objective functions, new search operators, intelligent interaction with users during the optimization process, multi-user authentication and simultaneous execution of several experiments to PLA optimization. Such a new version of OPLA-Tool is an important achievement to PLA design optimization as it provides an easier and more complete way to automate this task.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {818–823},
numpages = {6},
keywords = {Software product line, multi-objective evolutionary algorithms, product line architecture},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1145/2915970.2916014,
author = {Veado, Lucas and Vale, Gustavo and Fernandes, Eduardo and Figueiredo, Eduardo},
title = {TDTool: threshold derivation tool},
year = {2016},
isbn = {9781450336918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2915970.2916014},
doi = {10.1145/2915970.2916014},
abstract = {Software metrics provide basic means to quantify quality of software systems. However, the effectiveness of the measurement process is directly dependent on the definition of reliable thresholds. If thresholds are not properly defined, it is difficult to know, for instance, whether a given metric value indicates a potential problem in a class implementation. There are several methods proposed in literature to derive thresholds for software metrics. However, most of these methods (i) do not respect the skewed distribution of software metrics and (ii) do not provide a supporting tool. Aiming to fill the second gap, we propose a tool, called TDTool, to derive metric thresholds. TDTool is open source and supports four different methods for threshold derivation. This paper presents TDTool architecture and illustrates how to use it. It also presents the thresholds derived using each method based on a benchmark of 33 software product lines.},
booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {24},
numpages = {5},
keywords = {metrics, software systems, thresholds},
location = {Limerick, Ireland},
series = {EASE '16}
}

@inproceedings{10.1145/1368088.1368190,
author = {Mockus, Audris and Weiss, David},
title = {Interval quality: relating customer-perceived quality to process quality},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1368088.1368190},
doi = {10.1145/1368088.1368190},
abstract = {We investigate relationships among software quality measures commonly used to assess the value of a technology, and several aspects of customer perceived quality measured by Interval Quality (IQ): a novel measure of the probability that a customer will observe a failure within a certain interval after software release. We integrate information from development and customer support systems to compare defect density measures and IQ for six releases of a major telecommunications system. We find a surprising negative relationship between the traditional defect density and IQ. The four years of use in several large telecommunication products demonstrates how a software organization can control customer perceived quality not just during development and verification, but also during deployment by changing the release rate strategy and by increasing the resources to correct field problems rapidly. Such adaptive behavior can compensate for the variations in defect density between major and minor releases.},
booktitle = {Proceedings of the 30th International Conference on Software Engineering},
pages = {723–732},
numpages = {10},
keywords = {software metrics},
location = {Leipzig, Germany},
series = {ICSE '08}
}

@inproceedings{10.1145/3377024.3377026,
author = {Kenner, Andy and Dassow, Stephan and Lausberger, Christian and Kr\"{u}ger, Jacob and Leich, Thomas},
title = {Using variability modeling to support security evaluations: virtualizing the right attack scenarios},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377026},
doi = {10.1145/3377024.3377026},
abstract = {A software system's security is constantly threatened by vulnerabilities that result from faults in the system's design (e.g., unintended feature interactions) and which can be exploited with attacks. While various databases summarize information on vulnerabilities and other security issues for many software systems, these databases face severe limitations. For example, the information's quality is unclear, often only semi-structured, and barely connected to other information. Consequently, it can be challenging for any security-related stakeholder to extract and understand what information is relevant, considering that most systems exist in different variants and versions. To tackle this problem, we propose to design vulnerability feature models that represent the vulnerabilities of a system and enable developers to virtualize corresponding attack scenarios. In this paper, we report a first case study on Mozilla Firefox for which we extracted vulnerabilities and used them to virtualize vulnerable instances in Docker. To this end, we focused on extracting information from available databases and on evaluating the usability of the results. Our findings indicate several problems with the extraction that complicate modeling, understanding, and testing of vulnerabilities. Nonetheless, the databases provide a valuable foundation for our technique, which we aim to extend with automatic synthesis and analyses of feature models, as well as virtualization for attack scenarios in future work.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {10},
numpages = {9},
keywords = {attack scenarios, docker-container, exploit, feature model, software architecture, variability model, vulnerability},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1145/3023956.3023968,
author = {Mjeda, Anila and Wasala, Asanka and Botterweck, Goetz},
title = {Decision spaces in product lines, decision analysis, and design exploration: an interdisciplinary exploratory study},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023968},
doi = {10.1145/3023956.3023968},
abstract = {Context. From recent works on product properties resulting from configurations and the optimisation of these properties, one comes quickly to more complex challenges such as multi-objective optimisation, conflicting objectives, multiple stakeholders, and conflict resolution. The intuition is that Software Product Line Engineering (SPLE) can draw from other disciplines that deal with decision spaces and complex decision scenarios.Objectives. We aim to (1) explore links to such disciplines, (2) systematise and compare concepts, and (3) identify opportunities, where SPLE approaches can be enriched.Method. We undertake an exploratory study: Starting from common SPLE activities and artefacts, we identify aspects where we expect to find corresponding counterparts in other disciplines. We focus on Multiple Criteria Decision Analysis (MCDA), Multi-Objective Optimisation (MOO), and Design Space Exploration (DSE), and perform a comparison of the key concepts.Results. The resulting comparison relates SPLE activities and artefacts to concepts from MCDA, MOO, and DSE and identifies areas where SPLE approaches can be enriched. We also provide examples of existing work at the intersections of SPLE with the other fields. These findings are aimed to foster the conversation on research opportunities where SPLE can draw techniques from other disciplines dealing with complex decision scenarios.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {68–75},
numpages = {8},
keywords = {decision modelling, design-space exploration, multi-criteria decision analysis, multi-objective optimisation},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/2430502.2430522,
author = {von Rhein, Alexander and Apel, Sven and K\"{a}stner, Christian and Th\"{u}m, Thomas and Schaefer, Ina},
title = {The PLA model: on the combination of product-line analyses},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430522},
doi = {10.1145/2430502.2430522},
abstract = {Product-line analysis has received considerable attention in the last decade. As it is often infeasible to analyze each product of a product line individually, researchers have developed analyses, called variability-aware analyses, that consider and exploit variability manifested in a code base. Variability-aware analyses are often significantly more efficient than traditional analyses, but each of them has certain weaknesses regarding applicability or scalability. We present the Product-Line-Analysis model, a formal model for the classification and comparison of existing analyses, including traditional and variability-aware analyses, and lay a foundation for formulating and exploring further, combined analyses. As a proof of concept, we discuss different examples of analyses in the light of our model, and demonstrate its benefits for systematic comparison and exploration of product-line analyses.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {14},
numpages = {8},
keywords = {PLA model, product-line analysis, software product lines},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@inproceedings{10.1145/2556624.2556641,
author = {Berger, Thorsten and Guo, Jianmei},
title = {Towards system analysis with variability model metrics},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556641},
doi = {10.1145/2556624.2556641},
abstract = {Variability models are central artifacts in highly configurable systems. They aim at planning, developing, and configuring systems by describing configuration knowledge at different levels of formality. The existence of large models using a variety of modeling concepts in heterogeneous languages with intricate semantics calls for a unified measuring approach. In this position paper, we attempt to take a first step towards such a measurement. We discuss perspectives of metrics, define low-level measurement goals, and conceive and implement metrics based on variability modeling concepts found in real-world languages and models. An evaluation of these metrics with real-world models and codebases provides insight into the benefits of such metrics for the defined perspectives.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {23},
numpages = {8},
keywords = {empirical software engineering, feature modeling, metrics, software product lines, variability modeling},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@inproceedings{10.1145/2377816.2377818,
author = {Kamischke, Jochen and Lochau, Malte and Baller, Hauke},
title = {Conditioned model slicing of feature-annotated state machines},
year = {2012},
isbn = {9781450313094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2377816.2377818},
doi = {10.1145/2377816.2377818},
abstract = {Model-based behavioral specifications build the basis for comprehensive quality assurance techniques for complex software systems such as model checking and model-based testing. Various attempts exist to adopt those approaches to variant-rich applications as apparent in software product line engineering to efficiently analyze families of similar software systems. Therefore, models are usually enriched with capabilities to explicitly specify variable parts by means of annotations denoting selection conditions over feature parameters. However, a major drawback of model-based engineering is still its lack of scalability. Model slicing provides a promising technique to reduce models to only those objects being relevant for a certain criterion under consideration such as a particular test goal. Here, we present an approach for slicing feature-annotated state machine models. To support feature-oriented slicing on those models, our framework combines principles of variability encoding and conditioned slicing. We also present an implementation and provide experimental results concerning the efficiency of the slicing algorithm.},
booktitle = {Proceedings of the 4th International Workshop on Feature-Oriented Software Development},
pages = {9–16},
numpages = {8},
keywords = {model-based software engineering, software product lines},
location = {Dresden, Germany},
series = {FOSD '12}
}

@inproceedings{10.1145/2695664.2696059,
author = {Hozano, Mario and Ferreira, Henrique and Silva, Italo and Fonseca, Baldoino and Costa, Evandro},
title = {Using developers' feedback to improve code smell detection},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2696059},
doi = {10.1145/2695664.2696059},
abstract = {Several studies are focused on the study of code smells and many detection techniques have been proposed. In this scenario, the use of rules involving software-metrics has been widely used in refactoring tools as a mechanism to detect code smells automatically. However, actual approaches present two unsatisfactory aspects: they present a low agreement in its results and, they do not consider the developers' feedback. In this way, these approaches detect smells that are not relevant to the developers. In order to solve the above mentioned unsatisfactory aspects in the state-of the-art of code smells detection, we propose the Smell Platform able to recognize code smells more relevant to developers by using its feedback. In this paper we present how such platform is able to detect four well known code smells. Finally, we evaluate the Smell Platform comparing its results with traditional detection techniques.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1661–1663},
numpages = {3},
keywords = {code smell detection, developer's feedback, refactoring},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/3425269.3425271,
author = {Nicolodi, Luciane Baldo and Colanzi, Thelma Elita and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Architectural Feature Re-Modularization for Software Product Line Evolution},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425271},
doi = {10.1145/3425269.3425271},
abstract = {Extensive maintenance leads to the Software Product Line Architecture (PLA) degradation over time. When there is the need of evolving the Software Product Line (SPL) to include new features, or move to a new platform, a degraded PLA requires considerable effort to understand and modify, demanding expensive refactoring activity. In the state of the art, search-based algorithms are used to improve PLA at package level. However, recent studies have shown that the most variability and implementation details of an SPL are described in the level of classes. There is a gap between existing approaches and existing practical needs. In this work, we extend the current state of the art to deal with feature modularization in the level of classes by introducing a new search operator and a set of objective functions to deal with feature modularization in a finer granularity of the architectural elements, namely at class level. We evaluated the proposal in an exploratory study with a PLA widely investigated and a real-world PLA. The results of quantitative and qualitative analysis point out that our proposal provides solutions to properly re-modularize features in a PLA, being preferred by practitioners, in order to support the evolution of SPLs.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {31–40},
numpages = {10},
keywords = {Architectural Degradation, Feature Modularization, Search-based Software Engineering, Software Evolution},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1145/1456362.1456372,
author = {Shin, Yonghee and Williams, Laurie},
title = {Is complexity really the enemy of software security?},
year = {2008},
isbn = {9781605583211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1456362.1456372},
doi = {10.1145/1456362.1456372},
abstract = {Software complexity is often hypothesized to be the enemy of software security. We performed statistical analysis on nine code complexity metrics from the JavaScript Engine in the Mozilla application framework to investigate if this hypothesis is true. Our initial results show that the nine complexity measures have weak correlation (ρ=0.30 at best) with security problems for Mozilla JavaScript Engine. The study should be replicated on more products with design and code-level metrics. It may be necessary to create new complexity metrics to embody the type of complexity that leads to security problems.},
booktitle = {Proceedings of the 4th ACM Workshop on Quality of Protection},
pages = {47–50},
numpages = {4},
keywords = {fault prediction, reliability, security metrics, software complexity, software metrics, vulnerability prediction},
location = {Alexandria, Virginia, USA},
series = {QoP '08}
}

@inproceedings{10.1145/1837154.1837157,
author = {Siegmund, Norbert and Feigenspan, Janet and Soffner, Michael and Fruth, Jana and K\"{o}ppen, Veit},
title = {Challenges of secure and reliable data management in heterogeneous environments},
year = {2010},
isbn = {9781605589923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1837154.1837157},
doi = {10.1145/1837154.1837157},
abstract = {Ubiquitous computing is getting more important since requirements for complex systems grow fast. In these systems, embedded devices have to fulfill different tasks. They have to monitor the environment, store data, communicate with other devices, and react to user input. In addition to this complexity, quality issues such as security and reliability have to be considered, as well, due to their increasing use in life critical application scenarios. Finally, different devices with different application goals are used, which results in interoperability problems. In this paper, we highlight challenges for interoperability, data management, and security, which arise with complex systems. Furthermore, we present approaches to overcome different problems and how an integrated solution can be realized using software product line techniques.},
booktitle = {Proceedings of the First International Workshop on Digital Engineering},
pages = {17–24},
numpages = {8},
keywords = {data management, digital engineering, security, software product lines},
location = {Magdeburg, Germany},
series = {IWDE '10}
}

@inproceedings{10.1145/2791060.2791099,
author = {Filho, Jo\~{a}o Bosco Ferreira and Allier, Simon and Barais, Olivier and Acher, Mathieu and Baudry, Benoit},
title = {Assessing product line derivation operators applied to Java source code: an empirical study},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791099},
doi = {10.1145/2791060.2791099},
abstract = {Product Derivation is a key activity in Software Product Line Engineering. During this process, derivation operators modify or create core assets (e.g., model elements, source code instructions, components) by adding, removing or substituting them according to a given configuration. The result is a derived product that generally needs to conform to a programming or modeling language. Some operators lead to invalid products when applied to certain assets, some others do not; knowing this in advance can help to better use them, however this is challenging, specially if we consider assets expressed in extensive and complex languages such as Java. In this paper, we empirically answer the following question: which product line operators, applied to which program elements, can synthesize variants of programs that are incorrect, correct or perhaps even conforming to test suites? We implement source code transformations, based on the derivation operators of the Common Variability Language. We automatically synthesize more than 370,000 program variants from a set of 8 real large Java projects (up to 85,000 lines of code), obtaining an extensive panorama of the sanity of the operations.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {36–45},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1145/3313789,
author = {Reuling, Dennis and Kelter, Udo and B\"{u}rdek, Johannes and Lochau, Malte},
title = {Automated N-way Program Merging for Facilitating Family-based Analyses of Variant-rich Software},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3313789},
doi = {10.1145/3313789},
abstract = {Nowadays software tends to come in many different, yet similar variants, often derived from a common code base via clone-and-own. Family-based-analysis strategies have recently shown very promising potential for improving efficiency in applying quality-assurance techniques to such variant-rich programs, as compared to variant-by-variant approaches. Unfortunately, these strategies require a single program representation superimposing all program variants in a syntactically well-formed, semantically sound, and variant-preserving manner, which is usually not available and manually hard to obtain in practice. In this article, we present a novel methodology, called SiMPOSE, for automatically generating superimpositions of existing program variants to facilitate family-based analyses of variant-rich software. To this end, we propose a novel N-way model-merging methodology to integrate the control-flow automaton (CFA) representations of N given variants of a C program into one unified CFA representation. CFA constitute a unified program abstraction used by many recent software-analysis tools for automated quality assurance. To cope with the inherent complexity of N-way model-merging, our approach (1) utilizes principles of similarity-propagation to reduce the number of potential N-way matches, and (2) enables us to decompose a set of N variants into arbitrary subsets and to incrementally derive an N-way superimposition from partial superimpositions. We apply our tool implementation of SiMPOSE to a selection of realistic C programs, frequently considered for experimental evaluation of program-analysis techniques. In particular, we investigate applicability and efficiency/effectiveness trade-offs of our approach by applying SiMPOSE in the context of family-based unit-test generation as well as model-checking as sample program-analysis techniques. Our experimental results reveal very impressive efficiency improvements by an average factor of up to 2.6 for test-generation and up to 2.4 for model-checking under stable effectiveness, as compared to variant-by-variant approaches, thus amortizing the additional effort required for merging. In addition, our results show that merging all N variants at once produces, in almost all cases, clearly more precise results than incremental step-wise 2-way merging. Finally, our comparison with major existing N-way merging techniques shows that SiMPOSE constitutes, in most cases, the best efficiency/effectiveness trade-off.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {13},
numpages = {59},
keywords = {Program merging, control flow automata, model matching, quality assurance, variability encoding}
}

@inproceedings{10.1145/2884781.2884821,
author = {Devroey, Xavier and Perrouin, Gilles and Papadakis, Mike and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Featured model-based mutation analysis},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884821},
doi = {10.1145/2884781.2884821},
abstract = {Model-based mutation analysis is a powerful but expensive testing technique. We tackle its high computation cost by proposing an optimization technique that drastically speeds up the mutant execution process. Central to this approach is the Featured Mutant Model, a modelling framework for mutation analysis inspired by the software product line paradigm. It uses behavioural variability models, viz., Featured Transition Systems, which enable the optimized generation, configuration and execution of mutants. We provide results, based on models with thousands of transitions, suggesting that our technique is fast and scalable. We found that it outperforms previous approaches by several orders of magnitude and that it makes higher-order mutation practically applicable.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {655–666},
numpages = {12},
keywords = {featured transition systems, mutation analysis, variability},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/1809223.1809224,
author = {Kitchenham, Barbara and Brereton, Pearl},
title = {Problems adopting metrics from other disciplines},
year = {2010},
isbn = {9781605589763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1809223.1809224},
doi = {10.1145/1809223.1809224},
abstract = {In this paper, we argue that metrics validation approaches used in software engineering are problematic. In particular, theoretical validation is not rigorous enough to detect invalid metrics and empirical validation has no mechanism for making any final decisions about the validity of metrics. In addition, we argue that cohesion and information-theoretic metrics are problematic if they are based on mathematical graphs which do not consider program semantics. We conclude that we should not adopt metrics from other disciplines if we cannot validate them properly. We propose the use of the representation condition as a means to demonstrate metrics that are not valid. We also believe that design metrics must make sense to software designers or, even if they are valid, they will not be used.},
booktitle = {Proceedings of the 2010 ICSE Workshop on Emerging Trends in Software Metrics},
pages = {1–7},
numpages = {7},
keywords = {cohesion metrics, entropy metrics, information-theoretic metrics, metrics validation, representation conditions, software engineering metrics},
location = {Cape Town, South Africa},
series = {WETSoM '10}
}

@inproceedings{10.1145/1868688.1868690,
author = {Siegmund, Norbert and Rosenm\"{u}ller, Marko and Apel, Sven},
title = {Automating energy optimization with features},
year = {2010},
isbn = {9781450302081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868688.1868690},
doi = {10.1145/1868688.1868690},
abstract = {Mobile devices such as cell phones and notebooks rely on battery power supply. For these systems, optimizing the power consumption is important to increase the system's lifetime. However, this is hard to achieve because energy-saving functions often depend on the hardware, and operating systems. The diversity of hardware components and operating systems makes the implementation time consuming and difficult. We propose an approach to automate energy optimization of programs by implementing energy-saving functionality as modular, separate implementation units (e.g., feature modules or aspects). These units are bundled as energy features into an energy-optimization feature library. Based on aspect-oriented and feature-oriented programming, we discuss different techniques to compose the source code of a client program and the implementation units of the energy features.},
booktitle = {Proceedings of the 2nd International Workshop on Feature-Oriented Software Development},
pages = {2–9},
numpages = {8},
keywords = {energy consumption, feature-oriented programming, software product lines},
location = {Eindhoven, The Netherlands},
series = {FOSD '10}
}

@inproceedings{10.5555/2667025.2667027,
author = {Siegmund, Norbert and Mory, Maik and Feigenspan, Janet and Saake, Gunter and Nykolaychuk, Mykhaylo and Schumann, Marco},
title = {Interoperability of non-functional requirements in complex systems},
year = {2012},
isbn = {9781467318532},
publisher = {IEEE Press},
abstract = {Heterogeneity of embedded systems leads to the development of variable software, such as software product lines. From such a family of programs, stakeholders select the specific variant that satisfies their functional requirements. However, different functionality exposes different non-functional properties of these variants. Especially in the embedded-system domain, non-functional requirements are vital, because resources are scarce. Hence, when selecting an appropriate variant, we have to fulfill also non-functional requirements. Since more systems are interconnected, the challenge is to find a variant that additionally satisfies global nonfunctional (or quality) requirements. In this paper, we advert the problem of achieving interoperability of non-functional requirements among multiple interacting systems using a real-world scenario. Furthermore, we show an approach to find optimal variants for multiple systems that reduces computation effort by means of a stepwise configuration process.},
booktitle = {Proceedings of the Second International Workshop on Software Engineering for Embedded Systems},
pages = {2–8},
numpages = {7},
location = {Zurich, Switzerland},
series = {SEES '12}
}

@inproceedings{10.1145/2024587.2024596,
author = {Yamamuro, Masae and Tanitsu, Yukio and Komiyama, Toshihiro and Azuma, Motoei},
title = {Introduction of Japan's investigation activities on systems and software product quality metrics},
year = {2011},
isbn = {9781450308519},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2024587.2024596},
doi = {10.1145/2024587.2024596},
abstract = {In order to realize an environment where safe and secure system/software products can be used in daily life and social economic activities, it is required to visualize the quality of the product, evaluate whether it meets the user's needs objectively and achieve quality target. So, the Ministry of Economy, Trade and Industry, Japan (METI), Advanced Research Project on Software Metrics - Product Quality Metrics Working Group (WG) has worked on establishing metrics (the term "metric" is used to refer to base measures, derived measures and indexes as a batch. In this paper, metric is used to describe the measured volume.) that can be used commonly for these activities and summarized the contents in order to promote quality assurance activity. First of all the WG organized discussions related to metrics in order to clarify the quality of various system/software products existing in Japan, and aggregated characteristics of metrics and mutual relationship.Then, for the objective of establishing the quality of system/software products meeting the users' needs and to select metrics that can be used commonly for establishment, the in-depth research was performed and investigation was held for basic activities from quality requirement definition to quality evaluation and metrics recommended for use for each quality characteristic. This paper described the pertinent contents.},
booktitle = {Proceedings of the 8th International Workshop on Software Quality},
pages = {34–41},
numpages = {8},
keywords = {quality characteristics, quality metrics (measures), quality model, square},
location = {Szeged, Hungary},
series = {WoSQ '11}
}

@inproceedings{10.1145/1868433.1868445,
author = {Trujillo, Salvador and Perez, Antonio and Gonzalez, David and Hamid, Brahim},
title = {Towards the integration of advanced engineering paradigms into RCES: raising the issues for the safety-critical model-driven product-line case},
year = {2010},
isbn = {9781450303682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868433.1868445},
doi = {10.1145/1868433.1868445},
abstract = {The conception and design of Resource Constrained Embedded Systems is an inherently complex endeavor. In particular, non-functional requirements from security, dependability and variability are exacerbating this complexity. Recent times have seen a paradigm shift in terms of design through the combination of multiple software engineering paradigms together, namely, Model Driven Engineering and Software Product Line Engineering. Such paradigm shift is changing the way systems are developed nowadays, reducing development time significantly. Embedded systems are a case in point where a range of products for assorted domains such as energy, transportation, automotive, and so on are conceived as a family. However, most of the work so far has been focused on functional parts. The purpose of this talk is to foster some discussion during the workshop on the issues that need to be faced for these techniques to be applicable for Resource Constrained Embedded Systems for which security and dependability are primary requirements.},
booktitle = {Proceedings of the International Workshop on Security and Dependability for Resource Constrained Embedded Systems},
articleno = {9},
numpages = {4},
keywords = {dependability, model-driven development, resource constrained embedded systems, software product lines},
location = {Vienna, Austria},
series = {S&amp;D4RCES '10}
}

@inproceedings{10.1145/1145319.1145337,
author = {Phadke, Amit A. and Allen, Edward B.},
title = {Predicting risky modules in open-source software for high-performance computing},
year = {2005},
isbn = {1595931171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1145319.1145337},
doi = {10.1145/1145319.1145337},
abstract = {This paper presents the position that software-quality modeling of open-source software for high-performance computing can identify modules that have a high risk of bugs.Given the source code for a recent release, a model can predict which modules are likely to have bugs, based on data from past releases. If a user knows which software modules correspond to functionality of interest, then risks to operations become apparent. If the risks are too great, the user may prefer not to upgrade to the most recent release.Of course, such predictions are never perfect. After release, bugs are discovered. Some bugs are missed by the model, and some predicted errors do not occur. A successful model will be accurate enough for informed management action at the time of the predictions.As evidence for this position, this paper summarizes a case study of the Portable Extensible Toolkit for Scientific Computation (PETSC), which is a mathematical library for high-performance computing. Data was drawn from source-code and configuration management logs. The accuracy of logistic-regression and decision-tree models indicated that the methodology is promising. The case study also illustrated several modeling issues.},
booktitle = {Proceedings of the Second International Workshop on Software Engineering for High Performance Computing System Applications},
pages = {60–64},
numpages = {5},
keywords = {C4.5, PETSc, decision trees, empirical case study, high performance computing, logistic regression, open-source software, software metrics, software quality model, software reliability},
location = {St. Louis, Missouri},
series = {SE-HPCS '05}
}

@inproceedings{10.1145/1629716.1629738,
author = {Alf\'{e}rez, Mauricio and Moreira, Ana and Kulesza, Uir\'{a} and Ara\'{u}jo, Jo\~{a}o and Mateus, Ricardo and Amaral, Vasco},
title = {Detecting feature interactions in SPL requirements analysis models},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629738},
doi = {10.1145/1629716.1629738},
abstract = {The consequences of unwanted feature interactions in a Software Product Line (SPL) can range from minor problems to critical software failures. However, detecting feature interactions in reasonably complex model-based SPLs is a non-trivial task. This is due to the often large number of interdependent models that describe the SPL features and the lack of support for analyzing the relationships inside those models. We believe that the early detection of the points, where two or more features interact --- based on the models that describe the behavior of the features ---, is a starting point for the detection of conflicts and inconsistencies between features, and therefore, take an early corrective action.This vision paper foresees a process to find an initial set of points where it is likely to find potential feature interactions in model-based SPL requirements, by detecting: (i) dependency patterns between features using use case models; and (ii) overlapping between use case scenarios modeled using activity models.We focus on requirements models, which are special, since they do not contain many details about the structural components and the interactions between the higher-level abstraction modules of the system. Therefore, use cases and activity models are the means that help us to analyze the functionality of a complex system looking at it from a high level end-user view to anticipate the places where there are potential feature interactions. We illustrate the approach with a home automation SPL and then discuss about its applicability.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {117–123},
numpages = {7},
keywords = {feature interactions, software product lines requirements},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@inproceedings{10.1145/1629716.1629735,
author = {Asadi, Mohsen and Mohabbati, Bardia and Kaviani, Nima and Ga\v{s}evi\'{c}, Dragan and Bo\v{s}kovi\'{c}, Marko and Hatala, Marek},
title = {Model-driven development of families of Service-Oriented Architectures},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629735},
doi = {10.1145/1629716.1629735},
abstract = {The paradigms of Service Oriented Architecture (SOA) and Software Product Line Engineering (SPLE) facilitate the development of families of software-intensive products. Software Product Line practices can be leveraged to support the development of service-oriented applications to promote the reusability of assets throughout the iterative and incremental development of software product families. Such an approach enables various service oriented business processes and software products of the same family to be systematically created and integrated. In this paper, we advocate integration of software product line engineering with model driven engineering to enable a model driven specification of software services, capable of creating software products from a family of software services. Using the proposed method, we aim to provide a consistent view of a composed software system from a higher business administration perspective to lower levels of service implementation and deployment. We demonstrate how Model Driven Engineering (MDE) can help with injecting the set of required commonalities and variabilities of a software product from a high level business process design to the lower levels of service use.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {95–102},
numpages = {8},
keywords = {business process management, semantic web, service-oriented architectures, software product lines},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@inproceedings{10.1145/1809223.1809231,
author = {Burrows, Rachel and Ferrari, Fabiano Cutigi and Garcia, Alessandro and Ta\"{\i}ani, Fran\c{c}ois},
title = {An empirical evaluation of coupling metrics on aspect-oriented programs},
year = {2010},
isbn = {9781605589763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1809223.1809231},
doi = {10.1145/1809223.1809231},
abstract = {Coupling metrics received increased recognition by object-oriented (OO) software developers when they were found to be indicators of important quality attributes, such as fault-proneness. However, there is no consensus on which coupling metrics are effective quality indicators for emerging development paradigms, such as Aspect-Oriented Programming (AOP). AOP aims to improve software quality by providing significantly different decomposition mechanisms, such as pointcut, advice and intertype declarations. Therefore, it is not obvious if quality indicators for AOP can be derived from direct extensions of classical OO metrics. However, empirical studies of AOP do often rely on classical coupling metrics. Despite the recent adoption of AOP in industrial projects, coupling metrics have been rarely evaluated as useful indicators of fault-proneness in this context. This paper analyses the effectiveness of coupling metrics as indicators of fault-proneness in aspect-oriented (AO) systems. We collected faults from several releases of a real-world AO system. We applied and compared existing metrics for coupling and other internal attributes. We have also considered a novel metric that quantifies specific dependencies in AO software not captured by existing metrics. The results show that coupling metrics, which are not directives of object-oriented metrics, tended to be superior indicators of fault-proneness.},
booktitle = {Proceedings of the 2010 ICSE Workshop on Emerging Trends in Software Metrics},
pages = {53–58},
numpages = {6},
keywords = {aspect-oriented software, fault-proneness, metrics},
location = {Cape Town, South Africa},
series = {WETSoM '10}
}

@inproceedings{10.1145/3377930.3390215,
author = {Silva, Diego Fernandes da and Okada, Luiz Fernando and Colanzi, Thelma Elita and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Enhancing search-based product line design with crossover operators},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3390215},
doi = {10.1145/3377930.3390215},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA designing has been formulated as a multi-objective optimization problem and successfully solved by a state-of-the-art search-based approach. However, the majority of empirical studies optimize PLA designs without applying one of the fundamental genetic operators: the crossover. An operator for PLA design, named Feature-driven Crossover, was proposed in a previous study. In spite of the promising results, this operator occasionally generated incomplete solutions. To overcome these limitations, this paper aims to enhance the search-based PLA design optimization by improving the Feature-driven Crossover and introducing a novel crossover operator specific for PLA design. The proposed operators were evaluated in two well-studied PLA designs, using three experimental configurations of NSGA-II in comparison with a baseline that uses only mutation operators. Empirical results show the usefulness and efficiency of the presented operators on reaching consistent solutions. We also observed that the two operators complement each other, leading to PLA design solutions with better feature modularization than the baseline experiment.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {1250–1258},
numpages = {9},
keywords = {multi-objective evolutionary algorithm, recombination operators, software architecture, software product line},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1145/3382025.3414942,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Kr\"{u}ger, Jacob and Mendon\c{c}a, Willian D. F.},
title = {Variability management meets microservices: six challenges of re-engineering microservice-based webshops},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414942},
doi = {10.1145/3382025.3414942},
abstract = {A microservice implements a small unit of functionality that it provides through a network using lightweight protocols. So, microservices can be combined to fulfill tasks and implement features of a larger software system---resembling a variability mechanism in the context of a software product line (SPL). Microservices and SPLs have similar goals, namely facilitating reuse and customizing, but they are usually employed in different contexts. Any developer who has access to the network can provide a microservice for any task, while SPLs are usually intended to implement features of a specific domain. Due to their different concepts, using microservices to implement an SPL or adopting SPL practices (e.g., variability management) for microservices is a challenging cross-area research problem. However, both techniques can complement each other, and thus tackling this problem promises benefits for organizations that employ either technique. In this paper, we reason on the importance of advancing in this direction, and sketch six concrete challenges to initiate research, namely (1) feature identification, (2) variability modeling, (3) variable microservice architectures, (4) interchangeability, (5) deep customization, and (6) re-engineering an SPL. We intend these challenges to serve as a starting point for future research in this cross-area research direction---avoiding that the concepts of one area are reinvented in the other.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {22},
numpages = {6},
keywords = {cloud computing, microservices, re-engineering, software product line, variability management},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/1294325.1294333,
author = {M\"{a}kel\"{a}, Sami and Lepp\"{a}nen, Ville},
title = {A software metric for coherence of class roles in Java programs},
year = {2007},
isbn = {9781595936721},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1294325.1294333},
doi = {10.1145/1294325.1294333},
abstract = {The purpose of software metrics is to measure the quality of programs. The results can be for example used to predict maintenance costs or improve code quality. An emerging view is that if software metrics are going to be used to improve quality, they must help in finding code that should be refactored. Often refactoring or applying a design pattern is related to the role of the class to be refactored. In client-based metrics, a project gives the class a context. These metrics measure, how a class is used by other classes in the context. We present a new client-based metric LCIC (Lack of Coherence in Clients), which analyses, if the class being measured has a coherent set of roles in the program. Interfaces represent the roles of classes. If a class does not have a coherent set of roles, it should be refactored, or a new interface should be defined for the class.We have implemented a tool for measuring the metric LCIC for Java projects in the Eclipse environment. We calculated LCIC values for classes of several open source projects. We compare these results with results of other related metrics, and inspect the measured classes to find out, what kind of refactorings are needed. We also analyse the relation of different design patterns and refactorings to our metric. Our experiments reveal the usefulness of client-based metrics to improve the quality of code.},
booktitle = {Proceedings of the 5th International Symposium on Principles and Practice of Programming in Java},
pages = {51–60},
numpages = {10},
keywords = {Java, cohesion, design patterns, metrics, refactoring},
location = {Lisboa, Portugal},
series = {PPPJ '07}
}

@inproceedings{10.1145/2791060.2791082,
author = {Hotz, Lothar and Wang, Yibo and Riebisch, Matthias and G\"{o}tz, Olaf and Lackhove, Josef},
title = {Evaluation across multiple views for variable automation systems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791082},
doi = {10.1145/2791060.2791082},
abstract = {Automation systems in industry are often software-intensive systems consisting of software and hardware components. During their development several engineers of different disciplines are involved, such as mechanical, electrical and software engineering. Each engineer focuses on specific system aspects to be developed. To enable an efficient development, product lines especially with feature models for variability modeling are promising technologies. In order to reduce the complexity of both feature models and development process, views on feature models can be applied. The use of views for filtering purposes constitutes an established method. However, views also enable further options missing in current approaches, such as evaluations regarding requirements, including non-functional ones. This paper presents an approach for evaluation across multiple views to enable collaborative development for developers who focus on different system aspects. We validate our approach by applying it in an industrial project for the planning of flying saws.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {311–315},
numpages = {5},
keywords = {automation systems, configuration, consistency check, feature model, multi-criteria evaluation, product lines},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/1385486.1385488,
author = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Schirmeier, Horst and Sincero, Julio and Apel, Sven and Leich, Thomas and Spinczyk, Olaf and Saake, Gunter},
title = {FAME-DBMS: tailor-made data management solutions for embedded systems},
year = {2008},
isbn = {9781595939647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1385486.1385488},
doi = {10.1145/1385486.1385488},
abstract = {Data management functionality is not only needed in large-scale server systems, but also in embedded systems. Resource restrictions and heterogeneity of hardware, however, complicate the development of data management solutions for those systems. In current practice, this typically leads to the redevelopment of data management because existing solutions cannot be reused and adapted appropriately. In this paper, we present our ongoing work on FAME-DBMS, a research project that explores techniques to implement highly customizable data management solutions, and illustrate how such systems can be created with a software product line approach. With this approach a concrete instance of a DBMS is derived by composing features of the DBMS product line that are needed for a specific application scenario. This product derivation process is getting complex if a large number of features is available. Furthermore, in embedded systems also non-functional properties, e.g., memory consumption, have to be considered when creating a DBMS instance. To simplify the derivation process we present approaches for its automation.},
booktitle = {Proceedings of the 2008 EDBT Workshop on Software Engineering for Tailor-Made Data Management},
pages = {1–6},
numpages = {6},
location = {Nantes, France},
series = {SETMDM '08}
}

@inproceedings{10.1145/3425269.3425278,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Supporting user preferences in search-based product line architecture design using Machine Learning},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425278},
doi = {10.1145/3425269.3425278},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA design requires intensive human effort as it involves several conflicting factors. In order to support this task, an interactive search-based approach, automated by a tool named OPLA-Tool, was proposed in a previous work. Through this tool the software architect evaluates the generated solutions during the optimization process. Considering that evaluating PLA is a complex task and search-based algorithms demand a high number of generations, the evaluation of all solutions in all generations cause human fatigue. In this work, we incorporated in OPLA-Tool a Machine Learning (ML) model to represent the architect in some moments during the optimization process aiming to decrease the architect's effort. Through the execution of a quantiqualitative exploratory study it was possible to demonstrate the reduction of the fatigue problem and that the solutions produced at the end of the process, in most cases, met the architect's needs.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Human-computer interaction, Machine Learning, Product Line Architecture},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1145/2652524.2652528,
author = {Caneill, Matthieu and Zacchiroli, Stefano},
title = {Debsources: live and historical views on macro-level software evolution},
year = {2014},
isbn = {9781450327749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2652524.2652528},
doi = {10.1145/2652524.2652528},
abstract = {Context. Software evolution has been an active field of research in recent years, but studies on macro-level software evolution---i.e., on the evolution of large software collections over many years---are scarce, despite the increasing popularity of intermediate vendors as a way to deliver software to final users.Goal. We want to ease the study of both day-by-day and long-term Free and Open Source Software (FOSS) evolution trends at the macro-level, focusing on the Debian distribution as a proxy of relevant FOSS projects.Method. We have built Debsources, a software platform to gather, search, and publish on the Web all the source code of Debian and measures about it. We have set up a public Debsources instance at http://sources.debian.net, integrated it into the Debian infrastructure to receive live updates of new package releases, and written plugins to compute popular source code metrics. We have injected all current and historical Debian releases into it.Results. The obtained dataset and Web portal provide both long term-views over the past 20 years of FOSS evolution and live insights on what is happening at sub-day granularity. By writing simple plugins (~100 lines of Python each) and adding them to our Debsources instance we have been able to easily replicate and extend past empirical analyses on metrics as diverse as lines of code, number of packages, and rate of change---and make them perennial. We have obtained slightly different results than our reference study, but confirmed the general trends and updated them in light of 7 extra years of evolution history.Conclusions. Debsources is a flexible platform to monitor large FOSS collections over long periods of time. Its main instance and dataset are valuable resources for scholars interested in macro-level software evolution.},
booktitle = {Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {28},
numpages = {10},
keywords = {debian, free software, open source, software evolution, source code},
location = {Torino, Italy},
series = {ESEM '14}
}

@article{10.1145/857076.857090,
author = {Purao, Sandeep and Vaishnavi, Vijay},
title = {Product metrics for object-oriented systems},
year = {2003},
issue_date = {June 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/857076.857090},
doi = {10.1145/857076.857090},
abstract = {We survey metrics proposed for object-oriented systems, focusing on product metrics. The survey is intended for the purposes of understanding, classifying, and analyzing ongoing research in object-oriented metrics. The survey applies fundamental measurement theory to artifacts created by development activities. We develop a mathematical formalism that captures this perspective clearly, giving appropriate attention to the peculiarities of the object-oriented system developmenr process. Consistent representation of the available metrics, following this mathematical formalism, shows that current research in this area contains varying coverage of different products and their properties at different development stages. The consistent representation also facilitates several analyses including aggregation across metrics, usage across metrics, equivalent formulation of metrics by multiple researchers, and exploitation of traditional metrics for object-oriented metrics. We also trace the chronological development of research in this area, and uncover gaps that suggest opportunities for future research.},
journal = {ACM Comput. Surv.},
month = jun,
pages = {191–221},
numpages = {31},
keywords = {Software metrics, measurement theory, object-oriented metrics, object-oriented product metrics, object-oriented systems}
}

@inproceedings{10.1145/1852786.1852814,
author = {Robinson, Brian and Francis, Patrick},
title = {Improving industrial adoption of software engineering research: a comparison of open and closed source software},
year = {2010},
isbn = {9781450300391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1852786.1852814},
doi = {10.1145/1852786.1852814},
abstract = {Advances in software engineering have led to the creation of many new software engineering techniques. However, industrial adoption of these techniques is often quite low, as development organizations are skeptical of their value and applicability. Empirical studies are commonly used to show this value to potential adopters, with open source software used as an approximation of industrial applications. However, little data exists on the similarity of open source and industrial software. We present a large metrics-based study comparing the most commonly evaluated open source programs to a large set of industrial programs. Source metrics are calculated and compared between 24 open source and 21 industrial programs. The results identify open source programs that are most similar to industrial programs. Using these identified open source programs in empirical studies can provide the best generalization to industrial software.},
booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {21},
numpages = {10},
keywords = {empirical studies, industrial adoption, software comparison},
location = {Bolzano-Bozen, Italy},
series = {ESEM '10}
}

@inproceedings{10.1145/2660190.2660191,
author = {Kolesnikov, Sergiy and Roth, Judith and Apel, Sven},
title = {On the relation between internal and external feature interactions in feature-oriented product lines: a case study},
year = {2014},
isbn = {9781450329804},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660190.2660191},
doi = {10.1145/2660190.2660191},
abstract = {The feature-interaction problem has been explored for many years. Still, we lack sufficient knowledge about the interplay of different kinds of interactions in software product lines. Exploring the relations between different kinds of feature interactions will allow us to learn more about the nature of interactions and their causes. This knowledge can then be applied for improving existing approaches for detecting, managing, and resolving feature interactions. We present a framework for studying relations between different kinds of interactions. Furthermore, we report and discuss the results of a preliminary study in which we examined correlations between internal feature interactions (quantified by a set of software measures) and external feature interactions (represented by product-line-specific type errors). We performed the evaluation on a set of 15 feature-oriented, Java-based product lines. We observed moderate correlations between the interactions under discussion. This gives us confidence that we can apply our approach to studying other types of external feature interactions (e.g., performance interactions).},
booktitle = {Proceedings of the 6th International Workshop on Feature-Oriented Software Development},
pages = {1–8},
numpages = {8},
keywords = {feature interactions, feature-oriented software development, software measures},
location = {V\"{a}ster\r{a}s, Sweden},
series = {FOSD '14}
}

@inproceedings{10.1145/2568225.2568230,
author = {Akiki, Pierre A. and Bandara, Arosha K. and Yu, Yijun},
title = {Integrating adaptive user interface capabilities in enterprise applications},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568230},
doi = {10.1145/2568225.2568230},
abstract = {Many existing enterprise applications are at a mature stage in their development and are unable to easily benefit from the usability gains offered by adaptive user interfaces (UIs). Therefore, a method is needed for integrating adaptive UI capabilities into these systems without incurring a high cost or significantly disrupting the way they function. This paper presents a method for integrating adaptive UI behavior in enterprise applications based on CEDAR, a model-driven, service-oriented, and tool-supported architecture for devising adaptive enterprise application UIs. The proposed integration method is evaluated with a case study, which includes establishing and applying technical metrics to measure several of the method’s properties using the open-source enterprise application OFBiz as a test-case. The generality and flexibility of the integration method are also evaluated based on an interview and discussions with practitioners about their real-life projects.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {712–723},
numpages = {12},
keywords = {Adaptive user interfaces, enterprise systems, integration, model-driven engineering, software architectures, software metrics},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/2648511.2648513,
author = {Harman, M. and Jia, Y. and Krinke, J. and Langdon, W. B. and Petke, J. and Zhang, Y.},
title = {Search based software engineering for software product line engineering: a survey and directions for future work},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648513},
doi = {10.1145/2648511.2648513},
abstract = {This paper presents a survey of work on Search Based Software Engineering (SBSE) for Software Product Lines (SPLs). We have attempted to be comprehensive, in the sense that we have sought to include all papers that apply computational search techniques to problems in software product line engineering. Having surveyed the recent explosion in SBSE for SPL research activity, we highlight some directions for future work. We focus on suggestions for the development of recent advances in genetic improvement, showing how these might be exploited by SPL researchers and practitioners: Genetic improvement may grow new products with new functional and non-functional features and graft these into SPLs. It may also merge and parameterise multiple branches to cope with SPL branchmania.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {5–18},
numpages = {14},
keywords = {SBSE, SPL, genetic programming, program synthesis},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3646548.3676552,
author = {Malenfant, Jacques and Ziadi, Tewfik},
title = {Can Conditional Preferences and *CP-net Concepts Enhance Feature Models?},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676552},
doi = {10.1145/3646548.3676552},
abstract = {Variability in software systems is a key concept in mastering complexity. Most software systems exhibit functionalities that can either be implemented in several different ways or may be options that can be chosen late in their design, depending on the deployment conditions. Expressing these forms of variability attracted a lot of attention since several years, leading to a predominant expression media: Feature models. Feature models can express the mandatory/optional status of a feature, as well as alternative realizations of a feature, exclusive of each others or not. Though extensions of feature models have been proposed to express more properties, they are still limited in their ability to capture complex configuration problems, especially those involving extra functional properties. Lines of research have proposed extended feature models with non-functional attributes and constraint-solving approach to reason about them. Yet, these approaches appear too restrictive and inflexible to cater for really complex SPL. In this paper, we rather propose to extend feature models with conditional preferences concepts from the multi-criteria decision-making field, and more specifically on relationships introduced in the graphical modeling approach of the * CP-net family of models. We show how these new relationships enable the expression of complex configuration constraints, while ensuring that the feature models remain intuitive and user-friendly across various feature model analysis-based activities.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {66–74},
numpages = {9},
keywords = {conditional preferences, feature models, variability management},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/1287624.1287683,
author = {Mizuno, Osamu and Kikuno, Tohru},
title = {Training on errors experiment to detect fault-prone software modules by spam filter},
year = {2007},
isbn = {9781595938114},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1287624.1287683},
doi = {10.1145/1287624.1287683},
abstract = {The fault-prone module detection in source code is of importance for assurance of software quality. Most of previous fault-prone detection approaches are based on software metrics. Such approaches, however, have difficulties in collecting the metrics and constructing mathematical models based on the metrics. In order to mitigate such difficulties, we propose a novel approach for detecting fault-prone modules using a spam filtering technique, named Fault-Prone Filtering. Because of the increase of needs for spam e-mail detection, the spam filtering technique has been progressed as a convenient and effective technique for text mining. In our approach, fault-prone modules are detected in a way that the source code modules are considered as text files and are applied to the spam filter directly. This paper describes the training on errors procedure to apply fault-prone filtering in practice. Since no pre-training is required, this procedure can be applied to actual development field immediately. In order to show the usefulness of our approach, we conducted an experiment using a large source code repository of Java based open source project. The result of experiment shows that our approach can classify about 85% of software modules correctly. The result also indicates that fault-prone modules can be detected relatively low cost at an early stage.},
booktitle = {Proceedings of the the 6th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
pages = {405–414},
numpages = {10},
keywords = {fault-prone modules, spam filter, text mining},
location = {Dubrovnik, Croatia},
series = {ESEC-FSE '07}
}

@inproceedings{10.1145/1370152.1370163,
author = {Wenzel, Sven},
title = {Scalable visualization of model differences},
year = {2008},
isbn = {9781605580456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370152.1370163},
doi = {10.1145/1370152.1370163},
abstract = {If large models are compared their difference can contain a huge number of local changes. Conventional methods for displaying differences cannot reasonably handle such large differences. This paper proposes a solution to this problem. Our approach is based on the concept of polymetric views and extends it in two ways: firstly, we propose metrics for differences which quantify properties of differences and distinguish relevant from irrelevant changes. Moreover, we propose new graphical features of polymetric views. This combination provides a scalable presentation of differences which makes the changes of large models comprehensible.},
booktitle = {Proceedings of the 2008 International Workshop on Comparison and Versioning of Software Models},
pages = {41–46},
numpages = {6},
keywords = {difference computation, metrics, model-driven development},
location = {Leipzig, Germany},
series = {CVSM '08}
}

@inproceedings{10.5555/2093889.2093921,
author = {Bagheri, Ebrahim and Asadi, Mohsen and Ensan, Faezeh and Ga\v{s}evi\'{c}, Dragan and Mohabbati, Bardia},
title = {Bringing semantics to feature models with SAFMDL},
year = {2011},
publisher = {IBM Corp.},
address = {USA},
abstract = {Software product line engineering is a paradigm that advocates the reusability of software engineering assets and the rapid development of new applications for a target domain. These objectives are achieved by capturing the commonalities and variabilities between the applications of a target domain and through the development of comprehensive and variability-covering domain models. The domain models developed within the software product line development process need to cover all of the possible features and aspects of the target domain. In other words, the domain models often described using feature models should be elaborate representations of the feature space of that domain. In order to operationalize feature-based representations of a software application, appropriate implementation mechanisms need to be employed. In this paper, we propose a Semantic Web-oriented language, called Semantic Annotations for Feature Modeling Description Language (SAFMDL) that provides the means to semantically describe feature models. We will show that using SAFMDL along with Semantic Web Query techniques, we are able to bridge the gap between software product lines and SOA technology. Our proposed work allows software practitioners to use Semantic Web technology to quickly and rapidly develop new software products based on SOA technology from software product lines.},
booktitle = {Proceedings of the 2011 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {287–300},
numpages = {14},
location = {Toronto, Ontario, Canada},
series = {CASCON '11}
}

@inproceedings{10.1145/1852786.1852848,
author = {Ramler, Rudolf and Klammer, Claus and Natschl\"{a}ger, Thomas},
title = {The usual suspects: a case study on delivered defects per developer},
year = {2010},
isbn = {9781450300391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1852786.1852848},
doi = {10.1145/1852786.1852848},
abstract = {Individual differences of developers in performance and introduced defects have been reported by many research studies and are frequently observed in software development practice. Thus, when the source of defects in the final product is discussed, developers are usually the first under suspicion. However, defects residing in a released software product are the result of defects introduced throughout the sequence of development activities (e.g., specification, design, implementation, testing and stabilization) less the defects detected and removed in these activities. This case study explores and describes the difference between developers in terms of associated post-release (i.e., delivered) defects. The results are put in relation to the intensity with which a developer's changes and enhancements have been tested to identify a latent influence by pre-release quality assurance measures.},
booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {48},
numpages = {4},
keywords = {case study, defect introduction, defect removal, post-release defects},
location = {Bolzano-Bozen, Italy},
series = {ESEM '10}
}

@inproceedings{10.1145/2735386.2735389,
author = {Thimmegowda, Nishanth and Kienzle, J\"{o}rg},
title = {Visualization algorithms for feature models in concern-driven software development},
year = {2015},
isbn = {9781450332835},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2735386.2735389},
doi = {10.1145/2735386.2735389},
abstract = {Concern-Driven Development builds on the disciplines of model-driven engineering, software product lines and aspect-orientation to define broad units of reuse, so called concerns. The feature model of a concern plays a central role, since it describes the variants encapsulated by a concern, such as, alternative functionality or different design solutions. This paper argues that depending on who is working with the feature model, the encapsulated features and their relationships should be visualized differently: the concern designer is working within the concern module, and therefore needs to see the features of reused concerns that have been selected; the concern user is evaluating if a concern can be reused for a given purpose, and therefore wants to browse the choices that are available.},
booktitle = {Companion Proceedings of the 14th International Conference on Modularity},
pages = {39–42},
numpages = {4},
keywords = {concern-driven development, feature model visualization, reuse},
location = {Fort Collins, CO, USA},
series = {MODULARITY Companion 2015}
}

@inproceedings{10.1145/3377024.3377036,
author = {Sprey, Joshua and Sundermann, Chico and Krieter, Sebastian and Nieke, Michael and Mauro, Jacopo and Th\"{u}m, Thomas and Schaefer, Ina},
title = {SMT-based variability analyses in FeatureIDE},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377036},
doi = {10.1145/3377024.3377036},
abstract = {Handling configurable systems with thousands of configuration options is a challenging problem in research and industry. One of the most common approaches to manage the configuration options of large systems is variability modelling. The verification and configuration process of large variability models is manually infeasible. Hence, they are usually assisted by automated analyses based on solving satisfiability problems (SAT). Recent advances in satisfiability modulo theories (SMT) could prove SMT solvers as a viable alternative to SAT solvers. However, SMT solvers are typically not utilized for variability analyses. A comparison for SAT and SMT could help to estimate SMT solvers potential for the automated analysis. We integrated two SMT solvers into FeatureIDE and compared them against a SAT solver on analyses for feature models, configurations, and realization artifacts. We give an overview of all variability analyses in FeatureIDE and present the results of our empirical evaluation for over 122 systems. We observed that SMT solvers are generally faster in generating explanations of unsatisfiable requests. However, the evaluated SAT solver outperformed SMT solvers for other analyses.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {6},
numpages = {9},
keywords = {SAT, SAT analysis, SAT vs SMT, SMT, SMT analysis, attribute optimization, configuration analysis, feature attributes, feature model analysis, feature models, preprocessor analysis, variability analysis},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1145/1368088.1368214,
author = {Wenzel, Sven and Kelter, Udo},
title = {Analyzing model evolution},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1368088.1368214},
doi = {10.1145/1368088.1368214},
abstract = {Model-driven development leads to development processes in which a large number of different versions of models are produced. We present FAME, a tool environment which enables fine-grained analysis of the version history of a model. The tool is generic in the sense that it can work with various model types including UML and domain-specific languages.},
booktitle = {Proceedings of the 30th International Conference on Software Engineering},
pages = {831–834},
numpages = {4},
keywords = {history analysis, metrics, model-driven development, tracing},
location = {Leipzig, Germany},
series = {ICSE '08}
}

@inproceedings{10.1145/1414004.1414038,
author = {Layman, Lucas and Kudrjavets, Gunnar and Nagappan, Nachiappan},
title = {Iterative identification of fault-prone binaries using in-process metrics},
year = {2008},
isbn = {9781595939715},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1414004.1414038},
doi = {10.1145/1414004.1414038},
abstract = {Code churn, the amount of code change taking place within a software unit over time, has been correlated with fault-proneness in software systems. We investigate the use of code churn and static metrics collected at regular time intervals during the development cycle to predict faults in an iterative, in-process manner. We collected 159 churn and structure metrics from six, four-month snapshots of a 1 million LOC Microsoft product. The number of software faults fixed during each period is recorded per binary module. Using stepwise logistic regression, we create a prediction model to identify fault-prone binaries using three parameters: code churn (the number of new and changed blocks); class Fan In and class Fan Out (normalized by lines of code). The iteratively-built model is 80.0% accurate at predicting fault-prone and non-fault-prone binaries. These fault-prediction models have the advantage of allowing the engineers to observe how their fault-prediction profile evolves over time.},
booktitle = {Proceedings of the Second ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {206–212},
numpages = {7},
keywords = {code churn, fault prediction, regression, statistical models},
location = {Kaiserslautern, Germany},
series = {ESEM '08}
}

@inproceedings{10.1145/1566445.1566509,
author = {Wang, Ju An and Wang, Hao and Guo, Minzhe and Xia, Min},
title = {Security metrics for software systems},
year = {2009},
isbn = {9781605584218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1566445.1566509},
doi = {10.1145/1566445.1566509},
abstract = {Security metrics for software products provide quantitative measurement for the degree of trustworthiness for software systems. This paper proposes a new approach to define software security metrics based on vulnerabilities included in the software systems and their impacts on software quality. We use the Common Vulnerabilities and Exposures (CVE), an industry standard for vulnerability and exposure names, and the Common Vulnerability Scoring System (CVSS), a vulnerability scoring system designed to provide an open and standardized method for rating software vulnerabilities, in our metric definition and calculation. Examples are provided in the paper, which show that our definition of security metrics is consistent with the common practice and real-world experience about software quality in trustworthiness.},
booktitle = {Proceedings of the 47th Annual ACM Southeast Conference},
articleno = {47},
numpages = {6},
keywords = {security metrics, software quality, software security, software vulnerabilities},
location = {Clemson, South Carolina},
series = {ACMSE '09}
}

@inproceedings{10.1145/1868328.1868342,
author = {Jureczko, Marian and Madeyski, Lech},
title = {Towards identifying software project clusters with regard to defect prediction},
year = {2010},
isbn = {9781450304047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868328.1868342},
doi = {10.1145/1868328.1868342},
abstract = {Background: This paper describes an analysis that was conducted on newly collected repository with 92 versions of 38 proprietary, open-source and academic projects. A preliminary study perfomed before showed the need for a further in-depth analysis in order to identify project clusters.Aims: The goal of this research is to perform clustering on software projects in order to identify groups of software projects with similar characteristic from the defect prediction point of view. One defect prediction model should work well for all projects that belong to such group. The existence of those groups was investigated with statistical tests and by comparing the mean value of prediction efficiency.Method: Hierarchical and k-means clustering, as well as Kohonen's neural network was used to find groups of similar projects. The obtained clusters were investigated with the discriminant analysis. For each of the identified group a statistical analysis has been conducted in order to distinguish whether this group really exists. Two defect prediction models were created for each of the identified groups. The first one was based on the projects that belong to a given group, and the second one - on all the projects. Then, both models were applied to all versions of projects from the investigated group. If the predictions from the model based on projects that belong to the identified group are significantly better than the all-projects model (the mean values were compared and statistical tests were used), we conclude that the group really exists.Results: Six different clusters were identified and the existence of two of them was statistically proven: 1) cluster proprietary B -- T=19, p=0.035, r=0.40; 2) cluster proprietary/open - t(17)=3.18, p=0.05, r=0.59. The obtained effect sizes (r) represent large effects according to Cohen's benchmark, which is a substantial finding.Conclusions: The two identified clusters were described and compared with results obtained by other researchers. The results of this work makes next step towards defining formal methods of reuse defect prediction models by identifying groups of projects within which the same defect prediction model may be used. Furthermore, a method of clustering was suggested and applied.},
booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
articleno = {9},
numpages = {10},
keywords = {clustering, defect prediction, design metrics, size metrics},
location = {Timi\c{s}oara, Romania},
series = {PROMISE '10}
}

@inproceedings{10.1145/2642937.2642939,
author = {Segura, Sergio and S\'{a}nchez, Ana B. and Ruiz-Cort\'{e}s, Antonio},
title = {Automated variability analysis and testing of an E-commerce site.: an experience report},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2642939},
doi = {10.1145/2642937.2642939},
abstract = {In this paper, we report on our experience on the development of La Hilandera, an e-commerce site selling haberdashery products and craft supplies in Europe. The store has a huge input space where customers can place almost three millions of different orders which made testing an extremely difficult task. To address the challenge, we explored the applicability of some of the practices for variability management in software product lines. First, we used a feature model to represent the store input space which provided us with a variability view easy to understand, share and discuss with all the stakeholders. Second, we used techniques for the automated analysis of feature models for the detection and repair of inconsistent and missing configuration settings. Finally, we used test selection and prioritization techniques for the generation of a manageable and effective set of test cases. Our findings, summarized in a set of lessons learnt, suggest that variability techniques could successfully address many of the challenges found when developing e-commerce sites.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {139–150},
numpages = {12},
keywords = {automated testing, e-commerce, experience report, feature modelling, variability},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@inproceedings{10.5555/1273749.1273773,
author = {Choi, Kelvin H. T. and Tempero, Ewan},
title = {Dynamic measurement of polymorphism},
year = {2007},
isbn = {1920682430},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {Measuring "reuse" and "reusability" is difficult because there are so many different facets to these concepts. Before we can effectively measure reuse and reusability, we must first be able to effectively measure these different facets. One such facet is the programming language constructs that are available. For example whether or not a language supports polymorphism is believed to affect how reusable a developer can make a code artifact. Effectively measuring polymorphism is a challenge because its behaviour is only observable at run-time. In this paper, we present a metric for polymorphism based on the dynamic behaviour of the code. We evaluate the usefulness of the metric through two case studies.},
booktitle = {Proceedings of the Thirtieth Australasian Conference on Computer Science - Volume 62},
pages = {211–220},
numpages = {10},
keywords = {dynamic profiling, inheritance, polymorphism, software metrics},
location = {Ballarat, Victoria, Australia},
series = {ACSC '07}
}

@inproceedings{10.1145/1456362.1456370,
author = {Gegick, Michael and Williams, Laurie and Osborne, Jason and Vouk, Mladen},
title = {Prioritizing software security fortification throughcode-level metrics},
year = {2008},
isbn = {9781605583211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1456362.1456370},
doi = {10.1145/1456362.1456370},
abstract = {Limited resources preclude software engineers from finding and fixing all vulnerabilities in a software system. We create predictive models to identify which components are likely to have the most security risk. Software engineers can use these models to make measurement-based risk management decisions and to prioritize software security fortification efforts, such as redesign and additional inspection and testing. We mined and analyzed data from a large commercial telecommunications software system containing over one million lines of code that had been deployed to the field for two years. Using recursive partitioning, we built attack-prone prediction models with the following code-level metrics: static analysis tool alert density, code churn, and count of source lines of code. One model identified 100% of the attack-prone components (40% of the total number of components) with an 8% false positive rate. As such, the model could be used to prioritize fortification efforts in the system.},
booktitle = {Proceedings of the 4th ACM Workshop on Quality of Protection},
pages = {31–38},
numpages = {8},
keywords = {attack-prone, vulnerability-prone},
location = {Alexandria, Virginia, USA},
series = {QoP '08}
}

@inproceedings{10.1145/2499777.2500719,
author = {Schr\"{o}ter, Reimar and Siegmund, Norbert and Th\"{u}m, Thomas},
title = {Towards modular analysis of multi product lines},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500719},
doi = {10.1145/2499777.2500719},
abstract = {Software product-line engineering enables efficient development of tailor-made software by means of reusable artifacts. As practitioners increasingly develop software systems as product lines, there is a growing potential to reuse product lines in other product lines, which we refer to as multi product line. We identify challenges when developing multi product lines and propose interfaces for different levels of abstraction ranging from variability modeling to functional and non-functional properties. We argue that these interfaces ease the reuse of product lines and identify research questions that need to be solved toward modular analysis of multi product lines.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {96–99},
numpages = {4},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/1985793.1985859,
author = {Kim, Sunghun and Zhang, Hongyu and Wu, Rongxin and Gong, Liang},
title = {Dealing with noise in defect prediction},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1985859},
doi = {10.1145/1985793.1985859},
abstract = {Many software defect prediction models have been built using historical defect data obtained by mining software repositories (MSR). Recent studies have discovered that data so collected contain noises because current defect collection practices are based on optional bug fix keywords or bug report links in change logs. Automatically collected defect data based on the change logs could include noises.This paper proposes approaches to deal with the noise in defect data. First, we measure the impact of noise on defect prediction models and provide guidelines for acceptable noise level. We measure noise resistant ability of two well-known defect prediction algorithms and find that in general, for large defect datasets, adding FP (false positive) or FN (false negative) noises alone does not lead to substantial performance differences. However, the prediction performance decreases significantly when the dataset contains 20%-35% of both FP and FN noises. Second, we propose a noise detection and elimination algorithm to address this problem. Our empirical study shows that our algorithm can identify noisy instances with reasonable accuracy. In addition, after eliminating the noises using our algorithm, defect prediction accuracy is improved.},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {481–490},
numpages = {10},
keywords = {buggy changes, buggy files, data quality, defect prediction, noise resistance},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@article{10.1145/1363102.1363106,
author = {Gencel, Cigdem and Demirors, Onur},
title = {Functional size measurement revisited},
year = {2008},
issue_date = {June 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/1363102.1363106},
doi = {10.1145/1363102.1363106},
abstract = {There are various approaches to software size measurement. Among these, the metrics and methods based on measuring the functionality attribute have become widely used since the original method was introduced in 1979. Although functional size measurement methods have gone a long way, they still provide challenges for software managers. This article identifies improvement opportunities based on empirical studies we performed on ongoing projects. We also compare our findings with the extended dataset provided by the International Software Benchmarking Standards Group (ISBSG).},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {15},
numpages = {36},
keywords = {COSMIC FFP, Functional size measurement, MkII FPA, software benchmarking, software estimation}
}

@article{10.1145/3715111,
author = {Abrah\~{a}o, Silvia and Grundy, John and Pezz\`{e}, Mauro and Storey, Margaret-Anne and Andrew Tamburri, Damian},
title = {Software Engineering by and for Humans in an AI Era},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715111},
doi = {10.1145/3715111},
abstract = {The landscape of software engineering is undergoing a transformative shift driven by advancements in machine learning, artificial intelligence (AI), and autonomous systems. This roadmap paper explores how these technologies are reshaping the field, positioning humans not only as end users but also as critical components within expansive software ecosystems. We examine the challenges and opportunities arising from this human-centered paradigm, including ethical considerations, fairness, and the intricate interplay between technical and human factors. By recognizing humans at the heart of the software lifecycle —spanning professional engineers, end users, and end-user developers —we emphasize the importance of inclusivity, human-aligned workflows, and the seamless integration of AI-augmented socio-technical systems. As software systems evolve to become more intelligent and human-centric, software engineering practices must adapt to this new reality. This paper provides a comprehensive examination of this transformation, outlining current trends, key challenges, and opportunities that define the emerging research and practice landscape, and envisioning a future where software engineering and AI work synergistically to place humans at the core of the ecosystem.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb
}

@inproceedings{10.1145/1852786.1852813,
author = {Mandi\'{c}, Vladimir and Basili, Victor and Harjumaa, Lasse and Oivo, Markku and Markkula, Jouni},
title = {Utilizing GQM+Strategies for business value analysis: an approach for evaluating business goals},
year = {2010},
isbn = {9781450300391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1852786.1852813},
doi = {10.1145/1852786.1852813},
abstract = {Business value analysis (BVA) quantifies the factors that provide value and cost to an organization. It aims at capturing value, controlling risks, and capitalizing on opportunities. GQM+Strategies is an approach designed to aid in the definition and alignment of business goals, strategies, and an integrated measurement program at all levels in the organization. In this paper we describe how to perform business value analysis (BVA) using the GQM+Strategies approach. The integration of these two approaches provides a coupling of cost-benefit and risk analysis (value goals) with operationally measurable business goals and supports the evaluation of business goal success and the effectiveness of the chosen strategies. An application of the combined approach is provided to illustrate the feasibility of the proposed method. It deals with the business goal of modernizing the product for the evolving market.},
booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {20},
numpages = {10},
keywords = {GQM+Strategies, benefits realization, business value analysis, software metrics, value goals, value-based software engineering},
location = {Bolzano-Bozen, Italy},
series = {ESEM '10}
}

@inproceedings{10.1145/2556624.2556628,
author = {Lengauer, Philipp and Bitto, Verena and Angerer, Florian and Gr\"{u}nbacher, Paul and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {Where has all my memory gone? determining memory characteristics of product variants using virtual-machine-level monitoring},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556628},
doi = {10.1145/2556624.2556628},
abstract = {Non-functional properties such as memory footprint have recently gained importance in software product line research. However, determining the memory characteristics of individual features and product variants is extremely challenging. We present an approach that supports the monitoring of memory characteristics of individual features at the level of Java virtual machines. Our approach provides extensions to Java virtual machines to track memory allocations and deal-locations of individual features based on a feature-to-code mapping. The approach enables continuous monitoring at the level of features to detect anomalies such as memory leaks, excessive memory consumption, or abnormal garbage collection times in product variants. We provide an evaluation of our approach based on different product variants of the DesktopSearcher product line. Our experiment with different program inputs demonstrates the feasibility of our technique.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {13},
numpages = {8},
keywords = {Java, feature-oriented software development, memory footprint, monitoring},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@inproceedings{10.1145/1294904.1294909,
author = {Grammel, Lars and Schackmann, Holger and Lichter, Horst},
title = {BugzillaMetrics: an adaptable tool for evaluating metric specifications on change requests},
year = {2018},
isbn = {9781595937223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1294904.1294909},
doi = {10.1145/1294904.1294909},
abstract = {To manage the evolution of software processes and products, it is essential to evaluate their current state and how it evolved. This information can be obtained by analyzing the data available in change request management (CRM) systems like Bugzilla.Metrics and charts on change requests are already available in current CRM systems. They provide information about common metrics, but their adaptability is limited. This paper describes a more flexible approach for the evaluation of metrics on change requests.The main characteristics of the tool presented in this paper are the separation between metric specification and data retrieval, an event driven algorithm that calculates time series data, and an abstraction of its data sources.},
booktitle = {Ninth International Workshop on Principles of Software Evolution: In Conjunction with the 6th ESEC/FSE Joint Meeting},
pages = {35–38},
numpages = {4},
keywords = {change request management, evaluation of change requests, metrics specification, process metrics, software measurement},
location = {Dubrovnik, Croatia},
series = {IWPSE '07}
}

@inproceedings{10.1145/1294948.1294909,
author = {Grammel, Lars and Schackmann, Holger and Lichter, Horst},
title = {BugzillaMetrics: an adaptable tool for evaluating metric specifications on change requests},
year = {2007},
isbn = {9781595937223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1294948.1294909},
doi = {10.1145/1294948.1294909},
abstract = {To manage the evolution of software processes and products, it is essential to evaluate their current state and how it evolved. This information can be obtained by analyzing the data available in change request management (CRM) systems like Bugzilla.Metrics and charts on change requests are already available in current CRM systems. They provide information about common metrics, but their adaptability is limited. This paper describes a more flexible approach for the evaluation of metrics on change requests.The main characteristics of the tool presented in this paper are the separation between metric specification and data retrieval, an event driven algorithm that calculates time series data, and an abstraction of its data sources.},
booktitle = {Ninth International Workshop on Principles of Software Evolution: In Conjunction with the 6th ESEC/FSE Joint Meeting},
pages = {35–38},
numpages = {4},
keywords = {change request management, evaluation of change requests, metrics specification, process metrics, software measurement},
location = {Dubrovnik, Croatia},
series = {IWPSE '07}
}

@inproceedings{10.1145/1287624.1287679,
author = {Duboc, Leticia and Rosenblum, David and Wicks, Tony},
title = {A framework for characterization and analysis of software system scalability},
year = {2007},
isbn = {9781595938114},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1287624.1287679},
doi = {10.1145/1287624.1287679},
abstract = {The term scalability appears frequently in computing literature, but it is a term that is poorly defined and poorly understood. The lack of a clear, consistent and systematic treatment of scalability makes it difficult to evaluate claims of scalability and to compare claims from different sources. This paper presents a framework for precisely characterizing and analyzing the scalability of a software system. The framework treats scalability as a multi-criteria optimization problem and captures the dependency relationships that underlie typical notions of scalability. The paper presents the results of a case study in which the framework and analysis method were applied to a real-world system, demonstrating that it is possible to develop a precise, systematic characterization of scalability and to use the characterization to compare the scalability of alternative system designs.},
booktitle = {Proceedings of the the 6th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
pages = {375–384},
numpages = {10},
keywords = {design, microeconomics, requirements, scalability},
location = {Dubrovnik, Croatia},
series = {ESEC-FSE '07}
}

@inproceedings{10.1145/2491627.2491630,
author = {Linsbauer, Lukas and Lopez-Herrejon, E. Roberto and Egyed, Alexander},
title = {Recovering traceability between features and code in product variants},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491630},
doi = {10.1145/2491627.2491630},
abstract = {Many companies offer a palette of similar software products though they do not necessarily have a Software Product Line (SPL). Rather, they start building and selling individual products which they then adapt, customize and extend for different customers. As the number of product variants increases, these companies then face the severe problem of having to maintain them all. Software Product Lines can be helpful here - not so much as a platform for creating new products but as a means of maintaining the existing ones with their shared features. Here, an important first step is to determine where features are implemented in the source code and in what product variants. To this end, this paper presents a novel technique for deriving the traceability between features and code in product variants by matching code overlaps and feature overlaps. This is a difficult problem because a feature's implementation not only covers its basic functionality (which does not change across product variants) but may include code that deals with feature interaction issues and thus changes depending on the combination of features present in a product variant. We empirically evaluated the approach on three non-trivial case studies of different sizes and domains and found that our approach correctly identifies feature to code traces except for code that traces to multiple disjunctive features, a rare case involving less than 1% of the code.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {131–140},
numpages = {10},
keywords = {features, product variants, traceability},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/1960275.1960287,
author = {Figueiredo, Eduardo and Garcia, Alessandro and Maia, Marcelo and Ferreira, Gabriel and Nunes, Camila and Whittle, Jon},
title = {On the impact of crosscutting concern projection on code measurement},
year = {2011},
isbn = {9781450306058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1960275.1960287},
doi = {10.1145/1960275.1960287},
abstract = {Many concern metrics have been defined to quantify properties of crosscutting concerns, such as scattering, tangling, and dedication. To quantify these properties, concern metrics directly rely on the projection (assignment) of concerns into source code. Although concern identification tools have emerged over the last years, they are still rarely used in practice to support concern projection and, therefore, it is a task often performed manually. This means that the results of concern metrics are likely to be influenced by how accurately programmers assign concerns to code elements. Even though concern assignment is an important and long-standing problem in software engineering, its impact on accurate measures of crosscutting concerns has never been studied and quantified. This paper presents a series of 5 controlled experiments to quantify and analyse the impact of concern projection on crosscutting concern measures. A set of 80 participants from 4 different institutions projected 10 concern instances into the source code of two software systems. We analyse the accuracy of concern projections independently made by developers, and their impact on a set of 12 concern metrics. Our results suggest that: (i) programmers are conservative when projecting crosscutting concerns, (ii) all concern metrics suffer with such conservative behaviour, and (iii) fine-grained tangling measures are more sensitive to different concern projections than coarse-grained scattering metrics.},
booktitle = {Proceedings of the Tenth International Conference on Aspect-Oriented Software Development},
pages = {81–92},
numpages = {12},
keywords = {concern metrics, concern projection, crosscutting concerns},
location = {Porto de Galinhas, Brazil},
series = {AOSD '11}
}

@inproceedings{10.1145/2739480.2754752,
author = {Bruce, Bobby R. and Petke, Justyna and Harman, Mark},
title = {Reducing Energy Consumption Using Genetic Improvement},
year = {2015},
isbn = {9781450334723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739480.2754752},
doi = {10.1145/2739480.2754752},
abstract = {Genetic Improvement (GI) is an area of Search Based Software Engineering which seeks to improve software's non-functional properties by treating program code as if it were genetic material which is then evolved to produce more optimal solutions. Hitherto, the majority of focus has been on optimising program's execution time which, though important, is only one of many non-functional targets. The growth in mobile computing, cloud computing infrastructure, and ecological concerns are forcing developers to focus on the energy their software consumes. We report on investigations into using GI to automatically find more energy efficient versions of the MiniSAT Boolean satisfiability solver when specialising for three downstream applications. Our results find that GI can successfully be used to reduce energy consumption by up to 25%},
booktitle = {Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1327–1334},
numpages = {8},
keywords = {boolean satisfiability, energy consumption, energy efficiency, energy optimisation, genetic improvement, gi, minisat, non-functional improvement, optimisation, sat solver, sbse, search based software engineering},
location = {Madrid, Spain},
series = {GECCO '15}
}

@inproceedings{10.1145/2020390.2020406,
author = {Paikari, Elham and Sun, Bo and Ruhe, Guenther and Livani, Emadoddin},
title = {Customization support for CBR-based defect prediction},
year = {2011},
isbn = {9781450307093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020390.2020406},
doi = {10.1145/2020390.2020406},
abstract = {Background: The prediction performance of a case-based reasoning (CBR) model is influenced by the combination of the following parameters: (i) similarity function, (ii) number of nearest neighbor cases, (iii) weighting technique used for attributes, and (iv) solution algorithm. Each combination of the above parameters is considered as an instantiation of the general CBR-based prediction method. The selection of an instantiation for a new data set with specific characteristics (such as size, defect density and language) is called customization of the general CBR method.Aims: For the purpose of defect prediction, we approach the question which combinations of parameters works best at which situation. Three more specific questions were studied:(RQ1) Does one size fit all? Is one instantiation always the best?(RQ2) If not, which individual and combined parameter settings occur most frequently in generating the best prediction results?(RQ3) Are there context-specific rules to support the customization?Method: In total, 120 different CBR instantiations were created and applied to 11 data sets from the PROMISE repository. Predictions were evaluated in terms of their mean magnitude of relative error (MMRE) and percentage Pred(α) of objects fulfilling a prediction quality level α. For the third research question, dependency network analysis was performed.Results: Most frequent parameter options for CBR instantiations were neural network based sensitivity analysis (as the weighting technique), un-weighted average (as the solution algorithm), and maximum number of nearest neighbors (as the number of nearest neighbors). Using dependency network analysis, a set of recommendations for customization was provided.Conclusion: An approach to support customization is provided. It was confirmed that application of context-specific rules across groups of similar data sets is risky and produces poor results.},
booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
articleno = {16},
numpages = {10},
keywords = {case-based reasoning, customization, defect prediction, dependency network analysis, instantiation},
location = {Banff, Alberta, Canada},
series = {Promise '11}
}

@inproceedings{10.1145/1159733.1159739,
author = {Schr\"{o}ter, Adrian and Zimmermann, Thomas and Zeller, Andreas},
title = {Predicting component failures at design time},
year = {2006},
isbn = {1595932186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1159733.1159739},
doi = {10.1145/1159733.1159739},
abstract = {How do design decisions impact the quality of the resulting software? In an empirical study of 52 ECLIPSE plug-ins, we found that the software design as well as past failure history, can be used to build models which accurately predict failure-prone components in new programs. Our prediction only requires usage relationships between components, which are typically defined in the design phase; thus, designers can easily explore and assess design alternatives in terms of predicted quality. In the ECLIPSE study, 90% of the 5% most failure-prone components, as predicted by our model from design data, turned out to actually produce failures later; a random guess would have predicted only 33%.},
booktitle = {Proceedings of the 2006 ACM/IEEE International Symposium on Empirical Software Engineering},
pages = {18–27},
numpages = {10},
location = {Rio de Janeiro, Brazil},
series = {ISESE '06}
}

@inproceedings{10.1145/1081706.1081725,
author = {undefinedliwerski, Jacek and Zimmermann, Thomas and Zeller, Andreas},
title = {HATARI: raising risk awareness},
year = {2005},
isbn = {1595930140},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081706.1081725},
doi = {10.1145/1081706.1081725},
abstract = {As a software system evolves, programmers make changes which sometimes lead to problems. The risk of later problems significantly depends on the location of the change. Which are the locations where changes impose the greatest risk? Our HATARI prototype relates a version history (such as CVS) to a bug database (such as BUGZILLA) to detect those locations where changes have been risky in the past. HATARI makes this risk visible for developers by annotating source code with color bars. Furthermore, HATARI provides views to browse through the most risky locations and to analyze the risk history of a particular location.},
booktitle = {Proceedings of the 10th European Software Engineering Conference Held Jointly with 13th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {107–110},
numpages = {4},
location = {Lisbon, Portugal},
series = {ESEC/FSE-13}
}

@inproceedings{10.1145/1137702.1137712,
author = {Mohagheghi, Parastooi and Conradi, Reidar and B\o{}rretzen, Jon Arvid},
title = {Revisiting the problem of using problem reports for quality assessment},
year = {2006},
isbn = {1595933999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1137702.1137712},
doi = {10.1145/1137702.1137712},
abstract = {In this paper, we describe our experience with using problem reports from industry for quality assessment. The non-uniform terminology used in problem reports and validity concerns have been subject of earlier research but are far from settled. To distinguish between terms such as defects or errors, we propose to answer three questions on the scope of a study related to what (problem appearance or its cause), where (problems related to software; executable or not; or system), and when (problems recorded in all development life cycles or some of them). Challenges in defining research questions and metrics, collecting and analyzing data, generalizing the results and reporting them are discussed. Ambiguity in defining problem report fields and missing, inconsistent or wrong data threatens the value of collected evidence. Some of these concerns could be settled by answering some basic questions related to the problem reporting fields and improving data collection routines and tools.},
booktitle = {Proceedings of the 2006 International Workshop on Software Quality},
pages = {45–50},
numpages = {6},
keywords = {defect density, quality, validity},
location = {Shanghai, China},
series = {WoSQ '06}
}

@inproceedings{10.1145/259526.259550,
author = {Mooney, James D.},
title = {Portability and reusability: common issues and differences},
year = {1995},
isbn = {0897917375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/259526.259550},
doi = {10.1145/259526.259550},
booktitle = {Proceedings of the 1995 ACM 23rd Annual Conference on Computer Science},
pages = {150–156},
numpages = {7},
location = {Nashville, Tennessee, USA},
series = {CSC '95}
}

@inproceedings{10.1145/1101908.1101941,
author = {Langelier, Guillaume and Sahraoui, Houari and Poulin, Pierre},
title = {Visualization-based analysis of quality for large-scale software systems},
year = {2005},
isbn = {1581139934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1101908.1101941},
doi = {10.1145/1101908.1101941},
abstract = {We propose an approach for complex software analysis based on visualization. Our work is motivated by the fact that in spite of years of research and practice, software development and maintenance are still time and resource consuming, and high-risk activities. The most important reason in our opinion is the complexity of many phenomena related to software, such as its evolution and its reliability. In fact, there is very little theory explaining them. Today, we have a unique opportunity to empirically study these phenomena, thanks to large sets of software data available through open-source programs and open repositories. Automatic analysis techniques, such as statistics and machine learning, are usually limited when studying phenomena with unknown or poorly-understood influence factors. We claim that hybrid techniques that combine automatic analysis with human expertise through visualization are excellent alternatives to them. In this paper, we propose a visualization framework that supports quality analysis of large-scale software systems. We circumvent the problem of size by exploiting perception capabilities of the human visual system.},
booktitle = {Proceedings of the 20th IEEE/ACM International Conference on Automated Software Engineering},
pages = {214–223},
numpages = {10},
keywords = {metrics, quality assessment, software visualization},
location = {Long Beach, CA, USA},
series = {ASE '05}
}

@inproceedings{10.5555/2050655.2050659,
author = {Drago, Mauro Luigi and Ghezzi, Carlo and Mirandola, Raffaela},
title = {Towards quality driven exploration of model transformation spaces},
year = {2011},
isbn = {9783642244841},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Verifying that a software system has certain nonfunctional properties is a primary concern in many engineering fields. Although several model-driven approaches exist to predict quality attributes from system models, they still lack the proper level of automation envisioned by Model Driven Software Development. When a potential issue concerning non-functional properties is discovered, the identification of a solution is still entirely up to the engineer and to his/her experience. This paper presents QVT-Rational, our multi-modeling solution to automate the detection-solution loop. We leverage and extend existing model transformation techniques with constructs to elicit the space of the alternative solutions and to bind quality properties to them. Our framework is highly customizable, it supports the definition of nonfunctional requirements and provides an engine to automatically explore the solution space. We evaluate our approach by applying it to two well-known software engineering problems -- Object-Relational Mapping and components allocation -- and by showing how several solutions that satisfy given performance requirements can be automatically identified.},
booktitle = {Proceedings of the 14th International Conference on Model Driven Engineering Languages and Systems},
pages = {2–16},
numpages = {15},
keywords = {feedback provisioning, model transformations},
location = {Wellington, New Zealand},
series = {MODELS'11}
}

@inproceedings{10.1145/1453101.1453105,
author = {Pinzger, Martin and Nagappan, Nachiappan and Murphy, Brendan},
title = {Can developer-module networks predict failures?},
year = {2008},
isbn = {9781595939951},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1453101.1453105},
doi = {10.1145/1453101.1453105},
abstract = {Software teams should follow a well defined goal and keep their work focused. Work fragmentation is bad for efficiency and quality. In this paper we empirically investigate the relationship between the fragmentation of developer contributions and the number of post-release failures. Our approach is to represent developer contributions with a developer-module network that we call contribution network. We use network centrality measures to measure the degree of fragmentation of developer contributions. Fragmentation is determined by the centrality of software modules in the contribution network. Our claim is that central software modules are more likely to be failure-prone than modules located in surrounding areas of the network. We analyze this hypothesis by exploring the network centrality of Microsoft Windows Vista binaries using several network centrality measures as well as linear and logistic regression analysis. In particular, we investigate which centrality measures are significant to predict the probability and number of post-release failures. Results of our experiments show that central modules are more failure-prone than modules located in surrounding areas of the network. Results further confirm that number of authors and number of commits are significant predictors for the probability of post-release failures. For predicting the number of post-release failures the closeness centrality measure is most significant.},
booktitle = {Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {2–12},
numpages = {11},
keywords = {developer contribution network, failure prediction, network centrality measures, social network analysis},
location = {Atlanta, Georgia},
series = {SIGSOFT '08/FSE-16}
}

@inproceedings{10.1145/1134285.1134349,
author = {Nagappan, Nachiappan and Ball, Thomas and Zeller, Andreas},
title = {Mining metrics to predict component failures},
year = {2006},
isbn = {1595933751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1134285.1134349},
doi = {10.1145/1134285.1134349},
abstract = {What is it that makes software fail? In an empirical study of the post-release defect history of five Microsoft software systems, we found that failure-prone software entities are statistically correlated with code complexity measures. However, there is no single set of complexity metrics that could act as a universally best defect predictor. Using principal component analysis on the code metrics, we built regression models that accurately predict the likelihood of post-release defects for new entities. The approach can easily be generalized to arbitrary projects; in particular, predictors obtained from one project can also be significant for new, similar projects.},
booktitle = {Proceedings of the 28th International Conference on Software Engineering},
pages = {452–461},
numpages = {10},
keywords = {bug database, complexity metrics, empirical study, principal component analysis, regression model},
location = {Shanghai, China},
series = {ICSE '06}
}

@article{10.1145/567793.567795,
author = {Mockus, Audris and Fielding, Roy T. and Herbsleb, James D.},
title = {Two case studies of open source software development: Apache and Mozilla},
year = {2002},
issue_date = {July 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/567793.567795},
doi = {10.1145/567793.567795},
abstract = {According to its proponents, open source style software development has the capacity to compete successfully, and perhaps in many cases displace, traditional commercial development methods. In order to begin investigating such claims, we examine data from two major open source projects, the Apache web server and the Mozilla browser. By using email archives of source code change history and problem reports we quantify aspects of developer participation, core team size, code ownership, productivity, defect density, and problem resolution intervals for these OSS projects. We develop several hypotheses by comparing the Apache project with several commercial projects. We then test and refine several of these hypotheses, based on an analysis of Mozilla data. We conclude with thoughts about the prospects for high-performance commercial/open source process hybrids.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
pages = {309–346},
numpages = {38},
keywords = {Apache, Mozilla, Open source software, code ownership, defect density, repair interval}
}

@inproceedings{10.1145/3524459.3527353,
author = {Harman, Mark},
title = {Scaling genetic improvement and automated program repair},
year = {2022},
isbn = {9781450392853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524459.3527353},
doi = {10.1145/3524459.3527353},
abstract = {This paper outlines techniques and research directions for scaling genetic improvement and automated program repair, highlighting possible directions for future work and open challenges.},
booktitle = {Proceedings of the Third International Workshop on Automated Program Repair},
pages = {1–7},
numpages = {7},
keywords = {automated program repair, genetic improvement, search based software engineering (SBSE)},
location = {Pittsburgh, Pennsylvania},
series = {APR '22}
}

@proceedings{10.1145/3634713,
title = {VaMoS '24: Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bern, Switzerland}
}

@proceedings{10.1145/3613372,
title = {SBES '23: Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Campo Grande, Brazil}
}

@proceedings{10.1145/3571788,
title = {VaMoS '23: Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Odense, Denmark}
}

@inproceedings{10.1145/2577080.2577096,
author = {da Silva, Bruno C. and Sant'Anna, Claudio N. and Chavez, Christina von F.G.},
title = {An empirical study on how developers reason about module cohesion},
year = {2014},
isbn = {9781450327725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2577080.2577096},
doi = {10.1145/2577080.2577096},
abstract = {Several cohesion metrics have been proposed to support development and maintenance activities. The most traditional ones are the structural cohesion metrics, which rely on structural information in the source code. For instance, many of these metrics quantify cohesion based how methods and attributes are related to each other within a given module. Recently, conceptual cohesion metrics have been proposed for computing cohesion based on the responsibilities a given module realizes. Besides different flavors of cohesion, there is a lack of empirical evidence about how developers actually perceive cohesion and what kind of cohesion measurement aligns with developers' perception. In this paper we fill this gap by empirically investigating developers opinion through a web-based survey, which involved 80 participants from 9 countries with different levels of programming experience. We found that: most of the developers are familiar with cohesion; and they perceive cohesion based on class responsibilities, thus associating more with conceptual cohesion measurement. These results support the claim that conceptual cohesion seems to be more intuitive and closer to the human-oriented view of software cohesion. Moreover, the results showed that conceptual cohesion measurement captures the developers' notion of cohesion better than traditional structural cohesion measurement.},
booktitle = {Proceedings of the 13th International Conference on Modularity},
pages = {121–132},
numpages = {12},
keywords = {empirical software engineering, module cohesion},
location = {Lugano, Switzerland},
series = {MODULARITY '14}
}

@inproceedings{10.1145/3555776.3578611,
author = {Limaylla-Lunarejo, Maria-Isabel and Condori-Fernandez, Nelly and Luaces, Miguel R.},
title = {Towards a FAIR Dataset for non-functional requirements},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3578611},
doi = {10.1145/3555776.3578611},
abstract = {In the last years, the application of supervised Machine Learning (ML) algorithms in Requirements Engineering (RE) has allowed increasing the performance (e.g. accuracy, precision) and scalability of automatic requirements classification. However, the lack of publicly labeled datasets is still one concern when conducting ML experiments. Few publicly labeled datasets for non-functional requirements classification are available, and even less in the Spanish language. Moreover, most of the available datasets present some limitations, such as imbalanced classes (e.g. PROMISE NFR). This study aims to generate a FAIR dataset of non-functional requirements in the Spanish language for facilitating reuse in ML classification experiments. 109 non-functional requirements were collected from final degree projects from the University of A Coru\~{n}a. We conducted a pilot quasi-experiment for non-functional requirements labeling in the categories and subcategories of the ISO/IEC 25010 quality model. The labeling process was accomplished by 7 annotators. The inter-annotator agreement using a Fleiss' Kappa test obtained a substantial agreement in the category level (0.78) and a moderate agreement (0.48) when the classification is per subcategory.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1414–1421},
numpages = {8},
keywords = {data labeling, non-functional requirements, spanish dataset, FAIR principles},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00042,
author = {Valle, Pablo and Arrieta, Aitor and Arratibel, Maite},
title = {Automated Misconfiguration Repair of Configurable Cyber-Physical Systems with Search: An Industrial Case Study on Elevator Dispatching Algorithms},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00042},
doi = {10.1109/ICSE-SEIP58684.2023.00042},
abstract = {Real-world Cyber-Physical Systems (CPSs) are usually configurable. Through parameters, it is possible to configure, select or unselect different system functionalities. While this provides high flexibility, it also becomes a source for failures due to misconfigurations. The large number of parameters these systems have and the long test execution time in this context due to the use of simulation-based testing make the manual repair process a cumbersome activity. Subsequently, in this context, automated repairing methods are paramount. In this paper, we propose an approach to automatically repair CPSs' misconfigurations. Our approach is evaluated with an industrial CPS case study from the elevation domain. Experiments with a real building and data obtained from operation suggests that our approach outperforms a baseline algorithm as well as the state of the practice (i.e., manual repair carried out by domain experts).},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {396–408},
numpages = {13},
keywords = {cyber-physical systems, repair, debugging, configurable systems},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/2528265.2528267,
author = {Apel, Sven and Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Garvin, Brady},
title = {Exploring feature interactions in the wild: the new feature-interaction challenge},
year = {2013},
isbn = {9781450321686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2528265.2528267},
doi = {10.1145/2528265.2528267},
abstract = {The feature-interaction problem has been keeping researchers and practitioners in suspense for years. Although there has been substantial progress in developing approaches for modeling, detecting, managing, and resolving feature interactions, we lack sufficient knowledge on the kind of feature interactions that occur in real-world systems. In this position paper, we set out the goal to explore the nature of feature interactions systematically and comprehensively, classified in terms of order and visibility. Understanding this nature will have significant implications on research in this area, for example, on the efficiency of interaction-detection or performance-prediction techniques. A set of preliminary results as well as a discussion of possible experimental setups and corresponding challenges give us confidence that this endeavor is within reach but requires a collaborative effort of the community.},
booktitle = {Proceedings of the 5th International Workshop on Feature-Oriented Software Development},
pages = {1–8},
numpages = {8},
keywords = {feature interactions, feature modularity, feature-interaction problem, feature-oriented software development},
location = {Indianapolis, Indiana, USA},
series = {FOSD '13}
}

@proceedings{10.1145/3540250,
title = {ESEC/FSE 2022: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of all members of the organizing committee, we are delighted to welcome everyone to the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2022. The event continues the long, distinguished ESEC/FSE tradition of presenting the most innovative research, and facilitating interactions between scientists and engineers who are passionate about advancing the theory and practice of software engineering.},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3623476.3623517,
author = {Jouneaux, Gwendal and Fr\"{o}lich, Damian and Barais, Olivier and Combemale, Benoit and Le Guernic, Gurvan and Mussbacher, Gunter and van Binsbergen, L. Thomas},
title = {Adaptive Structural Operational Semantics},
year = {2023},
isbn = {9798400703966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623476.3623517},
doi = {10.1145/3623476.3623517},
abstract = {Software systems evolve more and more in complex and changing environments, often requiring runtime adaptation  
to best deliver their services. When self-adaptation is the main concern of the system, a manual implementation of the underlying feedback loop and trade-off analysis may be desirable. However, the required expertise and substantial development effort make such implementations prohibitively difficult when it is only a secondary concern for the given domain. In this paper, we present ASOS, a metalanguage abstracting the runtime adaptation concern of a given domain in the behavioral semantics of a domain-specific language (DSL), freeing the language user from implementing it from scratch for each system in the domain. We demonstrate our approach on RobLANG, a procedural DSL for robotics, where we abstract a recurrent energy-saving behavior depending on the context. We provide formal semantics for ASOS and pave the way for checking properties such as determinism, completeness, and termination of the resulting self-adaptable language. We provide first results on the performance of our approach compared to a manual implementation of this self-adaptable behavior. We demonstrate, for RobLANG, that our approach provides suitable abstractions for specifying sound adaptive operational semantics while being more efficient.},
booktitle = {Proceedings of the 16th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {29–42},
numpages = {14},
keywords = {DSL, Operational Semantics, Self-Adaptation},
location = {Cascais, Portugal},
series = {SLE 2023}
}

@inproceedings{10.1145/1944892.1944894,
author = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Th\"{u}m, Thomas and Saake, Gunter},
title = {Multi-dimensional variability modeling},
year = {2011},
isbn = {9781450305709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944892.1944894},
doi = {10.1145/1944892.1944894},
abstract = {The variability of a software product line (SPL)is often described with a feature model. To avoid highly complex models, stakeholders usually try to separate different variability dimensions, such as domain variability and implementation variability. This results in distinct variability models, which are easier to handle than one large model. On the other hand, it is sometimes required to analyze the variability dimensions of an SPL in combination using a single model only. To combine separate modeling and integrated analysis of variability, we present Velvet, a language for multi-dimensional variability modeling. Velvet allows stakeholders to model each variability dimension of an SPL separately and to compose the separated dimensions on demand. This improves reuse of feature models and supports independent modeling variability dimensions. Furthermore, Velvet integrates feature modeling and configuration in a single language. The combination of both concepts creates further reuse opportunities and allows stakeholders to independently configure variability dimensions.},
booktitle = {Proceedings of the 5th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {11–20},
numpages = {10},
keywords = {feature models, separation of concerns, variability modeling},
location = {Namur, Belgium},
series = {VaMoS '11}
}

@article{10.1145/3596217,
author = {Cazzola, Walter and Favalli, Luca},
title = {Scrambled Features for Breakfast: Concepts of Agile Language Development},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/3596217},
doi = {10.1145/3596217},
abstract = {Describing a framework to support simpler development of languages best suited to express the problems and solutions of each particular domain.},
journal = {Commun. ACM},
month = oct,
pages = {50–60},
numpages = {11}
}

@inproceedings{10.1145/2499777.2500711,
author = {Ciolfi Felice, Marianela and Filho, Joao Bosco Ferreira and Acher, Mathieu and Blouin, Arnaud and Barais, Olivier},
title = {Interactive visualisation of products in online configurators: a case study for variability modelling technologies},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500711},
doi = {10.1145/2499777.2500711},
abstract = {Numerous companies develop interactive environments to assist users in customising sales products through the selection of configuration options. A visual representation of these products is an important factor in terms of user experience. However, an analysis of 100+ existing configurators highlights that not all provide visual representations of configured products. One of the current challenges is the trade-off developers face between either the memory consuming use of pregenerated images of all the combinations of options, or rendering products on the fly, which is non trivial to implement efficiently. We believe that a new approach to associate product configurations to visual representations is needed to compose and render them dynamically. In this paper we present a formal statement of the problem and a model-driven perspective for addressing it as well as our ongoing work and further challenges.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {82–85},
numpages = {4},
keywords = {configurator, software product line, user interface, variability modelling},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/2025113.2025177,
author = {Mori, Marco},
title = {A software lifecycle process for context-aware adaptive systems},
year = {2011},
isbn = {9781450304436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2025113.2025177},
doi = {10.1145/2025113.2025177},
abstract = {It is increasingly important for computing systems to evolve their behavior at run-time because of resources uncertainty, system failures and emerging user needs. Our approach supports software engineers to analyze and develop context-aware adaptive applications. The software lifecycle process we propose supports static and dynamic decision making mechanisms, run-time consistent evolution and it is amenable to be automated.},
booktitle = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
pages = {412–415},
numpages = {4},
keywords = {consistent evolution, context-aware adaptive systems, feature engineering, software lifecycle process},
location = {Szeged, Hungary},
series = {ESEC/FSE '11}
}

@inproceedings{10.1145/3109729.3109737,
author = {Montalvillo, Leticia and D\'{\i}az, Oscar and Azanza, Maider},
title = {Visualizing product customization efforts for spotting SPL reuse opportunities},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109737},
doi = {10.1145/3109729.3109737},
abstract = {Migrating a set of product variants to a managed SPL is rarely a one-shot effort. Experiences from industry revealed that a complete migration to an SPL might take years, during which customers' requirements still need to be fulfilled by the company (customization effort). Analyzing the assets that have been customized by products (customization analysis) becomes a main stepping stone in ascertaining reuse opportunities. This requires to remain vigilant to arising reuse opportunities not just at the SPL onset, but throughout the whole process. Traditionally, a common mechanism to identify reuse opportunities is the diff utility whereby differences between two files are calculated and displayed. But this mechanism might not scale up. Given the sheer number of both core-assets and SPL products, visualizations that abstract from conventional line-level diffs to higher level visualization are required to spot reuse opportunities a ta glance. To this end, we introduce visualizations that help to estimate the extent of the customization effort broken down by product and core-asset. The aim: a prompt insight into questions such as, how much effort are product developers spending on customization?; or, which core-assets needed a larger tuning to meet product requirements? This vision is realized in CUSTOMS, a visualization utility on top of FeatureHouse that resorts to alluvial diagrams and tree maps to display customization effort. CUSTOMS might serve as a first stepping stone for spotting reuse opportunities.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {73–80},
numpages = {8},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3377812.3381387,
author = {Su\~{n}\'{e}, Agust\'{\i}n E. Martinez},
title = {Formalization and analysis of quantitative attributes of distributed systems},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3381387},
doi = {10.1145/3377812.3381387},
abstract = {While there is not much discussion on the importance of formally describing and analyzing quantitative requirements in the process of software construction; in the paradigm of API-based software systems, it could be vital. Quantitative attributes can be thought of as attributes determining the Quality of Service - QoS provided by a software component published as a service. In this sense, they play a determinant role in classifying software artifacts according to specific needs stated as requirements.In this work, we present a research program consisting of the development of formal languages and tools to characterize and analyze the Quality of Service attributes of software components in the context of distributed systems. More specifically, our main motivational scenario lays on the execution of a service-oriented architecture.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {210–213},
numpages = {4},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/2627508.2627510,
author = {Tang, Hao and Zhou, You and Huang, Xin and Rong, Guoping},
title = {Does pareto's law apply to evidence distribution in software engineering? an initial report},
year = {2014},
isbn = {9781450329651},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2627508.2627510},
doi = {10.1145/2627508.2627510},
abstract = {Data is the source as well as raw format of evidence. As an important research methodology in evidence-based software engineering, systematic literature reviews (SLRs) are used for identifying the evidence and critically appraising the evidence, i.e. empirical studies that report (empirical) data about specific research questions. The 80/20 Rule (or Pareto's Law) reveals a 'vital few' phenomenon widely observed in many disciplines in the last century. However, the applicability of Pareto's Law to evidence distribution in software engineering (SE) is never tested yet. The objective of this paper is to investigate the applicability of Pareto's Law to the evidence distribution on specific topic areas in software engineering (in the form of systematic reviews), which may help us better understand the possible distribution of evidence in software engineering, and further improve the effectiveness and efficiency of literature search. We performed a tertiary study of SLRs in software engineering dated between 2004 and 2012. We further tested the Pareto's Law by collecting, analyzing, and interpreting the distribution (over publication venues) of the primary studies reported in the existing SLRs. Our search identified 255 SLRs, 107 of which were included according to the selection criteria. The analysis of the extracted data from these SLRs presents a preliminary view of the evidence (study) distribution in software engineering. The nonuniform distribution of evidence is supported by the data from the existing SLRs in SE. However, the present observation reflects a weaker 'vital few' relation between study and venue than the 80/20 Rule statement. Top referenced venues are suggested when researchers search for studies in software engineering. It is also noticeable to the community that the primary studies are improperly or incompletely reported in many SLRs.},
booktitle = {Proceedings of the 2014 3rd International Workshop on Evidential Assessment of Software Technologies},
pages = {9–16},
numpages = {8},
keywords = {Pareto's Law (80/20 Rule), evidence distribution, evidence-based software engineering, systematic (literature) review},
location = {Nanjing, China},
series = {EAST 2014}
}

@article{10.1145/3469440,
author = {Gheibi, Omid and Weyns, Danny and Quin, Federico},
title = {Applying Machine Learning in Self-adaptive Systems: A Systematic Literature Review},
year = {2021},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1556-4665},
url = {https://doi.org/10.1145/3469440},
doi = {10.1145/3469440},
abstract = {Recently, we have been witnessing a rapid increase in the use of machine learning techniques in self-adaptive systems. Machine learning has been used for a variety of reasons, ranging from learning a model of the environment of a system during operation to filtering large sets of possible configurations before analyzing them. While a body of work on the use of machine learning in self-adaptive systems exists, there is currently no systematic overview of this area. Such an overview is important for researchers to understand the state of the art and direct future research efforts. This article reports the results of a systematic literature review that aims at providing such an overview. We focus on self-adaptive systems that are based on a traditional Monitor-Analyze-Plan-Execute (MAPE)-based feedback loop. The research questions are centered on the problems that motivate the use of machine learning in self-adaptive systems, the key engineering aspects of learning in self-adaptation, and open challenges in this area. The search resulted in 6,709 papers, of which 109 were retained for data collection. Analysis of the collected data shows that machine learning is mostly used for updating adaptation rules and policies to improve system qualities, and managing resources to better balance qualities and resources. These problems are primarily solved using supervised and interactive learning with classification, regression, and reinforcement learning as the dominant methods. Surprisingly, unsupervised learning that naturally fits automation is only applied in a small number of studies. Key open challenges in this area include the performance of learning, managing the effects of learning, and dealing with more complex types of goals. From the insights derived from this systematic literature review, we outline an initial design process for applying machine learning in self-adaptive systems that are based on MAPE feedback loops.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = aug,
articleno = {9},
numpages = {37},
keywords = {MAPE-K, Self-adaptation, feedback loops}
}

@proceedings{10.1145/3526071,
title = {RoSE '22: Proceedings of the 4th International Workshop on Robotics Software Engineering},
year = {2022},
isbn = {9781450393171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Software engineering is a crucial enabler for successful deployment of robotic applications. The research communities advancing software engineering in robotics, however, are spread over various spe-cialized conferences, such as ICRA, IROS, SIMPAR - each attended mostly by robotics researchers and practitioners - or ICSE andMODELS - mostly attended by software engineering researchers and practitioners. At robotics conferences, software engineering lacks visibility and vice versa.The objective of RoSE is bringing together researchers and practitioners from both domains at a prominent conference to foster cross-fertilization between the two domains. Being the most prominent conference in software engineering, ICSE is the best venue to attract experts from both domains. Hosting this workshop at ICSE enables software engineering researchers to learn more about the challenges of robotics practitioners that (i) require further research from the software engineering community or (ii) are already solved but solutions are unnoticed by roboticists, yet.},
location = {Pittsburgh, Pennsylvania}
}

@inproceedings{10.1145/2993236.2993246,
author = {Kienzle, J\"{o}rg and Mussbacher, Gunter and Collet, Philippe and Alam, Omar},
title = {Delaying decisions in variable concern hierarchies},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993246},
doi = {10.1145/2993236.2993246},
abstract = {Concern-Oriented Reuse (CORE) proposes a new way of structuring model-driven software development, where models of the system are modularized by domains of abstraction within units of reuse called concerns. Within a CORE concern, models are further decomposed and modularized by features. This paper extends CORE with a technique that enables developers of high-level concerns to reuse lower-level concerns without unnecessarily committing to a specific feature selection. The developer can select the functionality that is minimally needed to continue development, and reexpose relevant alternative lower-level features of the reused concern in the reusing concern's interface. This effectively delays decision making about alternative functionality until the higher-level reuse context, where more detailed requirements are known and further decisions can be made. The paper describes the algorithms for composing the variation (i.e., feature and impact models), customization, and usage interfaces of a concern, as well as the concern's realization models and finally an entire concern hierarchy, as is necessary to support delayed decision making in CORE.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {93–103},
numpages = {11},
keywords = {Delaying of Decisions, Model Interfaces, Model Reuse, Model-Driven Engineering, Reuse Hierarchies},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@proceedings{10.1145/3622748,
title = {SBCARS '23: Proceedings of the 17th Brazilian Symposium on Software Components, Architectures, and Reuse},
year = {2023},
isbn = {9798400709524},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Campo Grande, Brazil}
}

@inproceedings{10.1109/MiSE.2019.00018,
author = {Sch\"{o}ttle, Matthias and Kienzle, J\"{o}rg},
title = {On the difficulties of raising the level of abstraction and facilitating reuse in software modelling: the case for signature extension},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MiSE.2019.00018},
doi = {10.1109/MiSE.2019.00018},
abstract = {Reuse is central to improving the software development process, increasing software quality and decreasing time-to-market. Hence it is of paramount importance that modelling languages provide features that enable the specification and modularization of reusable artefacts, as well as their subsequent reuse. In this paper we outline several difficulties caused by the finality of method signatures that make it hard to specify and use reusable artefacts encapsulating several variants. The difficulties are illustrated with a running example. To evaluate whether these difficulties can be observed at the programming level, we report on an empirical study conducted on the Java Platform API as well as present workarounds used in various programming languages to deal with the rigid nature of signatures. Finally, we outline signature extension as an approach to overcome these problems at the modelling level.},
booktitle = {Proceedings of the 11th International Workshop on Modelling in Software Engineerings},
pages = {71–77},
numpages = {7},
location = {Montreal, Quebec, Canada},
series = {MiSE '19}
}

@article{10.1145/3447240,
author = {Bertolino, Antonia and Braione, Pietro and Angelis, Guglielmo De and Gazzola, Luca and Kifetew, Fitsum and Mariani, Leonardo and Orr\`{u}, Matteo and Pezz\`{e}, Mauro and Pietrantuono, Roberto and Russo, Stefano and Tonella, Paolo},
title = {A Survey of Field-based Testing Techniques},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3447240},
doi = {10.1145/3447240},
abstract = {Field testing refers to testing techniques that operate in the field to reveal those faults that escape in-house testing. Field testing techniques are becoming increasingly popular with the growing complexity of contemporary software systems. In this article, we present the first systematic survey of field testing approaches over a body of 80 collected studies, and propose their categorization based on the environment and the system on which field testing is performed. We discuss four research questions addressing how software is tested in the field, what is tested in the field, which are the requirements, and how field tests are managed, and identify many challenging research directions.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {92},
numpages = {39},
keywords = {Software testing, ex-vivo testing, field testing, in-vivo testing}
}

@inproceedings{10.1145/3030207.3030216,
author = {Valov, Pavel and Petkovich, Jean-Christophe and Guo, Jianmei and Fischmeister, Sebastian and Czarnecki, Krzysztof},
title = {Transferring Performance Prediction Models Across Different Hardware Platforms},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3030216},
doi = {10.1145/3030207.3030216},
abstract = {Many software systems provide configuration options relevant to users, which are often called features. Features influence functional properties of software systems as well as non-functional ones, such as performance and memory consumption. Researchers have successfully demonstrated the correlation between feature selection and performance. However, the generality of these performance models across different hardware platforms has not yet been evaluated.We propose a technique for enhancing generality of performance models across different hardware environments using linear transformation. Empirical studies on three real-world software systems show that our approach is computationally efficient and can achieve high accuracy (less than 10% mean relative error) when predicting system performance across 23 different hardware platforms. Moreover, we investigate why the approach works by comparing performance distributions of systems and structure of performance models across different platforms.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {39–50},
numpages = {12},
keywords = {linear transformation, model transfer, performance modelling, regression trees},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@inproceedings{10.1145/3236024.3264838,
author = {Brun, Yuriy and Meliou, Alexandra},
title = {Software fairness},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3264838},
doi = {10.1145/3236024.3264838},
abstract = {A goal of software engineering research is advancing software quality and the success of the software engineering process. However, while recent studies have demonstrated a new kind of defect in software related to its ability to operate in fair and unbiased manner, software engineering has not yet wholeheartedly tackled these new kinds of defects, thus leaving software vulnerable. This paper outlines a vision for how software engineering research can help reduce fairness defects and represents a call to action by the software engineering research community to reify that vision. Modern software is riddled with examples of biased behavior, from automated translation injecting gender stereotypes, to vision systems failing to see faces of certain races, to the US criminal justice sytem relying on biased computational assessments of crime recidivism. While systems may learn bias from biased data, bias can also emerge from ambiguous or incomplete requirement specification, poor design, implementation bugs, and unintended component interactions. We argue that software fairness is analogous to software quality, and that numerous software engineering challenges in the areas of requirements, specification, design, testing, and verification need to be tackled to solve this problem.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {754–759},
numpages = {6},
keywords = {Software fairness, software bias, software process},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/2593929.2593930,
author = {Garc\'{\i}a-Gal\'{a}n, Jes\'{u}s and Pasquale, Liliana and Trinidad, Pablo and Ruiz-Cort\'{e}s, Antonio},
title = {User-centric adaptation of multi-tenant services: preference-based analysis for service reconfiguration},
year = {2014},
isbn = {9781450328647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593929.2593930},
doi = {10.1145/2593929.2593930},
abstract = {Multi-tenancy is a key pillar of cloud services. It allows different tenants to share computing resources transparently and, at the same time, guarantees substantial cost savings for the providers. However, from a user perspective, one of the major drawbacks of multi-tenancy is lack of configurability. Depending on the isolation degree, the same service instance and even the same service configuration may be shared among multiple tenants (i.e. shared multi-tenant service). Moreover tenants usually have different - and in most of the cases - conflicting configuration preferences. To overcome this limitation, this paper introduces a novel approach to support user-centric adaptation in shared multi-tenant services. The adaptation objective aims to maximise tenants’ satisfaction, even when tenants and their preferences change during the service life-time. This paper describes how to engineer the activities of the MAPE loop to support user-centric adaptation, and focuses on the analysis of tenants’ preferences. In particular, we use a game theoretic analysis to identify a service configuration that maximises tenants’ preferences satisfaction. We illustrate and motivate our approach by utilising a multi-tenant desktop scenario. Obtained experimental results demonstrate the feasibility of the proposed analysis.},
booktitle = {Proceedings of the 9th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {65–74},
numpages = {10},
keywords = {Adaptive systems, cloud, game theory, multi-tenancy},
location = {Hyderabad, India},
series = {SEAMS 2014}
}

@proceedings{10.1145/3643667,
title = {Q-SE 2024: Proceedings of the 5th ACM/IEEE International Workshop on Quantum Software Engineering},
year = {2024},
isbn = {9798400705700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 5th International Workshop on Quantum Software Engineering (Q-SE 2024), co-located with ICSE 2024, provides a platform for researchers and practitioners to discuss challenges in developing quantum software in high-level quantum languages, novel solutions to build correct methods for testing quantum programs, executing quantum software, developing best practices, and creating a research roadmap of quantum software engineering.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3229345.3229419,
author = {Oliveira, Joyce Aline and Vargas, Matheus and Rodrigues, Roni},
title = {SOA Reuse: Systematic Literature Review Updating and Research Directions},
year = {2018},
isbn = {9781450365598},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229345.3229419},
doi = {10.1145/3229345.3229419},
abstract = {Service Oriented Architecture (SOA) reuse has been used strategically in organizations to reduce development costs and increase the quality of applications. This article analyzes a systematic literature review in order to identify concepts, goals, strategies, and metrics of SOA reuse. The results show that the main goal of SOA reuse is to decrease development costs. The factor that most negatively influences SOA reuse is the existence of legacy systems. The strategy used most to potentialize SOA reuse is business process management. Metrics proposed by studies to measure SOA reuse are related to modularity and adaptability indicators. The study is relevant because it increases the body of knowledge of the area. Additionally, a set of gaps to be addressed by researchers and reuse practitioners was identified.},
booktitle = {Proceedings of the XIV Brazilian Symposium on Information Systems},
articleno = {71},
numpages = {8},
keywords = {SOA reuse, Service Oriented Architecture, systematic literature review},
location = {Caxias do Sul, Brazil},
series = {SBSI '18}
}

@inproceedings{10.1145/2934466.2946046,
author = {Arrieta, Aitor and Wang, Shuai and Sagardui, Goiuria and Etxeberria, Leire},
title = {Search-based test case selection of cyber-physical system product lines for simulation-based validation},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946046},
doi = {10.1145/2934466.2946046},
abstract = {Cyber-Physical Systems (CPSs) are often tested at different test levels following "X-in-the-Loop" configurations: Model-, Software- and Hardware-in-the-loop (MiL, SiL and HiL). While MiL and SiL test levels aim at testing functional requirements at the system level, the HiL test level tests functional as well as non-functional requirements by performing a real-time simulation. As testing CPS product line configurations is costly due to the fact that there are many variants to test, test cases are long, the physical layer has to be simulated and co-simulation is often necessary. It is therefore extremely important to select the appropriate test cases that cover the objectives of each level in an allowable amount of time. We propose an efficient test case selection approach adapted to the "X-in-the-Loop" test levels. Search algorithms are employed to reduce the amount of time required to test configurations of CPS product lines while achieving the test objectives of each level. We empirically evaluate three commonly-used search algorithms, i.e., Genetic Algorithm (GA), Alternating Variable Method (AVM) and Greedy (Random Search (RS) is used as a baseline) by employing two case studies with the aim of integrating the best algorithm into our approach. Results suggest that as compared with RS, our approach can reduce the costs of testing CPS product line configurations by approximately 80% while improving the overall test quality.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {297–306},
numpages = {10},
keywords = {cyber-physical system product lines, search-based software engineering, test case selection},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2701319.2701335,
author = {Gamez, Nadia and El Haddad, Joyce and Fuentes, Lidia},
title = {Managing the Variability in the Transactional Services Selection},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701335},
doi = {10.1145/2701319.2701335},
abstract = {Web service composition is the capability to recursively construct a value added service by means of picking up existing services. An important step in the composition process is the selection step, which includes choosing services located in repositories. The selection approaches of Web services need to consider their specifics which raises important challenges as the management of the inherent service variability in functionality and implementation and ensuring correct execution termination between others. To realize reliable service compositions, transactional properties of services must be considered during the selection step. We argue that the transactional properties should be considered at the operation level of each service to be composed. However, modelling transactional services composition at the operation level drastically increment the complexity of service selection. In order to overcome this difficulty, in this paper we report on our research in progress on transactional service selection, which follows a Software Product Line approach considering the set of services that provide the same functionality as part of a service family. We model the variable operations of the service families using Feature Models. In this way, the selection process consists of selecting each service from a service family such that the aggregated transactional property satisfies the user preference.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {88–95},
numpages = {8},
keywords = {Discovery and Selection, Feature Modeling, Transactional Services},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@inproceedings{10.1145/2610384.2610411,
author = {Galindo, Jos\'{e} A. and Alf\'{e}rez, Mauricio and Acher, Mathieu and Baudry, Benoit and Benavides, David},
title = {A variability-based testing approach for synthesizing video sequences},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610411},
doi = {10.1145/2610384.2610411},
abstract = {A key problem when developing video processing software is the difficulty to test different input combinations. In this paper, we present VANE, a variability-based testing approach to derive video sequence variants. The ideas of VANE are i) to encode in a variability model what can vary within a video sequence; ii) to exploit the variability model to generate testable configurations; iii) to synthesize variants of video sequences corresponding to configurations. VANE computes T-wise covering sets while optimizing a function over attributes. Also, we present a preliminary validation of the scalability and practicality of VANE in the context of an industrial project involving the test of video processing algorithms.},
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {293–303},
numpages = {11},
keywords = {Combinatorial testing, Variability, Video analysis},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@inproceedings{10.1145/3368089.3409749,
author = {Shahbazian, Arman and Karthik, Suhrid and Brun, Yuriy and Medvidovic, Nenad},
title = {eQual: informing early design decisions},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409749},
doi = {10.1145/3368089.3409749},
abstract = {When designing a software system, architects make a series of design decisions that directly impact the system's quality. The number of available design alternatives grows rapidly with system size, creating an enormous space of intertwined design concerns that renders manual exploration impractical. We present eQual, a model-driven technique for simulation-based assessment of architectural designs. While it is not possible to guarantee optimal decisions so early in the design process, eQual improves decision quality. eQual is effective in practice because it (1) limits the amount of information the architects have to provide and (2) adapts optimization algorithms to effectively explore massive spaces of design alternatives. We empirically demonstrate that eQual yields designs whose quality is comparable to a set of systems' known optimal designs. A user study shows that, compared to the state-of-the-art, engineers using eQual produce statistically significantly higher-quality designs with a large effect size, are statistically significantly more confident in their designs, and find eQual easier to use.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1039–1051},
numpages = {13},
keywords = {Software design, design analysis, design decisions, optimization},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@proceedings{10.1145/3559712,
title = {SBCARS '22: Proceedings of the 16th Brazilian Symposium on Software Components, Architectures, and Reuse},
year = {2022},
isbn = {9781450397452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Uberlandia, Brazil}
}

@inproceedings{10.1109/SEAMS.2017.4,
author = {Tahara, Yasuyuki and Ohsuga, Akihiko and Honiden, Shinichi},
title = {Formal verification of dynamic evolution processes of UML models using aspects},
year = {2017},
isbn = {9781538615508},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SEAMS.2017.4},
doi = {10.1109/SEAMS.2017.4},
abstract = {The rapidly changing requirements and environments of system operation demand dynamic changes to systems with as short downtimes as possible. System availability is a relevant feature for such dynamic changes, which we call dynamic evolution. One of the most promising approaches to highly available dynamic evolution is dynamic aspect weaving, a technique of aspect-oriented programming technology. It enables part of a program to dynamically change without stopping its execution. Another feature relevant to dynamic evolution is the assurance of correctness of evolution. However, this is not easy for dynamic evolution, mainly because the evolution process is rather complicated. Formal modeling and verification (specifically, model checking) are other promising technologies. Many researchers have proposed various approaches to model and verify dynamic evolution. However, highly available dynamic evolution processes tend to be too complicated to verify with existing techniques because such processes need to be simultaneously controlled with system functionalities and the operations for evolution that include dynamic aspect weaving. We propose a formal verification tool called CAMPer that analyzes the unified modeling language (UML) models of dynamic evolution processes that consist of multiple steps with sophisticated control that includes dynamic aspect weaving. This tool is able to verify functional requirements for the processes that would be complicated to attain high availability. Our approach uses the Maude specification language to systematically express dynamic evolution and dynamic aspect weaving by using reflection. We also used a model checker for Maude to verify the evolution processes. We conducted experiments using an example Tele Assistance System (TAS) to demonstrate the advantages of our approach and evaluate its feasibility.},
booktitle = {Proceedings of the 12th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {152–162},
numpages = {11},
location = {Buenos Aires, Argentina},
series = {SEAMS '17}
}

@article{10.1145/1290993.1291005,
author = {Garcia, Alessandro and Greenwood, Phil and Heineman, George and Walker, Robert and Cai, Yuanfang and Yang, Hong Yul and Baniassad, Elisa and Lopes, Cristina Videira and Schwanninger, Christa and Zhao, Jianjun},
title = {Assessment of Contemporary Modularization Techniques - ACoM'07: workshop report},
year = {2007},
issue_date = {September 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/1290993.1291005},
doi = {10.1145/1290993.1291005},
abstract = {The effective assessment of emerging modularization technologies plays a pivotal role on: (i) a better understanding of their real benefits and drawbacks when compared to conventional development techniques, and (ii) their effective transfer to mainstream software development. This report is intended to summarize the results of the 1st International Workshop on Assessment of Contemporary Modularization Techniques (ACoM'07) held in Minneapolis, USA, May 22, 2007, as part of the 29th International Conference on Software Engineering (ICSE'07). The main purpose of this workshop was to share and pool the collective experience of people interested in and actively working on assessment of innovative modularization techniques. The workshop consisted of an opening presentation, several paper presentations organized into three technical sessions, and four discussion groups. During the workshop presentations and discussions, the authors and participants directly and indirectly reviewed ongoing and previous work and debated a number of important issues on contemporary modularity assessment. The ACoM'07 website, including the electronic version of this report, can be found at &lt;www.comp.lancs.ac.uk/computing/ACoM.07/&gt;. We begin by presenting an overview of our goals and the workshop structure, and then focus on the workshop technical program and results.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {31–37},
numpages = {7}
}

@inproceedings{10.1145/2602576.2602585,
author = {Etxeberria, Leire and Trubiani, Catia and Cortellessa, Vittorio and Sagardui, Goiuria},
title = {Performance-based selection of software and hardware features under parameter uncertainty},
year = {2014},
isbn = {9781450325769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602576.2602585},
doi = {10.1145/2602576.2602585},
abstract = {Configurable software systems allow stakeholders to derive variants by selecting software and/or hardware features. Performance analysis of feature-based systems has been of large interest in the last few years, however a major research challenge is still to conduct such analysis before achieving full knowledge of the system, namely under a certain degree of uncertainty. In this paper we present an approach to analyze the correlation between selection of features embedding uncertain parameters and system performance. In particular, we provide best and worst case performance bounds on the basis of selected features and, in cases of wide gaps among these bounds, we carry on a sensitivity analysis process aimed at taming the uncertainty of parameters. The application of our approach to a case study in the e-health domain demonstrates how to support stakeholders in the identification of system variants that meet performance requirements.},
booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {23–32},
numpages = {10},
keywords = {feature selection, performance analysis, software architectures, uncertainty},
location = {Marcq-en-Bareul, France},
series = {QoSA '14}
}

@inproceedings{10.1145/2593882.2593885,
author = {Orso, Alessandro and Rothermel, Gregg},
title = {Software testing: a research travelogue (2000–2014)},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593885},
doi = {10.1145/2593882.2593885},
abstract = {Despite decades of work by researchers and practitioners on numerous software quality assurance techniques, testing remains one of the most widely practiced and studied approaches for assessing and improving software quality. Our goal, in this paper, is to provide an accounting of some of the most successful research performed in software testing since the year 2000, and to present what appear to be some of the most significant challenges and opportunities in this area. To be more inclusive in this effort, and to go beyond our own personal opinions and biases, we began by contacting over 50 of our colleagues who are active in the testing research area, and asked them what they believed were (1) the most significant contributions to software testing since 2000 and (2) the greatest open challenges and opportunities for future research in this area. While our colleagues’ input (consisting of about 30 responses) helped guide our choice of topics to cover and ultimately the writing of this paper, we by no means claim that our paper represents all the relevant and noteworthy research performed in the area of software testing in the time period considered—a task that would require far more space and time than we have available. Nevertheless, we hope that the approach we followed helps this paper better reflect not only our views, but also those of the software testing community in general.},
booktitle = {Future of Software Engineering Proceedings},
pages = {117–132},
numpages = {16},
keywords = {Software testing},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@proceedings{10.1145/3639474,
title = {ICSE-SEET '24: Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/2721956.2721977,
author = {Kajtazovic, Nermin and Preschern, Christopher and H\"{o}ller, Andrea and Kreiner, Christian},
title = {Towards pattern-based reuse in safety-critical systems},
year = {2014},
isbn = {9781450334167},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2721956.2721977},
doi = {10.1145/2721956.2721977},
abstract = {Challenges such as time-to-market, reduced costs for change and maintenance have radically influenced development of today's safety-critical systems. Many domains have already adopted their system's engineering to support modular and component-based architectures. With the component-based design paradigm, the system engineering is utilized allowing to distribute development among different development teams, however, with the price that there is no full trust in independently developed parts, which makes their reuse challenging. Until now, many approaches that address reuse, on conceptual or detailed level, have been proposed. A very important aspect addressed here is to document the information flow between system parts in detail, i.e. from higher abstraction levels down to the implementation details, in order to put more trust into independently developed parts of the system.In this paper, we describe a compact pattern system with the aim to establish a link between high level concepts for reuse and detailed description of the behavior of system parts. The main goal is to document these details up to the higher levels of abstraction in more systematic way.},
booktitle = {Proceedings of the 19th European Conference on Pattern Languages of Programs},
articleno = {33},
numpages = {15},
location = {Irsee, Germany},
series = {EuroPLoP '14}
}

@article{10.1145/3105906,
author = {Monperrus, Martin},
title = {Automatic Software Repair: A Bibliography},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3105906},
doi = {10.1145/3105906},
abstract = {This article presents a survey on automatic software repair. Automatic software repair consists of automatically finding a solution to software bugs without human intervention. This article considers all kinds of repairs. First, it discusses behavioral repair where test suites, contracts, models, and crashing inputs are taken as oracle. Second, it discusses state repair, also known as runtime repair or runtime recovery, with techniques such as checkpoint and restart, reconfiguration, and invariant restoration. The uniqueness of this article is that it spans the research communities that contribute to this body of knowledge: software engineering, dependability, operating systems, programming languages, and security. It provides a novel and structured overview of the diversity of bug oracles and repair operators used in the literature.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {17},
numpages = {24},
keywords = {Program repair, self-healing software}
}

@proceedings{10.1145/2993236,
title = {GPCE 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.1145/2088876.2088879,
author = {Merle, Philippe and Rouvoy, Romain and Seinturier, Lionel},
title = {A reflective platform for highly adaptive multi-cloud systems},
year = {2011},
isbn = {9781450310703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2088876.2088879},
doi = {10.1145/2088876.2088879},
abstract = {Cloud platforms are increasingly used for hosting a broad diversity of services from traditional e-commerce applications to interactive web-based IDEs. However, we observe that the proliferation of offers by Cloud vendors raises several challenges. Developers will not only have to deploy applications for a specific Cloud, but will also have to consider migrating services from one cloud to another, and to manage applications spanning multiple Clouds. In this paper, we therefore report on a first experiment we conducted to build a multi-Cloud system on top of thirteen existing IaaS/PaaS. From this experiment, we advocate for two dimensions of adaptability---design and execution time---that applications for such systems require to exhibit. Finally, we propose a roadmap for future multi-Cloud systems.},
booktitle = {Adaptive and Reflective Middleware on Proceedings of the International Workshop},
pages = {14–21},
numpages = {8},
location = {Lisbon, Portugal},
series = {ARM '11}
}

@inproceedings{10.5555/2663546.2663573,
author = {Fredericks, Erik M. and Ramirez, Andres J. and Cheng, Betty H. C.},
title = {Towards run-time testing of dynamic adaptive systems},
year = {2013},
isbn = {9781467344012},
publisher = {IEEE Press},
abstract = {It is challenging to design, develop, and validate a dynamically adaptive system (DAS) that satisfies requirements, particularly when requirements can change at run time. Testing at design time can help verify and validate that a DAS satisfies its specified requirements and constraints. While offline tests may demonstrate that a DAS is capable of satisfying its requirements before deployment, a DAS may encounter unanticipated system and environmental conditions that can prevent it from achieving its objectives. In working towards a requirements-aware DAS, this paper proposes run-time monitoring and adaptation of tests as another technique for evaluating whether a DAS satisfies, or is even capable of satisfying, its requirements given its current execution context. To this end, this paper motivates the need and identifies challenges for adaptively testing a DAS at run time, as well as suggests possible methods for leveraging offline testing techniques for verifying run-time behavior.},
booktitle = {Proceedings of the 8th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {169–174},
numpages = {6},
location = {San Francisco, California},
series = {SEAMS '13}
}

@inproceedings{10.1145/2602928.2603080,
author = {Lytra, Ioanna and Sobernig, Stefan and Tran, Huy and Zdun, Uwe},
title = {A pattern language for service-based platform integration and adaptation},
year = {2012},
isbn = {9781450329439},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602928.2603080},
doi = {10.1145/2602928.2603080},
abstract = {Often software systems accommodate one or more software platforms on top of which various applications are developed and executed. Different application areas, such as enterprise resource planning, mobile devices, telecommunications, and so on, require different and specialized platforms. Many of them offer their services using standardized interface technologies to support integration with the applications built on top of them and with other platforms. The diversity of platform technologies and interfaces, however, renders the integration of multiple platforms challenging. In this paper, we discuss design alternatives for tailoring heterogeneous service platforms by studying high-level and low-level architectural design decisions for integrating and for adapting platforms. We survey and organize existing patterns and design decisions in the literature as a pattern language. With this pattern language, we address the various decision categories and interconnections for the service-based integration and the adaptation of applications developed based on software platforms. We apply this pattern language in an industry case study.},
booktitle = {Proceedings of the 17th European Conference on Pattern Languages of Programs},
articleno = {4},
numpages = {27},
keywords = {design patterns, pattern language, service-based platform integration},
location = {Irsee, Germany},
series = {EuroPLoP '12}
}

@inproceedings{10.1145/2517208.2517209,
author = {Siegmund, Norbert and von Rhein, Alexander and Apel, Sven},
title = {Family-based performance measurement},
year = {2013},
isbn = {9781450323734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517208.2517209},
doi = {10.1145/2517208.2517209},
abstract = {Most contemporary programs are customizable. They provide many features that give rise to millions of program variants. Determining which feature selection yields an optimal performance is challenging, because of the exponential number of variants. Predicting the performance of a variant based on previous measurements proved successful, but induces a trade-off between the measurement effort and prediction accuracy. We propose the alternative approach of family-based performance measurement, to reduce the number of measurements required for identifying feature interactions and for obtaining accurate predictions. The key idea is to create a variant simulator (by translating compile-time variability to run-time variability) that can simulate the behavior of all program variants. We use it to measure performance of individual methods, trace methods to features, and infer feature interactions based on the call graph. We evaluate our approach by means of five feature-oriented programs. On average, we achieve accuracy of 98%, with only a single measurement per customizable program. Observations show that our approach opens avenues of future research in different domains, such an feature-interaction detection and testing.},
booktitle = {Proceedings of the 12th International Conference on Generative Programming: Concepts &amp; Experiences},
pages = {95–104},
numpages = {10},
keywords = {family-based analysis, featurehouse, performance prediction},
location = {Indianapolis, Indiana, USA},
series = {GPCE '13}
}

@inproceedings{10.1145/1370175.1370249,
author = {Avgeriou, Paris and Lago, Patricia and Kruchten, Philippe},
title = {Third international workshop on sharing and reusing architectural knowledge (SHARK 2008)},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370175.1370249},
doi = {10.1145/1370175.1370249},
abstract = {The shift of the software architecture community towards architectural knowledge has brought along some promising research directions. In this workshop we discuss the issues that lead to the application of architectural knowledge in research and industrial practice; ongoing research and new ideas to advance the field. In its previous editions we examined the state of the art and practice, future challenges and trends. This third edition will discuss, among others, architectural knowledge as perceived by different research communities, including requirements engineering, service-oriented computing and international standardization.},
booktitle = {Companion of the 30th International Conference on Software Engineering},
pages = {1065–1066},
numpages = {2},
keywords = {architectural knowledge},
location = {Leipzig, Germany},
series = {ICSE Companion '08}
}

@inproceedings{10.1145/1596495.1596500,
author = {Peper, Christian and Schneider, Daniel},
title = {On runtime service quality models in adaptive ad-hoc systems},
year = {2009},
isbn = {9781605586816},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1596495.1596500},
doi = {10.1145/1596495.1596500},
abstract = {Ad-hoc computer systems can automatically realize higher services when at least two distributed and communicating (embedded) devices come together. For this purpose, they must able to manage appearance and loss of devices and resources, and they have to adapt to changes in requirements and environment. Based on a component-oriented approach for adaptive ad-hoc systems, this paper suggests a high-level service quality reference model to advocate further research on the quality matching problem between service provider and client components.},
booktitle = {Proceedings of the 2009 ESEC/FSE Workshop on Software Integration and Evolution @ Runtime},
pages = {11–18},
numpages = {8},
keywords = {ad-hoc systems, adaptivity, ambient intelligence, component-orientation, distributed systems, quality-of-service, ubiquitous computing},
location = {Amsterdam, The Netherlands},
series = {SINTER '09}
}

@inproceedings{10.5555/2337223.2337381,
author = {Budgen, David and Drummond, Sarah and Brereton, Pearl and Holland, Nikki},
title = {What scope is there for adopting evidence-informed teaching in SE?},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Context: In teaching about software engineering we currently make little use of any empirical knowledge. Aim: To examine the outcomes available from the use of Evidence-Based Software Engineering (EBSE) practices, so as to identify where these can provide support for, and inform, teaching activities. Method: We have examined all known secondary studies published up to the end of 2009, together with those published in major journals to mid-2011, and identified where these provide practical results that are relevant to student needs. Results: Starting with 145 candidate systematic literature reviews (SLRs), we were able to identify and classify potentially useful teaching material from 43 of them. Conclusions: EBSE can potentially lend authority to our teaching, although the coverage of key topics is uneven. Additionally, mapping studies can provide support for research-led teaching.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {1205–1214},
numpages = {10},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.5555/1753235.1753255,
author = {Sun, Hongyu and Lutz, Robyn R. and Basu, Samik},
title = {Product-line-based requirements customization for web service compositions},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Customizing web services according to users' individual functional and non-functional requirements has become increasingly difficult as the number of users increases. This paper introduces a new way to customize and verify composite web services by incorporating a software product-line engineering approach into web-service composition. The approach uses a partitioning similar to that between domain engineering and application engineering in the product-line context. It specifies the options that the user can select and constructs the resulting web-service compositions. By first creating a web-service composition search space that satisfies the common requirements and then querying the search space as the user selects values for the parameters of variation, we provide a more efficient way to customize web services. A decision model, illustrated with examples from an emergency-response application, is created to interact with the customers and ensure the consistency of their specifications. The capability to reuse the composition search space may also help improve the quality and reliability of the composite services and reduce the cost of re-verifying the same compositions.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {141–150},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@proceedings{10.1145/3674805,
title = {ESEM '24: Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
year = {2024},
isbn = {9798400710476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Barcelona, Spain}
}

@inproceedings{10.5555/1762828.1762850,
author = {Graf, Susanne and G\'{e}rard, S\'{e}bastien and Haugen, \O{}ystein and Ober, Iulian and Selic, Bran},
title = {Modelling and analysis of real time and embedded systems: using UML},
year = {2006},
isbn = {9783540694885},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents an overview on the outcomes of the workshop MARTES on Modelling and Analysis of Real Time and Embedded Systems that has taken place for the second time in association with the MoDELS/UML 2006 conference. Important themes discussed at this workshop concerned (1) tools for analysis and model transformation and (2) concepts for modelling quantitative aspects with the perspective of analysis.},
booktitle = {Proceedings of the 2006 International Conference on Models in Software Engineering},
pages = {126–130},
numpages = {5},
keywords = {analysis, embedded systems, modelling, real time},
location = {Genoa, Italy},
series = {MoDELS'06}
}

@inproceedings{10.1145/2047862.2047875,
author = {Steck, Andreas and Lotz, Alex and Schlegel, Christian},
title = {Model-driven engineering and run-time model-usage in service robotics},
year = {2011},
isbn = {9781450306898},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047862.2047875},
doi = {10.1145/2047862.2047875},
abstract = {The development of service robots has gained more and more attention over the last years. A major challenge on the way towards industrial-strength service robotic systems is to make the step from code-driven to model-driven engineering. In this work we propose to put models into the focus of the whole life-cycle of robotic systems covering design-time as well as run-time. We describe how to explicate parameters, properties and resource information in the models at design-time and how to take these information into account by the run-time system of the robot to support its decision making process. We underpin our work by an exhaustive real-world example which is completely developed with our tools.},
booktitle = {Proceedings of the 10th ACM International Conference on Generative Programming and Component Engineering},
pages = {73–82},
numpages = {10},
keywords = {component based software engineering, model-driven engineering, run-time model usage, service robotics},
location = {Portland, Oregon, USA},
series = {GPCE '11}
}

@inproceedings{10.1145/2019136.2019178,
author = {Brataas, Gunnar and Jiang, Shanshan and Reichle, Roland and Geihs, Kurt},
title = {Performance property prediction supporting variability for adaptive mobile systems},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019178},
doi = {10.1145/2019136.2019178},
abstract = {A performance property prediction (PPP) method for component-based self-adaptive applications is presented. Such performance properties are required by an adaptation middleware for reasoning about adaptation activities. Our PPP method is based on the Structure and Performance (SP) framework, a conceptually simple, yet powerful performance modelling framework based on matrices. The main contribution of this paper are the integration of SP-based PPP into a comprehensive model- and variability-based adaptation framework for context-aware mobile applications. A meta model for the SP method is described. The framework is demonstrated using a practical example.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {37},
numpages = {8},
keywords = {autonomic computing, mobile systems},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2304696.2304711,
author = {Huber, Nikolaus and Brosig, Fabian and Kounev, Samuel},
title = {Modeling dynamic virtualized resource landscapes},
year = {2012},
isbn = {9781450313469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2304696.2304711},
doi = {10.1145/2304696.2304711},
abstract = {Modern data centers are subject to an increasing demand for flexibility. Increased flexibility and dynamics, however, also result in a higher system complexity. This complexity carries on to run-time resource management for Quality-of-Service (QoS) enforcement, rendering design-time approaches for QoS assurance inadequate. In this paper, we present a set of novel meta-models that can be used to describe the resource landscape, the architecture and resource layers of dynamic virtualized data center infrastructures, as well as their run-time adaptation and resource management aspects. With these meta-models we introduce new modeling concepts to improve model-based run-time QoS assurance. We evaluate our meta-models by modeling a representative virtualized service infrastructure and using these model instances for run-time resource allocation. The results demonstrate the benefits of the new meta-models and show how they can be used to improve model-based system adaptation and run-time resource management in dynamic virtualized data centers.},
booktitle = {Proceedings of the 8th International ACM SIGSOFT Conference on Quality of Software Architectures},
pages = {81–90},
numpages = {10},
keywords = {meta-model, resources, virtualization},
location = {Bertinoro, Italy},
series = {QoSA '12}
}

@proceedings{10.1145/3564719,
title = {GPCE 2022: Proceedings of the 21st ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2022},
isbn = {9781450399203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 21st ACM SIGPLAN International Conference on Generative Programming: Concept &amp; Experiences (GPCE 2022) held on December 6th and 7th, 2022 in Auckland, New Zealand. GPCE is the premiere venue for researchers and practitioners interested in techniques that use program generation, domain-specific languages, and component deployment to increase programmer productivity, improve software quality, and shorten the time-to-market of software products. In addition to exploring cutting-edge techniques of generative software, GPCE seeks to foster cross-fertilization between software engineering and programming language.},
location = {Auckland, New Zealand}
}

@inproceedings{10.1145/2593882.2593895,
author = {Hatcliff, John and Wassyng, Alan and Kelly, Tim and Comar, Cyrille and Jones, Paul},
title = {Certifiably safe software-dependent systems: challenges and directions},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593895},
doi = {10.1145/2593882.2593895},
abstract = {The amount and impact of software-dependence in critical systems impinging on daily life is increasing rapidly. In many of these systems, inadequate software and systems engineering can lead to economic disaster, injuries or death. Society generally does not recognize the potential of losses from deficiencies of systems due to software until after some mishap occurs. Then there is an outcry, reflecting societal expectations; however, few know what it takes to achieve the expected safety and, in general, loss-prevention.  On the one hand there are unprecedented, exponential increases in size, inter-dependencies, intricacies, numbers and variety in the systems and distribution of development processes across organizations and cultures. On the other hand, industry's capability to verify and validate these systems has not kept up. Mere compliance with existing standards, techniques, and regulations cannot guarantee the safety properties of these systems. The gap between practice and capability is increasing rapidly.  This paper considers the future of software engineering as needed to support development and certification of safety-critical software-dependent systems. We identify a collection of challenges and document their current state, the desired state, gaps and barriers to reaching the desired state, and potential directions in software engineering research and education that could address the gaps and barriers.},
booktitle = {Future of Software Engineering Proceedings},
pages = {182–200},
numpages = {19},
keywords = {Certification, assurance, hazard analysis, requirements, safety, standards, validation, verification},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@inproceedings{10.1145/336512.336584,
author = {Boehm, Barry W. and Sullivan, Kevin J.},
title = {Software economics: a roadmap},
year = {2000},
isbn = {1581132530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/336512.336584},
doi = {10.1145/336512.336584},
booktitle = {Proceedings of the Conference on The Future of Software Engineering},
pages = {319–343},
numpages = {25},
location = {Limerick, Ireland},
series = {ICSE '00}
}

@inproceedings{10.1145/1233901.1233907,
author = {Lohmann, Daniel and Streicher, Jochen and Spinczyk, Olaf and Schr\"{o}der-Preikschat, Wolfgang},
title = {Interrupt synchronization in the CiAO operating system: experiences from implementing low-level system policies by AOP},
year = {2007},
isbn = {9781595936578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1233901.1233907},
doi = {10.1145/1233901.1233907},
abstract = {Configurability is a major issue in the domain of embedded system software. Existing systems specifically lack good techniques to implement configurability of architectural OS concerns, such as the choice of isolation or synchronization policies to use. As such policies have a very cross-cutting character, aspects should provide good means to implement them in a configurable way. While our results show that this is in fact the case, 1) things could have been easier if additional language features were available, and, 2) additional means to influence the back-end code generation turned out to be very important. This paper presents our experiences in using AspectC++ to design and implement interrupt synchronization as a configurable property in the CiAO operating system.},
booktitle = {Proceedings of the 6th Workshop on Aspects, Components, and Patterns for Infrastructure Software},
pages = {6–es},
keywords = {AspectC++, CiAO, aspect-aware operating system, aspect-oriented programming (AOP), configurability},
location = {Vancouver, British Columbia, Canada},
series = {ACP4IS '07}
}

@inproceedings{10.1145/1173706.1173740,
author = {Leavens, Gary T. and Abrial, Jean-Raymond and Batory, Don and Butler, Michael and Coglio, Alessandro and Fisler, Kathi and Hehner, Eric and Jones, Cliff and Miller, Dale and Peyton-Jones, Simon and Sitaraman, Murali and Smith, Douglas R. and Stump, Aaron},
title = {Roadmap for enhanced languages and methods to aid verification},
year = {2006},
isbn = {1595932372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1173706.1173740},
doi = {10.1145/1173706.1173740},
abstract = {This roadmap describes ways that researchers in four areas---specification languages, program generation, correctness by construction, and programming languages---might help further the goal of verified software. It also describes what advances the "verified software" grand challenge might anticipate or demand from work in these areas. That is, the roadmap is intended to help foster collaboration between the grand challenge and these research areas.A common goal for research in these areas is to establish language designs and tool architectures that would allow multiple annotations and tools to be used on a single program. In the long term, researchers could try to unify these annotations and integrate such tools.},
booktitle = {Proceedings of the 5th International Conference on Generative Programming and Component Engineering},
pages = {221–236},
numpages = {16},
keywords = {annotations, correctness by construction, program generation, programming languages, specification languages, tools, verification, verified software grand challenge},
location = {Portland, Oregon, USA},
series = {GPCE '06}
}

@inproceedings{10.5555/1787553.1787567,
author = {Boella, Guido and Steimann, Friedrich},
title = {Roles and relationships in object-oriented programming, multiagent systems and ontologies: report on the 2nd workshop on roles and relationships at ECOOP 2007},
year = {2007},
isbn = {3540781943},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This report describes the "Roles'07--Roles and Relationships" workshop held at ECOOP'07 in Berlin on July 30 and 31, 2007. The aims and organization of the workshop are described, and the main contributions of the presentations and invited talks are summarized, so to have a useful survey of current issues in the field. The description of the discussion and conclusions end the paper.},
booktitle = {Proceedings of the 2007 Conference on Object-Oriented Technology},
pages = {108–122},
numpages = {15},
location = {Berlin, Germany},
series = {ECOOP'07}
}

@proceedings{10.1145/3567512,
title = {SLE 2022: Proceedings of the 15th ACM SIGPLAN International Conference on Software Language Engineering},
year = {2022},
isbn = {9781450399197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 15th ACM SIGPLAN International Conference on Software Language Engineering (SLE), co-located with the ACM SIGPLAN conference on Systems, Programming, Languages, and Applications (SPLASH) in Auckland, a vibrant port city in northern New Zealand, from December 5th to December 10th 2022. Like its predecessors, the this edition of the SLE conference, SLE 2022, is devoted to the principles of software languages: their design, their implementation, and their evolution. As such, SLE brings together researchers united by their common interest in the creation, capture, and tooling of software languages.},
location = {Auckland, New Zealand}
}

@inproceedings{10.1145/2451436.2451442,
author = {Salvaneschi, Guido and Mezini, Mira},
title = {Reactive behavior in object-oriented applications: an analysis and a research roadmap},
year = {2013},
isbn = {9781450317665},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2451436.2451442},
doi = {10.1145/2451436.2451442},
abstract = {Reactive applications are difficult to implement. Traditional solutions based on event systems and the Observer pattern have a number of inconveniences, but programmers bear them in return for the benefits of OO design. On the other hand, reactive approaches based on automatic updates of dependencies - like functional reactive programming and dataflow languages - provide undoubted advantages but do not fit well with mutable objects. In this paper, we provide a research roadmap to overcome the limitations of the current approaches and to support reactive applications in the OO setting. To establish a solid background for our investigation, we propose a conceptual framework to model the design space of reactive applications and we study the flaws of the existing solutions. Then we highlight how reactive languages have the potential to address those issues and we formulate our research plan.},
booktitle = {Proceedings of the 12th Annual International Conference on Aspect-Oriented Software Development},
pages = {37–48},
numpages = {12},
keywords = {functional-reactive programming, incremental computation, object-oriented programming, reactive programming},
location = {Fukuoka, Japan},
series = {AOSD '13}
}

@inproceedings{10.1007/978-3-642-27216-5_11,
author = {Nunes, Ingrid and Barbosa, Simone Diniz Junqueira and de Lucena, Carlos J. P.},
title = {Dynamically adapting BDI agents based on high-level user specifications},
year = {2011},
isbn = {9783642272158},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-27216-5_11},
doi = {10.1007/978-3-642-27216-5_11},
abstract = {Users are facing an increasing challenge of managing information and being available anytime anywhere, as the web exponentially grows. As a consequence, assisting them in their routine tasks has become a relevant issue to be addressed. In this paper, we introduce an adaptation mechanism that is responsible for dynamically adapting a BDI agent-based running system in order to support software customisation for users. This mechanism is used within a software framework for supporting the development of Personal Assistance Software (PAS), which relies on the idea of exposing a high-level user model to empower users to manage it as well as increase user trust in the task delegation process.},
booktitle = {Proceedings of the 10th International Conference on Advanced Agent Technology},
pages = {139–163},
numpages = {25},
keywords = {BDI, framework, personal assistance software, software adaptation, user modeling},
location = {Taipei, Taiwan},
series = {AAMAS'11}
}

@proceedings{10.1145/3639478,
title = {ICSE-Companion '24: Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ICSE is the leading and, by far, the largest conference in Software Engineering, attracting researchers, practitioners, and students worldwide. ICSE2024 is co-located with 11 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3593434,
title = {EASE '23: Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Oulu, Finland}
}

@proceedings{10.1145/3555776,
title = {SAC '23: Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tallinn, Estonia}
}

@proceedings{10.1145/3641399,
title = {ISEC '24: Proceedings of the 17th Innovations in Software Engineering Conference},
year = {2024},
isbn = {9798400717673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bangalore, India}
}

@proceedings{10.1145/3623476,
title = {SLE 2023: Proceedings of the 16th ACM SIGPLAN International Conference on Software Language Engineering},
year = {2023},
isbn = {9798400703966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 16th ACM SIGPLAN International Conference on Software Language Engineering (SLE) held in October 2023 as part of SPLASH 2023. Software Language Engineering (SLE) is a thriving research discipline targeted at establishing an engineering approach to the development, use, and maintenance of software languages, that is, of languages for the specification, modeling and tooling of software. Key topics of interest for SLE include approaches, methodologies and tools for language design and implementation with a focus on techniques for static and behavioral semantics, generative or interpretative approaches (including transformation languages and code generation) as well as meta-languages and tools (including language workbenches). Techniques enabling the testing, simulation or formal verification for language validation purposes are also of particular interest. SLE also accommodates empirical evaluation and experience reports of language engineering tools, such as user studies evaluating usability, performance benchmarks or industrial applications.},
location = {Cascais, Portugal}
}

@techreport{10.1145/2965631,
author = {The Joint Task Force on Computing Curricula},
title = {Curriculum Guidelines for Undergraduate Degree Programs in Software Engineering},
year = {2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The primary purpose of this volume is to provide guidance to academic institutions and accreditation agencies about what should constitute an undergraduate software engineering education. These recommendations have been developed by a broad, internationally based group of volunteer participants. This group has taken into account much of the work that has been done in software engineering education over the last quarter of a century. Software engineering curriculum recommendations are of particular relevance, since there is currently a surge in the creation of software engineering degree programs and accreditation processes for such programs have been established in a number of countries.}
}

@proceedings{10.1145/3555228,
title = {SBES '22: Proceedings of the XXXVI Brazilian Symposium on Software Engineering},
year = {2022},
isbn = {9781450397353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Brazil}
}

@article{10.1145/1050849.1057988,
author = {ACM SIGSOFT Software Engineering Notes staff},
title = {Frontmatter (TOC, Letters, Open Source Software (OSS) Patent Search Engine, Calendar of Events, Workshop and Conference Information)},
year = {2005},
issue_date = {March 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/1050849.1057988},
doi = {10.1145/1050849.1057988},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {0},
numpages = {19}
}

@proceedings{10.1145/3629479,
title = {SBQS '23: Proceedings of the XXII Brazilian Symposium on Software Quality},
year = {2023},
isbn = {9798400707865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bras\'{\i}lia, Brazil}
}

@proceedings{10.5555/3623293,
title = {ICSE-SEIP '23: Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
location = {Melbourne, Australia}
}

@proceedings{10.1145/3551349,
title = {ASE '22: Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
year = {2022},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rochester, MI, USA}
}

@article{10.1145/638750.638775,
author = {Boehm, Barry},
title = {Value-based software engineering: reinventing},
year = {2003},
issue_date = {March 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/638750.638775},
doi = {10.1145/638750.638775},
abstract = {The Value-Based Software Engineering (VBSE) agenda described in the preceding article has the objectives of integrating value considerations into current and emerging software engineering principles and practices, and of developing an overall framework in which they compatibly reinforce each other. In this paper, we provide a case study illustrating some of the key VBSE practices, and focusing on a particular anomaly in the monitoring and control area: the "Earned Value Management System." This is a most useful technique for monitoring and controlling the cost, schedule, and progress of a complex project. But it has absolutely nothing to say about the stakeholder value of the system being developed. The paper introduces an example order-processing software project, and shows how the use of Benefits Realization Analysis, stake-holder value proposition elicitation and reconciliation, and business case analysis provides a framework for stakeholder-earned-value monitoring and control.},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {3},
numpages = {12}
}

@proceedings{10.1145/3578527,
title = {ISEC '23: Proceedings of the 16th Innovations in Software Engineering Conference},
year = {2023},
isbn = {9798400700644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Allahabad, India}
}

@proceedings{10.1145/3597926,
title = {ISSTA 2023: Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to ISSTA 2023, the 32nd edition of the International Symposium on Software Testing and Analysis, to be held on July 18–20, 2023 in Seattle, USA. The symposium has become a premier scientific event in the expanding area of software testing and analysis, with a strong appeal to researchers from all continents.},
location = {Seattle, WA, USA}
}

@proceedings{10.1145/3592813,
title = {SBSI '23: Proceedings of the XIX Brazilian Symposium on Information Systems},
year = {2023},
isbn = {9798400707599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Macei\'{o}, Brazil}
}

@proceedings{10.5555/3606010,
title = {ICSE '23: Proceedings of the 45th International Conference on Software Engineering},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Victoria, Australia}
}

@proceedings{10.1145/3583133,
title = {GECCO '23 Companion: Proceedings of the Companion Conference on Genetic and Evolutionary Computation},
year = {2023},
isbn = {9798400701207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {GECCO is the largest peer-reviewed conference in the field of Evolutionary Computation, and the main conference of the Special Interest Group on Genetic and Evolutionary Computation (SIGEVO) of the Association for Computing Machinery (ACM).},
location = {Lisbon, Portugal}
}

@article{10.1145/1218776.1218777,
author = {ACM SIGSOFT Software Engineering Notes staff},
title = {Frontmatter (TOC, Miscellaneous material)},
year = {2006},
issue_date = {November 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/1218776.1218777},
doi = {10.1145/1218776.1218777},
journal = {SIGSOFT Softw. Eng. Notes},
month = nov,
pages = {0},
numpages = {36}
}

@proceedings{10.1145/2970276,
title = {ASE '16: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@proceedings{10.1145/3658271,
title = {SBSI '24: Proceedings of the 20th Brazilian Symposium on Information Systems},
year = {2024},
isbn = {9798400709968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Juiz de Fora, Brazil}
}

@proceedings{10.1145/2986012,
title = {Onward! 2016: Proceedings of the 2016 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
year = {2016},
isbn = {9781450340762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@article{10.1145/1082983.1085124,
title = {Frontmatter (TOC, Letters, Election results, Software Reliability Resources!, Computing Curricula 2004 and the Software Engineering Volume SE2004, Software Reuse Research, ICSE 2005 Forward)},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1082983.1085124},
doi = {10.1145/1082983.1085124},
journal = {SIGSOFT Softw. Eng. Notes},
month = jul,
pages = {0},
numpages = {63}
}

@proceedings{10.1145/2950290,
title = {FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

