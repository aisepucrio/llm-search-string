@inproceedings{10.1145/3646548.3672583,
author = {Feichtinger, Mark and Muehlmann, Ulrich},
title = {Applying Software Product Line Techniques to an Integrated Circuit System Modelling Framework: An Experience Report},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672583},
doi = {10.1145/3646548.3672583},
abstract = {Integrated circuits (ICs) are an integral part of modern electronic hardware devices. The design cycle for ICs is getting shorter and shorter as the market demands new generations of ICs that are more reliable, use less energy and are equipped with new functionalities. Additionally, only a single, first-time-right, tape-out is targeted in the design cycle to save high production costs. That means that all effort needs to be put on model-based system design and verification far before the IC enters production, validation, and customer release. As a key requirement, the IC models must predict the performance and the behavior of the IC more accurately and all possible failure modes must be identified and eliminated upfront. In this short paper, we describe how we applied Software Product Line (SPL) techniques to our existing IC model simulation framework to support these challenges. Utilizing SPL, we split the simulation flows and IC settings from the front end parameters and IC model components to manage runtime and compile time variability more efficiently. In each simulation, the front end and IC model components for the IC generation of interest are loaded. With this approach we execute a high number of simulations of multiple different parameters and settings in parallel. As a result, we simulate more scenarios in less time. We show that this approach saves our team two to three weeks when setting up new IC revisions while mitigating redundant code. Additionally, we can compile subsets of the simulation framework more easily to deploy stand-alone debugging and system design tools for internal and external customers. With this work we want the emphasize the use of SPL techniques beyond traditional software engineering domain.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {164–169},
numpages = {6},
keywords = {IC model, behavior model, managing variability, model-based system design, software product line, system model, variability},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3646548.3672597,
author = {Purandare, Salil and Cohen, Myra B.},
title = {Exploration of Failures in an sUAS Controller Software Product Line},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672597},
doi = {10.1145/3646548.3672597},
abstract = {Small uncrewed aerial systems (sUAS) are growing in their use for commercial, scientific, recreational, and emergency management purposes. A critical part of a successful flight is a correctly tuned controller which manages the physics of the vehicle. If improperly configured, it can lead to flight instability, deviation, or crashes. These types of misconfigurations are often within the valid ranges specified in the documentation; hence, they are hard to identify. Recent research has used fuzzing or explored only a small part of the parameter space, providing little understanding of the configuration landscape itself. In this work we leverage software product line engineering to model a subset of the parameter space of a widely used flight control software, using it to guide a systematic exploration of the controller space. Via simulation, we test over 20,000 configurations from a feature model with 50 features and 8.88 \texttimes{} 1034 products, covering all single parameter value changes and all pairs of changes from their default values. Our results show that only a small number of single configuration changes fail (15%), however almost 40% fail when we evaluate changes to two-parameters at a time. We explore the interactions between parameters in more detail, finding what appear to be many dependencies and interactions between parameters which are not well documented. We then explore a smaller, exhaustive product line model, with eight of the most important features (and 6,561 configurations) and uncover a complex set of interactions; over 48% of all configurations fail.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {125–135},
numpages = {11},
keywords = {Configurability, Software Product Lines, sUAS},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3646548.3672587,
author = {Becker, Martin and Rabiser, Rick and Botterweck, Goetz},
title = {Not Quite There Yet: Remaining Challenges in Systems and Software Product Line Engineering as Perceived by Industry Practitioners},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672587},
doi = {10.1145/3646548.3672587},
abstract = {Research on system and software product line engineering (SPLE) and the community around it have been inspired by industrial applications. However, despite decades of research, industry is still struggling with adopting product line approaches and more generally with managing system variability. We argue that it is essential to better understand why this is the case. Particularly, we need to understand the current challenges industry is facing wrt. adopting SPLE practices, how far existing research helps industry practitioners to cope with their challenges, and where additional research would be required. We conducted a hybrid workshop at the 2023 Systems and Software Product Line Conference (SPLC) with over 30 participants from industry and academia. 9 companies from diverse domains and in different phases of SPLE adoption presented their context and perceived challenges. We grouped, discussed, and rated the relevance of the articulated challenges. We then formed clusters of relevant research topics to discuss existing literature as well as research opportunities. In this paper, we report the industry cases, the identified challenges and clusters of research topics, provide pointers to existing work, and discuss research opportunities. With this, we want to enable industry practitioners to become aware of typical challenges and find their way into the existing body of knowledge and to relevant fields of research.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {179–190},
numpages = {12},
keywords = {Software product line engineering, industry challenges},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@article{10.1145/3695987,
author = {Oliveria de Souza, Leandro and Santana de Almeida, Eduardo and Silveira Neto, Paulo Anselmo da Mota and Barr, Earl T. and Petke, Justyna},
title = {Software Product Line Engineering via Software Transplantation},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3695987},
doi = {10.1145/3695987},
abstract = {Software Product Lines (SPLs) improve time-to-market, enhance software quality, and reduce maintenance costs. Current SPL reengineering practices are largely manual and require domain knowledge. Thus, adopting and, to a lesser extent, maintaining SPLs are expensive tasks, preventing many companies from enjoying their benefits. To address these challenges, we introduce Foundry, an approach utilising software transplantation to reduce the manual effort of SPL adoption and maintenance. Foundry enables integrating features across different codebases, even codebases that are unaware that they are contributing features to a software product line. Each product produced by Foundry is pure code, without variability annotation, unlike feature flags, which eases variability management and reduces code bloat.We realise Foundry in prodScalpel, a tool that transplants multiple organs (i.e., a set of interesting features) from donor systems into an emergent product line for codebases written in C. Given tests and lightweight annotations identifying features and implantation points, prodScalpel automates feature extraction and integration. To evaluate its effectiveness, our evaluation compares feature transplantation using prodScalpel to the current state of practice: on our dataset, prodScalpel’s use speeds up feature migration by an average of 4.8 times when compared to current practice.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {31},
numpages = {27},
keywords = {Software Product Lines, Software Transplantation, Genetic Improvement}
}

@inproceedings{10.1145/3646548.3676551,
author = {Ramos-Vidal, Delfina},
title = {Advancing Legacy Software Modernization through Software Product Line Engineering: A Case Study in Digital Libraries},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676551},
doi = {10.1145/3646548.3676551},
abstract = {Legacy Document Information Systems (DIS) pose significant challenges due to their outdated architectures and the criticality of their business functions. This paper proposes a novel approach utilizing Software Product Line Engineering (SPLE) to modernize DIS, aiming to reduce development time and costs while enhancing system sustainability and adaptability. The framework integrates strategies for database migration and evolution modeling, crucial for seamless modernization. Case studies involving real-world digital libraries validate the framework’s effectiveness, comparing it against traditional methods. Results indicate substantial improvements in efficiency and cost-effectiveness, positioning SPLE as a promising strategy for legacy system modernization in digital library domains.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {17–21},
numpages = {5},
keywords = {Digital Libraries, Document Information Systems, Legacy Systems, Modernization, Software Product Lines},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3646548.3676537,
author = {Marinho, Euler and Ferreira, Fischer and Fernandes, Eduardo and Diniz, Jo\~{a}o Paulo and Figueiredo, Eduardo},
title = {Resource Interaction Failures in Mobile Applications: A Challenge for the Software Product Line Testing Community},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676537},
doi = {10.1145/3646548.3676537},
abstract = {Context: Many mobile applications run on multiple platforms with specific available resources. These resources are associated with communication capabilities, sensors, and user customization. Certain resource combinations imply interactions between resources that are likely to produce failures in mobile applications, thereby harming the user experience. Challenge: There may be a large number of resource combinations for a single mobile application. Consequently, exhaustively testing resource interactions to spot failures can be very challenging. However, in order to address this challenge, having robust, well-documented, and publicly available datasets for mobile application testing is necessary. Proposal: This paper proposes the Resource Interaction Challenge targeting mobile applications. We introduce a curated dataset of 20 mobile applications with varying sizes (up to 350K lines of code) and required resources (Bluetooth, Wi-Fi, etc.). Due to the shortage of sampling strategies for testing resource interactions in mobile applications, we opted for strategies commonly used for configurable systems in general. Our dataset includes failures detected and source code metrics computed for each mobile application. Conclusion: We expect to engage both researchers and practitioners in reusing our dataset, especially to propose and evaluate novel testing strategies.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {203–208},
numpages = {6},
keywords = {Mobile Application Testing, Resource Interaction Failures;},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3503229.3547055,
author = {Friesel, Birte and M\"{u}ller, Michael and Ferraz, Matheus and Spinczyk, Olaf},
title = {On the relation of variability modeling languages and non-functional properties},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547055},
doi = {10.1145/3503229.3547055},
abstract = {Non-functional properties (NFPs) such as code size (RAM, ROM), performance, and energy consumption are at least as important as functional properties in many software development domains. When configuring a software product line - especially in the area of resource-constrained embedded systems - developers must be aware of the NFPs of the configured product instance. Several NFP-aware variability modeling languages have been proposed to address this in the past. However, it is not clear whether a variability modeling language is the best place for handling NFP-related concerns, or whether separate NFP prediction models should be preferred. We shine light onto this question by discussing limitations of state-of-the-art NFP-aware variability modeling languages, and find that both in terms of the development process and model accuracy a separate NFP model is favorable. Our quantitative analysis is based on six different software product lines, including the widely used busybox multi-call binary and the x264 video encoder. We use classification and regression trees (CART) and our recently proposed Regression Model Trees [8] as separate NFP models. These tree-based models can cover the effects of arbitrary feature interactions and thus easily outperform variability models with static, feature-wise NFP annotations. For example, when estimating the throughput of an embedded AI product line, static annotations come with a mean generalization error of 114.5% while the error of CART is only 9.4 %.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {140–144},
numpages = {5},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3579028.3609012,
author = {Rosiak, Kamil and Schaefer, Ina},
title = {The e4CompareFramework: Annotation-based Software Product-Line Extraction},
year = {2023},
isbn = {9798400700927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579028.3609012},
doi = {10.1145/3579028.3609012},
abstract = {Software product-line engineering (SPLE) provides structured reuse strategies reducing the time-to-market and decreasing development and maintenance effort when developing variant-rich software systems. In practice, however, unstructured reuse strategies such as clone-and-own are frequently used. While copying, pasting, and modifying artifacts to create new variants seems straightforward, it increases the maintenance effort. SPLE requires the extraction of a software product-line (SPL) from existing variants. However, this task needs complex analyses that are not feasible manually, making tool support crucial.This paper presents a process for annotation-based extraction of an SPL from related variants with a detailed source code analysis starting from the expression level implemented as the e4CompareFramework (e4C). The e4CompareFramework provides a customizable detailed variability mining method, a generalization from our previous work. In addition, we implemented a feature location technique and an annotation-based feature model extraction technique to provide a complete SPL extraction process.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
pages = {34–38},
numpages = {5},
keywords = {variability mining, software product-line extraction, reverse engineering},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3382025.3414971,
author = {Martinez, Jabier and Wolfart, Daniele and Assun\c{c}\~{a}o, Wesley K. G. and Figueiredo, Eduardo},
title = {Insights on software product line extraction processes: ArgoUML to ArgoUML-SPL revisited},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414971},
doi = {10.1145/3382025.3414971},
abstract = {Software Product Lines (SPLs) are rarely developed from scratch. Commonly, they emerge from monolithic architectures when there is a need to create tailored variants, or from existing variants created in an ad-hoc way once their separated maintenance and evolution become challenging. Despite the vast literature about re-engineering systems into SPLs and related technical approaches, there is a lack of detailed analysis about the process itself and the effort that is involved. We provide and analyze empirical data of an existing SPL extraction process: the ArgoUML monolithic architecture transition to ArgoUML-SPL. The analysis relies on information mined from the version control history of the source-code repository and the discussion with developers that took part in the process. The contribution of this study is an in-depth characterization of the process compared to previous works that focused only on the structural results of the final SPL. We made publicly available the dataset and the analysis scripts to be used as baseline for extractive SPL adoption research and practice.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {6},
numpages = {6},
keywords = {software product line architecture, re-engineering, mining software repositories, ArgoUML},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3461001.3473060,
author = {Sch\"{a}fer, Andreas and Becker, Martin and Andres, Markus and Kistenfeger, Tim and Rohlf, Florian},
title = {Variability realization in model-based system engineering using software product line techniques: an industrial perspective},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3473060},
doi = {10.1145/3461001.3473060},
abstract = {Efficiently handling system variants is rising of importance in industry and challenges the application of model-based systems engineering.This paper reveals the increasing industrial demand of guidance and decision support on how to handle variants and variability within SysML and UML models. While a substantial amount of variability realization approaches has already been published on source code level, there is little guidance for practitioners on system model level. Hence, there is major uncertainty in dealing with system changes or concurrent system modeling of related system. Due to a poor modularization and variability realization these model variants are ending up in interwoven and complex system models.In this paper, we aim to raise awareness of the need for appropriate guidance and decision support, identify important contextual factors of MBSE that influence variability realization, and derive well known variability mechanisms used in software coding for their applicability in system modeling.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {25–34},
numpages = {10},
keywords = {SysML, UML, decision support, model-based systems engineering, system and software product line engineering, variability mechanism, variability realization, variant management},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3106195.3106205,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green Configurations of Functional Quality Attributes},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106205},
doi = {10.1145/3106195.3106205},
abstract = {Functional quality attributes (FQAs) are those quality attributes that, to be satisfied, require the incorporation of additional functionality into the application architecture. By adding an FQA (e.g., security) we can improve the quality of the final product, but there is also an increase in energy consumption. This paper proposes a solution to help the software architect to generate configurations of FQAs whilst keeping the energy consumed by the application as low as possible. For this, a usage model is defined for each FQA, taking into account the variables that affect the energy consumption, and that the values of these variables change according to the part of the application where the FQA is required. We extend a Software Product Line that models a family of FQAs to incorporate the variability of the usage model and the existing frameworks that implement FQAs. We generate the most eco-efficient configuration of FQAs by selecting the framework with the most suitable characteristics according to the requirements of the application.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {79–83},
numpages = {5},
keywords = {Energy Consumption, FQA, Quality Attributes, SPL, Variability},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3503229.3547057,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Gurov, Dilian and Fuentes, Lidia},
title = {Defining categorical reasoning of numerical feature models with feature-wise and variant-wise quality attributes},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547057},
doi = {10.1145/3503229.3547057},
abstract = {Automatic analysis of variability is an important stage of Software Product Line (SPL) engineering. Incorporating quality information into this stage poses a significant challenge. However, quality-aware automated analysis tools are rare, mainly because in existing solutions variability and quality information are not unified under the same model.In this paper, we make use of the Quality Variability Model (QVM), based on Category Theory (CT), to redefine reasoning operations. We start defining and composing the six most common operations in SPL, but now as quality-based queries, which tend to be unavailable in other approaches. Consequently, QVM supports interactions between variant-wise and feature-wise quality attributes. As a proof of concept, we present, implement and execute the operations as lambda reasoning for CQL IDE - the state-of-the-art CT tool.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {132–139},
numpages = {8},
keywords = {automated reasoning, category theory, extended feature model, numerical features, quality attribute},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3233027.3233038,
author = {Martinez, Jabier and T\"{e}rnava, Xhevahire and Ziadi, Tewfik},
title = {Software product line extraction from variability-rich systems: the robocode case study},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233038},
doi = {10.1145/3233027.3233038},
abstract = {The engineering of a Software Product Line (SPL), either by creating it from scratch or through the re-engineering of existing variants, it uses to be a project that spans several years with a high investment. It is often hard to analyse and quantify this investment, especially in the context of extractive SPL adoption when the related software variants are independently created by different developers following different system architectures and implementation conventions. This paper reports an experience on the creation of an SPL by re-engineering system variants implemented around an educational game called Robocode. The objective of this game is to program a bot (a battle tank) that battles against the bots of other developers. The world-wide Robocode community creates and maintains a large base of knowledge and implementations that are mainly organized in terms of features, although not presented as an SPL. Therefore, a group of master students analysed this variability-rich domain and extracted a Robocode SPL. We present the results of such extraction augmented with an analysis and a quantification regarding the spent time and effort. We believe that the results and the a-posteriori analysis can provide insights on global challenges on SPL adoption. We also provide all the elements to SPL educators to reproduce the teaching activity, and we make available this SPL to be used for any research purpose.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {132–142},
numpages = {11},
keywords = {education, extractive software product line adoption, reverse-engineering, robocode, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3233027.3233032,
author = {Kr\"{o}her, Christian and Gerling, Lea and Schmid, Klaus},
title = {Identifying the intensity of variability changes in software product line evolution},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233032},
doi = {10.1145/3233027.3233032},
abstract = {The evolution of a Software Product Line (SPL) typically affects a variety of artifact types. The intensity (the frequency and the amount) in which developers change variability information in these different types of artifacts is currently unknown. In this paper, we present a fine-grained approach for the variability-centric extraction and analysis of changes to code, build, and variability model artifacts introduced by commits. This approach complements existing work that is typically based on a feature-perspective and, thus, abstracts from this level of detail. Further, it provides a detailed understanding of the intensity of changes affecting variability information in these types of artifacts. We apply our approach to the Linux kernel revealing that changes to variability information occur infrequently and only affect small parts of the analyzed artifacts. Further, we outline how these results may improve certain analysis and verification tasks during SPL evolution.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {54–64},
numpages = {11},
keywords = {evolution analysis, intensity, software product line evolution, variability changes},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3522664.3528602,
author = {Friesel, Birte and Spinczyk, Olaf},
title = {Black-box models for non-functional properties of AI software systems},
year = {2022},
isbn = {9781450392754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3522664.3528602},
doi = {10.1145/3522664.3528602},
abstract = {Non-functional properties (NFPs) such as latency, memory requirements, or hardware cost are an important characteristic of AI software systems, especially in the domain of resource-constrained embedded devices. Embedded AI products require sufficient resources for satisfactory latency and accuracy, but should also be cost-efficient and therefore not use more powerful hardware than strictly necessary. Traditionally, modeling and optimization efforts focus on the AI architecture, utilizing methods such as neural architecture search (NAS). However, before developers can start optimizing, they need to know which architectures are suitable candidates for their use case. To this end, architectures must be viewed in context: model post-processing (e.g. quantization), hardware platform, and run-time configuration such as batching all have significant effects on NFPs and therefore on AI architecture performance. Moreover, scalar parameters such as batch size cannot be benchmarked exhaustively. We argue that it is worthwhile to address this issue by means of black-box models before deciding on AI architectures for optimization and hardware/software platforms for inference. To support our claim, we present an AI product line with variable hardware and software components, perform benchmarks, and present notable results. Additionally, we evaluate both compactness and generalization capabilities of regression tree-based modeling approaches from the machine learning and product line engineering communities. We find that linear model trees perform best: they can capture NFPs of known AI configurations with a mean error of up to 13 %, and can predict unseen configurations with a mean error of 10 to 26 %. We find linear model trees to be more compact and interpretable than other tree-based approaches.},
booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
pages = {170–180},
numpages = {11},
keywords = {AI, performance prediction, product lines, regression trees},
location = {Pittsburgh, Pennsylvania},
series = {CAIN '22}
}

@inproceedings{10.1145/3109729.3109744,
author = {Munoz, Daniel-Jesus},
title = {Achieving energy efficiency using a Software Product Line Approach},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109744},
doi = {10.1145/3109729.3109744},
abstract = {Green computing and energy-aware software engineering are trend approaches that try to address the development of applications respectful with the environment. To reduce the energy consumption of an application the developer needs: (i) to identify what are the concerns that will impact more in the energy consumption; (ii) to model the variability of alternative designs and implementations of each concern; (iii) to store and compare the experimentation results related with the energy and time consumption of concerns; (iv) to find out what is the most eco-efficient solution for each concern. HADAS addresses these issues by modelling the variability of energy consuming concerns for different energy contexts. It connects the variability model with a repository that stores energy measurements, providing a Software Product Line (SPL) service, helping developers to reason and find out what are the most eco-friendly configurations. We have an initial implementation of the HADAS toolkit using Clafer. We have tested our implementation with several case studies.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {131–138},
numpages = {8},
keywords = {Clafer, Energy Efficiency, Metrics, Optimisation, Repository, Software Product Line, Variability},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3483899.3483909,
author = {Furtado, Viviane and OliveiraJr, Edson and Kalinowski, Marcos},
title = {Guidelines for Promoting Software Product Line Experiments},
year = {2021},
isbn = {9781450384193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483899.3483909},
doi = {10.1145/3483899.3483909},
abstract = {The importance of experimentation for Software Engineering research has been notably established in the last years. The software engineering community has discussed how to proper report and evaluate experiments using different approaches, such as quality criteria, scales, and checklists. Nevertheless, there are no guidelines to support researchers and practitioners active in straightforward software engineering research areas, as in Software Product Lines (SPL), at conducting experiments. We hypothesize that experimentation guidelines may aid such a specific area by providing advice and actual excerpts reflecting good practices of SPL experimentation, thus experimentally evolving this area. Therefore, the goal of this paper is to provide guidelines for properly reporting and promoting SPL experiments. We defined such guidelines based on well-known software engineering experiment reports, quality evaluation checklists, and data extracted from 211 SPL experiments identified in a systematic mapping study. We evaluated the guidelines with a qualitative study with SPL and experimentation experts applying open and axial coding procedures. The evaluation enabled us to improve the guidelines. The resulting guidelines contain specific advice to researchers active in SPL and provide examples taken from published SPL experiments. The experts’ positive points indicate that the proposed guidelines can aid SPL researchers and practitioners. Sharing the resulting guidelines could support conducting SPL experiments and allow further area evolution based on prospective experiment replications and reproductions from well-designed and reported experiments.},
booktitle = {Proceedings of the 15th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {31–40},
numpages = {10},
keywords = {Experiment Reporting and Sharing, Guidelines, Qualitative Study, SPL Experiments},
location = {Joinville, Brazil},
series = {SBCARS '21}
}

@inproceedings{10.1145/3336294.3336310,
author = {Rabiser, Rick and Schmid, Klaus and Becker, Martin and Botterweck, Goetz and Galster, Matthias and Groher, Iris and Weyns, Danny},
title = {Industrial and Academic Software Product Line Research at SPLC: Perceptions of the Community},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336310},
doi = {10.1145/3336294.3336310},
abstract = {We present preliminary insights into the perception of researchers and practitioners of the software product line (SPL) community on previous, current, and future research efforts. We were particularly interested in up-and-coming and outdated topics and whether the views of academics and industry researchers differ. Also, we compared the views of the community with the results of an earlier literature survey published at SPLC 2018. We conducted a questionnaire-based survey with attendees of SPLC 2018. We received 33 responses (about a third of the attendees) from both, very experienced attendees and younger researchers, and from academics as well as industry researchers. We report preliminary findings regarding popular and unpopular SPL topics, topics requiring further work, and industry versus academic researchers' views. Differences between academic and industry researchers become visible only when analyzing comments on open questions. Most importantly, while topics popular among respondents are also popular in the literature, topics respondents think require further work have often already been well researched. We conclude that the SPL community needs to do a better job preserving and communicating existing knowledge and particularly also needs to widen its scope.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {189–194},
numpages = {6},
keywords = {software product lines, industry, academia, SPLC},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3233027.3233029,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Extracting software product line feature models from natural language specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233029},
doi = {10.1145/3233027.3233029},
abstract = {The specification of a family of software products may include documents written in natural language. Automatically extracting knowledge from these documents is a challenging problem that requires using Natural Language Processing (NLP) techniques. This knowledge can be formalized as a Feature Model (FM), a diagram capturing the key features and the relationships among them.In this paper, we first review previous works that have presented tools for extracting FMs from textual specifications and compare their strengths and limitations. Then, we propose a framework for feature and relationship extraction, which overcomes the identified limitations and is built upon state-of-the-art open-source NLP tools. This framework is evaluated against previous works using several case studies, showing improved results.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {43–53},
numpages = {11},
keywords = {NLTK, feature model extraction, natural language processing, requirements engineering, software product line},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3382026.3431248,
author = {Ferreira, Thiago Nascimento and Vergilio, Silvia Regina and Kessentini, Mauroane},
title = {Many-objective Search-based Selection of Software Product Line Test Products with Nautilus},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431248},
doi = {10.1145/3382026.3431248},
abstract = {The Variability Testing of Software Product Lines (VTSPL) concerns the selection of the most representative products to be tested according to specific goals. Works in the literature use a great variety of objectives and distinct algorithms. However, they neither address all the objectives at the same time nor offer an automatic tool to support this task. To this end, this work introduces Nautilus/VTSPL, a tool to address the VTSPL problem, created by instantiating Nautilus Framework. Nautilus/VTSPL allows the tester to experiment and configure different objectives and categories of many-objective algorithms. The tool also offers support to visualization of the generated solutions, easing the decision-making process.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {1–4},
numpages = {4},
keywords = {many-objective algorithms, product line testing, sbse},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3571788.3571792,
author = {Bettin, Giovanna and Herculani, Julio and Melo, Amanda and Andrade, Luiz C. M. and OliveiraJr, Edson},
title = {Efficacy, Efficiency and Effectiveness of SMarty-based Software Product Line Inspection Techniques: a Controlled Quasi-Experiment},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571788.3571792},
doi = {10.1145/3571788.3571792},
abstract = {Software quality is impacted by several factors, including the quality of the artifacts generated during the development process. Especially for Software Product Lines (SPL), as artifacts are reused for various specific products, a defect can be spread out over an SPL. Our research group has previously created two inspection techniques for UML-based SPLs: a checklist-based, named SMartyCheck, and a perspective-based, named SMartyPerspective. Seeking to understand the efficiency, efficacy, and effectiveness of such techniques, we carried out a controlled quasi-experiment with 16 participants from the Software Engineering area. It aimed at inspecting feature diagrams and use case, class, component, and sequence diagrams designed using the SMarty approach for UML-based variability support. We also considered ad hoc inspections in such a study. The results of this experiment provide incipient evidence of no statistical difference among the compared techniques for efficiency, efficacy, and effectiveness.},
booktitle = {Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {40–49},
numpages = {10},
keywords = {Ad hoc, Checklist-Based Reading, Defects, Perspective-Based Reading, SMarty, SPL Inspections, Software Product Line, UML},
location = {Odense, Denmark},
series = {VaMoS '23}
}

@inproceedings{10.1145/3307630.3342421,
author = {Chac\'{o}n-Luna, Ana E. and Ruiz, Elvira G. and Galindo, Jos\'{e} A. and Benavides, David},
title = {Variability Management in a Software Product Line Unaware Company: Towards a Real Evaluation},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342421},
doi = {10.1145/3307630.3342421},
abstract = {Software Product Lines (SPL) enable systematic reuse within an organization thus, enabling the reduction of costs, efforts, development time and the average number of defects per product. However, there is little empirical evidence of SPL adoption in the literature, which makes it difficult to strengthen or elaborate adjustments or improvements to SPL frameworks. In this article, we present the first steps towards an empirical evaluation by showing how companies that do not know about of SPL manage variability in their products, pointing out the strengths and weaknesses of their approaches. To this end, we present the design of a case study that we plan to carry out in the future in two companies to evaluate how companies perform variability management when they are not aware of software product lines. Our assumption is that most of the companies manage variability but no many of them are aware of software product lines. In addition, the first preliminary results of the case study applied in a company are presented.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {82–89},
numpages = {8},
keywords = {a case study, software product lines, variability management},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3559712.3559713,
author = {Silva, Leandro F. and OliveiraJr, Edson and Santos, Rodrigo Pereira dos},
title = {A Field Study on Reference Architectural Decisions for Developing a UML-based Software Product Line Tool},
year = {2022},
isbn = {9781450397452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3559712.3559713},
doi = {10.1145/3559712.3559713},
abstract = {Variability modeling in Software Product Lines (SPL) encompasses a set of activities, such as domain analysis, identification of requirements, implementation of variability, variant management, and generation of products. In this context, the existing literature does not present any tools with native support for UML-based SPLs. To deal with the lack of practical solutions, an alternative to variability modeling is to handle XMI files for general-purpose UML tools. However, it requires significant effort, is time-consuming and error-prone, and does not provide users control over data for the SPL life cycle activities. To cope with this scenario, we developed SMartyModeling, an environment to allow SPL-related variability modeling on requirements, features, and UML models, thus providing visualization techniques to SPL/variability information, traceability, and configuration of products. To evolve SMartyModeling, we previously evaluated it throughout two studies: a comparative experiment between SMartyModeling and a general-purpose UML modeling tool, and a survey-based qualitative study on its usability. Results made it possible to identify benefits, limitations, and corrections on the main problems reported by the participants. More specifically in this paper, we present results of a field study focused on analyzing architectural decisions taken during the SMartyModeling instantiation process from a variability tools reference architecture (RA). We took into consideration the opinion of 13 experts in SPL and RA. Experts considered the architectural decisions and the solutions proposed adequate, and the architecture clear and objective. In addition, the analysis of the experts quotes allowed us to identify improvements in the instantiation process, as well as in the instantiated architecture. For example, inclusion of notations to ease the understanding of the instantiation process and the underlying decisions, clear representation of the MVC Design Pattern, and inclusion of other elements to the source RA.},
booktitle = {Proceedings of the 16th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {20–29},
numpages = {10},
keywords = {Architectural Decisions, Field Study, Reference Architecture, Software Product Line, UML, Variability},
location = {Uberlandia, Brazil},
series = {SBCARS '22}
}

@inproceedings{10.1145/3422392.3422402,
author = {da Silva, Leandro Flores and Oliveira, Edson},
title = {Evaluating usefulness, ease of use and usability of an UML-based Software Product Line Tool},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422402},
doi = {10.1145/3422392.3422402},
abstract = {Software Product Line (SPL) is a software development approach that systematically applies reuse of artifacts in a specific domain. In the last years, the industry has increasingly required the support of tools for most SPL life cycle activities, targeting feature models and related diagrams, variability management and SPL specific products configuration. However, existing literature does not present any tools with native support to UML-based SPLs. In addition, relying on manipulating XMI files for general-purpose UML tools for such SPLs takes significant effort, and it is time-consuming and error-prone. In this scenario, we developed SMartyModeling, with support to UML stereotype-based variability management. To evolve our tool, we evaluated it throughout a survey answered by 37 participants. We adopted questions from the Technology Acceptance Model (TAM) and the System Usability Scale (SUS). We organized it into three sections of Likert-scaled questions for usefulness, ease of use, and usability. A last section consisted of open questions focused on positive and negative aspects and an overview of the evalaution. SMartyModeling was well evaluated in relation to usefulness, ease of use, and usability. We analyzed and interpreted the respondents quotes using correlation techniques and open and axial coding. The analysis of open questions allowed us a direct identification of points to improve the tool, its limitations and positive aspects.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {798–807},
numpages = {10},
keywords = {SPL tool support, Software Product Line, UML},
location = {Natal, Brazil},
series = {SBES '20}
}

@article{10.1145/3442389,
author = {Castro, Thiago and Teixeira, Leopoldo and Alves, Vander and Apel, Sven and Cordy, Maxime and Gheyi, Rohit},
title = {A Formal Framework of Software Product Line Analyses},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3442389},
doi = {10.1145/3442389},
abstract = {A number of product-line analysis approaches lift analyses such as type checking, model checking, and theorem proving from the level of single programs to the level of product lines. These approaches share concepts and mechanisms that suggest an unexplored potential for reuse of key analysis steps and properties, implementation, and verification efforts. Despite the availability of taxonomies synthesizing such approaches, there still remains the underlying problem of not being able to describe product-line analyses and their properties precisely and uniformly. We propose a formal framework that models product-line analyses in a compositional manner, providing an overall understanding of the space of family-based, feature-based, and product-based analysis strategies. It defines precisely how the different types of product-line analyses compose and inter-relate. To ensure soundness, we formalize the framework, providing mechanized specification and proofs of key concepts and properties of the individual analyses. The formalization provides unambiguous definitions of domain terminology and assumptions as well as solid evidence of key properties based on rigorous formal proofs. To qualitatively assess the generality of the framework, we discuss to what extent it describes five representative product-line analyses targeting the following properties: safety, performance, dataflow facts, security, and functional program properties.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {34},
numpages = {37},
keywords = {Software product lines, product-line analysis}
}

@inproceedings{10.1145/2934466.2934489,
author = {Nagamine, Motoi and Nakajima, Tsuyoshi and Kuno, Noriyoshi},
title = {A case study of applying software product line engineering to the air conditioner domain},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934489},
doi = {10.1145/2934466.2934489},
abstract = {Software development for embedded products requires high quality, high productivity, and short delivery time because of strong business demands. Although software product line engineering (SPLE) is widely recognized as a good approach for systematic reuse of software, few reports present the information needed for other organizations to implement SPLE. This paper describes a case study of applying SPLE to a product family of air-conditioners, including the effects on degree of implementation of SPLE'S three essential activities (domain engineering, application engineering, and management) and its evaluation over the long period. The use of an incomplete implementation of SPLE's three essential activities temporally improves the productivity of the application developments due to the effect of refactored software, but this gradually decreases through architecture erosion.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {220–226},
numpages = {7},
keywords = {SPL, case study, embedded system, software product line},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2934466.2934481,
author = {Sion, Laurens and Van Landuyt, Dimitri and Joosen, Wouter and de Jong, Gjalt},
title = {Systematic quality trade-off support in the software product-line configuration process},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934481},
doi = {10.1145/2934466.2934481},
abstract = {Software product line engineering is a compelling methodology that accomplishes systematic reuse in families of systems by relying on two key principles: (i) the decomposition of complex systems into composable and reusable building blocks (often logical units called features), and (ii) on-demand construction of products and product variants by composing these building blocks.However, unless the stakeholder responsible for product configuration has detailed knowledge of the technical ins and outs of the software product line (e.g., the architectural impact of a specific feature, or potential feature interactions), he is in many cases flying in the dark. Although many initial approaches and techniques have been proposed that take into account quality considerations and involve trade-off decisions during product configuration, no systematic support exists.In this paper, we present a reference architecture for product configuration tooling, providing support for (i) up-front generation of variants, and (ii) quality analysis of these variants. This allows pro-actively assessing and predicting architectural quality properties for each product variant and in turn, product configuration tools can take into account architectural considerations. In addition, we provide an in-depth discussion of techniques and tactics for dealing with the problem of variant explosion, and as such to maintain practical feasibility of such approaches.We validated and implemented our reference architecture in the context of a real-world industrial application, a product-line for the firmware of an automotive sensor. Our prototype, based on FeatureIDE, is open for extension and readily available.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {164–173},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3425269.3425271,
author = {Nicolodi, Luciane Baldo and Colanzi, Thelma Elita and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Architectural Feature Re-Modularization for Software Product Line Evolution},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425271},
doi = {10.1145/3425269.3425271},
abstract = {Extensive maintenance leads to the Software Product Line Architecture (PLA) degradation over time. When there is the need of evolving the Software Product Line (SPL) to include new features, or move to a new platform, a degraded PLA requires considerable effort to understand and modify, demanding expensive refactoring activity. In the state of the art, search-based algorithms are used to improve PLA at package level. However, recent studies have shown that the most variability and implementation details of an SPL are described in the level of classes. There is a gap between existing approaches and existing practical needs. In this work, we extend the current state of the art to deal with feature modularization in the level of classes by introducing a new search operator and a set of objective functions to deal with feature modularization in a finer granularity of the architectural elements, namely at class level. We evaluated the proposal in an exploratory study with a PLA widely investigated and a real-world PLA. The results of quantitative and qualitative analysis point out that our proposal provides solutions to properly re-modularize features in a PLA, being preferred by practitioners, in order to support the evolution of SPLs.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {31–40},
numpages = {10},
keywords = {Architectural Degradation, Feature Modularization, Search-based Software Engineering, Software Evolution},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1145/3307630.3342385,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {HADAS: Analysing Quality Attributes of Software Configurations},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342385},
doi = {10.1145/3307630.3342385},
abstract = {Software Product Lines (SPLs) are highly configurable systems. Automatic analyses of SPLs rely on solvers to navigate complex dependencies among features and find legal solutions. Variability analysis tools are complex due to the diversity of products and domain-specific knowledge. On that, while there are experimental studies that analyse quality attributes, the knowledge is not easily accessible for developers, and its appliance is not trivial. Aiming to allow the industry to quality-explore SPL design spaces, we developed the HADAS assistant that: (1) models systems and collects quality attributes metrics in a cloud repository, and (2) reasons about it helping developers with quality attributes requirements.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {13–16},
numpages = {4},
keywords = {NFQA, attribute, model, numerical, software product line, variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3233027.3233028,
author = {Rabiser, Rick and Schmid, Klaus and Becker, Martin and Botterweck, Goetz and Galster, Matthias and Groher, Iris and Weyns, Danny},
title = {A study and comparison of industrial vs. academic software product line research published at SPLC},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233028},
doi = {10.1145/3233027.3233028},
abstract = {The study presented in this paper aims to provide evidence for the hypothesis that software product line research has been changing and that the works in industry and academia have diverged over time. We analysed a subset (140) of all (593) papers published at the Software Product Line Conference (SPLC) until 2017. The subset was randomly selected to cover all years as well as types of papers. We assessed the research type of the papers (academic or industry), the kind of evaluation (application example, empirical, etc.), and the application domain. Also, we assessed which product line life-cycle phases, development practices, and topics the papers address. We present an analysis of the topics covered by academic vs. industry research and discuss the evolution of these topics and their relation over the years. We also discuss implications for researchers and practitioners. We conclude that even though several topics have received more attention than others, academic and industry research on software product lines are actually rather in line with each other.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {14–24},
numpages = {11},
keywords = {SPLC, academia, industry, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2701319.2701326,
author = {Soares, Larissa Rocha and do Carmo Machado, Ivan and de Almeida, Eduardo Santana},
title = {Non-Functional Properties in Software Product Lines: A Reuse Approach},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701326},
doi = {10.1145/2701319.2701326},
abstract = {Software Product Line Engineering (SPLE) emerges for software organizations interested in customized products at reasonable costs. Based on the selection of features, stakeholders can derive programs satisfying a range of functional properties and non-functional ones. The explicit definition of Non-Functional Properties (NFP) during software configuration has been considered a challenging task. Dealing with them is not well established yet, neither in theory nor in practice. In this sense, we present a framework to specify NFP for SPLE and we also propose a reuse approach that promotes the reuse of NFP values during the product configuration. We discuss the results of a case study aimed to evaluate the applicability of the proposed work.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {67–74},
numpages = {8},
keywords = {Empirical Software Engineering, Quality Attributes, Software Product Line},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@inproceedings{10.1145/3546932.3547008,
author = {Amraoui, Yassine El and Blay-Fornarino, Mireille and Collet, Philippe and Precioso, Fr\'{e}d\'{e}ric and Muller, Julien},
title = {Evolvable SPL management with partial knowledge: an application to anomaly detection in time series},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547008},
doi = {10.1145/3546932.3547008},
abstract = {In Machine Learning (ML), the resolution of anomaly detection problems in time series presents a great diversity of practices as it can correspond to many different contexts. These practices cover both grasping the business problem and designing the solution itself. By practice, we designate explicit and implicit steps toward resolving a problem, while a solution corresponds to a combination of algorithms selected for their performance on a given problem. Two related issues arise. The first one is that the practices are individual and not explicitly mutualized. The second one is that choosing one solution over another is all the more difficult to justify because the space of solutions and the evaluation criteria are vast and evolve rapidly with the advances in ML. To solve these issues and tame the evolving diversity in ML, a Software Product Line (SPL) approach can be envisaged to represent the variable set of solutions. However, this requires characterizing an ML business problem through an explicit set of criteria and justifying one ML solution over all others. The resolution of anomaly detection problems is thus different from finding the best configuration workflow from past configurations but lies more in guiding the configuration towards a solution that may never have been studied before. This paper proposes an SPL approach that capitalizes on past practices by exploiting a variability-aware representation to detect new criteria and constraints when practices adopt different solutions to seemingly similar problems. We report on the evaluation of our approach using a set of applications from the literature and an ML software company. We show how the analysis of practices makes it possible to consolidate the knowledge contained in the SPL.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {222–233},
numpages = {12},
keywords = {evolution, machine learning, metrics, software product line},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, M\'{a}rcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Configuration, Feature Model, Non-Functional Properties, Recommender Systems, Software Product Lines},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/3646548.3672582,
author = {Corti\~{n}as, Alejandro and Lamas, Victor and R. Luaces, Miguel},
title = {SensorPublisher: Applying Software Product Lines to the development of IoT dashboards},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672582},
doi = {10.1145/3646548.3672582},
abstract = {Geosciences have witnessed a revolution in data collection thanks to the Internet of Things (IoT), which has made it possible to monitor complex phenomena using sensor networks. However, developing web-centric, sensor-based, data warehousing information systems presents challenges because of their complexity and cost. This paper presents an intuitive low-code development system (called SensorPublisher), based on a software product line (SPL) and a domain-specific language (DSL), that speeds up the creation of data warehousing applications for geographic sensor data. SensorPublisher allows the geoscientist to define the sensor network, to generate a software product, and to deploy the product to a local or a remote server. Our tool seeks to encourage scientists to share the outcomes of their sensor data analysis projects with their communities by means of a simple, user-friendly and cost-effective approach. We showcase the system in different geoscientific domains, such as meteorological monitoring services, traffic data and air quality monitoring in urban areas, and marine area monitoring systems.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {153–163},
numpages = {11},
keywords = {Domain Specific Language (DSL), Internet of Things (IoT), Software Product Line (SPL)},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/2934466.2934483,
author = {Richenhagen, Johannes and Rumpe, Bernhard and Schlo\ss{}er, Axel and Schulze, Christoph and Thissen, Kevin and von Wenckstern, Michael},
title = {Test-driven semantical similarity analysis for software product line extraction},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934483},
doi = {10.1145/2934466.2934483},
abstract = {Software product line engineering rests upon the assumption that a set of products share a common base of similar functionality. The correct identification of similarities between different products can be a time-intensive task. Hence, this paper proposes an automated semantical similarity analysis supporting software product line extraction and maintenance. Under the assumption of an already identified compatible interface, the degree of semantical similarity is identified based on provided test cases. Therefore, the analysis can also be applied in a test-driven development. This is done by translating available test sequences for both components into two I/O extended finite automata and performing an abstraction of the defined behavior until a simulation relation is established. The test-based approach avoids complexity issues regarding the state space explosion problem, a common issue in model checking. The proposed approach is applied on different variants and versions of industrially used software components provided by an automotive supplier to demonstrate the method's applicability.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {174–183},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3167132.3167353,
author = {Pereira, Juliana Alves and Martinez, Jabier and Gurudu, Hari Kumar and Krieter, Sebastian and Saake, Gunter},
title = {Visual guidance for product line configuration using recommendations and non-functional properties},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167353},
doi = {10.1145/3167132.3167353},
abstract = {Software Product Lines (SPLs) are a mature approach for the derivation of a family of products using systematic reuse. Different combinations of predefined features enable tailoring the product to fit the needs of each customer. These needs are related to functional properties of the system (optional features) as well as non-functional properties (e.g., performance or cost of the final product). In industrial scenarios, the configuration process of a final product is complex and the tool support is usually limited to check functional properties interdependencies. In addition, the importance of nonfunctional properties as relevant drivers during configuration has been overlooked. Thus, there is a lack of holistic paradigms integrating recommendation systems and visualizations that can help the decision makers. In this paper, we propose and evaluate an interrelated set of visualizations for the configuration process filling these gaps. We integrate them as part of the FeatureIDE tool and we evaluate its effectiveness, scalability, and performance.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {2058–2065},
numpages = {8},
keywords = {configuration, feature model, recommendation systems, software product lines, visualization},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/2648511.2648535,
author = {Hartmann, Herman and van der Linden, Frank and Bosch, Jan},
title = {Risk based testing for software product line engineering},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648535},
doi = {10.1145/2648511.2648535},
abstract = {The variability of product lines increases over time thereby leading to an increasing effort for testing. Since the available time for test activities is limited an efficiency improvement is needed to ensure that products have sufficient quality.This paper introduces risk-based testing for software product lines. Our approach is based on risk based testing for single system engineering which is extended with a dimension that captures the percentage of product variants that use a particular development artifact. Based on the risk of development artifacts, the priorities for domain and application engineering are determined. We demonstrate our approach using a case study from an existing product line and discuss tool support.We conclude that the basic ideas behind risk-based testing for product lines are intuitive, pragmatic in nature, and provide the means for practitioners for guiding the test effort.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {227–231},
numpages = {5},
keywords = {risk based testing, software product line engineering},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3106195.3106224,
author = {Tizzei, Leonardo P. and Nery, Marcelo and Segura, Vin\'{\i}cius C. V. B. and Cerqueira, Renato F. G.},
title = {Using Microservices and Software Product Line Engineering to Support Reuse of Evolving Multi-tenant SaaS},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106224},
doi = {10.1145/3106195.3106224},
abstract = {In order to achieve economies of scale, a Software as a Service (SaaS) should be configurable, multi-tenant efficient, and scalable. But building SaaS with these characteristics comes at a price of having more complex services. Some works in the literature integrate software product line engineering and service-oriented architecture to tackle the complexity of building multi-tenant SaaS. Most of these works focused on centralized approaches that rely on middleware or platforms, but they do not investigate the use of decentralized architectural style. Microservices architecture is an architectural style that relies on small, decentralized, and autonomous services that work together. Thus, this paper investigates the integrated use of microservices architecture and software produt line techniques to develop multi-tenant SaaS. We conducted an empirical study that analyzes the behavior of software reuse during the evolution of a multi-tenant SaaS. This empirical study showed an average software reuse of 62% of lines of code among tenants. We also provide lessons we learned during the the re-engineering and maintenance of such multi-tenant SaaS.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {205–214},
numpages = {10},
keywords = {Microservices, Multi-tenancy, Service-oriented Architectures, Software Evolution, Software Reuse},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3233027.3233050,
author = {Kuiter, Elias and Kr\"{u}ger, Jacob and Krieter, Sebastian and Leich, Thomas and Saake, Gunter},
title = {Getting rid of clone-and-own: moving to a software product line for temperature monitoring},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233050},
doi = {10.1145/3233027.3233050},
abstract = {Due to its fast and simple applicability, clone-and-own is widely used in industry to develop software variants. In cooperation with different companies for thermoelectric products, we implemented multiple variants of a heat monitoring tool based on clone-and-own. After encountering redundancy-related problems during development and maintenance, we decided to migrate towards a software product line. Within this paper, we describe this case study of migrating cloned variants to a software product line based on the extractive approach. The resulting software product line encapsulates variability on several levels, including the underlying hardware systems, interfaces, and use cases. Currently, we support monitoring hardware from three different companies that use the same core system and provide a configurable front-end. We share our experiences and encountered problems with cloning and migration towards a software product line---focusing on feature extraction and modeling in particular. Furthermore, we provide a lightweight, web-based tool for modeling, configuring, and implementing software product lines, which we use to migrate and manage features. Besides this experience report, we contribute most of the created artifacts as open-source and freely available for the research community.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {179–189},
numpages = {11},
keywords = {case study, extraction, feature modeling, software product line},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3289402.3289504,
author = {Sebbaq, Hanane and Retbi, Asmaa and Idrissi, Mohammed Khalidi and Bennani, Samir},
title = {Software Product Line to overcome the variability issue in E-Learning: Systematic literature review},
year = {2018},
isbn = {9781450364621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289402.3289504},
doi = {10.1145/3289402.3289504},
abstract = {The disparity of educational technologies, pedagogies and learning styles implies a problem of variability when modeling E-learning systems. Furthermore, the current learning context, which has become very open and heterogeneous, raises the problem of automating the modeling, development and maintenance of personalized E-learning systems based on various pedagogies. For its part, the "Software Product Line" is a paradigm that aims to produce product families based on the principles of reuse, configuration and derivation. The main purpose of this literature review is to explore the different potential applications of "SPL" in the E-learning domain to figure out the problem of variability. We will adopt a protocol for a systematic review of literature, after which we will draw up an analysis report.},
booktitle = {Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications},
articleno = {4},
numpages = {8},
keywords = {E-learning, Software Product line, Variability, heterogeneity, scale, systematic literature review, variety},
location = {Rabat, Morocco},
series = {SITA'18}
}

@inproceedings{10.1145/3646548.3672592,
author = {Douglas Ferrari Mendon\c{c}a, Willian and Assun\c{c}\~{a}o, Wesley K. G. and Vergilio, Silvia},
title = {Feature-oriented Test Case Prioritization Strategies: An Evaluation for Highly Configurable Systems},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672592},
doi = {10.1145/3646548.3672592},
abstract = {Test Case Prioritization (TCP) has proved to be beneficial when time constraints exist. During TCP, most relevant test cases are executed first in order to maximize failure detection and produce rapid feedback. There are several TCP approaches in the literature, but most of them do not consider particularities of Highly-Configurable Systems (HCSs), namely the notion of features. Existing work is usually based on artifacts (e.g., feature models) that can be outdated or are not available. Some code-oriented approaches usually employ dynamic analysis or require a failure-history, which can be costly. Furthermore, they present limitations when new variants and test cases are added during the system evolution. To address such limitations, we propose and evaluate three feature-oriented TCP strategies: (i) FeatChgHist: prioritizes test cases associated to features that often changed over commits; (ii)&nbsp;FeatChgCommit: prioritizes test cases associated to features that changed more in the current commit; and (iii) FeatCov: prioritizes test cases that cover more features simultaneously. Features are the building blocks of HCSs that are used for communication and design purposes. Then the basis to introduce these strategies is the fact that test cases associated to a great number of features, or to features that changed often over commits, are more fault-prone. In the evaluation, conducted with a real open-source HCS, FeatChgHist and FeatChgCommit show better performance regarding failure detection than FeatCov, and also than the changed-file-oriented strategies, used as baseline. Moreover, combining test selection and prioritization allows a reduced test set without decreasing significantly early failure detection.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {72–83},
numpages = {12},
keywords = {Regression Testing, Software Evolution, Software Product Line},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/2648511.2648513,
author = {Harman, M. and Jia, Y. and Krinke, J. and Langdon, W. B. and Petke, J. and Zhang, Y.},
title = {Search based software engineering for software product line engineering: a survey and directions for future work},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648513},
doi = {10.1145/2648511.2648513},
abstract = {This paper presents a survey of work on Search Based Software Engineering (SBSE) for Software Product Lines (SPLs). We have attempted to be comprehensive, in the sense that we have sought to include all papers that apply computational search techniques to problems in software product line engineering. Having surveyed the recent explosion in SBSE for SPL research activity, we highlight some directions for future work. We focus on suggestions for the development of recent advances in genetic improvement, showing how these might be exploited by SPL researchers and practitioners: Genetic improvement may grow new products with new functional and non-functional features and graft these into SPLs. It may also merge and parameterise multiple branches to cope with SPL branchmania.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {5–18},
numpages = {14},
keywords = {SBSE, SPL, genetic programming, program synthesis},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.5555/2525401.2525415,
author = {Tan, Lei and Lin, Yuqing and Ye, Huilin and Zhang, Guoheng},
title = {Improving product configuration in software product line engineering},
year = {2013},
isbn = {9781921770203},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {Software Product Line Engineering (SPLE) is a emerging software reuse paradigm. SPLE focuses on systematic software reuse from requirement engineering to product derivation throughout the software development life-cycle. Feature model is one of the most important reusable assets which represents all design considerations of a software product line. Feature model will be used in the product configuration process to produce a software. The product configuration is a decision-making process, where all kinds of relationships among configurable features will be considered to select the desired features for the product. To improve the efficiency and quality of product configuration, we are proposing a new approach which aims at identifying a small set of key features. The product configuration should always start from this set of features since, based on the feature dependencies, the decisions made on these features will imply decisions on the rest of the features of the product line, thus reduce the features visited in the configuration process. We have also conducted some experiments to demonstrate how the proposed approach works and evaluate the efficiency of the approach.},
booktitle = {Proceedings of the Thirty-Sixth Australasian Computer Science Conference - Volume 135},
pages = {125–133},
numpages = {9},
keywords = {feature model, minimum vertex cover, product configuration, software product line},
location = {Adelaide, Australia},
series = {ACSC '13}
}

@inproceedings{10.1145/2362536.2362557,
author = {Elsner, Christoph},
title = {Light-weight tool support for staged product derivation},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362557},
doi = {10.1145/2362536.2362557},
abstract = {Tool support that checks for configuration errors and generates product parts from configurations can significantly improve on product derivation in product line engineering. Up to now, however, derivation tools commonly disregard the staged derivation process. They do not restrict configuration consistency checks to process entities such as configuration stages, stakeholders, or build tasks. As a result, constraints that are only valid for certain process entities must either be checked permanently, leading to false positive errors, or one must refrain from defining them at all.This paper contributes a light-weight approach to provide tailored tool support for staged product derivation. Compared to previous approaches, it is not tied to a single configuration mechanism (e.g., feature modeling), and also accounts for the stakeholders involved and the build tasks that generate product parts. First, the product line engineer describes the derivation process in a concise model. Then, based on constraint checks on the configuration (e.g., a feature model configuration) that are linked to the modeled entities, comprehensive tool support can be provided: Configuration actions can be guided and restricted depending on the configuring stakeholder in a fine-grained manner, and constraints attached to a build task will only be checked if it actually shall be executed. Finally, in combination with previous work, the paper provides evidence that the approach is applicable to legacy product lines in a light-weight manner and that it technically scales to thousands of constraint checks.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {146–155},
numpages = {10},
keywords = {product line, staged product derivation, tool support},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2648511.2648537,
author = {Colanzi, Thelma Elita and Vergilio, Silvia Regina and Gimenes, Itana M. S. and Oizumi, Willian Nalepa},
title = {A search-based approach for software product line design},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648537},
doi = {10.1145/2648511.2648537},
abstract = {The Product Line Architecture (PLA) can be improved by taking into account key factors such as feature modularization, and by continuously evaluating its design according to metrics. Search-Based Software Engineering (SBSE) principles can be used to support an informed-design of PLAs. However, existing search-based design works address only traditional software design not considering intrinsic Software Product Line aspects. This paper presents MOA4PLA, a search-based approach to support the PLA design. It gives a multi-objective treatment to the design problem based on specific PLA metrics. A metamodel to represent the PLA and a novel search operator to improve feature modularization are proposed. Results point out that the application of MOA4PLA leads to PLA designs with well modularized features, contributing to improve features reusability and extensibility. It raises a set of solutions with different design trade-offs that can be used to improve the PLA design.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {237–241},
numpages = {5},
keywords = {multi-objective algorithms, searchbased PLA design, software product lines},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3646548.3676546,
author = {G\"{u}thing, Lukas and Pett, Tobias and Schaefer, Ina},
title = {Out-of-the-Box Prediction of Non-Functional Variant Properties Using Automated Machine Learning},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676546},
doi = {10.1145/3646548.3676546},
abstract = {A configurable system is characterized by the configuration options present or absent in its variants. Selecting and deselecting those configuration options directly influences the functional properties of the system. Apart from functional properties, there are system characteristics that influence the performance (e.g., power demand), safety (e.g., fault probabilities), and security (e.g., susceptibility to attacks) of the system, called Non-Functional Properties (NFPs). Knowledge of NFPs is crucial for evaluating a system’s feasibility, usability, and resource demands. Although variability influences these characteristics, NFPs do not compose linearly for every selected feature. Feature interactions can increase the overall NFP values through (potentially exponential) amplification or decrease them through mitigation effects. In this paper, we propose an automated machine learning (AutoML) approach to predict NFP values for new configurations based on previously measured configuration values. Using AutoML, we leverage the advantages of machine learning for predicting NFPs without having to parameterize and fine-tune machine learning models. This approach and the resulting pipeline aim to reduce the complexity of performance prediction for configurable systems. We test the feasibility of our pipeline in a first evaluation on 4 real-world subject systems and discuss cases where AutoML may improve the prediction of NFPs.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {82–87},
numpages = {6},
keywords = {AutoML, Cyber-physical systems, Machine learning, Software product lines},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3546932.3546994,
author = {May, Richard and Biermann, Christian and Kr\"{u}ger, Jacob and Saake, Gunter and Leich, Thomas},
title = {A systematic mapping study of security concepts for configurable data storages},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3546994},
doi = {10.1145/3546932.3546994},
abstract = {Most modern software systems can be configured to fulfill specific customer requirements, adapting their behavior as required. However, such adaptations also increase the need to consider security concerns, for instance, to avoid that unintended feature interactions cause a vulnerability that an attacker can exploit. A particularly interesting aspect in this context are data storages (e.g., databases) used within the system, since the adapted behavior may change how (critical) data is collected, stored, processed, and accessed. Unfortunately, there is no comprehensive overview of the state-of-the-art on security concerns of configurable data storages. To address this gap, we conducted a systematic mapping study in which we analyzed 50 publications from the last decade (2013--2022). We compare these publications based on the configurable systems, data storages, and security concerns involved; using established classification criteria of the respective research fields. Overall, we identified 14 research opportunities, which we discuss in detail. Our key insight is that the security of configurable data storages seems to be under-explored and is rarely considered in a practice-oriented way, for instance, regarding relevant security standards. Furthermore, data storages and their security concerns are usually only mentioned briefly, even though they are either highly configurable or store critical data. Our mapping study aims to help practitioners and researchers to understand the current state-of-the-art research, identify open issues, and guide future research.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {108–119},
numpages = {12},
keywords = {configurable systems, data storage, mapping study, security, software product line engineering},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/2019136.2019172,
author = {Zhang, Guoheng and Ye, Huilin and Lin, Yuqing},
title = {Using knowledge-based systems to manage quality attributes in software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019172},
doi = {10.1145/2019136.2019172},
abstract = {Product configuration in a feature model in software product line engineering is a process, in which the desired features are selected based on the customers' functional requirements and non-functional requirements. The functional requirements of the target product can be satisfied by including the proper functional features. However, there is no such a straightforward way to realize the non-functional requirements and quality attributes of the target product. In our early work, we have developed a quantitative based method to assess the quality attributes for a configured product. However, this approach cannot adequately represent the inter-relationships among quality attributes which play an important role in product configuration process. We supplement our previous work by introducing a quality attribute knowledge base (QA_KB) to represent the inter-relationships among different quality attributes in a SPL. Furthermore, we develop algorithms for configuring a product based on customers' quality requirements. We also use a case study to illustrate our approach.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {32},
numpages = {7},
keywords = {feature model, non-functional requirements, product configuration, quality attributes, software product line},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3646548.3672594,
author = {Bombarda, Andrea and Gargantini, Angelo},
title = {On the Use of Multi-valued Decision Diagrams to Count Valid Configurations of Feature Models},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672594},
doi = {10.1145/3646548.3672594},
abstract = {This paper addresses the challenge of efficiently counting valid configurations in Software Product Lines (SPLs). We propose a novel approach leveraging Multi-Valued Decision Diagrams (MDDs) for building the set of products. Building upon the MDD structure, we introduce several algorithmic optimizations to achieve a more compact and efficient representation of the product set compared to existing methods based on Binary Decision Diagrams. The effectiveness of our approach is evaluated through experimentation on two datasets: a set of synthetic benchmarks and large-scale industrial feature models. The results demonstrate significant improvements in scalability for models of medium complexity, particularly those rich in alternative groups. However, challenges remain for other model types, highlighting areas for future research.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {96–106},
numpages = {11},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3646548.3676550,
author = {He\ss{}, Tobias and Karrer, Simon and Ostheimer, Lukas},
title = {Multi-Version Decision Propagation for Configuring Feature Models in Space and Time},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676550},
doi = {10.1145/3646548.3676550},
abstract = {Real-world feature models are typically too large and complex to be configured manually. In practice, configuration tasks are, therefore, accomplished by employing interactive configurators. After each explicit feature selection or deselection by the user, these configurators use decision propagation to detect features that are implied by the current partial configuration and, consequently, select or deselect them accordingly. This way, the configuration remains valid throughout the configuration process. However, valid configurations may become invalid when then underlying model changes due to model evolution. As one is potentially interested in retaining a configuration for multiple versions, for instance, in testing or certification applications, this prompts the question on how to configure for multiple versions at once. In this work, we introduce multi-version decision propagation which allows to interactively configure on multiple model versions at once. Our prototype adapts the set of possible versions to the current configuration but also allows users to configure for a fixed set of versions.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {88–92},
numpages = {5},
keywords = {Configuration, Decision Propagation, Feature Models, Feature-Model Evolution},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3579027.3608975,
author = {Burgue\~{n}o, Lola and Horcas, Jose-Miguel and Kienzle, J\"{o}rg},
title = {Development and Evolution of Software Product Lines Driven by Stakeholder Beliefs},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608975},
doi = {10.1145/3579027.3608975},
abstract = {The planning, realization, and release of a Software Product Line (SPL) are driven by features. Therefore, many high-level decisions about the evolution of an SPL are made at the feature level. However, a feature can involve many stakeholders with different expertise, and taking their opinions into account to make the right decisions is not trivial. In this paper, we propose using belief uncertainty in conjunction with feature models to assist in the evolution of SPLs by explicitly quantifying opinions. We outline three evolution scenarios in which subjective logic can be used to represent the opinions of stakeholders and explain in detail how to use subjective logic to make decisions in the context of the next release problem. We illustrate our ideas with a Smartwatch SPL. Finally, we discuss different ways of combining the opinions of stakeholders depending on the situation, the goals and the risks that can be assumed.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {34–40},
numpages = {7},
keywords = {Decision making support, feature model, software product line, subjective logic, uncertainty},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3579027.3608994,
author = {Fadhlillah, Hafiyyan Sayyid and Fern\'{a}ndez, Antonio M. Guti\'{e}rrez and Rabiser, Rick and Zoitl, Alois},
title = {Managing Cyber-Physical Production Systems Variability using V4rdiac: Industrial Experiences},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608994},
doi = {10.1145/3579027.3608994},
abstract = {Cyber-Physical Production Systems (CPPSs) are highly robust and versatile production systems that utilize diverse hardware components through control software. Employing a systematic variability management approach for developing variants of control software can reduce cost and time-to-market to build such complex systems. However, employing this approach in the CPPS domain is challenging. Engineering CPPSs require multidisciplinary engineering knowledge (e.g., process, signal, mechanical). Knowledge about CPPS variability is thus typically scattered across diverse engineering artifacts. Also, variability knowledge is usually not documented explicitly but rather tacit knowledge of mostly senior engineers. Furthermore, control software is commonly implemented using a graphical Domain-Specific Modeling Language (DSML) which only provides minimal support to express variability. This paper describes our experiences dealing with these challenges in an industrial context using a multidisciplinary variability management approach called Variability for 4diac (V4rdiac). V4rdiac is an integrated approach that allows CPPS engineers to conduct stepwise product configuration based on heterogeneous variability models from multiple engineering disciplines. V4rdiac also provides a mechanism to automatically generate control software based on a set of selected configuration options. We evaluate how V4rdiac implements and manages CPPS control software variants in the metallurgical production plant domain. We describe the benefits and lessons learned from using V4rdiac in this domain based on feedback from industrial practitioners.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {223–233},
numpages = {11},
keywords = {Variability Modeling, Software Product Line, Software Configuration, Cyber-Physical Production System},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/2364412.2364428,
author = {Zhang, Bo and Becker, Martin},
title = {Code-based variability model extraction for software product line improvement},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364428},
doi = {10.1145/2364412.2364428},
abstract = {Successful Software Product Lines (SPLs) evolve over time. However, one practical problem is that during SPL evolution the core assets, especially the code, tend to become complicated and difficult to understand, use, and maintain. Typically, more and more problems arise over time with implicit or already lost adaptation knowledge about the interdependencies of the different system variants and the supported variability. In this paper, we present a model-based SPL improvement process that analyzes existing large-scale SPL reuse infrastructure to identify improvement potential with respective metrics. Since Conditional Compilation (CC) is one of the most widely used mechanisms to implement variability, we parse variability-related facts from preprocessor code. Then we automatically extract an implementation variability model, including product configuration and variation points that are structured in a hierarchical variability tree. The extraction process is presented with concrete measurement results from an industrial case study.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {91–98},
numpages = {8},
keywords = {conditional compilation, software product line maintenance, variability model},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2892664.2892686,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Towards the dynamic reconfiguration of quality attributes},
year = {2016},
isbn = {9781450340335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2892664.2892686},
doi = {10.1145/2892664.2892686},
abstract = {There are some Quality Attributes (QAs) whose variability is addressed through functional variability in the software architecture. Separately modelling the variability of these QAs from the variability of the base functionality of the application has many advantages (e.g., a better reusability), and facilitates the reconfiguration of the QA variants at runtime. Many factors may vary the QA functionality: variations in the user preferences and usage needs; variations in the non-functional QAs; variations in resources, hardware, or even in the functionality of the base application, that directly affect the product's QAs. In this paper, we aim to elicit the relationships and dependencies between the functionalities required to satisfy the QAs and all those factors that can provoke a reconfiguration of the software architecture at runtime. We follow an approach in which the variability of the QAs is modelled separately from the base application functionality, and propose a dynamic approach to reconfigure the software architecture based on those reconfiguration criteria.},
booktitle = {Companion Proceedings of the 15th International Conference on Modularity},
pages = {131–136},
numpages = {6},
keywords = {Quality attributes, SPL, reconfiguration, software architecture, variability},
location = {M\'{a}laga, Spain},
series = {MODULARITY Companion 2016}
}

@inproceedings{10.1145/2019136.2019182,
author = {McGregor, John D. and Monteith, J. Yates and Zhang, Jie},
title = {Quantifying value in software product line design},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019182},
doi = {10.1145/2019136.2019182},
abstract = {A software product line is a strategic investment for an organization. Besides the initial decision to use a product line approach other strategic decisions are made, including which variations to accommodate. In this paper we present an adaptation of an equation for computing option values. The equation can be used to understand the economic impact of adding a variation point to the product line architecture. The equation was exercised on multiple sets of hypothetical data and and produced the expected changes from one data set to another. In the future the equation will be validated with data from real projects. We describe some practical sources of values for the parameters of the equation.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {40},
numpages = {7},
keywords = {software engineering, strategic software design},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3056662.3056663,
author = {Kang, Sungwon and Kim, Jungmin and Baek, Haeun and Ahn, Hwi and Jung, Pilsu and Lee, Jihyun},
title = {Comparison of software product line test derivation methods from the reuse viewpoint},
year = {2017},
isbn = {9781450348577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3056662.3056663},
doi = {10.1145/3056662.3056663},
abstract = {Product line test development is more complicated than test development for a single application, as the former has to deal with variability among assets (artifacts) and is carried out in two separate but related development phases, i.e. domain engineering and application engineering. Different software product test development methods provide different opportunities for reuse depending on how variability is represented in the domain test artifacts, when binding is formed and applied and also when test data are determined. This paper compares and analyzes the six major methods for the software product line test development in the literature. Through the comparison, we find out that existing software product line testing methods did not fully consider the aspects of software product line that are essential for reuse in software product line development such as variability representation, binding formation and application time and test data determination time. As the conclusion of this literature review, this paper suggests future research opportunities for software product line testing to explore.},
booktitle = {Proceedings of the 6th International Conference on Software and Computer Applications},
pages = {1–8},
numpages = {8},
keywords = {software product line development, software testing, systematic product line testing},
location = {Bangkok, Thailand},
series = {ICSCA '17}
}

@inproceedings{10.1145/2647908.2655968,
author = {Lackner, Hartmut and Schmidt, Martin},
title = {Towards the assessment of software product line tests: a mutation system for variable systems},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655968},
doi = {10.1145/2647908.2655968},
abstract = {Software product line engineering is an emerging methodology for the development of variant-rich software systems. As software product lines are viable for this purpose, testing them is complicated in contrast to non-variable systems, as there is an increasingly amount of possible products due to the number of features. There exist many methods proposed for testing software product lines, but seldom the quality of the resulting tests was assessed. For assessing test quality mutation analysis is a well-known technique and is frequently applied to non-variable software systems. However, mutation analysis cannot be applied straight-forward onto software product lines.We present a mutation system for assessing the quality of software product line tests by means of fault detection capability. Our mutation system comprises model-based mutation operators, test case adaption, automated model and test execution, and automated mutation analysis. So far, we developed several mutation operators for feature models, UML state machines, and mapping models. We evaluated the mutation operators against tests that were generated from the specifications and applied them for three case studies. From the results we draw conclusions about the effectiveness of the individual mutation operators.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {62–69},
numpages = {8},
keywords = {mutation analysis, software product lines, software testing, test quality},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2491627.2491649,
author = {Lanman, Jeremy and Darbin, Rowland and Rivera, Jorge and Clements, Paul and Krueger, Charles},
title = {The challenges of applying service orientation to the U.S. Army's live training software product line},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491649},
doi = {10.1145/2491627.2491649},
abstract = {Live Training Transformation (LT2) is the product line strategy put in place by the United States Army Program Executive Office for Simulation, Training and Instrumentation (PEO STRI). The purpose of the LT2 product line is to provide a common set of core assets including architectures, software components, standards and processes that form the basis of all Army Live Training systems. As products consuming LT2 core assets evolve to meet the latest requirements of the military live training community, changes to the core product line architecture must also be made. Based on thorough analysis of the LT2 core capabilities and user trends toward web-enabled and mobile computing technologies, a Service Oriented Architecture (SOA) strategy was identified and adopted as the objective architecture for the evolving LT2 product line. Future success of the LT2 product line now depends on the alignment of product line engineering concepts with the business and technical benefits of SOA, and to ensure that systematic reuse continues to provide substantial return-on-investment for the Army. This paper addresses the challenges of adopting SOA into an existing software product line, the unique circumstances of the LT2 SOA environment, and present a set of analysis and design considerations for the product line engineering community.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {244–253},
numpages = {10},
keywords = {bill-of-features, feature modeling, feature profiles, hierarchical product lines, product audit, product baselines, product configurator, product derivation, product line engineering, product portfolio, second generation product line engineering, software product lines, variation points},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3546932.3546997,
author = {Acher, Mathieu and Martin, Hugo and Lesoil, Luc and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc and Khelladi, Djamel Eddine and Barais, Olivier and Pereira, Juliana Alves},
title = {Feature subset selection for learning huge configuration spaces: the case of linux kernel size},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3546997},
doi = {10.1145/3546932.3546997},
abstract = {Linux kernels are used in a wide variety of appliances, many of them having strong requirements on the kernel size due to constraints such as limited memory or instant boot. With more than nine thousands of configuration options to choose from, developers and users of Linux actually spend significant effort to document, understand, and eventually tune (combinations of) options for meeting a kernel size. In this paper, we describe a large-scale endeavour automating this task and predicting a given Linux kernel binary size out of unmeasured configurations. We first experiment that state-of-the-art solutions specifically made for configurable systems such as performance-influence models cannot cope with that number of options, suggesting that software product line techniques may need to be adapted to such huge configuration spaces. We then show that tree-based feature selection can learn a model achieving low prediction errors over a reduced set of options. The resulting model, trained on 95 854 kernel configurations, is fast to compute, simple to interpret and even outperforms the accuracy of learning without feature selection.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {85–96},
numpages = {12},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3503229.3547041,
author = {Kucher, Maximilian and Balyo, Tom\'{a}\v{s} and Christensen, Noemi},
title = {Black-box optimization in a configuration system},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547041},
doi = {10.1145/3503229.3547041},
abstract = {The product configurator Merlin is a CPQ solution (Configure, Price, Quote) that enables fast, error-free configuration and quotation generation for products with many variants. In the context of this paper an optimization module was developed and integrated into Merlin. Previously, Merlin could only minimize the number of changes that must be made when a user changes a configuration. With this work, the optimization capability in Merlin was extended in a way, that a user can define a custom target function. Specific features and variables then can be selected for optimization. The optimization module can optimize the values of these attributes and variables with respect to the defined target function. The optimization process has no limited runtime and does not stop automatically when reaching certain predefined values, since in the field of optimization often no promises can be made on finding global extrema. Instead, the optimization process is monitored live by the user and can be terminated at any time as soon as the user is satisfied with the current solution. In addition to the adaptation of the Merlin frontend, two black-box and derivative-free optimization algorithms are implemented and tested for performance to solve the optimization problem.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {229–236},
numpages = {8},
keywords = {black-box, configuration, optimization},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3579027.3608995,
author = {Yoshimura, Kentaro and Yamauchi, Yuta and Takahashi, Hideo},
title = {Managing Variability of Logistics Robot System: Experience at Hitachi},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608995},
doi = {10.1145/3579027.3608995},
abstract = {This paper reports an industrial experience of managing the variability of a logistics robot system. Logistics centers need to dispatch products rapidly and accurately from huge inventories in accordance with the daily flow of orders, and autonomous logistics robot systems provide a solution that can handle a wide variety of products without relying on manual tasks. However, the development of such systems can be challenging due to the many different operation environments and product characteristics that must be considered. We have therefore developed a variability management approach that models the variability of robot systems and the items to be handled in such a way that the software components can be reused and expanded across different products and environments. This paper presents the motivation, challenges, and benefits of applying the variability management approach to a logistics robot system at Hitachi.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {234–241},
numpages = {8},
keywords = {Logistics Automation, Robotics, Software Product Line, Variability Management},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3307630.3342384,
author = {El-Sharkawy, Sascha and Krafczyk, Adam and Schmid, Klaus},
title = {MetricHaven: More than 23,000 Metrics for Measuring Quality Attributes of Software Product Lines},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342384},
doi = {10.1145/3307630.3342384},
abstract = {Variability-aware metrics are designed to measure qualitative aspects of software product lines. As we identified in a prior SLR [6], there exist already many metrics that address code or variability separately, while the combination of both has been less researched. MetricHaven fills this gap, as it extensively supports combining information from code files and variability models. Further, we also enable the combination of well established single system metrics with novel variability-aware metrics, going beyond existing variability-aware metrics. Our tool supports most prominent single system and variability-aware code metrics. We provide configuration support for already implemented metrics, resulting in 23,342 metric variations. Further, we present an abstract syntax tree developed for MetricHaven, that allows the realization of additional code metrics.Tool: https://github.com/KernelHaven/MetricHavenVideo: https://youtu.be/vPEmD5Sr6gM},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {25–28},
numpages = {4},
keywords = {SPL, feature models, implementation, metrics, software product lines, variability models},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3579028.3609008,
author = {Galindo, Jos\'{e} A. and Horcas, Jose-Miguel and Felferning, Alexander and Fernandez-Amoros, David and Benavides, David},
title = {FLAMA: A collaborative effort to build a new framework for the automated analysis of feature models},
year = {2023},
isbn = {9798400700927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579028.3609008},
doi = {10.1145/3579028.3609008},
abstract = {Nowadays, feature models are the de facto standard when representing commonalities and variability, with modern examples spanning up to 7000 features. Manual analysis of such models is challenging and error-prone due to sheer size. To help in this task, automated analysis of feature models (AAFM) has emerged over the past three decades. However, the diversity of these tools and their supported languages presents a significant challenge that motivated the MOD-EVAR community to initiate a project for a new tool that supports the UVL language. Despite the rise of machine learning and data science, along with robust Python-based libraries, most AAFM tools have been implemented in Java, creating a collaboration gap. This paper introduces Flama, an innovative framework that automates the analysis of variability models. It focuses on UVL model analysis and aims for easy integration and extensibility to bridge this gap and foster better community and cross-community collaboration.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
pages = {16–19},
numpages = {4},
keywords = {data visualization, effective communication, graphs and tables, software product line, variability, visualization design process},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/2019136.2019180,
author = {Abrah\~{a}o, Silvia and Nolan, Andy J. and Clements, Paul C. and McGregor, John D.},
title = {Quantitative methods in software product line engineering},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019180},
doi = {10.1145/2019136.2019180},
abstract = {Many of the benefits expected from Software Product Lines (SPLs) are based on the assumption that the additional investment in setting up a product line pays off later when products are created. However, to fully exploit this assumption, organizations need to optimize the development of core assets and products that bring the maximum business value. The objective of this workshop is to bring together researchers and practitioners to report and discuss the challenges and opportunities for integrating quantitative methods in product line engineering with the objective of achieving both technical and business goals. In particular, we are seeking contributions that, on the one hand, deal with product line estimation and metrics for the effective management of product line projects; and on the other hand, provide some insight into new trends in value-based product line engineering.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {38},
numpages = {2},
keywords = {product line economics, quantitative methods, software engineering, software product lines, value-based product line engineering},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3646548.3672584,
author = {Kogler, Philipp and Chen, Wei and Falkner, Andreas and Haselb\"{o}ck, Alois and Wallner, Stefan},
title = {Modelling Engineering Processes in Natural Language: A Case Study},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672584},
doi = {10.1145/3646548.3672584},
abstract = {Engineering process management aims to formally specify processes which are executable, measurable, and controllable. Common representations include text-based domain-specific languages (DSLs) or graphical notations such as the Business Process Modelling Notation (BPMN). The specification itself can be seen as a Software Product Line (SPL), building upon concepts such as tasks, UI forms, fields and actions. Domain experts provide requirements for processes but often lack the technical programming skills to formalize them in a process specification language. We present an interactive SPL application prototype that allows domain experts to model simple processes in natural language. Our framework for the reliable generation of formal specifications with Large Language Models (LLMs) supports the machine-translation from natural language to a JSON-based process DSL. In this case study, five domain experts were asked to model any process of their choice through natural-language interactions. As a result, the user interface corresponding to the process DSL was shown as immediate feedback. We documented their perceived translation quality and interviewed them on their impressions of this methodology. An average user-assessed performance rating of 68% was achieved. Even though the modelling strategies differed greatly between individuals, the tool was able to adequately capture the majority of instructions, leaving an overall positive impression on the participants. More context awareness and additional conventional interaction elements were the main aspects found to be improved for a productive implementation.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {170–178},
numpages = {9},
keywords = {Domain-specific Languages, Generative Artificial Intelligence, Large Language Models, Process Management, Process Modelling, Reliable Code Generation},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.5555/1753235.1753250,
author = {Jepsen, Hans Peter and Beuche, Danilo},
title = {Running a software product line: standing still is going backwards},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Danfoss Drives - one of the largest producers of frequency converters in the world - has been doing Software Product Line development for its frequency converter products for about 3 years. This paper describes the approach used and the experiences with it. It discusses processes, ways to convince the unconvinced and arising tool issues when doing product line development.This paper is a follow-up on a previous article which described the product line migration process in detail.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {101–110},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/3646548.3672585,
author = {Landsberg, Tobias and Dietrich, Christian and Lohmann, Daniel},
title = {Should I Bother? Fast Patch Filtering for Statically-Configured Software Variants},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672585},
doi = {10.1145/3646548.3672585},
abstract = {In the face of critical security vulnerabilities, patch and update management are a crucial and challenging part of the software life cycle. In software product families, patching becomes even more challenging as we have to support different variants, which are not equally affected by critical patches. While the naive “better-patched-than-sorry” approach will apply all necessary updates, it provokes avoidable costs for developers and customers. In this paper we introduce SiB (Should I Bother?), a heuristic patch-filtering method for statically-configurable software that efficiently identifies irrelevant patches for specific variants. To solve the variability-aware patch-filtering problem, SiB compares modified line ranges from patches with those source-code ranges included in variants currently deployed. We apply our prototype for CPP-managed variability to four open-source projects (Linux, OpenSSL, SQLite, Bochs), demonstrating that SiB is both effective and efficient in reducing the number of to-be-considered patches for unaffected software variants. It correctly classifies up to 68 percent of variants as unaffected, with a recall of 100 percent, thus reducing deployments significantly, without missing any relevant patches.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {12–23},
numpages = {12},
keywords = {Patch Filtering, Software Evolution, Software Product Lines},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3546932.3547001,
author = {Six, Nicolas and Herbaut, Nicolas and Lopez-Herrejon, Roberto Erick and Salinesi, Camille},
title = {Using software product lines to create blockchain products: application to supply chain traceability},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547001},
doi = {10.1145/3546932.3547001},
abstract = {In recent years, blockchain has been growing rapidly from a niche technology to a promising solution for many sectors, due to its unique properties that empower the design of innovative applications. Nevertheless, the development of blockchain applications is still a challenge. Due to the technological novelty, only a few developers are familiar with blockchain technologies and smart contracts. Others might face a steep learning curve or difficulties to reuse existing code to build blockchain applications. This study proposes a novel approach to tackle these issues, through software product line engineering. To support the approach, a web platform to configure and generate a blockchain application for on-chain traceability is introduced. First, a feature model has been designed to model core features of the chosen domain, based on the existing literature. Then, a configurator has been implemented to support the feature selection phase. Finally, a generator is able to ingest such configurations to generate on-the-shelf blockchain products. The generalizability of the contribution is validated by reproducing on-chain traceability applications proposed in the literature by using the platform. This work provides the first evidence that the implementation of blockchain applications using software product lines enhances the quality of produced applications and reduces the time to market.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {97–107},
numpages = {11},
keywords = {blockchain, code generation, software product line},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3503229.3547026,
author = {Friesel, Birte and Elmenhorst, Kathrin and Kaiser, Lennart and M\"{u}ller, Michael and Spinczyk, Olaf},
title = {kconfig-webconf: retrofitting performance models onto kconfig-based software product lines},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547026},
doi = {10.1145/3503229.3547026},
abstract = {Despite decades of research and clear advantages, performance-aware configuration of real-world software product lines is still an exception rather than the norm. One reason for this may be tooling: configuration software with support for non-functional property models is generally not compatible with the configuration and build process of existing product lines. Specifically, the Kconfig language is popular in open source software projects, but neither language nor configuration frontends support performance models. To address this, we present kconfig-webconf: a performance-aware, Kconfig-compatible software product line configuration frontend. It is part of a toolchain that can automatically generate performance models with a minimal amount of changes to a software product line's build process. With such a performance model, kconfig-webconf can serve as a performance-aware drop-in replacement for existing Kconfig frontends. We evaluate its usage in five examples, including the busybox multi-call binary and the resKIL agricultural AI product line.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {58–61},
numpages = {4},
keywords = {kconfig, performance prediction, product lines, regression trees},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3336294.3336309,
author = {Temple, Paul and Acher, Mathieu and Perrouin, Gilles and Biggio, Battista and Jezequel, Jean-Marc and Roli, Fabio},
title = {Towards Quality Assurance of Software Product Lines with Adversarial Configurations},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336309},
doi = {10.1145/3336294.3336309},
abstract = {Software product line (SPL) engineers put a lot of effort to ensure that, through the setting of a large number of possible configuration options, products are acceptable and well-tailored to customers' needs. Unfortunately, options and their mutual interactions create a huge configuration space which is intractable to exhaustively explore. Instead of testing all products, machine learning is increasingly employed to approximate the set of acceptable products out of a small training sample of configurations. Machine learning (ML) techniques can refine a software product line through learned constraints and a priori prevent non-acceptable products to be derived. In this paper, we use adversarial ML techniques to generate adversarial configurations fooling ML classifiers and pinpoint incorrect classifications of products (videos) derived from an industrial video generator. Our attacks yield (up to) a 100% misclassification rate and a drop in accuracy of 5%. We discuss the implications these results have on SPL quality assurance.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {277–288},
numpages = {12},
keywords = {machine learning, quality assurance, software product line, software testing, software variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3503229.3547069,
author = {Comploi-Taupe, Richard and Francescutto, Giulia and Schenner, Gottfried},
title = {Applying incremental answer set solving to product configuration},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547069},
doi = {10.1145/3503229.3547069},
abstract = {In this paper, we apply incremental answer set solving to product configuration. Incremental answer set solving is a step-wise incremental approach to Answer Set Programming (ASP). We demonstrate how to use this technique to solve product configurations problems incrementally. Every step of the incremental solving process corresponds to a predefined configuration action. Using complex domain-specific configuration actions makes it possible to tightly control the level of non-determinism and performance of the solving process. We show applications of this technique for reasoning about product configuration, like simulating the behavior of a deterministic configuration algorithm and describing user actions.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {150–155},
numpages = {6},
keywords = {answer set programming, incremental solving, product configuration},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.5555/2814058.2814112,
author = {Lobato, Luanna Lopes and Bittar, Thiago Jabur},
title = {A Risk Management Approach for Software Product Line Engineering},
year = {2015},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {TSoftware Product Line (SPL) Engineering is a software development paradigm that fosters systematic reuse. It is focused on improving software practices, leading companies to experience benefits, such as reduced time-to-market and effort, and higher quality for the products delivered to customers. However, establishing a SPL is neither a simple nor a cheap task, and may affect several aspects of a software company. Besides, it involves a range of risks that may hinder project success. These have to be managed accordingly, so as to minimize the likelihood of project failure. Despite the importance of Risk Management (RM) for SPL Engineering, little has been published in terms of suitable and structured practices to cope with that. This present paper reports an approach for RM in SPL Engineering, named RiPLERM (Rise Product Line Engineering and Risk Management). The approach presents activities to structure RM in SPL projects, The design of the RiPLE-RM approach elaborated on results from empirical investigations, and was proposed to facilitate the management and provide significant insights that can be used to avoid and solve risks.},
booktitle = {Proceedings of the Annual Conference on Brazilian Symposium on Information Systems: Information Systems: A Computer Socio-Technical Perspective - Volume 1},
pages = {331–338},
numpages = {8},
keywords = {Project management, Risk Management, Software Process, Software Product Line Engineering},
location = {Goiania, Goias, Brazil},
series = {SBSI '15}
}

@inproceedings{10.1145/3023956.3023957,
author = {Wille, David and Runge, Tobias and Seidl, Christoph and Schulze, Sandro},
title = {Extractive software product line engineering using model-based delta module generation},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023957},
doi = {10.1145/3023956.3023957},
abstract = {To satisfy demand for customized products, companies commonly apply so-called clone-and-own strategies by copying functionality from existing products and modifying it to create product variants that have to be developed, maintained, and evolved in isolation. In previous work, we introduced a variability mining technique to identify variability information (commonalities and differences) in block-based model variants (e.g., MATLAB/Simulink models), which can be used to guide manual transition from clone-and-own to managed reuse of a software product line (SPL). In this paper, we present a procedure that uses the extracted variability information to generate a transformational delta-oriented SPL fully automatically. We generate a delta language specifically tailored to transforming models in the analyzed modeling language and utilize it to generate delta modules expressing variation of the SPL's implementation artifacts. The procedure seamlessly integrates with our variability mining technique and allows to fully adopt a managed reuse strategy (i.e., generation of products from a single code base) without manual overhead. We show the feasibility of the procedure by applying it to state chart and MATLAB/Simulink model variants from two industrial case studies.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {36–43},
numpages = {8},
keywords = {clone-and-own, delta modeling, extractive product line engineering, model-based, variability mining},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/3503229.3547033,
author = {Linsbauer, Lukas and Westphal, Paul and Bittner, Paul Maximilian and Krieter, Sebastian and Th\"{u}m, Thomas and Schaefer, Ina},
title = {Derivation of subset product lines in FeatureIDE},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547033},
doi = {10.1145/3503229.3547033},
abstract = {The development and configuration of software product lines can be challenging tasks. During development, engineers often need to focus on a particular subset of features that is relevant for them. In such cases, it would be beneficial to hide other features and their implementation. During product configuration, requirements of potentially multiple stakeholders need to be considered. Therefore, configuration often happens in stages, in which different people contribute configuration decisions for different features. Moreover, in some cases, stakeholders want to share a set of products rather than a specific one. In all these cases, the necessary operation is the same: some features from the product line are assigned a value (e.g., via a partial configuration) while other features remain configurable. In this work, we propose a subset operation that takes a product line and a partial configuration to derive a subset product line comprising only the desired subset of features and implementation artifacts. Furthermore, we present, evaluate, and publish our implementation of the proposed subset operation within the FeatureIDE framework.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {38–41},
numpages = {4},
keywords = {partial configuration, software product line, subset product line},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3503229.3547044,
author = {Abbasi, Ebrahim Khalil and Leclercq, Tony and Heymans, Patrick},
title = {A meta-model for product configuration ontologies},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547044},
doi = {10.1145/3503229.3547044},
abstract = {Conceptual modelling of product configuration is an essential step towards improving reuse and configuration knowledge sharing, systems interoperability, and people communication. Among several approaches proposed, ontology-based approaches are known to provide better support for the conceptualization of product configuration knowledge in terms of precision and expressiveness of reasoning and representation. This paper studies product configuration ontologies and presents a meta-model of concepts and their relationships that are used in those ontologies. The proposed meta-model consists of a generalisation hierarchy of configuration types, the compositional structure of a configurable product, the generalisation hierarchy of a component, and constraint types. The meta-model has been designed with the aim of first understanding the current state of product configuration ontologies and then extending it for integrating new concepts.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {166–173},
numpages = {8},
keywords = {knowledge representation, ontology, product configuration},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/2996890.3007893,
author = {Ruiz, Carlos and Duran-Limon, Hector A. and Parlavantzas, Nikos},
title = {Towards a software product line-based approach to adapt IaaS cloud configurations},
year = {2016},
isbn = {9781450346160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2996890.3007893},
doi = {10.1145/2996890.3007893},
abstract = {Cloud computing is nowadays one of the most promising IT technologies, since it provides seemingly unlimited resources on demand at low costs. Hence, different types of applications have been migrated to IaaS environments, e.g. multi-tier (distributed) applications. However, in order to benefit from such characteristics, cloud configurations (i.e. virtual resource configurations) should be designed accordingly to the necessities of the applications. Furthermore, such configurations have to provide the required resources not only at the application deployment-time, but also during the whole application execution time. Hence, adaptive paradigms are required when designing solutions to cloud applications with dynamic resource requirements. Software Product Lines (SPLs) provide great flexibility and a high level of abstraction to describe complete system configurations. Even though SPLs are not commonly used to describe changes after an initial product (configuration) has been created, their inherent characteristics can enable producing the required virtual resource configuration to adapt applications after their initial deployment, i.e., at runtime. In this paper, we present an approach to create and adapt cloud configurations at the IaaS level by using SPLs. We focus on the architectural design of our solution as well as on the possible implementation challenges we could face.},
booktitle = {Proceedings of the 9th International Conference on Utility and Cloud Computing},
pages = {398–403},
numpages = {6},
keywords = {cloud computing, self-adaptation, software product lines},
location = {Shanghai, China},
series = {UCC '16}
}

@inproceedings{10.1145/3646548.3672588,
author = {Burgstaller, Tamim and Garber, Damian and Le, Viet-Man and Felfernig, Alexander},
title = {Optimization Space Learning: A Lightweight, Noniterative Technique for Compiler Autotuning},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672588},
doi = {10.1145/3646548.3672588},
abstract = {Compilers are highly configurable systems. One can influence the performance of a compiled program by activating and deactivating selected compiler optimizations. However, automatically finding well-performing configurations is a challenging task. We consider expensive iteration, paired with recompilation of the program to optimize, as one of the main shortcomings of state-of-the-art approaches. Therefore, we propose Optimization Space Learning, a lightweight and noniterative technique. It exploits concepts known from configuration space learning and recommender systems to discover well-performing compiler configurations. This reduces the overhead induced by the approach significantly, compared to existing approaches. The process of finding a well-performing configuration is 800k times faster than with the state-of-the-art techniques.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {36–46},
numpages = {11},
keywords = {Collaborative Filtering, Compiler, Compiler Autotuning, Configuration, Configuration Space Learning, Performance Optimization},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3646548.3677031,
author = {Krieter, Sebastian and Greiner, Sandra and Assun\c{c}\~{a}o, Wesley K. G. and Lopez-Herrejon, Roberto E.},
title = {1st International Workshop on Reverse Variability Engineering and Evolution of Software-Intensive Systems (Re:Volution)},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3677031},
doi = {10.1145/3646548.3677031},
abstract = {Managing variability in configurable systems remains a challenging endeavor for multiple reasons. First, variability must be properly defined using domain knowledge and reusable software artifacts, often relying on extraction and refinement of legacy assets. Second, variability must be maintained during evolution in time (revisions) and space (variants). Recent research activities have focused on enabling the integrated management of evolution and variability. Existing approaches stem from multiple origins, notably software configuration management and product line engineering. The 1st International Workshop on Reverse Variability Engineering and Evolution of Software-Intensive Systems joins the motivation originating from REVE and VariVolution. It shall bring together active researchers eliciting software variability and studying its evolution, and practitioners who encounter these phenomena in the real-world. Re:Volution offers a platform to exchange ideas, case studies, and tools, fostering research collaborations and synergies.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {225},
numpages = {1},
keywords = {Evolution, configuration management, variability, version control},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3646548.3672591,
author = {Huyghe, Maxime and Quinton, Cl\'{e}ment and Rudametkin, Walter},
title = {Taming the Variability of Browser Fingerprints},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672591},
doi = {10.1145/3646548.3672591},
abstract = {Browser fingerprinting has become a prevalent technique for tracking and identifying users online, posing significant privacy risks. The increasing variability in web browser configurations, coupled with the continuous evolution of browser features, presents complex challenges in understanding and mitigating the impact of fingerprinting. In this paper, we introduce a novel approach that combines feature modeling techniques with tree-based representations to capture the intricate relationships and constraints within browser fingerprints. By translating 22,773 fingerprints into a feature model with 34,557 nodes, we enable a comprehensive analysis of their variability and uniqueness across 1,519 switches and 596 flags on 7 headless and headful browser versions. Our methodology facilitates various use cases, such as generating representative fingerprints for testing, detecting anomalies, and identifying discriminating attributes. We aim to provide developers and researchers with a powerful tool for studying browser fingerprints and developing effective strategies to enhance user privacy in the face of evolving tracking techniques.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {66–71},
numpages = {6},
keywords = {Browser Fingerprinting, Configuration, Feature Model Synthesis, Variability},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3579028.3609017,
author = {Bombarda, Andrea and Bonfanti, Silvia and Gargantini, Angelo},
title = {On the Reuse of Existing Configurations for Testing Evolving Feature Models},
year = {2023},
isbn = {9798400700927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579028.3609017},
doi = {10.1145/3579028.3609017},
abstract = {Software Product Lines (SPLs) are used for representing a variety of highly configurable systems or families of systems. They are commonly represented by feature models (FMs). Starting from FMs, configurations, used as test cases, can be generated to identify the products of interest for further activities. As the other types of software, SPLs and their FMs may evolve due to changing requirements or bug-fixing. However, no guidance is usually given on what to do with derived configurations when an FM evolves. The common approach is based on generating all configurations from scratch, which is not optimal since a greater effort is required for concretizing the new tests, and some of the old ones may be still applicable.In this paper, we present the use of a technique for generating combinatorial tests for evolving feature models: this technique incrementally builds the new combinatorial configuration set starting from the one generated from the previous model. Furthermore, we present a novel definition of dissimilarity among configuration sets that can be used to evaluate how much an evolved test suite differs from the previous one and thus allows evaluating the effort required for adapting old test cases to the new ones.Our experiments confirm that using the proposed technique, in general, leads to lower dissimilarity and test suite size w.r.t. the generation of tests from scratch.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
pages = {67–76},
numpages = {10},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3461001.3471147,
author = {Kenner, Andy and May, Richard and Kr\"{u}ger, Jacob and Saake, Gunter and Leich, Thomas},
title = {Safety, security, and configurable software systems: a systematic mapping study},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471147},
doi = {10.1145/3461001.3471147},
abstract = {Safety and security are important properties of any software system, particularly in safety-critical domains, such as embedded, automotive, or cyber-physical systems. Moreover, particularly those domains also employ highly-configurable systems to customize variants, for example, to different customer requirements or regulations. Unfortunately, we are missing an overview understanding of what research has been conducted on the intersection of safety and security with configurable systems. To address this gap, we conducted a systematic mapping study based on an automated search, covering ten years (2011--2020) and 65 relevant (out of 367) publications. We classified each publication based on established security and safety concerns (e.g., CIA triad) as well as the connection to configurable systems (e.g., ensuring security of such a system). In the end, we found that considerably more research has been conducted on safety concerns, but both properties seem under-explored in the context of configurable systems. Moreover, existing research focuses on two directions: Ensuring safety and security properties in product-line engineering; and applying product-line techniques to ensure safety and security properties. Our mapping study provides an overview of the current state-of-the-art as well as open issues, helping practitioners identify existing solutions and researchers define directions for future research.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {148–159},
numpages = {12},
keywords = {configurable systems, mapping study, safety, security, software product line engineering},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2797433.2797456,
author = {Galster, Matthias},
title = {Architecting for Variability in Quality Attributes of Software Systems},
year = {2015},
isbn = {9781450333931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797433.2797456},
doi = {10.1145/2797433.2797456},
abstract = {Variability in software systems is usually concerned with variability in features and functionality. However, variability also occurs in quality attributes (e.g., performance, security) and quality attribute requirements (for example, a performance requirement may state that a system must respond to a user request within 0.1 seconds). We discuss what variability in quality attributes is, including several scenarios in which variability in quality attributes can occur. We then discuss the state of research and what we know about variability in quality attributes, including some existing research to address the challenge of identifying, implementing and managing variability in quality attributes. Finally, we discuss potential directions for future research.},
booktitle = {Proceedings of the 2015 European Conference on Software Architecture Workshops},
articleno = {23},
numpages = {4},
keywords = {Variability, quality attributes, software architecture},
location = {Dubrovnik, Cavtat, Croatia},
series = {ECSAW '15}
}

@inproceedings{10.1145/3503229.3547043,
author = {Grosso, Chiara and Sazen, Noorie and Boselli, Roberto},
title = {AI-implemented toolkit to assist users with career "configuration": the case of create your own future},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547043},
doi = {10.1145/3503229.3547043},
abstract = {Labor markets (LMs) are witnessing an acceleration that is fueled by digital disruption. The main drivers of this change are the transition to the digital paradigm (that is, Industry 4.0 and Smart Industry 5.0) and the socioeconomic effects of the ongoing health emergency. In addition, the recent Russia/Ukraine conflict has resulted in the migration of an increasing number of refugees across EU countries who may also need to change their career focus in order to find job opportunities in their host countries. These new changes in the LMs primarily affect lower-skilled workers and those who lack or have low levels of digital literacy. In such a situation, there is a need for a solution that can be adopted on a mass scale to reallocate human resources in order to help the most vulnerable workers who may face difficulties in maintaining or finding new job opportunities. The present paper aims to contribute to research on possible solutions to help individuals become career adaptable. To this end, we present the case of Create Your Own Future (CYOF), an employability support toolkit by Saffron Interactive, to demonstrate how an AI-implemented toolkit can support individuals in configuring a road map for their career pathway. CYOF is designed to support individuals on their personal journey towards sustainable employment and provides individuals with a way of determining their vocational personality and find a tailored roadmap to progress in their career or a pathway to a new one. The CYOF toolkit could represent a large-scale solution to help workers become career adaptable and help them face an increasingly changing and unpredictable employment landscape.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {158–165},
numpages = {8},
keywords = {digital disruption, employability support platform, job career configuration, skills assessment},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.5555/3106050.3106053,
author = {H\"{a}nsel, Joachim and Giese, Holger},
title = {Towards collective online and offline testing for dynamic software product line systems},
year = {2017},
isbn = {9781538628034},
publisher = {IEEE Press},
abstract = {Dynamic Software Product Line (DSPLs) based Systems are capable of adapting in response to changes concerning their observations at runtime in order to exhibit appropriate behavior. The observation space and the variability in the configuration space is usually known at design time. However, running a set of tests with all combinations of configuration and observation from these spaces is likely to be infeasible. We propose to make use of monitoring results from multiple instances of systems derived from a DSPL at runtime collecting their observations and the employed configurations. The collective of systems is enabled to profit from an operational profile with regard to proper coverage by systematic tests. The systematic tests are carried out offline. Additional online testing further improves the confidence in the system.},
booktitle = {Proceedings of the 2nd International Workshop on Variability and Complexity in Software Design},
pages = {9–12},
numpages = {4},
keywords = {dynamic software product lines, offline testing, online testing, self-adaptive software systems},
location = {Buenos Aires, Argentina},
series = {VACE '17}
}

@inproceedings{10.1145/3646548.3672595,
author = {Bounouas, Nassim and Blay-Fornarino, Mireille and Collet, Philippe},
title = {Tracing and Fixing Inconsistencies in Clone-and-Own Tabular Data Models},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672595},
doi = {10.1145/3646548.3672595},
abstract = {Many data-intensive applications handle tabular data with more advanced structuring and processes than spreadsheets, enabling end-users to copy and adapt tabular data and processes to create new templates or datasets anytime. Recent research advances demonstrated that, in such clone-and-own scenarios, actions performed on the data structure, together with cloning and adaptation actions, can be captured within an operation-based model to prevent the drift of the internal tabular data model. However, this approach is limited by the assumption that each operation must maintain consistency regarding dependencies generated by the domain-specific languages that connect the observed and computed data. To address this challenge, this paper first introduces an evolved operation-based model that is designed to capture inconsistent tabular data while keeping a fine-grained trace of what part of the model is inconsistent. We then define specific trace operations to either fix a dependency in a model or remove one if its creating process is no longer relevant to the user. These operations support high-level editing scenarios on the tabular data, which enables easily fixing the equivalent of a spreadsheet formula or a process statement, or making the user aware that some part of the model is inconsistent while it is cloned. Additionally, we report on a positive scalability experiment on the tracing of large tabular data models with inconsistencies.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {191–202},
numpages = {12},
keywords = {Tabular data, agronomy, clone-and-own, model-driven engineering, operation-based modeling, variability management},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3167132.3167350,
author = {Fischer, Stefan and Lopez-Herrejon, Roberto Erick and Egyed, Alexander},
title = {Towards a fault-detection benchmark for evaluating software product line testing approaches},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167350},
doi = {10.1145/3167132.3167350},
abstract = {Software Product Lines (SPLs) are families of related software systems distinguished by the set of features each one provides. The commonly large number of variants that can be derived from an SPL poses a unique set of challenges, because it is not feasible to test all the individual variants. Over the last few years many approaches for SPL testing have been devised. They usually select a set of variants to test based on some covering criterion. A problem when evaluating these testing approaches is properly comparing them to one another. Even though some benchmarks have been proposed, they focus on covering criteria and do not consider fault data in their analysis. Considering the dire lack of publicly available fault data, in this paper we present the first results of our ongoing project to introduce simulated faults into SPLs along with using evolutionary techniques for synthesizing unit test cases for SPL examples.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {2034–2041},
numpages = {8},
keywords = {mutation testing, software product lines},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3646548.3674548,
author = {Mannion, Mike and Kaindl, Hermann},
title = {Comparing Products using Similarity Matching},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3674548},
doi = {10.1145/3646548.3674548},
abstract = {Software product lines can generate significant cost-efficiencies but managing their evolution is becoming more difficult. There are several reasons. The volume, variety and velocity of products is increasing particularly as customers demand personalized products. The corporate memory about the product line's history and development recedes as the personnel that manage and develop the product line ebb and flow from the organization. The economic, environmental, regulatory and social contexts into which the products were launched also continues to evolve. A significant challenge of the management task is product comparison. Product comparison is important for gauging whether (i) to build a new product or not (ii) a product aligns with the corporate brand (iii) a product falls within legislative and regulatory boundaries (iv) a product line platform needs to be reorganized. Product comparison can be of value during product-line scoping, domain engineering and application engineering. We will discuss two different approaches to address this challenge. One is grounded in feature modelling, the other in case-based reasoning.We will also describe a specific product comparison approach using similarity matching, in which a product configured from a product line feature model is represented as a weighted binary string. Weighted binary strings can represent product features and a binary string metric can be used to evaluate product and feature similarity (where 1 represents a feature's presence, 0 its absence, and the weight represents its relative importance to the product). Binary strings map easily to feature selection processes, are low on storage requirements and enable fast comparison computations with existing similarity metrics and measuring tools. For large feature trees, the deployment of weights for each feature is only feasible if the initial allocation is done automatically.The overall similarity between products can be compared using a binary string metric, and the significance of individual feature combinations for product similarity can be explored by modifying the weights. We will illustrate our ideas with a mobile phone example, and discuss some of the benefits and limitations of this approach.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {221},
numpages = {1},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@article{10.1145/2382756.2382778,
author = {Tekinerdogan, Bedir},
title = {First turkish software product line engineering workshop summary},
year = {2012},
issue_date = {November 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/2382756.2382778},
doi = {10.1145/2382756.2382778},
abstract = {Software reuse has been a goal of the software community since the early days of software engineering. In this context software product line engineering (SPLE) has gained a broad interest in both academic institutions and industry. This trend can also be observed in Turkey. In the recent years an increasing number of software companies in Turkey have adopted a SPLE approach while others are planning to make the transition. This paper summarizes the results of the First Turkish Software Product Line Engineering Workshop that has been organized in Ankara in June 2012. The primary goal of the workshop was to reflect on the state of practice in SPLE in Turkey. For this five leading SPLE companies in Turkey have shared their experiences in adopting SPLE, and using interactive discussions a research agenda for SPLE in Turkey has been defined. We report both on the experiences from the workshop and the resulting research topics.},
journal = {SIGSOFT Softw. Eng. Notes},
month = nov,
pages = {30–34},
numpages = {5},
keywords = {software product line engineering, software reuse, technology transfer, workshop organization}
}

@inproceedings{10.1145/3132498.3133835,
author = {Cardoso, Mateus Passos Soares and Lima, Crescencio and de Almeida, Eduardo Santana and do Carmo Machado, Ivan and von Flach G. Chavez, Christina},
title = {Investigating the variability impact on the recovery of software product line architectures: an exploratory study},
year = {2017},
isbn = {9781450353250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132498.3133835},
doi = {10.1145/3132498.3133835},
abstract = {The Product Line Architecture (PLA) of a Software Product Line (SPL) is the core architecture that represents a high-level design for all the products of an SPL, including variation points and variants. If PLA documentation is missing, it can be recovered by reverse engineering the products. The recovered PLA is a relevant asset for developers and architects, that can be used to drive specific activities of SPL development and evolution, such as, understanding its structure and its variation points, and assessing reuse. This paper presents an exploratory study that investigated the effectiveness of recovered PLAs to address variability identification and support reuse assessment. We recovered the PLA of 15 open source SPL projects using the PLAR, a tool that supports PLA recovery and assessment based on information extracted from SPL products' source code. For each project, reuse assessment was supported by existing reuse metrics. The yielded results revealed that the number of products used in PLA recovery affected the variability identification, and the number of optional features affected the components reuse rate. These findings suggest that a minimum set of representative products should be identified and selected for PLA recovery, and the component reuse rate is a candidate metric for SPL reuse assessment.},
booktitle = {Proceedings of the 11th Brazilian Symposium on Software Components, Architectures, and Reuse},
articleno = {12},
numpages = {10},
keywords = {product line architecture, product line architecture recovery, software product lines, variability},
location = {Fortaleza, Cear\'{a}, Brazil},
series = {SBCARS '17}
}

@inproceedings{10.1145/2851613.2851964,
author = {Cool, Benjamin and Knieke, Christoph and Rausch, Andreas and Schindler, Mirco and Strasser, Arthur and Vogel, Martin and Brox, Oliver and Jauns-Seyfried, Stefanie},
title = {From product architectures to a managed automotive software product line architecture},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851964},
doi = {10.1145/2851613.2851964},
abstract = {To keep the software development for vehicles cost efficient, software components are reused for different variants as well as for succeeding generations. Furthermore, cost reductions are achieved by software sharing between the Original Equipment Manufacturer (OEM) and the suppliers. However, as a consequence of the blackboxed view caused by software sharing, no common detailed software product line architecture specification for the Electronic Control Unit (ECU) software exists, as it would be required for analyzing the quality of the product line architecture, planning changes on the product line architecture, checking the compliance between the product architecture and the product line architecture, and therefore, avoiding architecture erosion. Thus, after several product generations, software erosion is growing steadily, resulting in an increasing effort of reusing software components, and planning of further development. Here, we propose an approach for repairing an eroded software consisting of a set of product architectures by applying strategies for recovery and discovery of the product line architecture. Furthermore, we give a methodology for a long-term manageable, plannable, and reuseable software product line architecture for automotive software systems.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1350–1353},
numpages = {4},
keywords = {architecture evolution, architecture quality measures, automotive, software erosion, software product lines},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/2430502.2430529,
author = {Zhang, Bo and Becker, Martin},
title = {Mining complex feature correlations from software product line configurations},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430529},
doi = {10.1145/2430502.2430529},
abstract = {As a Software Product Line (SPL) evolves with increasing number of features and feature values, the feature correlations become extremely intricate, and the specifications of these correlations tend to be either incomplete or inconsistent with their realizations, causing misconfigurations in practice. In order to guide product configuration processes, we present a solution framework to recover complex feature correlations from existing product configurations. These correlations are further pruned automatically and validated by domain experts. During implementation, we use association mining techniques to automatically extract strong association rules as potential feature correlations. This approach is evaluated using a large-scale industrial SPL in the embedded system domain, and finally we identify a large number of complex feature correlations.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {19},
numpages = {7},
keywords = {association mining, feature correlation, product line configuration},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@inproceedings{10.1145/3267183.3267188,
author = {Barbosa, Jefferson and Andrade, Rossana M. C. and Filho, Jo\~{a}o Bosco F. and Bezerra, Carla I. M. and Barreto, Isaac and Capilla, Rafael},
title = {Cloning in Customization Classes: A Case of a Worldwide Software Product Line},
year = {2018},
isbn = {9781450365543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267183.3267188},
doi = {10.1145/3267183.3267188},
abstract = {Cloning-and-owning, in the long run, can severely affect evolution, as changes in cloned fragments may require modifications in various parts of the system. This problem scales if cloning is used in classes that derive products in a Software Product Line, because these classes can impact in several features and products. However, it is hard to know to which extent cloning in customization classes can impact in a project. We conduct a study, within an SPL that generates mobile software for over 150 countries, to analyze cloning practices and how cloned parts relate to the maintainability of customization classes. We collect and identify clones inside customization classes during a period of 13 months, involving 70 customization classes and 5 branches. In parallel, we collect the respective issues from the issue tracking tool of the SPL project, obtaining over 140 issues related to customization classes. We then confront the time spent to solve each issue with its nature (i.e., if it relates to cloned code or not). As first result, we verify that issues related to cloning take in average 136% more time to be solved. Our study helps to understand how cloning relates to maintainability in the context of mass customization, giving insights about cloned code evolution and its impacts in a software product line project.},
booktitle = {Proceedings of the VII Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {43–52},
numpages = {10},
keywords = {Clone, Customization, Software Product Line},
location = {Sao Carlos, Brazil},
series = {SBCARS '18}
}

@inproceedings{10.1145/1808937.1808938,
author = {Hanssen, Geir Kjetil},
title = {Opening up software product line engineering},
year = {2010},
isbn = {9781605589688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1808937.1808938},
doi = {10.1145/1808937.1808938},
abstract = {The software industry is experiencing a shift towards more open processes, a globalized market and more active and engaged customers and end users. This change seems natural and inevitable, imposing necessary changes in how software product line organizations plan and drive the development of their products. This paper gives insight into some recent developments in a product line organization and discusses how their efforts have helped them in improving their development processes and their product line. Based on this experience, this paper provides some preliminary guidelines to both industry and research, indicating that software product line organizations should exploit open innovation, engage customers, build communities and simplify processes and organization.},
booktitle = {Proceedings of the 2010 ICSE Workshop on Product Line Approaches in Software Engineering},
pages = {1–7},
numpages = {7},
keywords = {open processes, software product line engineering},
location = {Cape Town, South Africa},
series = {PLEASE '10}
}

@inproceedings{10.1145/3546932.3547003,
author = {Eggert, Matthias and G\"{u}nther, Karsten and Maletschek, Jochen and Maxiniuc, Alexandru and Mann-Wahrenberg, Alexander},
title = {In three steps to software product lines: a practical example from the automotive industry},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547003},
doi = {10.1145/3546932.3547003},
abstract = {In the automotive industry, suppliers aim to increase their revenue and try to keep up with the pace of the market trends to stay competitive by offering off-the-shelf products to car manufacturers. On the other hand those car manufacturers request tailored products to gain unique selling points. Each new customer request may result in a new software project. To save time one might find it a good idea to create the new software project as a copy of an older one. This method guarantees initial functionality, but prevents refactoring and leads to continuous software erosion. The implementations diverge from each other and improvements cannot be shared. Software Product Lines (SPL) can help to maximize reusability and quality by building up shared core assets and customer-specific functionality. In our paper, we propose a method to migrate a customer project landscape into a scalable SPL in three steps.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {170–177},
numpages = {8},
keywords = {active projects, adoption, automotive, functionality reuse, incremental migration, migration, software product line},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.5555/2662572.2662582,
author = {Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {Representation of software product line architectures for search-based design},
year = {2013},
isbn = {9781467362849},
publisher = {IEEE Press},
abstract = {The Product-Line Architecture (PLA) is the main artifact of a Software Product Line (SPL). Search-based approaches can provide automated discovery of near-optimal PLAs and make its design less dependent on human architects. To do this, it is necessary to adopt a suitable PLA representation to apply the search operators. In this sense, we review existing architecture representations proposed by related work, but all of them need to be extended to encompass specific characteristics of SPL. Then, the use of such representations for PLA is discussed and, based on the performed analysis, we introduce a novel direct PLA representation for search-based optimization. Some implementation aspects are discussed involving implementation details about the proposed PLA representation, constraints and impact on specific search operators. Ongoing work addresses the application of specific search operators for the proposed representation and the definition of a fitness function to be applied in a multi-objective search-based approach for the PLA design.},
booktitle = {Proceedings of the 1st International Workshop on Combining Modelling and Search-Based Software Engineering},
pages = {28–33},
numpages = {6},
keywords = {architecture modelling, multi-objective search-based approach, software product line},
location = {San Francisco, California},
series = {CMSBSE '13}
}

@inproceedings{10.1145/3461001.3471144,
author = {Uta, Mathias and Felfernig, Alexander and Le, Viet-Man and Popescu, Andrei and Tran, Thi Ngoc Trang and Helic, Denis},
title = {Evaluating recommender systems in feature model configuration},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471144},
doi = {10.1145/3461001.3471144},
abstract = {Configurators can be evaluated in various ways such as efficiency and completeness of solution search, optimality of the proposed solutions, usability of configurator user interfaces, and configuration consistency. Due to the increasing size and complexity of feature models, the integration of recommendation algorithms with feature model configurators becomes relevant. In this paper, we show how the output of a recommender system can be evaluated within the scope of feature model configuration scenarios. Overall, we argue that the discussed ways of measuring recommendation quality help developers to gain a broader view on evaluation techniques in constraint-based recommendation domains.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {58–63},
numpages = {6},
keywords = {configuration, evaluation, feature models, recommender systems},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3579027.3608989,
author = {K\"{o}nig, Christoph and Rosiak, Kamil and Cleophas, Loek and Schaefer, Ina},
title = {True Variability Shining Through Taxonomy Mining},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608989},
doi = {10.1145/3579027.3608989},
abstract = {Software variants of a Software Product Line (SPL) consist of a set of artifacts specified by features. Variability models document the valid relationships between features and their mapping to artifacts. However, research has shown inconsistencies between the variability of variants in features and artifacts, with negative effects on system safety and development effort. To analyze this mismatch in variability, the causal relationships between features, artifacts, and variants must be uncovered, which has only been addressed to a limited extent. In this paper, we propose taxonomy graphs as novel variability models that reflect the composition of variants from artifacts and features, making mismatches in variability explicit. Our evaluation with two SPL case studies demonstrates the usefulness of our variability model and shows that mismatches in variability can vary significantly in detail and severity.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {182–193},
numpages = {12},
keywords = {Software Product Lines, Taxonomy, Variability Modeling},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3503229.3547039,
author = {Vandevelde, Simon and Callewaert, Benjamin and Vennekens, Joost},
title = {Interactive feature modeling with background knowledge for validation and configuration},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547039},
doi = {10.1145/3503229.3547039},
abstract = {Feature modeling enables a straightforward representation of a product's features, components, and the relations between them. In this way, feature models serve as an excellent approach to diagrammatically model a product design for manufacturing purposes. However, the actual usage of such a feature model to generate suitable designs in the context of real-life industry applications is often limited, as crucial background knowledge cannot be expressed. Moreover, even though expert validation of a feature model is an important aspect of its creation, current tooling often falls short on this aspect. Indeed, although state-of-the-art tools are capable of generating possible configurations, this is not sufficient to completely validate complex applications: instead, we should enable the expert to interactively explore the problem domain. In this paper, we present our feature modeling tool, called FM-IDP, which aims to overcome both of these shortcomings. In FM-IDP, background knowledge can be expressed in FO(·), a rich extension of classical first-order logic. Using an off-the-shelf logical reasoning engine and an integrated interactive configuration interface, modelers can interact with the feature model and its background knowledge to explore the problem space on-the-fly. We motivate our approach using an industrial use case focused on real-life component design.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {209–216},
numpages = {8},
keywords = {IDP, background knowledge, feature modeling, interactive configuration},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3646548.3672586,
author = {Fernandez-Amoros, David and Heradio, Ruben and Horcas Aguilera, Jose Miguel and Galindo, Jos\'{e} A. and Benavides, David and Fuentes, Lidia},
title = {Pragmatic Random Sampling of the Linux Kernel: Enhancing the Randomness and Correctness of the conf Tool},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672586},
doi = {10.1145/3646548.3672586},
abstract = {The configuration space of some systems is so large that it cannot be computed. This is the case with the Linux Kernel, which provides almost 19,000 configurable options described across more than 1,600 files in the Kconfig language. As a result, many analyses of the Kernel rely on sampling its configuration space (e.g., debugging compilation errors, predicting configuration performance, finding the configuration that optimizes specific performance metrics, etc.). The Kernel can be sampled pragmatically, with its built-in tool conf, or idealistically, translating the Kconfig files into logic formulas. The pros of the idealistic approach are that it provides statistical guarantees for the sampled configurations, but the cons are that it sets out many challenging problems that have not been solved yet, such as scalability issues. This paper introduces a new version of conf called randconfig+, which incorporates a series of improvements that increase the randomness and correctness of pragmatic sampling and also help validate the Boolean translation required for the idealistic approach. randconfig+ has been tested on 20,000 configurations generated for 10 different Kernel versions from 2003 to the present day. The experimental results show that randconfig+ is compatible with all tested Kernel versions, guarantees the correctness of the generated configurations, and increases conf’s randomness for numeric and string options.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {24–35},
numpages = {12},
keywords = {Kconfig, SAT, configurable systems, randconfig, random sampling, software product lines, variability modeling},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3646548.3672589,
author = {Pett, Tobias and Krieter, Sebastian and Th\"{u}m, Thomas and Schaefer, Ina},
title = {MulTi-Wise Sampling: Trading Uniform T-Wise Feature Interaction Coverage for Smaller Samples},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672589},
doi = {10.1145/3646548.3672589},
abstract = {Ensuring the functional safety of highly configurable systems often requires testing representative subsets of all possible configurations. The ratio of covered t-wise feature interactions is a common criterion for determining whether a subset of configurations is representative and capable of finding faults. Existing t-wise sampling algorithms cover t-wise feature interactions for all features equally, resulting in extensive sampling times and large sample sizes. In this paper, we introduce a novel approach to t-wise feature interaction sampling, relating the necessity of equal coverage across all t-wise feature interactions, called MulTi-Wise Sampling. MulTi-Wise Sampling prioritizes between subsets of critical and non-critical features, considering higher t-values for subsets of critical features when generating a sample. We evaluate MulTi-Wise Sampling using subject systems from real-world applications, including BusyBox, Soletta, Fiasco, and uCLibc-ng. Our results show that MulTi-Wise Sampling provides a flexible mechanism to reduce the number of configurations for testing, while the performance of the algorithm is still on par with state-of-the-art sampling algorithms.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {47–53},
numpages = {7},
keywords = {sampling, software-product lines, spl testing, t-wise coverage},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3233027.3233045,
author = {Becker, Martin and Zhang, Bo},
title = {How do our neighbours do product line engineering? a comparison of hardware and software product line engineering approaches from an industrial perspective},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233045},
doi = {10.1145/3233027.3233045},
abstract = {Product line engineering (PLE) approaches have been followed in industry for hardware and software solutions for more than three decades now. However, the different engineering disciplines (e.g. mechanics, electrics, software) have developed and evolved their approaches within their own realms, which is fine as long as there is no need for integrated approaches. Driven by the increasing complexity of systems, there is a rising need for interdisciplinary systems engineering these days. Companies engineering cyber-physical systems and their components have to integrate product line engineering approaches across the involved engineering disciplines to enable a global optimization of portfolio, solution structures, and assets along their lifecycle. From a bird's-eye view, there is noticeable commonality but also variety in the approaches followed for PLE in the different engineering disciplines, which renders the integration of approaches a non-trivial endeavour. In order to foster the development of integrated PLE approaches, this paper explores, maps, and compares PLE approaches in the field of hardware and software engineering. Furthermore, the paper identifies integration opportunities and challenges. As the paper targets industrial practitioners, it mainly provides references to respective industrial events and material and does not fully cover related work in the respective research communities.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {190–195},
numpages = {6},
keywords = {SPLC, academia, industry, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/1842752.1842773,
author = {McGregor, John D.},
title = {A method for analyzing software product line ecosystems},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842773},
doi = {10.1145/1842752.1842773},
abstract = {The ecosystem for a software product line includes all of the entities with which the software product line organization interacts. Information, artifacts, customers, money and products move among these entities as a part of the planning, development, and deployment processes. In this paper we present an analysis technique that uses the economic notion of a transaction to examine the transfers between the entities. The result of the analysis is data that is used to evaluate and structure the organization. We illustrate with an example.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {73–80},
numpages = {8},
keywords = {software ecosystem, software product line},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@inproceedings{10.5555/1753235.1753273,
author = {Takebe, Yasuaki and Fukaya, Naohiko and Chikahisa, Masaki and Hanawa, Toshihide and Shirai, Osamu},
title = {Experiences with software product line engineering in product development oriented organization},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Core asset development is a key activity for software product line engineering. However, for many organizations, it is difficult to set up a core asset development team due to environmental factors such as their business, scale and culture.This paper describes our method for software product line engineering in such product development oriented organization. In this method, a champion team is organized from champion members belonging to each product development teams, which is in charge of decision making on core asset issues.We present our experiences of applying our method to embedded software for a clinical analyzer product line in a product development oriented organization. We analyzed the cost of core asset development with a champion team and confirmed that the overhead of coordinating between product development teams is negligible in our case study. We also discuss some practices we had in our case study to overcome possible problems of core asset development in a product development oriented organizations.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {275–283},
numpages = {9},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2593882.2593888,
author = {Metzger, Andreas and Pohl, Klaus},
title = {Software product line engineering and variability management: achievements and challenges},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593888},
doi = {10.1145/2593882.2593888},
abstract = {Software product line engineering has proven to empower organizations to develop a diversity of similar software-intensive systems (applications) at lower cost, in shorter time, and with higher quality when compared with the development of single systems. Over the last decade the software product line engineering research community has grown significantly. It has produced impressive research results both in terms of quality as well as quantity. We identified over 600 relevant research and experience papers published within the last seven years in established conferences and journals. We briefly summarize the major research achievements of these past seven years. We structure this research summary along a standardized software product line framework. Further, we outline current and future research challenges anticipated from major trends in software engineering and technology.},
booktitle = {Future of Software Engineering Proceedings},
pages = {70–84},
numpages = {15},
keywords = {Software product lines, design, quality assurance, requirements engineering, variability management, variability modeling},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@inproceedings{10.1145/2889443.2889451,
author = {Cafeo, Bruno B. P. and Hunsen, Claus and Garcia, Alessandro and Apel, Sven and Lee, Jaejoon},
title = {Segregating feature interfaces to support software product line maintenance},
year = {2016},
isbn = {9781450339957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889443.2889451},
doi = {10.1145/2889443.2889451},
abstract = {Although software product lines are widely used in practice, their maintenance is challenging. Features as units of behaviour can be heavily scattered across the source code of a product line, hindering modular reasoning. To alleviate this problem, feature interfaces aim at enhancing modular reasoning about features. However, considering all members of a feature interface is often cumbersome, especially due to the large number of members arising in practice. To address this problem, we present an approach to group members of a feature interface based on their mutual dependencies. We argue that often only a subset of all interface members is relevant to a maintenance task. Therefore, we propose a graph representation that is able to capture the collaboration between members and apply a clustering algorithm to it to group highly-related members and segregate non-related members. On a set of ten versions of a real-world product line, we evaluate the effectiveness of our approach, by comparing the two types of feature interfaces (segregated vs. original interfaces) with co-change information from the version-control system. We found a potential reduction of 62% of the interface members to be considered during maintenance. This way, the effort to reason about features can be reduced.},
booktitle = {Proceedings of the 15th International Conference on Modularity},
pages = {1–12},
numpages = {12},
keywords = {Feature Dependencies, Feature Interface, Software Product Lines},
location = {M\'{a}laga, Spain},
series = {MODULARITY 2016}
}

@inproceedings{10.1145/3646548.3672593,
author = {Greiner, Sandra and Schulthei\ss{}, Alexander and Bittner, Paul Maximilian and Th\"{u}m, Thomas and Kehrer, Timo},
title = {Give an Inch and Take a Mile? Effects of Adding Reliable Knowledge to Heuristic Feature Tracing},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672593},
doi = {10.1145/3646548.3672593},
abstract = {Tracing features to software artifacts is a crucial yet challenging activity for developers of variability-intensive software projects. Developers can provide feature traces either proactively in a manual and rarely semi-automated way or recover them retroactively where automated approaches mainly rely on heuristics. While proactive tracing promises high reliability as developers know which features they realize when working on them, the task is cumbersome and without immediate benefit. Conversely, automated retroactive tracing offers high automation by employing heuristics but remains unreliable and dependent on the quality of the heuristic. To exploit the benefits of proactive and retroactive tracing while mitigating their drawbacks, this paper examines how providing a minimal seed of accurate feature traces proactively (give an inch) can boost the accuracy of automated, heuristic-based retroactive tracing (take&nbsp;a&nbsp;mile). We examine how comparison-based feature location, as one representative of retroactive feature tracing, can benefit from increasing amounts of proactively provided feature mappings. For retroactive comparison-based feature tracing, we find not only that increasing amounts of proactive information can boost the overall accuracy of the tracing but also that the number of variants available for comparison affects the effectiveness of the combined tracing. As a result, our work lays the foundations to optimize the accuracy of retroactive feature tracing techniques with pinpointed proactive knowledge exploitation.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {84–95},
numpages = {12},
keywords = {software evolution, software product lines, software variability},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3461001.3471148,
author = {Krieter, Sebastian and Arens, Rahel and Nieke, Michael and Sundermann, Chico and He\ss{}, Tobias and Th\"{u}m, Thomas and Seidl, Christoph},
title = {Incremental construction of modal implication graphs for evolving feature models},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471148},
doi = {10.1145/3461001.3471148},
abstract = {A feature model represents a set of variants as configurable features and dependencies between them. During variant configuration, (de)selection of a feature may entail that other features must or cannot be selected. A Modal Implication Graph (MIG) enables efficient decision propagation to perform automatic (de)selection of subsequent features. In addition, it facilitates other configuration-related activities such as t-wise sampling. Evolution of a feature model may change its configuration logic, thereby invalidating an existing MIG and forcing a full recomputation. However, repeated recomputation of a MIG is expensive, and thus hampers the overall usefulness of MIGs for frequently evolving feature models. In this paper, we devise a method to incrementally compute updated MIGs after feature model evolution. We identify expensive steps in the MIG construction algorithm, enable them for incremental computation, and measure performance compared to a full rebuild of a complete MIG within the evolution histories of four real-world feature models. Results show that our incremental method can increase the speed of MIG construction by orders of magnitude, depending on the given scenario and extent of evolutionary changes.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {64–74},
numpages = {11},
keywords = {configurable system, evolution, software product line},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2591062.2591089,
author = {Cafeo, Bruno B. P.},
title = {Enhancing feature interfaces for supporting software product line maintenance},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591089},
doi = {10.1145/2591062.2591089},
abstract = {Software product line (SPL) is a technology aimed at speeding up the development process. Although SPLs are widely used, their maintenance is a challenging task. In particular, when maintaining a SPL feature, developers need to know which parts of other dependent features might be affected by this maintenance. Otherwise, further maintenance problems can be introduced in the SPL implementation. However, the identification and understanding of the so-called feature dependencies in the source code are an exhaustive and error-prone task. In fact, developers often ignore unconsciously feature dependencies while reasoning about SPL maintenance. To overcome this problem, this PhD research aims at understanding the properties of feature dependencies in the source code that exert impact on SPL maintenance. Furthermore, we propose a way to structure and segregate feature interfaces in order to help developers to identify and understand feature dependencies, thus reducing the effort and avoiding undesirable side effects in SPL maintenance.},
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {710–713},
numpages = {4},
keywords = {Feature Dependencies, Feature Interface, Feature Modularisation, Software Maintenance, Software Product Lines},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@inproceedings{10.1145/3579028.3609018,
author = {Nienaber, S\"{o}ren and Soorati, Mohammad D. and Ghasemzadeh, Arash and Ghofrani, Javad},
title = {Software Product Lines for Development of Evolutionary Robots},
year = {2023},
isbn = {9798400700927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579028.3609018},
doi = {10.1145/3579028.3609018},
abstract = {Evolutionary Robotics utilizes evolutionary algorithms for training robot controllers (e.g., neural networks) and adapting robot morphologies for different environments in design and runtime. One of the main challenges in robotics is the lack of reusability as AI-based robot controllers have to be trained from scratch for any change in the environment or a new task specification that a robot should adapt to. Training Artificial Neural Networks can be computationally heavy, time-consuming, and hard to reuse due to their monolithic black-box nature. The building blocks of emerging behaviors from Artificial Neural Networks cannot be fully separated or reused. We address the issue of reusability and propose an incremental approach for applying the reusability of behaviors. We implemented an Evolutionary Robotics framework to form a product family of robots. This product family is used to show the feasibility of our method for handling variability in a domain. Our results can be used to demonstrate a sample binding between the software product lines and machine learning domains.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
pages = {77–84},
numpages = {8},
keywords = {Configuration, Evolutionary Robotics, Mobile Robots, Primitive Behaviors, Software Product Lines},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@article{10.1145/2853073.2853082,
author = {Soujanya, K. L.S. and AnandaRao, A.},
title = {A Generic Framework for Configuration Management of SPL and Controlling Evolution of Complex Software Products},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853082},
doi = {10.1145/2853073.2853082},
abstract = {Efficient configuration management system is crucial for the success of any software product line (SPL). Due to ever changing needs of customers, SPL undergoes constant changes that are to be tracked in real time. In the context of customer-driven development, anticipation and change management are to be given paramount importance. It demands implementation of software variability that drives home changed, extended and customized configurations besides economy at scale. Moreover, the emergence of distributed technologies, the unprecedented growth of component based, serviceoriented systems throw ever increasing challenges to software product line configuration management. Derivation of a new product is a dynamic process in software product line that should consider functionality and quality attributes. Very few approaches are found on configuration management (CM) of SPL though CM is enough matured for traditional products. They are tailor made and inadequate to provide a general solution. Stated differently, a comprehensive approach for SPL configuration management and product derivation is still to be desired. In this paper, we proposed a framework that guides in doing so besides helping in SPL definitions in generic way. Our framework facilitates SPL configuration management and product derivation based on critical path analysis, weight computation and feedback. We proposed two algorithms namely Quality Driven Product Derivation (QDPD) and Composition Analysis algorithm for generating satisfied compositions and to find best possible composition respectively. The usage of weights and critical path analysis improves quality of product derivation. The framework is extensible and flexible thus it can be leveraged with variability-aware design patterns and ontology. We built a prototype that demonstrates the proof of concept. We tested our approach with Dr. School product line. The results reveal that the framework supports configuration management of SPL and derivation of high quality product in the product line. We evaluated results with ground truth to establish significance of our implementation},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–10},
numpages = {10},
keywords = {Software product line, configuration management, critical path analysis, product derivation, weighted approach}
}

@inproceedings{10.1145/3233027.3236402,
author = {Martinez, Jabier and Ordo\~{n}ez, Nicolas and T\"{e}rnava, Xhevahire and Ziadi, Tewfik and Aponte, Jairo and Figueiredo, Eduardo and Valente, Marco Tulio},
title = {Feature location benchmark with argoUML SPL},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236402},
doi = {10.1145/3233027.3236402},
abstract = {Feature location is a traceability recovery activity to identify the implementation elements associated to a characteristic of a system. Besides its relevance for software maintenance of a single system, feature location in a collection of systems received a lot of attention as a first step to re-engineer system variants (created through clone-and-own) into a Software Product Line (SPL). In this context, the objective is to unambiguously identify the boundaries of a feature inside a family of systems to later create reusable assets from these implementation elements. Among all the case studies in the SPL literature, variants derived from ArgoUML SPL stands out as the most used one. However, the use of different settings, or the omission of relevant information (e.g., the exact configurations of the variants or the way the metrics are calculated), makes it difficult to reproduce or benchmark the different feature location techniques even if the same ArgoUML SPL is used. With the objective to foster the research area on feature location, we provide a set of common scenarios using ArgoUML SPL and a set of utils to obtain metrics based on the results of existing and novel feature location techniques.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {257–263},
numpages = {7},
keywords = {argoUML, benchmark, extractive software product line adoption, feature location, reverse-engineering, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3579028.3609009,
author = {de Castro, David and Corti\~{n}as, Alejandro and Lamas, Victor and Luaces, Miguel R.},
title = {GIS-Publisher: From a Geographic Data Set to a Deployed Product with One Command},
year = {2023},
isbn = {9798400700927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579028.3609009},
doi = {10.1145/3579028.3609009},
abstract = {In our research laboratory, we have been working on developing a software product line (SPL) specifically tailored for generating web-based geographic information systems (GIS). In addition, we have also designed a domain specific language (DSL) to make configuring our products as easy and flexible as possible. Over time, we have utilized this product line to create small GIS products, aiming to simplify the process of publishing and sharing geographic data. The steps involved in generating and deploying this kind of products are consistently repeated, so they can be easily automated. Doing so, we further reduce the time to market for this set of simple products, and minimize the complexity associated with the entire process.This article introduces GIS-Publisher, a tool that allows users to easily generate web applications from a directory containing a collection of shapefiles (a popular format for storing geographic data). These web applications can be also automatically deployed on their preferred machine, whether it is locally, remotely (via SSH), or even on an AWS instance. Moreover, the tool also supports the definition of custom styles for each shapefile, granting users full control over the visual representation of their geographic data.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
pages = {20–24},
numpages = {5},
keywords = {automatic deployment, geograhpic data, software product lines, tool},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3646548.3672590,
author = {Sundermann, Chico and Brancaccio, Vincenzo Francesco and Kuiter, Elias and Krieter, Sebastian and He\ss{}, Tobias and Th\"{u}m, Thomas},
title = {Collecting Feature Models from the Literature: A Comprehensive Dataset for Benchmarking},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672590},
doi = {10.1145/3646548.3672590},
abstract = {Feature models are widely used for specifying the valid configurations of product lines. Many automated analyses on feature models have been considered, but they often depend on computationally complex algorithms (e.g., solving satisfiability problems). To identify and develop efficient reasoning engines, it is necessary to compare their performance on practically relevant feature models. However, empirical evaluations on feature-model analysis often suffer from the limitations of available feature-model datasets in terms of transferability. A major problem is the accessibility of relevant feature models as they are scattered over numerous publications. In this work, we perform a literature survey on empirical evaluations that target the performance of feature-model analyses to examine common evaluation practices and collect feature models for future evaluations. Furthermore, we examine the suitability of the derived collection for benchmarking performance. To improve accessibility, we provide a repository including all 2,518 identified feature models from 13 application domains, such as system software.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {54–65},
numpages = {12},
keywords = {benchmark, evaluation, feature model, product line, survey},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3461001.3471152,
author = {Silva, Publio and Bezerra, Carla I. M. and Machado, Ivan},
title = {A machine learning model to classify the feature model maintainability},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471152},
doi = {10.1145/3461001.3471152},
abstract = {Software Product Lines (SPL) are generally specified using a Feature Model (FM), an artifact designed in the early stages of the SPL development life cycle. This artifact can quickly become too complex, which makes it challenging to maintain an SPL. Therefore, it is essential to evaluate the artifact's maintainability continuously. The literature brings some approaches that evaluate FM maintainability through the aggregation of maintainability measures. Machine Learning (ML) models can be used to create these approaches. They can aggregate the values of independent variables into a single target data, also called a dependent variable. Besides, when using white-box ML models, it is possible to interpret and explain the ML model results. This work proposes white-box ML models intending to classify the FM maintainability based on 15 measures. To build the models, we performed the following steps: (i) we compared two approaches to evaluate the FM maintainability through a human-based oracle of FM maintainability classifications; (ii) we used the best approach to pre-classify the ML training dataset; (iii) we generated three ML models and compared them against classification accuracy, precision, recall, F1 and AUC-ROC; and, (iv) we used the best model to create a mechanism capable of providing improvement indicators to domain engineers. The best model used the decision tree algorithm that obtained accuracy, precision, and recall of 0.81, F1-Score of 0.79, and AUC-ROC of 0.91. Using this model, we could reduce the number of measures needed to evaluate the FM maintainability from 15 to 9 measures.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {35–45},
numpages = {11},
keywords = {feature model, machine learning, quality evaluation, software product line},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3646548.3676539,
author = {M\"{u}ller, Robert and Wei\ss{}, Mathis and Lochau, Malte},
title = {Mapping Cardinality-based Feature Models to Weighted Automata over Featured Multiset Semirings},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676539},
doi = {10.1145/3646548.3676539},
abstract = {Cardinality-based feature models permit to select multiple copies of the same feature, thus generalizing the notion of product configurations from subsets of Boolean features to multisets of feature instances. This increased expressiveness shapes a-priori infinite and non-convex configuration spaces, which renders established solution-space mappings based on Boolean presence conditions insufficient for cardinality-based feature models. To address this issue, we propose weighted automata over featured multiset semirings as a novel behavioral variability modeling formalism for cardinality-based feature models. The formalism uses multisets over features as a predefined semantic domain for transition weights. It permits to use any algebraic structure forming a proper semiring on multisets to aggregate the weights traversed along paths to map accepted words to multiset configurations. In particular, tropical semirings constitute a promising sub-class with a reasonable trade-off between expressiveness and computational tractability of canonical analysis problems. The formalism is strictly more expressive than featured transition systems, as it enables upper-bound multiplicity constraints depending on the length of words. We provide a tool implementation of the behavioral variability model and present preliminary experimental results showing applicability and computational feasibility of the proposed approach.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {1–11},
numpages = {11},
keywords = {Behavioral Variability Modeling, Cardinality-Based Feature Models, Weighted Automata},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/2602458.2602460,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Injecting quality attributes into software architectures with the common variability language},
year = {2014},
isbn = {9781450325776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602458.2602460},
doi = {10.1145/2602458.2602460},
abstract = {Quality attributes that add new behavior to the functional software architecture are known as functional quality attributes (FQAs). These FQAs are applied to pieces of software from small components to entire systems, usually crosscutting some of them. Due to this crosscutting nature, modeling them separately from the base application has many advantages (e.g. reusability, less coupled architectures). However, different applications may require different configurations of an FQA (e.g. different levels of security), so we need a language that: (i) easily expresses the variability of the FQAs at the architectural level; and that (ii) also facilitates the automatic generation of architectural configurations with custom-made FQAs. In this sense, the Common Variability Language (CVL) is extremely suited for use at the architectural level, not requiring the use of a particular architectural language to model base functional requirements. In this paper we propose a method based on CVL to: (i) model separately and generate FQAs customized to the application requirements; (ii) automatically inject customized FQA components into the architecture of the applications. We quantitatively evaluate our approach and discuss its benefits with a case study.},
booktitle = {Proceedings of the 17th International ACM Sigsoft Symposium on Component-Based Software Engineering},
pages = {35–44},
numpages = {10},
keywords = {cvl, quality attributes, spl, variability, weaving},
location = {Marcq-en-Bareul, France},
series = {CBSE '14}
}

@inproceedings{10.1145/3382025.3414953,
author = {Abbas, Muhammad and Jongeling, Robbert and Lindskog, Claes and Enoiu, Eduard Paul and Saadatmand, Mehrdad and Sundmark, Daniel},
title = {Product line adoption in industry: an experience report from the railway domain},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414953},
doi = {10.1145/3382025.3414953},
abstract = {The software system controlling a train is typically deployed on various hardware architectures and must process various signals across those deployments. The increase of such customization scenarios and the needed adherence of the software to various safety standards in different application domains has led to the adoption of product line engineering within the railway domain. This paper explores the current state-of-practice of software product line development within a team developing industrial embedded software for a train propulsion control system. Evidence is collected using a focus group session with several engineers and through inspection of archival data. We report several benefits and challenges experienced during product line adoption and deployment. Furthermore, we identify and discuss improvement opportunities, focusing mainly on product line evolution and test automation.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {3},
numpages = {11},
keywords = {challenges and opportunities, overloaded assets, software product-line engineering},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3307630.3342705,
author = {Krieter, Sebastian},
title = {Enabling Efficient Automated Configuration Generation and Management},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342705},
doi = {10.1145/3307630.3342705},
abstract = {Creating and managing valid configurations is one of the main tasks in software product line engineering. Due to the often complex constraints from a feature model, some kind of automated configuration generation is required to facilitate the configuration process for users and developers. For instance, decision propagation can be applied to support users in configuring a product from a software product line (SPL) with less manual effort and error potential, leading to a semi-automatic configuration process. Furthermore, fully-automatic configuration processes, such as random sampling or t-wise interaction sampling can be employed to test or to optimize an SPL. However, current techniques for automated configuration generation still do not scale well to SPLs with large and complex feature models. Within our thesis, we identify current challenges regarding the efficiency and effectiveness of the semi- and fully-automatic configuration process and aim to address these challenges by introducing novel techniques and improving current ones. Our preliminary results show already show promising progress for both, the semi- and fully-automatic configuration process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {215–221},
numpages = {7},
keywords = {configurable system, decision propagation, software product lines, t-wise sampling, uniform random sampling},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3646548.3673037,
author = {Brault, Yann and Collet, Philippe and Pinna-Dery, Anne-Marie},
title = {Visualizing Variability Implemented with Object-Orientation and Code Clones: A Tale of Two Cities},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3673037},
doi = {10.1145/3646548.3673037},
abstract = {Understanding variability in large software systems poses significant challenges for developers, especially when variability is implemented within a single codebase using diverse language mechanisms like in TypeScript. In this language, one can implement variability with traditional Object-Oriented (OO) techniques, but also with dynamic loading mechanisms that organize different forms of clones in sub-directories and files serving as internal variants. While certain approaches may facilitate partial identification or visualization, there exists no solution for handling all variability mechanisms simultaneously. In this paper, we propose an approach by detecting all mechanisms and integrating two city-based representations to visualize these implemented variabilities. The first representation adapts the VariCity visualization and focuses on OO variability with classes as buildings and usage relationships as streets. The second representation leverages a codebase analysis combined with a code clone detection technique to visualize the directory hierarchy as streets and files as circular districts with shades and colors to highlight cloning. Some visual mechanisms enable to display relevant relationships between them, unveiling patterns of cross-usage and variability architecture. We also report on the application of the tooled approach on several large open-source systems.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {107–112},
numpages = {6},
keywords = {clone detection, code clones, reverse-engineering, software variability, software visualization},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3503229.3547042,
author = {Hofbauer, Andreas and Felfernig, Alexander},
title = {ConGuess: a learning environment for configuration tasks},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547042},
doi = {10.1145/3503229.3547042},
abstract = {Learning the concepts of constraint solving and configuration is often a effortful task since it requires the development of a basic understanding of configuration rule semantics. Also, students engaged in topic-related courses have to tackle the challenge of understanding formal configuration knowledge representations. In this paper we introduce a gamification-based environment (ConGuess) that can help to learn configuration rule semantics. This environment is based on the idea of presenting configuration knowledge to game players (learners) and let players figure out correct solutions.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {156–157},
numpages = {2},
keywords = {e-learning, gamification, knowledge-based configuration},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.5555/1753235.1753258,
author = {Ganesan, Dharmalingam and Lindvall, Mikael and Ackermann, Chris and McComas, David and Bartholomew, Maureen},
title = {Verifying architectural design rules of the flight software product line},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {This paper presents experiences of verifying architectural design rules of the NASA Core Flight Software (CFS) product line implementation. The goal is to check whether the implementation is consistent with the CFS' architectural rules derived from the developer's guide. The results indicate that consistency checking helps a) identifying architecturally significant deviations that were eluded during code reviews, b) clarifying the design rules to the team, and c) assessing the overall implementation quality. Furthermore, it helps connecting business goals to architectural principles, and to the implementation. This paper is the first step in the definition of a method for analyzing and evaluating product line implementations from an architecture-centric perspective.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {161–170},
numpages = {10},
keywords = {architectural rules, business goals, flight software, implemented architecture},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2556624.2556633,
author = {Benavides, David and Galindo, Jos\'{e} A.},
title = {Variability management in an unaware software product line company: an experience report},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556633},
doi = {10.1145/2556624.2556633},
abstract = {Software product line adoption is a challenging task in software development organisations. There are some reports in the literature of how software product line engineering has been adopted in several companies using different variability management techniques and patterns. However, to the best of our knowledge, there are no empirical reports on how variability management is handled in companies that do not know about software product line methods and tools. In this paper we present an experience report observing variability management practices in a software development company that was unaware of software product line approaches. We briefly report how variability management is performed in different areas ranging from business architecture to software assets management. From the observation we report some open research opportunities for the future and foster further similar and more structured empirical studies on unaware software product line companies.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {5},
numpages = {6},
keywords = {experience report, software product lines, unaware variability techniques organisation},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@inproceedings{10.1145/3646548.3672598,
author = {He\ss{}, Tobias and Semmler, Sean Niklas and Sundermann, Chico and Tor\'{a}n, Jacobo and Th\"{u}m, Thomas},
title = {Towards Deterministic Compilation of Binary Decision Diagrams From Feature Models},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672598},
doi = {10.1145/3646548.3672598},
abstract = {Compiling binary decision diagrams (BDD) from feature models is a challenging task whose outcome and performance depends on many interconnected factors. As almost all of these factors trace back to the BDD’s variable order, finding suitable orders is paramount for compilation success. In addition to cross-tree constraints, Alternative groups in feature models pose a challenge for variable ordering as separating group variables in the order can lead to duplication of large parts of the BDD. Previous approaches only scale to under-constrained models and require knowledge of the feature hierarchy. We address both shortcomings with a novel deterministic variable-ordering heuristic that detects Alternative groups in Boolean formulas and exploits them for variable ordering using the well-known FORCE heuristic in a divide-and-conquer approach. Our evaluation shows that this heuristic, together with our compilation strategy, scales to many models for which BDDs could not be compiled previously. Thereby, this work resolves SPLC’s knowledge compilation challenge for an important subset of real-world models.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {136–147},
numpages = {12},
keywords = {Binary Decision Diagrams, Feature-Model Analysis, Variable Ordering},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/2554850.2554987,
author = {Parra, Carlos and Joya, Diego and Giral, Leonardo and Infante, Alvaro},
title = {An SOA approach for automating software product line adoption},
year = {2014},
isbn = {9781450324694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554850.2554987},
doi = {10.1145/2554850.2554987},
abstract = {Nowadays, the software industry is faced with challenges regarding complexity, time to market, quality standards and evolution. To face those challenges, two strategies that are gaining interest both in academy and industry are Service Oriented Architecture (SOA) and Software Product Lines (SPL). While SOA aims at building applications from an orchestration of services, SPL consists in building a set of core-assets and a derivation strategy based on such assets. Adopting such approaches involves important challenges with regard to existing software artifacts that must be transformed in order to respect an architecture that focus on modularity and reuse. This paper presents an industrial experience of such transformation. We propose a non-intrusive reverse engineering process for the development of modular services obtained automatically from existing software artifacts, and a variability-driven derivation process to assembly products out of such services. To validate our approach, we have implemented the reverse engineering and derivation processes using real software JEE artifacts from a component framework of reusable functionalities in several different enterprise applications. The results show important benefits in terms of the development time and flexibility.},
booktitle = {Proceedings of the 29th Annual ACM Symposium on Applied Computing},
pages = {1231–1238},
numpages = {8},
keywords = {model-driven engineering, reverse engineering, service oriented architecture, software product lines},
location = {Gyeongju, Republic of Korea},
series = {SAC '14}
}

@inproceedings{10.5555/2666064.2666075,
author = {Fant, Julie Street and Gomaa, Hassan and Pettit, Robert G.},
title = {Software product line engineering of space flight software},
year = {2012},
isbn = {9781467317511},
publisher = {IEEE Press},
abstract = {This paper presents a practical solution to a real-life industrial problem in the unmanned space flight software (FSW) domain using software product lines and software architectural design patterns. In the FSW domain, there exists a significant amount of variability in the required capabilities. For example, some FSW have a significant amount of hardware to control and operate in a nearly autonomous fashion. In contrast, other FSW have a small amount of hardware to control and rely heavily of commanding from the ground station to operate the spacecraft. The underlying architecture and component interactions needed for the different FSWs are quite different. This amount of architectural variability makes it difficult to develop a SPL architecture that covers the all possible variability in the FSW domain. Therefore, this paper presents a practical solution to this real world problem that leverages software product line concepts and software architectural design patterns.},
booktitle = {Proceedings of the Third International Workshop on Product LinE Approaches in Software Engineering},
pages = {41–44},
numpages = {4},
keywords = {UML, software architectural design patterns, software product lines, unmanned space flight software},
location = {Zurich, Switzerland},
series = {PLEASE '12}
}

@inproceedings{10.1145/3503229.3547064,
author = {Trombetta, Valeria},
title = {BEEHIVE: behaviour-induced configuration of high variability-intensive systems},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547064},
doi = {10.1145/3503229.3547064},
abstract = {Nowadays, Cyber-Physical Systems (CPS) represent one of the main core elements of the Industry 4.0. It is common practice to run simulations on a model of the CPS, by adopting specific tools and approaches. Since the purpose of such models is to represent real systems, it is appropriate to assume that several components may be affected by noises and disturbances (N&amp;D), and that these latter may have a different impact on the system depending on the considered configuration and the simulation scenarios. The analysis of signals belonging to a CPS system permits the understanding of the relationships that discipline the behavior of the whole system in presence of N&amp;D. Depending on the context and the considered scenarios, the simulations in presence of N&amp;D might generate very different numerical results compared to the simulations that do not include them. However, the simulations with additional N&amp;D are non-trivial to be computed and analyzed, especially when the considered CPS have also high variability and configurability. The adopted approach investigates the validation of possible cross-configurations, in order that the solution includes sets of suitable configurations for both the CPS parameters and N&amp;D wrt scenarios.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {17–22},
numpages = {6},
keywords = {cyber-physical systems, simulation, verification},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/1868688.1868691,
author = {Torres, M\'{a}rio and Kulesza, Uir\'{a} and Sousa, Matheus and Batista, Thais and Teixeira, Leopoldo and Borba, Paulo and Cirilo, Elder and Lucena, Carlos and Braga, Rosana and Masiero, Paulo},
title = {Assessment of product derivation tools in the evolution of software product lines: an empirical study},
year = {2010},
isbn = {9781450302081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868688.1868691},
doi = {10.1145/1868688.1868691},
abstract = {Product derivation approaches automate the customization process of software product lines. Over the last years, many tools have been proposed aiming at synthesize and generate products from a set of reusable assets. These tools adopt different techniques and strategies to implement and automate the product derivation activities. In this paper, we analyzed six modern product derivation tools (Captor, CIDE, GenArch, MSVCM, pure::variants, XVCL) in the context of evolution scenarios of a software product line. Our study has adopted several metrics to analyze the modularity, complexity and stability of product derivation artifacts related to configuration knowledge along different releases of a mobile product line. The preliminary results of our study have shown that approaches with a dedicated model or file to represent the CK specification can bring several benefits to the modularization and stability of a software product line.},
booktitle = {Proceedings of the 2nd International Workshop on Feature-Oriented Software Development},
pages = {10–17},
numpages = {8},
keywords = {measurement, product derivation tools},
location = {Eindhoven, The Netherlands},
series = {FOSD '10}
}

@inproceedings{10.1145/3461002.3473948,
author = {Xu, Hao and Baarir, Souheib and Ziadi, Tewfik and Hillah, Lom Messan and Essodaigui, Siham and Bossu, Yves},
title = {Optimisation for the product configuration system of Renault: towards an integration of symmetries},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473948},
doi = {10.1145/3461002.3473948},
abstract = {The problem of configuring model variability is widespread in many different domains. Renault, a leading french automobile manufacturer, has developed its technology internally to model vehicle diversity. This technology relies on the approach known as knowledge compilation. Since its inception, continuous progress has been made in the tool while monitoring the latest developments from the software field and academia. However, the growing number of vehicle models brings potential risks and higher requirements for the tool. This paper presents a short reminder of Renault's technology principles and the improvements we intend to achieve by analyzing and leveraging notable data features of Renault problem instances. In particular, the aim is to exploit symmetry properties.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {86–90},
numpages = {5},
keywords = {SAT, knowledge compilation, product line, symmetries},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/2420942.2420948,
author = {Gonz\'{a}lez-Huerta, Javier and Insfran, Emilio and Abrah\~{a}o, Silvia and McGregor, John D.},
title = {Non-functional requirements in model-driven software product line engineering},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420948},
doi = {10.1145/2420942.2420948},
abstract = {Developing variant-rich software systems through the application of the software product line approach requires the management of a wide set of requirements. However, in most cases, the focus of those requirements is limited to the functional requirements. The non-functional requirements are often informally defined and their management does not provide traceability mechanisms for their validation. In this paper, we present a multimodel approach that allows the explicit representation of non-functional requirements for software product lines both at domain engineering, and application engineering levels. The multimodel allows the representation of different viewpoints of a software product line, including the non-functional requirements and the relationships that these non-functional requirements might have with features and functionalities. The feasibility of this approach is illustrated through a specific example from the automotive domain.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {6},
numpages = {6},
keywords = {model driven engineering, non-functional requirements, software product lines},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@inproceedings{10.1145/2362536.2362548,
author = {Soltani, Samaneh and Asadi, Mohsen and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Bagheri, Ebrahim},
title = {Automated planning for feature model configuration based on functional and non-functional requirements},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362548},
doi = {10.1145/2362536.2362548},
abstract = {Feature modeling is one of the main techniques used in Software Product Line Engineering to manage the variability within the products of a family. Concrete products of the family can be generated through a configuration process. The configuration process selects and/or removes features from the feature model according to the stakeholders' requirements. Selecting the right set of features for one product from amongst all of the available features in the feature model is a complex task because: 1) the multiplicity of stakeholders' functional requirements; 2) the positive or negative impact of features on non-functional properties; and 3) the stakeholders' preferences w.r.t. the desirable non-functional properties of the final product. Many configurations techniques have already been proposed to facilitate automated product derivation. However, most of the current proposals are not designed to consider stakeholders' preferences and constraints especially with regard to non-functional properties. We address the software product line configuration problem and propose a framework, which employs an artificial intelligence planning technique to automatically select suitable features that satisfy both the stakeholders' functional and non-functional preferences and constraints. We also provide tooling support to facilitate the use of our framework. Our experiments show that despite the complexity involved with the simultaneous consideration of both functional and non-functional properties our configuration technique is scalable.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {56–65},
numpages = {10},
keywords = {artificial intelligence, configuration, feature model, planning techniques, software product line engineering},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3461001.3471150,
author = {Pietsch, Christopher and Kelter, Udo and Kehrer, Timo},
title = {From pairwise to family-based generic analysis of delta-oriented model-based SPLs},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471150},
doi = {10.1145/3461001.3471150},
abstract = {One way to implement model-based software product lines (MBSPLs) is to use a transformational approach known as Delta Modeling (DM). Here, an MBSPL is implemented by one core model and a set of delta modules. Delta modules define model transformations using edit operations which add, remove or modify model elements. Editings of different delta modules can be in conflict or depend on each other, leading to conflict and dependency relations between delta modules. Conflicts and unfulfilled dependencies can cause the generation of a product to fail or to lead to invalid models. In order to spot such defects, one needs analysis tools for each modeling (sub-)language used. Existing generic approaches to statically detect such defects in a language-agnostic manner analyze pairs of delta modules. However, the pairwise approach can lead to false positives, i.e., conflicts and unfulfilled dependencies are reported although product generation does not fail. Following the idea of family-based analysis, this paper presents a new approach to detect pseudo defects resolved by "healing effects" implied by the network of dependencies. These effects typically occur when a delta module (partially) reverts the effect of a preceding delta module. We have implemented our approach within the SiPL framework and evaluated our family-based analysis using a realistic MBSPL known as Body Comfort System (BCS).},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {13–24},
numpages = {12},
keywords = {conflicts and dependencies, delta modeling, family-based analysis, graph transformation, model-based software product line engineering},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3646548.3672596,
author = {Chueca, Jorge and Blasco, Daniel and Cetina, Carlos and Font, Jaime},
title = {Leveraging Phylogenetics in Software Product Families: The Case of Latent Content Generation in Video Games},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3672596},
doi = {10.1145/3646548.3672596},
abstract = {A family of software products comprises similar products within a defined scope that share common characteristics, often due to reuse techniques applied during development. This paper introduces an approach that applies biological insights to map the landscape of a software product family, identifying potential gaps within its scope. Phylogenetics studies the gene similarity among groups of organisms to understand ancestry among species. Leveraging Phylogenetics in software, our approach offers a structured view of a product family, aiding in the discovery of unexplored areas fitting the scope of the family. Our approach creates a phylogenetic tree that enables to easily identify latent products (ancestors) that did not exist in the original family. Those ancestors can then be reconstructed from existing products (descendants). The product family evaluated is a set of industry-scale video game non-playable characters. We assess this approach through video game simulations and scope metrics to determine how closely the reconstructed products align with the family’s scope. The results confirm that the content generated with phylogenetics aligns better with the family scope than the state-of-the-art procedural content generation techniques using evolutionary algorithms. Phylogenetics enhances content generation by providing a framework to understand and expand the product family with new content.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {113–124},
numpages = {12},
keywords = {Game Software Engineering, Phylogenetics, Procedural Content Generation, Software Product Families},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3236405.3237205,
author = {Ziadi, Tewfik and Martinez, Jabier and T\"{e}rnava, Xhevahire},
title = {Teaching projects and research objectives in SPL extraction},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3237205},
doi = {10.1145/3236405.3237205},
abstract = {This year at SPLC we present a teaching and research project where a group of master students analysed a variability-rich domain and extracted an SPL (The Robocode SPL). We present the results of such extraction augmented with an analysis and a quantification regarding the time and effort spent. The research objective was to get and share data about an end-to-end SPL extraction which is usually unavailable in industrial cases because of their large size, complexity, and duration. We provide all the material to replicate, reproduce or extend the case study so it can be easily reused for teaching by anyone in our community. However, we were asking ourselves how can we leverage such case study for teaching to pursue research objectives. In this position paper, we aim to outline our initial ideas that we want to enrich with the others' viewpoints during SPLTea. Towards planning the settings of future teaching projects around this Robocode SPL case study, which can be the timely research objectives that we can identify? Can we involve others in planning this project in their institutions to get further relevant results?},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {44–45},
numpages = {2},
keywords = {extractive software product line adoption, reverse-engineering, software product lines, teaching},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2031759.2031768,
author = {Lence, Ram\'{o}n and Fuentes, Lidia and Pinto, M\'{o}nica},
title = {Quality attributes and variability in AO-ADL software architectures},
year = {2011},
isbn = {9781450306188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2031759.2031768},
doi = {10.1145/2031759.2031768},
abstract = {The quality attributes of a system are determined, to a large extend, by the decisions taken early on in the development process, noticeably affecting the specification of its software architecture. This is especially true for attributes such as security, usability, context awareness, etc., that have strong functional implications -- i.e. they require the incorporation of Specific functionality to the application architecture in order to satisfy them. Our approach models functional quality attributes considering that: (1) they are complex enough so as to be modeled by a large set of related concerns and the compositions among them. For instance, security includes authentication, access control, privacy, encryption, auditing, etc; (2) the same quality attributes are required by several applications, and thus should be modeled as separate, ready-to-use (re)usable architectural solutions that final applications can incorporate without "being previously prepared" for it; and (3) not all the concerns that are part of a quality attribute need to be instantiated for a particular application (e.g. only the authentication and access control concerns of security are required). In order to consider all the above requirements, in this paper we present a software product line approach that permits modeling the variability of quality attributes using feature models, and generating different configurations of their software architecture depending on the particular concerns required by each application.},
booktitle = {Proceedings of the 5th European Conference on Software Architecture: Companion Volume},
articleno = {7},
numpages = {10},
keywords = {AO-ADL, Hydra, VML, architectural templates, quality attributes, variability},
location = {Essen, Germany},
series = {ECSA '11}
}

@inproceedings{10.1145/3579027.3608980,
author = {Pett, Tobias and He\ss{}, Tobias and Krieter, Sebastian and Th\"{u}m, Thomas and Schaefer, Ina},
title = {Continuous T-Wise Coverage},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608980},
doi = {10.1145/3579027.3608980},
abstract = {Quality assurance for highly configurable systems uses t-wise feature interaction coverage as a metric to measure the quality of selected samples for testing. Achieving t-wise feature interaction coverage requires testing many configurations, often exceeding the available testing time for frequently evolving systems. As testing time is a limiting factor, current testing procedures face the challenge of finding a reasonable trade-off between achieving t-wise feature interaction coverage and reducing the time required for testing. To address this challenge, we can consider t-wise feature interactions covered in previous test executions when calculating the achieved t-wise feature interaction coverage. However, the current definition of t-wise feature interaction coverage does not consider previously tested configurations. Therefore, we propose continuous t-wise coverage as a new customizable metric for tracking the ratio of achieved t-wise feature interaction coverage over time. Our metric allows customizing the tradeoff between test effort per system version and the time to achieve t-wise coverage. We evaluate various parameterizations for our metric on four real-world evolution histories and investigate how they impact the calculated t-wise feature interaction coverage. Our results show that a high t-wise feature interaction coverage can be achieved by testing significant (up to 50%) smaller samples per commit, when the evolution of the system is considered.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {87–98},
numpages = {12},
keywords = {sampling, software-product lines, spl evolution, spl testing, t-wise coverage},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3109729.3109759,
author = {Corti\~{n}as, Alejandro and Luaces, Miguel R. and Pedreira, Oscar and Places, \'{A}ngeles S.},
title = {Scaffolding and in-browser generation of web-based GIS applications in a SPL tool},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109759},
doi = {10.1145/3109729.3109759},
abstract = {The SME (Small and medium-sized enterprise) Enxenio has developed many web-based Geographic Information Systems within the last decade. Since the demand for GIS is increasing, Enxenio decided to apply Software Product Line Engineering to this domain to facilitate the development of complete web-based GIS applications, increasing their quality, improving the time-to-market and, at the same time, reducing their cost to its clients. This demo shows the resulting tool of this process, which is able to generate the source code of a web-based GIS from the set of desired features and the definition of its data model. This tool can be run within a web browser and the derivation engine in charge of generating the code is based on the scaffolding technique.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {46–49},
numpages = {4},
keywords = {Software product line engineering, scaffolding, web-based geographic information systems},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.5555/2486788.2487011,
author = {Gonzalez-Sanchez, Javier},
title = {Toward a software product line for affective-driven self-adaptive systems},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {One expected characteristic in modern systems is self-adaptation, the capability of monitoring and reacting to changes into the environment. A particular case of self-adaptation is affective-driven self-adaptation. Affective-driven self-adaptation is about having consciousness of user’s affects (emotions) and drive self-adaptation reacting to changes in those affects. Most of the previous work around self-adaptive systems deals with performance, resources, and error recovery as variables that trigger a system reaction. Moreover, most effort around affect recognition has been put towards offline analysis of affect, and to date only few applications exist that are able to infer user’s affect in real-time and trigger self-adaptation mechanisms. In response to this deficit, this work proposes a software product line approach to jump-start the development of affect-driven self-adaptive systems by offering the definition of a domain-specific architecture, a set of components (organized as a framework), and guidelines to tailor those components. Study cases with systems for learning and gaming will confirm the capability of the software product line to provide desired functionalities and qualities.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1381–1384},
numpages = {4},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/3546932.3547014,
author = {Tavassoli, Shaghayegh and Damasceno, Carlos Diego N. and Mousavi, Mohammad Reza and Khosravi, Ramtin},
title = {A benchmark for active learning of variability-intensive systems},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547014},
doi = {10.1145/3546932.3547014},
abstract = {Behavioral models are the key enablers for behavioral analysis of Software Product Lines (SPL), including testing and model checking. Active model learning comes to the rescue when family behavioral models are non-existent or outdated. A key challenge on active model learning is to detect commonalities and variability efficiently and combine them into concise family models. Benchmarks and their associated metrics will play a key role in shaping the research agenda in this promising field and provide an effective means for comparing and identifying relative strengths and weaknesses in the forthcoming techniques. In this challenge, we seek benchmarks to evaluate the efficiency (e.g., learning time and memory footprint) and effectiveness (e.g., conciseness and accuracy of family models) of active model learning methods in the software product line context. These benchmark sets must contain the structural and behavioral variability models of at least one SPL. Each SPL in a benchmark must contain products that requires more than one round of model learning with respect to the basic active learning L* algorithm. Alternatively, tools supporting the synthesis of artificial benchmark models are also welcome.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {245–249},
numpages = {5},
keywords = {behavioral variability, benchmarking, featured finite state machines, model learning},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3382026.3425775,
author = {Nair, Suparna S. and Becker, Martin and Tenev, Vasil},
title = {A Comparative Study on Variability Code Analysis Technology},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3425775},
doi = {10.1145/3382026.3425775},
abstract = {Product line engineering is often conducted in an incremental way, in which the variability artifacts evolve in the space, the time, as well as the asset dimension. In order to cope with the evolution of the variability, the VITAL approach and tool have been developed and used in different industrial settings to analyze variability realizations relying on the C preprocessor. Over the last decade, further promising analysis approaches and tools have been developed. To understand, if and how they could enhance the VITAL approach, we have conducted an analysis of promising technologies.In this paper, we share some of our findings along our comparative study on variability code analysis technologies. As we have conducted the study in the light of the intended VITAL enhancement, the study does not claim completeness. Nevertheless, we believe that the findings can help researchers and industrial practitioners to gain an overview and find entry points for their own investigations.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {37–43},
numpages = {7},
keywords = {Variability realization, configuration knowledge, reverse engineering, software product line},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3579027.3608991,
author = {Bounouas, Nassim and Blay-Fornarino, Mireille and Collet, Philippe},
title = {An Action-based Model to Handle Cloning and Adaptation in Tabular Data Applications},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608991},
doi = {10.1145/3579027.3608991},
abstract = {Many software systems require diverse data gathering and handling through processes that manipulate tabular data, often with a spreadsheet orientation. Variability in tabular data cannot be captured in a complete up-front analysis as everything is done at the final user level. She progressively adapts or clones some tabular data organized to conduct a process. Consequently these organized data are constantly both a final usable product and a potential candidate for cloning. This huge diversity, the high frequency of their evolution over time, and the intrinsic need to use cloning lead naturally to the usage of a clone-and-own approach with well-known negative impacts on maintenance and quality. In this paper we advocate that this can be replaced by controlling the clone-and-own process with provenance information that completely captures, at the domain level, the cloning actions and the adaptations applied on a product defining its clones. Each action over the process, its observations, and its data are captured in a complete model through traces of atomic adaptations, complemented with specific derivation and extraction actions. This model enables obtaining the whole history of both data and processes over time, as well as the accountability of variability-related actions. We report on a study showing the relevance of tackled problem in a variability-rich agronomy software of an industrial partner. We also show that a first prototype covers the extracted usage scenarios, from simple and entire cloning to more complex partial cloning.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {201–212},
numpages = {12},
keywords = {Tabular data, agronomy, clone-and-own, model-driven engineering, operation-based modeling, variability management},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3546932.3547007,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Quality-aware analysis and optimisation of virtual network functions},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547007},
doi = {10.1145/3546932.3547007},
abstract = {The softwarisation and virtualisation of network functionality is the last milestone in the networking industry. Software-Defined Networks (SDN) and Network Function Virtualization (NFV) offer the possibility of using software to manage computer and mobile networks and build novel Virtual Network Functions (VNFs) deployed in heterogeneous devices. To reason about the variability of network functions and especially about the quality of a software product defined as a set of VNFs instantiated as part of a service (i.e., Service Function Chaining), a variability model along with a quality model is required.However, this domain imposes certain challenges to quality-aware reasoning of service function chains, such as numerical features or configuration-level Quality Attributes (QAs) (e.g., energy consumption). Incorporating numerical reasoning with quality data into SPL analyses is challenging and tool support is rare. In this work, we present 3 groups of operations: model report, aggregate functions to dynamically convert QAs at the feature-level into the configuration-level, and quality-aware optimisation. Our objective is to test the most complete reasoning tools to exploit the extended variability with quality attributes needed for VNFs.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {210–221},
numpages = {12},
keywords = {numerical feature, optimization, quality attribute, reasoning, variability, virtual network function},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3503229.3547067,
author = {Martinez, Jabier and Str\"{u}ber, Daniel and Horcas, Jose Miguel and Burdusel, Alexandru and Zschaler, Steffen},
title = {Acapulco: an extensible tool for identifying optimal and consistent feature model configurations},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547067},
doi = {10.1145/3503229.3547067},
abstract = {Configuring feature-oriented variability-rich systems is complex because of the large number of features and, potentially, the lack of visibility of the implications on quality attributes when selecting certain features. We present Acapulco as an alternative to the existing tools for automating the configuration process with a focus on mono- and multi-criteria optimization. The soundness of the tool has been proven in a previous publication comparing it to SATIBEA and MODAGAME. The main advantage was obtained through consistency-preserving configuration operators (CPCOs) that guarantee the validity of the configurations during the IBEA genetic algorithm evolution process. We present a new version of Acapulco built on top of FeatureIDE, extensible through the easy integration of objective functions, providing pre-defined reusable objectives, and being able to handle complex feature model constraints.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {50–53},
numpages = {4},
keywords = {genetic algorithms, software product lines, variability management},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3233027.3236399,
author = {Kuiter, Elias and Krieter, Sebastian and Kr\"{u}ger, Jacob and Ludwig, Kai and Leich, Thomas and Saake, Gunter},
title = {PClocator: a tool suite to automatically identify configurations for code locations},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236399},
doi = {10.1145/3233027.3236399},
abstract = {The source code of highly-configurable software is challenging to comprehend, analyze, and test. In particular, it is hard to identify all configurations that comprise a certain code location. We contribute PCLocator, a tool suite that solves this problem by utilizing static analysis tools for compile-time variability. Using BusyBox and the Variability Bugs Database (VBDb), we evaluate the correctness and performance of PCLocator. The results show that we are able to analyze files in a matter of seconds and derive correct configurations in 95% of all cases.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {284–288},
numpages = {5},
keywords = {build system, configuration, preprocessor, software product line, static source code analysis},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3382025.3414942,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Kr\"{u}ger, Jacob and Mendon\c{c}a, Willian D. F.},
title = {Variability management meets microservices: six challenges of re-engineering microservice-based webshops},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414942},
doi = {10.1145/3382025.3414942},
abstract = {A microservice implements a small unit of functionality that it provides through a network using lightweight protocols. So, microservices can be combined to fulfill tasks and implement features of a larger software system---resembling a variability mechanism in the context of a software product line (SPL). Microservices and SPLs have similar goals, namely facilitating reuse and customizing, but they are usually employed in different contexts. Any developer who has access to the network can provide a microservice for any task, while SPLs are usually intended to implement features of a specific domain. Due to their different concepts, using microservices to implement an SPL or adopting SPL practices (e.g., variability management) for microservices is a challenging cross-area research problem. However, both techniques can complement each other, and thus tackling this problem promises benefits for organizations that employ either technique. In this paper, we reason on the importance of advancing in this direction, and sketch six concrete challenges to initiate research, namely (1) feature identification, (2) variability modeling, (3) variable microservice architectures, (4) interchangeability, (5) deep customization, and (6) re-engineering an SPL. We intend these challenges to serve as a starting point for future research in this cross-area research direction---avoiding that the concepts of one area are reinvented in the other.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {22},
numpages = {6},
keywords = {cloud computing, microservices, re-engineering, software product line, variability management},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3106195.3106223,
author = {Iglesias, Aitziber and Lu, Hong and Arellano, Crist\'{o}bal and Yue, Tao and Ali, Shaukat and Sagardui, Goiuria},
title = {Product Line Engineering of Monitoring Functionality in Industrial Cyber-Physical Systems: A Domain Analysis},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106223},
doi = {10.1145/3106195.3106223},
abstract = {In recent years, manufacturing technology is evolving and progressively becoming more dynamic and complex. This means that manufacturing technology (e.g., based on Industry 4.0) should be able to control the production process at runtime by monitoring physical elements and adapting itself. Such functionality is aimed at increasing production effectiveness and reducing the production cost. We argue that monitoring process can be viewed as a software product line having commonalities and variability. To support our argument, we analyzed and conducted domain analysis of two monitoring systems of Industrial Cyber-Physical Systems (ICPSs) from two industrial domains including automated warehouses and press machines. Based on the domain analysis, we present a common solution for monitoring including a software product line. With such product line, a user can configure, monitor, and visualize data of an ICPS at runtime. However, such solution could not handle the dynamic functionality related to monitoring of ICPS. Thus, we propose the use of dynamic product line and present a set of research questions that must be addressed for such solution.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {195–204},
numpages = {10},
keywords = {Cyber Physical System, Dynamic Software Product Line, Industrial domains, Software Product Line},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3336294.3336303,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Galindo, Jos\'{e} A. and Ramos-Guti\'{e}rrez, Bel\'{e}n and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Benavides, David},
title = {Process Mining to Unleash Variability Management: Discovering Configuration Workflows Using Logs},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336303},
doi = {10.1145/3336294.3336303},
abstract = {Variability models are used to build configurators. Configurators are programs that guide users through the configuration process to reach a desired configuration that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the elements that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suites stakeholders according to previous configurations. For example, when configuring a Linux distribution, the configuration process start by choosing the network or the graphic card, and then other packages with respect to a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), an automated technique that given a set of logs of previous configurations and a variability model can automatically assist to determine the configuration workflow that better fits the configuration logs generated by user activities. The technique is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Our proposal is validated using existing data from an ERP configuration environment showing its feasibility. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {265–276},
numpages = {12},
keywords = {clustering, configuration workflow, process discovery, process mining, variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/1629716.1629724,
author = {Elsner, Christoph and Lohmann, Daniel and Schr\"{o}der-Preikschat, Wolfgang},
title = {Product derivation for solution-driven product line engineering},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629724},
doi = {10.1145/1629716.1629724},
abstract = {Solution-driven product line engineering is a project business where products are created for each customer individually. Although reuse of results from former projects is widely done, configuration and integration of the results currently is often a manual, time-consuming, and error-prone task and needs considerable knowledge about implementation details.In this paper, we elaborate and approach the challenges when giving automated support for product derivation (i.e., product configuration and generation) in a large-scale solution-driven product line context. Our PLiC approach resembles the fact that, in practice, the domain of a large product line is divided into sub-domains. A PLiC (product line component) packages all results (configuration, generation, and implementation assets) of a sub-domain and offers interfaces for configuration and generation. With our approach we tackle the challenges of using multiple and different types of configuration models and text files, give support for automated product generation, and integrate feature modeling to support application engineering as an extensive development task.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {35–41},
numpages = {7},
keywords = {feature modeling, software product line development, solution-driven software development},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@inproceedings{10.1145/1383559.1383571,
author = {Tawhid, Rasha and Petriu, Dorina C.},
title = {Towards automatic derivation of a product performance model from a UML software product line model},
year = {2008},
isbn = {9781595938732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1383559.1383571},
doi = {10.1145/1383559.1383571},
abstract = {Software Product Line (SPL) engineering is a software development approach that takes advantage of the commonality and variability between products from a family, and supports the generation of specific products by reusing a set of core family assets. This paper proposes a UML model transformation approach for software product lines to derive a performance model for a specific product. The input to the proposed technique, the "source model", is a UML model of a SPL with performance annotations, which uses two separate profiles: a "product line" profile from literature for specifying the commonality and variability between products, and the MARTE profile recently standardized by OMG for performance annotations. The source model is generic and therefore its performance annotations must be parameterized. The proposed derivation of a performance model for a concrete product requires two steps: a) the transformation of a SPL model to a UML model with performance annotations for a given product, and b) the transformation of the outcome of the first step into a performance model. This paper focuses on the first step, whereas the second step will use the PUMA transformation approach of annotated UML models to performance models, developed in previous work. The output of the first step, named "target model", is a UML model with MARTE annotations, where the variability expressed in the SPL model has been analyzed and bound to a specific product, and the generic performance annotations have been bound to concrete values for the product. The proposed technique is illustrated with an e-commerce case study.},
booktitle = {Proceedings of the 7th International Workshop on Software and Performance},
pages = {91–102},
numpages = {12},
keywords = {marte, model transformation, software performance engineering, software product line, uml},
location = {Princeton, NJ, USA},
series = {WOSP '08}
}

@inproceedings{10.1145/3461001.3471149,
author = {Lesoil, Luc and Acher, Mathieu and T\'{e}rnava, Xhevahire and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {The interplay of compile-time and run-time options for performance prediction},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471149},
doi = {10.1145/3461001.3471149},
abstract = {Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {100–111},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3579027.3608985,
author = {Bittner, Paul Maximilian and Schulthei\ss{}, Alexander and Greiner, Sandra and Moosherr, Benjamin and Krieter, Sebastian and Tinnes, Christof and Kehrer, Timo and Th\"{u}m, Thomas},
title = {Views on Edits to Variational Software},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608985},
doi = {10.1145/3579027.3608985},
abstract = {Software systems are subject to frequent changes, for example to fix bugs or meet new customer requirements. In variational software systems, developers are confronted with the complexity of evolution and configurability on a daily basis; essentially handling changes to many distinct software variants simultaneously. To reduce the complexity of configurability for developers, filtered or projectional editing was introduced: By providing a partial or complete configuration, developers can interact with a simpler view of the variational system that shows only artifacts belonging to that configuration. Yet, such views are available for individual revisions only but not for edits performed across revisions. To reduce the complexity of evolution in variational software for developers, we extend the concept of views to edits. We formulate a correctness criterion for views on edits and introduce two correct operators for view generation, one operator suitable for formal reasoning, and a runtime optimized operator. In an empirical study, we demonstrate the feasibility of our operators by applying them to the change histories of 44 open-source software systems.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {141–152},
numpages = {12},
keywords = {projectional editing, software evolution, software product lines, software variability, variation control},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3382025.3414946,
author = {Fritsch, Claudia and Abt, Richard and Renz, Burkhardt},
title = {The benefits of a feature model in banking},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414946},
doi = {10.1145/3382025.3414946},
abstract = {This experience report describes the surprisingly beneficial introduction of feature modeling at KfW, a government promotional bank. On behalf of the government and based on promotional directives, KfW grants retail loans to small and medium enterprises, business founders, self-employed professionals, municipalities and private individuals. The promotional directives, called programs, define mandatory and optional properties of these loans. We have now successfully built a feature model from these properties.Our feature model will be presented with its outstanding characteristic, which is an additional subtree containing the programs as features. Complete and correct cross-tree constraints will also allow us to analyze and scope the portfolio, reduce complexity, and speed-up time-to-market. This is the advent of product line development at KfW.In order to standardize our portfolio, we have subsequently developed tools on top of the feature model, namely, a browser-based, multi-user configurator assisting non-technical-affine users in their product design, and a generator producing complete product documentation from the feature model and partial configurations. More applications are currently underway.This is our story of applying Software Product Line Engineering in banking, a domain where it is unusual or even unknown. We share our ideas, analyses, progress, and findings where the results have been thrilling us for the past two years and will continue to do so.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {9},
numpages = {11},
keywords = {document generation, experience report, feature modeling, mass customization, partial configuration, retail loans, software product line engineering},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and J\'{e}z\'{e}quel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3382025.3414952,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Gasca, Rafael M. and Carmona-Fombella, Jose Antonio and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa},
title = {AMADEUS: towards the AutoMAteD secUrity teSting},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414952},
doi = {10.1145/3382025.3414952},
abstract = {The proper configuration of systems has become a fundamental factor to avoid cybersecurity risks. Thereby, the analysis of cybersecurity vulnerabilities is a mandatory task, but the number of vulnerabilities and system configurations that can be threatened is extremely high. In this paper, we propose a method that uses software product line techniques to analyse the vulnerable configuration of the systems. We propose a solution, entitled AMADEUS, to enable and support the automatic analysis and testing of cybersecurity vulnerabilities of configuration systems based on feature models. AMADEUS is a holistic solution that is able to automate the analysis of the specific infrastructures in the organisations, the existing vulnerabilities, and the possible configurations extracted from the vulnerability repositories. By using this information, AMADEUS generates automatically the feature models, that are used for reasoning capabilities to extract knowledge, such as to determine attack vectors with certain features. AMADEUS has been validated by demonstrating the capacities of feature models to support the threat scenario, in which a wide variety of vulnerabilities extracted from a real repository are involved. Furthermore, we open the door to new applications where software product line engineering and cybersecurity can be empowered.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {11},
numpages = {12},
keywords = {cybersecurity, feature model, pentesting, reasoning, testing, vulnerabilities, vulnerable configuration},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3336294.3336316,
author = {Tolvanen, Juha-Pekka and Kelly, Steven},
title = {How Domain-Specific Modeling Languages Address Variability in Product Line Development: Investigation of 23 Cases},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336316},
doi = {10.1145/3336294.3336316},
abstract = {Domain-Specific Modeling raises the level of abstraction beyond programming by specifying the solution directly with domain concepts. Within product lines domain-specific approaches are applied to specify variability and then generate final products together with commonality. Such automated product derivation is possible because both the modeling language and generator are made for a particular product line --- often inside a single company. In this paper we examine which kinds of reuse and product line approaches are applied in industry with domain-specific modeling. Our work is based on empirical analysis of 23 cases and the languages and models created there. The analysis reveals a wide variety and some commonalities in the size of languages and in the ways they apply reuse and product line approaches.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {155–163},
numpages = {9},
keywords = {code generation, domain-specific language, domain-specific modeling, product derivation, product line variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3503229.3547058,
author = {M\'{a}rquez, Germ\'{a}n and Galindo, Jos\'{e} A. and Varela-Vaca, \'{A}ngel Jes\'{u}s and L\'{o}pez, Mar\'{\i}a Teresa G\'{o}mez and Benavides, David},
title = {Advisory: vulnerability analysis in software development project dependencies},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547058},
doi = {10.1145/3503229.3547058},
abstract = {Security has become a crucial factor in the development of software systems. The number of dependencies in software systems is becoming a source of countless bugs and vulnerabilities. In the past, the product line community has proposed several techniques and mechanisms to cope with the problems that arise when dealing with variability and dependency management in such systems. In this paper, we present Advisory, a solution that allows automated dependency analysis for vulnerabilities within software projects based on techniques from the product line community. Advisory first inspects software dependencies, then generates a dependency graph, to which security information about vulnerabilities is attributed and translated into a formal model, in this case, based on SMT. Finally, Advisory provides a set of analysis and reasoning operations on these models that allow extracting helpful information about the location of vulnerabilities of the project configuration space, as well as details for advising on the security risk of these projects and their possible configurations.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {99–102},
numpages = {4},
keywords = {CVE, dependency, impact, library, risk, security, software project, verification, vulnerability},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3503229.3547063,
author = {Fadhlillah, Hafiyyan Sayyid},
title = {Multidisciplinary variability management for cyber-physical production systems},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547063},
doi = {10.1145/3503229.3547063},
abstract = {Cyber-Physical Production Systems (CPPSs) are complex, versatile systems interacting with the environment by sensors and actuators. Specific customer demands and technical requirements lead to high engineering efforts for the control software of CPPSs, especially when following a clone-and-own approach to reuse, as is still common in industry. Utilizing systematic variability management to derive and configure control software variants from a product line could help to reduce the cost of developing and/or maintaining CPPSs. However, modeling CPPS variability is challenging as knowledge from multiple disciplines (e.g., mechanics, electrics, software) is needed, which is either implicit in practice or expressed in multiple heterogeneous engineering artifacts with diverse semantics. Furthermore, techniques commonly used to implement CPPS control software (e.g., graphical programming or modeling languages) do not have any formal mechanism to express variability. In this paper, we report on our ongoing efforts to create a multidisciplinary variability management approach for CPPSs, particularly CPPS control software. We designed our approach as an integrated approach providing configuration options based on related heterogeneous variability models from multiple disciplines. Our integrated approach can generate control software based on related domain-specific implementation artifacts.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {23–28},
numpages = {6},
keywords = {cyber-physical production systems, product configuration, software product lines, variability modeling},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3579027.3608974,
author = {Brault, Yann and El Amraoui, Yassine and Blay-Fornarino, Mireille and Collet, Philippe and Jaillet, Florent and Precioso, Fr\'{e}d\'{e}ric},
title = {Taming the Diversity of Computational Notebooks},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608974},
doi = {10.1145/3579027.3608974},
abstract = {In many applications of Computational Science and especially Data Science, notebooks are the cornerstone of knowledge and experiment sharing. Their diversity is multiple (problem addressed, input data, algorithm used, overall quality) and is not made explicit at all. As they are heavily reused through a clone-and-own approach, the tailoring process from an existing notebook to a specific problem is cumbersome, error-prone, and particularly uncertain. In this paper, we propose a tooled approach that captures the different dimensions of variability in computational notebooks. It allows one to seek an existing notebook that suits her requirements, or to generate most parts of a new one.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {27–33},
numpages = {7},
keywords = {clone-and-own, computational science, software variability},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3546932.3547023,
author = {Mannion, Mike and Kaindl, Hermann},
title = {Similarity matching for product comparison},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547023},
doi = {10.1145/3546932.3547023},
abstract = {The volume, variety and velocity of products in software-intensive systems product lines is increasing. One challenge is to understand the range of similarity between products. Reasons for product comparison include (i) to decide whether to build a new product or not (ii) to evaluate how products of the same type differ for strategic positioning or branding reasons (iii) to gauge if a product line needs to be reorganized (iv) to assess if a product falls within the national legislative and regulatory boundaries. We will discuss two different approaches to address this challenge. One is grounded in feature modelling, the other in case-based reasoning. We will also describe a specific product comparison approach using similarity matching, in which a product configured from a product line feature model is represented as a weighted binary string, the overall similarity between products is compared using a binary string metric, and the significance of individual feature combinations for product similarity can be explored by modifying the weights. We will illustrate our ideas with a mobile phone example, and discuss some of the benefits and limitations of this approach.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {258–259},
numpages = {2},
keywords = {binary strings, feature reuse, product similarity},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3461001.3473058,
author = {Ngo, Kien-Tuan and Nguyen, Thu-Trang and Nguyen, Son and Vo, Hieu Dinh},
title = {Variability fault localization: a benchmark},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3473058},
doi = {10.1145/3461001.3473058},
abstract = {Software fault localization is one of the most expensive, tedious, and time-consuming activities in program debugging. This activity becomes even much more challenging in Software Product Line (SPL) systems due to the variability of failures in SPL systems. These unexpected behaviors are caused by variability faults which can only be exposed under some combinations of system features. Although localizing bugs in non-configurable code has been investigated in-depth, variability fault localization in SPL systems still remains mostly unexplored. To approach this challenge, we propose a benchmark for variability fault localization with a large set of 1,570 buggy versions of six SPL systems and baseline variability fault localization performance results. Our hope is to engage the community to propose new and better approaches to the problem of variability fault localization in SPL systems.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {120–125},
numpages = {6},
keywords = {benchmark, variability bug, variability fault localization},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3236405.3236410,
author = {Kr\"{o}her, Christian and El-Sharkawy, Sascha and Schmid, Klaus},
title = {KernelHaven: an open infrastructure for product line analysis},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236410},
doi = {10.1145/3236405.3236410},
abstract = {KernelHaven is an open infrastructure for Software Product Line (SPL) analysis. It is intended both as a production-quality analysis tool set as well as a research support tool, e.g., to support researchers in systematically exploring research hypothesis. For flexibility and ease of experimentation KernelHaven components are plug-ins for extracting certain information from SPL artifacts and processing this information, e.g., to check the correctness and consistency of variability information or to apply metrics. A configuration-based setup along with automatic documentation functionality allows different experiments and supports their easy reproduction.Here, we describe KernelHaven as a product line analysis research tool and highlight its basic approach as well as its fundamental capabilities. In particular, we describe available information extraction and processing plug-ins and how to combine them. On this basis, researchers and interested professional users can rapidly conduct a first set of experiments. Further, we describe the concepts for extending KernelHaven by new plug-ins, which reduces development effort when realizing new experiments.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {5–10},
numpages = {6},
keywords = {empirical software engineering, software product line analysis, static analysis, variability extraction},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3336294.3336320,
author = {Dahlem, Marc and Rahm, Ricarda and Becker, Martin},
title = {App Variants and Their Impact on Mobile Architecture: An Experience Report},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336320},
doi = {10.1145/3336294.3336320},
abstract = {In order to raise the awareness of industrial practitioners and researchers regarding specific PLE-related issues and approaches, this paper shares some experiences made by Insiders Technologies regarding the development and provisioning of mobile app variants and the impact of variability on the app architecture. Using the smart MOBILE app product line as an example, the paper characterizes the mobile app market, identifies key variant drivers, introduces influential technologies and their constraints, and discusses viable tactics to support adequate variability in the architecture of a mobile app.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {33–38},
numpages = {6},
keywords = {mobile app development, mobile application, software architecture, software product line},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3579027.3608971,
author = {Eichhorn, Domenik and Pett, Tobias and Osborne, Tobias and Schaefer, Ina},
title = {Quantum Computing for Feature Model Analysis: Potentials and Challenges},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608971},
doi = {10.1145/3579027.3608971},
abstract = {Feature modeling is a technique to model the variability of configurable systems. When working with feature models, it is possible to analyze them, for instance, by counting the number of valid configurations, searching feature model anomalies, or creating samples of configurations for testing. Classical feature model analysis techniques are based on solving algorithmic problems such as boolean satisfiability, satisfiability modulo theories, or integer linear programming. Existing analysis approaches provide satisfactory solutions for small and medium-sized problem instances, but scaling issues are observed for large-sized feature models. Quantum computers provide up to superpolynomial speedups for specific algorithmic problems and have the potential to solve those scaling issues. This paper analyzes the algorithmic techniques used in classical product line analysis and identifies potentials and challenges for quantum speedups. Our findings show that quantum algorithms like QAOA and Grover have the potential to speed up SAT and ILP-based feature model analysis techniques, but only after additional improvements in quantum hardware have been made.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {1–7},
numpages = {7},
keywords = {feature model analysis, quantum algorithms, quantum computing},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3307630.3342416,
author = {Rodriguez, Germania and P\'{e}rez, Jennifer and Benavides, David},
title = {Accessibility Variability Model: The UTPL MOOC Case Study},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342416},
doi = {10.1145/3307630.3342416},
abstract = {Several approaches to define Variability Models (VM) of non-functional requirements or quality attributes have been proposed. However, these approaches have focused on specific quality attributes rather than more general non-functional aspects established by standards such as ISO/IEC 25010 for software evaluation and quality. Thus, developing specific software products by selecting features and at the same time measuring the level of compliance with a standard/guideline is a challenge. In this work, we present the definition of an accessibility VM based on the web content accessibility guides (WCAG) 2.1 W3C recommendation, to obtain a quantitative measure to improve or construct specific SPL products that require to be accessibility-aware. This paper is specially focused on illustrating the experience of measuring the accessibility in a software product line (SPL) in order to check if it is viable measuring products and recommending improvements in terms of features before addressing the construction of accessibility-aware products. The adoption of the VM accessibility has been putted into practice through a pilot case study, the MOOC (Massive Open Online Course) initiative of the Universidad T\'{e}cnica Particular de Loja. The conduction of this pilot case study has allowed us to illustrate how it is possible to model and measure the accessibility in SPL using accessibility VM, as well as to recommend accessibility configuration improvements for the construction of new or updated MOOC platforms.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {114–121},
numpages = {8},
keywords = {reusability, software and its engineering, software creation and management, software development techniques, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1145/1640162.1640164,
author = {Catal, Cagatay},
title = {Barriers to the adoption of software product line engineering},
year = {2009},
issue_date = {November 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/1640162.1640164},
doi = {10.1145/1640162.1640164},
abstract = {Software development costs and time to deploy a software-intensive system significantly decrease when Software Product Line Engineering (SPLE) approach is applied. Numerous case stu-dies in industrial and military domains have shown that reliability, quality, productivity and user interface consistency improve drasti-cally in addition to the decrease of cost and time-to-market. Also, this system engineering approach is very effective in three market strategies known as cost leadership, differentiation, and focusing. Despite these measurable benefits, product line engineering adop-tion is slower than the other technological trends such as Service Oriented Architecture (SOA), Model Driven Development (MDD), and Aspect Oriented Software Development (AOSD). In this pa-per, we investigate the barriers to the adoption of SPLE and ex-plore the root causes of them from three points of views: Project sponsor, organization, and SPLE community. We provide sugges-tions for how the industry and SPLE community can solve these multi-dimensional issues in a short term.},
journal = {SIGSOFT Softw. Eng. Notes},
month = dec,
pages = {1–4},
numpages = {4},
keywords = {commonality, core asset development, product families, software product lines, software reuse, variability}
}

@inproceedings{10.1145/2491627.2491631,
author = {Myll\"{a}rniemi, Varvana and Savolainen, Juha and M\"{a}nnist\"{o}, Tomi},
title = {Performance variability in software product lines: a case study in the telecommunication domain},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491631},
doi = {10.1145/2491627.2491631},
abstract = {In the research on software product lines, product variants typically differ by their functionality, and quality attributes are more or less similar across products. To accumulate empirical evidence, this paper presents a descriptive case study of performance variability in a software product line of mobile network base stations. The goal is to study the motivation to vary performance, and the strategy for realizing performance variability in the product line architecture. The results highlight that the evolution of customer needs motivates performance variability; performance variability can be realized either with software or hardware variability strategy, with the latter often being prevailing; and the software strategy can be kept focused by downgrading performance.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {32–41},
numpages = {10},
keywords = {architecture, case study, software product line, variability},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3503229.3547038,
author = {Workalemahu, Robel Negussie and Forza, Cipriano and Suzic, Nikola},
title = {Product configurators for additively manufactured products: exploring their peculiar characteristics},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547038},
doi = {10.1145/3503229.3547038},
abstract = {The capability of realizing individually customized products with complex geometries makes additive manufacturing (AM) ever more considered by companies engaged in mass customized manufacturing. In order to be exploited in the market, the AM allowed geometry freedom has to be transferred to the customers for the customer-specific customization. Notably, this is a new request posed to product configurators (PC). So, in this research we ask: How is this request being answered by pioneers who engage in this challenge? Are there other new requests that AM poses to configurators? The present paper aims at answering these exploratory questions by looking at how these issues have been considered in existing literature and by providing some examples. We hope that considerations derived from this investigation will open a discussion on this topic in the product configuration research community with the goal to identify peculiar PC capabilities needed to customize additively manufactured products using PCs.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {201–208},
numpages = {8},
keywords = {additive manufacturing, mass customization, personalization, product configuration},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3579027.3608996,
author = {Fischer, Stefan and Ramler, Rudolf and Assun\c{c}\~{a}o, Wesley K. G. and Egyed, Alexander and Gradl, Christian and Auberger, Sebastian},
title = {Model-based Testing for a Family of Mobile Applications: Industrial Experiences},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608996},
doi = {10.1145/3579027.3608996},
abstract = {Testing is a fundamental verification activity to produce high-quality software. However, testing is a costly and complex activity. The success of software testing depends on the quality of test cases but finding a good set of test cases is laborious. To make matters worse, when dealing with a family of systems (e.g., variants of a mobile applications), test cases must assure that a diversity of configurations in potentially many variants work as expected. This is the case of hello again GmbH, a company that develops mobile applications for customer loyalty (e.g., discounts, free products, rewards, or insider perks). The company targets several business domains, and currently supports about 700 application variants. Testing such applications including all their variability is a cumbersome task. Even simple test cases designed for one variant most likely cannot be reused for other variants. To support developers at hello again GmbH, we present a solution to employ a model-based testing approach to their family of mobile apps. Model-based testing focuses on automatizing the design and generation of test cases. We present results of applying model-based testing on 27 applications from hello again GmbH and report the challenges and lessons learned for designing a variable test model. Our expected contribution is to support companies and practitioners looking for solutions to test families of software products.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {242–253},
numpages = {12},
keywords = {Mobile Testing, Software Product Lines, Variability Testing},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/1595808.1595819,
author = {Anastasopoulos, Michail},
title = {Increasing efficiency and effectiveness of software product line evolution: an infrastructure on top of configuration management},
year = {2009},
isbn = {9781605586786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1595808.1595819},
doi = {10.1145/1595808.1595819},
abstract = {Software Product Line Engineering entails the strategic development of software assets that are to be reused many times across the members of a product line. Assuring that the investment in reuse holds over time is an important requirement in this case. To that end it is necessary that evolution is carefully managed: Changes in reusable assets and their customized instances need to be tracked and propagated efficiently. Configuration Management is a mature discipline for that purpose. However traditional configuration management does not address product line evolution scenarios explicitly. Over time this can lead to great evolution management effort. This paper presents an infrastructure - in particular its validation - that sits on top of traditional configuration management and is tailored to evolution scenarios in Product Line Engineering. The result is a reduction of effort and an increase of correctness},
booktitle = {Proceedings of the Joint International and Annual ERCIM Workshops on Principles of Software Evolution (IWPSE) and Software Evolution (Evol) Workshops},
pages = {47–56},
numpages = {10},
keywords = {evolution, software product lines},
location = {Amsterdam, The Netherlands},
series = {IWPSE-Evol '09}
}

@inproceedings{10.1145/3579027.3608984,
author = {Krieter, Sebastian and Kr\"{u}ger, Jacob and Leich, Thomas and Saake, Gunter},
title = {VariantInc: Automatically Pruning and Integrating Versioned Software Variants},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608984},
doi = {10.1145/3579027.3608984},
abstract = {Developers use version-control systems and software-hosting platforms to manage their software systems. They rely on the provided branching and forking mechanisms to implement new features, fix bugs, and develop customized system variants. A particular problem arises when forked variants are not re-integrated (i.e., merged), but kept and co-evolved as individual systems. This can cause maintenance overheads, due to change propagation and limitations in simultaneously managing variations in space (variants) and time (revisions). Thus, most organizations decide to integrate their set of variants into a single platform at some point, and several techniques have been proposed to semi-automate such an integration. However, existing techniques usually consider only a single revision of each variant and do not merge the revision histories, disregarding that not only variants (i.e., configuring the features of the system) but also revisions (i.e., checking out specific versions of the features) are important. We propose an automated technique, VariantInc, for analyzing, pruning, and integrating variants of a system that also merges the revision history of each variant into the resulting platform (i.e., using presence conditions). To validate VariantInc, we employed it on 160 open-source C systems of various sizes (i.e., number of forks, revisions, source code). The results show that VariantInc works as intended, and allows developers or researchers to automatically integrate variants into a platform as well as to perform software analyses.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {129–140},
numpages = {12},
keywords = {Forks, Variant integration, Variant-rich systems, Version control},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/2934466.2934484,
author = {Vasilevskiy, Anatoly and Chauvel, Franck and Haugen, \O{}ystein},
title = {Toward robust product realisation in software product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934484},
doi = {10.1145/2934466.2934484},
abstract = {Product derivation is a building process of products from selected features in software product lines (SPLs). Realisation paves the way for automatic product derivation. A realisation defines a mapping between abstract features in a feature tree and their implementation artefacts in a model, and therefore governs the derivation of a new product. We experience that a realisation is not always straightforward and robust against modifications in the model. In the paper, we introduce an approach to build robust realisations. It consists of automated planning techniques and a layered architecture to yield a product. We demonstrate how our approach can leverage modern means of software design, development and validation. We evaluate the approach on a use-case provided by an industry partner and compare our technique to the existing realisation layer in the Base Variability Resolution (BVR) language.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {184–193},
numpages = {10},
keywords = {automated planning, bvr, fragment substitution, model, product derivation, product line, realisation, variation point},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3233027.3233039,
author = {Pereira, Juliana Alves and Schulze, Sandro and Figueiredo, Eduardo and Saake, Gunter},
title = {N-dimensional tensor factorization for self-configuration of software product lines at runtime},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233039},
doi = {10.1145/3233027.3233039},
abstract = {Dynamic software product lines demand self-adaptation of their behavior to deal with runtime contextual changes in their environment and offer a personalized product to the user. However, taking user preferences and context into account impedes the manual configuration process, and thus, an efficient and automated procedure is required. To automate the configuration process, context-aware recommendation techniques have been acknowledged as an effective mean to provide suggestions to a user based on their recognized context. In this work, we propose a collaborative filtering method based on tensor factorization that allows an integration of contextual data by modeling an N-dimensional tensor User-Feature-Context instead of the traditional two-dimensional User-Feature matrix. In the proposed approach, different types of non-functional properties are considered as additional contextual dimensions. Moreover, we show how to self-configure software product lines by applying our N-dimensional tensor factorization recommendation approach. We evaluate our approach by means of an empirical study using two datasets of configurations derived for medium-sized product lines. Our results reveal significant improvements in the predictive accuracy of the configuration over a state-of-the-art non-contextual matrix factorization approach. Moreover, it can scale up to a 7-dimensional tensor containing hundred of configurations in a couple of milliseconds.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {87–97},
numpages = {11},
keywords = {recommender systems, runtime decision-making, self-configuration, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3106195.3106212,
author = {Marimuthu, C. and Chandrasekaran, K.},
title = {Systematic Studies in Software Product Lines: A Tertiary Study},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106212},
doi = {10.1145/3106195.3106212},
abstract = {Software product lines are widely used in the software industries to increase the re-usability and to decrease maintenance cost. On the other hand, systematic reviews are widely used in the software engineering research community to provide the overview of the research field and practitioners guidelines. Researchers have conducted many systematic studies on the different aspects of SPLs. To the best of our knowledge, till now there is no tertiary study conducted on systematic studies of SPL related research topics. In this paper, we aim at conducting a systematic mapping study of existing systematic studies to report the overview of the findings for researchers and practitioners. We performed snowballing and automated search to find out the relevant systematic studies. As a result, we analyzed 60 relevant studies to answer 5 research questions. The main focus of this tertiary study is to highlight the research topics, type of published reviews, active researchers and publication forums. Additionally, we highlight some of the limitations of the systematic studies. The important finding of this study is that the research field is well matured as the systematic studies covered a wide range of research topics. Another important finding is that many studies provided information for practitioners as well as researchers which is a notable improvement in the systematic reviews. However, many studies failed to assess the quality of the primary studies which is the major limitation of the existing systematic studies.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {143–152},
numpages = {10},
keywords = {software product line, systematic review, tertiary study},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3336294.3342370,
author = {Li, Yan and Yue, Tao and Ali, Shaukat and Zhang, Li},
title = {Enabling Automated Requirements Reuse and Configuration},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342370},
doi = {10.1145/3336294.3342370},
abstract = {Software-intensive systems belonging to a product line (PL) often have their shared architecture design available before they are developed. Therefore, the PL often has a large number of reusable and configurable requirements, which are naturally organized hierarchically based on the architecture of the PL. To enable reuse of requirements through configuration at the requirements engineering phase, it is important to provide a methodology (with tool support) to help practitioners to systematically and automatically develop structured and configuration-ready PL requirements repositories. Such a repository can cost-effectively facilitate the development of requirement repositories specific to individual products, i.e., individual systems. In addition, configurations to the repository at the requirements engineering phase of developing a system are part of its complete configurations and can be naturally carried on to downstream product configuration phases such as the design level configuration phase. A complete set system configurations can then be systematically obtained and managed. In this paper, we propose a methodology with tool support, named as Zen-ReqConfig, which is built on existing model-based technologies, natural language processing, and similarity measure techniques, for developing PL requirement repositories and facilitating requirements configuration. Zen-ReqConfig first automatically devises a hierarchical structure for a PL requirements repository. Then, it automatically identifies variabilities in textual requirements. Based on the developed configuration-ready PL requirements repository, it can then facilitate the configuration of products/systems at the requirements level. Zen-ReqConfig relies on two types of variability modeling techniques: cardinality-based feature modeling (CBFM) and a UML-based variability modeling methodology (named as SimPL). Both CBFM and SimPL have been used to address real-world variability modelling problems. To gain insights on the performance of Zen-ReqConfig, we evaluated it with five case studies and experimented with two different similarity measures and two different modelling methods: SimPL and CBFM. Results show that Zen-ReqConfig performed better when it is combined with the Jaro similarity measure. When Zen- ReqConfig is integrated with Jaro, it can (1) structure PL textual requirements under the most fit match criterion with high precision and recall, over 95% for both CBFM and SimPL; (2) identify variabilities in textual requirements under the most fit match criterion, with the average precision over 97% for SimPL and CBFM, and with the average recall over 94% for both SimPL and CBFM; and (3) generate repository structures within 1 second; 4) and allocate a requirement to the repository within 2 seconds on average. When looking into the impact of the two modelling methods on the performance of Zen-ReqConfig, we did not observe practical differences between SimPL and CBFM, implying that Zen-ReqConfig works well with both SimPL and CBFM.Pointer to the original paper: https://link.springer.com/article/10.1007/s10270-017-0641-6},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {206},
numpages = {1},
keywords = {model-based engineering, requirements engineering, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3546932.3546995,
author = {Sepasi, Elmira Rezaei and Balouchi, Kambiz Nezami and Mercier, Julien and Lopez-Herrejon, Roberto Erick},
title = {Towards a cognitive model of feature model comprehension: an exploratory study using eye-tracking},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3546995},
doi = {10.1145/3546932.3546995},
abstract = {Feature models are pivotal components of Software Product Lines. Therefore, their correct comprehension is crucial for performing adequately all the tasks where they are involved. Despite their importance, to the best of our knowledge, no research has been done on feature model comprehension using eye-trackers. As a first step to address this lack, our work contributes an empirical study of feature model comprehension in simple configuration validation tasks. We propose a first cognitive model for this type of tasks that we analyze by measuring eye gaze fixations on the different visual elements involved in the tasks. Our results identified three main components of the cognitive model and their distribution in terms of the cognitive effort for performing these tasks. We argue that further research on feature model comprehension can inform language design and tool development to provide more suitable language structures, user interfaces and support for this kind of models.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {21–31},
numpages = {11},
keywords = {eye-trackers, feature models, gaze analysis, software product lines},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3503229.3547032,
author = {He\ss{}, Tobias and M\"{u}ller, Tobias and Sundermann, Chico and Th\"{u}m, Thomas},
title = {ddueruem: a wrapper for feature-model analysis tools},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547032},
doi = {10.1145/3503229.3547032},
abstract = {We present ddueruem, a tool for interfacing with BDD libraries, d-DNNF compilers, uniform, and t-wise samplers. To ease usage by researchers and practitioners, ddueruem is capable of automatically installing all tools it interfaces with, provides a coherent but customizable command-line interface to all the tools, unifies their outputs, and gathers additional statistics. In its present form, ddueruem supports the BDD libraries BuDDy and CUDD, five uniform samplers, as well as proof-of-concept support for the t-wise sampler YASA and the d-DNNF compiler d4.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {54–57},
numpages = {4},
keywords = {benchmark, feature-model analysis, knowledge compilation, sampling},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3579027.3608973,
author = {Galindo, Jos\'{e} A. and Dominguez, Antonio J. and White, Jules and Benavides, David},
title = {Large Language Models to generate meaningful feature model instances},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608973},
doi = {10.1145/3579027.3608973},
abstract = {Feature models are the "de facto" standard for representing variability in software-intensive systems. Automated analysis of feature models is the computer-aided extraction of information of feature models and is used in testing, maintenance, configuration, and derivation, among other tasks. Testing the analyses of feature models often requires relying on a large number of models that are as realistic as possible. There exist different proposals to generate synthetic feature models using random techniques or metamorphic relations; however, the existing methods do not take into account the semantics of the concepts of the domain that are being represented and the interrelations between them, leading to less realistic feature models. In this paper, we propose a novel approach that uses Large Language Models (LLMs), such as Codex or GPT-3, to generate realistic feature models that preserve semantic coherence while maintaining syntactic validity. The approach automatically generates instances of feature models from a given domain. Concretely, two language models were used, first OpenAI's Codex to generate new instances of feature models using the Universal Variability Language (UVL) syntax and then Cohere's semantic analysis to verify if the newly introduced concepts are from the same domain. This approach enabled the generation of 90% of valid instances according to the UVL syntax. In addition, the valid models score well on model complexity metrics, and the generated features mirror the domain of the original UVL instance used as prompts. With this work, we envision a new thread of research where variability is generated and analyzed using LLMs. This opens the door for a new generation of techniques and tools for variability management.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {15–26},
numpages = {12},
keywords = {deep learning, large language models, synthetic models, universal variability language},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3382026.3431251,
author = {Mortara, Johann and Collet, Philippe and T\"{e}rnava, Xhevahire},
title = {Identifying and Mapping Implemented Variabilities in Java and C++ Systems using symfinder},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431251},
doi = {10.1145/3382026.3431251},
abstract = {Variability is present in most modern object-oriented softwareintensive systems, despite that they commonly do not follow a product line approach. In these systems, variability is implicit and hardly documented as it is implemented by different traditional mechanisms, namely inheritance, overloading, or design patterns. This hampers variability management as automatic identification of variation points (vp-s) with variants is very difficult. symfinder is a symmetry-based tooled approach that enables automatic identification of potential vp-s with variants in such systems. Then, it visualizes them relying on their density in code assets. From the Java-only version presented at SPLC'2019, we present here several notable improvements. They concern an added support for C++ systems, the identification of vp-s implemented by Decorator and Template pattern instances, an enhanced visualization (e.g., to display all variants, and package coloring), as well as automation of the mapping of potential vp-s to domain features.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {9–12},
numpages = {4},
keywords = {Identifying software variability, object-oriented variability-rich systems, software product line engineering, tool support for understanding software variability, visualizing software variability},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2648511.2648523,
author = {Urli, Simon and Blay-Fornarino, Mireille and Collet, Philippe},
title = {Handling complex configurations in software product lines: a tooled approach},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648523},
doi = {10.1145/2648511.2648523},
abstract = {As Software Product Lines (SPLs) are now more widely applied in new application fields such as IT or Web systems, complex and large-scale configurations have to be handled. In these fields, the strong domain orientation leads to the need to manage interrelated SPLs and multiple instances of configured sub-products, resulting in complex configurations that cannot be easily represented by simple sets of features. In this paper we propose a tooled approach to manage such SPLs through a domain model that interrelates several feature models in a consistent way. The approach thus shifts part of the domain knowledge to the problem space and supports the derivation of complex configurations with multiple instantiations and associations of sub-products. We also report on the application of our approach to an industrial-strength software development in the field of digital signage.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {112–121},
numpages = {10},
keywords = {configuration, software product line},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3382025.3414967,
author = {Lima, Jackson A. Prado and Mendon\c{c}a, Willian D. F. and Vergilio, Silvia R. and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Learning-based prioritization of test cases in continuous integration of highly-configurable software},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414967},
doi = {10.1145/3382025.3414967},
abstract = {Continuous Integration (CI) is a practice widely adopted in the industry to allow frequent integration of code changes. During the CI process, many test cases are executed multiple times a day, subject to time constraints. In this scenario, a learning-based approach, named COLEMAN, has been successfully applied. COLEMAN allows earlier execution of the most promising test cases to reveal faults. This approach considers CI particularities such as time budget and volatility of test cases, related to the fact that test cases can be added/removed along the CI cycles. In the CI of Highly Configuration System (HCS), many product variants must be tested, each one with different configuration options, but having test cases that are common to or reused from other variants. In this context, we found, by analogy, another particularity, the volatility of variants, that is, some variants can be included/discontinued along CI cycles. Considering this context, this work introduces two strategies for the application of COLEMAN in the CI of HCS: the Variant Test Set Strategy (VTS) that relies on the test set specific for each variant, and the Whole Test Set Strategy (WST) that prioritizes the test set composed by the union of the test cases of all variants. Both strategies are evaluated in a real-world HCS, considering three test budgets. The results show that the proposed strategies are applicable regarding the time spent for prioritization. They perform similarly regarding early fault detection, but WTS better mitigates the problem of beginning without knowledge, and is more suitable when a new variant to be tested is added.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {31},
numpages = {11},
keywords = {continuous integration, family of products, software product line, test case prioritization},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3233027.3236397,
author = {Mendon\c{c}a, Willian D. F. and Assun\c{c}\~{a}o, Wesley K. G. and Linsbauer, Lukas},
title = {Multi-objective optimization for reverse engineering of apo-games feature models},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236397},
doi = {10.1145/3233027.3236397},
abstract = {Software Product Lines Engineering (SPLE) is a software development approach intended for the development and maintenance of variable systems, i.e. systems that exist in many different variants. In the long run SPLE has many advantages. However, it requires a large upfront investment of time and money, which is why in practice Software Product Lines (SPLs) are rarely developed from scratch. Instead, they are often built using an extractive approach by which a set of existing system variants is consolidated (i.e. reverse engineered) into an SPL. A crucial part of this process is the construction of a variability model like a Feature Model (FM) that describes the common and variable parts of the system variants. In this paper we apply an approach for reverse engineering feature models based on a multi-objective optimization algorithm to the given challenge of constructing a feature model for a set of game variants and we present the results.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {279–283},
numpages = {5},
keywords = {feature model, reverse engineering, software product line},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3233027.3236403,
author = {Kr\"{u}ger, Jacob and Fenske, Wolfram and Th\"{u}m, Thomas and Aporius, Dirk and Saake, Gunter and Leich, Thomas},
title = {Apo-games: a case study for reverse engineering variability from cloned Java variants},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236403},
doi = {10.1145/3233027.3236403},
abstract = {Software-product-line engineering is an approach to systematically manage reusable software features and has been widely adopted in practice. Still, in most cases, organizations start with a single product that they clone and modify when new customer requirements arise (a.k.a. clone-and-own). With an increasing number of variants, maintenance can become challenging and organizations may consider migrating towards a software product line, which is referred to as extractive approach. While this is the most common approach in practice, techniques to extract variability from cloned variants still fall short in several regards. In particular, this accounts for the low accuracy of automated analyses and refactoring, our limited understanding of the costs involved, and the high manual effort. A main reason for these limitations is the lack of realistic case studies. To tackle this problem, we provide a set of cloned variants. In this paper, we characterize these variants and challenge the research community to apply techniques for reverse engineering feature models, feature location, code smell analysis, architecture recovery, and the migration towards a software product line. By evaluating solutions with the developer of these variants, we aim to contribute to a larger body of knowledge on this real-world case study.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {251–256},
numpages = {6},
keywords = {case study, data set, extractive approach, feature location, reverse engineering, software-product-line engineering},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3382025.3414725,
author = {Ferreira, Fischer and Viggiato, Markos and Souza, Maur\'{\i}cio and Figueiredo, Eduardo},
title = {Testing configurable software systems: the failure observation challenge},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414725},
doi = {10.1145/3382025.3414725},
abstract = {Configurable software systems can be adapted or configured according to a set of features to increase reuse and productivity. The testing process is essential because configurations that fail may potentially hurt user experience and degrade the reputation of a project. However, testing configurable systems is very challenging due to the number of configurations to run with each test, leading to a combinatorial explosion in the number of configurations and tests. Currently, several testing techniques and tools have been proposed to deal with this challenge, but their potential practical application remains mostly unexplored. To encourage the research area on testing configurable systems, researchers and practitioners should be able to try out their solutions in common datasets. In this paper, we propose a dataset with 22 configurable software systems and an extensive test suite. Moreover, we report failures found in these systems and source code metrics to allow evaluating candidate solutions. We hope to engage the community and stimulate new and existing approaches to the problem of testing configurable systems.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {28},
numpages = {6},
keywords = {software product line, testing configurable systems},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/1244002.1244266,
author = {Inoki, Mari and Fukazawa, Yoshiaki},
title = {Software product line evolution method based on kaizen approach},
year = {2007},
isbn = {1595934804},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1244002.1244266},
doi = {10.1145/1244002.1244266},
abstract = {Continuing optimal product line development needs to evolve core assets in response to market, technology or organization changes. In this paper, we propose a product line evolution method based on the kaizen approach. Kaizen is a continuous improvement method that is adopted in Japanese industry. The important points of the kaizen are to prepare a work standard and continue to improve processes by correcting the differences between the standard and actual results. Our core asset kaizen method provides a standard that includes core asset types based on simple metrics, kaizen patterns representing expertise, and kaizen processes for continuous improvement.},
booktitle = {Proceedings of the 2007 ACM Symposium on Applied Computing},
pages = {1207–1214},
numpages = {8},
keywords = {kaizen, core asset, evolution, pattern, software product line},
location = {Seoul, Korea},
series = {SAC '07}
}

@inproceedings{10.1145/3109729.3109737,
author = {Montalvillo, Leticia and D\'{\i}az, Oscar and Azanza, Maider},
title = {Visualizing product customization efforts for spotting SPL reuse opportunities},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109737},
doi = {10.1145/3109729.3109737},
abstract = {Migrating a set of product variants to a managed SPL is rarely a one-shot effort. Experiences from industry revealed that a complete migration to an SPL might take years, during which customers' requirements still need to be fulfilled by the company (customization effort). Analyzing the assets that have been customized by products (customization analysis) becomes a main stepping stone in ascertaining reuse opportunities. This requires to remain vigilant to arising reuse opportunities not just at the SPL onset, but throughout the whole process. Traditionally, a common mechanism to identify reuse opportunities is the diff utility whereby differences between two files are calculated and displayed. But this mechanism might not scale up. Given the sheer number of both core-assets and SPL products, visualizations that abstract from conventional line-level diffs to higher level visualization are required to spot reuse opportunities a ta glance. To this end, we introduce visualizations that help to estimate the extent of the customization effort broken down by product and core-asset. The aim: a prompt insight into questions such as, how much effort are product developers spending on customization?; or, which core-assets needed a larger tuning to meet product requirements? This vision is realized in CUSTOMS, a visualization utility on top of FeatureHouse that resorts to alluvial diagrams and tree maps to display customization effort. CUSTOMS might serve as a first stepping stone for spotting reuse opportunities.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {73–80},
numpages = {8},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3106195.3106217,
author = {Kr\"{u}ger, Jacob and Nielebock, Sebastian and Krieter, Sebastian and Diedrich, Christian and Leich, Thomas and Saake, Gunter and Zug, Sebastian and Ortmeier, Frank},
title = {Beyond Software Product Lines: Variability Modeling in Cyber-Physical Systems},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106217},
doi = {10.1145/3106195.3106217},
abstract = {Smart IT has an increasing influence on the control of daily life. For instance, smart grids manage power supply, autonomous automobiles take part in traffic, and assistive robotics support humans in production cells. We denote such systems as Cyber-physical Systems (CPSs), where cyber addresses the controlling software, while physical describes the controlled hardware. One key aspect of CPSs is their capability to adapt to new situations autonomously or with minimal human intervention. To achieve this, CPSs reuse, reorganize and reconfigure their components during runtime. Some components may even serve in different CPSs and different situations simultaneously. The hardware of a CPS usually consists of a heterogeneous set of variable components. While each component can be designed as a software product line (SPL), which is a well established approach to describe software and hardware variability, it is not possible to describe CPSs' variability solely on a set of separate, non-interacting product lines. To properly manage variability, a CPS must specify dependencies and interactions of its separate components and cope with variable environments, changing requirements, and differing safety properties. In this paper, we i) propose a classification of variability aspects, ii) point out current challenges in variability modeling, and iii) sketch open research questions. Overall, we aim to initiate new research directions for variable CPSs based on existing product-line techniques.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {237–241},
numpages = {5},
keywords = {Cyber-physical system, Modeling, Software product line},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3233027.3233040,
author = {Kr\"{u}ger, Jacob and Al-Hajjaji, Mustafa and Schulze, Sandro and Saake, Gunter and Leich, Thomas},
title = {Towards automated test refactoring for software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233040},
doi = {10.1145/3233027.3233040},
abstract = {In practice, organizations often rely on the clone-and-own approach to reuse and customize existing systems. While increasing maintenance costs encourage some organizations to adopt their development processes towards more systematic reuse, others still avoid migrating to a reusable platform. Based on our experiences, a barrier preventing the adoption of software product lines is the fear of introducing new and more problematic bugs---during the migration or later on. We are aware of several works that automate software-product-line adoption, but they neglect the migration and maintenance of test cases. Automating the refactoring of tests can help to facilitate the adoption barrier, compare the quality after migrations, and support maintenance. In this vision paper, we i) discuss open research challenges that are based on our experiences and ii) sketch a first framework to develop automated solutions. Overall, we aim to illustrate our idea and initiate further research to facilitate the adoption and maintenance of software product lines.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {143–148},
numpages = {6},
keywords = {extractive approach, legacy system, maintenance, migration, software product line, testing},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3307630.3342704,
author = {Ca\~{n}ete, Angel},
title = {Energy Efficient Assignment and Deployment of Tasks in Structurally Variable Infrastructures},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342704},
doi = {10.1145/3307630.3342704},
abstract = {The importance of cyber-physical systems is growing very fast, being part of the Internet of Things vision. These devices generate data that could collapse the network and can not be assumed by the cloud. New technologies like Mobile Cloud Computing and Mobile Edge Computing are taking importance as solution for this issue. The idea is offloading some tasks to devices situated closer to the user device, reducing network congestion and improving applications performance (e.g., in terms of latency and energy). However, the variability of the target devices' features and processing tasks' requirements is very diverse, being difficult to decide which device is more adequate to deploy and run such processing tasks. Once decided, task offloading used to be done manually. Then, it is necessary a method to automatize the task assignation and deployment process. In this thesis we propose to model the structural variability of the deployment infrastructure and applications using feature models, on the basis of a SPL engineering process. Combining SPL methodology with Edge Computing, the deployment of applications is addressed as the derivation of a product. The data of the valid configurations is used by a task assignment framework, which determines the optimal tasks offloading solution in different network devices, and the resources of them that should be assigned to each task/user. Our solution provides the most energy and latency efficient deployment solution, accomplishing the QoS requirements of the application in the process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {222–229},
numpages = {8},
keywords = {energy efficiency, latency, mobile cloud computing, mobile edge computing, optimisation, software product line},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3461001.3471142,
author = {Gu\'{e}gain, \'{E}douard and Quinton, Cl\'{e}ment and Rouvoy, Romain},
title = {On reducing the energy consumption of software product lines},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471142},
doi = {10.1145/3461001.3471142},
abstract = {Along the last decade, several studies considered green software design as a key development concern to improve the energy efficiency of software. Yet, few techniques address this concern for Software Product Lines (SPL). In this paper, we therefore introduce two approaches to measure and reduce the energy consumption of a SPL by analyzing a limited set of products sampled from this SPL. While the first approach relies on the analysis of individual feature consumptions, the second one takes feature interactions into account to better mitigate energy consumption of resulting products.Our experimental results on a real-world SPL indicate that both approaches succeed to produce significant energy improvements on a large number of products, while consumption data was modeled from a small set of sampled products. Furthermore, we show that taking feature interactions into account leads to more products improved with higher energy savings per product.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {89–99},
numpages = {11},
keywords = {consumption, energy, measurement, mitigation, software product lines},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3546932.3547074,
author = {Dhungana, Deepak and Haselb\"{o}ck, Alois and Ruiz-Torrubiano, Rub\'{e}n and Wallner, Stefan},
title = {Variability of safety risks in production environments},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547074},
doi = {10.1145/3546932.3547074},
abstract = {One of the major steps between (re-)configuration of a factory and the start of production is the commissioning phase, where certification of safety requirements and assessment of potential hazards is a key activity. Typically, assessment of safety risks is a manual process that incorporates the experience and knowledge of the involved stakeholders. The flexibility and the speed gained by automated (re-)configuration of production environments is decelerated by the manual safety certification process, before the factory can start production. This paper is an attempt to eliminate this bottleneck by proposing a model-driven approach to safety risk assessment. An approach based on several models of safety risks enables potential safety risks to be "instantiated" for any given factory at hand. This shortens the recurring process of identifying the risks. A model-driven approach was chosen to capture and utilize the tacit knowledge of the involved stakeholders.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {178–187},
numpages = {10},
keywords = {automated identification of risks, hazards in production environments, model-driven commissioning, safety certification},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3336294.3336311,
author = {T\"{e}rnava, Xhevahire and Mortara, Johann and Collet, Philippe},
title = {Identifying and Visualizing Variability in Object-Oriented Variability-Rich Systems},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336311},
doi = {10.1145/3336294.3336311},
abstract = {In many variability-intensive systems, variability is implemented in code units provided by a host language, such as classes or functions, which do not align well with the domain features. Annotating or creating an orthogonal decomposition of code in terms of features implies extra effort, as well as massive and cumbersome refactoring activities. In this paper, we introduce an approach for identifying and visualizing the variability implementation places within the main decomposition structure of object-oriented code assets in a single variability-rich system. First, we propose to use symmetry, as a common property of some main implementation techniques, such as inheritance or overloading, to identify uniformly these places. We study symmetry in different constructs (e.g., classes), techniques (e.g., subtyping, overloading) and design patterns (e.g., strategy, factory), and we also show how we can use such symmetries to find variation points with variants. We then report on the implementation and application of a toolchain, symfinder, which automatically identifies and visualizes places with symmetry. The publicly available application to several large open-source systems shows that symfinder can help in characterizing code bases that are variability-rich or not, as well as in discerning zones of interest w.r.t. variability.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {231–243},
numpages = {13},
keywords = {identifying software variability, object-oriented variability-rich systems, software product line engineering, tool support for understanding software variability, visualizing software variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3503229.3547059,
author = {Wittler, Jan Willem and K\"{u}hn, Thomas and Reussner, Ralf},
title = {Towards an integrated approach for managing the variability and evolution of both software and hardware components},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547059},
doi = {10.1145/3503229.3547059},
abstract = {Although the development of mass-customized products has been successfully applied to both hardware and software, companies struggle managing the variability and evolution of software-intensive products within a coherent product engineering approach. While the variability and evolution of software alone is manageable, managing both software and hardware within one product line is a complex task and requires an integrated approach. Moreover, as the release cycle for hardware is longer than for software, a product line of hardware and software is usually developed in generations. While one generation is validated and produced, the next generation is already planned and designed, reusing both software and hardware of previous generations. Thus, the different generations and artifacts shared between them must be managed together. Finally, when approaches directly assign software to hardware, managing their evolution becomes increasingly complex. Evolved resource demands may be missed, exhausting the resources provided by the hardware, possibly leading to degraded or faulty functionality. To remedy this, we refine the Unified Conceptual Model to our Variability Model for both Software and Hardware capturing the notion of product line generations, versions and variants of both software and hardware components, as well as resource demands of software on hardware. This is the first step towards the development of an integrated product engineering approach for managing the variability and evolution of software-intensive products.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {94–98},
numpages = {5},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3503229.3547025,
author = {Horcas, Jose M. and Galindo, Jose A. and Pinto, M\'{o}nica and Fuentes, Lidia and Benavides, David},
title = {FM fact label: a configurable and interactive visualization of feature model characterizations},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547025},
doi = {10.1145/3503229.3547025},
abstract = {Recognizing specific characteristics of feature models (FM) can be challenging due to the different nature and domains of the models. There are several metrics to characterize FMs. However, there is no standard way to visualize and identify the properties that make an FM unique and distinguishable. We propose FM Fact Label as a tool to visualize an FM characterization based on its metadata, structural measures, and analytical metrics. Although existing tools can provide a visualization of the FM and report some metrics, the feature diagram of large-scale FMs becomes ineffective to take an overall shape of the FM properties. Moreover, the reported metrics are often embedded in the tool user interface, preventing further analysis. FM Fact Label is a standalone web-based tool that provides a configurable and interactive visualization of FM characterizations that can be exported to several formats. Our contribution becomes important because the Universal Variability Language (UVL) is starting to gain attraction in the software product line community as a unified textual language to specify FMs and share knowledge. With this contribution, we help to advance the UVL ecosystem one step forward while providing a common representation for the results of existing analysis tools.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {42–45},
numpages = {4},
keywords = {characterization, feature model, metrics, variability, visualization},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3461001.3472730,
author = {Mannion, Mike and Kaindl, Hermann},
title = {Reuse for mass personalisation through feature models and similarities},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3472730},
doi = {10.1145/3461001.3472730},
abstract = {This tutorial explores the impact of the socio-economic trends of customization and personalization on software reuse and describes a product similarity evaluation process to support the management of a product line.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {211},
numpages = {1},
keywords = {mass personalization, product lines, software reuse},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3503229.3547068,
author = {Plappert, Stefan and Teves, Simon and \"{O}zt\"{u}rk, Mevali and Gembarski, Paul Christoph},
title = {Constraint solver for a fixture design: results of a student case study},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547068},
doi = {10.1145/3503229.3547068},
abstract = {For teaching students the skills of programming and usage of knowledge-based engineering systems, we conduct student projects in a lecture in which they independently represent a configuration solution space and resolve it using a constraint solver. For this purpose, the lecture is conducted in a flipped classroom concept to not only teach the students the theoretical basics but to enable them to independently formulate and integrate design problems, which can be abstracted as configuration problems, so that they develop a sustainable competence through learning-by-doing. The configuration problem of the student case study represented here is the positioning of a cast part for manufacturing, where the positioning is done via three subassemblies consisting of parts from a fixture toolbox. For this purpose, a development environment written in the Python programming language was set up, which uses an external Excel database as a knowledge base to provide the sizes of the fixture elements. Through a graphical user interface, the designer can specify how the fixture should be used so that the constraint solver can find a solution. If there are several possible solutions, an optimization loop is executed so that the designer can be given a clear recommendation. An interface to the CAD program Autodesk Inventor offers the possibility to build the fixture assembly of the selected solution from parameterized CAD models of the components by linking their custom coordinate systems. To reduce computing time, a case base is also provided for configurations that have already been created, so that existing subassemblies can be used if the same or similar configuration problem arises.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {237–244},
numpages = {8},
keywords = {case study, constraint solver, fixture design, knowledge-based engineering system, product configuration},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3233027.3236404,
author = {Gazzillo, Paul and Koc, Ugur and Nguyen, ThanhVu and Wei, Shiyi},
title = {Localizing configurations in highly-configurable systems},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236404},
doi = {10.1145/3233027.3236404},
abstract = {The complexity of configurable systems has grown immensely, and it is only getting more complex. Such systems are a challenge for software testing and maintenance, because bugs and other defects can and do appear in any configuration. One common requirement for many development tasks is to identify the configurations that lead to a given defect or some other program behavior. We distill this requirement down to a challenge question: given a program location in a source file, what are valid configurations that include the location? The key obstacle is scalability. When there are thousands of configuration options, enumerating all combinations is exponential and infeasible. We provide a set of target programs of increasing difficulty and variations on the challenge question so that submitters of all experience levels can try out solutions. Our hope is to engage the community and stimulate new and interesting approaches to the problem of analyzing configurations.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {269–273},
numpages = {5},
keywords = {configurations, program analysis, testing, variability},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2934466.2934478,
author = {Galindo, Jos\'{e} A. and Acher, Mathieu and Tirado, Juan Manuel and Vidal, Cristian and Baudry, Benoit and Benavides, David},
title = {Exploiting the enumeration of all feature model configurations: a new perspective with distributed computing},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934478},
doi = {10.1145/2934466.2934478},
abstract = {Feature models are widely used to encode the configurations of a software product line in terms of mandatory, optional and exclusive features as well as propositional constraints over the features. Numerous computationally expensive procedures have been developed to model check, test, configure, debug, or compute relevant information of feature models. In this paper we explore the possible improvement of relying on the enumeration of all configurations when performing automated analysis operations. We tackle the challenge of how to scale the existing enumeration techniques by relying on distributed computing. We show that the use of distributed computing techniques might offer practical solutions to previously unsolvable problems and opens new perspectives for the automated analysis of software product lines.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {74–78},
numpages = {5},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/1147249.1147257,
author = {Cohen, Myra B. and Dwyer, Matthew B. and Shi, Jiangfan},
title = {Coverage and adequacy in software product line testing},
year = {2006},
isbn = {1595934596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1147249.1147257},
doi = {10.1145/1147249.1147257},
abstract = {Software product line modeling has received a great deal of attention for its potential in fostering reuse of software artifacts across development phases. Research on the testing phase, has focused on identifying the potential for reuse of test cases across product line instances. While this offers potential reductions in test development effort for a given product line instance, it does not focus on and leverage the fundamental abstraction that is inherent in software product lines - variability.In this paper, we illustrate how rich software product line modeling notations can be mapped onto an underlying relational model that captures variability in the feasible product line instances. This relational model serves as the semantic basis for defining a family of coverage criteria for testing of a product line. These criteria make it possible to accumulate test coverage information for the product line itself over the course of multiple product line instance development efforts. Cumulative coverage, in turn, enables targeted testing efforts for new product line instances. We describe how combinatorial interaction testing methods can be applied to define test configurations that achieve a desired level of coverage and identify challenges to scaling such methods to large, complex software product lines.},
booktitle = {Proceedings of the ISSTA 2006 Workshop on Role of Software Architecture for Testing and Analysis},
pages = {53–63},
numpages = {11},
location = {Portland, Maine},
series = {ROSATEA '06}
}

@inproceedings{10.1145/3336294.3342358,
author = {M\"{u}ller, Richard and Eisenecker, Ulrich},
title = {A Graph-Based Feature Location Approach Using Set Theory},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342358},
doi = {10.1145/3336294.3342358},
abstract = {The ArgoUML SPL benchmark addresses feature location in Software Product Lines (SPLs), where single features as well as feature combinations and feature negations have to be identified. We present a solution for this challenge using a graph-based approach and set theory. The results are promising. Set theory allows to exactly define which parts of feature locations can be computed and which precision and which recall can be achieved. This has to be complemented by a reliable identification of feature-dependent class and method traces as well as refinements. The application of our solution to one scenario of the benchmark supports this claim.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {88–92},
numpages = {5},
keywords = {ArgoUML, Neo4j, benchmark, cypher, extractive software product line adoption, feature location, graph database, jQAssistant, reverse engineering, set theory, software product lines, static analysis},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3382026.3425772,
author = {Ca\~{n}ete, Angel and Amor, Mercedes and Fuentes, Lidia},
title = {Supporting the evolution of applications deployed on edge-based infrastructures using multi-layer feature models},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3425772},
doi = {10.1145/3382026.3425772},
abstract = {The proliferation of cyber-physical systems has encouraged the emergence of new technologies and paradigms to improve the performance of IoT-based applications. Edge Computing proposes using the nearby devices in the frontier/Edge of the access network for deploying application tasks. However, the functionality of cyberphysical systems, which is usually distributed in several devices and computers, imposes specific requirements on the infrastructure to run properly. The evolution of an application to meet new user requirements and the high diversity of hardware and software technologies in the edge can complicate the deployment of evolved applications.The aim of our approach is to apply Multi Layer Feature Models, which capture the variability of applications and the infrastructure, to support the deployment in edge-based environments of cyber-physical systems applications. This separation can support the evolution of application and infrastructure. Considering that IoT/Edge/Cloud infrastructures are usually shared by many applications, the SPL deployment process has to assure that there will be enough resources for all of them, informing developers about the alternatives of deployment. Prior to its deployment and leaning on the infrastructure feature models, the developer can calculate what is the configuration of minimal set of devices supporting application requirements of the evolved application. In addition, the developer can find which is the application configuration that can be hosted in the current evolved infrastructure.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {79–87},
numpages = {9},
keywords = {Edge Computing, Internet of Things, Multi Layer Feature Models, Software Evolution, Software Product Line},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3382025.3414964,
author = {Hoff, Adrian and Nieke, Michael and Seidl, Christoph and S\ae{}ther, Eirik Halvard and Motzfeldt, Ida Sandberg and Din, Crystal Chang and Yu, Ingrid Chieh and Schaefer, Ina},
title = {Consistency-preserving evolution planning on feature models},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414964},
doi = {10.1145/3382025.3414964},
abstract = {A software product line (SPL) enables large-scale reuse in a family of related software systems through configurable features. SPLs represent a long-term investment so that their ongoing evolution becomes paramount and requires careful planning. While existing approaches enable to create an evolution plan for an SPL on feature-model (FM) level, they assume the plan to be rigid and do not support retroactive changes. In this paper, we present a method that enables to create and retroactively adapt an FM evolution plan while preventing undesired impacts on its structural and logical consistency. This method is founded in structural operational semantics and linear temporal logic. We implement our method using rewriting logic, integrate it within an FM tool suite and perform an evaluation using a collection of existing FM evolution scenarios.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {8},
numpages = {12},
keywords = {feature model evolution, feature models, formal semantics, linear temporal logic, rewriting logic, software evolution, software product lines, structural operational semantics},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3546932.3546991,
author = {Tavassoli, Shaghayegh and Damasceno, Carlos Diego N. and Khosravi, Ramtin and Mousavi, Mohammad Reza},
title = {Adaptive behavioral model learning for software product lines},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3546991},
doi = {10.1145/3546932.3546991},
abstract = {Behavioral models enable the analysis of the functionality of software product lines (SPL), e.g., model checking and model-based testing. Model learning aims to construct behavioral models. Due to the commonalities among the products of an SPL, it is possible to reuse the previously-learned models during the model learning process. In this paper, an adaptive approach, called PL*, for learning the product models of an SPL is presented based on the well-known L* algorithm. In this method, after learning each product, the sequences in the final observation table are stored in a repository which is used to initialize the observation table of the remaining products. The proposed algorithm is evaluated on two open-source SPLs and the learning cost is measured in terms of the number of rounds, resets, and input symbols. The results show that for complex SPLs, the total learning cost of PL* is significantly lower than that of the non-adaptive method in terms of all three metrics. Furthermore, it is observed that the order of learning products affects the efficiency of PL*. We introduce a heuristic to determine an ordering which reduces the total cost of adaptive learning.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {142–153},
numpages = {12},
keywords = {adaptive model learning, automata learning, finite state machines, software product lines},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3579027.3608982,
author = {Lubos, Sebastian and Felfernig, Alexander and Le, Viet-Man and Tran, Thi Ngoc Trang and Benavides, David and Zamudio, Jos\'{e} A. and Garber, Damian},
title = {Analysis Operations On The Run: Feature Model Analysis in Constraint-based Recommender Systems},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608982},
doi = {10.1145/3579027.3608982},
abstract = {The development and maintenance of feature models is often an error-prone activity requiring different types of analysis operations that help developers to restore required feature model properties. Fulfilling such properties helps to assure compliance between feature model and corresponding domain variability properties and -at the same time - helps to increase feature model maintainability. In this paper, we propose a set of additional analysis operations that provide insights regarding potential impacts of applying feature models in constraint-based recommendation scenarios where feature models are used to define user preference spaces. Our proposed analysis operations provide a.o. insights into aspects such as feature restrictiveness and product accessibility when applying a constraint-based recommender system. We analyze usage scenarios of the operations on the basis of an example implementation with a digital camera feature model and discuss open research issues.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {111–116},
numpages = {6},
keywords = {Variability models, analysis operations, constraint solving, constraint-based recommender systems, feature models},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3546932.3547002,
author = {Schulze, Sandro and Kr\"{u}ger, Jacob and W\"{u}nsche, Johannes},
title = {Towards developer support for merging forked test cases},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547002},
doi = {10.1145/3546932.3547002},
abstract = {Developers rely on branching and forking mechanisms of modern versioning systems to evolve and maintain their software systems. As a result, systems often exist in the form of various short-living or even long-living (i.e., clone &amp; own development) variants. Such variants may have to be merged with the main system or other variants, for instance, to propagate features or bug fixes. Within such merging processes, test cases are highly interesting, since they allow to improve the test coverage and hopefully the reliability of the system (e.g., by merging missing tests and bug fixes in test code). However, as all source code, test cases may evolve independently between two or more variants, which makes it non-trivial to decide what changes of the test cases are relevant for the merging. For instance, some test cases in one variant may be irrelevant in another variant (e.g., because the feature shall not be propagated) or may subsume existing test cases. In this paper, we propose a technique that allows for a fine-grained comparison of test cases to support developers in deciding whether and how to merge these. Precisely, inspired by code-clone detection, we use abstract syntax trees to decide on the relations between test cases of different variants. We evaluate the applicability of our technique qualitatively on five open-source systems written in Java (e.g., JUnit 5, Guava). Our insights into the merge potential of 50 pull requests with test cases from these systems indicate that our technique can support the comprehension of differences in variants' test cases, and also highlight future research opportunities.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {131–141},
numpages = {11},
keywords = {feature forks, merging, test cases, variant-rich systems},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3546932.3547073,
author = {Mortara, Johann and Collet, Philippe and Pinna-Dery, Anne-Marie},
title = {Customizable visualization of quality metrics for object-oriented variability implementations},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547073},
doi = {10.1145/3546932.3547073},
abstract = {Many large-scale software systems intensively implement variability to reuse software and speed up development. Such mechanisms, however, bring additional complexity, which eventually leads to technical debt, threatening the software quality, and hampering maintenance and evolution. This is especially the case for variability-rich object-oriented (OO) systems that implement variability in a single codebase. They heavily rely on existing OO mechanisms to implement their variability, making them especially prone to variability debt at the code level. In this paper, we propose VariMetrics, an extension of a visualization relying on the city metaphor to reveal such zones of indebted OO variability implementations. VariMetrics extends the VariCity visualization and displays standard OO quality metrics, such as code duplication, code complexity, or test coverage, as additional visual properties on the buildings representing classes. Extended configuration options allow the user to choose and combine quality metrics, uncovering the critical zones of OO variability implementations. We evaluate VariMetrics both by reporting on the exposed quality-critical zones found on multiple large open-source projects, and by correcting the reported issues in such zones of one project, showing an improvement in quality.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {43–54},
numpages = {12},
keywords = {object-oriented systems, quality metrics, reverse-engineering, software variability, software visualization, technical debt},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3382025.3414955,
author = {Ananieva, Sofia and Greiner, Sandra and K\"{u}hn, Thomas and Kr\"{u}ger, Jacob and Linsbauer, Lukas and Gr\"{u}ner, Sten and Kehrer, Timo and Klare, Heiko and Koziolek, Anne and L\"{o}nn, Henrik and Krieter, Sebastian and Seidl, Christoph and Ramesh, S. and Reussner, Ralf and Westfechtel, Bernhard},
title = {A conceptual model for unifying variability in space and time},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414955},
doi = {10.1145/3382025.3414955},
abstract = {Software engineering faces the challenge of developing and maintaining systems that are highly variable in space (concurrent variations of the system at a single point in time) and time (sequential variations of the system due to its evolution). Recent research aims to address this need by managing variability in space and time simultaneously. However, such research often relies on nonuniform terminologies and a varying understanding of concepts, as it originates from different communities: software product-line engineering and software configuration management. These issues complicate the communication and comprehension of the concepts involved, impeding the development of techniques to unify variability in space and time. To tackle this problem, we performed an iterative, expert-driven analysis of existing tools to derive the first conceptual model that integrates and unifies terminologies and concepts of both dimensions of variability. In this paper, we present the unification process of concepts for variability in space and time, and the resulting conceptual model itself. We show that the conceptual model achieves high coverage and that its concepts are of appropriate granularity with respect to the tools for managing variability in space, time, or both that we considered. The conceptual model provides a well-defined, uniform terminology that empowers researchers and developers to compare their work, clarifies communication, and prevents redundant developments.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {15},
numpages = {12},
keywords = {product lines, revision management, variability, version control},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3461002.3473944,
author = {Ballesteros, Joaqu\'{\i}n and Fuentes, Lidia},
title = {Transfer learning for multiobjective optimization algorithms supporting dynamic software product lines},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473944},
doi = {10.1145/3461002.3473944},
abstract = {Dynamic Software Product Lines (DSPLs) are a well-accepted approach for self-adapting Cyber-Physical Systems (CPSs) at run-time. The DSPL approaches make decisions supported by performance models, which capture system features' contribution to one or more optimization goals. Combining performance models with Multi-Objectives Evolutionary Algorithms (MOEAs) as decision-making mechanisms is common in DSPLs. However, MOEAs algorithms start solving the optimization problem from a randomly selected population, not finding good configurations fast enough after a context change, requiring too many resources so scarce in CPSs. Also, the DSPL engineer must deal with the hardware and software particularities of the target platform in each CPS deployment. And although each system instantiation has to solve a similar optimization problem of the DSPL, it does not take advantage of experiences gained in similar CPS. Transfer learning aims at improving the efficiency of systems by sharing the previously acquired knowledge and applying it to similar systems. In this work, we analyze the benefits of transfer learning in the context of DSPL and MOEAs testing on 8 feature models with synthetic performance models. Results are good enough, showing that transfer learning solutions dominate up to 71% of the non-transfer learning ones for similar DSPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {51–59},
numpages = {9},
keywords = {cyber-physical systems, dynamic software product lines, multiobjective optimization algorithms, self-adaptation, transfer learning},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/2420942.2420944,
author = {Olaechea, Rafael and Stewart, Steven and Czarnecki, Krzysztof and Rayside, Derek},
title = {Modelling and multi-objective optimization of quality attributes in variability-rich software},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420944},
doi = {10.1145/2420942.2420944},
abstract = {Variability-rich software, such as software product lines, offers optional and alternative features to accommodate varying needs of users. Designers of variability-rich software face the challenge of reasoning about the impact of selecting such features on the quality attributes of the resulting software variant. Attributed feature models have been proposed to model such features and their impact on quality attributes, but existing variability modelling languages and tools have limited or no support for such models and the complex multi-objective optimization problem that arises. This paper presents ClaferMoo, a language and tool that addresses these shortcomings. ClaferMoo uses type inheritance to modularize the attribution of features in feature models and allows specifying multiple optimization goals. We evaluate an implementation of the language on a set of attributed feature models from the literature, showing that the optimization infrastructure can handle small-scale feature models with about a dozen features within seconds.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {2},
numpages = {6},
keywords = {multi-objective optimization, software product lines},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@inproceedings{10.1145/3503229.3547066,
author = {Mortara, Johann and Collet, Philippe and Pinna-Dery, Anne-Marie},
title = {IDE-assisted visualization of indebted OO variability implementations},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547066},
doi = {10.1145/3503229.3547066},
abstract = {Object-Oriented (OO) variability-rich software systems often implement their variability in a single codebase, using the mechanisms provided by the host language (i.e., inheritance, overloading, design patterns). This variability is not documented and buried deep down in the code, thus impeding its identification and making it especially prone to variability debt at the code level. While this kind of variability implementation can now be detected, visualization support such as VariCity helps architects and developers understand the implemented variability using a city metaphor. In this paper, we demonstrate VariMetrics-IDE, an extension of VariCity that allows to visualize multiple quality metrics (e.g., code complexity, test coverage) together with the variability implementations, while supporting navigation between the source code and the visualization in an IDE. This extension thus facilitates the identification of zones of variability implementations with variability debt.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {74–77},
numpages = {4},
keywords = {object-oriented systems, quality metrics, reverse-engineering, software variability, software visualization, technical debt},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/2934466.2934474,
author = {Myll\"{a}rniemi, Varvana and Raatikainen, Mikko and Savolainen, Juha and M\"{a}nnist\"{o}, Tomi},
title = {Purposeful performance variability in software product lines: a comparison of two case studies},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934474},
doi = {10.1145/2934466.2934474},
abstract = {Within software product lines, customers may have different quality needs. To produce products with purposefully different quality attributes, several challenges must be addressed. First, one must be able to distinguish product quality attributes to the customers in a meaningful way. Second, one must create the desired quality attribute differences during product-line architecture design and derivation. To study how performance is varied purposefully in software product lines, we conducted a comparison and re-analysis of two industrial case studies in the telecommunication and mobile game domains. The results show that performance variants must be communicated to the customer in a way that links to customer value and her role. When performance or its adaptation are crucial for the customer, performance differences must be explicitly "designed in" with software or hardware means. Due to the emergent nature of performance, it is important to test performance and manage how other variability affects performance.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {144–153},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3233027.3233048,
author = {Hayashi, Kengo and Aoyama, Mikio},
title = {A multiple product line development method based on variability structure analysis},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233048},
doi = {10.1145/3233027.3233048},
abstract = {This article proposes a multiple product line development method based on variability structure analysis. In product line development, the problem area is divided into the domain engineering and application engineering for delivering diverse products. Now, the development of automotive software requires to meet both agility and extreme diversity, which is a big challenge. We developed a structural analysis method of variability for multiple product lines using an extended model of OVM (Orthogonal Variability Model). Together with the variability analysis method, we propose an agile application development method to refine development items according to variability dependency based on the analysis, and develop them incrementally. We applied the proposed method to the development of the multiple product lines of automotive software systems, and demonstrated to reduce the volatility of the test efforts and usage of the test environment, and higher velocity and better manageability of the value stream.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {160–169},
numpages = {10},
keywords = {agile development, automotive software, multiple product lines, software product line, variability analysis},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3461002.3473070,
author = {Acher, Mathieu and Perrouin, Gilles and Cordy, Maxime},
title = {BURST: a benchmarking platform for uniform random sampling techniques},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473070},
doi = {10.1145/3461002.3473070},
abstract = {We present BURST, a benchmarking platform for uniform random sampling techniques. With BURST, researchers have a flexible, controlled environment in which they can evaluate the scalability and uniformity of their sampling. BURST comes with an extensive --- and extensible --- benchmark dataset comprising 128 feature models, including challenging, real-world models of the Linux kernel. BURST takes as inputs a sampling tool, a set of feature models and a sampling budget. It automatically translates any feature model of the set in DIMACS and invokes the sampling tool to generate the budgeted number of samples. To evaluate the scalability of the sampling tool, BURST measures the time the tool needs to produce the requested sample. To evaluate the uniformity of the produced sample, BURST integrates the state-of-the-art and proven statistical test Barbarik. We envision BURST to become the starting point of a standardisation initiative of sampling tool evaluation. Given the huge interest of research for sampling algorithms and tools, this initiative would have the potential to reach and crosscut multiple research communities including AI, ML, SAT and SPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {36–40},
numpages = {5},
keywords = {SAT, benchmark, configurable systems, sampling, software product lines, variability model},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3382026.3431252,
author = {Michelon, Gabriela Karoline},
title = {Evolving System Families in Space and Time},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431252},
doi = {10.1145/3382026.3431252},
abstract = {Managing the evolution of system families in space and time, i.e., system variants and their revisions is still an open challenge. The software product line (SPL) approach can support the management of product variants in space by reusing a common set of features. However, feature changes over time are often necessary due to adaptations and/or bug fixes, leading to different product versions. Such changes are commonly tracked in version control systems (VCSs). However, VCSs only deal with the change history of source code, and, even though their branching mechanisms allow to develop features in isolation, VCS does not allow propagating changes across variants. Variation control systems have been developed to support more fine-grained management of variants and to allow tracking of changes at the level of files or features. However, these systems are also limited regarding the types and granularity of artifacts. Also, they are cognitively very demanding with increasing numbers of revisions and variants. Furthermore, propagating specific changes over variants of a system is still a complex task that also depends on the variability-aware change impacts. Based on these existing limitations, the goal of this doctoral work is to investigate and define a flexible and unified approach to allow an easy and scalable evolution of SPLs in space and time. The expected contributions will aid the management of SPL products and support engineers to reason about the potential impact of changes during SPL evolution. To evaluate the approach, we plan to conduct case studies with real-world SPLs.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {104–111},
numpages = {8},
keywords = {feature-oriented software development, software evolution, software product lines, version control systems},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3307630.3342413,
author = {Arcaini, Paolo and Gargantini, Angelo and Radavelli, Marco},
title = {A Process for Fault-Driven Repair of Constraints Among Features},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342413},
doi = {10.1145/3307630.3342413},
abstract = {The variability of a Software Product Line is usually both described in the problem space (by using a variability model) and in the solution space (i.e., the system implementation). If the two spaces are not aligned, wrong decisions can be done regarding the system configuration. In this work, we consider the case in which the variability model is not aligned with the solution space, and we propose an approach to automatically repair (possibly) faulty constraints in variability models. The approach takes as input a variability model and a set of combinations of features that trigger conformance faults between the model and the real system, and produces the repaired set of constraints as output. The approach consists of three major phases. First, it generates a test suite and identifies the condition triggering the faults. Then, it modifies the constraints of the variability model according to the type of faults. Lastly, it uses a logic minimization method to simplify the modified constraints. We evaluate the process on variability models of 7 applications of various sizes. An empirical analysis on these models shows that our approach can effectively repair constraints among features in an automated way.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {73–81},
numpages = {9},
keywords = {automatic repair, fault, system evolution, variability model},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2000259.2000264,
author = {Kavimandan, Amogh and Gokhale, Aniruddha and Karsai, Gabor and Gray, Jeff},
title = {Managing the quality of software product line architectures through reusable model transformations},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000264},
doi = {10.1145/2000259.2000264},
abstract = {In model-driven engineering of applications, the quality of the software architecture is realized and preserved in the successive stages of its lifecycle through model transformations. However, limited support for reuse in contemporary model transformation techniques forces developers of product line architectures to reinvent transformation rules for every variant of the product line, which can adversely impact developer productivity and in turn degrade the quality of the resulting software architecture for the variant. To overcome these challenges, this paper presents the MTS (Model-transformation Templatization and Specialization generative transformation process, which promotes reuse in model transformations through parameterization and specialization of transformation rules. MTS defines two higher order transformations to capture the variability in transformation rules and to specialize them across product variants. The core idea behind MTS is realized within a graphical model transformation tool in a way that is minimally intrusive to the underlying tool's implementation. The paper uses two product line case studies to evaluate MTS in terms of reduction in efforts to define model transformation rules as new variants are added to the product line, and the overhead in executing the higher order transformations. These metrics provide an indirect measure of how potential degradation in the quality of software architectures of product lines caused due to lack of reuse can be alleviated by MTS.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {13–22},
numpages = {10},
keywords = {model transformations, reuse, software quality},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@inproceedings{10.1145/1176887.1176897,
author = {Yoshimura, Kentaro and Ganesan, Dharmalingam and Muthig, Dirk},
title = {Defining a strategy to introduce a software product line using existing embedded systems},
year = {2006},
isbn = {1595935428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1176887.1176897},
doi = {10.1145/1176887.1176897},
abstract = {Engine Control Systems (ECS) for automobiles have numerous variants for many manufactures and different markets. To improve development efficiency, exploiting ECS commonalities and predicting their variability are mandatory. The concept of software product line engineering meets the business background of ECS. However, we should carefully investigate the expected technical, economical, and organizational effects of introducing this strategy into existing products.This paper explains an approach for assessing the potential of merging existing embedded software into a product line approach. The definition of an economically useful product line approach requires two things: analyzing return on investment (ROI) expectations of a product line and understanding the effort required for building reusable assets. We did a clone analysis to provide the basis for effort estimation for merge potential assessment of existing variants. We also report on a case study with ECS. We package the lessons learned and open issues that arose during the case study.},
booktitle = {Proceedings of the 6th ACM &amp; IEEE International Conference on Embedded Software},
pages = {63–72},
numpages = {10},
keywords = {clone detection and classification, economics, engine control systems, reverse rngineering, software, software product line},
location = {Seoul, Korea},
series = {EMSOFT '06}
}

@inproceedings{10.1145/3109729.3109736,
author = {Kr\"{u}ger, Jacob and Nell, Louis and Fenske, Wolfram and Saake, Gunter and Leich, Thomas},
title = {Finding Lost Features in Cloned Systems},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109736},
doi = {10.1145/3109729.3109736},
abstract = {Copying and adapting a system, also known as clone-and-own, is a common reuse approach that requires little initial effort. However, the drawbacks of clones are increasing maintenance costs as bug fixes and updates must be propagated. To reduce these costs, migrating cloned legacy systems towards a software product line promises to enable systematic reuse and customization. For both, managing and migrating cloned systems, it remains a challenge to identify and map features in the systems. In this paper, we i) propose a semi-automatic process to identify and map features between legacy systems, ii) suggest a corresponding visualization approach, and iii) assess our process on a case study. The results indicate that our process is suitable to identify features and present commonalities and variability in cloned systems. Our process can be used to enable traceability, prepare refactorings, and extract software product lines.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {65–72},
numpages = {8},
keywords = {Software product line, code clone detection, extractive approach, feature location, legacy system, reverse engineering},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1109/ASE56229.2023.00091,
author = {Xia, Yuanjie and Ding, Zishuo and Shang, Weiyi},
title = {CoMSA: A Modeling-Driven Sampling Approach for Configuration Performance Testing},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00091},
doi = {10.1109/ASE56229.2023.00091},
abstract = {Highly configurable systems enable customers to flexibly configure the systems in diverse deployment environments. The flexibility of configurations also poses challenges for performance testing. On one hand, there exist a massive number of possible configurations; while on the other hand, the time and resources are limited for performance testing, which is already a costly process during software development. Modeling the performance of configurations is one of the solutions to reduce the cost of configuration performance testing. Although prior research proposes various modeling and sampling techniques to build configuration performance models, the sampling approaches used in the model typically do not consider the accuracy of the performance models, leading to potential suboptimal performance modeling results in practice. In this paper, we present a modeling-driven sampling approach (CoMSA) to improve the performance modeling of highly configurable systems. The intuition of CoMSA is to select samples based on their uncertainties to the performance models. In other words, the configurations that have the more uncertain performance prediction results by the performance models are more likely to be selected as further training samples to improve the model. CoMSA is designed by considering both scenarios where 1) the software projects do not have historical performance testing results (cold start) and 2) there exist historical performance testing results (warm start). We evaluate the performance of our approach in four subjects, namely LRZIP, LLVM, x264, and SQLite. Through the evaluation result, we can conclude that our sampling approaches could highly enhance the accuracy of the prediction models and the efficiency of configuration performance testing compared to other baseline sampling approaches.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1352–1363},
numpages = {12},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3106195.3106219,
author = {Gregg, Susan P. and Albert, Denise M. and Clements, Paul},
title = {Product Line Engineering on the Right Side of the "V"},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106219},
doi = {10.1145/3106195.3106219},
abstract = {Product line engineering (PLE) is well-known for the savings it brings to organizations. This paper shows how a very large, in-service systems and software product line is achieving PLE-based savings in their verification and validation phase of development. The paper addresses how to achieve the sharing across product variants while the products being tested are evolving over time. Additionally, we will give a pragmatic set of decision criteria to help answer the longstanding issue in PLE-based testing of whether to test on the domain side or the application (product) side of the product derivation process.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {165–174},
numpages = {10},
keywords = {AEGIS Combat System, PLE factory, Product line engineering, bill-of-features, feature modeling, feature profiles, product configurator, product portfolio, second generation product line engineering, software product lines, variation points},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3461002.3473943,
author = {Mortara, Johann and T\"{e}rnava, Xhevahire and Collet, Philippe and Pinna-Dery, Anne-Marie},
title = {Extending the identification of object-oriented variability implementations using usage relationships},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473943},
doi = {10.1145/3461002.3473943},
abstract = {Many variability-rich object-oriented systems rely on multiple traditional techniques (inheritance, patterns) to implement their variability in a single codebase. These variability implementation places are neither explicit nor documented, hampering their detection and variability comprehension. Based on the identification of symmetry property in seven implementation techniques, a first approach was proposed with symfinder to automatically identify and display the variability of a system in a graph-based visualization structured by inheritance. However, composition, or more generally the usage relationship, is extensively used to implement the variability in object-oriented systems, and without this information, comprehending the large amount of variability identified by symfinder is not trivial. In this paper, we present symfinder-2, an extension of the former approach that incorporates the usage relationships to better identify potential variability implementations. We provide two ways to mark classes as entry points, user-defined and automatic, so that the visualization is filtered and enables users to have a better focus when they identify variability. We also report on the evaluation of this extension to ten open-source Java-based systems.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {91–98},
numpages = {8},
keywords = {symfinder, variability identification, variability visualization, variability-rich object-oriented software systems},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3555776.3578613,
author = {T\"{e}rnava, Xhevahire and Acher, Mathieu and Combemale, Benoit},
title = {Specialization of Run-time Configuration Space at Compile-time: An Exploratory Study},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3578613},
doi = {10.1145/3555776.3578613},
abstract = {Numerous software systems are highly configurable through runtime options (e.g., command-line parameters). Users can tune some of the options to meet various functional and non-functional requirements such as footprint, security, or execution time. However, some options are never set for a given system instance, and their values remain the same whatever the use cases of the system. Herein, we design a controlled experiment in which the system's run-time configuration space can be specialized at compile-time and combinations of options can be removed on demand. We perform an in-depth study of the well-known x264 video encoder and quantify the effects of its specialization to its non-functional properties, namely on binary size, attack surface, and performance while ensuring its validity. Our exploratory study suggests that the configurable specialization of a system has statistically significant benefits on most of its analysed non-functional properties, which benefits depend on the number of the debloated options. While our empirical results and insights show the importance of removing code related to unused run-time options to improve software systems, an open challenge is to further automate the specialization process.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1459–1468},
numpages = {10},
keywords = {program specialization, performance, debloating, unused variability},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/3336294.3336297,
author = {Munoz, Daniel-Jesus and Oh, Jeho and Pinto, M\'{o}nica and Fuentes, Lidia and Batory, Don},
title = {Uniform Random Sampling Product Configurations of Feature Models That Have Numerical Features},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336297},
doi = {10.1145/3336294.3336297},
abstract = {Analyses of Software Product Lines (SPLs) rely on automated solvers to navigate complex dependencies among features and find legal configurations. Often these analyses do not support numerical features with constraints because propositional formulas use only Boolean variables. Some automated solvers can represent numerical features natively, but are limited in their ability to count and Uniform Random Sample (URS) configurations, which are key operations to derive unbiased statistics on configuration spaces.Bit-blasting is a technique to encode numerical constraints as propositional formulas. We use bit-blasting to encode Boolean and numerical constraints so that we can exploit existing #SAT solvers to count and URS configurations. Compared to state-of-art Satisfiability Modulo Theory and Constraint Programming solvers, our approach has two advantages: 1) faster and more scalable configuration counting and 2) reliable URS of SPL configurations. We also show that our work can be used to extend prior SAT-based SPL analyses to support numerical features and constraints.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {289–301},
numpages = {13},
keywords = {bit-blasting, feature model, model counting, numerical features, propositional formula, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3503229.3547061,
author = {Ghofrani, Javad and Heravi, Paria and Babaei, Kambiz A. and Soorati, Mohammad D.},
title = {Trust challenges in reusing open source software: an interview-based initial study},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547061},
doi = {10.1145/3503229.3547061},
abstract = {Open source projects play a significant role in software production. Most of the software projects reuse and build upon the existing open source projects and libraries. While reusing is a time and cost saving strategy, some of the key factors are often neglected that create vulnerability in the software system. We look beyond the static code analysis and dependency chain tracing to prevent vulnerabilities at the human factors level. Literature lacks a comprehensive study of the human factors perspective to the issue of trust in reusing open source projects. We performed an interview-based initial study with software developers to get an understanding of the trust issue and limitations among the practitioners. We outline some of the key trust issues in this paper and layout the first steps towards a trustworthy reuse of software.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {110–116},
numpages = {7},
keywords = {empirical study, open source software, package dependency, reusability, systematic reuse, trust},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3383219.3383229,
author = {Li, Yang and Schulze, Sandro and Xu, Jiahua},
title = {Feature Terms Prediction: A Feasible Way to Indicate the Notion of Features in Software Product Line},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383229},
doi = {10.1145/3383219.3383229},
abstract = {In Software Product Lines (SPL), feature extraction from software requirements specifications has been subject to intense research in order to assist domain analysis in a time-saving way. Although various approaches are proposed to extract features, there still exists a gap to achieve the complete view of features, that is, how to figure out the intention of a feature. Feature terms as the smallest units in a feature can be regarded as vital indicators for describing a feature. Automated feature term extraction can provide key information regarding the intention of a feature, which improves the efficiency of domain analysis. In this paper, we propose an approach to train prediction models by using machine learning techniques to identify feature terms. To this end, we extract candidate terms from requirement specifications in one domain and take six attributes of each term into account to create a labeled dataset. Subsequently, we apply seven commonly used machine algorithms to train prediction models on the labeled dataset. We then use these prediction models to predict feature terms from the requirements belonging to the other two different domains. Our results show that (1) feature terms can be predicted with high accuracy of ≈ 90% within a domain (2) prediction across domains leads to a decreased but still good accuracy (≈ 80%), and (3) machine learning algorithms perform differently.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {90–99},
numpages = {10},
keywords = {Feature Extraction, Feature Terms Identification, Requirement Documents, Software Product Lines},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1145/1297846.1297914,
author = {Krueger, Charles W.},
title = {Leveraging integrated model-driven development and software product line development technologies},
year = {2007},
isbn = {9781595938657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1297846.1297914},
doi = {10.1145/1297846.1297914},
abstract = {As emerging Software Product Line (SPL) technologies have evolved, Model-Driven Development (MDD) has remained an under-served part of the SPL portfolio development lifecycle, making it difficult to simultaneously leverage the benefits of both practices. The Telelogic Rhapsody®/BigLever Gears™ Bridge is the industry's first solution to provide fully integrated MDD and SPL technologies. With the Bridge's innovative capabilities, you can achieve new levels of efficiency by utilizing: (1) Rhapsody MDD models, rather than working with conventional source code, and (2) Gears' SPL consolidation, first-class model variation points, and automated production capabilities -- rather than creating "clone-and-own" copies of MDD models for each product or building "one-size-fits-all" models for all products. This increased efficiency enables you to deliver more new products and features faster, while reducing the development effort and optimizing product quality.},
booktitle = {Companion to the 22nd ACM SIGPLAN Conference on Object-Oriented Programming Systems and Applications Companion},
pages = {836–837},
numpages = {2},
keywords = {model-driven development, software product lines},
location = {Montreal, Quebec, Canada},
series = {OOPSLA '07}
}

@inproceedings{10.1145/3503229.3547048,
author = {Prikler, Liliana Marie and Wotawa, Franz},
title = {Challenges of testing self-adaptive systems},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547048},
doi = {10.1145/3503229.3547048},
abstract = {Self-adaptive systems can change their behavior due to internal or external issues detected during operation. Such systems should be able to change their internal structure or functionality to cope with broken motors or changes in the infrastructure. Assuring that the adaptations taken during operation do not impact the desired behavior or functionality of the system is of uttermost interest. In this paper, we contribute to the corresponding quality assurance challenge. In particular, we focus on a specific class of self-adaptive systems utilizing health states for computing repair actions. We discuss the requirements of testing methodologies for such systems and raise relevant research questions.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {224–228},
numpages = {5},
keywords = {quality assurance, self-adaptive systems, self-healing, testing},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3233027.3233049,
author = {Horcas, Jose-Miguel and Corti\~{n}as, Alejandro and Fuentes, Lidia and Luaces, Miguel R.},
title = {Integrating the common variability language with multilanguage annotations for web engineering},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233049},
doi = {10.1145/3233027.3233049},
abstract = {Web applications development involves managing a high diversity of files and resources like code, pages or style sheets, implemented in different languages. To deal with the automatic generation of custom-made configurations of web applications, industry usually adopts annotation-based approaches even though the majority of studies encourage the use of composition-based approaches to implement Software Product Lines. Recent work tries to combine both approaches to get the complementary benefits. However, technological companies are reticent to adopt new development paradigms such as feature-oriented programming or aspect-oriented programming. Moreover, it is extremely difficult, or even impossible, to apply these programming models to web applications, mainly because of their multilingual nature, since their development involves multiple types of source code (Java, Groovy, JavaScript), templates (HTML, Markdown, XML), style sheet files (CSS and its variants, such as SCSS), and other files (JSON, YML, shell scripts). We propose to use the Common Variability Language as a composition-based approach and integrate annotations to manage fine grained variability of a Software Product Line for web applications. In this paper, we (i) show that existing composition and annotation-based approaches, including some well-known combinations, are not appropriate to model and implement the variability of web applications; and (ii) present a combined approach that effectively integrates annotations into a composition-based approach for web applications. We implement our approach and show its applicability with an industrial real-world system.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {196–207},
numpages = {12},
keywords = {CVL, SPL, annotations, automation, composition, variability, web engineering},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3382025.3414962,
author = {Chrszon, Philipp and Baier, Christel and Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha},
title = {From features to roles},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414962},
doi = {10.1145/3382025.3414962},
abstract = {The detection of interactions is a challenging task present in almost all stages of software development. In feature-oriented system design, this task is mainly investigated for interactions of features within a single system, detected by their emergent behaviors. We propose a formalism to describe interactions in hierarchies of feature-oriented systems (hierarchical interactions) and the actual situations where features interact (active interplays). Based on the observation that such interactions are also crucial in role-based systems, we introduce a compositional modeling framework based on concepts and notions of roles, comprising role-based automata (RBAs). To describe RBAs, we present a modeling language that is close to the input language of the probabilistic model checker Prism. To exemplify the use of RBAs, we implemented a tool that translates RBA models into Prism and thus enables the formal analysis of functional and non-functional properties including system dynamics, contextual changes, and interactions. We carry out two case studies as a proof of concept of such analyses: First, a peer-to-peer protocol case study illustrates how undesired hierarchical interactions can be discovered automatically. Second, a case study on a self-adaptive production cell demonstrates how undesired interactions influence quality-of-service measures such as reliability and throughput.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {19},
numpages = {11},
keywords = {feature-oriented systems, formal methods, roles, verification},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2648511.2648528,
author = {Barreiros, Jorge and Moreira, Ana},
title = {A cover-based approach for configuration repair},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648528},
doi = {10.1145/2648511.2648528},
abstract = {Feature models are often used to describe variability and commonality in Software Product Lines, specifying admissible configurations of valid products. However, invalid configurations may arise in some scenarios. These include feature model evolution that invalidates pre-existing products or collaborative configuration by multiple stakeholders with conflicting goals, among others. This problem has been acknowledged in the literature and some techniques for configuration repair have already been proposed. However, common optimization criteria such as proximity between original and repaired configurations can result in a significant number of alternative repair possibilities, easily attaining thousands of alternatives for models of practical dimension. Consequently, rather than just efficiently providing an exhaustive list of possibilities, an approach that specifically addresses this issue should be able to offer the user a manageable and comprehensible view of the configuration problems and potential repair options. We offer a novel approach for configuration repair, based on partitioning and cover analysis, with high performance and generating high quality solutions, which allows efficient identification and presentation of multiple competing repairs.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {157–166},
numpages = {10},
keywords = {configuration, configuration diagnosis, configuration repair, feature modeling, software product lines},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3106195.3106221,
author = {Hayashi, Kengo and Aoyama, Mikio and Kobata, Keiji},
title = {Agile Tames Product Line Variability: An Agile Development Method for Multiple Product Lines of Automotive Software Systems},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106221},
doi = {10.1145/3106195.3106221},
abstract = {This article proposes an agile development method and its management method for multiple product lines of automotive software systems. In product line development, the problem area is divided into the domain engineering and application engineering for delivering diverse products. Now, the development of automotive software requires agility and extreme diversity. Conventional simple product line model could not accommodate the requirements. Therefore, we developed an agile development method for automotive multiple product lines. First, we propose an agile development method for multiple product lines by iteratively reusing process assets in application engineering. To manage the diversity of the products derived from the multiple product lines, we propose a management method which integrates the portfolio management and product development management into the agile development method. We applied the proposed method to the development of multiple product lines for our automotive software systems, and demonstrated a reduction of the cost and complexity of the multiple product lines, and improvement of on-time delivery.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {180–189},
numpages = {10},
keywords = {agile development, automotive software, multiple product lines, portfolio management, process asset, software product line},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2430502.2430516,
author = {Lanceloti, Leandro A. and Maldonado, Jos\'{e} C. and Gimenes, Itana M. S. and Oliveira, Edson A.},
title = {SMartyParser: a XMI parser for UML-based software product line variability models},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430516},
doi = {10.1145/2430502.2430516},
abstract = {Variability management is an important issue for the software-intensive systems domain. Such an issue is essential for the success of software product line (SPL) adoption strategies. Although it is a well-discussed subject in the SPL community, there is a lack of tool support for environments that handle UML-based SPL variabilities, as several variability management approaches take UML as a basis, specially its profiling mechanism. Such environments might handle variabilities for several reasons, such as, evaluating SPLs, defining and applying metrics based on a SPL modeling, and automating the product generation. Therefore, this paper presents the SMartyParser, a parser for processing UML-based SPL models. Such models can be obtained, in the XMI format, from every UML specification-compliant tool. Such a parser provides several services to make it easier the handling of variability data in a particular SPL environment/tool. SMartyParser was built by taking the Open Core framework as a basis for processing XMI files. A parser use example is presented by taking into account the SPL Arcade Game Maker UML models.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {10},
numpages = {5},
keywords = {UML, XMI, parser, software product line, stereotype, variability management},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@article{10.1145/3611663,
author = {Oh, Jeho and Batory, Don and Heradio, Rub\'{e}n},
title = {Finding Near-optimal Configurations in Colossal Spaces with Statistical Guarantees},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3611663},
doi = {10.1145/3611663},
abstract = {A Software Product Line (SPL) is a family of similar programs. Each program is defined by a unique set of features, called a configuration, that satisfies all feature constraints. “What configuration achieves the best performance for a given workload?” is the SPLOptimization (SPLO) challenge. SPLO is daunting: just 80 unconstrained features yield 1024 unique configurations, which equals the estimated number of stars in the universe. We explain (a) how uniform random sampling and random search algorithms solve SPLO more efficiently and accurately than current machine-learned performance models and (b) how to compute statistical guarantees on the quality of a returned configuration; i.e., it is within x% of optimal with y% confidence.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {7},
numpages = {36},
keywords = {Software product lines, configuration optimization, product spaces, machine learning, uniform random sampling, random search, order statistics}
}

@inproceedings{10.1145/3382025.3414976,
author = {Pereira, Juliana Alves and Martin, Hugo and Temple, Paul and Acher, Mathieu},
title = {Machine learning and configurable systems: a gentle introduction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414976},
doi = {10.1145/3382025.3414976},
abstract = {The goal of this tutorial is to give a gentle introduction to how machine learning can be used to support software product line configuration. This is our second practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance and bug prediction) on real-world systems (Linux, VaryLaTeX, x264). The material is designed for academics and practitioners with basic knowledge in software product lines and machine learning.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {40},
numpages = {1},
keywords = {configurable systems, machine learning, software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3233027.3233042,
author = {Mesa, Oslien and Vieira, Reginaldo and Viana, Marx and Durelli, Vinicius H. S. and Cirilo, Elder and Kalinowski, Marcos and Lucena, Carlos},
title = {Understanding vulnerabilities in plugin-based web systems: an exploratory study of wordpress},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233042},
doi = {10.1145/3233027.3233042},
abstract = {A common software product line strategy involves plugin-based web systems that support simple and quick incorporation of custom behaviors. As a result, they have been widely adopted to create web-based applications. Indeed, the popularity of ecosystems that support plugin-based development (e.g., WordPress) is largely due to the number of customization options available as community-contributed plugins. However, plugin-related vulnerabilities tend to be recurrent, exploitable and hard to be detected and may lead to severe consequences for the customized product. Hence, there is a need to further understand such vulnerabilities to enable preventing relevant security threats. Therefore, we conducted an exploratory study to characterize vulnerabilities caused by plugins in web-based systems. To this end, we went over WordPress vulnerability bulletins cataloged by the National Vulnerability Database as well as associated patches maintained by the WordPress plugins repository. We identified the main types of vulnerabilities caused by plugins as well as their impact and the size of the patch to fix the vulnerability. Moreover, we identified the most common security-related topics discussed among WordPress developers. We observed that, while plugin-related vulnerabilities may have severe consequences and might remain unnoticed for years before being fixed, they can commonly be mitigated with small and localized changes to the source code. The characterization helps to provide an understanding on how typical plugin-based vulnerabilities manifest themselves in practice. Such information can be helpful to steer future research on plugin-based vulnerability detection and prevention.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {149–159},
numpages = {11},
keywords = {exploratory study, plugin-based web systems, security, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3461001.3471146,
author = {Horcas, Jose-Miguel and Galindo, Jos\'{e} A. and Heradio, Ruben and Fernandez-Amoros, David and Benavides, David},
title = {Monte Carlo tree search for feature model analyses: a general framework for decision-making},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471146},
doi = {10.1145/3461001.3471146},
abstract = {The colossal solution spaces of most configurable systems make intractable their exhaustive exploration. Accordingly, relevant analyses remain open research problems. There exist analyses alternatives such as SAT solving or constraint programming. However, none of them have explored simulation-based methods. Monte Carlo-based decision making is a simulation-based method for dealing with colossal solution spaces using randomness. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo methods, which have proven to succeed in vast search spaces (e.g., game theory). Our general framework is described formally, and its flexibility to cope with a diversity of analysis problems is discussed (e.g., finding defective configurations, feature model reverse engineering or getting optimal performance configurations). Additionally, we present a Python implementation of the framework that shows the feasibility of our proposal. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state of the art a step forward.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {190–201},
numpages = {12},
keywords = {configurable systems, feature models, monte carlo tree search, software product lines, variability modeling},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3382025.3414989,
author = {Krieter, Sebastian},
title = {Large-scale T-wise interaction sampling using YASA},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414989},
doi = {10.1145/3382025.3414989},
abstract = {Testing highly-configurable software systems (i.e., software product lines) is challenging due to their large configuration space. T-wise sampling is one method of finding a representative subset of configurations for a system, which can then be tested. However, for large-scale systems, such as Linux, existing t-wise sampling algorithms do not scale well. To this end, Pett et al. proposed the sampling challenge for large-scale systems at SPLC 2019. In this paper, we attempt to solve the proposed challenge using our sampling algorithm YASA. We report our experience for all three of the given systems FinancialServices01, Automotive02, and Linux. In addition, we present the results for computing samples for all versions of the system FinancialServices01.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {29},
numpages = {4},
keywords = {T-wise sampling, configurable system, software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3233027.3233041,
author = {Montalvillo, Leticia and D\'{\i}az, Oscar and Fogdal, Thomas},
title = {Reducing coordination overhead in SPLs: peering in on peers},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233041},
doi = {10.1145/3233027.3233041},
abstract = {SPL product customers might not always wait for the next core asset release. When an organization aims to react to market events, quick bug fixes or urgent customer requests, strategies are needed to support fast adaptation, e.g. with product-specific extensions, which are later propagated into the SPL. This leads to the grow-and-prune model where quick reaction to changes often requires copying and specialization (grow) to be later cleaned up by merging and refactoring (prune). This paper focuses on the grow stage. Here, application engineers branch off the core-asset Master branch to account for their products' specifics within the times and priorities of their customers without having to wait for the next release of the core assets. However, this practice might end up in the so-called "integration hell". When long-living branches are merged back into the Master, the amount of code to be integrated might cause build failures or requires complex troubleshooting. On these premises, we advocate for making application engineers aware of potential coordination problems right during coding rather than deferring it until merge time. To this end, we introduce the notion of "peering bar" for Version Control Systems, i.e. visual bars that reflect whether your product's features are being upgraded in other product branches. In this way, engineers are aware of what their peers are doing on the other SPL's products. Being products from the same SPL, they are based on the very same core assets, and hence, bug ixes or functional enhancements undertaken for a product might well serve other products. This work introduces design principles for peering bars. These principles are fleshed out for GitHub as the Version Control System, and pure::variants as the SPL framework.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {110–120},
numpages = {11},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3106195.3106222,
author = {Corti\~{n}as, Alejandro and Luaces, Miguel R. and Pedreira, Oscar and Places, \'{A}ngeles S. and P\'{e}rez, Jennifer},
title = {Web-based Geographic Information Systems SPLE: Domain Analysis and Experience Report},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106222},
doi = {10.1145/3106195.3106222},
abstract = {Geographic Information Systems (GIS) play a critical role for supporting the development of Cyber Physical Systems (CPS), since they allow geolocating users and the "things" or smart objects that constitute a CPS, providing a realistic vision in quasi real-time. This has increased the demand of developing web-based GIS applications to be deployed in the different devices and wearables of the CPS with short time-to-market. This demand and the fact that web-based GIS applications of CPS share many features and known variability justifies why they present the perfect setting to apply software product-lines engineering (SPLE). In this paper, we present the experience of developing a web-based GIS product line in the SME Enxenio, and the methodology applied to define the product line. In addition, we present the results obtained providing the GIS community with a reference SPL that is ready for its evolution and enrichment.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {190–194},
numpages = {5},
keywords = {Software product line engineering, scaffolding, web-based geographic information systems},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3233027.3233030,
author = {Weckesser, Markus and Kluge, Roland and Pfannem\"{u}ller, Martin and Matth\'{e}, Michael and Sch\"{u}rr, Andy and Becker, Christian},
title = {Optimal reconfiguration of dynamic software product lines based on performance-influence models},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233030},
doi = {10.1145/3233027.3233030},
abstract = {Today's adaptive software systems (i) are often highly configurable product lines, exhibiting hundreds of potentially conflicting configuration options; (ii) are context dependent, forcing the system to reconfigure to ever-changing contextual situations at runtime; (iii) need to fulfill context-dependent performance goals by optimizing measurable nonfunctional properties. Usually, a large number of consistent configurations exists for a given context, and each consistent configuration may perform differently with regard to the current context and performance goal(s). Therefore, it is crucial to consider nonfunctional properties for identifying an appropriate configuration. Existing black-box approaches for estimating the performance of configurations provide no means for determining context-sensitive reconfiguration decisions at runtime that are both consistent and optimal, and hardly allow for combining multiple context-dependent quality goals. In this paper, we propose a comprehensive approach based on Dynamic Software Product Lines (DSPL) for obtaining consistent and optimal reconfiguration decisions. We use training data obtained from simulations to learn performance-influence models. A novel integrated runtime representation captures both consistency properties and the learned performance-influence models. Our solution provides the flexibility to define multiple context-dependent performance goals. We have implemented our approach as a standalone component. Based on an Internet-of-Things case study using adaptive wireless sensor networks, we evaluate our approach with regard to effectiveness, efficiency, and applicability.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {98–109},
numpages = {12},
keywords = {dynamic software product lines, machine learning, performance-influence models},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3510003.3510094,
author = {He, Haochen and Jia, Zhouyang and Li, Shanshan and Yu, Yue and Zhou, Chenglong and Liao, Qing and Wang, Ji and Liao, Xiangke},
title = {Multi-intention-aware configuration selection for performance tuning},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510094},
doi = {10.1145/3510003.3510094},
abstract = {Automatic configuration tuning helps users who intend to improve software performance. However, the auto-tuners are limited by the huge configuration search space. More importantly, they focus only on performance improvement while being unaware of other important user intentions (e.g., reliability, security). To reduce the search space, researchers mainly focus on pre-selecting performance-related parameters which requires a heavy stage of dynamically running under different configurations to build performance models. Given that other important user intentions are not paid attention to, we focus on guiding users in pre-selecting performance-related parameters in general while warning about side-effects on non-performance intentions. We find that the configuration document often, if it does not always, contains rich information about the parameters' relationship with diverse user intentions, but documents might also be long and domain-specific.In this paper, we first conduct a comprehensive study on 13 representative software containing 7,349 configuration parameters, and derive six types of ways in which configuration parameters may affect non-performance intentions. Guided by this study, we design SafeTune, a multi-intention-aware method that preselects important performance-related parameters and warns about their side-effects on non-performance intentions. Evaluation on target software shows that SafeTune correctly identifies 22--26 performance-related parameters that are missed by state-of-the-art tools but have significant performance impact (up to 14.7x). Furthermore, we illustrate eight representative cases to show that SafeTune can effectively prevent real-world and critical side-effects on other user intentions.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1431–1442},
numpages = {12},
keywords = {non-performance property, performance tuning, user intention},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/2362536.2362560,
author = {Lettner, Daniela and Vierhauser, Michael and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {Supporting end users with business calculations in product configuration},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362560},
doi = {10.1145/2362536.2362560},
abstract = {Business calculations like break-even, return on investment, or cost are essential in many domains to support decision making while configuring products. For instance, customers and sales people need to estimate and compare the business value of different product variants. Some product line approaches provide initial support, e.g., by defining quality attributes in relation to features. However, an approach that allows domain engineers to easily define business calculations together with variability models is still lacking. In product configuration, calculation results need to be instantly presented to end users after making configuration choices. Further, due to the often high number of calculations, the presentation of calculation results to end users can be challenging. These challenges cannot be addressed by integrating off-the-shelf applications performing the calculations with product line tools. We thus present an approach based on dedicated calculation models that are related to variability models. Our approach seamlessly integrates business calculations with product configuration and provides support for formatting calculations and calculation results. We use the DOPLER tool suite to deploy calculations together with variability models to end users in product configuration. We evaluate the expressiveness and practical relevance of the approach by investigating the development of business calculations for 15 product lines from the domain of industrial automation.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {171–180},
numpages = {10},
keywords = {business calculations, product configuration, variability models},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2364412.2364429,
author = {Parra, Carlos and Giral, Leonardo and Infante, Alvaro and Cort\'{e}s, Camilo},
title = {Extractive SPL adoption using multi-level variability modeling},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364429},
doi = {10.1145/2364412.2364429},
abstract = {Software Product Line engineering aims at reusing and automating software development to reduce costs, have shorter development cycles, and maintain quality. However, for organizations with settled development processes and a large code base, adopting an SPL approach may prove to be a daunting task. In this paper we present an industrial experimentation and a proposal for an SPL adoption in Heinsohn Business Technology (HBT), a software development company specialized in financial, transportation, mortgage-backed securities, and pension-fund solutions. We start by identifying and modeling multiple levels of variability inherent to the kind of developments undertaken by HBT. Next, we define restrictions inside every level as well as between the levels to fully characterize an HBT software product. To limit the impact on the organization development process, we use an extractive approach. This allows us to design core assets starting from current software artifacts. The overall approach is based on real-world software artifacts developed over the years by HBT, whose combinations result in approximately 4.88e11 possible product configurations.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {99–106},
numpages = {8},
keywords = {model-driven engineering, software product lines},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3503229.3547030,
author = {Baranov, Eduard and Legay, Axel},
title = {Baital: an adaptive weighted sampling platform for configurable systems},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547030},
doi = {10.1145/3503229.3547030},
abstract = {The diversity of software application scenarios has led the evolution towards highly configurable systems. Testing of such systems is challenging due to an immense number of configurations and is usually performed on a small sample set. Sampling is a promising approach for the sample set generation. t-wise coverage is often used to measure the quality of sample sets. Uniform sampling being most known method can fail to achieve high coverage in presence of complex constraints on configurations. Another challenge is a scalability hurdle for the t-wise coverage computation leaving sampling for higher values of t unexplored.In this work, we present Baital, a platform that combines two novel techniques for sampling of configurable systems. It is based on the adaptive weighted sampling approach to generate sample sets with high t-wise coverage. The approximation techniques for the t-wise coverage computation allow the consideration of higher values of t; they improve scalability for both t-wise coverage computation and sampling process.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {46–49},
numpages = {4},
keywords = {adaptive weighted sampling, configurable systems, t-wise coverage},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/2934466.2966353,
author = {Sion, Laurens and Van Landuyt, Dimitri and Yskout, Koen and Joosen, Wouter},
title = {Towards systematically addressing security variability in software product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2966353},
doi = {10.1145/2934466.2966353},
abstract = {With the increasingly pervasive role of software in society, security is becoming an important quality concern, emphasizing security by design, but it requires intensive specialization.Security in families of systems is even harder, as diverse variants of security solutions must be considered, with even different security goals per product. Furthermore, security is not a static object but a moving target, adding variability.For this, an approach to systematically address security concerns in software product lines is needed. It should consider security separate from other variability dimensions. The main challenges to realize this are: (i) expressing security and its variability, (ii) selecting the right solution, (iii) properly instantiating a solution, and (iv) verifying and validating it.In this paper, we present our research agenda towards addressing the aforementioned challenges.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {342–343},
numpages = {2},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3307630.3342386,
author = {D\'{\i}az, Oscar and Medeiros, Raul and Montalvillo, Leticia},
title = {Change Analysis of #if-def Blocks with FeatureCloud},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342386},
doi = {10.1145/3307630.3342386},
abstract = {FeatureCloud is a git client for the visualization of evolution in annotated SPL, i.e. those resorting to ifdefs for variability. Specifically, FeatureCloud (1) mines git repositories; (2) extracts ifdefs; (3) works out differences between two versions of the same ifdefs; (4) abstracts ifdefs in terms of their code churn, tangling and scattering; and finally (5), aggregates and visualizes these properties through "feature clouds". Feature clouds aim to play the same role for SPLs that "word clouds" for textual content: provide an abstract view of the occurrence of features along evolving code, where "repetition" account for scattering and tangling of features. Here, we introduce the analysis goals, the perspective (i.e. the object of analysis) and visualization strategies that underpin FeatureCloud.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {17–20},
numpages = {4},
keywords = {SPL evolution, preprocessor directives, tag clouds},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2934466.2934473,
author = {Olaechea, Rafael and Fahrenberg, Uli and Atlee, Joanne M. and Legay, Axel},
title = {Long-term average cost in featured transition systems},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934473},
doi = {10.1145/2934466.2934473},
abstract = {A software product line is a family of software products that share a common set of mandatory features and whose individual products are differentiated by their variable (optional or alternative) features. Family-based analysis of software product lines takes as input a single model of a complete product line and analyzes all its products at the same time. As the number of products in a software product line may be large, this is generally preferable to analyzing each product on its own. Family-based analysis, however, requires that standard algorithms be adapted to accomodate variability.In this paper we adapt the standard algorithm for computing limit average cost of a weighted transition system to software product lines. Limit average is a useful and popular measure for the long-term average behavior of a quality attribute such as performance or energy consumption, but has hitherto not been available for family-based analysis of software product lines. Our algorithm operates on weighted featured transition systems, at a symbolic level, and computes limit average cost for all products in a software product line at the same time. We have implemented the algorithm and evaluated it on several examples.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {109–118},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2791060.2796539,
author = {Santos, Alcemir Rodrigues},
title = {Understanding hybrid SPL composition impact on the refactoring into SPL},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2796539},
doi = {10.1145/2791060.2796539},
abstract = {Feature-oriented software development still register few cases of adoption in industry. At least to some extent, such lower adotion might derive from the lack of evidence of its gains in the overall software project costs. This Ph.D thesis aims to improve the state-of-the-art by adding evidence to the body of knowledge on the adoption of Software Product Line (SPL) hybrid composition approahces.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {348–351},
numpages = {4},
keywords = {conditional compitlation, consistency checking, eclipse plugin, exploratory study, software product lines engineering},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2648511.2648517,
author = {Angerer, Florian and Pr\"{a}hofer, Herbert and Lettner, Daniela and Grimmer, Andreas and Gr\"{u}nbacher, Paul},
title = {Identifying inactive code in product lines with configuration-aware system dependence graphs},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648517},
doi = {10.1145/2648511.2648517},
abstract = {Application engineers frequently create customer-specific products in two stages: the required software components are first selected to create an initial product which is then evolved by refining the selected features and adapting the code to meet the customers' requirements. For instance, developers frequently set configuration options in the code to adjust the product. However, given that such changes are often necessary in the entire code base it is hard to know which part of the code is still relevant for the chosen configuration options. This means that engineers need to understand and maintain a lot of code that is potentially inactive in a particular product variant. Existing approaches provide only partial solutions: for instance, feature-to-code mappings do not adequately consider complex code dependencies of the implemented features. Static analysis techniques provide better results but usually do not consider variability aspects. We present an approach to automatically identify inactive code in product variants using a configuration-aware code analysis technique. We demonstrate the flexibility of our approach by customizing it to a product line of an industry partner in the domain of industrial automation. We further evaluate the approach to demonstrate its effectiveness, accuracy, and performance.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {52–61},
numpages = {10},
keywords = {application engineering, clone-and-own product lines, configuration, maintenance, static analysis},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3336294.3336318,
author = {Ebert, Rolf and Jolianis, Jahir and Kriebel, Stefan and Markthaler, Matthias and Pruenster, Benjamin and Rumpe, Bernhard and Salman, Karin Samira},
title = {Applying Product Line Testing for the Electric Drive System},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336318},
doi = {10.1145/3336294.3336318},
abstract = {The growth in electrification and digitalization of vehicles leads to increasing variability and complexity of automotive systems. This poses new challenges for verification and validation, identified in a Product Line Engineering case study for the electric drive system. To overcome those challenges we developed a Product Line Testing methodology called TIGRE. In this paper, we present the TIGRE methodology. TIGRE comprises the identification and documentation of relevant data for efficient product line testing and the application of this data in the test management of an agile project environment. Furthermore, we present our experiences from the introduction into a large-scale industrial context. Based on our results from the introduction, we conclude that the TIGRE approach reduces the testing effort for automotive product lines significantly and, furthermore, allows us to transfer the results to untested products.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {14–24},
numpages = {11},
keywords = {automotive industry, product line engineering, product line testing, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2362536.2362559,
author = {Gillain, Joseph and Faulkner, Stephane and Heymans, Patrick and Jureta, Ivan and Snoeck, Monique},
title = {Product portfolio scope optimization based on features and goals},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362559},
doi = {10.1145/2362536.2362559},
abstract = {In this paper we propose a mathematical program able to optimize the product portfolio scope of a software product line and sketch both a development and a release planning. Our model is based on the description of customer needs in terms of goals. We show that this model can be instantiated in several contexts such as a market customization strategy or a mass-customization strategy. It can deal with Software Product Line development from scratch as well as starting from a legacy software base. We demonstrate its applicability with an example based on a case study.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {161–170},
numpages = {10},
keywords = {product portfolio, release planning, scoping optimization, software product line},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3233027.3233043,
author = {Masri, Samer AL and Nadi, Sarah and Gaudet, Matthew and Liang, Xiaoli and Young, Robert W.},
title = {Using static analysis to support variability implementation decisions in C++},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233043},
doi = {10.1145/3233027.3233043},
abstract = {Eclipse OMR is an open-source C++ framework for building robust language runtimes. The OMR toolkit includes a dynamic Just-In-Time (JIT) compiler, a garbage collector, a platform abstraction library, and a set of developer tooling capabilities. To support the diverse languages and architectures targeted by the framework, OMR's variability implementation uses a combination of build-system variability and static polymorphism. That is, all implementation classes that depend on the selected language and architecture are decided at compile time. However, OMR developers now realize that the current variability design decision, specifically the static polymorphism implementation, has its drawbacks. They are considering using dynamic polymorphism instead of static polymorphism. Before making such a fundamental design change, however, it is crucial to collect function information and overload/override statistics about the current variability in the code base.In this paper, we present OMRStatistics, a static analysis tool that we built for OMR developers to help them collect this information. Specifically, OMRStatistics (1) visualizes the class hierarchy from OMR's current static polymorphic implementation, (2) visualizes the function overloads and overrides with their respective locations in the source code, (3) collects important information about the classes and functions, and (4) stores all the collected information in a database for further analysis. Our tool OMRStatistics allows OMR developers to make better design decisions on which variability extension points should be switched from static polymorphism to dynamic polymorphism.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {236–245},
numpages = {10},
keywords = {C++, build path variability, clang plugin, dynamic polymorphism, software variability analysis, static analysis, static polymorphism},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2934466.2934469,
author = {Zhang, Yi and Guo, Jianmei and Blais, Eric and Czarnecki, Krzysztof and Yu, Huiqun},
title = {A mathematical model of performance-relevant feature interactions},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934469},
doi = {10.1145/2934466.2934469},
abstract = {Modern software systems have grown significantly in their size and complexity, therefore understanding how software systems behave when there are many configuration options, also called features, is no longer a trivial task. This is primarily due to the potentially complex interactions among the features. In this paper, we propose a novel mathematical model for performance-relevant, or quantitative in general, feature interactions, based on the theory of Boolean functions. Moreover, we provide two algorithms for detecting all such interactions with little measurement effort and potentially guaranteed accuracy and confidence level. Empirical results on real-world configurable systems demonstrated the feasibility and effectiveness of our approach.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {25–34},
numpages = {10},
keywords = {boolean functions, feature interactions, fourier transform, performance},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3106195.3106201,
author = {Kim, Jongwook and Batory, Don and Dig, Danny},
title = {Refactoring Java Software Product Lines},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106201},
doi = {10.1145/3106195.3106201},
abstract = {Refactoring is a staple of Object-Oriented (OO) program development. It should be a staple of OO Software Product Line (SPL) development too. X15 is the first tool to support the refactoring of Java SPL codebases. X15 (1) uses Java custom annotations to encode variability in feature-based Java SPLs, (2) projects a view of an SPL product (a program that corresponds to a legal SPL configuration), and (3) allows programmers to edit and refactor the product, propagating changes back to the SPL codebase. Case studies apply 2316 refactorings in 8 public Java SPLs and show that X15 is as efficient, expressive, and scalable as a state-of-the-art feature-unaware Java refactoring engine.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {59–68},
numpages = {10},
keywords = {refactoring, software product lines},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3336294.3336302,
author = {Str\"{u}ber, Daniel and Mukelabai, Mukelabai and Kr\"{u}ger, Jacob and Fischer, Stefan and Linsbauer, Lukas and Martinez, Jabier and Berger, Thorsten},
title = {Facing the Truth: Benchmarking the Techniques for the Evolution of Variant-Rich Systems},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336302},
doi = {10.1145/3336294.3336302},
abstract = {The evolution of variant-rich systems is a challenging task. To support developers, the research community has proposed a range of different techniques over the last decades. However, many techniques have not been adopted in practice so far. To advance such techniques and to support their adoption, it is crucial to evaluate them against realistic baselines, ideally in the form of generally accessible benchmarks. To this end, we need to improve our empirical understanding of typical evolution scenarios for variant-rich systems and their relevance for benchmarking. In this paper, we establish eleven evolution scenarios in which benchmarks would be beneficial. Our scenarios cover typical lifecycles of variant-rich system, ranging from clone &amp; own to adopting and evolving a configurable product-line platform. For each scenario, we formulate benchmarking requirements and assess its clarity and relevance via a survey with experts in variant-rich systems and software evolution. We also surveyed the existing benchmarking landscape, identifying synergies and gaps. We observed that most scenarios, despite being perceived as important by experts, are only partially or not at all supported by existing benchmarks-a call to arms for building community benchmarks upon our requirements. We hope that our work raises awareness for benchmarking as a means to advance techniques for evolving variant-rich systems, and that it will lead to a benchmarking initiative in our community.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {177–188},
numpages = {12},
keywords = {benchmark, product lines, software evolution, software variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3382025.3414965,
author = {Young, Jeffrey M. and Walkingshaw, Eric and Th\"{u}m, Thomas},
title = {Variational satisfiability solving},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414965},
doi = {10.1145/3382025.3414965},
abstract = {Incremental satisfiability (SAT) solving is an extension of classic SAT solving that allows users to efficiently solve a set of related SAT problems by identifying and exploiting shared terms. However, using incremental solvers effectively is hard since performance is sensitive to a problem's structure and the order sub-terms are fed to the solver, and the burden to track results is placed on the end user. For analyses that generate sets of related SAT problems, such as those in software product lines, incremental SAT solvers are either not used at all, used but not explicitly stated so in the literature, or used but suffer from the aforementioned usability problems. This paper translates the ordering problem to an encoding problem and automates the use of incremental SAT solving. We introduce variational SAT solving, which differs from incremental SAT solving by accepting all related problems as a single variational input and returning all results as a single variational output. Our central idea is to make explicit the operations of incremental SAT solving, thereby encoding differences between related SAT problems as local points of variation. Our approach automates the interaction with the incremental solver and enables methods to automatically optimize sharing of the input. To evaluate our methods we construct a prototype variational SAT solver and perform an empirical analysis on two real-world datasets that applied incremental solvers to software evolution scenarios. We show, assuming a variational input, that the prototype solver scales better for these problems than naive incremental solving while also removing the need to track individual results.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {18},
numpages = {12},
keywords = {choice calculus, satisfiability solving, software product lines, variation},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2019136.2019168,
author = {Nakagawa, Elisa Yumi and Antonino, Pablo Oliveira and Becker, Martin},
title = {Exploring the use of reference architectures in the development of product line artifacts},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019168},
doi = {10.1145/2019136.2019168},
abstract = {Software Product Line (SPL) has arisen as an approach for developing a family of software-intensive systems at lower costs, within shorter time, and with higher quality. In particular, SPL is supported by a product line architecture (sometimes also referred to as reference architecture) that captures the architectures of a product family. In another context, a special type of architecture that contains knowledge about a specific domain has been increasingly investigated, resulting in the Reference Architecture research area. In spite of the positive impact of this type of architecture on reuse and productivity, the use of existing domain-specific reference architectures as basis of SPL has not been widely explored. The main contribution of this paper is to present how and when elements contained in existing reference architectures could contribute to the building of SPL artifacts during development of an SPL. We have observed that, in fact, reference architectures could make an important contribution to improving reuse and productivity, which are also important concerns in SPL.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {28},
numpages = {8},
keywords = {SPL design method, reference architecture, software product line},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2648511.2648525,
author = {Stein, Jacob and Nunes, Ingrid and Cirilo, Elder},
title = {Preference-based feature model configuration with multiple stakeholders},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648525},
doi = {10.1145/2648511.2648525},
abstract = {Feature model configuration is known to be a hard, error-prone and time-consuming activity. This activity gets even more complicated when it involves multiple stakeholders in the configuration process. Research work has proposed approaches to aid multi-stakeholder feature model configuration, but they rely on systematic processes that constraint decisions of some of the stakeholders. In this paper, we propose a novel approach to improve the multi-stakeholder configuration process, considering stakeholders' preferences expressed through both hard and soft constraints. Based on such preferences, we recommend different product configurations using different strategies from the social choice theory. We conducted an empirical study to evaluate the effectiveness of our strategies with respect to individual stakeholder satisfaction and fairness among all stakeholders. Results indicate that particular strategies perform best with respect to these aspects.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {132–141},
numpages = {10},
keywords = {feature model configuration, preferences, social choice},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3461001.3471145,
author = {Sundermann, Chico and Feichtinger, Kevin and Engelhardt, Dominik and Rabiser, Rick and Th\"{u}m, Thomas},
title = {Yet another textual variability language? a community effort towards a unified language},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471145},
doi = {10.1145/3461001.3471145},
abstract = {Variability models are commonly used to model commonalities and variability in a product line. There is a large variety of textual formats to represent and store variability models. This variety causes overhead to researchers and practitioners as they frequently need to translate models. The MODEVAR initiative consists of dozens of researchers and aims to find a unified language for variability modeling. In this work, we describe the cooperative development of a textual variability language. We evaluate preferences of the community regarding properties of existing formats and applications for an initial design of a unified variability language. Then, we examine the acceptance of the community for our proposal. The results indicate that our proposal is a promising start towards a unified variability language instead of yet another language. We envision that the community applies our language proposal in teaching, research prototypes, and industrial applications to further evolve the design and then ultimately reach a unified language.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {136–147},
numpages = {12},
keywords = {exchange format, software product lines, unified language, variability language, variability modeling},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3579027.3608998,
author = {Luo, Chuan and Song, Jianping and Zhao, Qiyuan and Li, Yibei and Cai, Shaowei and Hu, Chunming},
title = {Generating Pairwise Covering Arrays for Highly Configurable Software Systems},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608998},
doi = {10.1145/3579027.3608998},
abstract = {Highly configurable software systems play crucial roles in real-world applications, which urgently calls for useful testing methods. Combinatorial interaction testing (CIT) is an effective methodology for detecting those faults that are triggered by the interaction of any t options, where t is the testing strength. Pairwise testing, i.e., CIT with t = 2, is known to be the most practical and popular CIT technique, and the pairwise covering array generation (PCAG) problem is the most critical problem in pairwise testing. Due to the practical importance of PCAG, many PCAG algorithms have been proposed. Unfortunately, existing PCAG algorithms suffer from the severe scalability problem. To this end, the SPLC Scalability Challenge (i.e., Product Sampling for Product Lines: The Scalability Challenge) has been proposed since 2019, in order to motivate researchers to develop practical PCAG algorithms for overcoming this scalability problem. In this work, we present a practical PCAG algorithm dubbed SamplingCA-ASF. To the best of our knowledge, our experiments show that SamplingCA-ASF is the first algorithm that can generate PCAs for Automotive02 and Linux, the two hardest and largest-scale instances in the SPLC Scalability Challenge, within reasonable time. Our experimental results indicate that SamplingCA-ASF can effectively alleviate the scalability problem in pairwise testing.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {261–267},
numpages = {7},
keywords = {covering array generation, pairwise testing, scalability problem},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3307630.3342403,
author = {Berger, Thorsten and Collet, Philippe},
title = {Usage Scenarios for a Common Feature Modeling Language},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342403},
doi = {10.1145/3307630.3342403},
abstract = {Feature models are recognized as a de facto standard for variability modeling. Presented almost three decades ago, dozens of different variations and extensions to the original feature-modeling notation have been proposed, together with hundreds of variability management techniques building upon feature models. Unfortunately, despite several attempts to establish a unified language, there is still no emerging consensus on a feature-modeling language that is both intuitive and simple, but also expressive enough to cover a range of important usage scenarios. There is not even a documented and commonly agreed set of such scenarios.Following an initiative among product-line engineering researchers in September 2018, we present 14 usage scenarios together with examples and requirements detailing each scenario. The scenario descriptions are the result of a systematic process, where members of the initiative authored original descriptions, which received feedback via a survey, and which we then refined and extended based on the survey results, reviewers' comments, and our own expertise. We also report the relevance of supporting each usage scenario for the language, as perceived by the initiative's members, prioritizing each scenario. We present a roadmap to build and implement a first version of the envisaged common language.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {174–181},
numpages = {8},
keywords = {feature models, software product lines, unified language},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1145/3672555,
author = {Mahdavi-Hezaveh, Rezvan and Fatima, Sameeha and Williams, Laurie},
title = {Paving a Path for a Combined Family of Feature Toggle and Configuration Option Research},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3672555},
doi = {10.1145/3672555},
abstract = {Feature toggles and configuration options are techniques to include or exclude functionality in software. The research contributions to these two techniques have most often been focused on either one of them. However, focusing on the similarities of these two techniques and the use of a common terminology may enable a combined family of research on software configuration (a term we use to encompass both techniques) and prevent duplication of effort. The goal of this study is to aid researchers in conducting a family of research on software configuration by extending an existing model of software configuration that provides a common terminology for feature toggles and configuration options in research studies. We started with Siegmund et al.’s Model of Software Configuration (MSC), which was developed based on configuration option-related resources. We extend the MSC by qualitative analysis of feature toggle-related resources. From our analysis, we proposed MSCv2 and evaluated it through its application on publications and an industrial system. Our results indicate researchers studying the same system may provide different definitions of software configuration in publications, similar research questions may be answered repeatedly because of a lack of a clear definition of software configuration, and having an MSC may enable generalized research on this family of research.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {172},
numpages = {27},
keywords = {Feature toggle, configuration option, software configuration, software engineering}
}

@inproceedings{10.1145/2648511.2648541,
author = {Gregg, Susan P. and Scharadin, Rick and LeGore, Eric and Clements, Paul},
title = {Lessons from AEGIS: organizational and governance aspects of a major product line in a multi-program environment},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648541},
doi = {10.1145/2648511.2648541},
abstract = {This paper tells the story of the AEGIS Weapon System product line and how it evolved from a series of standalone software programs with no sharing into a true systems and software product line. The paper focuses on the strong internal and external governance of the product line. The need for strong governance is brought about by the strong role that the AEGIS customer community plays in oversight of design, development, and procurement. The paper recounts the product line's beginnings, and describes how the product line is operated today. Organizational issues, measurement issues, and governance issues are covered, along with a summary of important lessons learned about operating a product line in an environment of strong competing interests.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {264–273},
numpages = {10},
keywords = {AEGIS, Navy, bill-of-features, combat systems, command and control, feature modeling, feature profiles, hierarchical product lines, product audit, product baselines, product configurator, product derivation, product line engineering, product line governance, product portfolio, second generation product line engineering, software product lines, variation points},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3336294.3336314,
author = {Asano, Masaki and Nishiura, Yoichi and Nakanishi, Tsuneo and Fujiwara, Keiichi},
title = {Feature Oriented Refinement from Requirements to System Decomposition: Quantitative and Accountable Approach},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336314},
doi = {10.1145/3336294.3336314},
abstract = {This paper presents the revised domain engineering process to develop product lines of automotive body parts in Aisin Seiki Co., Ltd. In the process, feature analysis is conducted by a limited number of engineers with talent of abstraction and separation and other work including specifications and architecture design is conducted by average engineers who know the products. Feature analysis defines a hierarchy of abstraction, achieves separation of concerns, and disciplines other artifacts to follow the structure of abstraction and separation. Requirements and specifications are refined by the use case, use case scenario, and hierarchical tabular description (USDM) in a step-wise manner. The specification in USDM is refined to a system decomposition in a quantitative and accountable manner using the robustness diagram and design structure matrix. The revised domain engineering process reduced the issues pointed out in software reviews concerning errors on specifications and architecture design. Moreover, it reduced lead time for architecture design and produced the architecture tolerant to changes.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {195–205},
numpages = {11},
keywords = {automotive body parts, design structure matrix, feature analysis, robustness analysis, software product lines, use case approach},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2499777.2500723,
author = {Alsawalqah, Hamad and Kang, Sungwon and Lee, Danhyung},
title = {A method for software product platform design based on features},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500723},
doi = {10.1145/2499777.2500723},
abstract = {Due to the increased competition and the advent of mass customization, software firms are applying the Software Product Line Engineering (SPLE) approach to provide product variety in a cost-effective manner. Although the key to designing a successful software product family is the product platform, yet there is lack of measures and methods that are useful to optimize the product platform design. This paper proposes a method to provide decision support to determine the optimized product platform design. The method targets at identifying the optimized product platform design in order to maximize the cost savings and the amount of commonality while meeting the goals and needs of the envisioned customers' segments. It generates, validates, and evaluates alternative product platform designs while considering market concerns (e.g., customer preferences) and technical product platform concerns (e.g., decisions regarding shared features, economic benefit). We demonstrate its applicability with an example of platform design problem in smart phones domain.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {18–25},
numpages = {8},
keywords = {Kano scheme, commonality index, product platform design, software product line},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3382025.3414951,
author = {Heradio, Ruben and Fernandez-Amoros, David and Galindo, Jos\'{e} A. and Benavides, David},
title = {Uniform and scalable SAT-sampling for configurable systems},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414951},
doi = {10.1145/3382025.3414951},
abstract = {Several relevant analyses on configurable software systems remain intractable because they require examining vast and highly-constrained configuration spaces. Those analyses could be addressed through statistical inference, i.e., working with a much more tractable sample that later supports generalizing the results obtained to the entire configuration space. To make this possible, the laws of statistical inference impose an indispensable requirement: each member of the population must be equally likely to be included in the sample, i.e., the sampling process needs to be "uniform". Various SAT-samplers have been developed for generating uniform random samples at a reasonable computational cost. Unfortunately, there is a lack of experimental validation over large configuration models to show whether the samplers indeed produce genuine uniform samples or not. This paper (i) presents a new statistical test to verify to what extent samplers accomplish uniformity and (ii) reports the evaluation of four state-of-the-art samplers: Spur, QuickSampler, Unigen2, and Smarch. According to our experimental results, only Spur satisfies both scalability and uniformity.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {17},
numpages = {11},
keywords = {SAT, configurable systems, software product lines, uniform sampling, variability modeling},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2362536.2362561,
author = {de Oliveira, Thiago Henrique Burgos and Becker, Martin and Nakagawa, Elisa Yumi},
title = {Supporting the analysis of bug prevalence in software product lines with product genealogy},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362561},
doi = {10.1145/2362536.2362561},
abstract = {The term bug prevalence is derived from the medical world vocabulary and applied to Software Product Line (SPL), meaning all products that are affected by one particular bug. In single systems development, this concept is not relevant since a bug is either present or not. However, when it comes to SPL, analyzing the bug prevalence of a certain bug is still a challenge and a highly relevant topic, since the same bug may be present in several products. To support this analysis, the main contribution of this paper is the Product Genealogy approach. A core concept in our approach is the Product Genealogy Tree, in which the hierarchy of products in the SPL is represented, reflecting how each product evolved or was derived from another or from the core assets. In this context, the benefit of such a tree is the rapid visualization of the product's structure in the SPL, providing input on which products are to be examined initially. Besides that, in this paper we introduce a novel analogy between the medical genetics world and SPL in order to better explain the principles of our approach.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {181–185},
numpages = {5},
keywords = {bug prevalence, change impact, product genealogy, software product line},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3307630.3342418,
author = {Rinc\'{o}n, Luisa and Mazo, Ra\'{u}l and Salinesi, Camille},
title = {Analyzing the Convenience of Adopting a Product Line Engineering Approach: An Industrial Qualitative Evaluation},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342418},
doi = {10.1145/3307630.3342418},
abstract = {Engineering Software Product Lines may be a strategy to reduce costs and efforts for developing software and increasing business productivity. However, it cannot be considered as a "silver bullet" that applies to all types of organizations. Companies must consider pros and cons to determine sound reasons and justify its adoption. In previous work, we proposed the APPLIES evaluation framework to help decision-makers find arguments that may justify (or not) adopting a product line engineering approach. This paper presents our experience using this framework in a mid-sized software development company with more than 25 years of experience but without previous experience in product line engineering. This industrial experience, conducted as a qualitative empirical evaluation, helped us to evaluate to what extent APPLIES is practical to be used in a real environment and to gather ideas from real potential users to improve the framework.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {90–97},
numpages = {8},
keywords = {empirical evaluation, product line adoption, product line engineering, qualitative evaluation},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2791060.2791106,
author = {Smiley, Karen and Schmidt, Werner and Dagnino, Aldo},
title = {Evolving an industrial analytics product line architecture},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791106},
doi = {10.1145/2791060.2791106},
abstract = {This paper focuses on an industrial experience with software product lines of analytics-enabled solutions, specifically the evolution of the software product line architecture for a Subject Matter Expert Workbench toolset which supports analytic plugins for multiple software product lines. As context, the toolset product line was intended for integration of expert knowledge into a family of industrial asset health applications at runtime. The toolset architecture is now being evolved to build and manage plugins for multiple Industrial Analytics solutions (software systems and services) beyond asset health. This evolution is driving changes in the desired architecture qualities of the toolset; widening the stakeholder pool and influencing priorities; affecting the architecture tradeoffs and decisions; and triggering updates to the product line architecture, the guidance for applying it, and the current prototype of the toolset. We describe our experiences in handling this evolution, assess lessons learned, and discuss potential relevance to other product line scenarios.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {263–272},
numpages = {10},
keywords = {asset health, extensibility, industrial analytics, interoperability, knowledge, performance, reusability, software product line},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3486609.3487196,
author = {Bragan\c{c}a, Alexandre and Azevedo, Isabel and Bettencourt, Nuno and Morais, Carlos and Teixeira, Diogo and Caetano, David},
title = {Towards supporting SPL engineering in low-code platforms using a DSL approach},
year = {2021},
isbn = {9781450391122},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486609.3487196},
doi = {10.1145/3486609.3487196},
abstract = {Low-code application platforms enable citizen developers to autonomously build complete applications, such as web applications or mobile applications. Some of these platforms also offer support for reuse to facilitate the development of similar applications. The offered mechanisms are usually elementary, they allow module reuse or building a new application from a template. However, they are insufficient to achieve the industrial level reuse necessary for software product lines (SPL). In fact, these platforms were conceived to help build standalone applications, not software families and even fewer software product lines. In this paper, we argue that the major limitation is that these platforms seldom provide access to their metamodel, the access to applications’ models and code is also limited and, therefore, makes it harder to analyze commonality and variability and construct models based on it. An approach is proposed to surpass these limitations: firstly, a metamodel of the applications built with the platform is obtained, and then, based on the metamodel, a domain-specific language (DSL) that can express the models of the applications, including variability, is constructed. With this DSL, users can combine and reuse models from different applications to explore and build similar applications. The solution is illustrated with an industrial case study. A discussion of the results is presented as well as its limitations and related work. The authors hope that this work provides inspiration and some ideas that the community can explore to facilitate the adoption and implementation of SPLs in the context, and supported by, low-code platforms.},
booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {16–28},
numpages = {13},
keywords = {domain specific languages, low-code platforms, software product line engineering},
location = {Chicago, IL, USA},
series = {GPCE 2021}
}

@inproceedings{10.1145/3307630.3342400,
author = {Kamali, Seiede Reyhane and Kasaei, Shirin and Lopez-Herrejon, Roberto E.},
title = {Answering the Call of the Wild? Thoughts on the Elusive Quest for Ecological Validity in Variability Modeling},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342400},
doi = {10.1145/3307630.3342400},
abstract = {Ecological validity is a term commonly used in several disciplines to refer to the fact that in a research study, the methods, the materials, and the settings must approximate the real world, i.e. what happens in everyday life. Variability modeling is no exception, it has striven for this form of validity by looking at two main sources, industrial projects and open source projects. Despite their unquestionable value, industrial projects inherently pose limitations; for instance, in terms of open access or results replication, which are two important tenets for any scientific endeavor. In this paper, we present our first findings on the use of open source projects in variability modeling research, and identify trends and avenues for further research.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {143–150},
numpages = {8},
keywords = {feature models, open source projects, variability models},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2499777.2500717,
author = {Lee, Jaejoon},
title = {Dynamic feature deployment and composition for dynamic software product lines},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500717},
doi = {10.1145/2499777.2500717},
abstract = {We aim to tackle problems with feature interoperability in dynamic software product lines: a feature allows collaborations with other features not conceived when it is deployed. In this position paper, we propose a Dynamic Feature Deployment (DFD) idea, which is a model-driven approach to support seamless integration of new features and changes of product configuration at runtime. The approach is based on a feature-modelling technique that directly deals with flexibility of reusable software assets in software product line engineering. We also propose a Hybrid between Passive/Active Behaviours (Hy-PAB) architecture model to support two extreme sets of behaviours for DFD: an active coordinating behaviour that controls the interactions with other features, and a passive subordinating behaviour that allows other features to control their interactions with other features.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {114–116},
numpages = {3},
keywords = {dynamic software product line, feature interoperability, feature modelling, smart home systems, software architecture},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3336294.3336322,
author = {Pett, Tobias and Th\"{u}m, Thomas and Runge, Tobias and Krieter, Sebastian and Lochau, Malte and Schaefer, Ina},
title = {Product Sampling for Product Lines: The Scalability Challenge},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336322},
doi = {10.1145/3336294.3336322},
abstract = {Quality assurance for product lines is often infeasible for each product separately. Instead, only a subset of all products (i.e., a sample) is considered during testing such that at least the coverage of certain feature interactions is guaranteed. While pair-wise interaction sampling only covers all interactions between two features, its generalization to t-wise interaction sampling ensures coverage for all interactions among t features. However, sampling large product lines poses a challenge, as today's algorithms tend to run out of memory, do not terminate, or produce samples, which are too large to be tested. To initiate a community effort, we provide a set of large real-world feature models with up-to 19 thousand features, which are supposed to be sampled. The performance of sampling approaches is evaluated based on the CPU time and memory consumed to retrieve a sample, the sample size for a given coverage (i.e. the value of t) and whether the sample achieves full t-wise coverage. A well-performing sampling algorithm achieves full t-wise coverage, while minimizing the other properties as best as possible.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {78–83},
numpages = {6},
keywords = {product line testing, product sampling, real-world feature models, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2934466.2934486,
author = {Santos, Alcemir Rodrigues and do Carmo Machado, Ivan and de Almeida, Eduardo Santana},
title = {RiPLE-HC: javascript systems meets spl composition},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934486},
doi = {10.1145/2934466.2934486},
abstract = {Context. Software Product Lines (SPL) engineering is increasingly being applied to handle variability in industrial software systems. Problem. The research community has pointed out a series of benefits which modularity brings to software composition, a key aspect in SPL engineering. However, in practice, the reuse in Javascript-based systems relies on the use of package managers (e.g., npm, jam, bower, requireJS), but these approaches do not allow the management of project features. Method. This paper presents the RiPLE-HC, a strategy aimed at blending compositional and annotative approaches to implement variability in Javascript-based systems. Results. We applied the approach in an industrial environment and conducted an academic case study with six open-source systems to evaluate its robustness and scalability. Additionally, we carried a controlled experiment to analyze the impact of the RiPLE-HC code organization on the feature location maintenance tasks. Conclusion. The empirical evaluations yielded evidence of reduced effort in feature location, and positive benefits when introducing systematic reuse aspects in Javascript-based systems.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {154–163},
numpages = {10},
keywords = {eclipse plugin, feature composition, featureIDE, software product line engineering, web systems domain},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3233027.3233047,
author = {El-Sharkawy, Sascha and Dhar, Saura Jyoti and Krafczyk, Adam and Duszynski, Slawomir and Beichter, Tobias and Schmid, Klaus},
title = {Reverse engineering variability in an industrial product line: observations and lessons learned},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233047},
doi = {10.1145/3233027.3233047},
abstract = {Ideally, a variability model is a correct and complete representation of product line features and constraints among them. Together with a mapping between features and code, this ensures that only valid products can be configured and derived. However, in practice the modeled constraints might be neither complete nor correct, which causes problems in the configuration and product derivation phases. This paper presents an approach to reverse engineer variability constraints from the implementation, and thus improve the correctness and completeness of variability models.We extended the concept of feature effect analysis [22] to extract variability constraints from code artifacts of the Bosch PS-EC large-scale product line. We present an industrial application of the approach and discuss its required modifications to handle non-Boolean variability and heterogeneous artifact types.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {215–225},
numpages = {11},
keywords = {reverse engineering, software product lines, static analysis, variability modeling},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2499777.2500725,
author = {Varshosaz, Mahsa and Khosravi, Ramtin},
title = {Discrete time Markov chain families: modeling and verification of probabilistic software product lines},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500725},
doi = {10.1145/2499777.2500725},
abstract = {Software product line engineering (SPLE) enables systematic reuse in development of a family of related software systems by explicitly defining commonalities and variabilities among the individual products in the family. Nowadays, SPLE is used in a variety of complex domains such as avionics and automotive. As such domains include safety critical systems which exhibit probabilistic behavior, there is a major need for modeling and verification approaches dealing with probabilistic aspects of systems in the presence of variabilities. In this paper, we introduce a mathematical model, Discrete Time Markov Chain Family (DTMCF), which compactly represents the probabilistic behavior of all the products in the product line. We also provide a probabilistic model checking method to verify DTMCFs against Probabilistic Computation Tree Logic (PCTL) properties. This way, instead of verifying each product individually, the whole family is model checked at once, resulting in the set of products satisfying the desired property. This reduces the required cost for model checking by eliminating redundant processing caused by the commonalities among the products.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {34–41},
numpages = {8},
keywords = {probabilistic model checking, software product line, variable discrete time Markov chains},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3382025.3414945,
author = {G\"{o}ttmann, Hendrik and Luthmann, Lars and Lochau, Malte and Sch\"{u}rr, Andy},
title = {Real-time-aware reconfiguration decisions for dynamic software product lines},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414945},
doi = {10.1145/3382025.3414945},
abstract = {Dynamic Software Product Lines (DSPL) have recently shown promising potentials as integrated engineering methodology for (self-)adaptive software systems. Based on the software-configuration principles of software product lines, DSPL additionally foster reconfiguration capabilities to continuously adapt software products to ever-changing environmental contexts. However, in most recent works concerned with finding near-optimal reconfiguration decisions, real-time aspects of reconfiguration processes are usually out of scope. In this paper, we present a model-based methodology for specifying and automatically analyzing real-time constraints of reconfiguration decisions in a feature-oriented and compositional way. Those real-time aware DSPL specifications are internally translated into timed automata, a well-founded formalism for real-time behaviors. This representation allows for formally reasoning about consistency and worst-case/best-case execution-time behaviors of sequences of reconfiguration decisions. The technique is implemented in a prototype tool and experimentally evaluated with respect to a set of case studies1.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {13},
numpages = {11},
keywords = {dynamic software product lines, reconfiguration decisions, timed automata},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/1217935.1217955,
author = {Krishna, Arvind S. and Gokhale, Aniruddha S. and Schmidt, Douglas C.},
title = {Context-specific middleware specialization techniques for optimizing software product-line architectures},
year = {2006},
isbn = {1595933220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1217935.1217955},
doi = {10.1145/1217935.1217955},
abstract = {Product-line architectures (PLAs) are an emerging paradigm for developing software families for distributed real-time and embedded (DRE) systems by customizing reusable artifacts, rather than hand-crafting software from scratch. To reduce the effort of developing software PLAs and product variants for DRE systems, developers are applying general-purpose -- ideally standard -- middleware platforms whose reusable services and mechanisms support a range of application quality of service (QoS) requirements, such as low latency and jitter. The generality and flexibility of standard middleware, however, often results in excessive time/space overhead for DRE systems, due to lack of optimizations tailored to meet the specific QoS requirements of different product variants in a PLA.This paper provides the following contributions to the study of middleware specialization techniques for PLA-based DRE systems. First, we identify key dimensions of generality in standard middleware stemming from framework implementations, deployment platforms, and middleware standards. Second, we illustrate how context-specific specialization techniques can be automated and used to tailor standard middleware to better meet the QoS needs of different PLA product variants. Third, we quantify the benefits of applying automated tools to specialize a standard Realtime CORBA middleware implementation. When applied together, these middleware specializations improved our application product variant throughput by ~65%, average- and worst-case end-to-end latency measures by ~43% and ~45%, respectively, and predictability by a factor of two over an already optimized middleware implementation, with little or no effect on portability, standard middleware APIs, or application software implementations, and interoperability.},
booktitle = {Proceedings of the 1st ACM SIGOPS/EuroSys European Conference on Computer Systems 2006},
pages = {205–218},
numpages = {14},
keywords = {middleware, product lines, specializations},
location = {Leuven, Belgium},
series = {EuroSys '06}
}

@inproceedings{10.1145/2019136.2019153,
author = {Nascimento, Amanda S\'{a}vio and Rubira, Cec\'{\i}lia Mary Fischer and Lee, Jaejoon},
title = {An SPL approach for adaptive fault tolerance in SOA},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019153},
doi = {10.1145/2019136.2019153},
abstract = {It is challenging to apply existing fault tolerance strategies for developing dependable Service Oriented systems, due to the lack of capabilities to adapt themselves at runtime to cope with dynamic changes of (a) user requirements and (b) the level of quality of services (QoS). In order to support such dynamic changes, we propose to adopt Software Product Line techniques. In particular, we adopt a feature model and product line architecture to capture the variability among software fault tolerance strategies based on design diversity. We propose an infrastructure that supports the strategy changes at runtime through dynamic management of variability and is responsible for the dependable mediation logic between service clients and redundant services. The infrastructure has an autonomous controller (i.e. managing a loop of collection, analysis, planning and execution), which is responsible for monitoring the changes of (i) QoS level and (ii) user requirements and decides, in accordance with high-level policies, an appropriate fault tolerance strategy to be executed. Also, our solution allows the dynamic provision of redundant services by describing them in terms of semantics. Finally, we performed a proof of concept which indicates the feasibility of the proposed solution.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {15},
numpages = {8},
keywords = {QoS-aware, autonomic control loop, dependability improvement, dynamic software product line, semantic services},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2491627.2491648,
author = {Krueger, Charles W.},
title = {Multistage configuration trees for managing product family trees},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491648},
doi = {10.1145/2491627.2491648},
abstract = {It is not unusual for commercial product line organizations to manufacture millions of product instances every year, in thousands of different "flavors". The scale and scope of diversity in product lines of this size can be high, creating significant challenges to engineers implementing the product line, product marketers defining the space of available products, and customers selecting from available products. Companies often organize their products into a product family tree to provide clarity about their product groupings and offerings, better enabling their customers to effectively navigate among the huge number of offerings and to efficiently converge on a suitable product instance. This paper describes a 2nd Generation Product Line Engineering (2GPLE) feature modeling structure called a multistage configuration tree that supports the engineering, deployment and maintenance of complex product family trees. Feature selections and downselections are incrementally staged throughout the nodes in a product family tree. Feature decisions made at any node are inherited by all descendants of that node, thereby defining a product family subtree.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {188–197},
numpages = {10},
keywords = {multistage configuration, product family tree, product line engineering, staged configuration, systems and software product lines},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@article{10.1145/3628158,
author = {Xiang, Yi and Huang, Han and Li, Sizhe and Li, Miqing and Luo, Chuan and Yang, Xiaowei},
title = {Automated Test Suite Generation for Software Product Lines Based on Quality-Diversity Optimization},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3628158},
doi = {10.1145/3628158},
abstract = {A Software Product Line (SPL) is a set of software products that are built from a variability model. Real-world SPLs typically involve a vast number of valid products, making it impossible to individually test each of them. This arises the need for automated test suite generation, which was previously modeled as either a single-objective or a multi-objective optimization problem considering only objective functions. This article provides a completely different mathematical model by exploiting the benefits of Quality-Diversity (QD) optimization that is composed of not only an objective function (e.g., t-wise coverage or test suite diversity) but also a user-defined behavior space (e.g., the space with test suite size as its dimension). We argue that the new model is more suitable and generic than the two alternatives because it provides at a time a large set of diverse (measured in the behavior space) and high-performing solutions that can ease the decision-making process. We apply MAP-Elites, one of the most popular QD algorithms, to solve the model. The results of the evaluation, on both realistic and artificial SPLs, are promising, with MAP-Elites significantly and substantially outperforming both single- and multi-objective approaches, and also several state-of-the-art SPL testing tools. In summary, this article provides a new and promising perspective on the test suite generation for SPLs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {46},
numpages = {52},
keywords = {Software Product Line, automated test suite generation, Quality-Diversity (QD) optimization}
}

@inproceedings{10.1145/2648511.2648546,
author = {Van Landuyt, Dimitri and Op de beeck, Steven and Hovsepyan, Aram and Michiels, Sam and Joosen, Wouter and Meynckens, Sven and de Jong, Gjalt and Barais, Olivier and Acher, Mathieu},
title = {Towards managing variability in the safety design of an automotive hall effect sensor},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648546},
doi = {10.1145/2648511.2648546},
abstract = {This paper discusses the merits and challenges of adopting software product line engineering (SPLE) as the main development process for an automotive Hall Effect sensor. This versatile component is integrated into a number of automotive applications with varying safety requirements (e.g., windshield wipers and brake pedals).This paper provides a detailed explanation as to why the process of safety assessment and verification of the Hall Effect sensor is currently cumbersome and repetitive: it must be repeated entirely for every automotive application in which the sensor is to be used. In addition, no support is given to the engineer to select and configure the appropriate safety solutions and to explain the safety implications of his decisions.To address these problems, we present a tailored SPLE-based approach that combines model-driven development with advanced model composition techniques for applying and reasoning about specific safety solutions. In addition, we provide insights about how this approach can reduce the overall complexity, improve reusability, and facilitate safety assessment of the Hall Effect sensor.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {304–309},
numpages = {6},
keywords = {ASIL validation, automotive, hardware/software co-design, safety patterns, software product line engineering},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3336294.3336305,
author = {Fischer, Stefan and Ramler, Rudolf and Linsbauer, Lukas and Egyed, Alexander},
title = {Automating Test Reuse for Highly Configurable Software},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336305},
doi = {10.1145/3336294.3336305},
abstract = {Dealing with highly configurable systems is generally very complex. Hundreds of different analysis techniques have been conceived to deal with different aspects of configurable systems. One large focal point is the testing of configurable software. This is challenging due to the large number of possible configurations and because tests themselves are rarely configurable and instead built for specific configurations. Existing tests can usually not be reused on other configurations. Therefore, tests need to be adapted for the specific configuration they are supposed to test. In this paper we report on an experiment about reusing tests in a configurable system. We used manually developed tests for specific configurations of Bugzilla and investigated which of them could be reused for other configurations. Moreover, we automatically generated new test variants (by automatically reusing from existing ones) for combinations of previous configurations. Our results showed that we can directly reuse some tests for configurations which they were not intended for. Nonetheless, our automatically generated test variants generally yielded better results. When applying original tests to new configurations we found an average success rate for the tests of 81,84%. In contrast, our generated test variants achieved an average success rate of 98,72%. This is an increase of 16,88%.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {1–11},
numpages = {11},
keywords = {clone-and-own, configurable software, reuse, testing, variability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2491627.2491629,
author = {Clements, Paul and Krueger, Charles and Shepherd, James and Winkler, Andrew},
title = {A PLE-based auditing method for protecting restricted content in derived products},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491629},
doi = {10.1145/2491627.2491629},
abstract = {Many organizations that produce a portfolio of products for different customers need to ensure that sensitive or restricted content that may appear in some products must not appear in others. Examples of this need include complying with statutes in different countries of sale, protection of intellectual property developed specifically for one customer, and more. For organizations operating under these requirements and producing their products under a product line engineering paradigm that relies on automation in product derivation, there is a need for a method to ensure that the content restrictions have been met in the derived products. This paper describes an auditing method that meets this need. It was created for use in the Second Generation Product Line Engineering approach that is being applied by Lockheed Martin in their AEGIS ship combat system product line.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {218–226},
numpages = {9},
keywords = {bill-of-features, feature modeling, feature profiles, hierarchical product lines, product audit, product baselines, product configurator, product derivation, product line engineering, product portfolio, second generation product line engineering, software product lines, variation points},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2499777.2499783,
author = {Kalender, Mert Emin and T\"{u}z\"{u}n, Eray and Tekinerdogan, Bedir},
title = {Decision support for adopting SPLE with Transit-PL},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499783},
doi = {10.1145/2499777.2499783},
abstract = {It is generally acknowledged that the decision to adopt a software product line engineering (SPLE) approach needs to be performed carefully due to the different risks involved in taking such an important decision. To mitigate the potential risks of the transition to SPLE, several studies have been proposed that include many different rules for analyzing the feasibility of the SPLE adoption and the selection of transition process. However, it is not easy to apply these manually and likewise provide a proper decision with the corresponding justification. In this paper, we propose the tool Transit-PL, a web based decision support system for analyzing the feasibility of SPLE for an organization and selecting the appropriate transition strategy. Transit-PL provides a framework to build particular decision support system for selected strategies using different types of questions and corresponding rules and set of answers.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {150–153},
numpages = {4},
keywords = {decision support systems, software product line engineering},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3336294.3342359,
author = {Oh, Jeho and Gazzillo, Paul and Batory, Don},
title = {t-wise Coverage by Uniform Sampling},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342359},
doi = {10.1145/3336294.3342359},
abstract = {Efficiently testing large configuration spaces of Software Product Lines (SPLs) needs a sampling algorithm that is both scalable and provides good t-wise coverage. The 2019 SPLC Sampling Challenge provides large real-world feature models and asks for a t-wise sampling algorithm that can work for those models.We evaluated t-wise coverage by uniform sampling (US) the configurations of one of the provided feature models. US means that every (legal) configuration is equally likely to be selected. US yields statistically representative samples of a configuration space and can be used as a baseline to compare other sampling algorithms.We used existing algorithm called Smarch to uniformly sample SPL configurations. While uniform sampling alone was not enough to produce 100% 1-wise and 2-wise coverage, we used standard probabilistic analysis to explain our experimental results and to conjecture how uniform sampling may enhance the scalability of existing t-wise sampling algorithms.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {84–87},
numpages = {4},
keywords = {t-wise coverage, software product lines, uniform sampling},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2019136.2019186,
author = {Corr\^{e}a, Chessman K. F.},
title = {Towards automatic consistency preservation for model-driven software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019186},
doi = {10.1145/2019136.2019186},
abstract = {Model-Driven Software Product Line Engineering (MD-SPLE) is the combination of Model-Driven Software Development and Software Product Line Engineering. In this paradigm, there is a strong dependency relationship between meta-models, models, transformation specifications and traceability links. Moreover, there are also dependency relationships between core assets and product specific artifacts, which raise dependency complexity. Due to the pressure to release new product versions and the quantity of dependent elements, there is a probability that models and related artifacts are not updated at all and become inconsistent with each other. The proposal of this thesis is to keep MD-SPLE models, meta-models, transformation specifications and traceability links consistent.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {43},
numpages = {7},
keywords = {consistency, evolution, maintenance, model-driven, software product line},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3483899.3483905,
author = {Freire, Willian and Tonh\~{a}o, Simone and Bonetti, Tiago and Shigenaga, Marcelo and Cadette, William and Felizardo, Fernando and Amaral, Aline and OliveiraJr, Edson and Colanzi, Thelma},
title = {On the configuration of multi-objective evolutionary algorithms for PLA design optimization},
year = {2021},
isbn = {9781450384193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483899.3483905},
doi = {10.1145/3483899.3483905},
abstract = {Search-based algorithms have been successfully applied in the Product Line Architecture (PLA) optimization using the seminal approach called Multi-Objective Approach for Product-Line Architecture Design (MOA4PLA). This approach produces a set of alternative PLA designs intending to improve the different factors being optimized. Currently, the MOA4PLA uses the NSGA-II algorithm, a multi-objective evolutionary algorithm (MOEA) that can optimize several architectural properties simultaneously. Despite the promising results, studying the best values for the algorithm parameters is essential to obtain even better results. This is also crucial to ease the adoption of MOA4PLA by newcomers or non-expert companies willing to start using search-based software engineering to PLA design. Three crossover operators for the PLA design optimization were proposed recently. However, reference values for parameters have not been defined for PLA design optimization using crossover operators. In this context, the objective of this work is conducting an experimental study to discover which are the most effective crossover operators and the best values to configure the MOEA parameters, such as population size, number of generations, and mutation and crossover rates. A quantitative analysis based on quality indicators and statistical tests was performed using four PLA designs to determine the most suitable parameter values to the search-based algorithm. Empirical results pointed out the best combination of crossover operators and the most suitable values to configure MOA4PLA.},
booktitle = {Proceedings of the 15th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Multi-objective evolutionary algorithm, recombination operators, software architecture, software product line},
location = {Joinville, Brazil},
series = {SBCARS '21}
}

@inproceedings{10.5555/998675.999419,
author = {Matinlassi, Mari},
title = {Comparison of Software Product Line Architecture Design Methods: COPA, FAST, FORM, KobrA and QADA},
year = {2004},
isbn = {0769521630},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Product line architectures (PLAs) have been undercontinuous attention in the software research communityduring the past few years. Although several methods havebeen established to create PLAs there are not availablestudies comparing PLA methods. Five methods are knownto answer the needs of software product lines: COPA,FAST, FORM, KobrA and QADA. In this paper, anevaluation framework is introduced for comparing PLAdesign methods. The framework considers the methodsfrom the points of view of method context, user, structureand validation. Comparison revealed distinguishableideologies between the methods. Therefore, methods donot overlap even though they all are PLA design methods.All the methods have been validated on various domains.The most common domains are telecommunicationinfrastructure and information domains. Some of themethods apply software standards; at least OMG\'{y}s MDAfor method structures, UML for language and IEEE Std-1471-2000 for viewpoint definitions.},
booktitle = {Proceedings of the 26th International Conference on Software Engineering},
pages = {127–136},
numpages = {10},
series = {ICSE '04}
}

@inproceedings{10.1145/3109729.3109740,
author = {McGee, Ethan T. and McGregor, John D.},
title = {A Realization Effort Estimation Model for Dynamic Software Product Lines},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109740},
doi = {10.1145/3109729.3109740},
abstract = {Decision Support Frameworks, information systems that guide business and organization choices, are often used to help planners and designers make complex choices. These frameworks provide a means of aggregating the data needed to successfully reach a decision while providing a means of logically analyzing the compiled information. The framework can also provide additional information to help acclimate a planner or designer to special considerations of a domain if they are unfamiliar with the area.Dynamic Software Product Lines, product lines which can self-adapt their architecture at run-time, are complex systems usually constructed to cope with high levels of run-time uncertainty. During construction, selecting an appropriate design from among the set of possible designs for a Dynamic Software Product Line is a non-trivial task, and no existing Decision Support Framework provides guidance for this process. In this paper, we present a framework, Structured Intuitive Model for Dynamic Adaptive System Economics, for comparing and selecting between designs for a Dynamic Software Product Line.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {111–116},
numpages = {6},
keywords = {Architecture Analysis, Dynamic Software Product Lines, Effort Estimation},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@article{10.1145/1279711.1279715,
author = {Krishna, Arvind S. and Gokhale, Aniruddha and Schmidt, Douglas C. and Ranganath, Venkatesh Prasad and Hatcliff, John},
title = {Towards highly optimized real-time middleware for software product-line architectures},
year = {2006},
issue_date = {January 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/1279711.1279715},
doi = {10.1145/1279711.1279715},
abstract = {This paper provides the following contributions to the study of middleware optimization techniques for product line architectures in real-time systems. First, we identify different dimensions of generality in standards based middleware implementations. Second, we describe how specialization approaches used in other domains including OS, compiler and programming languages can be applied to address middleware generality challenges. Third, we present preliminary results from the application of our specialization techniques. Our results illustrate that specialization techniques represent a promising approach for minimizing time/space overheads in middleware.},
journal = {SIGBED Rev.},
month = jan,
pages = {13–16},
numpages = {4}
}

@inproceedings{10.1145/2934466.2962730,
author = {Bezerra, Carla I. M. and Barbosa, Jefferson and Freires, Joao Holanda and Andrade, Rossana M. C. and Monteiro, Jos\'{e} Maria},
title = {DyMMer: a measurement-based tool to support quality evaluation of DSPL feature models},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2962730},
doi = {10.1145/2934466.2962730},
abstract = {For Dynamic Software Product Lines (DSPLs), evaluating the quality of a feature model is important to ensure that errors in the early stages do not spread throughout the DSPL. Measures extracted from feature models have been proved to be useful in the quality evaluation of such models. However, the process used for computing the values of these quality measures for a large set of feature models can be cumbersome and error prone. To cope with this problem, we present DyMMer, a tool to support the automatic extraction of quality measures from feature models in DSPLs. After that, we can analyse the results and propose improvements for the feature models. Currently, the DyMMer tool is able to collect 40 different quality measures from a DSPL feature model.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {314–317},
numpages = {4},
keywords = {dynamic software product lines, feature models, quality measures},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3106195.3106218,
author = {Krueger, Charles and Clements, Paul},
title = {Enterprise Feature Ontology for Feature-based Product Line Engineering and Operations},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106218},
doi = {10.1145/3106195.3106218},
abstract = {Feature trees have been the standard data structure for representing product diversity in feature-based systems and software product line engineering (PLE). For basic product lines of modest size or complexity, one or several modular feature trees can be sufficient for managing the and resolving the variation present across the engineering assets in the systems engineering 'V' --- from requirements, to design, through implementation, verification, validation, documentation, and more --- in the software, mechanical, and electrical disciplines. However, enterprises seeking to adopt PLE at all levels of their organization, including areas such as product marketing, portfolio planning, manufacturing, supply chain, product sales, product service and maintenance, Internet-of-Things, resource planning, and much more are finding that thousands of nonengineering users need different views and interaction scenarios with a feature diversity representation. This paper describes a feature ontology (a specification of the meaning of terms in the feature modeling realm) that is suitable for managing the feature-based product line engineering and operations in the largest and most complex product line organizations. This ontology is based on layers of abstraction that each incrementally constrain the complexity and combinatorics and targets specific roles in the organization for greater degrees of efficiency, precision, and automation across an entire business enterprise.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {227–236},
numpages = {10},
keywords = {PLE factory, Product line engineering, bill-of-features, enterprise feature ontology, feature modeling, feature profiles, feature-based product line engineering, product configurator, product portfolio, software product lines, variation points},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3350768.3351299,
author = {de Oliveira, Davi Cedraz S. and Bezerra, Carla I. M.},
title = {Development of the Maintainability Index for SPLs Feature Models Using Fuzzy Logic},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3351299},
doi = {10.1145/3350768.3351299},
abstract = {The variability of the common features in an Software Product Line (SPL) can be managed by an feature model, an artifact that consist of a tree-shaped diagram, that describe the features identified in the products and the possible relationships between them. Guarantee the quality of the feature model may be essential to ensure that errors do not propagate across all products. The process of evaluating the quality of a product or artifact can be done using measures, which may reflect the characteristics, sub-characteristics or attributes of quality. However, the isolated values of each measure do not allow access to a whole quality of the feature model, since most of the measures cover several specific aspects that are not correlated. In this context, this paper proposes the aggregation of measures in order to evaluate the maintainability of the feature model in SPL. We aim to investigate how to aggregate these measures and access the respective sub-characteristics by means of a single aggregate value that has the same available information as a set of measures. For this, we have used the theory of Fuzzy Logic as a technique for aggregation of these measures. The new aggregate measure represents the maintainability index of a feature models (MIFM) was obtained. Moreover, to evaluate the MIFM, we applied it to a set of models. It was verified that the aggregate measure obtained allows to measure if a feature models has a high or low maintainability index, supporting the domain engineer in the evaluation of the maintenance of the feature model in a faster and more precise way.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {357–366},
numpages = {10},
keywords = {Feature Models, Fuzzy Logic, Measures, Quality Evaluation, Software Product Line},
location = {Salvador, Brazil},
series = {SBES '19}
}

@inproceedings{10.1145/3307630.3342401,
author = {Villota, Angela and Mazo, Ra\'{u}l and Salinesi, Camille},
title = {The High-Level Variability Language: An Ontological Approach},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342401},
doi = {10.1145/3307630.3342401},
abstract = {Given its relevance, there is an extensive body of research for modeling variability in diverse domains. Regretfully, the community still faces issues and challenges to port or share variability models among tools and methodological approaches. There are researchers, for instance, implementing the same algorithms and analyses again because they use a specific modeling language and cannot use some existing tool. This paper introduces the High-Level Variability Language (HLVL), an expressive and extensible textual language that can be used as a modeling and an intermediate language for variability. HLVL was designed following an ontological approach, i.e., by defining their elements considering the meaning of the concepts existing on different variability languages. Our proposal not only provides a unified language based on a comprehensive analysis of the existing ones but also sets foundations to build tools that support different notations and their combination.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {162–169},
numpages = {8},
keywords = {domain specific language, variability language, variability specification},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2364412.2364445,
author = {Martinez, Jabier and Thurimella, Anil Kumar},
title = {Collaboration and source code driven bottom-up product line engineering},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364445},
doi = {10.1145/2364412.2364445},
abstract = {Companies that develop similar software systems often transition from single-system development to software product line development. In this transition, reusable assets are identified and incrementally created over a period of time. Bottom-up Software Product Line Engineering approaches aid stakeholders to identify variability from the legacy artifacts. One of these artifacts is the legacy source code. In this paper, we contribute the Collaboration and Source Code Driven Bottom-up approach, with two main enhancements. We apply clone detection and architecture reengineering techniques for identifying variability from the legacy artifacts. These techniques which have been traditionally used for maintaining software are now used for identifying variability and analyze code coupling and cohesion from the legacy code. Our second enhancement is improving stakeholder collaboration by guiding the domain experts in order to decide on variability. In particular, we apply Questions, Options and Criteria technique for capturing rationale and supporting collaboration.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {196–200},
numpages = {5},
keywords = {architecture reengineering, clone detection, knowledge management, rationale, software product line engineering, variability modeling},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3307630.3342417,
author = {Achtaich, Asmaa and Roudies, Ounsa and Souissi, Nissrine and Salinesi, Camille and Mazo, Ra\'{u}l},
title = {Evaluation of the State-Constraint Transition Modelling Language: A Goal Question Metric Approach},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342417},
doi = {10.1145/3307630.3342417},
abstract = {Self-adaptive systems (SAS) are exceptional systems, on account of their versatile composition, dynamic behavior and evolutive nature. Existing formal languages for the specification of SAS focus on adapting system elements to achieve a target goal, following specific rules, without much attention on the adaptation of requirements themselves. The State-Constraint Transition (SCT) modeling language enables the specification of dynamic requirements, both at the domain and application level, as a result of space or time variability. This language, evaluated in this paper, enables the specification of a variety of requirement types, for SASs from different domains, while generating a configuration, all configurations, and number of possible configurations, in milliseconds. This paper presents these results, namely; expressiveness, domain independence and scalability, from the viewpoint of designers and domain engineers, following a goal-question-metric approach. However, being primarily based on constraint programming (CP), the language suffers from drawbacks inherited from this paradigm, specifically time related requirements, like (e.g. order, frequency and staged requirements).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {106–113},
numpages = {8},
keywords = {IoT, constraint programming, dynamic software product lines, modeling language, state machine},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3180155.3180159,
author = {Krieter, Sebastian and Th\"{u}m, Thomas and Schulze, Sandro and Schr\"{o}ter, Reimar and Saake, Gunter},
title = {Propagating configuration decisions with modal implication graphs},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180159},
doi = {10.1145/3180155.3180159},
abstract = {Highly-configurable systems encompass thousands of interdependent configuration options, which require a non-trivial configuration process. Decision propagation enables a backtracking-free configuration process by computing values implied by user decisions. However, employing decision propagation for large-scale systems is a time-consuming task and, thus, can be a bottleneck in interactive configuration processes and analyses alike. We propose modal implication graphs to improve the performance of decision propagation by precomputing intermediate values used in the process. Our evaluation results show a significant improvement over state-of-the-art algorithms for 120 real-world systems.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {898–909},
numpages = {12},
keywords = {configuration, decision propagation, software product line},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3307630.3342396,
author = {Markiegi, Urtzi and Arrieta, Aitor and Etxeberria, Leire and Sagardui, Goiuria},
title = {White-Box and Black-Box Test Quality Metrics for Configurable Simulation Models},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342396},
doi = {10.1145/3307630.3342396},
abstract = {Simulation models are widely employed to model and simulate complex systems from different domains, such as automotive. These systems are becoming highly configurable to support different users' demands. Testing all of them is impracticable, and thus, cost-effective techniques are mandatory. Costs are usually attributed either to the time it takes to test a configurable system or to its monetary value. Nevertheless, for the case of test effectiveness several quality metrics can be found in the literature. This paper aims at proposing both black-box and white-box test quality metrics for configurable simulation models relying on 150% variability modeling approaches.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {211–214},
numpages = {4},
keywords = {product lines, simulation models, test quality metrics},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3382025.3414966,
author = {Basile, Davide and Beek, Maurice H. ter and Cordy, Maxime and Legay, Axel},
title = {Tackling the equivalent mutant problem in real-time systems: the 12 commandments of model-based mutation testing},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414966},
doi = {10.1145/3382025.3414966},
abstract = {Mutation testing can effectively drive test generation to reveal faults in software systems. However, it faces a typical efficiency issue as it can produce many mutants that are equivalent to the original system, making it impossible to generate test cases from them.We consider this problem when model-based mutation testing is applied to real-time system product lines, represented as timed automata. We define novel, time-specific mutation operators and formulate the equivalent mutant problem in the frame of timed refinement relations.Further, we study in which cases a mutation yields an equivalent mutant. Our theoretical results provide guidance to system engineers, allowing them to eliminate mutations from which no test case can be produced. Our evaluation, based on a proof-of-concept tool and an industrial case from the automotive domain, confirms the validity of our theory and demonstrates that our approach can eliminate many of the equivalent mutants (88% in our case study).},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {30},
numpages = {11},
keywords = {mutation-based testing, real-time systems, software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/1151433.1151437,
author = {Ajila, Samuel A. and Bailetti, Antonio J. and Dumitrescu, Razvan T.},
title = {Experience report on software product line evolution due to market reposition},
year = {2004},
isbn = {9781450378185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1151433.1151437},
doi = {10.1145/1151433.1151437},
abstract = {This paper presents the result of a study on the changes that occurred in the product line of a telecommunication equipments supplier because of the top management decision to change the product line's target market. The study examines six years of data and identifies potential relationships between changes in the product line and changes in the company's customer, inner context, and product layers. Some of the key findings are: (i) Sales are negatively related to product line growth and positively related to design turnover and the number of designers assigned to the product line. (ii) There is no relationship between the size of the code added to the product line and the number of designers required to develop and test it. (iii) There is a positive relationship between designer turnover and impact of change. (iv) The market downturn has an explicit impact on software development activities.},
booktitle = {Proceedings of the 2004 Workshop on Quantitative Techniques for Software Agile Process},
pages = {25–33},
numpages = {9},
keywords = {change, customer, designer, metrics, product line evolution, software product},
location = {Newport Beach, California},
series = {QUTE-SWAP '04}
}

@inproceedings{10.1145/3106195.3106209,
author = {T\"{e}rnava, Xhevahire and Collet, Philippe},
title = {Early Consistency Checking between Specification and Implementation Variabilities},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106209},
doi = {10.1145/3106195.3106209},
abstract = {In a software product line (SPL) engineering approach, the addressed variability in core-code assets must be consistent with the specified domain variability, usually captured in a variability model, e.g., a feature model. Currently, the support for checking such consistency is limited, mostly when a single variability implementation technique is used, e.g., preprocessors in C. In realistic SPLs, variability is implemented using a combined set of traditional techniques, e.g., inheritance, overloading, design patterns. An inappropriate choice and combination of such techniques become the source of variability inconsistencies. In this paper, we present a tooled approach to check the consistency of variability between the specification and implementation levels, when several variability implementation techniques are used together. The proposed method models the implemented variability in terms of variation points and variants, in a forest-like structure, and uses slicing to partially check the resulting propositional formulas at both levels. As a result, it offers an early and automatic detection of inconsistencies when the mapping of variability between both levels is ideal, and with a possible extension to 1 -- to -- m mapping. We implemented and successfully applied the approach in four case studies. Our implementation, publicly available, detects inconsistencies in a very short time, which makes possible to ensure consistency earlier in the development process.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {29–38},
numpages = {10},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3336294.3336315,
author = {Wolschke, Christian and Becker, Martin and Schneickert, S\"{o}ren and Adler, Rasmus and MacGregor, John},
title = {Industrial Perspective on Reuse of Safety Artifacts in Software Product Lines},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336315},
doi = {10.1145/3336294.3336315},
abstract = {In the future, safety-critical industrial products will have to be maintained and variants will have to be produced. In order to do this economically, the safety artifacts of the components should also be reused. At present, however, it is still unclear how this reuse could take place. Moreover this reuse is complicated, by the different situations in the various industries involved and by the corresponding standards applied.Current industrial practice for certification processes relies on a component-based view of reuse. We investigate the possibilities of product lines with managed processes for reuse also across multiple domains.In order to identify the challenges and possible solutions, we conducted interviews with industry partners from the domains of ICT, Rail, Automotive, and Industrial Automation, and from small- and medium-sized enterprises to large organizations. The semi-structured interviews identified the characteristics of current safety engineering processes, the handling of general variety and reuse, the approach followed for safety artifacts, and the need for improvement.In addition, a detailed literature survey summarizes existing approaches. We investigate which modularity concepts exist for dealing with safety, how variability concepts integrate safety, by which means process models can consider safety, and how safety cases are evolved while maintenance takes place. An overview of similar research projects complements the analysis.The identified challenges and potential solution proposals show how safety is related to Software Product Lines.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {143–154},
numpages = {12},
keywords = {modular safety, open source certification, product line certification, safety reuse, safety standards},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2934466.2946045,
author = {Noir, J\'{e}rome Le and Madel\'{e}nat, S\'{e}bastien and Gailliard, Gr\'{e}gory and Labreuche, Christophe and Acher, Mathieu and Barais, Olivier and Constant, Olivier},
title = {A decision-making process for exploring architectural variants in systems engineering},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946045},
doi = {10.1145/2934466.2946045},
abstract = {In systems engineering, practitioners shall explore numerous architectural alternatives until choosing the most adequate variant. The decision-making process is most of the time a manual, time-consuming, and error-prone activity. The exploration and justification of architectural solutions is ad-hoc and mainly consists in a series of tries and errors on the modeling assets. In this paper, we report on an industrial case study in which we apply variability modeling techniques to automate the assessment and comparison of several candidate architectures (variants). We first describe how we can use a model-based approach such as the Common Variability Language (CVL) to specify the architectural variability. We show that the selection of an architectural variant is a multi-criteria decision problem in which there are numerous interactions (veto, favor, complementary) between criteria.We present a tooled process for exploring architectural variants integrating both CVL and the MYRIAD method for assessing and comparing variants based on an explicit preference model coming from the elicitation of stakeholders' concerns. This solution allows understanding differences among variants and their satisfactions with respect to criteria. Beyond variant selection automation improvement, this experiment results highlight that the approach improves rationality in the assessment and provides decision arguments when selecting the preferred variants.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {277–286},
numpages = {10},
keywords = {architecture, decision-making, design exploration, model-driven engineering, multi-criteria decision analysis, systems engineering},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3307630.3342419,
author = {Ghofrani, Javad and Kozegar, Ehsan and Bozorgmehr, Arezoo and Soorati, Mohammad Divband},
title = {Reusability in Artificial Neural Networks: An Empirical Study},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342419},
doi = {10.1145/3307630.3342419},
abstract = {Machine learning, especially deep learning has aroused interests of researchers and practitioners for the last few years in development of intelligent systems such as speech, natural language, and image processing. Software solutions based on machine learning techniques attract more attention as alternatives to conventional software systems. In this paper, we investigate how reusability techniques are applied in implementation of artificial neural networks (ANNs). We conducted an empirical study with an online survey among experts with experience in developing solutions with ANNs. We analyze the feedback of more than 100 experts to our survey. The results show existing challenges and some of the applied solutions in an intersection between reusability and ANNs.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {122–129},
numpages = {8},
keywords = {artificial neural networks, empirical study, reusability, survey, systematic reuse},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3336294.3336307,
author = {Damasceno, Carlos Diego N. and Mousavi, Mohammad Reza and Simao, Adenilso},
title = {Learning from Difference: An Automated Approach for Learning Family Models from Software Product Lines},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336307},
doi = {10.1145/3336294.3336307},
abstract = {Substantial effort has been spent on extending specification notations and their associated reasoning techniques to software product lines (SPLs). Family-based analysis techniques operate on a single artifact, referred to as a family model, that is annotated with variability constraints. This modeling approach paves the way for efficient model-based testing and model checking for SPLs. Albeit reasonably efficient, the creation and maintenance of family models tend to be time consuming and error-prone, especially if there are crosscutting features. To tackle this issue, we introduce FFSMDiff, a fully automated technique to learn featured finite state machines (FFSM), a family-based formalism that unifies Mealy Machines from SPLs into a single representation. Our technique incorporates variability to compare and merge Mealy machines and annotate states and transitions with feature constraints. We evaluate our technique using 34 products derived from three different SPLs. Our results support the hypothesis that families of Mealy machines can be effectively merged into succinct FFSMs with fewer states, especially if there is high feature sharing among products. These indicate that FFSMDiff is an efficient family-based model learning technique.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {52–63},
numpages = {12},
keywords = {150% model, family model, model learning, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3336294.3336298,
author = {Cashman, Mikaela and Firestone, Justin and Cohen, Myra B. and Thianniwet, Thammasak and Niu, Wei},
title = {DNA as Features: Organic Software Product Lines},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336298},
doi = {10.1145/3336294.3336298},
abstract = {Software product line engineering is a best practice for managing reuse in families of software systems. In this work, we explore the use of product line engineering in the emerging programming domain of synthetic biology. In synthetic biology, living organisms are programmed to perform new functions or improve existing functions. These programs are designed and constructed using small building blocks made out of DNA. We conjecture that there are families of products that consist of common and variable DNA parts, and we can leverage product line engineering to help synthetic biologists build, evolve, and reuse these programs. As a first step towards this goal, we perform a domain engineering case study that leverages an open-source repository of more than 45,000 reusable DNA parts. We are able to identify features and their related artifacts, all of which can be composed to make different programs. We demonstrate that we can successfully build feature models representing families for two commonly engineered functions. We then analyze an existing synthetic biology case study and demonstrate how product line engineering can be beneficial in this domain.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {108–118},
numpages = {11},
keywords = {BioBricks, software product lines, synthetic biology},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3382025.3414960,
author = {Str\"{u}der, Stefan and Mukelabai, Mukelabai and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Feature-oriented defect prediction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414960},
doi = {10.1145/3382025.3414960},
abstract = {Software errors are a major nuisance in software development and can lead not only to reputation damages, but also to considerable financial losses for companies. Therefore, numerous techniques for predicting software defects, largely based on machine learning methods, have been developed over the past decades. These techniques usually rely on code and process metrics in order to predict defects at the granularity of typical software assets, such as subsystems, components, and files. In this paper, we present the first systematic investigation of feature-oriented defect prediction: the prediction of defects at the granularity of features---domain-oriented entities abstractly representing (and often cross-cutting) typical software assets. Feature-oriented prediction can be beneficial, since: (i) particular features might be more error-prone than others, (ii) characteristics of features known as defective might be useful to predict other error-prone features, (iii) feature-specific code might be especially prone to faults arising from feature interactions. We present a dataset derived from 12 software projects and introduce two metric sets for feature-oriented defect prediction. We evaluated seven machine learning classifiers with three different attribute sets each, using our two new metric sets as well as an existing metric set from the literature. We observe precision and recall values of around 85% and better robustness when more diverse metrics sets with richer feature information are used.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {21},
numpages = {12},
keywords = {classification, defect, feature, prediction},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2934466.2934471,
author = {Th\"{u}m, Thomas and Ribeiro, M\'{a}rcio and Schr\"{o}ter, Reimar and Siegmund, Janet and Dalton, Francisco},
title = {Product-line maintenance with emergent contract interfaces},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934471},
doi = {10.1145/2934466.2934471},
abstract = {A software product line evolves whenever one of its products need to evolve. Maintenance of preprocessor-based product lines is a difficult task, as changes to the code base may unintentionally influence the behavior of uninvolved products. Hence, developers should be supported during maintenance. We present emergent contract interfaces to make product-line development more efficient and less error-prone. The key idea is that for a given maintenance point (i.e., an assignment), we calculate (a) features in the source code that may be affected and (b) assertions based on contracts defined in the code base. By means of a controlled experiment, we provide empirical evidence regarding efficiency and error-avoidance with emergent contract interfaces.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {134–143},
numpages = {10},
keywords = {design by contract, evolution, maintenance, preprocessor variability, software product lines, weakest precondition},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3233027.3233044,
author = {Al-Hajjaji, Mustafa and Schulze, Michael and Ryssel, Uwe},
title = {Similarity analysis of product-line variants},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233044},
doi = {10.1145/3233027.3233044},
abstract = {Many existing approaches have exploited the similarity notion to analyze software systems. In product-line engineering, similarity notion has been considered to fulfill analysis objectives, such as improving the testing effectiveness and reducing the testing efforts. However, most of the existing approaches consider in the similarity measurement only information of high level of abstraction, such as the feature selections of variants. In this paper, we present the notion of similarity in product-line engineering using different types of problem-space as well as solution-space information. In particular, we discuss different scenarios of measuring the similarity between variants and the possibility of combining different types of information to output the similarity between the compared variants. Moreover, we realized these scenarios in the industrial variant management tool pure::variants to fulfill analysis functionalities.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {226–235},
numpages = {10},
keywords = {highly configurable systems, similarity, software product lines, variants analysis},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2791060.2791099,
author = {Filho, Jo\~{a}o Bosco Ferreira and Allier, Simon and Barais, Olivier and Acher, Mathieu and Baudry, Benoit},
title = {Assessing product line derivation operators applied to Java source code: an empirical study},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791099},
doi = {10.1145/2791060.2791099},
abstract = {Product Derivation is a key activity in Software Product Line Engineering. During this process, derivation operators modify or create core assets (e.g., model elements, source code instructions, components) by adding, removing or substituting them according to a given configuration. The result is a derived product that generally needs to conform to a programming or modeling language. Some operators lead to invalid products when applied to certain assets, some others do not; knowing this in advance can help to better use them, however this is challenging, specially if we consider assets expressed in extensive and complex languages such as Java. In this paper, we empirically answer the following question: which product line operators, applied to which program elements, can synthesize variants of programs that are incorrect, correct or perhaps even conforming to test suites? We implement source code transformations, based on the derivation operators of the Common Variability Language. We automatically synthesize more than 370,000 program variants from a set of 8 real large Java projects (up to 85,000 lines of code), obtaining an extensive panorama of the sanity of the operations.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {36–45},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2791060.2791109,
author = {Krueger, Charles W.},
title = {Mechanical product lifecycle management meets product line engineering},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791109},
doi = {10.1145/2791060.2791109},
abstract = {Early generation Software Product Line (SPL) engineering has evolved into Systems and Software Product Line Engineering (PLE) approaches that extend well beyond the original focus on source code, to a more holistic perspective of the engineering lifecycle. PLE tools and methods in commercial practice today support variation management in requirements, architecture, design models, source code, documentation, configuration data, test cases and more. One of the last lifecycle holdouts from PLE has been mechanical engineering, or Product Lifecycle Management (PLM). The engineering complexity of mechanical product families with embedded software has increased to a threshold where it is intractable for mechanical and software product line engineering to remain disjoint. This paper explores the convergence of mechanical, systems and software product line engineering and why it has been slow to emerge. The reasons are based both on conceptual misalignment among the traditionally distinct disciplines, as well the differences between the physics of mechanical and software systems. The Aras Innovator / BigLever Gears Bridge, an example PLM and PLE integration, is used to illustrate key concepts.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {316–320},
numpages = {5},
keywords = {bill of features, bill of materials, mechanical product line engineering, product lifecycle management},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3233027.3233035,
author = {Varshosaz, Mahsa and Al-Hajjaji, Mustafa and Th\"{u}m, Thomas and Runge, Tobias and Mousavi, Mohammad Reza and Schaefer, Ina},
title = {A classification of product sampling for software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233035},
doi = {10.1145/3233027.3233035},
abstract = {The analysis of software product lines is challenging due to the potentially large number of products, which grow exponentially in terms of the number of features. Product sampling is a technique used to avoid exhaustive testing, which is often infeasible. In this paper, we propose a classification for product sampling techniques and classify the existing literature accordingly. We distinguish the important characteristics of such approaches based on the information used for sampling, the kind of algorithm, and the achieved coverage criteria. Furthermore, we give an overview on existing tools and evaluations of product sampling techniques. We share our insights on the state-of-the-art of product sampling and discuss potential future work.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {1–13},
numpages = {13},
keywords = {domain models, feature interaction, sampling algorithms, software product lines, testing},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2737182.2737183,
author = {Myll\"{a}rniemi, Varvana and Raatikainen, Mikko and M\"{a}nnist\"{o}, Tomi},
title = {Representing and Configuring Security Variability in Software Product Lines},
year = {2015},
isbn = {9781450334709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737182.2737183},
doi = {10.1145/2737182.2737183},
abstract = {In a software product line, security may need to be varied. Consequently, security variability must be managed both from the customer and product line architecture point of view. We utilize design science to build an artifact and a generalized design theory for representing and configuring security and functional variability from the requirements to the architecture in a configurable software product line. An open source web shop product line, Magento, is used as a case example to instantiate and evaluate the contribution. The results indicate that security variability can be represented and distinguished as countermeasures; and that a configurator tool is able to find consistent products as stable models of answer set programs.},
booktitle = {Proceedings of the 11th International ACM SIGSOFT Conference on Quality of Software Architectures},
pages = {1–10},
numpages = {10},
keywords = {security, software architecture, software product line, variability},
location = {Montr\'{e}al, QC, Canada},
series = {QoSA '15}
}

@inproceedings{10.1145/2934466.2934480,
author = {Martinez, Jabier and Ziadi, Tewfik and Bissyand\'{e}, Tegawend\'{e} F. and Klein, Jacques and Traon, Yves Le},
title = {Name suggestions during feature identification: the variclouds approach},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934480},
doi = {10.1145/2934466.2934480},
abstract = {Reengineering a Software Product Line from legacy variants remains a challenging endeavour. Among various challenges, it is a complex task to retrieve enough information for inferring the variability from experts' domain knowledge and from the semantics of software elements. We propose the VariClouds process that can be leveraged by domain experts to understand the semantics behind the different blocks identified during software variants analysis. VariClouds is based on interactive word cloud visualisations providing name suggestions for these blocks using tf-idf as weighting factor. We evaluate our approach by assessing its added-value to several previous works in the literature where no tool support was provided to domain experts to characterise features from software blocks.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {119–123},
numpages = {5},
keywords = {feature identification, feature naming, software product lines, visualisation, word clouds},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2791060.2791069,
author = {Valov, Pavel and Guo, Jianmei and Czarnecki, Krzysztof},
title = {Empirical comparison of regression methods for variability-aware performance prediction},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791069},
doi = {10.1145/2791060.2791069},
abstract = {Product line engineering derives product variants by selecting features. Understanding the correlation between feature selection and performance is important for stakeholders to acquire a desirable product variant. We infer such a correlation using four regression methods based on small samples of measured configurations, without additional effort to detect feature interactions. We conduct experiments on six real-world case studies to evaluate the prediction accuracy of the regression methods. A key finding in our empirical study is that one regression method, called Bagging, is identified as the best to make accurate and robust predictions for the studied systems.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {186–190},
numpages = {5},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3382025.3414954,
author = {Michelon, Gabriela Karoline and Obermann, David and Linsbauer, Lukas and Assun\c{c}\~{a}o, Wesley Klewerton G. and Gr\"{u}nbacher, Paul and Egyed, Alexander},
title = {Locating feature revisions in software systems evolving in space and time},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414954},
doi = {10.1145/3382025.3414954},
abstract = {Software companies encounter variability in space as variants of software systems need to be produced for different customers. At the same time, companies need to handle evolution in time because the customized variants need to be revised and kept up-to-date. This leads to a predicament in practice with many system variants significantly diverging from each other. Maintaining these variants consistently is difficult, as they diverge across space, i.e., different feature combinations, and over time, i.e., revisions of features. This work presents an automated feature revision location technique that traces feature revisions to their implementation. To assess the correctness of our technique, we used variants and revisions from three open source highly configurable software systems. In particular, we compared the original artifacts of the variants with the composed artifacts that were located by our technique. The results show that our technique can properly trace feature revisions to their implementation, reaching traces with 100% precision and 98% recall on average for the three analyzed subject systems, taking on average around 50 seconds for locating feature revisions per variant used as input.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {14},
numpages = {11},
keywords = {feature location, feature revisions, repository mining, variants},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3233027.3233031,
author = {Kaindl, Hermann and Kramer, Stefan and Hoch, Ralph},
title = {An inductive learning perspective on automated generation of feature models from given product specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233031},
doi = {10.1145/3233027.3233031},
abstract = {For explicit representation of commonality and variability of a product line, a feature model is mostly used. An open question is how a feature model can be inductively learned in an automated way from a limited number of given product specifications in terms of features.We propose to address this problem through machine learning, more precisely inductive generalization from examples. However, no counter-examples are assumed to exist. Basically, a feature model needs to be complete with respect to all the given example specifications. First results indicate the feasibility of this approach, even for generating hierarchies, but many open challenges remain.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {25–30},
numpages = {6},
keywords = {generating feature models, inductive generalization from examples, machine learning},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2934466.2934472,
author = {Temple, Paul and Galindo, Jos\'{e} A. and Acher, Mathieu and J\'{e}z\'{e}quel, Jean-Marc},
title = {Using machine learning to infer constraints for product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934472},
doi = {10.1145/2934466.2934472},
abstract = {Variability intensive systems may include several thousand features allowing for an enormous number of possible configurations, including wrong ones (e.g. the derived product does not compile). For years, engineers have been using constraints to a priori restrict the space of possible configurations, i.e. to exclude configurations that would violate these constraints. The challenge is to find the set of constraints that would be both precise (allow all correct configurations) and complete (never allow a wrong configuration with respect to some oracle). In this paper, we propose the use of a machine learning approach to infer such product-line constraints from an oracle that is able to assess whether a given product is correct. We propose to randomly generate products from the product line, keeping for each of them its resolution model. Then we classify these products according to the oracle, and use their resolution models to infer cross-tree constraints over the product-line. We validate our approach on a product-line video generator, using a simple computer vision algorithm as an oracle. We show that an interesting set of cross-tree constraint can be generated, with reasonable precision and recall.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {209–218},
numpages = {10},
keywords = {constraints and variability mining, machine learning, software product lines, software testing, variability modeling},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2791060.2791065,
author = {Gregg, Susan P. and Scharadin, Rick and Clements, Paul},
title = {The more you do, the more you save: the superlinear cost avoidance effect of systems product line engineering},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791065},
doi = {10.1145/2791060.2791065},
abstract = {Product lines that use automated tools to configure shared assets (e.g., software or requirements or test cases or user documentation) based on product descriptions have long been known to bring about substantial development cost avoidance when compared to clone-and-own or product-specific development techniques. Now, however, it can be shown that the cost avoidance for configuring multiple shared assets is superlinear -- that is, the overall cost avoidance exceeds the sum of the that brought about by working with each of the shared assets in isolation. That is, a product line that configures (for example) requirements and code will avoid more cost than the sum of code-based plus requirements-based cost avoidance. In addition, we also observe a superlinear effect in terms of the number of products in the portfolio as well. This paper explores why these effects occur, and presents analytical and empirical evidence for their existence from one of the largest and most successful product lines in the literature, the AEGIS Weapon System. The result may lead to new insight into the economics of product line engineering in the systems engineering realm.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {303–310},
numpages = {8},
keywords = {AEGIS, feature modeling, product configurator, product derivation, product line economics, product line engineering, product line measurement, second generation product line engineering, systems and software product lines, variation points},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3236405.3236407,
author = {Ghofrani, Javad and Fehlhaber, Anna Lena},
title = {ProductlinRE: online management tool for requirements engineering of software product lines},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236407},
doi = {10.1145/3236405.3236407},
abstract = {The lack of online tools for managing various artifacts of software product lines is problematic, and stands in contradiction to findings about the need to support collaboration. In this paper, we present ProductLinRE, a web application allowing product line engineers to work cooperatively on artifacts of requirements engineering for software product lines. Our proposed online tool allows distributed teamwork, using a tracking mechanism for projects, artifacts and features while tailoring the requirements artifacts according to the selected features.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {17–22},
numpages = {6},
keywords = {online tools, requirements engineering, software product lines},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3233027.3233036,
author = {Hamza, Mostafa and Walker, Robert J. and Elaasar, Maged},
title = {CIAhelper: towards change impact analysis in delta-oriented software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233036},
doi = {10.1145/3233027.3233036},
abstract = {Change is inevitable for software systems to deal with the evolving environment surrounding them, and applying changes requires careful design and implementation not to break existing functionalities. Evolution in software product lines (SPLs) is more complex compared to evolution for individual products: a change applied to a single feature might affect all the products in the whole product family. In this paper we present an approach for change impact analysis in delta-oriented programming (DOP), an existing language aimed at supporting SPLs. We propose the CIAHelper tool to identify dependencies within a DOP program, by analyzing the semantics of both the code artifacts and variability models to construct a directed dependency graph. We also consider how the source code history could be used to enhance the recall of detecting the affected artifacts given a change proposal. We evaluate our approach by means of five case studies on two different DOP SPLs.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {31–42},
numpages = {12},
keywords = {change impact analysis, code assets, delta-oriented programming, feature model, variability model},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2791060.2791118,
author = {ter Beek, Maurice H. and Fantechi, Alessandro and Gnesi, Stefania and Mazzanti, Franco},
title = {Using FMC for family-based analysis of software product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791118},
doi = {10.1145/2791060.2791118},
abstract = {We show how the FMC model checker can successfully be used to model and analyze behavioural variability in Software Product Lines. FMC accepts parameterized specifications in a process-algebraic input language and allows the verification of properties of such models by means of efficient on-the-fly model checking. The properties can be expressed in a logic that allows to correlate the parameters of different actions within the same formula. We show how this feature can be used to tailor formulas to the verification of only a specific subset of products of a Software Product Line, thus allowing for scalable family-based analyses with FMC. We present a proof-of-concept that shows the application of FMC to an illustrative Featured Transition System from the literature.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {432–439},
numpages = {8},
keywords = {featured transition systems, features, model transformation, process algebra, variability},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3106195.3106202,
author = {Wille, David and Wehling, Kenny and Seidl, Christoph and Pluchator, Martin and Schaefer, Ina},
title = {Variability Mining of Technical Architectures},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106202},
doi = {10.1145/3106195.3106202},
abstract = {Technical architectures (TAs) represent the computing infrastructure of a company with all its hardware and software components. Over the course of time, the number of TAs grows with the companies' requirements and usually a large variety of TAs has to be maintained. Core challenge is the missing information on relations between the existing variants of TAs, which complicates reuse of solutions across systems. However, identifying these relations is an expensive task as architects have to manually analyze each TA individually. Restructuring the existing TAs poses severe risks as often sufficient information is not available (e.g., due to time constraints). To avoid failures in productive systems and resulting loss of profit, companies continue to create new solutions without restructuring existing ones. This increased variability in TAs represents technical debt. In this paper, we adapt the idea of variability mining from the software product line domain and present an efficient and automatic mining algorithm to identify the common and varying parts of TAs by analyzing a potentially arbitrary number of TAs in parallel. Using the identified variability information, architects are capable of analyzing the relations of TAs, identifying reuse potential, and making well-founded maintenance decisions. We show the feasibility and scalability of our approach by applying it to a real-world industrial case study with large sets of TAs.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {39–48},
numpages = {10},
keywords = {enterprise architecture, technical architecture, variability mining},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2647908.2655964,
author = {Mannion, Mike and Kaindl, Hermann},
title = {Using similarity metrics for mining variability from software repositories},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655964},
doi = {10.1145/2647908.2655964},
abstract = {Much activity within software product line engineering has been concerned with explicitly representing and exploiting commonality and variability at the feature level for the purpose of a particular engineering task e.g. requirements specification, design, coding, verification, product derivation process, but not for comparing how similar products in the product line are with each other. In contrast, a case-based approach to software development is concerned with descriptions and models as a set of software cases stored in a repository for the purpose of searching at a product level, typically as a foundation for new product development. New products are derived by finding the most similar product descriptions in the repository using similarity metrics.The new idea is to use such similarity metrics for mining variability from software repositories. In this sense, software product line engineering could be informed by the case-based approach. This approach requires defining and implementing such similarity metrics based on the representations used for the software cases in such a repository. It provides complementary benefits to the ones given through feature-based representations of variability and may help mining such variability.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {32–35},
numpages = {4},
keywords = {case-based reasoning, commonality and variability, feature-based representation, product lines, similarity metrics},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.5555/1753235.1753238,
author = {White, Jules and Dougherty, Brian and Schmidt, Doulas C. and Benavides, David},
title = {Automated reasoning for multi-step feature model configuration problems},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {The increasing complexity and cost of software-intensive systems has led developers to seek ways of increasing software reusability. One software reuse approach is to develop a Software Product-line (SPL), which is a reconfigurable software architecture that can be reused across projects. Creating configurations of the SPL that meets arbitrary requirements is hard.Existing research has focused on techniques that produce a configuration of the SPL in a single step. This paper provides three contributions to the study of multi-step configuration for SPLs. First, we present a formal model of multi-step SPL configuration and map this model to constraint satisfaction problems (CSPs). Second, we show how solutions to these CSP configuration problem CSPs can be derived automatically with a constraint solver. Third, we present empirical results demonstrating that our CSP-based technique can solve multi-step configuration problems involving hundreds of features in seconds.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {11–20},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2019136.2019158,
author = {Guana, Victor and Correal, Dario},
title = {Variability quality evaluation on component-based software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019158},
doi = {10.1145/2019136.2019158},
abstract = {Quality assurance and evaluation in Model Driven Software Product Lines (MD-SPLs) are pivotal points for the growing and solidification of the generative software factories. They are framed as one of the future fact methodologies for the construction of software systems. Although several approximations address the problem of generative environments, software product line scope expression, and core asset definition, not many of them try to solve, as a fundamental step, the automation of the quality attribute evaluation in the MD-SPL development cycle. This paper presents a model-driven engineering method and a tool for the quality evaluation of product line configurations through a cross architectural view analysis.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {19},
numpages = {8},
keywords = {domain specific modeling, model composition, model-driven software product line, quality attribute, sensitivity point},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2499777.2500719,
author = {Schr\"{o}ter, Reimar and Siegmund, Norbert and Th\"{u}m, Thomas},
title = {Towards modular analysis of multi product lines},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500719},
doi = {10.1145/2499777.2500719},
abstract = {Software product-line engineering enables efficient development of tailor-made software by means of reusable artifacts. As practitioners increasingly develop software systems as product lines, there is a growing potential to reuse product lines in other product lines, which we refer to as multi product line. We identify challenges when developing multi product lines and propose interfaces for different levels of abstraction ranging from variability modeling to functional and non-functional properties. We argue that these interfaces ease the reuse of product lines and identify research questions that need to be solved toward modular analysis of multi product lines.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {96–99},
numpages = {4},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3510466.3511271,
author = {Nieke, Michael and Hoff, Adrian and Schaefer, Ina and Seidl, Christoph},
title = {Experiences with Constructing and Evolving aSoftware Product Line with Delta-Oriented Programming},
year = {2022},
isbn = {9781450396042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510466.3511271},
doi = {10.1145/3510466.3511271},
abstract = {A Software Product Line (SPL) captures families of closely related software variants. The configuration options of an SPL are represented by features. Typically, SPLs are developed in a feature-centric manner and, thus, require different development methods and technologies from developing software products individually. For developers of single systems, this means a shift in paradigm and technology. Especially with invasive variability realization mechanisms, such as Delta-Oriented Programming (DOP), centering development around configurable features realized via source code transformation is commonly expected to pose an obstacle, but concrete experience reports are lacking. In this paper, we investigate how DOP and cutting-edge SPL development tools are picked up by non-expert developers. To this end, we report on our experiences from a student capstone SPL development project. Our results show that participants find easy access to SPL development concepts and tools. Based on our observations and the participants’ practices, we define guidelines for developers using DOP.},
booktitle = {Proceedings of the 16th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {11},
numpages = {9},
keywords = {Case Study, Delta Oriented Programming, Evolution, Experience Report, Guidelines, Observations, Software Product Line},
location = {Florence, Italy},
series = {VaMoS '22}
}

@inproceedings{10.1145/3106195.3106204,
author = {Luthmann, Lars and Stephan, Andreas and B\"{u}rdek, Johannes and Lochau, Malte},
title = {Modeling and Testing Product Lines with Unbounded Parametric Real-Time Constraints},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106204},
doi = {10.1145/3106195.3106204},
abstract = {Real-time requirements are crucial for embedded software in many modern application domains of software product lines. Hence, techniques for modeling and analyzing time-critical software have to be lifted to software product line engineering, too. Existing approaches extend timed automata (TA) by feature constraints to so-called featured timed automata (FTA) facilitating efficient verification of real-time properties for entire product lines in a single run. In this paper, we propose a novel modeling formalism, called configurable parametric timed automata (CoPTA), extending expressiveness of FTA by supporting freely configurable and therefore a-priori unbounded timing intervals for real-time constraints, which are defined as feature attributes in extended feature models with potentially infinite configuration spaces. We further describe an efficient test-suite generation methodology for CoPTA models, achieving location coverage on every possible model configuration. Finally, we present evaluation results gained from applying our tool implementation to a collection of case studies, demonstrating efficiency improvements compared to a variant-by-variant analysis.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {104–113},
numpages = {10},
keywords = {Model-based Testing, Real-Time Systems, Software Product Lines, Timed Automata},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3622748.3622754,
author = {Lima, Rafael and Bezerra, Carla and Machado, Ivan},
title = {A Self-Adaptation Mechanism for Variability Management in Dynamic Software Product Lines},
year = {2023},
isbn = {9798400709524},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622748.3622754},
doi = {10.1145/3622748.3622754},
abstract = {Variability management is crucial for companies that need to offer multiple product variants quickly. However, the increasing complexity of software-intensive systems has made variability management increasingly challenging. This challenge is compounded by the need for such systems to run continuously and adapt to changes in the environment and user needs at runtime. To address this challenge, Dynamic Software Product Line (DSPL) Engineering has emerged as a strategy for managing variability in complex and dynamic environments. The key challenge in DSPL engineering is to manage product configurations at runtime by detecting changes in the context and adapting accordingly. In this paper, we propose an adaptation mechanism for DSPL feature models that supports dynamic variability and is based on the MAPE-K model. The mechanism transforms feature model constraints into rules that enable the activation of each feature and annotates contexts in the corresponding features to be activated when changes occur. We have implemented the mechanism in the DyMMer 2.0 modeling tool and evaluated its performance using various DSPL feature models. Additionally, we performed a preliminary evaluation with a proof-of-concept study with an expert to assess its practical usage. Our results demonstrate the effectiveness and practicality of the proposed mechanism in managing variability in complex and dynamic environments.},
booktitle = {Proceedings of the 17th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {51–60},
numpages = {10},
keywords = {dynamic software product line, feature model, variability management},
location = {Campo Grande, Brazil},
series = {SBCARS '23}
}

@inproceedings{10.1145/2791060.2791088,
author = {Domis, Dominik and Adler, Rasmus and Becker, Martin},
title = {Integrating variability and safety analysis models using commercial UML-based tools},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791088},
doi = {10.1145/2791060.2791088},
abstract = {Software and System Product Lines (SSPL) are the state-of-the-art for systematically reusing a common set of core assets in the development of similar products in a product family. A large number of SSPL success stories have been published in the last decade and commercial tool support is also available. SSPLs promise to reduce cost, to shorten time-to-market for new features, and to increase product quality by systematically reusing core assets in the development of three or more systems. However, an open challenge is SSPL engineering for safety-relevant systems such as automotive, avionic, or industrial automation systems. Safety-relevant systems have to be developed, analyzed, and certified according to safety standards such as IEC 61508. These standards demand the application of safety analyses such as Fault Tree Analysis and Failure Mode and Effect Analysis. Starting the safety analysis of each product variant of a SSPL from scratch is complex and very time-consuming. However, there are only few convincing cases, where SSPL approaches have been followed in safety engineering. To pave the way for a broader adoption of SSPL approaches, this paper reports practical experiences with industrial-strength methods and tools along an adaptive cruise control SSPL. The paper demonstrates how commercial tools can be used (i) to analyze safety-related aspects already in the architectural design, (ii) to model the results as component integrated component fault trees (C2FT), and (iii) to systematically reuse C2FT in the safety analysis of a concrete product. The results of the case study show that C2FT (i) can be easily integrated into a feature-oriented development process of SSPL, (ii) facilitate early consideration of safety in domain engineering, and (iii) reduce effort and complexity of safety analyses in application engineering.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {225–234},
numpages = {10},
keywords = {fault tree analysis, feature model, functional-safety, safety analysis, safety engineering, software product line engineering, tool support, variability, variant management},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2934466.2934492,
author = {Groher, Iris and Weinreich, Rainer and Buchgeher, Georg and Schossleitner, Robert},
title = {Reusable architecture variants for customer-specific automation solutions},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934492},
doi = {10.1145/2934466.2934492},
abstract = {Manufacturing execution systems (MES) are key elements of industrial automation systems. MES can be deployed at different levels of scale from a single site or plant to a company with globally distributed production sites all over the world. Establishing or extending an MES is a complex process, which requires taking the already existing software and system architecture into account in addition to the desired MES features. We developed an approach and an associated tool to support the process of creating offers for customer-specific MES solutions based on a vendor-specific automation platform. We define architecture variants for selecting a specific MES feature set and for supporting different MES expansion stages. Additionally, we provide an architecture modeling approach to explore the integration with existing software and system infrastructures. The approach has been applied at the STIWA Group, a vendor of MES for industrial production lines.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {242–251},
numpages = {10},
keywords = {architecture variants, automation platform, customer-specific offer, feature set, manufacturing execution system (MES)},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3336294.3342375,
author = {Arcaini, Paolo and Gargantini, Angelo and Radavelli, Marco},
title = {Achieving Change Requirements of Feature Models by an Evolutionary Approach},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342375},
doi = {10.1145/3336294.3342375},
abstract = {Software Product Lines (SPLs) are families of products that share some common features, and differ on some others. The variability of SPLs is usually described at design time by using variability models; one of the main used variability models are feature models (FMs).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {256},
numpages = {1},
keywords = {feature models, mutation, search-based software engineering, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3336294.3336296,
author = {Ludwig, Kai and Kr\"{u}ger, Jacob and Leich, Thomas},
title = {Covert and Phantom Features in Annotations: Do They Impact Variability Analysis?},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336296},
doi = {10.1145/3336294.3336296},
abstract = {The annotation-based variability of the C preprocessor (CPP) has a bad reputation regarding comprehensibility and maintainability of software systems, but is widely adopted in practice. To assess the complexity of such systems' variability, several analysis techniques and metrics have been proposed in scientific communities. While most metrics seem reasonable at first glance, they do not generalize over all possible usages of C preprocessor variability that appear in practice. Consequently, some analyses may neglect the actual complexity of variability in these systems and may not properly reflect the real situation. In this paper, we investigate two types of variation points, namely negating and #else directives, to which we refer to as corner cases, as they are seldom explicitly considered in research. To investigate these directives, we rely on three commonly used metrics: lines of feature code, scattering degree, and tangling degree. We (1) describe how the considered directives impact these metrics, (2) unveil the resulting differences within 19 systems, and (3) propose how to address the arising issues. The results show that the corner cases appear regularly in variable feature code and can heavily change the results obtained with established metrics. We argue that we need to refine metrics and improve variability analysis techniques to provide more precise results, but we also need to reason about the meaning of corner cases and metrics.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {218–230},
numpages = {13},
keywords = {empirical study, preprocessor, software metrics, software product lines, variability analysis},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3336294.3342360,
author = {Michelon, Gabriela Karoline and Linsbauer, Lukas and Assun\c{c}\~{a}o, Wesley K. G. and Egyed, Alexander},
title = {Comparison-Based Feature Location in ArgoUML Variants},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342360},
doi = {10.1145/3336294.3342360},
abstract = {Identifying and extracting parts of a system's implementation for reuse is an important task for re-engineering system variants into Software Product Lines (SPLs). An SPL is an approach that enables systematic reuse of existing assets across related product variants. The re-engineering process to adopt an SPL from a set of individual variants starts with the location of features and their implementation, to be extracted and migrated into an SPL and reused in new variants. Therefore, feature location is of fundamental importance to the success in the adoption of SPLs. Despite its importance, existing feature location techniques struggle with huge, complex, and numerous system artifacts. This is the scenario of ArgoUML-SPL, which stands out as the most used case study for the validation of feature location approaches. In this paper we use an automated feature location technique and apply it to the ArgoUML feature location challenge posed.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {93–97},
numpages = {5},
keywords = {clones, feature location, reuse, software product lines, traceability, variants},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2934466.2971318,
author = {Tamai, Tetsuo},
title = {Product-centered view vs process-centered view},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2971318},
doi = {10.1145/2934466.2971318},
abstract = {Looking back the history of software engineering, we can observe an alternating cycle of interest on product-centered view vs. process-centered view in software research and practices.From the late 1980's to early 1990's, software process became quite an active field. Activities concerning software process were hot in academia as well as in industry. The interest on software process saw its peak in early 1990's but lost the momentum soon. Then came the fever on software architecture.The book "Software Architecture" by M. Shaw and D. Garlan was published in 1996 and widely read. Design patterns and application frameworks drew attention about the same time, which together indicate a shift of interest from process to product.In 2000's, the interest on processes revived. One phenomenon is the upsurge of interest on the agile process. As software product lines (SPL) contains the word product in the term, it deals with a variety of products but its focus is also on the process of managing development and evolution of a family of similar software products.We will give a perspective on the alternating cycle of interest on product-centered view vs. process-centered view and then characterize SPL in this framework.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {18–21},
numpages = {4},
keywords = {product line, software architecture, software product and process},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2934466.2934491,
author = {Fogdal, Thomas and Scherrebeck, Helene and Kuusela, Juha and Becker, Martin and Zhang, Bo},
title = {Ten years of product line engineering at Danfoss: lessons learned and way ahead},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934491},
doi = {10.1145/2934466.2934491},
abstract = {Software and systems product line engineering (PLE) has been an established approach for reducing time to market as well as cost and increasing quality in a set of related products for two decades now. Although there is a huge body of knowledge on PLE, adopting a concrete PLE approach is still not a trivial endeavor for interested companies. With the increasing importance of development speed, the advent of agile engineering approaches, and decreasing management interest in improvements that require large organizational transformations and only show benefits after several years, companies are facing challenges in successfully adopting this approach. They often hesitate as there is no clear adoption path, nor any certainty, that the intended improvement steps will also provide added value in the short- and mid-term perspective. In consequence, a considerable amount of PLE potential still remains unexploited.To help such companies with the adoption of PLE, the goal of this paper is to provide inspiration and evidence that PLE is a sound approach and its successful introduction is possible even in settings that differ substantially from those of pioneer product lines.To this end, this paper presents the following main contributions with the PLE adoption case at Danfoss Drives: an overview of the key change drivers and the motivation for adopting a PLE approach, a discussion of incremental PLE introduction in an agile engineering context, a presentation of the current PLE setting with a focus on key concepts, and finally a presentation of motivators and directions for future improvements.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {252–261},
numpages = {10},
keywords = {industrial experiences, product line adoption, product line evaluation},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2647908.2655972,
author = {Meinicke, Jens and Th\"{u}m, Thomas and Schr\"{o}ter, Reimar and Benduhn, Fabian and Saake, Gunter},
title = {An overview on analysis tools for software product lines},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655972},
doi = {10.1145/2647908.2655972},
abstract = {A software product line is a set of different software products that share commonalities. For a selection of features, specialized products of one domain can be generated automatically from domain artifacts. However, analyses of software product lines need to handle a large number of products that can be exponential in the number of features. In the last decade, many approaches have been proposed to analyze software product lines efficiently. For some of these approaches tool support is available. Based on a recent survey on analysis for software product lines, we provide a first overview on such tools. While our discussion is limited to analysis tools, we provide an accompanying website covering further tools for product-line development. We compare tools according to their analysis and implementation strategy to identify underrepresented areas. In addition, we want to ease the reuse of existing tools for researchers and students, and to simplify research transfer to practice.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {94–101},
numpages = {8},
keywords = {code metrics, model checking, non-functional properties, sampling, software product lines, static analysis, testing, theorem proving, tool support, type checking},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2019136.2019165,
author = {Lettner, Daniela and Thaller, Daniel and Vierhauser, Michael and Rabiser, Rick and Gr\"{u}nbacher, Paul and Heider, Wolfgang},
title = {Supporting business calculations in a product line engineering tool suite},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019165},
doi = {10.1145/2019136.2019165},
abstract = {Software Product Line Engineering (SPLE) involves defining the commonalities and variability of similar products to leverage extensive reuse and to accelerate the derivation of customized products. However, sales people and customers do not only care about technical properties of product features during product derivation. They also need information concerning the business value of product features. Existing approaches have addressed this issue by combining business information with variability models, e.g., by defining feature attributes or by integrating third party tools. However, a solution that seamlessly integrates variability and business calculations within a SPLE tool is still lacking. We report on our ongoing efforts to integrate business calculations in the DOPLER tool suite. We use examples of product lines from the industrial plant automation domain to motivate and demonstrate our solution.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {26},
numpages = {4},
keywords = {business calculations, tool support, value-based software engineering, variability models},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2647908.2655971,
author = {Devroey, Xavier and Perrouin, Gilles and Schobbens, Pierre-Yves},
title = {Abstract test case generation for behavioural testing of software product lines},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655971},
doi = {10.1145/2647908.2655971},
abstract = {In Model Based Testing (MBT), test cases are generated automatically from a partial representation of expected behaviour of the System Under Test (SUT) (i.e., the model). For most industrial systems, it is impossible to generate all the possible test cases from the model. The test engineer recourse to generation algorithms that maximize a given coverage criterion, a metric indicating the percentage of possible behaviours of the SUT covered by the test cases. Our previous work redefined classical Transition Systems (TSs) criteria for SPLs, using Featured Transition Systems (FTSs), a mathematical structure to compactly represent the behaviour of a SPL, as model for test case generation. In this paper, we provide one all-states coverage driven generation algorithm and discuss its scalability and efficiency with respect to random generation. All-states and random generation are compared on fault-seeded FTSs.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {86–93},
numpages = {8},
keywords = {model-based testing, software product line, test case generation},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2934466.2934490,
author = {Iida, Takahiro and Matsubara, Masahiro and Yoshimura, Kentaro and Kojima, Hideyuki and Nishino, Kimio},
title = {PLE for automotive braking system with management of impacts from equipment interactions},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934490},
doi = {10.1145/2934466.2934490},
abstract = {We report here an industrial application of the Product Line Engineering (PLE) for the development of electronic braking systems.The cost of software engineering in automotive control systems is increasing as new functions for safety, comfort, and improved fuel efficiency are integrated into electronic control units.Therefore, Component suppliers for automotive control systems adapt their products to the requirements of car manufacturers by modifying the software specifications, such that it makes minimal changes to the mechanical structure and the electrical and electronic (E/E) components hence reduces the cost. PLE is an effective approach to manage or even reduce the software variations resulting from these modifications.However, one problem is that the software specifications of automotive control systems need to be redesigned after system testing with vehicles. This is because vehicles consist of many mechanical parts manufactured by different suppliers, and the characteristics of the parts can interact with each other. This problem makes it difficult to reap the full benefits of PLE.We propose an approach to analyze the potential impact from such interactions by using a system model that expresses the system architecture that includes the parts of different suppliers. Based on this model, the software architecture was designed to localize the impact to several software components. Additionally, a feature model was designed to the enable management of the localized impact by expressing it as variability. This method helps software engineers specify the software components that can have an effect on the actual equipment, and determine which modifications to the software specifications are necessary.We applied PLE with the proposed method in the development of electronic brake control system. We confirmed that our approach greatly increased the efficiency of PLE for the development of such automotive control systems.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {232–241},
numpages = {10},
keywords = {control system, feature modeling, product lines engineering, software engineering, software modification, system modeling},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3109729.3109739,
author = {Hamza, Mostafa and Walker, Robert J. and Elaasar, Maged},
title = {Unanticipated Evolution in Software Product Lines versus Independent Products: A Case Study},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109739},
doi = {10.1145/3109729.3109739},
abstract = {Real product families need to evolve in ways that are not always anticipated by a pre-planned design. Any given approach for software product lines will likely lead to both positive and negative consequences during unanticipated software evolution. Unfortunately, we know little about the evolvability characteristics of SPL approaches that concern both modelling and implementation, limiting our ability to make rational and disciplined decisions about adoption. We conduct a case study into the unanticipated evolution of a software product family using two approaches: separate products versus a common codebase using delta-oriented programming (DOP). We compare the ease of change within the two versions through a set of quantitative measurements and qualitative observations. We find that both versions have strengths and weaknesses: complexity and incomplete support from DOP tools versus significant duplication and error-proneness in the separate products.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {97–104},
numpages = {8},
keywords = {Software product lines, case study, comparative study, delta-oriented programming, retrospective study, separate products, unanticipated evolution},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@proceedings{10.1145/3674805,
title = {ESEM '24: Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
year = {2024},
isbn = {9798400710476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Barcelona, Spain}
}

@inproceedings{10.1145/2019136.2019149,
author = {Murugesupillai, Esan and Mohabbati, Bardia and Ga\v{s}evi\'{c}, Dragan},
title = {A preliminary mapping study of approaches bridging software product lines and service-oriented architectures},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019149},
doi = {10.1145/2019136.2019149},
abstract = {Service Oriented Architectures (SOA) and Software Product Lines (SPL) have individually proven to be software engineering concepts that create added value to the development of software systems. Recently, the research community has recognized and investigated potentials for combining these two concepts. However, there have been no mapping study and literature surveys that systematically review the present research results in combining the two. This paper presents results of a preliminary work on a systematic mapping study of research papers that report on combining SOA and SPL. The main goal of a systematic mapping study is to provide a breath overview, classification of approaches and the quantity and type of research as well as available research results, which is complimentary step toward further systematic literature review. This paper, based on selected papers published from 2002 to mid-2010, reports on various aspects of the analyzed literature, including the motivations for combining the two concepts; contributions to specific stages of software engineering lifecycles; types of synergies and characteristics that are accomplished through combinations of the two concepts; and the methods used for and the rigor of the evaluations of the research conducted on the studied topic.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {11},
numpages = {8},
keywords = {service-oriented architecture, service-oriented product line, software product line, software variability, variability management},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2648511.2648542,
author = {Hellebrand, Robert and Silva, Adeline and Becker, Martin and Zhang, Bo and Sierszecki, Krzysztof and Savolainen, Juha},
title = {Coevolution of variability models and code: an industrial case study},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648542},
doi = {10.1145/2648511.2648542},
abstract = {In Software Engineering, reuse of artifacts is essential for high productivity. Different studies have shown that efficient reuse needs systematic planning and realization. Variability Management plays a key role in Software Product Line Engineering. We investigate code artifacts and variability models of a real-world Software Product Line over time in order to clarify whether code and variability model evolve congeneric. Furthermore, we suggest and test metrics that would allow detecting variability erosion in the code based on changes in the variability model.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {274–283},
numpages = {10},
keywords = {coevolution, feature models, metrics, product line evolution},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/1964138.1964142,
author = {Fayad, M. E. and Singh, Shivanshu K.},
title = {Software stability model: software product line engineering overhauled},
year = {2010},
isbn = {9781450305426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1964138.1964142},
doi = {10.1145/1964138.1964142},
abstract = {Software Product Lines are increasingly being perceived as a viable option of manufacturing software, and as a software development paradigm. Software Product Lines allow organizations to yield high returns on initial investments, better times to market, cut costs and increase profits and increase the ability to enter any market or domain of application, rather quickly, than what has been possible in the past. Software Stability Model, along with the multiple benefits that it brings along with the approach to software architecture, design and development that it incorporates into the development methodology that it follows, is significantly better than the existing concept of software product lines. Benefits of using this model range from moving from a specialized section or sub-section of a domain that a product line targets to a situation where the product line can be made to be applicable in various situations that involve problems spanning across multiple domains, leave alone a specialized section in a specific domain only, to making the product a highly dynamic and evolvable base resource, such that it virtually never fades out or the longevity of the product line itself being increased manifold This work discusses the various benefits that Software Stability has over the regular Product Line Engineering and how some of Software Stability Model's concepts can make the product lines of today, much better.},
booktitle = {Proceedings of the 2010 Workshop on Knowledge-Oriented Product Line Engineering},
articleno = {4},
numpages = {4},
keywords = {domain analysis, scalability, software product lines, software stability model, stability, unified solutions},
location = {Reno, Nevada},
series = {KOPLE '10}
}

@inproceedings{10.1145/3106195.3106213,
author = {Fu\ss{}berger, Nicolas and Zhang, Bo and Becker, Martin},
title = {A Deep Dive into Android's Variability Realizations},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106213},
doi = {10.1145/3106195.3106213},
abstract = {The open source Android operation system is widely used in both mobile consumer electronics as well as other industrial devices. It has actually become a variability-intensive system that can be highly customized to support different customers' requirements and hardware environments, which is a good inspiration for both practitioners and researchers. However, it is still unclear where and how variability is realized in its source code repository. In this paper, we conduct a systematic analysis on the variability realization of the Android operation system. The analysis focuses on the usage of different variability realization mechanisms (e.g., Conditional Compilation) in the Android source code and build environment. Finally, the study provides qualitative and quantitative results that help to understand i) what variability-specific artefacts exist in the Android source repository using which variability mechanisms and techniques; ii) how these artefacts express and instantiate variability along the layered Android realization architecture.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {69–78},
numpages = {10},
keywords = {Android, Variability Mechanisms, Variability Realization},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2491627.2491646,
author = {Marijan, Dusica and Gotlieb, Arnaud and Sen, Sagar and Hervieu, Aymeric},
title = {Practical pairwise testing for software product lines},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491646},
doi = {10.1145/2491627.2491646},
abstract = {One key challenge for software product lines is efficiently managing variability throughout their lifecycle. In this paper, we address the problem of variability in software product lines testing. We (1) identify a set of issues that must be addressed to make software product line testing work in practice and (2) provide a framework that combines a set of techniques to solve these issues. The framework integrates feature modelling, combinatorial interaction testing and constraint programming techniques. First, we extract variability in a software product line as a feature model with specified feature interdependencies. We then employ an algorithm that generates a minimal set of valid test cases covering all 2-way feature interactions for a given time interval. Furthermore, we evaluate the framework on an industrial SPL and show that using the framework saves time and provides better test coverage. In particular, our experiments show that the framework improves industrial testing practice in terms of (i) 17% smaller set of test cases that are (a) valid and (b) guarantee all 2-way feature coverage (as opposite to 19.2% 2-way feature coverage in the hand made test set), and (ii) full flexibility and adjustment of test generation to available testing time.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {227–235},
numpages = {9},
keywords = {feature modelling, software product lines, variability management},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3106195.3106220,
author = {Young, Bobbi and Cheatwood, Judd and Peterson, Todd and Flores, Rick and Clements, Paul},
title = {Product Line Engineering Meets Model Based Engineering in the Defense and Automotive Industries},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106220},
doi = {10.1145/3106195.3106220},
abstract = {Product line engineering and model based engineering are two powerful engineering approaches that each bring significant advantages to system engineering projects. This paper explores how three companies - Raytheon, General Dynamics, and General Motors - are combining these two paradigms in unique and innovative ways in very challenging application domains to achieve engineering goals of critical importance to them.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {175–179},
numpages = {5},
keywords = {PLE factory, Product line engineering, feature models, feature profiles, feature-based product line engineering, model-based engineering, product configurator, variation points},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2791060.2791075,
author = {Fang, Miao and Leyh, Georg and Doerr, Joerg and Elsner, Christoph and Zhao, Jingjing},
title = {Towards model-based derivation of systems in the industrial automation domain},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791075},
doi = {10.1145/2791060.2791075},
abstract = {Many systems in the industrial automation domain include information systems. They manage manufacturing processes and control numerous distributed hardware and software components. In current practice, the development and reuse of such systems is costly and time-consuming, due to the variability of systems' topology and processes. Up to now, product line approaches for systematic modeling and management of variability have not been well established for such complex domains.In this paper, we present a model-based approach to support the derivation of systems in the target domain. The proposed architecture of the derivation infrastructure enables feature-, topology- and process configuration to be integrated into the multi-staged derivation process. We have developed a prototype to prove feasibility and improvement of derivation efficiency. We report the evaluation results that we collected through semi-structured interviews from domain stakeholders. The results show high potential to improve derivation efficiency by adopting the approach in practice. Finally, we report the lessons learned that raise the opportunities and challenges for future research.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {283–292},
numpages = {10},
keywords = {derivation, model-based engineering, product line, variability modeling},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1145/3579644,
author = {Thuijsman, Sander and Reniers, Michel},
title = {Supervisory Control for Dynamic Feature Configuration in Product Lines},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {5},
issn = {1539-9087},
url = {https://doi.org/10.1145/3579644},
doi = {10.1145/3579644},
abstract = {In this paper a framework for engineering supervisory controllers for product lines with dynamic feature configuration is proposed. The variability in valid configurations is described by a feature model. Behavior of system components is achieved using (extended) finite automata and both behavioral and dynamic configuration constraints are expressed by means of requirements as is common in supervisory control theory. Supervisory controller synthesis is applied to compute a behavioral model in which the requirements are adhered to. For the challenges that arise in this setting, multiple solutions are discussed. The solutions are exemplified in the CIF toolset using a model of a coffee machine. A use case of the much larger Body Comfort System product line is performed to showcase feasibility for industrial-sized systems.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = aug,
articleno = {71},
numpages = {25},
keywords = {Discrete event systems, supervisory controller synthesis, feature models}
}

@inproceedings{10.1145/2491627.2491633,
author = {Tsuchiya, Ryosuke and Kato, Tadahisa and Washizaki, Hironori and Kawakami, Masumi and Fukazawa, Yoshiaki and Yoshimura, Kentaro},
title = {Recovering traceability links between requirements and source code in the same series of software products},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491633},
doi = {10.1145/2491627.2491633},
abstract = {If traceability links between requirements and source code are not clarified when conducting maintenance and enhancements for the same series of software products, engineers cannot immediately find the correction location in the source code for requirement changes. However, manually recovering links in a large group of products requires significant costs and some links may be overlooked. Here, we propose a semi-automatic method to recover traceability links between requirements and source code in the same series of large software products. In order to support differences in representation between requirements and source code, we recover links by using the configuration management log as an intermediary. We refine the links by classifying requirements and code elements in terms of whether they are common or specific to the products. As a result of applying our method to real products that have 60KLOC, we have recovered valid traceability links within a reasonable amount of time. Automatic parts have taken 13 minutes 36 seconds, and non-automatic parts have taken about 3 hours, with a recall of 76.2% and a precision of 94.1%. Moreover, we recovered some links that were unknown to engineers. By recovering traceability links, software reusability will be improved, and software product line introduction will be facilitated.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {121–130},
numpages = {10},
keywords = {commonality and variability analysis, configuration management log, traceability recovery},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3236405.3237202,
author = {Krafczyk, Adam and El-Sharkawy, Sascha and Schmid, Klaus},
title = {Reverse engineering code dependencies: converting integer-based variability to propositional logic},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3237202},
doi = {10.1145/3236405.3237202},
abstract = {A number of SAT-based analysis concepts and tools for software product lines exist, that extract code dependencies in propositional logic from the source code assets of the product line. On these extracted conditions, SAT-solvers are used to reason about the variability. However, in practice, a lot of software product lines use integer-based variability. The variability variables hold integer values, and integer operators are used in the conditions. Most existing analysis tools can not handle this kind of variability; they expect pure Boolean conditions.This paper introduces an approach to convert integer-based variability conditions to propositional logic. Running this approach as a preparation on an integer-based product line allows the existing SAT-based analyses to work without any modifications. The pure Boolean formulas, that our approach builds as a replacement for the integer-based conditions, are mostly equivalent to the original conditions with respect to satisfiability. Our approach was motivated by and implemented in the context of a real-world industrial case-study, where such a preparation was necessary to analyze the variability.Our contribution is an approach to convert conditions, that use integer variables, into propositional formulas, to enable easy usage of SAT-solvers on the result. It works well on restricted variables (i.e. variables with a small range of allowed values); unrestricted integer variables are handled less exact, but still retain useful variability information.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {34–41},
numpages = {8},
keywords = {integer-based expressions, propositional logic, reverse engineering, satisfiability, software product lines, variability management},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/1176617.1176726,
author = {Hetrick, William A. and Krueger, Charles W. and Moore, Joseph G.},
title = {Incremental return on incremental investment: Engenio's transition to software product line practice},
year = {2006},
isbn = {159593491X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1176617.1176726},
doi = {10.1145/1176617.1176726},
abstract = {Engenio made the transition to software product line practice in order to keep pace with growing business demand for its products. By using an incremental transition strategy, Engenio avoided the typical upfront adoption barrier - the equivalent development effort of 2 to 3 standalone products - which in their case was projected to be 900 to 1350 developer-months. Engenio discovered that by making an upfront investment of only 4 developer-months, they were able to start a chain reaction in which the tactical and strategic incremental returns quickly outpaced the incremental investments, making the transition pay for itself.},
booktitle = {Companion to the 21st ACM SIGPLAN Symposium on Object-Oriented Programming Systems, Languages, and Applications},
pages = {798–804},
numpages = {7},
keywords = {incremental methods, software product lines, transition to software product line practice},
location = {Portland, Oregon, USA},
series = {OOPSLA '06}
}

@inproceedings{10.1145/3106195.3106210,
author = {Markiegi, Urtzi and Arrieta, Aitor and Sagardui, Goiuria and Etxeberria, Leire},
title = {Search-based product line fault detection allocating test cases iteratively},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106210},
doi = {10.1145/3106195.3106210},
abstract = {The large number of possible configurations makes it unfeasible to test every single system variant in a product line. Consequently, a small subset of the product line products must be selected, typically following combinatorial interaction testing approaches. Recently, many product line engineering approaches have considered the selection and prioritization of relevant products within the product line. In a further step, these products are thoroughly tested individually. However, the test cases that must be executed in each of the products are not always insignificant, and in systems such as Cyber-Physical System Product Lines (CPSPLs), their test execution time can vary from tens to thousands of seconds. This issue leads to spending a lot of time testing each individual product. To solve this problem we propose a search-based approach to perform the testing of product lines by allocating small number of test cases in each of the products. This approach increases the probability of detecting faults faster. Specifically, our search-based approach obtains a set of products, which are derived from using any state-of-the-art approach as inputs, and a set of attributed test cases. As an output a list of allocated test cases for each product is obtained. We also define a novel fitness function to guide the search and we propose corresponding crossover and mutation operators. The search and test process is iteratively repeated until the time budget is consumed. We performed an evaluation with a CPSPL as a case study. Results suggest that our approach can reduce the fault detection time by 61% and 65% on average when compared with the traditional test process and the Random Search algorithm respectively.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {123–132},
numpages = {10},
keywords = {Fault Detection, Product Line Testing, Search-based Software Engineering},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2934466.2934485,
author = {Lape\~{n}a, Ra\'{u}l and Ballarin, Manuel and Cetina, Carlos},
title = {Towards clone-and-own support: locating relevant methods in legacy products},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934485},
doi = {10.1145/2934466.2934485},
abstract = {Clone-and-Own (CAO) is a common practice in families of software products consisting of reusing code from methods in legacy products in new developments. In industrial scenarios, CAO consumes high amounts of time and effort without guaranteeing good results. We propose a novel approach, Computer Assisted CAO (CACAO), that given the natural language requirements of a new product, and the legacy products from that family, ranks the legacy methods in the family for each of the new product requirements according to their relevancy to the new development. We evaluated our approach in the industrial domain of train control software. Without CACAO, software engineers tasked with the development of a new product had to manually review a total of 2200 methods in the family. Results show that CACAO can reduce the number of methods to be reviewed, and guide software engineers towards the identification of relevant legacy methods to be reused in the new product.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {194–203},
numpages = {10},
keywords = {clone and own, families of software products, software reuse},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2364412.2364417,
author = {Ripon, Shamim and Azad, Keya and Hossain, Sk Jahir and Hassan, Mehidee},
title = {Modeling and analysis of product-line variants},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364417},
doi = {10.1145/2364412.2364417},
abstract = {Formal verification of variant requirements has gained much interest in the software product line (SPL) community. Feature diagrams are widely used to model product line variants. However, there is a lack of precisely defined formal notation for representing and verifying such models. This paper presents an approach to modeling and analyzing SPL variant feature diagrams using first-order logic. It provides a precise and rigorous formal interpretation of the feature diagrams. Logical expressions can be built by modeling variants and their dependencies by using propositional connectives. These expressions can then be validated by any suitable verification tool such as Alloy. A case study of a Computer Aided Dispatch (CAD) system variant feature model is presented to illustrate the analysis and verification process.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {26–31},
numpages = {6},
keywords = {alloy, feature model, first-order logic, product line, variants},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2648511.2648521,
author = {Olaechea, Rafael and Rayside, Derek and Guo, Jianmei and Czarnecki, Krzysztof},
title = {Comparison of exact and approximate multi-objective optimization for software product lines},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648521},
doi = {10.1145/2648511.2648521},
abstract = {Software product lines (SPLs) allow stakeholders to manage product variants in a systematical way and derive variants by selecting features. Finding a desirable variant is often difficult, due to the huge configuration space and usually conflicting objectives (e.g., lower cost and higher performance). This scenario can be characterized as a multi-objective optimization problem applied to SPLs. We address the problem using an exact and an approximate algorithm and compare their accuracy, time consumption, scalability, parameter setting requirements on five case studies with increasing complexity. Our empirical results show that (1) it is feasible to use exact techniques for small SPL multi-objective optimization problems, and (2) approximate methods can be used for large problems but require substantial effort to find the best parameter setting for acceptable approximation which can be ameliorated with known good parameter ranges. Finally, we discuss the tradeoff between accuracy and time consumption when using exact and approximate techniques for SPL multi-objective optimization and guide stakeholders to choose one or the other in practice.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {92–101},
numpages = {10},
keywords = {multi-objective optimization, software product lines},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2499777.2499778,
author = {Henard, Christopher and Papadakis, Mike and Perrouin, Gilles and Klein, Jacques and Traon, Yves Le},
title = {PLEDGE: a product line editor and test generation tool},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499778},
doi = {10.1145/2499777.2499778},
abstract = {Specific requirements of clients lead to the development of variants of the same software. These variants form a Software Product Line (SPL). Ideally, testing a SPL involves testing all the software products that can be configured through the combination of features. This, however, is intractable in practice since a) large SPLs can lead to millions of possible software variants and b) the testing process is usually limited by budget and time constraints. To overcome this problem, this paper introduces PLEDGE, an open source tool that selects and prioritizes the product configurations maximizing the feature interactions covered. The uniqueness of PLEDGE is that it bypasses the computation of the feature interactions, allowing to scale to large SPLs.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {126–129},
numpages = {4},
keywords = {T-wise, combinatorial interaction testing, prioritization, scalability, search-based approaches, software product lines},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3106195.3106206,
author = {Arcaini, Paolo and Gargantini, Angelo and Vavassori, Paolo},
title = {Automated Repairing of Variability Models},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106206},
doi = {10.1145/3106195.3106206},
abstract = {Variability models are a common means for describing the commonalities and differences in Software Product Lines (SPL); configurations of the SPL that respect the constraints imposed by the variability model define the problem space. The same variability is usually also captured in the final implementation through implementation constraints, defined in terms of preprocessor directives, build files, build-time errors, etc. Configurations satisfying the implementation constraints and producing correct (compilable) programs define the solution space. Since sometimes the variability model is defined after the implementation exists, it could wrongly assess the validity of some system configurations, i.e., it could consider acceptable some configurations (not belonging to the solution space) that do not permit to obtain a correct program. We here propose an approach that automatically repairs variability models such that the configurations they consider valid are also part of the solution space. Experiments show that some existing variability models are indeed faulty and can be repaired by our approach.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {9–18},
numpages = {10},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2934466.2934475,
author = {Sousa, Gustavo and Rudametkin, Walter and Duchien, Laurence},
title = {Extending feature models with relative cardinalities},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934475},
doi = {10.1145/2934466.2934475},
abstract = {Feature modeling is widely used to capture and manage commonalities and variabilities in software product lines. Cardinality-based feature models are used when variability applies not only to the selection or exclusion of features but also to the number of times a feature can be included in a product. Feature cardinalities are usually considered to apply in either a local or global scope. However, we have identified that these interpretations are insufficient to capture the variability of cloud environments. In this paper, we redefine cardinality-based feature models to allow multiple relative cardinalities between features and we discuss the effects of relative cardinalities on feature modeling semantics, consistency and cross-tree constraints. To evaluate our approach we conducted an analysis of relative cardinalities in four cloud computing providers. In addition, we developed tools for reasoning on feature models with relative cardinalities and performed experiments to verify the performance and scalability of the approach. The results from our study indicate that extending feature models with relative cardinalities is feasible and improves variability modeling, particularly in the case of cloud environments.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {79–88},
numpages = {10},
keywords = {cardinality, constraints, feature model},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2934466.2946046,
author = {Arrieta, Aitor and Wang, Shuai and Sagardui, Goiuria and Etxeberria, Leire},
title = {Search-based test case selection of cyber-physical system product lines for simulation-based validation},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946046},
doi = {10.1145/2934466.2946046},
abstract = {Cyber-Physical Systems (CPSs) are often tested at different test levels following "X-in-the-Loop" configurations: Model-, Software- and Hardware-in-the-loop (MiL, SiL and HiL). While MiL and SiL test levels aim at testing functional requirements at the system level, the HiL test level tests functional as well as non-functional requirements by performing a real-time simulation. As testing CPS product line configurations is costly due to the fact that there are many variants to test, test cases are long, the physical layer has to be simulated and co-simulation is often necessary. It is therefore extremely important to select the appropriate test cases that cover the objectives of each level in an allowable amount of time. We propose an efficient test case selection approach adapted to the "X-in-the-Loop" test levels. Search algorithms are employed to reduce the amount of time required to test configurations of CPS product lines while achieving the test objectives of each level. We empirically evaluate three commonly-used search algorithms, i.e., Genetic Algorithm (GA), Alternating Variable Method (AVM) and Greedy (Random Search (RS) is used as a baseline) by employing two case studies with the aim of integrating the best algorithm into our approach. Results suggest that as compared with RS, our approach can reduce the costs of testing CPS product line configurations by approximately 80% while improving the overall test quality.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {297–306},
numpages = {10},
keywords = {cyber-physical system product lines, search-based software engineering, test case selection},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2648511.2648540,
author = {Kodama, Ryuichiro and Shimabukuro, Jun and Takagi, Yoshimitsu and Koizumi, Shinobu and Tano, Shun'ichi},
title = {Experiences with commonality control procedures to develop clinical instrument system},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648540},
doi = {10.1145/2648511.2648540},
abstract = {This paper reports our experience with software development based on the Software Product Line (SPL) approach employed for Clinical Instrument Integration Management Software (CIIMS). CIIMS is the system software which systemizes heterogeneous clinical instruments. These instruments require their particular management so that various parts of CIIMS are forced to be changed. This makes it difficult to create development plans to connect new instruments to CIIMS. In this paper we summarize a new estimate method called the Architecture Domain Matrix (ADM) method which effectively solved this problem in our experience. In ADM each architectural element is further decomposed into clinical operation flow elements and core assets of software are extracted from these elements. This method estimates the CIIMS commonality with precision and finally enables to successfully connect new instruments. In addition this method provides a Work Breakdown Structure (WBS) and supports development team building. WBS is generated by collecting all the changes for each operational flow element. A development team suitable for change is organized by taking into consideration all the changes for each architecture element. We integrated three different instruments into CIIMS in 18 months after applying this method to a real project and achieved 2.5 times greater productivity with the embedded software than that with our previous non-SPL process.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {254–263},
numpages = {10},
keywords = {architectures, cost estimation, domain analysis, software product lines},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2491627.2491642,
author = {Martini, Antonio and Pareto, Lars and Bosch, Jan},
title = {Communication factors for speed and reuse in large-scale agile software development},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491642},
doi = {10.1145/2491627.2491642},
abstract = {An open issue in industry is the combination of software reuse in the context of large scale Agile Software Development. The speed offered by Agile Software Development is needed for short time to market, while reuse strategies such as Software Product Line Engineering are needed for long-term productivity, efficiency, and profit. The paper investigates, through a survey, communication factors affecting both speed and reuse in 3 large companies developing embedded systems and employing Agile Software Development and Software Product Line Engineering. Our results include a prioritized list of communication related factors obtained by statistical analysis and the recognition and spread of the factors in the companies. We have recognized 5 interfaces with the Agile development team that need to be improved: system engineers (architects), product management, distributed teams, inter-project teams and sales unit. Few factors (involving inter-project communication) depend on the business drivers for the company. We also reveal that Agile teams need strategic and architectural inputs in order to be implanted in a large company employing Software Product Line Engineering. Academic and industrial training as well as different tactics for co-location would improve the communication skills of engineers. There is also a need for solutions, in the reference architecture, for fostering Agile Software Development: the goal is the combination of the focus on customer value of the teams, reusability, system requirements and avoidance of organizational dependencies.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {42–51},
numpages = {10},
keywords = {agile software development, communication, development speed, embedded systems, factors, software process improvement (SPI), software reuse, speed},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2019136.2019178,
author = {Brataas, Gunnar and Jiang, Shanshan and Reichle, Roland and Geihs, Kurt},
title = {Performance property prediction supporting variability for adaptive mobile systems},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019178},
doi = {10.1145/2019136.2019178},
abstract = {A performance property prediction (PPP) method for component-based self-adaptive applications is presented. Such performance properties are required by an adaptation middleware for reasoning about adaptation activities. Our PPP method is based on the Structure and Performance (SP) framework, a conceptually simple, yet powerful performance modelling framework based on matrices. The main contribution of this paper are the integration of SP-based PPP into a comprehensive model- and variability-based adaptation framework for context-aware mobile applications. A meta model for the SP method is described. The framework is demonstrated using a practical example.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {37},
numpages = {8},
keywords = {autonomic computing, mobile systems},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2362536.2362563,
author = {Heider, Wolfgang and Rabiser, Rick and Gr\"{u}nbacher, Paul and Lettner, Daniela},
title = {Using regression testing to analyze the impact of changes to variability models on products},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362563},
doi = {10.1145/2362536.2362563},
abstract = {Industrial product lines are typically maintained for a long time and evolve continuously to address changing requirements and new technologies. Already derived products often have to be re-derived after such changes to benefit from new and updated features. Product line engineers thus frequently need to analyze the impact of changes to variability models to prevent unexpected changes of re-derived products. In this paper we present a tool-supported approach that informs engineers about the impacts of variability model changes on existing products. Regression tests are used to determine whether existing product configurations and generated product outputs can be re-derived without unexpected effects. We evaluate the feasibility of the approach based on changes observed in a real-world software product line. More specifically, we show how our approach helps engineers performing specific evolution tasks to analyze the change impacts on existing products. We also evaluate the performance and scalability of our approach. Our results show that variability change impact analyses can be automated using model regression testing and can help reducing the gap between domain engineering and application engineering.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {196–205},
numpages = {10},
keywords = {product line evolution, regression testing, variability models},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3109729.3109734,
author = {Marc\'{e}n, Ana C. and Font, Jaime and Pastor, \'{O}scar and Cetina, Carlos},
title = {Towards Feature Location in Models through a Learning to Rank Approach},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109734},
doi = {10.1145/3109729.3109734},
abstract = {In this work, we propose a feature location approach to discover software artifacts that implement the feature functionality in a model. Given a model and a feature description, model fragments extracted from the model and the feature description are encoded based on a domain ontology. Then, a Learning to Rank algorithm is used to train a classifier that is based on the model fragments and feature description encoded. Finally, the classifier assesses the similarity between a population of model fragments and the target feature being located to find the set of most suitable feature realizations. We have evaluated the approach with an industrial case study, locating features with mean precision and recall values of around 73.75% and 73.31%, respectively (the sanity check obtains less than 35%).},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {57–64},
numpages = {8},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3442391.3442397,
author = {Silva, Leandro Flores da and OliveiraJr, Edson},
title = {SMartyModeling: an Environment for Engineering UML-based Software Product Lines},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442397},
doi = {10.1145/3442391.3442397},
abstract = {Software Product Line (SPL) has been successfully consolidated as an approach for systematic reuse. The adoption of the SPL approach aims at increasing the reuse of requirements and artifacts, thus reusing documents, source code and artifacts and ensuring better quality control to software production in a large-scale. One of the essential activities for SPL management is the modeling of variability. Variability modeling in UML-based SPL has been carried out mostly using the UML Profiling mechanism, in which new stereotypes and tagged values are created for such purpose. The available option in general-purpose UML tools for exporting UML models is through XMI files, standardized by OMG. This option is important to process XMI files in particular environments or tools, for example, managing variabilities, generating product configurations from an SPL, and even collecting metrics, and estimating SPLs. However, different versions, tool restrictions, and different file standards compromise operations involving XMI files. In this scenario, the industry has increasingly required the supporting tools for the SPL approach. However, the current support tools are mainly restricted to the problem space based on feature modeling and present problems with data integration with other tools. Therefore, we developed SMartyModeling, an environment for engineering UML-based SPLs in which variabilities are modeled as stereotypes using any UML compliant profile. This paper presents an overview of SMartyModeling, describing its motivation, main components, and available features.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {5},
numpages = {5},
keywords = {SMarty, UML, software product line, variability modeling},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.1145/2866614.2866627,
author = {Devroey, Xavier and Perrouin, Gilles and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Search-based Similarity-driven Behavioural SPL Testing},
year = {2016},
isbn = {9781450340199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2866614.2866627},
doi = {10.1145/2866614.2866627},
abstract = {Dissimilar test cases have been proven to be effective to reveal faults in software systems. In the Software Product Line (SPL) context, this criterion has been applied successfully to mimic combinatorial interaction testing in an efficient and scalable manner by selecting and prioritising most dissimilar configurations of feature models using evolutionary algorithms. In this paper, we extend dissimilarity to behavioural SPL models (FTS) in a search-based approach, and evaluate its effectiveness in terms of product and fault coverage. We investigate different distances as well as as single-objective algorithms, (dissimilarity on actions, random, all-actions). Our results on four case studies show the relevance of dissimilarity-based test generation for behavioural SPL models, especially on the largest case-study where no other approach can match it.},
booktitle = {Proceedings of the 10th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {89–96},
numpages = {8},
keywords = {Dissimilarity Testing, Featured Transition System, Software Product Line Testing},
location = {Salvador, Brazil},
series = {VaMoS '16}
}

@inproceedings{10.1145/3109729.3109753,
author = {Tenev, Vasil and Duszynski, Slawomir and Becker, Martin},
title = {Variant Analysis: Set-Based Similarity Visualization for Cloned Software Systems},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109753},
doi = {10.1145/3109729.3109753},
abstract = {Software product lines are frequently created using an extractive approach, in which a group of existing software products is reengineered to extract their reusable core. To direct that effort, it is necessary to analyze the reuse potential and the code similarity across the products. We present Variant Analysis, a tool visualizing code similarity across a group of software systems. We represent the systems as intersecting sets of content elements, and place the elements similar between any n systems into the intersection of the respective n sets. Using the resulting set model and the system structure hierarchy, we provide similarity visualizations scaling for tens of compared software systems and millions lines of code. The current Variant Analysis tool analyzes similarity of text files such as source code. However, the underlying models and visualizations can also be used for other types of data, even beyond the software domain.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {22–27},
numpages = {6},
keywords = {Similarity, set model, set visualization, software cloning, tools},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3571788.3571797,
author = {Georges, Thomas and Rice, Liam and Huchard, Marianne and K\"{o}nig, M\'{e}lanie and Nebut, Cl\'{e}mentine and Tibermacine, Chouki},
title = {Guiding Feature Models Synthesis from User-Stories: An Exploratory Approach},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571788.3571797},
doi = {10.1145/3571788.3571797},
abstract = {User-stories are commonly used to define requirements in agile project management. In Software Product Lines (SPL), a user-story corresponds to a feature description (or part of it), that can be shared by several products. In practice, large SPL include a huge number of user-stories, making variability hard to grasp and handle. In this paper we present an exploratory approach that aims to guide the synthesis of Feature Models that capture and structure the commonalities and the variability expressed in these user-stories. The built Feature Models aim to help the project understanding, maintenance and evolution. Our approach first decomposes the user-stories to extract the roles and the features, using natural language processing techniques. In a second step, we group user-stories having the same topics thanks to a clustering method. This contributes to extract more general features. In a third step, we leverage the use of Formal Concept Analysis to extract logical constraints between the features that guide Feature Model synthesis. We illustrate our approach using a dataset from our industrial partner.},
booktitle = {Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {65–70},
numpages = {6},
keywords = {Agile Process, Feature Model, Formal Concept Analysis, Natural Language Processing, Reengineering, SPL domain engineering, Software Product Line, User Story},
location = {Odense, Denmark},
series = {VaMoS '23}
}

@inproceedings{10.1145/3368089.3409675,
author = {Siegmund, Norbert and Ruckel, Nicolai and Siegmund, Janet},
title = {Dimensions of software configuration: on the configuration context in modern software development},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409675},
doi = {10.1145/3368089.3409675},
abstract = {With the rise of containerization, cloud development, and continuous integration and delivery, configuration has become an essential aspect not only to tailor software to user requirements, but also to configure a software system’s environment and infrastructure. This heterogeneity of activities, domains, and processes blurs the term configuration, as it is not clear anymore what tasks, artifacts, or stakeholders are involved and intertwined. However, each re- search study and each paper involving configuration places their contributions and findings in a certain context without making the context explicit. This makes it difficult to compare findings, translate them to practice, and to generalize the results. Thus, we set out to evaluate whether these different views on configuration are really distinct or can be summarized under a common umbrella. By interviewing practitioners from different domains and in different roles about the aspects of configuration and by analyzing two qualitative studies in similar areas, we derive a model of configuration that provides terminology and context for research studies, identifies new research opportunities, and allows practitioners to spot possible challenges in their current tasks. Although our interviewees have a clear view about configuration, it substantially differs due to their personal experience and role. This indicates that the term configuration might be overloaded. However, when taking a closer look, we see the interconnections and dependencies among all views, arriving at the conclusion that we need to start considering the entire spectrum of dimensions of configuration.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {338–349},
numpages = {12},
keywords = {Dimensions of software configuration, configuration management and life cycle, developer study, variability},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3106195.3106225,
author = {Schlie, Alexander and Wille, David and Schulze, Sandro and Cleophas, Loek and Schaefer, Ina},
title = {Detecting Variability in MATLAB/Simulink Models: An Industry-Inspired Technique and its Evaluation},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106225},
doi = {10.1145/3106195.3106225},
abstract = {Model-based languages such as MATLAB/Simulink play an essential role in the model-driven development of software systems. To comply with new requirements, it is common practice to create new variants by copying existing systems and modifying them. Commonly referred to as clone-and-own, severe problems arise in the long-run when no dedicated variability management is installed. To allow for a documented and structured reuse of systems, their variability information needs to be reverse-engineered. In this paper, we propose an advanced comparison procedure, the Matching Window Technique, and a customizable metric. Both allow us to overcome structural alterations commonly performed during clone-and-own. We analyze related MATLAB/Simulink models and determine, classify and represent their variability information in an understandable way. With our technique, we assist model engineers in maintaining and evolving existing variants. We provide three feasibility studies with real-world models from the automotive domain and show our technique to be fast and precise. Furthermore, we perform semi-structured interviews with domain experts to assess the potential applicability of our technique in practice.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {215–224},
numpages = {10},
keywords = {MATLAB/Simulink, software maintainability, variability mining},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/2362536.2362567,
author = {Savolainen, Juha and Mannion, Mike and Kuusela, Juha},
title = {Developing platforms for multiple software product lines},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362567},
doi = {10.1145/2362536.2362567},
abstract = {Many approaches to software product line engineering have been founded on the development of a single product line platform. However as customer requirements change and new products are added to the product line, software producers recognize that the platform cannot be "stretched" indefinitely and a significant problem is striking a balance between development efficiency by increasing platform commonality and customer dissatisfaction from products with additional undesirable features and properties.One alternative is to develop multiple product lines (MPLs). However the challenge remains about what to include in a multiple product line platform. Drawing upon industrial experience of working with 4 companies, this paper explores the characteristics of the contexts in which MPLs are a viable alternative development strategy and then proposes a framework of approaches to platform development.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {220–228},
numpages = {9},
keywords = {industrial experience, multiple product lines, software reuse},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791085,
author = {Font, Jaime and Arcega, Lorena and Haugen, \O{}ystein and Cetina, Carlos},
title = {Building software product lines from conceptualized model patterns},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791085},
doi = {10.1145/2791060.2791085},
abstract = {Software Product Lines (SPLs) can be established from a set of similar models. Establishing the Product Line by mechanically finding model differences may not be the best approach. The identified model fragments may not be seen as recognizable units by the application engineers. We propose to identify model patterns by human-in-the-loop and conceptualize them as reusable model fragments. The approach provides the means to identify and extract those model patterns and further apply them to existing product models. Model fragments obtained by applying our approach seem to perform better than mechanically found ones. It turns out that the repetition of a fragment does not guarantee its relevance as reusable asset for the SPL engineers and vice versa, a fragment that has not been repeated yet, may be relevant as a reusable asset. We have validated these ideas with our industrial partner BSH, an induction hobs manufacturer that generates the firmware of their products from a model-driven SPL.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {46–55},
numpages = {10},
keywords = {human-in-the-loop, model-based software product lines, reverse engineering, variability identification},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2934466.2934493,
author = {Beuche, Danilo and Schulze, Michael and Duvigneau, Maurice},
title = {When 150% is too much: supporting product centric viewpoints in an industrial product line},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934493},
doi = {10.1145/2934466.2934493},
abstract = {Use of product lines promises easier production of varying products from a common base using the concepts of variation points and binding of these. This paper describes a successful industrial application of product line concepts based on the superset approach (aka 150%), where the success provided strong improvements in many aspects (e.g. product quality, amount of code to be maintained, time to delivery of new variants) but also introduced new challenges in the production of certain required product assets such as documentation or source code. We focus on the latter in this paper. We'll discuss the challenges which arose in the industrial use case from using the 150% superset approach with standard engineering programming languages and workflows and how the challenges have been solved. We evaluate our approach in a real industrial product line setting and the results show the effectiveness as well as the efficiency of the realized solution.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {262–269},
numpages = {8},
keywords = {industry use case, product lines, transformation, variant management},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2791060.2791087,
author = {ter Beek, M. H. and Legay, A. and Lafuente, A. Lluch and Vandin, A.},
title = {Statistical analysis of probabilistic models of software product lines with quantitative constraints},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791087},
doi = {10.1145/2791060.2791087},
abstract = {We investigate the suitability of statistical model checking for the analysis of probabilistic models of software product lines with complex quantitative constraints and advanced feature installation options. Such models are specified in the feature-oriented language QFLan, a rich process algebra whose operational behaviour interacts with a store of constraints, neatly separating product configuration from product behaviour. The resulting probabilistic configurations and behaviour converge seamlessly in a semantics based on DTMCs, thus enabling quantitative analyses ranging from the likelihood of certain behaviour to the expected average cost of products. This is supported by a Maude implementation of QFLan, integrated with the SMT solver Z3 and the distributed statistical model checker MultiVeStA. Our approach is illustrated with a bikes product line case study.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {11–15},
numpages = {5},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3023956.3023959,
author = {Ochoa, Lina and Pereira, Juliana Alves and Gonz\'{a}lez-Rojas, Oscar and Castro, Harold and Saake, Gunter},
title = {A survey on scalability and performance concerns in extended product lines configuration},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023959},
doi = {10.1145/3023956.3023959},
abstract = {Product lines have been employed as a mass customisation method that reduces production costs and time-to-market. Multiple product variants are represented in a product line, however the selection of a particular configuration depends on stakeholders' functional and non-functional requirements. Methods like constraint programming and evolutionary algorithms have been used to support the configuration process. They consider a set of product requirements like resource constraints, stakeholders' preferences, and optimization objectives. Nevertheless, scalability and performance concerns start to be an issue when facing large-scale product lines and runtime environments. Thus, this paper presents a survey that analyses strengths and drawbacks of 21 approaches that support product line configuration. This survey aims to: i) evidence which product requirements are currently supported by studied methods; ii) how scalability and performance is considered in existing approaches; and iii) point out some challenges to be addressed in future research.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {5–12},
numpages = {8},
keywords = {configuration, literature review, performance, product line, product requirements, scalability, survey},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/3493244.3493274,
author = {Silva, Leandro F. and OliveiraJr, Edson},
title = {SMartyModeling: an instance of VMTools-RA for Engineering UML-based Software Product Lines},
year = {2021},
isbn = {9781450395533},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493244.3493274},
doi = {10.1145/3493244.3493274},
abstract = {Software Product Line (SPL) life cycle comprises a set of essential activities. Variability Management (VM) is one of its most important activities to the success of an SPL, especially those based on UML, as the solution space encompasses different diagrams and perspectives on variability. However, the lack of tools to support UML-based SPLs reflects difficulties in adopting this approach. This scenario motivated the development of SMartyModeling, an environment for engineering UML-based SPL. The SMartyModeling architecture was instantiated based on VMTools-RA, an existing reference architecture for software variability tools. VMTools-RA describes architectural requirements, elements and views on software variability, which aid one to instantiate variability tool architectures. The instantiation process started from the identification of requirements, selection of elements, modules, and visions of VMTools-RA, planning and design of the architectural solutions, implementation of modules and organization of features. We then analyzed the feasibility of adopting VMTools-RA for instantiating an specific tool architecture. In this sense, such instantiation is part of the development process of SMartyModeling, which includes the main activities related to VM. We also empirically evaluated SMartyModeling in three ways: (i) a field study to analyze the instantiation process and the decisions taken; (ii) a comparative experiment analyzing efficiency and effectiveness of SMartyModeling in relation to a general purpose UML tool; and (iii) an evaluation of aspects related to perceived ease of use and perceived usability. The results of such evaluations provide initial evidence VMTools-RA is feasible to instantiate specific architectures and SMartyModeling is feasible to support to VM for UML-based SPLs.},
booktitle = {Proceedings of the XX Brazilian Symposium on Software Quality},
articleno = {33},
numpages = {10},
keywords = {Software Product Line. SMartyModeling. Environment for Modeling Software Product Line. VMTools-RA. UML. Empirical studies.},
location = {Virtual Event, Brazil},
series = {SBQS '21}
}

@inproceedings{10.1145/2491627.2491630,
author = {Linsbauer, Lukas and Lopez-Herrejon, E. Roberto and Egyed, Alexander},
title = {Recovering traceability between features and code in product variants},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491630},
doi = {10.1145/2491627.2491630},
abstract = {Many companies offer a palette of similar software products though they do not necessarily have a Software Product Line (SPL). Rather, they start building and selling individual products which they then adapt, customize and extend for different customers. As the number of product variants increases, these companies then face the severe problem of having to maintain them all. Software Product Lines can be helpful here - not so much as a platform for creating new products but as a means of maintaining the existing ones with their shared features. Here, an important first step is to determine where features are implemented in the source code and in what product variants. To this end, this paper presents a novel technique for deriving the traceability between features and code in product variants by matching code overlaps and feature overlaps. This is a difficult problem because a feature's implementation not only covers its basic functionality (which does not change across product variants) but may include code that deals with feature interaction issues and thus changes depending on the combination of features present in a product variant. We empirically evaluated the approach on three non-trivial case studies of different sizes and domains and found that our approach correctly identifies feature to code traces except for code that traces to multiple disjunctive features, a rare case involving less than 1% of the code.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {131–140},
numpages = {10},
keywords = {features, product variants, traceability},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3483899.3483907,
author = {Uch\^{o}a, Anderson and Assun\c{c}\~{a}o, Wesley Klewerton Guez and Garcia, Alessandro},
title = {Do Critical Components Smell Bad? An Empirical Study with Component-based Software Product Lines},
year = {2021},
isbn = {9781450384193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483899.3483907},
doi = {10.1145/3483899.3483907},
abstract = {Component-based software product line (SPL) consists of a set of software products that share common components. For a proper SPL product composition, each component has to follow three principles: encapsulating a single feature, restricting data access, and be replaceable. However, it is known that developers usually introduce anomalous structures, i.e., code smells, along the implementation of components. These code smells might violate one or more component principles and hinder the SPL product composition. Thus, developers should identify code smells in component-based SPLs, especially those affecting highly interconnected components, which are called critical components. Nevertheless, there is limited evidence of how smelly these critical components tend to be in component-based SPLs. To address this limitation, this paper presents a survey with developers of three SPLs. We inquire these developers about their perceptions of a critical component. Then, we characterize critical components per SPL, and identify nine recurring types of code smells. Finally, we quantitatively assess the smelliness of the critical components. Our results suggest that: (i) critical components are ten times more prone to have code smells than non-critical ones; (ii) the most frequent code smell types affecting critical components violate several component principles together; and (iii) these smell types affect multiple SPL components.},
booktitle = {Proceedings of the 15th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {21–30},
numpages = {10},
keywords = {Component-based software product line, empirical study, smell},
location = {Joinville, Brazil},
series = {SBCARS '21}
}

@inproceedings{10.1145/2791060.2791071,
author = {Wozniak, Len and Clements, Paul},
title = {How automotive engineering is taking product line engineering to the extreme},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791071},
doi = {10.1145/2791060.2791071},
abstract = {Automotive manufacturing ranks among the most extreme instances of systems and software product line engineering (PLE). The product family numbers in the millions, each product is highly complex in its own right, and the variation across products is literally astronomical in scale. This paper explores the aspects that make the domain extreme and the very specific implications they have for PLE. These implications include the need for efficient manufacturing, complexity management, concurrent development streams, globally distributed engineering and production, a hierarchical product family tree, multi-level variation binding, constraint management, and a highly robust and integrated PLE tooling environment. Happily, the PLE paradigm supporting these implications brings about a number of opportunities for analysis and automation that provide efficiencies of production previously unattainable. We focus on one example in depth: The management and automated generation of the many thousands of calibration parameters that determine vehicle-specific software behavior. Throughout, we use the vehicle product line at General Motors, which we believe to be the world's largest, to illustrate and ground our journey through automotive PLE.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {327–336},
numpages = {10},
keywords = {automotive product lines, bill-of-features, feature modeling, feature profiles, product configurator, product line engineering, product portfolio, second generation product line engineering, software product lines, variation points},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2934466.2962733,
author = {Lillack, Max and Berger, Thorsten and Hebig, Regina},
title = {Experiences from reengineering and modularizing a legacy software generator with a projectional language workbench},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2962733},
doi = {10.1145/2934466.2962733},
abstract = {We present a case study of migrating a legacy language infrastructure and its codebase to a projectional language workbench. Our subject is the generator tool ADS used for generating COBOL code for critical software systems. We decompose the ADS language into smaller sub-languages, which we implement as individual DSLs in the projectional language workbench JetBrains Meta Programming System (MPS). Our focus is on ADS' preprocessor sub-language, used to realize static variability by conditionally including or parameterizing target code. The modularization of ADS supports future extensions and tailoring the language infrastructure to the needs of individual customers. We re-implement the generation process of target code as chained model-to-model and model-to-text transformations. For migrating existing ADS code, we implement an importer relying on a parser in order to create a model in MPS. We validate the approach using an ADS codebase for handling car registrations in the Netherlands. Our case study shows the feasibility and benefits (e.g., language extensibility and modern editors) of the migration, but also smaller caveats (e.g., small syntax adaptations, the necessity of import tools, and providing training to developers). Our experiences are useful for practitioners attempting a similar migration of legacy generators to a projectional language workbench.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {346–353},
numpages = {8},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2866614.2866625,
author = {Nieke, Michael and Seidl, Christoph and Schuster, Sven},
title = {Guaranteeing Configuration Validity in Evolving Software Product Lines},
year = {2016},
isbn = {9781450340199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2866614.2866625},
doi = {10.1145/2866614.2866625},
abstract = {Software Product Lines (SPLs) are an approach to capture families of closely related software systems in terms of commonalities and variabilities where individual variants are defined by configurations of selected features. Specific (partial) configurations may be of particular importance to SPL manufacturers, e.g., if they are very popular or used by major customers. SPLs are subject to evolution, which may inadvertently break existing configurations, e.g., if a previously selected feature does no longer exist. This is problematic as it may delay or completely prevent creation of previously existing important variants causing monetary loss and customer dissatisfaction. In this paper, we present a method to lock specific configurations to ensure their validity during evolution of the SPL. For this, we present Temporal Feature Models (TFMs) and dedicated evolution operations as a semantic-enriched first-class notion for evolution of feature models, which we use to assess the impact on existing configurations. Using the presented method, it is possible to guarantee that locked configurations remain valid during SPL evolution and make statements on which part of the evolution would break the configurations.},
booktitle = {Proceedings of the 10th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {73–80},
numpages = {8},
keywords = {Configuration, Evolution, Software Product Line (SPL), Temporal Feature Model (TFM)},
location = {Salvador, Brazil},
series = {VaMoS '16}
}

@inproceedings{10.1145/2934466.2934488,
author = {Plakidas, Konstantinos and Stevanetic, Srdjan and Schall, Daniel and Ionescu, Tudor B. and Zdun, Uwe},
title = {How do software ecosystems evolve? a quantitative assessment of the r ecosystem.},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934488},
doi = {10.1145/2934466.2934488},
abstract = {In this work we advance the understanding of software eco-systems research by examining the structure and evolution of the R statistical computing open-source ecosystem. Our research attempts to shed light on the following intriguing question: what makes software ecosystems successful? The approach we follow is to perform a quantitative analysis of the R ecosystem. R is a well-established and popular ecosystem, whose community and marketplace are steadily growing. We assess and quantify the ecosystem throughout its history, and derive metrics on its core software components, the marketplace as well as its community. We use our insights to make observations that are applicable to ecosystems in general, validate existing theories from the literature, and propose a predictive model for the evolution of software packages. Our results show that the success of the ecosystem relies on a strong commitment by a small core of users who support a large and growing community.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {89–98},
numpages = {10},
keywords = {R, empirical study, predictive model, software ecosystems},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2362536.2362543,
author = {N\"{o}hrer, Alexander and Biere, Armin and Egyed, Alexander},
title = {A comparison of strategies for tolerating inconsistencies during decision-making},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362543},
doi = {10.1145/2362536.2362543},
abstract = {Tolerating inconsistencies is well accepted in design modeling because it is often neither obvious how to fix an inconsistency nor important to do so right away. However, there are technical reasons why inconsistencies are not tolerated in many areas of software engineering. The most obvious being that common reasoning engines are rendered (partially) useless in the presence of inconsistencies. This paper investigates automated strategies for tolerating inconsistencies during decision-making in product line engineering, based on isolating parts from reasoning that cause inconsistencies. We compare trade offs concerning incorrect and incomplete reasoning and demonstrate that it is even possible to fully eliminate incorrect reasoning in the presence of inconsistencies at the expense of marginally less complete reasoning. Our evaluation is based on seven medium-to-large size software product line case studies. It is important to note that our mechanism for tolerating inconsistencies can be applied to arbitrary SAT problems and thus the basic principles of this approach are applicable to other domains also.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {11–20},
numpages = {10},
keywords = {formal reasoning, inconsistencies, user guidance},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791068,
author = {B\'{e}can, Guillaume and Behjati, Razieh and Gotlieb, Arnaud and Acher, Mathieu},
title = {Synthesis of attributed feature models from product descriptions},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791068},
doi = {10.1145/2791060.2791068},
abstract = {Many real-world product lines are only represented as nonhierarchical collections of distinct products, described by their configuration values. As the manual preparation of feature models is a tedious and labour-intensive activity, some techniques have been proposed to automatically generate boolean feature models from product descriptions. However, none of these techniques is capable of synthesizing feature attributes and relations among attributes, despite the huge relevance of attributes for documenting software product lines. In this paper, we introduce for the first time an algorithmic and parametrizable approach for computing a legal and appropriate hierarchy of features, including feature groups, typed feature attributes, domain values and relations among these attributes. We have performed an empirical evaluation by using both randomized configuration matrices and real-world examples. The initial results of our evaluation show that our approach can scale up to matrices containing 2,000 attributed features, and 200,000 distinct configurations in a couple of minutes.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {1–10},
numpages = {10},
keywords = {attributed feature models, product descriptions},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2019136.2019187,
author = {Abbas, Nadeem},
title = {Towards autonomic software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019187},
doi = {10.1145/2019136.2019187},
abstract = {We envision an Autonomic Software Product Line (ASPL). The ASPL is a dynamic software product line that supports self adaptable products. We plan to use reflective architecture to model and develop ASPL. To evaluate the approach, we have implemented three autonomic product lines which show promising results. The ASPL approach is at initial stages, and require additional work. We plan to exploit online learning to realize more dynamic software product lines to cope with the problem of product line evolution. We propose on-line knowledge sharing among products in a product line to achieve continuous improvement of quality in product line products.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {44},
numpages = {8},
keywords = {knowledge, on-line learning, self-adaptation},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2362536.2362553,
author = {Andersen, Nele and Czarnecki, Krzysztof and She, Steven and W\k{a}sowski, Andrzej},
title = {Efficient synthesis of feature models},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362553},
doi = {10.1145/2362536.2362553},
abstract = {Variability modeling, and in particular feature modeling, is a central element of model-driven software product line architectures. Such architectures often emerge from legacy code, but, unfortunately creating feature models from large, legacy systems is a long and arduous task.We address the problem of automatic synthesis of feature models from propositional constraints. We show that this problem is NP-hard. We design efficient techniques for synthesis of models from respectively CNF and DNF formulas, showing a 10- to 1000-fold performance improvement over known techniques for realistic benchmarks.Our algorithms are the first known techniques that are efficient enough to be applied to dependencies extracted from real systems, opening new possibilities of creating reverse engineering and model management tools for variability models. We discuss several such scenarios in the paper.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {106–115},
numpages = {10},
keywords = {feature models, software product lines, variability models},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2362536.2362571,
author = {Flores, Rick and Krueger, Charles and Clements, Paul},
title = {Mega-scale product line engineering at General Motors},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362571},
doi = {10.1145/2362536.2362571},
abstract = {General Motors faces probably the most complex Systems and Software Product Line Engineering (PLE) challenges ever, in terms of product complexity, richness of variation, size of organization, and an unforgiving requirement to support over a dozen simultaneous development streams all geared towards each new model year. To meet this challenge, GM turned to an advanced set of explicitly defined product line engineering solutions, which have been referred to as Second Generation PLE (2GPLE). This includes reliance on features as the lingua franca to express product differences in all phases of the lifecycle, deeply nested hierarchical product lines, industrial strength automation to provide modeling consistency throughout, and more. This paper explains how 2GPLE is being applied at General Motors, and the technical and organizational lessons learned so far.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {259–268},
numpages = {10},
keywords = {bill-of-features, feature modeling, feature profiles, hierarchical product lines, product baselines, product configurator, product line engineering, product portfolio, software product lines, variation points},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2364412.2364456,
author = {Roos-Frantz, Fabricia and Galindo, Jos\'{e} A. and Benavides, David and Ruiz-Cort\'{e}s, Antonio},
title = {FaMa-OVM: a tool for the automated analysis of OVMs},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364456},
doi = {10.1145/2364412.2364456},
abstract = {Orthogonal Variability Model (OVM) is a modelling language for representing variability in Software Product Line Engineering. The automated analysis of OVMs is defined as the computer-aided extraction of information from such models. In this paper, we present FaMa-OVM, which is a pioneer tool for the automated analysis of OVMs. FaMa-OVM is easy to extend or integrate in other tools. It has been developed as part of the FaMa ecosystem enabling the benefits coming from other tools of that ecosystem as FaMaFW and BeTTy.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {250–254},
numpages = {5},
keywords = {OVMs, software products lines, tools},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791074,
author = {Reuling, Dennis and B\"{u}rdek, Johannes and Rot\"{a}rmel, Serge and Lochau, Malte and Kelter, Udo},
title = {Fault-based product-line testing: effective sample generation based on feature-diagram mutation},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791074},
doi = {10.1145/2791060.2791074},
abstract = {Testing every member of a product line individually is often impracticable due to large number of possible product configurations. Thus, feature models are frequently used to generate samples, i.e., subsets of product configurations under test. Besides the extensively studied combinatorial interaction testing (CIT) approach for coverage-driven sample generation, only few approaches exist so far adopting mutation testing to emulate faults in feature models to be detected by a sample. In this paper, we present a mutation-based sampling framework for fault-based product-line testing. We define a comprehensive catalog of atomic mutation operators on the graphical representation of feature models. This way, we are able (1) to also define complex mutation operators emulating more subtle faults, and (2) to classify operators semantically, e.g., to avoid redundant and equivalent mutants. We further introduce similarity-based mutant selection and higher order mutation strategies to reduce testing efforts. Our implementation is based on the graph transformation engine Henshin and is evaluated concerning effectiveness/efficiency trade-offs.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {131–140},
numpages = {10},
keywords = {combinatorial interaction testing, mutation testing},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2019136.2019188,
author = {Mather, Neil},
title = {Concepts and implementation techniques for web systems product-lines using existing frameworks},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019188},
doi = {10.1145/2019136.2019188},
abstract = {In the construction of an enterprise web system spanning multiple domains and customers, software product line engineering is needed to manage the commonalities and variabilities of the product family. Concepts and implementation techniques need to be defined for web systems product lines to enable systematic reuse. Through the definition of these concepts, and the documentation of these techniques, a methodology for reusing existing web frameworks to create enterprise web systems product lines can be defined.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {45},
numpages = {7},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2934466.2934467,
author = {Ferreira, Gabriel and Malik, Momin and K\"{a}stner, Christian and Pfeffer, J\"{u}rgen and Apel, Sven},
title = {Do #ifdefs influence the occurrence of vulnerabilities? an empirical study of the linux kernel},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934467},
doi = {10.1145/2934466.2934467},
abstract = {Preprocessors support the diversification of software products with #ifdefs, but also require additional effort from developers to maintain and understand variable code. We conjecture that #ifdefs cause developers to produce more vulnerable code because they are required to reason about multiple features simultaneously and maintain complex mental models of dependencies of configurable code.We extracted a variational call graph across all configurations of the Linux kernel, and used configuration complexity metrics to compare vulnerable and non-vulnerable functions considering their vulnerability history. Our goal was to learn about whether we can observe a measurable influence of configuration complexity on the occurrence of vulnerabilities.Our results suggest, among others, that vulnerable functions have higher variability than non-vulnerable ones and are also constrained by fewer configuration options. This suggests that developers are inclined to notice functions appear in frequently-compiled product variants. We aim to raise developers' awareness to address variability more systematically, since configuration complexity is an important, but often ignored aspect of software product lines.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {65–73},
numpages = {9},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2791060.2791100,
author = {ter Beek, Maurice H. and Fantechi, Alessandro and Gnesi, Stefania},
title = {Applying the product lines paradigm to the quantitative analysis of collective adaptive systems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791100},
doi = {10.1145/2791060.2791100},
abstract = {Engineering a Collective Adaptive System (CAS) requires the support of a framework for quantitative modeling and analysis of the system. In order to jointly address variability and quantitative analysis, we apply the Product Lines paradigm, considered at the level of system engineering, to a case study of the European project QUANTICOL, by first defining a reference feature model and then adding feature attributes and global quantitative constraints, in the form of a Clafer attributed feature model. ClaferMOOVisualizer is subsequently used for quantitative analyses and multi-objective optimization of the resulting attributed feature model.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {321–326},
numpages = {6},
keywords = {ClaferMOO, collective adaptive systems, multi-objective optimization, quantitative analysis, quantitative modeling, variability analysis},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@proceedings{10.1145/3658644,
title = {CCS '24: Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is with great enthusiasm that we, on behalf of the Organizing Committee, invite you to join us for the 31st ACM SIGSAC Conference on Computer and Communications Security (CCS), a premier security and privacy conference where researchers, practitioners, and educators come together to present, learn, and debate research, innovation, and trends in the field of Computer and Communications Security and Privacy.This year, we are proud to introduce our conference theme to be "Inclusion, Mentorship, Community." These three pillars reflect our collective commitment to fostering a vibrant, supportive, and forwardthinking environment within the CCS community. Particularly, we host our inaugural Doctoral Symposium, which offers PhD students a unique platform to receive timely, constructive feedback on their dissertation research from leading experts in our community. Additionally, our first-ever Diversity, Equity, and Inclusion (DEI) Workshop is designed to cultivate a culture that embraces diversity and champions equity in our field. Moreover, understanding the importance of guidance and support, we have organized panels focusing on Student Mentoring, Faculty Mentoring, and Public Service. These panels are designed to facilitate mentorship connections, share valuable experiences, and encourage service that extends the impact of our work beyond academia. These new initiatives are also opportunities to strengthen the bonds within our CCS community.Regarding the main conference, this year's main conference is our largest ever, featuring 328 paper presentations that showcase the latest research and developments in our field. We are also honored to have two distinguished keynote speakers: Dr. Dan Boneh and Dr. Gene Tsudik, who will share their invaluable insights and perspectives on pressing topics in security and privacy. Additionally, 18 specialized workshops will take place on the pre-conference and post-conference days, providing platforms for focused discussions and collaborations on numerous specialized topics.},
location = {Salt Lake City, UT, USA}
}

@inproceedings{10.1145/2362536.2362542,
author = {Pleuss, Andreas and Hauptmann, Benedikt and Keunecke, Markus and Botterweck, Goetz},
title = {A case study on variability in user interfaces},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362542},
doi = {10.1145/2362536.2362542},
abstract = {Software Product Lines (SPL) enable efficient derivation of products. SPL concepts have been applied successfully in many domains including interactive applications. However, the user interface (UI) part of applications has barely been addressed yet. While standard SPL concepts allow derivation of functionally correct UIs, there are additional non-functional requirements, like usability, which have to be considered. This paper presents a case study investigating UI variability found in variants of the commercial web-based information system HIS-GX/QIS. We analyze which aspects of a UI vary and to which degree. The results show that just tweaking the final UI (e.g., using stylesheets) is not sufficient but there is a need for more customization which must be supported by, e.g., UI-specific models.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {6–10},
numpages = {5},
keywords = {software product lines, user interface engineering},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3233027.3233046,
author = {Beek, Maurice H. ter and Fantechi, Alessandro and Gnesi, Stefania},
title = {Product line models of large cyber-physical systems: the case of ERTMS/ETCS},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233046},
doi = {10.1145/3233027.3233046},
abstract = {A product line perspective may help to understand the possible variants in interactions between the subsystems of a large, cyber-physical system. This observation is exemplified in this paper by proposing a feature model of the family of ERTMS/ETCS train control systems and their foreseen extensions. This model not only shows the different components that have to be installed when deploying the system at the different levels established by the ERTMS/ETCS standards, but it also helps to identify and discuss specific issues, such as the borders between onboard and wayside equipment, different manufacturers of the subsystems, interoperability among systems developed at different levels, backward compatibility of trains equipped with higher level equipment running on lines equipped with lower level equipment, and evolution towards future trends of railway signalling. The feature model forms the basis for formal modelling of the behaviour of the critical components of the system and for evaluating the overall cost, effectiveness and sustainability, for example by adding cost and performance attributes to the feature model.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {208–214},
numpages = {7},
keywords = {ERTMS/ETCS train control systems, cyber-physical systems, feature models, product lines, variability},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2362536.2362547,
author = {Johansen, Martin Fagereng and Haugen, \O{}ystein and Fleurey, Franck},
title = {An algorithm for generating t-wise covering arrays from large feature models},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362547},
doi = {10.1145/2362536.2362547},
abstract = {A scalable approach for software product line testing is required due to the size and complexity of industrial product lines. In this paper, we present a specialized algorithm (called ICPL) for generating covering arrays from feature models. ICPL makes it possible to apply combinatorial interaction testing to software product lines of the size and complexity found in industry. For example, ICPL allows pair-wise testing to be readily applied to projects of about 7,000 features and 200,000 constraints, the Linux Kernel, one of the largest product lines where the feature model is available. ICPL is compared to three of the leading algorithms for t-wise covering array generation. Based on a corpus of 19 feature models, data was collected for each algorithm and feature model when the algorithm could finish 100 runs within three days. These data are used for comparing the four algorithms. In addition to supporting large feature models, ICPL is quick, produces small covering arrays and, even though it is non-deterministic, produces a covering array of a similar size within approximately the same time each time it is run with the same feature model.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {46–55},
numpages = {10},
keywords = {combinatorial interaction testing, feature models, product lines, testing},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791107,
author = {Ji, Wenbin and Berger, Thorsten and Antkiewicz, Michal and Czarnecki, Krzysztof},
title = {Maintaining feature traceability with embedded annotations},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791107},
doi = {10.1145/2791060.2791107},
abstract = {Features are commonly used to describe functional and nonfunctional aspects of software. To effectively evolve and reuse features, their location in software assets has to be known. However, locating features is often difficult given their crosscutting nature. Once implemented, the knowledge about a feature's location quickly deteriorates, requiring expensive recovering of these locations. Manually recording and maintaining traceability information is generally considered expensive and error-prone. In this paper, we argue to the contrary and hypothesize that such information can be effectively embedded into software assets, and that arising costs will be amortized by the benefits of this information later during development. We test this hypothesis in a study where we simulate the development of a product line of cloned/forked projects using a lightweight code annotation approach. We identify annotation evolution patterns and measure the cost and benefit of these annotations. Our results show that not only the cost of adding annotations, but also that of maintaining them is small compared to the actual development cost. Embedding the annotations into assets significantly reduced the maintenance cost because they naturally co-evolve with the assets. Our results also show that a majority of these annotations provides a benefit for feature-related code maintenance tasks, such as feature propagation and migrating clones into a platform.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {61–70},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2499777.2500718,
author = {Abbas, Nadeem and Andersson, Jesper},
title = {Architectural reasoning for dynamic software product lines},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500718},
doi = {10.1145/2499777.2500718},
abstract = {Software quality is critical in today's software systems. A challenge is the trade-off situation architects face in the design process. Designers often have two or more alternatives, which must be compared and put into context before a decision is made. The challenge becomes even more complex for dynamic software product lines, where domain designers have to take runtime variations into consideration as well. To address the problem we propose extensions to an architectural reasoning framework with constructs/artifacts to define and model a domain's scope and dynamic variability. The extended reasoning framework encapsulates knowledge to understand and reason about domain quality behavior and self-adaptation as a primary variability mechanism. The framework is demonstrated for a self-configuration property, self-upgradability on an educational product-line.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {117–124},
numpages = {8},
keywords = {adaptation, architectural reasoning, software product lines, variability},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/2648511.2648516,
author = {Reinhartz-Berger, Iris and Figl, Kathrin},
title = {Comprehensibility of orthogonal variability modeling languages: the cases of CVL and OVM},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648516},
doi = {10.1145/2648511.2648516},
abstract = {As the complexity and variety of systems and software products have increased, the ability to manage their variability effectively and efficiently became crucial. To this end, variability can be specified either as an integral part of the development artifacts or in a separate orthogonal variability model. Lately, orthogonal variability models attract a lot of attention due to the fact that they do not require changing the complexity of the development artifacts and can be used in conjunction with different development artifacts. Despite this attention and to the best of our knowledge, no empirical study examined the comprehensibility of orthogonal variability models.In this work, we conducted an exploratory experiment to examine potential comprehension problems in two common orthogonal variability modeling languages, namely, Common Variability Language (CVL) and Orthogonal Variability Model (OVM). We examined the comprehensibility of the variability models and their relations to the development artifacts for novice users. To measure comprehensibility we used comprehension score (i.e., percentage of correct solution), time spent to complete tasks, and participants' perception of difficulty of different model constructs. The results showed high comprehensibility of the variability models, but low comprehensibility of the relations between the variability models and the development artifacts. Although the comprehensibility of CVL and OVM was similar in terms of comprehension score and time spent to complete tasks, novice users perceived OVM as more difficult to comprehend.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {42–51},
numpages = {10},
keywords = {CVL, OVM, empirical study, model comprehension, variability analysis},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2019136.2019173,
author = {Fukuda, Takeshi and Atarashi, Yoshitaka and Yoshimura, Kentaro},
title = {An approach to evaluate time-dependent changes in feature constraints},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019173},
doi = {10.1145/2019136.2019173},
abstract = {Feature selections mining is the process of discovering potentially feature associations and constraints in data. Especially, mining from time-series data obtains feature constraint trends. In this paper, we describe an approach to evaluate feature constraint trends and present results of two case studies. Feature selections mining was applied to a product transactions database at Hitachi. The product transactions had 148 optional features, and 8,372 products were derived from the product line. Both case studies focus on transaction-time periods: time series and time intervals. Feature selections mining discovered feature constraints around 100 rules in each study, and determined they constantly change.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {33},
numpages = {5},
keywords = {embedded systems, feature modeling, industry case study, software product line engineering},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2791060.2791102,
author = {Mu\~{n}oz-Fern\'{a}ndez, Juan C. and Tamura, Gabriel and Raicu, Irina and Mazo, Ra\'{u}l and Salinesi, Camille},
title = {REFAS: a PLE approach for simulation of self-adaptive systems requirements},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791102},
doi = {10.1145/2791060.2791102},
abstract = {Model simulation has demonstrated its usefulness in evaluation and decision-making for improving preliminary versions of artefacts before production. Particularly, one of the main goals of simulation is to verify model properties based on data collected from its execution. In this paper, we present the simulation capabilities of our REFAS framework for specifying requirements models for dynamic software products lines and self-adaptive systems. The simulation is controlled by a feedback loop and a reasoning engine that operates on the functional and non-functional requirements. The paper contribution is threefold. First, REFAS allows developers to evaluate and improve requirements models through their simulation capabilities. Second, REFAS provides rich feedback in its interactive simulations for the human modeller to make informed decisions to improve her model. Third, REFAS automates the generation of simulation scenarios required to verify the model adequacy and correctness. We evaluate our contribution by comparing the application of REFAS to a case study used in other approaches.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {121–125},
numpages = {5},
keywords = {MAPE-K loops, dynamic adaptation, dynamic software product lines, requirements engineering, simulation},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.5555/1753235.1753263,
author = {Than Tun, Thein and Boucher, Quentin and Classen, Andreas and Hubaux, Arnaud and Heymans, Patrick},
title = {Relating requirements and feature configurations: a systematic approach},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {A feature model captures various possible configurations of products within a product family. When configuring a product, several features are selected and composed. Selecting features at the program level has a general limitation of not being able to relate the resulting configuration to its requirements. As a result, it is difficult to decide whether a given configuration of features is optimal. An optimal configuration satisfies all stakeholder requirements and quantitative constraints, while ensuring that there is no extraneous feature in it. In relating requirements and feature configurations, we use the description of the problem world context in which the software is designed to operate as the intermediate description between them. The advantage of our approach is that feature selection can be done at the requirements level, and an optimal program level configuration can be generated from the requirements selected. Our approach is illustrated with a real-life problem of configuring a satellite communication software. The use of an existing tool to support our approach is also discussed.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {201–210},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2684200.2684314,
author = {Murwantara, I Made and Bordbar, Behzad and Minku, Leandro L.},
title = {Measuring Energy Consumption for Web Service Product Configuration},
year = {2014},
isbn = {9781450330015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684200.2684314},
doi = {10.1145/2684200.2684314},
abstract = {Because of the economies of scale that Cloud provides, there is great interest in hosting web services on the Cloud. Web services are created from components such as Database Management Systems and HTTP servers. There is a wide variety of components that can be used to configure a web service. The choice of components influences the performance and energy consumption. Most current research in the web service technologies focuses on system performance, and only small number of researchers give attention to energy consumption. In this paper, we propose a method to select the web service configurations which reduce energy consumption. Our method has capabilities to manage feature configuration and predict energy consumption of web service systems. To validate, we developed a technique to measure energy consumption of several web service configurations running in a Virtualized environment. Our approach allows Cloud companies to provide choices of web service technology that consumes less energy.},
booktitle = {Proceedings of the 16th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {224–228},
numpages = {5},
keywords = {Energy Aware, Machine Learning, Software Product Line, Web System},
location = {Hanoi, Viet Nam},
series = {iiWAS '14}
}

@inproceedings{10.1145/2499777.2500708,
author = {Wille, David and Holthusen, S\"{o}nke and Schulze, Sandro and Schaefer, Ina},
title = {Interface variability in family model mining},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500708},
doi = {10.1145/2499777.2500708},
abstract = {Model-driven development of software gains more and more importance, especially in domains with high complexity. In order to develop differing but still similar model-based systems, these models are often copied and modified according to the changed requirements. As the variability between these different models is not documented, issues arise during maintenance. For example, applying patches becomes a tedious task because errors have to be fixed in all of the created models and no information about modified and unchanged parts exists. In this paper, we present an approach to analyze related models and determine the variability between them. This analysis provides crucial information about the variability (i.e., changed parts, additional parts, and parts without any modification) between the models in order to create family models. The particular focus is the analysis of models containing components with differing interfaces.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {44–51},
numpages = {8},
keywords = {SPL, analysis, family model mining, variability},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3218585.3218670,
author = {Martins, Luana Almeida and Parreira, Paulo Afonso and Freire, Andr\'{e} Pimenta and Costa, Heitor},
title = {Exploratory Study on the Use of Software Product Lines in the Development of Quality Assistive Technology Software},
year = {2018},
isbn = {9781450364676},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3218585.3218670},
doi = {10.1145/3218585.3218670},
abstract = {The use of Software Product Line for the development of Assistive Technologies has not been widely explored yet. However, some studies point to the viability of using this approach to develop Assistive Technology software. Through this approach, important limiting factors to use Assistive Technologies can be overcome. These factors are related to the acquisition costs and difficulty to find products corresponding to specific and varying user needs. Considering that Software Product Line approach provides mass customization of software products, the specific needs of each user can be more easily satisfied by software developers. Furthermore, the reuse of code artifacts to development provides a fall in the acquisition cost of these software products. We present in this paper a literature review that aims to investigate how this approach has been applied to the development of Assistive Technology software. Also, we present some quality factors that should be considered to develop Assistive Technologies using Software Product Lines. Thus, the main findings of the review are grouped in order to find the main gaps to be explored in future work.},
booktitle = {Proceedings of the 8th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-Exclusion},
pages = {262–269},
numpages = {8},
keywords = {Assistive Technology, Software Product Line, Software Quality},
location = {Thessaloniki, Greece},
series = {DSAI '18}
}

@inproceedings{10.1145/3129790.3129818,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green software development and research with the HADAS toolkit},
year = {2017},
isbn = {9781450352178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129790.3129818},
doi = {10.1145/3129790.3129818},
abstract = {Energy is a critical resource, and designing a sustainable software architecture is a non-trivial task. Developers require energy metrics that support sustainable software architectures reflecting quality attributes such as security, reliability, performance, etc., identifying what are the concerns that impact more in the energy consumption. A variability model of different designs and implementations of an energy model should exist for this task, as well as a service that stores and compares the experimentation results of energy and time consumption of each concern, finding out what is the most eco-efficient solution. The experimental measurements are performed by energy experts and researchers that share the energy model and metrics in a collaborative repository. HADAS confronts these tasks modelling and reasoning with the variability of energy consuming concerns for different energy contexts, connecting HADAS variability model with its energy efficiency collaborative repository, establishing a Software Product Line (SPL) service. Our main goal is to help developers to perform sustainability analyses finding out the eco-friendliest architecture configurations. A HADAS toolkit prototype is implemented based on a Clafer model and Choco solver, and it has been tested with several case studies.},
booktitle = {Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings},
pages = {205–211},
numpages = {7},
keywords = {CVL, clafer, energy efficiency, metrics, optimisation, repository, software product line, variability},
location = {Canterbury, United Kingdom},
series = {ECSA '17}
}

@article{10.1145/3660774,
author = {Lyu, Jun and Li, Shanshan and Zhang, He and Yang, Lanxin and Liu, Bohan and Rigger, Manuel},
title = {Towards Efficient Build Ordering for Incremental Builds with Multiple Configurations},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660774},
doi = {10.1145/3660774},
abstract = {Software products have many configurations to meet different environments and diverse needs. Building software with multiple software configurations typically incurs high costs in terms of build time and computing resources. Incremental builds could reuse intermediate artifacts if configuration settings affect only a portion of the build artifacts. The efficiency gains depend on the strategic ordering of the incremental builds as the order influences which build artifacts can be reused. Deriving an efficient order is challenging and an open problem, since it is infeasible to reliably determine the degree of re-use and time savings before an actual build. In this paper, we propose an approach, called BUDDI—BUild Declaration DIstance, for C-based and Make-based projects to derive an efficient order for incremental builds from the static information provided by the build scripts (i.e., Makefile). The core strategy of BUDDI is to measure the distance between the build declarations of configurations and predict the build size of a configuration from the build targets and build commands in each configuration. Since some artifacts could be reused in the subsequent builds if there is a close distance between the build scripts for different configurations. We implemented BUDDI as an automated tool called BuddiPlanner and evaluated it on 20 popular open-source projects, by comparing it to a baseline that randomly selects a build order. The experimental results show that the order created by BuddiPlanner outperforms 96.5% (193/200) of the random build orders in terms of build time and reduces the build time by an average of 305.94s (26%) compared to the random build orders, with a median saving of 64.88s (28%). BuddiPlanner demonstrates its potential to relieve practitioners of excessive build times and computational resource burdens caused by building multiple software configurations.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {67},
numpages = {24},
keywords = {Build Tool, Incremental Builds, Software Configurations}
}

@inproceedings{10.1145/2648511.2648545,
author = {Nasr, Sana Ben and Sannier, Nicolas and Acher, Mathieu and Baudry, Benoit},
title = {Moving toward product line engineering in a nuclear industry consortium},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648545},
doi = {10.1145/2648511.2648545},
abstract = {Nuclear power plants are some of the most sophisticated and complex energy systems ever designed. These systems perform safety critical functions and must conform to national safety institutions and international regulations. In many cases, regulatory documents provide very high level and ambiguous requirements that leave a large margin for interpretation. As the French nuclear industry is now seeking to spread its activities outside France, it is but necessary to master the ins and the outs of the variability between countries safety culture and regulations. This sets both an industrial and a scientific challenge to introduce and propose a product line engineering approach to an unaware industry whose safety culture is made of interpretations, specificities, and exceptions.This paper presents our current work within the French R&amp;D project CONNEXION, while introducing variability modeling to the French nuclear industry. In particular, we discuss the background, the quest for the best variability paradigm, the practical modeling of requirements variability as well as the mapping between variable requirements and variable architecture elements.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {294–303},
numpages = {10},
keywords = {product line engineering, regulations, requirements variability modeling, variability mining},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/1858996.1859064,
author = {Boucher, Quentin and Classen, Andreas and Heymans, Patrick and Bourdoux, Arnaud and Demonceau, Laurent},
title = {Tag and prune: a pragmatic approach to software product line implementation},
year = {2010},
isbn = {9781450301169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858996.1859064},
doi = {10.1145/1858996.1859064},
abstract = {To realise variability at the code level, product line methods classically advocate usage of inheritance, components, frameworks, aspects or generative techniques. However, these might require unaffordable paradigm shifts for the developers if the software was not thought at the outset as a product line. Furthermore, these techniques can be conflicting with a company's coding practices or external regulations.These concerns were the motivation for the industry-university collaboration described in this paper where we develop a minimally intrusive coding technique based on tags. It is supported by a toolchain and is now in use in the partner company for the development of flight grade satellite communication software libraries.},
booktitle = {Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering},
pages = {333–336},
numpages = {4},
keywords = {code tagging, feature diagram},
location = {Antwerp, Belgium},
series = {ASE '10}
}

@inproceedings{10.1145/2019136.2019143,
author = {Oster, Sebastian and Zink, Marius and Lochau, Malte and Grechanik, Mark},
title = {Pairwise feature-interaction testing for SPLs: potentials and limitations},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019143},
doi = {10.1145/2019136.2019143},
abstract = {A fundamental problem of testing Software Product Lines (SPLs) is that variability enables the production of a large number of instances and it is difficult to construct and run test cases even for SPLs with a small number of variable features. Interacting features is a foundation of a fault model for SPLs, where faults are likely to be revealed at execution points where features exchange information with other features or influence one another. Therefore, a test adequacy criterion is to cover as many interactions among different features as possible, thus increasing the probability of finding bugs. Our approach combines a combinatorial designs algorithm for pairwise feature generation with model-based testing to reduce the size of the SPL required for comprehensive coverage of interacting features. We implemented our approach and applied it to an SPL from the automotive domain provided by one of our industrial partners. The results suggest that with our approach higher coverage of feature interactions is achieved at a fraction of cost when compared with a baseline approach of testing all feature interactions.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {6},
numpages = {8},
keywords = {combinatorial testing, feature interaction, feature model, model-based testing, product lines, reusable test model},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2491627.2491635,
author = {Henard, Christopher and Papadakis, Mike and Perrouin, Gilles and Klein, Jacques and Traon, Yves Le},
title = {Multi-objective test generation for software product lines},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491635},
doi = {10.1145/2491627.2491635},
abstract = {Software Products Lines (SPLs) are families of products sharing common assets representing code or functionalities of a software product. These assets are represented as features, usually organized into Feature Models (FMs) from which the user can configure software products. Generally, few features are sufficient to allow configuring millions of software products. As a result, selecting the products matching given testing objectives is a difficult problem.The testing process usually involves multiple and potentially conflicting testing objectives to fulfill, e.g. maximizing the number of optional features to test while at the same time both minimizing the number of products and minimizing the cost of testing them. However, most approaches for generating products usually target a single objective, like testing the maximum amount of feature interactions. While focusing on one objective may be sufficient in certain cases, this practice does not reflect real-life testing situations.The present paper proposes a genetic algorithm to handle multiple conflicting objectives in test generation for SPLs. Experiments conducted on FMs of different sizes demonstrate the effectiveness, feasibility and practicality of the introduced approach.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {62–71},
numpages = {10},
keywords = {feature models, genetic algorithms, multi-objective optimization, software product lines, test generation},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@proceedings{10.1145/3629527,
title = {ICPE '24 Companion: Companion of the 15th ACM/SPEC International Conference on Performance Engineering},
year = {2024},
isbn = {9798400704451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to present the ICPE 2024 workshops program. ICPE workshops extend the main conference by providing a forum to foster discussion on hot and emerging topics from the broad field of performance engineering. They offer a highly dynamic venue to exchange ideas, establish new collaborations, and bootstrap debates on novel techniques, methodologies, and their associated early research results. Workshops feature various presentation formats, including research paper presentations, panel discussions, and keynote talks. Through these presentations and discussions with peer researchers, ICPE workshops help shape future research and identify promising research directions for performance engineering.},
location = {London, United Kingdom}
}

@inproceedings{10.1145/2019136.2019181,
author = {Weiss, Michael},
title = {Economics of collectives},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019181},
doi = {10.1145/2019136.2019181},
abstract = {The transition from a software product line to a software ecosystem, as reported by Bosch [5], takes place, when the product line company makes its platform available to developers outside the company. A similar transition takes place from a software ecosystem to a collective, when the platform is jointly created and owned by a group of members. Building on the literature on software product line economics, this research identifies three factors affecting the economics of collectives (level of contribution, number of members, and diversity of use), and develops a model linking those factors to three economic outcomes (time, quality, and cost).},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {39},
numpages = {8},
keywords = {collectives, open source software, product line economics, software ecosystems, software product lines},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2648511.2648550,
author = {Dillon, Michael and Rivera, Jorge and Darbin, Rowland},
title = {A methodical approach to product line adoption},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648550},
doi = {10.1145/2648511.2648550},
abstract = {The evolution of the U.S. Army's Live Training Transformation (LT2) product line of combat training systems, including the move by the Army to consolidate management of the product line under a single contracting team, has provided a natural experiment that validates the hypothesis that product line engineering practices are more effective than traditional software engineering practices, and has demonstrated which product line adoption approaches are more successful than others. By analyzing this natural experiment, the product line team has been able to apply a methodical approach to product line adoption across the development organization and successfully adopt second generation product line processes. This paper explores that methodical approach. It will enumerate the steps that led to successes and explore the contributing factors and unintended consequences of failures along the way. Additionally this paper will explore how this approach is being employed to extend the LT2 product line beyond software.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {340–349},
numpages = {10},
keywords = {bill-of-features, feature constraints hierarchical product lines, feature modeling, feature profiles, product baselines, product configurator, product line adoption, product line engineering, product line governance, product portfolio, second generation product line engineering, software product lines, variation points},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2791060.2791070,
author = {Liang, Jia Hui and Ganesh, Vijay and Czarnecki, Krzysztof and Raman, Venkatesh},
title = {SAT-based analysis of large real-world feature models is easy},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791070},
doi = {10.1145/2791060.2791070},
abstract = {Modern conflict-driven clause-learning (CDCL) Boolean SAT solvers provide efficient automatic analysis of real-world feature models (FM) of systems ranging from cars to operating systems. It is well-known that solver-based analysis of real-world FMs scale very well even though SAT instances obtained from such FMs are large, and the corresponding analysis problems are known to be NP-complete. To better understand why SAT solvers are so effective, we systematically studied many syntactic and semantic characteristics of a representative set of large real-world FMs. We discovered that a key reason why large real-world FMs are easy-to-analyze is that the vast majority of the variables in these models are unrestricted, i.e., the models are satisfiable for both true and false assignments to such variables under the current partial assignment. Given this discovery and our understanding of CDCL SAT solvers, we show that solvers can easily find satisfying assignments for such models without too many backtracks relative to the model size, explaining why solvers scale so well. Further analysis showed that the presence of unrestricted variables in these real-world models can be attributed to their high-degree of variability. Additionally, we experimented with a series of well-known nonbacktracking simplifications that are particularly effective in solving FMs. The remaining variables/clauses after simplifications, called the core, are so few that they are easily solved even with backtracking, further strengthening our conclusions. We explain the connection between our findings and backdoors, an idea posited by theorists to explain the power of SAT solvers. This connection strengthens our hypothesis that SAT-based analysis of FMs is easy. In contrast to our findings, previous research characterizes the difficulty of analyzing randomly-generated FMs in terms of treewidth. Our experiments suggest that the difficulty of analyzing real-world FMs cannot be explained in terms of treewidth.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {91–100},
numpages = {10},
keywords = {SAT-based analysis, feature model},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3321408.3326676,
author = {Yan, Liu and Hu, Wenxin and Han, Longzhe},
title = {Optimize SPL test cases with adaptive simulated annealing genetic algorithm},
year = {2019},
isbn = {9781450371582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321408.3326676},
doi = {10.1145/3321408.3326676},
abstract = {In Software Product Line (SPL) testing, reduced test suite with high coverage is useful for early features interaction detection. sGA (simplified genetic algorithm) and SAGA(simulated annealing genetic algorithm) can generate high coverage test suite. However, small probability mutations in updating test suite may reduce search efficiency and thus miss better solutions. An improved test cases generation method based on ASAGA (Adaptive simulated annealing genetic algorithm) is proposed. Experiments on SPLOT (Software Product Lines Online Tools) feature models show that the proposed hybrid ASAGA method can ensure local optimization accuracy and achieve smaller-size test suite with higher coverage.},
booktitle = {Proceedings of the ACM Turing Celebration Conference - China},
articleno = {148},
numpages = {7},
keywords = {ASAGA, feature model, similarity measurement, software test, test case},
location = {Chengdu, China},
series = {ACM TURC '19}
}

@inproceedings{10.1145/2491627.2499882,
author = {Moonen, Leon and Behjati, Razieh and Rabiser, Rick and Acharya, Mithun and Tekinerdogan, Bedir and Kang, Kyo-Chul},
title = {First International Workshop on Multi Product Line Engineering (MultiPLE 2013)},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2499882},
doi = {10.1145/2491627.2499882},
abstract = {In an industrial context, software systems are rarely developed by a single organization. For software product lines, this means that various organizations collaborate to provide and integrate the assets used in a product line. It is not uncommon that these assets themselves are built as product lines, a practice which is referred to as multi product lines. This cross-organizational distribution of reusable assets leads to numerous challenges, such as inconsistent configuration, costly and time-consuming integration, diverging evolution speed and direction, and inadequate testing.The MultiPLE workshop is aimed at discussing the challenges involved with the development and evolution of multi product lines and the assets used for their production.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {269–270},
numpages = {2},
keywords = {certification, cross-organizational product line engineering, design, evolution, multi product lines, software ecosystems},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2019136.2019177,
author = {Abbas, Nadeem and Andersson, Jesper and Weyns, Danny},
title = {Knowledge evolution in autonomic software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019177},
doi = {10.1145/2019136.2019177},
abstract = {We describe ongoing work in knowledge evolution management for autonomic software product lines. We explore how an autonomic product line may benefit from new knowledge originating from different source activities and artifacts at run time. The motivation for sharing run-time knowledge is that products may self-optimize at run time and thus improve quality faster compared to traditional software product line evolution. We propose two mechanisms that support knowledge evolution in product lines: online learning and knowledge sharing. We describe two basic scenarios for runtime knowledge evolution that involves these mechanisms. We evaluate online learning and knowledge sharing in a small product line setting that shows promising results.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {36},
numpages = {8},
keywords = {knowledge sharing, online learning, product-line management, self-adaptation, software design, software product-lines},
location = {Munich, Germany},
series = {SPLC '11}
}

@proceedings{10.1145/3634737,
title = {ASIA CCS '24: Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to ACM AsiaCCS 2024, the 19th ACM Asia Conference on Computer and Communications Security. AsiaCCS 2024 takes place in Singapore from 1 July to 5 July.},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/1629716.1629729,
author = {Liebig, J\"{o}rg and Apel, Sven and Lengauer, Christian and Leich, Thomas},
title = {RobbyDBMS: a case study on hardware/software product line engineering},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629729},
doi = {10.1145/1629716.1629729},
abstract = {The development of a highly configurable data management system is a challenging task, especially if it is to be implemented on an embedded system that provides limited resources. We present a case study of such a data management system, called RobbyDBMS, and give it a feature-oriented design. In our case study, we evaluate the system's efficiency and variability. We pay particular attention to the interaction between the features of the data management system and the components of the underlying embedded platform. We also propose an integrated development process covering both hardware and software.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {63–68},
numpages = {6},
keywords = {FeatureC++, domain engineering, feature oriented software development, hardware product lines, software product lines},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@inproceedings{10.1145/2648511.2648529,
author = {Vacchi, Edoardo and Cazzola, Walter and Combemale, Benoit and Acher, Mathieu},
title = {Automating variability model inference for component-based language implementations},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648529},
doi = {10.1145/2648511.2648529},
abstract = {Recently, domain-specific language development has become again a topic of interest, as a means to help designing solutions to domain-specific problems. Componentized language frameworks, coupled with variability modeling, have the potential to bring language development to the masses, by simplifying the configuration of a new language from an existing set of reusable components. However, designing variability models for this purpose requires not only a good understanding of these frameworks and the way components interact, but also an adequate familiarity with the problem domain.In this paper we propose an approach to automatically infer a relevant variability model from a collection of already implemented language components, given a structured, but general representation of the domain. We describe techniques to assist users in achieving a better understanding of the relationships between language components, and find out which languages can be derived from them with respect to the given domain.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {167–176},
numpages = {10},
keywords = {DSL implementation, SW product lines, variability models},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2648511.2648524,
author = {Quinton, Cl\'{e}ment and Pleuss, Andreas and Berre, Daniel Le and Duchien, Laurence and Botterweck, Goetz},
title = {Consistency checking for the evolution of cardinality-based feature models},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648524},
doi = {10.1145/2648511.2648524},
abstract = {Feature-models (fms) are a widely used approach to specify the commonalities and variability in variable systems and software product lines. Various works have addressed edits to fms for fm evolution and tool support to ensure consistency of fms. An important extension to fms are feature cardinalities and related constraints, as extensively used e.g., when modeling variability of cloud computing environments. Since cardinality-based fms pose additional complexity, additional support for evolution and consistency checking with respect to feature cardinalities would be desirable, but has not been addressed yet. In this paper, we discuss common cardinality-based fm edits and resulting inconsistencies based on experiences with fms in cloud domain. We introduce tool-support for automated inconsistency detection and explanation based on an off-the-shelf solver. We demonstrate the feasibility of the approach by an empirical evaluation showing the performance of the tool.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {122–131},
numpages = {10},
keywords = {cardinality, consistency, edit, feature model},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3468264.3468578,
author = {Oh, Jeho and Y\i{}ld\i{}ran, Necip Faz\i{}l and Braha, Julian and Gazzillo, Paul},
title = {Finding broken Linux configuration specifications by statically analyzing the Kconfig language},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468578},
doi = {10.1145/3468264.3468578},
abstract = {Highly-configurable software underpins much of our computing infrastructure. It enables extensive reuse, but opens the door to broken configuration specifications. The configuration specification language, Kconfig, is designed to prevent invalid configurations of the Linux kernel from being built. However, the astronomical size of the configuration space for Linux makes finding specification bugs difficult by hand or with random testing. In this paper, we introduce a software model checking framework for building Kconfig static analysis tools. We develop a formal semantics of the Kconfig language and implement the semantics in a symbolic evaluator called kclause that models Kconfig behavior as logical formulas. We then design and implement a bug finder, called kismet, that takes kclause models and leverages automated theorem proving to find unmet dependency bugs. kismet is evaluated for its precision, performance, and impact on kernel development for a recent version of Linux, which has over 140,000 lines of Kconfig across 28 architecture-specific specifications. Our evaluation finds 781 bugs (151 when considering sharing among Kconfig specifications) with 100% precision, spending between 37 and 90 minutes for each Kconfig specification, although it misses some bugs due to underapproximation. Compared to random testing, kismet finds substantially more true positive bugs in a fraction of the time.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {893–905},
numpages = {13},
keywords = {Kconfig, formal verification, software configuration, static analysis},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@proceedings{10.1145/3658664,
title = {IH&amp;MMSec '24: Proceedings of the 2024 ACM Workshop on Information Hiding and Multimedia Security},
year = {2024},
isbn = {9798400706370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 12th ACM Information Hiding and Multimedia Security Workshop - IH&amp;MMSec'24 in Baiona, Galicia, Spain, organized by the Research Group in Signal Processing in Communications (GPSC) at the University of Vigo. GPSC is one of the pioneering research groups in Information Hiding and Multimedia Security with over 25 years of active engagement in this domain. During this time, our field has seen remarkable growth, evolution, and reinvention, yet it continues to preserve the same effervescence of its inception. This vibrancy is reflected in the diverse range of topics covered in this year's program.In response to our call for papers, we received in total 69 submissions. The top five countries with the highest number of submissions (first author) were Germany, China, France, Italy, and the United States. Each submission underwent rigorous evaluation, with a minimum of three independent reviews provided by members of the Program Committee, supplemented by external reviewers as needed. Based on these timely and high-quality reviews, the Technical Program Chairs selected the 33 most outstanding submissions. The acceptance rate of 47.8% (33/69) reflects our commitment to uphold IH&amp;MMSec as a premier scientific venue in the field of Information Hiding and Multimedia Security. The accepted papers cover the fields of forensics, steganography, steganalysis, watermarking, biometrics, anonymity, security and privacy.},
location = {Baiona, Spain}
}

@inproceedings{10.1145/3427423.3427450,
author = {Haris, M Syauqi and Kurniawan, Tri Astoto},
title = {Automated requirement sentences extraction from software requirement specification document},
year = {2021},
isbn = {9781450376051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427423.3427450},
doi = {10.1145/3427423.3427450},
abstract = {In the requirement reuse and natural language document-based Software Product Line (SPL) domain analysis, requirement sentences of the requirement document are the primary concern. Most studies conducted in this research area have document preprocessing stage in their methods that is a manual process to separate requirement sentences and non-requirement sentences from the document. This manual labor process might be tedious and error-prone since it will need much time and expert intervention to make this process completely done. In this paper, we present a method to automate requirement sentence extraction from the Software Requirement Specification (SRS) document by leveraging Natural Language Processing (NLP) approach and requirement boilerplate sentence patterns. Conducted experiments in this research show this method has such accuracy from 64% to 100% on precision value and recall value in the range of 64% to 89%.},
booktitle = {Proceedings of the 5th International Conference on Sustainable Information Engineering and Technology},
pages = {142–147},
numpages = {6},
keywords = {domain analysis, natural language processing, requirement boilerplate, software product line, software requirement reuse},
location = {Malang, Indonesia},
series = {SIET '20}
}

@article{10.1145/3361146,
author = {Hierons, Robert M. and Li, Miqing and Liu, Xiaohui and Parejo, Jose Antonio and Segura, Sergio and Yao, Xin},
title = {Many-Objective Test Suite Generation for Software Product Lines},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3361146},
doi = {10.1145/3361146},
abstract = {A Software Product Line (SPL) is a set of products built from a number of features, the set of valid products being defined by a feature model. Typically, it does not make sense to test all products defined by an SPL and one instead chooses a set of products to test (test selection) and, ideally, derives a good order in which to test them (test prioritisation). Since one cannot know in advance which products will reveal faults, test selection and prioritisation are normally based on objective functions that are known to relate to likely effectiveness or cost. This article introduces a new technique, the grid-based evolution strategy (GrES), which considers several objective functions that assess a selection or prioritisation and aims to optimise on all of these. The problem is thus a many-objective optimisation problem. We use a new approach, in which all of the objective functions are considered but one (pairwise coverage) is seen as the most important. We also derive a novel evolution strategy based on domain knowledge. The results of the evaluation, on randomly generated and realistic feature models, were promising, with GrES outperforming previously proposed techniques and a range of many-objective optimisation algorithms.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {2},
numpages = {46},
keywords = {Software product line, multi-objective optimisation, test prioritisation, test selection}
}

@inproceedings{10.1145/3652620.3687815,
author = {Raeisdanaei, Ali and Murphy, Logan and Di Sandro, Alessio and Askarpour, Mehrnoosh and Viger, Torin and Chechik, Marsha},
title = {Evaluation of Automotive OTA Updates Using Assurance Cases},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687815},
doi = {10.1145/3652620.3687815},
abstract = {Software-intensive vehicles require regular over-the-air (OTA) updates. To ensure that OTA updates do not compromise system safety, such updates should be assured. Automotive safety engineers need to efficiently estimate the effort it would take to assure these updates for the entire fleet. To address this challenge, we propose a process for supporting assurance of OTA updates. Our process proposes to model the fleet of vehicles as a software product line (SPL), assured with a variability-aware assurance case. It then measures the difficulty of assuring the proposed update using this variability-aware AC model via a set of heuristics.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {720–724},
numpages = {5},
keywords = {assurance, over-the-air updates, impact assessment},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/1774088.1774573,
author = {Espinoza, Angelina and Botterweck, Goetz and Garbajosa, Juan},
title = {A formal approach to reuse successful traceability practices in SPL projects},
year = {2010},
isbn = {9781605586397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1774088.1774573},
doi = {10.1145/1774088.1774573},
abstract = {Software Product Line (SPL) Engineering has to deal with interrelated, complex models such as feature and architecture models, hence traceability is fundamental to keep them consistent. Commonly, a traceability schema must be started from scratch from project to project. To avoid that, useful traceability practices to solve day to day problems should be modeled explicitly and kept as part of the traceability knowledge gained, and then organizations can reduce time and effort in implementing traceability in new projects. This paper presents an approach for formalizing and reusing traceability practices in SPL Engineering. Using this formalization approach a traceability metamodel is defined, incorporating the particular traceability practices performed in SPL Engineering. Customized traceability methodologies for SPL projects will be systematically and formally generated from this metamodel. These resulting methodologies will have already incorporated the traceability knowledge proven as successful in previous projects, facilitating the reuse of such practices. In this paper, we focus specifically on the product derivation process, to show the advantages of this formalization approach to reuse traceability knowledge.},
booktitle = {Proceedings of the 2010 ACM Symposium on Applied Computing},
pages = {2352–2359},
numpages = {8},
keywords = {feature configuration, knowledge reuse, product derivation, software product lines, traceability metamodeling, traceability methodology},
location = {Sierre, Switzerland},
series = {SAC '10}
}

@inproceedings{10.1145/2364412.2364452,
author = {Berger, Thorsten},
title = {Variability modeling in the wild},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364452},
doi = {10.1145/2364412.2364452},
abstract = {Variability modeling is one of the key disciplines in software product line engineering and has been addressed by academic and industrial research over the past twenty years. While the research community's focus was on creating notations and tools, most of which based on feature modeling, there are relatively few empirical studies that aim at understanding the actual use of these techniques.In this light, we present empirical work that investigates variability modeling in the context of software product lines. We study concepts and semantics of real-world variability languages and the usage of these concepts in real, large-scale variability models. We further extend our discussion to variability in software ecosystems, which target inter-organizational reuse and are often seen as natural successors of software product lines. We provide empirical evidence that the well-researched concepts of feature modeling are used in practice, but also that more advanced concepts are needed. We observe that some assumptions about realistic variability models in the literature do not hold. Further, our findings indicate that variability models are not suited for software ecosystems, and that particular kinds of dependencies are needed to enable growth of such ecosystems.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {233–241},
numpages = {9},
keywords = {empirical software engineering, software ecosystems, software product lines, variability modeling},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3377930.3390215,
author = {Silva, Diego Fernandes da and Okada, Luiz Fernando and Colanzi, Thelma Elita and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Enhancing search-based product line design with crossover operators},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3390215},
doi = {10.1145/3377930.3390215},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA designing has been formulated as a multi-objective optimization problem and successfully solved by a state-of-the-art search-based approach. However, the majority of empirical studies optimize PLA designs without applying one of the fundamental genetic operators: the crossover. An operator for PLA design, named Feature-driven Crossover, was proposed in a previous study. In spite of the promising results, this operator occasionally generated incomplete solutions. To overcome these limitations, this paper aims to enhance the search-based PLA design optimization by improving the Feature-driven Crossover and introducing a novel crossover operator specific for PLA design. The proposed operators were evaluated in two well-studied PLA designs, using three experimental configurations of NSGA-II in comparison with a baseline that uses only mutation operators. Empirical results show the usefulness and efficiency of the presented operators on reaching consistent solutions. We also observed that the two operators complement each other, leading to PLA design solutions with better feature modularization than the baseline experiment.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {1250–1258},
numpages = {9},
keywords = {multi-objective evolutionary algorithm, recombination operators, software architecture, software product line},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1145/3377024.3377041,
author = {Beek, Maurice H. ter and Legay, Axel and Lafuente, Alberto Lluch and Vandin, Andrea},
title = {Variability meets security: qantitative security modeling and analysis of highly customizable attack scenarios},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377041},
doi = {10.1145/3377024.3377041},
abstract = {We present a framework for quantitative security modeling and analysis of highly customizable attack scenarios, which resulted as a spin-off from our research in software product line engineering. The graphical security models are based on attributed attack-defense diagrams to capture the structure and properties of vulnerabilities, defenses and countermeasures---with notable similarities to feature diagrams---and on probabilistic models of attack behavior, capable of capturing resource constraints and attack effectiveness. In this paper, we provide an overview of the framework that is described in full technical detail in twin papers, which present the formal syntax and semantics of the domain-specific language and showcase the associated tool with advanced IDE support for performing analyses based on statistical model checking. The properties of interest range from average cost and success probability of attacks to the effectiveness of defenses and countermeasures. Here we illustrate the capabilities of the DSL and the tool by applying them to an example scenario from the security domain. This shows how techniques from variability modeling can be applied to security. We conclude with a vision and roadmap for future research.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {11},
numpages = {9},
keywords = {attack-defense trees, formal analysis tools, graphical security models, quantitative security, statistical model checking, variability models},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@article{10.1145/3528100,
author = {Cheng, Jiezhu and Gao, Cuiyun and Zheng, Zibin},
title = {HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3528100},
doi = {10.1145/3528100},
abstract = {Modern software systems are usually highly configurable, providing users with customized functionality through various configuration options. Understanding how system performance varies with different option combinations is important to determine optimal configurations that meet specific requirements. Due to the complex interactions among multiple options and the high cost of performance measurement under a huge configuration space, it is challenging to study how different configurations influence the system performance. To address these challenges, we propose HINNPerf, a novel hierarchical interaction neural network for performance prediction of configurable systems. HINNPerf employs the embedding method and hierarchic network blocks to model the complicated interplay between configuration options, which improves the prediction accuracy of the method. In addition, we devise a hierarchical regularization strategy to enhance the model robustness. Empirical results on 10 real-world configurable systems show that our method statistically significantly outperforms state-of-the-art approaches by achieving average 22.67% improvement in prediction accuracy. In addition, combined with the Integrated Gradients method, the designed hierarchical architecture provides some insights about the interaction complexity and the significance of configuration options, which might help users and developers better understand how the configurable system works and efficiently identify significant options affecting the performance.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {46},
numpages = {30},
keywords = {Software performance prediction, highly configurable systems, deep neural network, machine learning}
}

@inproceedings{10.1145/2364412.2364439,
author = {Vale, Tassio and Figueiredo, Gustavo Bittencourt and de Almeida, Eduardo Santana and de Lemos Meira, Silvio Romero},
title = {A study on service identification methods for software product lines},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364439},
doi = {10.1145/2364412.2364439},
abstract = {The combination of service-orientation and software product line engineering, called Service-Oriented Product Line Engineering (SOPLE) have received attention by researchers and practitioners in the last years, and these areas can address issues of each other. One service-orientation issue is service identification. It consists of determining candidate services to a service-oriented environment based on pre-existing software artifacts, e.g., business process, source code, and so on. In order to provide a systematic identification of services, there are many available service identification methods in the literature, regarding different understanding of services, goals, and techniques. Due to this heterogeneity, this paper presents an in-depth comparison of service identification methods as well as a recommendation of the most suitable ones in the SOPLE context. This work can help the decision making of the most suitable method according to stakeholders' needs.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {156–163},
numpages = {8},
keywords = {service identification, service-oriented computing, service-oriented product lines, software product lines},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791117,
author = {Ferrari, Alessio and Spagnolo, Giorgio O. and Gnesi, Stefania and Dell'Orletta, Felice},
title = {CMT and FDE: tools to bridge the gap between natural language documents and feature diagrams},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791117},
doi = {10.1145/2791060.2791117},
abstract = {A business subject who wishes to enter an established technological market is required to accurately analyse the features of the products of the different competitors. Such features are normally accessible through natural language (NL) brochures, or NL Web pages, which describe the products to potential customers. Building a feature model that hierarchically summarises the different features available in competing products can bring relevant benefits in market analysis. A company can easily visualise existing features, and reason about aspects that are not covered by the available solutions. However, designing a feature model starting from publicly available documents of existing products is a time consuming and error-prone task. In this paper, we present two tools, namely Commonality Mining Tool (CMT) and Feature Diagram Editor (FDE), which can jointly support the feature model definition process. CMT allows mining common and variant features from NL descriptions of existing products, by leveraging a natural language processing (NLP) approach based on contrastive analysis, which allows identifying domain-relevant terms from NL documents. FDE takes the commonalities and variabilities extracted by CMT, and renders them in a visual form. Moreover, FDE allows the graphical design and refinement of the final feature model, by means of an intuitive GUI.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {402–410},
numpages = {9},
keywords = {software product lines, tools, variability mining},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2648511.2648560,
author = {Bosch, Jan},
title = {ESAO: towards data- and ecosystem-driven R&amp;D},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648560},
doi = {10.1145/2648511.2648560},
abstract = {With the increasing rate of change in industry, companies need to manage their internal alignment between strategy, software architecture and organizing R&amp;D as well as with their ecosystem more proactively. Lack of alignment is the root cause of many problems experienced in the software-intensive systems industry. The ESAO model provides a framework for addressing the aforementioned challenges. It provides an internal perspective as well as an ecosystem perspective and for each perspective offers a strategic, architectural and ways of working dimension. ESAO offers tools for each of the dimensions as well as mechanisms for establishing and maintaining alignment.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {363},
numpages = {1},
keywords = {customer data, data driven development, software ecosystems},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1145/3527153,
author = {Yan, Chen and Ji, Xiaoyu and Wang, Kai and Jiang, Qinhong and Jin, Zizhi and Xu, Wenyuan},
title = {A Survey on Voice Assistant Security: Attacks and Countermeasures},
year = {2022},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3527153},
doi = {10.1145/3527153},
abstract = {Voice assistants (VA) have become prevalent on a wide range of personal devices such as smartphones and smart speakers. As companies build voice assistants with extra functionalities, attacks that trick a voice assistant into performing malicious behaviors can pose a significant threat to a user’s security, privacy, and even safety. However, the diverse attacks and stand-alone defenses in the literature often lack a systematic perspective, making it challenging for designers to properly identify, understand, and mitigate the security threats against voice assistants. To overcome this problem, this article provides a thorough survey of the attacks and countermeasures for voice assistants. We systematize a broad category of relevant but seemingly unrelated attacks by the vulnerable system components and attack methods, and categorize existing countermeasures based on the defensive strategies from a system designer’s perspective. To assist designers in planning defense based on their demands, we provide a qualitative comparison of existing countermeasures by the implementation cost, usability, and security and propose practical suggestions. We envision this work can help build more reliability into voice assistants and promote research in this fast-evolving area.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {84},
numpages = {36},
keywords = {Voice assistant, security, attack, defense, speech, voice interaction}
}

@inproceedings{10.1145/3168365.3168375,
author = {Bezerra, Carla I. M. and Andrade, Rossana M. C. and Monteiro, Jos\'{e} M. S. and Cedraz, Davi},
title = {Aggregating Measures using Fuzzy Logic for Evaluating Feature Models},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168375},
doi = {10.1145/3168365.3168375},
abstract = {In the context of Software Product Lines (SPLs), evaluating the quality of a feature model is essential to ensure that errors in the early stages do not spread throughout the SPL. One way to evaluate a feature model is to use measures. However, measures alone are not enough to characterize the feature model quality, because most of them cover specific aspects, such as the number of features. So, there is a need for methods to aggregate measures at the level of quality sub-characteristic or characteristic. In this paper, we aim to investigate how to aggregate measures that have been proposed to evaluate the quality of feature models in SPL. We have used the fuzzy logic theory in order to aggregate these measures. The new aggregated measures can be applied to evaluate different and complex aspects of a feature model, such as: size, stability, flexibility and dynamicity. Moreover, to evaluate the use of the new aggregate measures, we applied them in different feature models. Our findings suggest that aggregate measures can assist the domain engineer in evaluating the maintainability of feature models.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {35–42},
numpages = {8},
keywords = {Feature Models, Fuzzy Logic, Measures, Software Product Line},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/2791060.2791067,
author = {Yue, Tao and Ali, Shaukat and Selic, Bran},
title = {Cyber-physical system product line engineering: comprehensive domain analysis and experience report},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791067},
doi = {10.1145/2791060.2791067},
abstract = {Cyber-Physical Systems (CPSs) are the future generation of highly connected embedded systems having applications in diverse domains including Oil and Gas. Employing Product Line Engineering (PLE) is believed to bring potential benefits with respect to reduced cost, higher productivity, higher quality, and faster time-to-market. However, relatively few industrial field studies are reported regarding the application of PLE to develop large-scale systems, and more specifically CPSs. In this paper, we report about our experiences and insights gained from investigating the application of model-based PLE at a large international organization developing subsea production systems (typical CPSs) to manage the exploitation of oil and gas production fields. We report in this paper 1) how two systematic domain analyses (on requirements engineering and product configuration/derivation) were conducted to elicit CPS PLE requirements and challenges, 2) key results of the domain analysis (commonly observed in other domains), and 3) our initial experience of developing and applying two Model Based System Engineering (MBSE) PLE solution to address some of the requirements and challenges elicited during the domain analyses.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {338–347},
numpages = {10},
keywords = {cyber physical system (CPS), domain analysis, model based system engineering, product line engineering (PLE), requirements engineering},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2019136.2019150,
author = {Serajzadeh, Hadi and Shams, Fereidoon},
title = {The application of swarm intelligence in service-oriented product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019150},
doi = {10.1145/2019136.2019150},
abstract = {Changing markets and environments has made the ability to rapidly adapt to these changes a necessity in software systems. However the costs of changing and adapting systems to new requirements still remains an unsolved issue. In this context service-oriented software product lines were introduced with the aim to combine the reusability of software product line with the flexibility of service-oriented architecture. Although this approach helps build flexible software systems with high levels of reuse, certain issues are raised. The main issue is the complexity that a service-oriented product line will face. Developing systems from internal and external assets, taking into consideration the variety and number of these assets, can cause problems in deciding which asset is best suited for the system. To help solve these issues we propose the use of approaches based on artificial intelligence. In this paper we show how swarm intelligence can be used in service-oriented product lines to reduce complexity and find optimal solutions for the development of software systems. We also present an example of the application of swarm intelligence in finding the optimal product for a service-oriented product line.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {12},
numpages = {7},
keywords = {optimization, service-oriented product line, swarm intelligence},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2647908.2655975,
author = {Wagner, Michael and Dudeck, Grit and Hein, Christian and Tcholtchev, Nikolay and Gebhardt, Christian and Korff, Andreas},
title = {VARIES framework to support tool integration in product line engineering},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655975},
doi = {10.1145/2647908.2655975},
abstract = {Even though product line technologies and methods are well established in today's development environments, various challenges still remain. Different ways of handling variability in system development tools have arisen posing an integration challenge to today's tool chains. This issue is further amplified by the variety of integration approaches. The VARIES framework addresses these challenges through technology adaptation, i.e. the utilization of model transformations and traceability support.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {117–120},
numpages = {4},
keywords = {CVL, ModelBus, OSLC, SPL, tool chain, traceability, transformation, variability management},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2791060.2791095,
author = {Arrieta, Aitor and Sagardui, Goiuria and Etxeberria, Leire},
title = {Test control algorithms for the validation of cyber-physical systems product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791095},
doi = {10.1145/2791060.2791095},
abstract = {Cyber-Physical Systems (CPSs) product lines appear in a wide range of applications of different domains (e.g., car's doors' windows, doors of a lift, etc.). The variability of these systems is large and as a result they can be configured into plenty of configurations. Testing each of the configurations can be time consuming as not only software has to be simulated, but also the hardware and the physical layer of the CPS, which is often modelled with complex mathematical models. Choosing the adequate test control strategy is critical when testing CPSs product lines. This paper presents a set of test control algorithms organized in an architecture of three layers (domain, application and simulation) for testing CPSs product lines. An illustrative example of a CPS product line is presented and three experiments are conducted to measure the performance of the proposed test control algorithms. We conclude that test scheduling and test suite minimization significantly help to reduce the overall test costs while preserving the test quality in CPSs product lines. In addition, we conclude that knowing the results of the previously tested configurations permits reducing the time for the detection of anomalous designs.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {273–282},
numpages = {10},
keywords = {cyber-physical systems product lines, product line engineering, testing, validation},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2364412.2364414,
author = {Derakhshanmanesh, Mahdi and Fox, Joachim and Ebert, J\"{u}rgen},
title = {Adopting feature-centric reuse of requirements assets: an industrial experience report},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364414},
doi = {10.1145/2364412.2364414},
abstract = {In this paper, we share practical experiences from an ongoing effort towards adopting a feature-centric method that enhances reuse of requirements at TRW Automotive's slip control system department (based in Koblenz, Germany). After introducing identified challenges in detail, key solution factors and a technical reuse concept for managing and deriving product-specific requirements are presented. Then, we demonstrate one way of implementing this solution approach based on industry-standard tools. In addition, identified pitfalls and lessons learned are discussed.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {2–9},
numpages = {8},
keywords = {features, requirements, reuse, software product lines},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791073,
author = {Lachmann, Remo and Lity, Sascha and Lischke, Sabrina and Beddig, Simon and Schulze, Sandro and Schaefer, Ina},
title = {Delta-oriented test case prioritization for integration testing of software product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791073},
doi = {10.1145/2791060.2791073},
abstract = {Software product lines have potential to allow for mass customization of products. Unfortunately, the resulting, vast amount of possible product variants with commonalities and differences leads to new challenges in software testing. Ideally, every product variant should be tested, especially in safety-critical systems. However, due to the exponentially increasing number of product variants, testing every product variant is not feasible. Thus, new concepts and techniques are required to provide efficient SPL testing strategies exploiting the commonalities of software artifacts between product variants to reduce redundancy in testing. In this paper, we present an efficient integration testing approach for SPLs based on delta modeling. We focus on test case prioritization. As a result, only the most important test cases for every product variant are tested, reducing the number of executed test cases significantly, as testing can stop at any given point because of resource constraints while ensuring that the most important test cases have been covered. We present the general concept and our evaluation results. The results show a measurable reduction of executed test cases compared to single-software testing approaches.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {81–90},
numpages = {10},
keywords = {architecture-based testing, delta-oriented software product lines, regression testing, test case prioritization},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2499777.2500716,
author = {Saller, Karsten and Lochau, Malte and Reimund, Ingo},
title = {Context-aware DSPLs: model-based runtime adaptation for resource-constrained systems},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500716},
doi = {10.1145/2499777.2500716},
abstract = {Dynamic Software Product Lines (DSPLs) provide a promising approach for planning and applying runtime reconfiguration scenarios to adaptive software systems. However, applying DSPLs in the vital domain of highly context-aware systems, e.g., mobile devices, is obstructed by the inherently limited resources being insufficient to handle large, constrained (re-)configurations spaces. To tackle these drawbacks, we propose a novel model-based approach for designing DSPLs in a way that allows for a trade-off between precomputation of reconfiguration scenarios at development time and on-demand evolution at runtime. Therefore, we (1) enrich feature models with context information to reason about potential context changes, and (2) specify context-aware reconfiguration processes on the basis of a scalable transition system incorporating state space abstractions and incremental refinement at runtime. We illustrate our concepts by means of a smartphone case study and present an implementation and evaluation considering different trade-off metrics.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {106–113},
numpages = {8},
keywords = {DSPL, adaptive systems, contexts, feature models, state space reduction},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/2491627.2491647,
author = {Murashkin, Alexandr and Antkiewicz, Micha\l{} and Rayside, Derek and Czarnecki, Krzysztof},
title = {Visualization and exploration of optimal variants in product line engineering},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491647},
doi = {10.1145/2491627.2491647},
abstract = {The decision-making process in Product Line Engineering (PLE) is often concerned with variant qualities such as cost, battery life, or security. Pareto-optimal variants, with respect to a set of objectives such as minimizing a variant's cost while maximizing battery life and security, are variants in which no single quality can be improved without sacrificing other qualities. We propose a novel method and a tool for visualization and exploration of a multi-dimensional space of optimal variants (i.e., a Pareto front). The visualization method is an integrated, interactive, and synchronized set of complementary views onto a Pareto front specifically designed to support PLE scenarios, including: understanding differences among variants and their positioning with respect to quality dimensions; solving trade-offs; selecting the most desirable variants; and understanding the impact of changes during product line evolution on a variant's qualities. We present an initial experimental evaluation showing that the visualization method is a good basis for supporting these PLE scenarios.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {111–115},
numpages = {5},
keywords = {ClaferMoo, ClaferMoo visualizer, clafer, exploration, feature modeling, optimal variant, pareto front, product line engineering, visualization},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2791060.2791096,
author = {F\'{e}derle, \'{E}dipo Luis and do Nascimento Ferreira, Thiago and Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {OPLA-tool: a support tool for search-based product line architecture design},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791096},
doi = {10.1145/2791060.2791096},
abstract = {The Product Line Architecture (PLA) design is a complex task, influenced by many factors such as feature modularization and PLA extensibility, which are usually evaluated according to different metrics. Hence, the PLA design is an optimization problem and problems like that have been successfully solved in the Search-Based Software Engineering (SBSE) area, by using metaheuristics such as Genetic Algorithm. Considering this fact, this paper introduces a tool named OPLA-Tool, conceived to provide computer support to a search-based approach for PLA design. OPLA-Tool implements all the steps necessary to use multi-objective optimization algorithms, including PLA transformations and visualization through a graphical interface. OPLA-Tool receives as input a PLA at the class diagram level, and produces a set of good alternative diagrams in terms of cohesion, feature modularization and reduction of crosscutting concerns.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {370–373},
numpages = {4},
keywords = {multi-objective evolutionary algorithms, product line architecture design, search-based software engineering},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2362536.2362556,
author = {Nunes, Camila and Garcia, Alessandro and Lucena, Carlos and Lee, Jaejoon},
title = {History-sensitive heuristics for recovery of features in code of evolving program families},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362556},
doi = {10.1145/2362536.2362556},
abstract = {A program family might degenerate due to unplanned changes in its implementation, thus hindering the maintenance of family members. This degeneration is often induced by feature code that is changed individually in each member without considering other family members. Hence, as a program family evolves over time, it might no longer be possible to distinguish between common and variable features. One of the imminent activities to address this problem is the history-sensitive recovery of program family's features in the code. This recovery process encompasses the analysis of the evolution history of each family member in order to classify the implementation elements according to their variability nature. In this context, this paper proposes history-sensitive heuristics for the recovery of features in code of degenerate program families. Once the analysis of the family history is carried out, the feature elements are structured as Java project packages; they are intended to separate those elements in terms of their variability degree. The proposed heuristics are supported by a prototype tool called RecFeat. We evaluated the accuracy of the heuristics in the context of 33 versions of 2 industry program families. They presented encouraging results regarding recall measures that ranged from 85% to 100%; whereas the precision measures ranged from 71% to 99%.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {136–145},
numpages = {10},
keywords = {feature recovery, heuristics, program families, software evolution},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3510466.3511273,
author = {Fadhlillah, Hafiyyan Sayyid and Feichtinger, Kevin and Meixner, Kristof and Sonnleithner, Lisa and Rabiser, Rick and Zoitl, Alois},
title = {Towards Multidisciplinary Delta-Oriented Variability Management in Cyber-Physical Production Systems},
year = {2022},
isbn = {9781450396042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510466.3511273},
doi = {10.1145/3510466.3511273},
abstract = {Cyber-Physical Production Systems (CPPSs) are complex systems comprised of software and hardware interacting with each other and the environment. In industry, over time, a plethora of CPPSs are developed to satisfy varying customer requirements and changing technologies. Managing variability is challenging, especially in multidisciplinary environments like in CPPS engineering. For instance, when supporting the automatic derivation and configuration of control software, one needs to understand variability from not only a software perspective, but also a mechatronic, electrical, process, and business perspective. It is unrealistic to use a single model or even one type of model across these perspectives. In this paper, we describe a Multidisciplinary Delta-Oriented Variability Management approach for CPPSs that we are currently developing. Our approach aims to express CPPS variability in different disciplines using heterogeneous variability models, relating models via cross-discipline constraints, and automatically generating control software based on variability models. We implemented a prototype of our approach by realizing delta-oriented variability modeling for IEC&nbsp;61499-based distributed control software and a configuration tool to enact the configuration options from multiple variability models. We performed a feasibility study of our approach using two systems of different size and complexity. We conclude that, despite current limitations, our approach can successfully and automatically generate control software based on related multidisciplinary variability models. We think that our approach is a good starting point to manage CPPS variability in practice.},
booktitle = {Proceedings of the 16th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {13},
numpages = {10},
keywords = {Cyber-Physical Production System, Software Configuration, Software Product Line, Variability Modeling},
location = {Florence, Italy},
series = {VaMoS '22}
}

@inproceedings{10.5555/1753235.1753255,
author = {Sun, Hongyu and Lutz, Robyn R. and Basu, Samik},
title = {Product-line-based requirements customization for web service compositions},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Customizing web services according to users' individual functional and non-functional requirements has become increasingly difficult as the number of users increases. This paper introduces a new way to customize and verify composite web services by incorporating a software product-line engineering approach into web-service composition. The approach uses a partitioning similar to that between domain engineering and application engineering in the product-line context. It specifies the options that the user can select and constructs the resulting web-service compositions. By first creating a web-service composition search space that satisfies the common requirements and then querying the search space as the user selects values for the parameters of variation, we provide a more efficient way to customize web services. A decision model, illustrated with examples from an emergency-response application, is created to interact with the customers and ensure the consistency of their specifications. The capability to reuse the composition search space may also help improve the quality and reliability of the composite services and reduce the cost of re-verifying the same compositions.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {141–150},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2362536.2362573,
author = {Bartholdt, J\"{o}rg and Becker, Detlef},
title = {Scope extension of an existing product line},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362573},
doi = {10.1145/2362536.2362573},
abstract = {At the beginning, creating a product line needs a well defined and narrow scope to meet short time to market demands. When established, there is a tendency to broaden the scope and to cover more domains and products.We have undergone a scope extension of our medical diagnostic platform that was implemented while the platform and (existing) products were evolving. In this paper, we list best practices for the migration process and how to come to a sustainable solution without cannibalizing the existing platform and products.In particular, we describe our way of identification beneficial sub-domains using C/V analysis and give an example scenario with alignments in order to increase commonality. We explain the maturity considerations for deciding on reuse of existing implementations and a carve-out strategy to split existing assets into common modules and product-line specific extensions. Furthermore, we describe our best practices for making the scope extension sustainable in a long term, using various types of governance means. We briefly complement these experiences with further insights gained during execution of this endeavor.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {275–282},
numpages = {8},
keywords = {C/V analysis, governance, hierarchical product-line, scope extension},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3302333.3302343,
author = {Cruz, Daniel and Figueiredo, Eduardo and Martinez, Jabier},
title = {A Literature Review and Comparison of Three Feature Location Techniques using ArgoUML-SPL},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302343},
doi = {10.1145/3302333.3302343},
abstract = {Over the last decades, the adoption of Software Product Line (SPL) engineering for supporting software reuse has increased. An SPL can be extracted from one single product or from a family of related software products, and feature location strategies are widely used for variability mining. Several feature location strategies have been proposed in the literature and they usually aim to map a feature to its source code implementation. In this paper, we present a systematic literature review that identifies and characterizes existing feature location strategies. We also evaluated three different strategies based on textual information retrieval in the context of the ArgoUML-SPL feature location case study. In this evaluation, we compare the strategies based on their ability to correctly identify the source code of several features from ArgoUML-SPL ground truth. We then discuss the strengths and weaknesses of each feature location strategy.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {16},
numpages = {10},
keywords = {benchmark, feature location, reverse engineering, software product lines, variability mining},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/2648511.2648530,
author = {Th\"{u}m, Thomas and Meinicke, Jens and Benduhn, Fabian and Hentschel, Martin and von Rhein, Alexander and Saake, Gunter},
title = {Potential synergies of theorem proving and model checking for software product lines},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648530},
doi = {10.1145/2648511.2648530},
abstract = {The verification of software product lines is an active research area. A challenge is to efficiently verify similar products without the need to generate and verify them individually. As solution, researchers suggest family-based verification approaches, which either transform compile-time into runtime variability or make verification tools variability-aware. Existing approaches either focus on theorem proving, model checking, or other verification techniques. For the first time, we combine theorem proving and model checking to evaluate their synergies for product-line verification. We provide tool support by connecting five existing tools, namely FeatureIDE and FeatureHouse for product-line development, as well as KeY, JPF, and OpenJML for verification of Java programs. In an experiment, we found the synergy of improved effectiveness and efficiency, especially for product lines with few defects. Further, we experienced that model checking and theorem proving are more efficient and effective if the product line contains more defects.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {177–186},
numpages = {10},
keywords = {design by contract, family-based verification, feature-based specification, feature-oriented contracts, model checking, software product lines, theorem proving, variability encoding},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2491627.2491645,
author = {Zhang, Bo and Becker, Martin and Patzke, Thomas and Sierszecki, Krzysztof and Savolainen, Juha Erik},
title = {Variability evolution and erosion in industrial product lines: a case study},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491645},
doi = {10.1145/2491627.2491645},
abstract = {Successful software products evolve continuously to meet the changing stakeholder requirements. For software product lines, modifying variability is an additional challenge that must be carefully tackled during the evolution of the product line. This bears considerable challenges for industry as understanding on how variability realizations advance over time is not trivial. Moreover, it may lead to an erosion of variability, which needs an investigation of techniques on how to identify the variability erosion in practice, especially in the source code. To address various erosion symptoms, we have investigated the evolution of a large-scale industrial product line over a period of four years. Along improvement goals, we have researched a set of appropriate metrics and measurement approaches in a goal-oriented way, applied them in this case study with tool support, and interpreted the results including identified erosion symptoms.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {168–177},
numpages = {10},
keywords = {industrial case study, product line evolution, static code analysis, variability erosion},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3168365.3170426,
author = {Krieter, Sebastian and Kr\"{u}ger, Jacob and Leich, Thomas},
title = {Don't Worry About it: Managing Variability On-The-Fly},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3170426},
doi = {10.1145/3168365.3170426},
abstract = {Software-product-line engineering (SPLE) has become a widely adopted concept to implement reusable source code. However, instead of using SPLE from the beginning (the proactive approach), a software product line (SPL) is often only introduced after a set of similar systems is already developed (the extractive approach). This can lead to additional costs, new bugs introduced by refactoring, and an overall inconsistent SPL. In particular, inconsistencies between the variability implemented in the source code and the one represented in a variability model can become a major problem. To address this issue, we propose the concept of variability management derivation: We aim to (semi-)automatically model features and their dependencies while developers implement variable source code to facilitate the initial development, reusability, and later maintainability of SPLs, utilizing the reactive approach. In this paper, we demonstrate our concept by means of preprocessors. However, we claim that it can be adapted for other SPLE implementation techniques to facilitate SPL development.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {19–26},
numpages = {8},
keywords = {Software product line, adoption strategy, reactive development, variability model},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/2791060.2791078,
author = {Vale, Gustavo and Albuquerque, Danyllo and Figueiredo, Eduardo and Garcia, Alessandro},
title = {Defining metric thresholds for software product lines: a comparative study},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791078},
doi = {10.1145/2791060.2791078},
abstract = {A software product line (SPL) is a set of software systems that share a common and variable set of features. Software metrics provide basic means to quantify several modularity aspects of SPLs. However, the effectiveness of the SPL measurement process is directly dependent on the definition of reliable thresholds. If thresholds are not properly defined, it is difficult to actually know whether a given metric value indicates a potential problem in the feature implementation. There are several methods to derive thresholds for software metrics. However, there is little understanding about their appropriateness for the SPL context. This paper aims at comparing three methods to derive thresholds based on a benchmark of 33 SPLs. We assess to what extent these methods derive appropriate values for four metrics used in product-line engineering. These thresholds were used for guiding the identification of a typical anomaly found in features' implementation, named God Class. We also discuss the lessons learned on using such methods to derive thresholds for SPLs.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {176–185},
numpages = {10},
keywords = {metrics, software product lines, thresholds},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2934466.2934482,
author = {Sampaio, Gabriela and Borba, Paulo and Teixeira, Leopoldo},
title = {Partially safe evolution of software product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934482},
doi = {10.1145/2934466.2934482},
abstract = {A key challenge developers might face when evolving a product line is not to inadvertently affect users of existing products. In refactoring and conservative extension scenarios, we can avoid this problem by checking for behavior preservation, either by testing the generated products or by using formal theories. Product line refinement theories support that by requiring behavior preservation for all existing products. However, in many evolution scenarios, such as bug fixing, there is a high chance that only some of the products are refined. To support developers in these and other non full-refinement situations, we define a theory of partial product line refinement that helps to precisely understand which products should not be affected by an evolution scenario. This provides a kind of impact analysis that could, for example, reduce test effort, since products not affected do not need to be tested. Additionally, we formally derive a catalog of eight partial refinement templates that capture evolution scenarios, and associated preconditions, not covered before. Finally, by analyzing 79218 commits from the Linux repository, we find evidence that the proposed templates could cover a number of practical evolution scenarios.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {124–133},
numpages = {10},
keywords = {product line evolution, product line maintenance, product line refinement},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2019136.2019139,
author = {Passos, Leonardo and Novakovic, Marko and Xiong, Yingfei and Berger, Thorsten and Czarnecki, Krzysztof and W\k{a}sowski, Andrzej},
title = {A study of non-Boolean constraints in variability models of an embedded operating system},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019139},
doi = {10.1145/2019136.2019139},
abstract = {Many variability modeling tasks can be supported by automated analyses of models. Unfortunately, most analyses for Boolean variability models are NP-hard, while analyses for non-Boolean models easily become undecidable. It is thus crucial to exploit the properties of realistic models to construct viable analysis algorithms. Unfortunately, little work exists about non-Boolean models, and no benchmarks are available for such.We present the non-Boolean aspects of 116 variability models available in the codebase of eCos---a real time embedded operating system. We characterize the types of non-Boolean features in the models, kinds and quantities of non-Boolean constraints in use, and the impact of these characteristics on the hardness of this model from analysis perspective. This way we provide researchers and practitioners with a basis for discussion of relevance of non-Boolean models and their analyses, along with the first ever benchmark for effectiveness of such analyses.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {2},
numpages = {8},
keywords = {automated model analysis, decision models, feature models, variability modeling},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3278122.3278126,
author = {Peldszus, Sven and Str\"{u}ber, Daniel and J\"{u}rjens, Jan},
title = {Model-based security analysis of feature-oriented software product lines},
year = {2018},
isbn = {9781450360456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278122.3278126},
doi = {10.1145/3278122.3278126},
abstract = {Today's software systems are too complex to ensure security after the fact – security has to be built into systems by design. To this end, model-based techniques such as UMLsec support the design-time specification and analysis of security requirements by providing custom model annotations and checks. Yet, a particularly challenging type of complexity arises from the variability of software product lines. Analyzing the security of all products separately is generally infeasible. In this work, we propose SecPL, a methodology for ensuring security in a software product line. SecPL allows developers to annotate the system design model with product-line variability and security requirements. To keep the exponentially large configuration space tractable during security checks, SecPL provides a family-based security analysis. In our experiments, this analysis outperforms the naive strategy of checking all products individually. Finally, we present the results of a user study that indicates the usability of our overall methodology.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {93–106},
numpages = {14},
keywords = {OCL, Security, Software Product Lines, UML},
location = {Boston, MA, USA},
series = {GPCE 2018}
}

@inproceedings{10.1145/2961111.2962635,
author = {Echeverr\'{\i}a, Jorge and P\'{e}rez, Francisca and Abellanas, Andr\'{e}s and Panach, Jose Ignacio and Cetina, Carlos and Pastor, \'{O}scar},
title = {Evaluating Bug-Fixing in Software Product Lines: an Industrial Case Study},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962635},
doi = {10.1145/2961111.2962635},
abstract = {[Background] Bug-fixing could be complex in industrial practice since thousands of products share features in their configuration. Despite the importance and complexity of bug-fixing, there is still a lack of empirical data about the difficulties found in industrial Software Product Lines (SPLs). [Aims] This paper aims to evaluate engineers' performance fixing errors and propagating the fixes to other configured products in the context of an industrial SPL. [Method] We designed and conducted an empirical study to collect data with regard to bug-fixing tasks within the context of a Induction Hob SPL in the BSH group, the largest manufacturer of home appliances in Europe. [Results] We found that effectiveness, efficiency and satisfaction got reached good values. Through interviews we also found difficulties related to unused features, cloning features unintentionally, detecting modified features, and propagating the fix when the source of the bug is the interaction between features. [Conclusions] The identified difficulties are relevant to know how to better apply SPLs in industry in the future.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {24},
numpages = {6},
keywords = {Software Product Line, Usability, Variability Modeling},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@inproceedings{10.1145/3510003.3510190,
author = {Randrianaina, Georges Aaron and T\"{e}rnava, Xhevahire and Khelladi, Djamel Eddine and Acher, Mathieu},
title = {On the benefits and limits of incremental build of software configurations: an exploratory study},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510190},
doi = {10.1145/3510003.3510190},
abstract = {Software projects use build systems to automate the compilation, testing, and continuous deployment of their software products. As software becomes increasingly configurable, the build of multiple configurations is a pressing need, but expensive and challenging to implement. The current state of practice is to build independently (a.k.a., clean build) a software for a subset of configurations. While incremental build has been studied for software evolution and relatively small changes of the source code, it has surprisingly not been considered for software configurations. In this exploratory study, we examine the benefits and limits of building software configurations incrementally, rather than always building them cleanly. By using five real-life configurable systems as subjects, we explore whether incremental build works, outperforms a sequence of clean builds, is correct w.r.t. clean build, and can be used to find an optimal ordering for building configurations. Our results show that incremental build is feasible in 100% of the times in four subjects and in 78% of the times in one subject. In average, 88.5% of the configurations could be built faster with incremental build while also finding several alternatives faster incremental builds. However, only 60% of faster incremental builds are correct. Still, when considering those correct incremental builds with clean builds, we could always find an optimal order that is faster than just a collection of clean builds with a gain up to 11.76%.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1584–1596},
numpages = {13},
keywords = {build systems, configurable software systems, configuration build},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3023956.3023960,
author = {Pietsch, Christopher and Reuling, Dennis and Kelter, Udo and Kehrer, Timo},
title = {A tool environment for quality assurance of delta-oriented model-based SPLs},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023960},
doi = {10.1145/3023956.3023960},
abstract = {With the advent of model-driven engineering, software product line (SPL) technologies must be able to generate models as instances of a model-based SPL (MBSPL). Delta modeling is a variability mechanism which can be easily adopted for MBSPLs. The idea is to generate products by applying one or several deltas onto a core model. Hence, the main task during SPL implementation is to develop a set of deltas and to maintain several kinds of interrelations, e.g. dependencies and conflicts, in order to be able to generate all products of a MBSPL upon request. The resulting network of deltas is often complex and hard to maintain without appropriate tool support. This paper presents a tool environment for quality assurance in delta-oriented MBSPLs supporting the identification and elimination of design flaws in a network of deltas based on the principles of quality metrics and refactoring. We build upon previous work on the integration of model versioning tools with delta-oriented development of MBSPLs. Our solution is agnostic of the modeling language and may be easily extended by additional quality metrics and refactorings. We illustrate our approach using a delta-oriented MBSPL from the automation domain.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {84–91},
numpages = {8},
keywords = {delta modeling, model-based software product line, quality assurance, refactoring},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/2362536.2362566,
author = {Kircher, Michael and Hofman, Peter},
title = {Combining systematic reuse with Agile development: experience report},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362566},
doi = {10.1145/2362536.2362566},
abstract = {This paper documents the experiences of Siemens Healthcare in mastering challenges when transitioning a large-scale dispersed platform development organization to Agile. Product Line Engineering aims at increasing productivity through reuse, but since strategic reuse requires up-front decisions, is also seen as heavy weight and process driven. Agile development on the other hand is perceived as lightweight, change friendly, but at the same time neglecting long term strategic planning. With this paper we want to report on our experience combining both approaches, PLE for strategic reuse and agile principles for achieving steady progress while still leveraging the long-term benefits. The key was to build the foundation on the common best practice of 'feature-orientation' present in flavors in both disciplines. Feature-orientation allowed merging both disciplines into a holistic approach that blends the benefits of product line engineering with those of Agility -- resulting in improved product delivery, as well as employee and customer satisfaction.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {215–219},
numpages = {5},
keywords = {Agile, hierarchical platform, lean},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1109/ICSE43902.2021.00028,
author = {Gao, Yanjie and Zhu, Yonghao and Zhang, Hongyu and Lin, Haoxiang and Yang, Mao},
title = {Resource-Guided Configuration Space Reduction for Deep Learning Models},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00028},
doi = {10.1109/ICSE43902.2021.00028},
abstract = {Deep learning models, like traditional software systems, provide a large number of configuration options. A deep learning model can be configured with different hyperparameters and neural architectures. Recently, AutoML (Automated Machine Learning) has been widely adopted to automate model training by systematically exploring diverse configurations. However, current AutoML approaches do not take into consideration the computational constraints imposed by various resources such as available memory, computing power of devices, or execution time. The training with non-conforming configurations could lead to many failed AutoML trial jobs or inappropriate models, which cause significant resource waste and severely slow down development productivity.In this paper, we propose DnnSAT, a resource-guided AutoML approach for deep learning models to help existing AutoML tools efficiently reduce the configuration space ahead of time. DnnSAT can speed up the search process and achieve equal or even better model learning performance because it excludes trial jobs not satisfying the constraints and saves resources for more trials. We formulate the resource-guided configuration space reduction as a constraint satisfaction problem. DnnSAT includes a unified analytic cost model to construct common constraints with respect to the model weight size, number of floating-point operations, model inference time, and GPU memory consumption. It then utilizes an SMT solver to obtain the satisfiable configurations of hyperparameters and neural architectures. Our evaluation results demonstrate the effectiveness of DnnSAT in accelerating state-of-the-art AutoML methods (Hyperparameter Optimization and Neural Architecture Search) with an average speedup from 1.19X to 3.95X on public benchmarks. We believe that DnnSAT can make AutoML more practical in a real-world environment with constrained resources.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {175–187},
numpages = {13},
keywords = {AutoML, configurable systems, constraint solving, deep learning},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3425174.3425211,
author = {Ferreira, Thiago do Nascimento and Vergilio, Silvia Regina and Kessentini, Marouane},
title = {Applying Many-objective Algorithms to the Variability Test of Software Product Lines},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425211},
doi = {10.1145/3425174.3425211},
abstract = {The problem known as Variability Test of Software Product Line (VTSPL) is related to the selection of the most representative products for the SPL testing. This is an optimization problem because a possible exponential number of products can be derived from the SPL variability model, such as the Feature Model (FM). In the literature many works are dedicated to this research subject, each one applying a different search-based algorithm and using distinct criteria. However, there is no study encompassing all these criteria at the same time. To this end, this paper investigates the use of two Many-Objective Evolutionary Algorithms (MaOEAs). We apply the algorithm NSGA-III, widely used for many-objective algorithms, and the algorithm PCA-NSGA-II, a reduction dimensionality algorithm, which uses the Principal-Component Analysis (PCA) in combination with NSGA-II, to evaluate the objectives used in the literature for the VTSPL problem. PCA-NSGA-II reduces the search space dimensionality by eliminating the redundant objectives. The analysis shows the importance of some objectives such as the number of alive mutants, similarity between products, and unselected features. NSGA-III reaches the best results regarding the quality indicators for all instances, but taking a longer time. Besides, PCA-NSGA-II can find different solutions in the search space that are not found by NSGA-III.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {11–20},
numpages = {10},
keywords = {Software product line testing, dimensionality reduction, many-objective problems},
location = {Natal, Brazil},
series = {SAST '20}
}

@inproceedings{10.1109/ICSE.2019.00112,
author = {Kaltenecker, Christian and Grebhahn, Alexander and Siegmund, Norbert and Guo, Jianmei and Apel, Sven},
title = {Distance-based sampling of software configuration spaces},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00112},
doi = {10.1109/ICSE.2019.00112},
abstract = {Configurable software systems provide a multitude of configuration options to adjust and optimize their functional and non-functional properties. For instance, to find the fastest configuration for a given setting, a brute-force strategy measures the performance of all configurations, which is typically intractable. Addressing this challenge, state-of-the-art strategies rely on machine learning, analyzing only a few configurations (i.e., a sample set) to predict the performance of other configurations. However, to obtain accurate performance predictions, a representative sample set of configurations is required. Addressing this task, different sampling strategies have been proposed, which come with different advantages (e.g., covering the configuration space systematically) and disadvantages (e.g., the need to enumerate all configurations). In our experiments, we found that most sampling strategies do not achieve a good coverage of the configuration space with respect to covering relevant performance values. That is, they miss important configurations with distinct performance behavior. Based on this observation, we devise a new sampling strategy, called distance-based sampling, that is based on a distance metric and a probability distribution to spread the configurations of the sample set according to a given probability distribution across the configuration space. This way, we cover different kinds of interactions among configuration options in the sample set. To demonstrate the merits of distance-based sampling, we compare it to state-of-the-art sampling strategies, such as t-wise sampling, on 10 real-world configurable software systems. Our results show that distance-based sampling leads to more accurate performance models for medium to large sample sets.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1084–1094},
numpages = {11},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/2648511.2648519,
author = {Moens, Hendrik and De Turck, Filip},
title = {Feature-based application development and management of multi-tenant applications in clouds},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648519},
doi = {10.1145/2648511.2648519},
abstract = {In recent years, there has been a rising interest in cloud computing, which is often used to offer Software as a Service (SaaS) over the Internet. SaaS applications can be offered to clients at a lower cost as they are usually multi-tenant: many end users make use of a single application instance, even when they are from different organisations. It is difficult to offer highly customizable SaaS applications that are still multi-tenant, which is why these SaaS applications are often offered in a one size fits all approach.In some application domains applications must be highly customizable, making it more difficult to migrate them to a cloud environment, and losing the benefits of multi-tenancy. In this paper we compare multiple approaches for the development and management of highly customizable multitenant SaaS applications, and present a methodology for developing and managing these applications. We compare two approaches, an application-based approach focusing on deploying multiple multi-tenant applications variants, and a feature-based approach where applications are composed out of multi-tenant services using a service oriented architecture. In addition, we also discuss a hybrid approach combining properties of both. We conclude that the feature-based approach results in the fewest application instances at runtime resulting in more multi-tenancy.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {72–81},
numpages = {10},
keywords = {cloud computing, feature modeling, multi-tenancy},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2362536.2362554,
author = {Martini, Antonio and Pareto, Lars and Bosch, Jan},
title = {Enablers and inhibitors for speed with reuse},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362554},
doi = {10.1145/2362536.2362554},
abstract = {An open issue in industry is software reuse in the context of large scale Agile product development. The speed offered by agile practices is needed to hit the market, while reuse is needed for long-term productivity, efficiency, and profit. The paper presents an empirical investigation of factors influencing speed and reuse in three large product developing organizations seeking to implement Agile practices. The paper identifies, through a multiple case study with 3 organizations, 114 business-, process-, organizational-, architecture-, knowledge- and communication factors with positive or negative influences on reuse, speed or both. Contributions are a categorized inventory of influencing factors, a display for organizing factors for the purpose of process improvement work, and a list of key improvement areas to address when implementing reuse in organizations striving to become more Agile. Categories identified include good factors with positive influences on reuse or speed, harmful factors with negative influences, and complex factors involving inverse or ambiguous relationships. Key improvement areas in the studied organizations are intra-organizational communication practices, reuse awareness and practices, architectural integration and variability management. Results are intended to support process improvement work in the direction of Agile product development. Feedback on results from the studied organizations has been that the inventory captures current situations, and is useful for software process improvement work.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {116–125},
numpages = {10},
keywords = {agile software development, embedded systems, enablers, inhibitors, software process improvement (SPI), software reuse, speed},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3377024.3377046,
author = {Kr\"{u}ger, Jacob and Krieter, Sebastian and Saake, Gunter and Leich, Thomas},
title = {EXtracting product lines from vAriaNTs (EXPLANT)},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377046},
doi = {10.1145/3377024.3377046},
abstract = {The project EXtracting Product Lines from vAriaNTs (EXPLANT) funded by the German Research Foundation (DFG) is currently in its second phase. In this project, we are concerned with the stepwise migration of cloned variants into a software product line (i.e., the extractive approach of adopting systematic software reuse). While the extractive approach is the most common one in practice, many of its characteristics (e.g., processes, costs, best practices) are still unclear and tool support is limited (e.g., for feature location, refactoring, quality assurance). Within this extended abstract, we report on a selection of results we achieved in EXPLANT so far, and highlight our goals as well as opportunities for research collaborations in the second phase.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {13},
numpages = {2},
keywords = {clone and own, extractive approach, reverse engineering, software evolution, software product line},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1109/ICSE43902.2021.00029,
author = {Zhang, Yuanliang and He, Haochen and Legunsen, Owolabi and Li, Shanshan and Dong, Wei and Xu, Tianyin},
title = {An Evolutionary Study of Configuration Design and Implementation in Cloud Systems},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00029},
doi = {10.1109/ICSE43902.2021.00029},
abstract = {Many techniques were proposed for detecting software misconfigurations in cloud systems and for diagnosing unintended behavior caused by such misconfigurations. Detection and diagnosis are steps in the right direction: misconfigurations cause many costly failures and severe performance issues. But, we argue that continued focus on detection and diagnosis is symptomatic of a more serious problem: configuration design and implementation are not yet first-class software engineering endeavors in cloud systems. Little is known about how and why developers evolve configuration design and implementation, and the challenges that they face in doing so.This paper presents a source-code level study of the evolution of configuration design and implementation in cloud systems. Our goal is to understand the rationale and developer practices for revising initial configuration design/implementation decisions, especially in response to consequences of misconfigurations. To this end, we studied 1178 configuration-related commits from a 2.5 year version-control history of four large-scale, actively-maintained open-source cloud systems (HDFS, HBase, Spark, and Cassandra). We derive new insights into the software configuration engineering process. Our results motivate new techniques for proactively reducing misconfigurations by improving the configuration design and implementation process in cloud systems. We highlight a number of future research directions.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {188–200},
numpages = {13},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3634713.3634724,
author = {Romero-Organvidez, David and Benavides, David and Horcas, Jose-Miguel and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa},
title = {Variability in data transformation: towards data migration product lines},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634724},
doi = {10.1145/3634713.3634724},
abstract = {Software evolution often requires data management and more concretely data migration. Data migration follows an ETL process: extracting (E) data from a source, transforming (T) the data depending on migration needs, and loading (L) the data in a target data storage. Data migration projects are recognised to be complex and challenging to manage, which can lead to resource loss and planning delays. Among the reasons for data migration project failure is the lack of systematic artifact reuse (e.g., transformation script) in the data migration process. Every new data migration project is often developed from scratch. Software product line (SPL) engineering has been applied in many different domains to systematically reuse artifacts (e.g., code platforms, test cases) in software development processes and there are many positive experiences when applying SPL to reduce cost and time. In this paper, we present an approach using SPL techniques for data migration projects, concretely, in the data transformation stage. Our solution facilitates the automated creation of scripts that can be reused in different data migration projects. The feasibility of the proposal is validated in the domain of web information systems modernization. The validation shows how various migration scripts can be created to transform data between different content management systems. With this work, new opportunities are opened for studying the synergies of SPL and data migration. To the best of our knowledge, this is the first proposal that uses a complete stack of SPL that materializes the reuse of artifacts for data migration.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {83–92},
numpages = {10},
keywords = {SPL, data migration, data transformation, feature model},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.1145/2701319.2701325,
author = {Devroey, Xavier and Perrouin, Gilles and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Covering SPL Behaviour with Sampled Configurations: An Initial Assessment},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701325},
doi = {10.1145/2701319.2701325},
abstract = {Structural approaches to Software Product Lines (SPL) testing (such as pairwise testing) have gained momentum as they are able to scale to larger SPLs described as feature diagrams (FD). However, these methods are agnostic with respect to behaviour: the sampled configurations have thus no reason to satisfy any given behavioural criterion. In this paper, we investigate the behavioural coverage of two structural testing criteria: pairwise and similarity. To do so, we modelled four SPLs in terms of feature diagrams and associated featured transitions systems (FTSs). We then computed state, action and transition coverage for a set of generated configurations. Preliminary results indicate that for relatively small variability models with few cross-tree constraints, structural coverage-driven tools tend to cover large parts of behaviour with less than 8 configurations. Though structural coverage cannot be used directly as a replacement for behavioural driven SPL test generation, opportunities to mix structural and behavioural coverage for efficient and effective SPL testing do exist.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {59–66},
numpages = {8},
keywords = {Featured Transition System, SPL Testing, Structural Coverage},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@inproceedings{10.1145/2491627.2491650,
author = {Xu, Zhihong and Cohen, Myra B. and Motycka, Wayne and Rothermel, Gregg},
title = {Continuous test suite augmentation in software product lines},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491650},
doi = {10.1145/2491627.2491650},
abstract = {Software Product Line (SPL) engineering offers several advantages in the development of families of software products. There is still a need, however, to generate test cases for individual products in product lines more efficiently. In this paper we propose an approach, CONTESA, for generating test cases for SPLs using test suite augmentation. Instead of generating test cases for products independently, our approach generates new test cases for products in an order that allows it to build on test cases created for products tested earlier. In this work, we use a genetic algorithm to generate test cases, targeting branches not yet covered in each product, although other algorithms and coverage criteria could be utilized. We have evaluated CONTESA on two non-trivial SPLs, and have shown that CONTESA is more efficient and effective than an approach that generates test cases for products independently. A further evaluation shows that CONTESA is more effective at achieving coverage, and reveals as many faults as an existing feature-based testing approach.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {52–61},
numpages = {10},
keywords = {software product lines, software testing, test generation},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2647908.2655966,
author = {Itzik, Nili and Reinhartz-Berger, Iris},
title = {Generating feature models from requirements: structural vs. functional perspectives},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655966},
doi = {10.1145/2647908.2655966},
abstract = {Adoption of SPLE techniques is challenging and expensive. Hence, automation in the adoption process is desirable, especially with respect to variability management. Different methods have been suggested for (semi-)automatically generating feature models from requirements or textual descriptions of products. However, while there are different ways to represent the same SPL in feature models, addressing different stakeholders' needs and preferences, existing methods usually follow fixed, predefined ways to generate feature models. As a result, the generated feature models may represent perspectives less relevant to the given tasks.In this paper we suggest an ontological approach that measures the semantic similarity, extracts variability, and automatically generates feature models that represent structural (objects-related) or functional (actions-related) perspectives. The stakeholders are able to control the perspective of the generated feature models, considering their needs and preferences for given tasks.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {44–51},
numpages = {8},
keywords = {feature models, mining, ontology, reverse engineering, semantic similarity},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.5555/1753235.1753274,
author = {Pech, Daniel and Knodel, Jens and Carbon, Ralf and Schitter, Clemens and Hein, Dirk},
title = {Variability management in small development organizations: experiences and lessons learned from a case study},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Product line practices promise to reduce development and maintenance efforts, to improve the productivity and to reduce the time to market by systematic reuse of commonalities and variabilities. However, in order to reap the fruits of exploiting those, an upfront investment is required. This paper presents a case study, which analyzes the cost-benefit ratio for one product line discipline -- variability management. Wikon GmbH -- a small German development organization evolving a product line of remote monitoring and controlling devices -- switched from manual, file-based conditional compilation to tool-supported decision models. We discuss experiences made and show that the break-even was reached with the 4th product derivation.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {285–294},
numpages = {10},
keywords = {decision model, evolution, product line engineering, software architecture, variability management},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/3660515.3661325,
author = {Mei\ss{}ner, Simon and Degbelo, Auriol},
title = {User Performance Modelling for Spatial Entities Comparison with Geodashboards: Using View Quality and Distractor as Concepts},
year = {2024},
isbn = {9798400706516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660515.3661325},
doi = {10.1145/3660515.3661325},
abstract = {Geodashboards are increasingly available but there is still a lack of understanding about the design elements that contribute to a positive user experience. This work addresses that gap with a focus on the task of comparing spatial entities. The study explored ways of quantitatively modelling distraction and view quality during the use of geodashboards in question-answering scenarios. Eight types of questions related to the comparison of spatial entities were considered by varying the question-answering scenarios along three dimensions: number of comparison targets, comparison action, and map interaction operand. An exploratory study showed that an exponential model of distractors yields the best results for time and accuracy modelling. These initial findings can serve as a building block for researchers developing theoretical models for the computational design of geodashboards.},
booktitle = {Companion Proceedings of the 16th ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
pages = {7–14},
numpages = {8},
keywords = {geodashboard, mathematical models, user experience modelling, visual search},
location = {Cagliari, Italy},
series = {EICS '24 Companion}
}

@inproceedings{10.1145/3183399.3183403,
author = {Xu, Tianyin and Marinov, Darko},
title = {Mining container image repositories for software configuration and beyond},
year = {2018},
isbn = {9781450356626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183399.3183403},
doi = {10.1145/3183399.3183403},
abstract = {This paper introduces the idea of mining container image repositories for configuration and other deployment information of software systems. Unlike traditional software repositories (e.g., source code repositories and app stores), image repositories encapsulate the entire execution ecosystem for running target software, including its configurations, dependent libraries and components, and OS-level utilities, which contributes to a wealth of data and information. We showcase the opportunities based on concrete software engineering tasks that can benefit from mining image repositories. To facilitate future mining efforts, we summarize the challenges of analyzing image repositories and the approaches that can address these challenges. We hope that this paper will stimulate exciting research agenda of mining this emerging type of software repositories.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {49–52},
numpages = {4},
keywords = {configuration, container, docker, image, software repository},
location = {Gothenburg, Sweden},
series = {ICSE-NIER '18}
}

@inproceedings{10.1145/2019136.2019140,
author = {Pichler, Christian and Huemer, Christian},
title = {Feature modeling for business document models},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019140},
doi = {10.1145/2019136.2019140},
abstract = {The United Nations Centre for Trace Facilitation and eBusiness (UN/CEFACT) provides a conceptual approach named Core Components for defining business document types based on generic, reusable building blocks. For facilitating interoperability in Electronic Data Interchange, these reusable building blocks are defined in an all-embracing manner. Accordingly, business partners customize the standard business document types for fitting their needs and requirements, resulting in different business document type variants. However, the approach is missing sufficient mechanisms for managing business document model variants. First, customizing standardized business document types is purely based on a textual specification. Second, the variability present within the Core Component approach lacks an explicit representation. In this paper, we aim at making variability explicit as well as adding a formal aspect to the business document type customization process by employing variability concepts from Product Line Engineering. Furthermore, based on having explicit variability models, business partners are provided with an approach for customizing business document types through configuring variability models.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {3},
numpages = {8},
keywords = {business document models, feature modeling, service-oriented architecture, variability modeling},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2362536.2362569,
author = {Patzke, Thomas and Becker, Martin and Steffens, Michaela and Sierszecki, Krzysztof and Savolainen, Juha Erik and Fogdal, Thomas},
title = {Identifying improvement potential in evolving product line infrastructures: 3 case studies},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362569},
doi = {10.1145/2362536.2362569},
abstract = {Successful software products evolve continuously to meet the changing stakeholder requirements. For software product lines, an additional challenge is that variabilities, characteristics that vary among products, change as well over time. That challenge must be carefully tackled during the evolution of the product line infrastructure. This is a significant problem for many software development organizations, as practical guidelines on how to evolve core assets, and especially source code, are missing.This paper investigates how to achieve "good enough" variability management during the evolution of variation in software design and implementation assets. As a first contribution, we present a customizable goal-based approach which helps to identify improvement potential in existing core assets to ease evolution. To find concrete ways to improve the product line infrastructure, we list the typical symptoms of variability "code smells" and show how to refine them to root causes, questions, and finally to metrics that can be extracted from large code bases.As a second main contribution, we show how this method was applied to evaluate the reuse quality of three industrial embedded systems. These systems are implemented in C or C++ and use Conditional Compilation as the main variability mechanism. We also introduce the analysis and refactoring tool set that was used in the case studies and discuss the lessons learnt.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {239–248},
numpages = {10},
keywords = {PuLSE-E, goal-based product line measurement, industrial case study, product line code evolution, variability code smells},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2188286.2188304,
author = {Tawhid, Rasha and Petriu, Dorina},
title = {User-friendly approach for handling performance parameters during predictive software performance engineering},
year = {2012},
isbn = {9781450312028},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2188286.2188304},
doi = {10.1145/2188286.2188304},
abstract = {A Software Product Line (SPL) is a set of similar software systems that share a common set of features. Instead of building each product from scratch, SPL development takes advantage of the reusability of the core assets shared among the SPL members. In this work, we integrate performance analysis in the early phases of SPL development process, applying the same reusability concept to the performance annotations. Instead of annotating from scratch the UML model of every derived product, we propose to annotate the SPL model once with generic performance annotations. After deriving the model of a product from the family model by an automatic transformation, the generic performance annotations need to be bound to concrete product-specific values provided by the developer. Dealing manually with a large number of performance annotations, by asking the developer to inspect every diagram in the generated model and to extract these annotations is an error-prone process. In this paper we propose to automate the collection of all generic parameters from the product model and to present them to the developer in a user-friendly format (e.g., a spreadsheet per diagram, indicating each generic parameter together with guiding information that helps the user in providing concrete binding values). There are two kinds of generic parametric annotations handled by our approach: product-specific (corresponding to the set of features selected for the product) and platform-specific (such as device choices, network connections, middleware, and runtime environment). The following model transformations for (a) generating a product model with generic annotations from the SPL model, (b) building the spreadsheet with generic parameters and guiding information, and (c) performing the actual binding are all realized in the Atlas Transformation Language (ATL).},
booktitle = {Proceedings of the 3rd ACM/SPEC International Conference on Performance Engineering},
pages = {109–120},
numpages = {12},
keywords = {atl, marte, model-driven development, performance completion, performance model, spl, uml},
location = {Boston, Massachusetts, USA},
series = {ICPE '12}
}

@inproceedings{10.1145/3377024.3377030,
author = {Reuling, Dennis and Pietsch, Christopher and Kelter, Udo and Kehrer, Timo},
title = {Towards projectional editing for model-based SPLs},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377030},
doi = {10.1145/3377024.3377030},
abstract = {Model-based software product lines (MBSPLs) are implemented using various variability mechanisms. These are commonly categorized whether they separate features virtually (e.g., using annotations) or physically (e.g., using modules). Each of these mechanisms comprises advantages and disadvantages regarding MBSPL development, maintenance and analysis. To date, MBSPL developers have to choose upfront which variability mechanism to use, and the chosen mechanism including its drawbacks is bound to the MBSPL's entire lifecycle. In contrast, projectional editing has recently shown very promising potential of making the development of classical SPLs (e.g., implemented in C/C++) more flexible. User-editable projections allow developers to switch fluidly between different variability mechanisms based upon a common internal representation known as variational abstract syntax tree.In this paper, we report on ongoing work on the projectional editing of MBSPLs, which is challenged by a set of additional requirements. We lay the foundation for different editable projections using a common variational abstract syntax graph (vASG) as internal representation. This vASG is used for a fine-grained variability representation of EMOF-based models. We demonstrate the feasibility of our approach by incorporating different variability mechanism projections (150% models and delta modules) and modeling languages (Ecore class diagrams and UML state machines) used in existing MBSPL case studies.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {25},
numpages = {10},
keywords = {Physical Separation, model-based software product line engineering, projectional editing, virtual separation},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@proceedings{10.1145/3672919,
title = {CSAIDE '24: Proceedings of the 2024 3rd International Conference on Cyber Security, Artificial Intelligence and Digital Economy},
year = {2024},
isbn = {9798400718212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nanjing, China}
}

@article{10.1145/3524301,
author = {Kuo, Hsuan-Chi and Chen, Jianyan and Mohan, Sibin and Xu, Tianyin},
title = {Set the configuration for the heart of the OS: on the practicality of operating system kernel debloating},
year = {2022},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/3524301},
doi = {10.1145/3524301},
abstract = {This paper presents a study on the practicality of operating system (OS) kernel debloating, that is, reducing kernel code that is not needed by the target applications. Despite their significant benefits regarding security (attack surface reduction) and performance (fast boot time and reduced memory footprints), the state-of-the-art OS kernel debloating techniques are not widely adopted in practice, especially in production environments. We identify the limitations of existing kernel debloating techniques that hinder their practical adoption, such as both accidental and essential ones. To understand these limitations, we build an advanced debloating framework named Cozart that enables us to conduct a number of experiments on different types of OS kernels (such as Linux and the L4 microkernel) with a wide variety of applications (such as HTTPD, Memcached, MySQL, NGINX, PHP, and Redis). Our experimental results reveal the challenges and opportunities in making OS kernel debloating practical. We share these insights and our experience to shed light on addressing the limitations of kernel debloating techniques in future research and development efforts.},
journal = {Commun. ACM},
month = apr,
pages = {101–109},
numpages = {9}
}

@inproceedings{10.1109/MOBILESoft.2017.12,
author = {D\"{u}rschmid, Tobias and Trapp, Matthias and D\"{o}llner, J\"{u}rgen},
title = {Towards architectural styles for Android app software product lines},
year = {2017},
isbn = {9781538626696},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MOBILESoft.2017.12},
doi = {10.1109/MOBILESoft.2017.12},
abstract = {Software product line development for Android apps is difficult due to an inflexible design of the Android framework. However, since mobile applications become more and more complex, increased code reuse and thus reduced time-to-market play an important role, which can be improved by software product lines. We propose five architectural styles for developing software product lines of Android apps: (1) activity extensions, (2) activity connectors, (3) dynamic preference entries, (4) decoupled definition of domain-specific behavior via configuration files, (5) feature model using Android resources. We demonstrate the benefits in an early case study using an image processing product line which enables more than 90% of code reuse.},
booktitle = {Proceedings of the 4th International Conference on Mobile Software Engineering and Systems},
pages = {58–62},
numpages = {5},
keywords = {Android, image processing, reuse, software product line},
location = {Buenos Aires, Argentina},
series = {MOBILESoft '17}
}

@inproceedings{10.1145/3640310.3674090,
author = {Restrepo, Camilo Correa and Robin, Jacques and Mazo, Raul},
title = {Extensions and Scalability Experiments of a Generic Model-Driven Architecture for Variability Model Reasoning},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674090},
doi = {10.1145/3640310.3674090},
abstract = {Until recently, the state-of-the-art of Software Product Line (SPL) configuration and verification automation consisted of a collection of ad-hoc approaches tightly coupling a single input Variability Modeling Language (VML) with a single constraint solver. To remedy this situation, a novel generic model-driven architecture was then proposed that enables using a variety of VMLs and solvers. The key ideas of this proposal were (a) the use of a standard logical language (CLIF) as a pivot between VMLs and solvers, and (b) the use of a standard data exchange format (JSON) to explicilty and declaratively specify the abstract syntax and semantics of the VMLs to be used in an SPL engineering project and the automated reasoning task to be performed by the solvers.In this article, we overcome the limitations of this initial proposal in three key ways: (1) we add the ability to reason on textual or hybrid VMLs, rather than only on diagrammatic VMLs, enhancing the versatility of the architecture on the input side; (2) we enable the use of solvers from a third paradigm, enhancing the versatility of the architecture on the output side; and, (3) we present the results of scalability performance experiments of an implementation of this architecture. These results have been achieved without significantly altering the architecture, demonstrating its agnosticism with respect to specific VMLs and solvers. It also shows that it can underlie the implementation of practical variability reasoning tools that scale up to real sized variability model analysis and configuration needs.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {126–137},
numpages = {12},
keywords = {Automated Reasoning, Configuration Automation, Generic Architecture, Software Product Lines},
location = {Linz, Austria},
series = {MODELS '24}
}

@inproceedings{10.1145/2362536.2362551,
author = {Tischer, Christian and Boss, Birgit and M\"{u}ller, Andreas and Thums, Andreas and Acharya, Rajneesh and Schmid, Klaus},
title = {Developing long-term stable product line architectures},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362551},
doi = {10.1145/2362536.2362551},
abstract = {Product lines are usually built for the long term in order to repay the initial investment. While long-term stable software systems are already hard, if they are developed individually, it is even harder for complete product lines. At the time a new product line is created, the details of future product line characteristics are typically not known, no matter how well and detailed scoping and planning is done. Thus, any product line needs to evolve and adapt over time to incorporate new customer requirements as well as new technology constraints.Stability of the product line architecture is very important to the successful long-term evolution of a product line. In this paper, we discuss how a form of domain decomposition, which we call conceptual architecture, can be used to guide product line engineering towards long-term viability. We will illustrate this approach in the context of a large-scale product line development and analyze the evolution properties of the product line. Transferability of the approach is suggested to other embedded software systems that drive mature, well-understood physical control system.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {86–95},
numpages = {10},
keywords = {AUTOSAR, multi product lines, scoping, software architecture, software product lines},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.5555/1753235.1753249,
author = {Montagud, Sonia and Abrah\~{a}o, Silvia},
title = {Gathering current knowledge about quality evaluation in software product lines},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Recently, a number of methods and techniques for assessing the quality of software product lines have been proposed. However, to the best of our knowledge, there is no study which summarizes all the existing evidence about them. This paper presents a systematic review that investigates what methods and techniques have been employed (in the last 10 years) to evaluate the quality of software product lines and how they were employed. A total of 39 research papers have been reviewed from an initial set of 1388 papers. The results show that 25% of the papers reported evaluations at the Design phase of the Domain Engineering phase. The most widely used mechanism for modeling quality attributes was extended feature models and the most evaluated artifact was the base architecture. In addition, the results of the review have identified several research gaps. Specifically, 77% of the papers employed case studies as a "proof of concept" whereas 23% of the papers did not perform any type of validation. Our results are particularly relevant in positioning new research activities and in the selection of quality evaluation methods or techniques that best fit a given purpose.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {91–100},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2648511.2648527,
author = {Villela, Karina and Silva, Adeline and Vale, Tassio and de Almeida, Eduardo Santana},
title = {A survey on software variability management approaches},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648527},
doi = {10.1145/2648511.2648527},
abstract = {Variability Management (VM) is a key practice in the development of variant-rich systems. Over the years, attention has been paid to VM approaches adopted by traditional software product lines. The increasing demand for dynamic and highly configurable systems, however, calls for a closer look at the approaches used to develop these systems. We therefore conducted a survey with practitioners from organizations developing variant-rich systems in order to characterize the state of the practice. We also wanted to identify factors that might influence the adoption of specific VM approaches as well as the perception of problems/difficulties posed by those. We analyzed the answers of 31 respondents from thirteen countries and found that there is a correlation between the business domain and the adopted VM approaches. With regard to the problems/difficulties, the difficulty of assuring the quality of maintenance due to the explosion of dependencies was a major issue. This paper reports on relevant findings that could help companies to better understand their problems and researchers to design new/improved solutions.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {147–156},
numpages = {10},
keywords = {product line, state-of-the-practice, survey, variability, variability management},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1145/3389397,
author = {Lu, Hong and Yue, Tao and Ali, Shaukat},
title = {Pattern-based Interactive Configuration Derivation for Cyber-physical System Product Lines},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2378-962X},
url = {https://doi.org/10.1145/3389397},
doi = {10.1145/3389397},
abstract = {Deriving a Cyber-Physical System (CPS) product from a product line requires configuring hundreds to thousands of configurable parameters of components and devices from multiple domains, e.g., computing, control, and communication. A fully automated configuration process for a CPS product line is seldom possible in practice, and a dynamic and interactive process is expected. Therefore, some configurable parameters are to be configured manually, and the rest can be configured either automatically or manually, depending on pre-defined constraints, the order of configuration steps, and previous configuration data in such a dynamic and interactive configuration process. In this article, we propose a pattern-based, interactive configuration derivation methodology (named as Pi-CD) to maximize opportunities of automatically deriving correct configurations of CPSs by benefiting from pre-defined constraints and configuration data of previous configuration steps. Pi-CD requires architectures of CPS product lines modeled with Unified Modeling Language extended with four types of variabilities, along with constraints specified in Object Constraint Language (OCL). Pi-CD is equipped with 324 configuration derivation patterns that we defined by systematically analyzing the OCL constructs and semantics. We evaluated Pi-CD by configuring 20 CPS products of varying complexity from two real-world CPS product lines. Results show that Pi-CD can achieve up to 72% automation degree with a negligible time cost. Moreover, its time performance remains stable with the increase in the number of configuration parameters as well as constraints.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = jun,
articleno = {44},
numpages = {24},
keywords = {Product line engineering, configuration derivation, object constraint language, product configuration}
}

@inproceedings{10.1145/3425269.3425276,
author = {Silva, Publio and Bezerra, Carla I. M. and Lima, Rafael and Machado, Ivan},
title = {Classifying Feature Models Maintainability based on Machine Learning Algorithms},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425276},
doi = {10.1145/3425269.3425276},
abstract = {Maintenance in the context of SPLs is a topic of interest, and that still needs further investigation. There are several ways to evaluate the maintainability of a feature model (FM), one of which is a manual or automated analysis of quality measures. However, the use of measures does not allow to evaluate the FM quality as a whole, as each measure considers a specific characteristic of FM. In general, the measures have wide ranges of values and do not have a clear definition of what is appropriate and inappropriate. In this context, the goal of this work is to investigate the use of machine learning techniques to classify the feature model maintainability. The research questions investigated in the study were: (i) how could machine learning techniques aid to classify FMs maintainability; and, (ii) which FM classification model has the best accuracy and precision. In this work, we proposed an approach for FM maintainability classification using machine learning technics. For that, we used a dataset of 15 FM maintainability measures calculated for 326 FMs, and we used machine learning algorithms to clustering. After this, we used thresholds to evaluate the general maintainability of each cluster. With this, we built 5 maintainability classification models that have been evaluated with the accuracy and precision metrics.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {1–10},
numpages = {10},
keywords = {feature model, machine learning, quality evaluation, software product line},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.5555/1753235.1753265,
author = {Weston, Nathan and Chitchyan, Ruzanna and Rashid, Awais},
title = {A framework for constructing semantically composable feature models from natural language requirements},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Software Product Line Engineering (SPLE) requires the construction of feature models from large, unstructured and heterogeneous documents, and the reliable derivation of product variants from the resulting model. This can be an arduous task when performed manually, and can be error-prone in the presence of a change in requirements. In this paper we introduce a tool suite which automatically processes natural-language requirements documents into a candidate feature model, which can be refined by the requirements engineer. The framework also guides the process of identifying variant concerns and their composition with other features. We also provide language support for specifying semantic variant feature compositions which are resilient to change. We show that feature models produced by this framework compare favourably with those produced by domain experts by application to a real-life industrial example.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {211–220},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/3510455.3512792,
author = {Randrianaina, Georges Aaron and Khelladi, Djamel Eddine and Zendra, Olivier and Acher, Mathieu},
title = {Towards incremental build of software configurations},
year = {2022},
isbn = {9781450392242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510455.3512792},
doi = {10.1145/3510455.3512792},
abstract = {Building software is a crucial task to compile, test, and deploy software systems while continuously ensuring quality. As software is more and more configurable, building multiple configurations is a pressing need, yet, costly and challenging to instrument. The common practice is to independently build (a.k.a., clean build) a software for a subset of configurations. While incremental build has been considered for software evolution and relatively small modifications of the source code, it has surprisingly not been considered for software configurations. In this vision paper, we formulate the hypothesis that incremental build can reduce the cost of exploring the configuration space of software systems. We detail how we apply incremental build for two real-world application scenarios and conduct a preliminary evaluation on two case studies, namely x264 and Linux Kernel. For x264, we found that one can incrementally build configurations in an order such that overall build time is reduced. Nevertheless, we could not find any optimal order with the Linux Kernel, due to a high distance between random configurations. Therefore, we show it is possible to control the process of generating configurations: we could reuse commonality and gain up to 66% of build time compared to only clean builds.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {101–105},
numpages = {5},
keywords = {build system, highly configurable system, incremental build},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-NIER '22}
}

@inproceedings{10.1145/2647908.2655961,
author = {Seidl, Christoph and Domachowska, Irena},
title = {Teaching variability engineering to cognitive psychologists},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655961},
doi = {10.1145/2647908.2655961},
abstract = {In research of cognitive psychology, experiments to measure cognitive processes may be run in many similar yet slightly different configurations. Variability engineering offers techniques to handle variable configurations both conceptually and technically. However, these techniques are largely unknown to cognitive psychologists so that experiment configurations are specified informally or too coarse grain. This is problematic, because it becomes difficult to get an overview of paradigm configurations used in the so far conducted experiments. Variability engineering techniques provide, i.a., concise notations for capturing variability in software and can also be used to express the configurable nature of a wide range of experiments in cognitive psychology. Furthermore, it enables cognitive psychologists to structure configuration knowledge, to identify suitably similar experiment setups and to more efficiently identify individual configuration options as relevant reasons for a particular effect in the outcome of an experiment. In this paper, we present experiences with teaching variability engineering to cognitive psychologists along with a suitable curriculum.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {16–23},
numpages = {8},
keywords = {cognitive psychology, feature model, teaching, variability engineering},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2019136.2019155,
author = {Ribeiro, Heberth Braga G. and de Almeida, Eduardo Santana and de Lemos Meira, Silvio R.},
title = {An approach for implementing core assets in service-oriented product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019155},
doi = {10.1145/2019136.2019155},
abstract = {In the combination of Software Product Lines (SPL) and Service-Oriented Architectures (SOA), a key aspect is the understanding about the use of variability mechanisms for realizing variabilities in different levels of granularity, e.g., components, services, and service-compositions, addressed in a specific service technology and systematic fashion. In this paper, we propose an approach for implementing core assets in service-oriented product lines based on a well defined set of activities, tasks, inputs, outputs and roles. In order to assess the quality of the proposed approach, an initial case study conducted at the university was performed.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {17},
numpages = {4},
keywords = {service-oriented architecture, service-oriented product lines, software development process, software product lines},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3129790.3129796,
author = {Morales, Germania Rodriguez and Bened\'{\i}, Jennifer P\'{e}rez},
title = {Towards a reference software architecture for improving the accessibility and usability of open course ware},
year = {2017},
isbn = {9781450352178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129790.3129796},
doi = {10.1145/3129790.3129796},
abstract = {The main goal of OCW is to provide high quality and advance education knowledge worldwide, however OCW's access and use levels are below expectations. This paper presents a research summary of a PhD Thesis for providing a framework that assist engineers in promoting the accessibility and usability in the construction and improvement of Open Course Ware (OCW) sites. The guidance of the framework will provide standard-based solutions and improvements, as well as architectural design guidelines.},
booktitle = {Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings},
pages = {35–38},
numpages = {4},
keywords = {MOOC text tagging, accessibility, open course ware, open educational resources, software architecture, software product line, usability},
location = {Canterbury, United Kingdom},
series = {ECSA '17}
}

@inproceedings{10.1145/2771783.2771808,
author = {Tan, Tian Huat and Xue, Yinxing and Chen, Manman and Sun, Jun and Liu, Yang and Dong, Jin Song},
title = {Optimizing selection of competing features via feedback-directed evolutionary algorithms},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2771808},
doi = {10.1145/2771783.2771808},
abstract = {Software that support various groups of customers usually require complicated configurations to attain different functionalities. To model the configuration options, feature model is proposed to capture the commonalities and competing variabilities of the product variants in software family or Software Product Line (SPL). A key challenge for deriving a new product is to find a set of features that do not have inconsistencies or conflicts, yet optimize multiple objectives (e.g., minimizing cost and maximizing number of features), which are often competing with each other. Existing works have attempted to make use of evolutionary algorithms (EAs) to address this problem. In this work, we incorporated a novel feedback-directed mechanism into existing EAs. Our empirical results have shown that our method has improved noticeably over all unguided version of EAs on the optimal feature selection. In particular, for case studies in SPLOT and LVAT repositories, the feedback-directed Indicator-Based EA (IBEA) has increased the number of correct solutions found by 72.33% and 75%, compared to unguided IBEA. In addition, by leveraging a pre-computed solution, we have found 34 sound solutions for Linux X86, which contains 6888 features, in less than 40 seconds.},
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {246–256},
numpages = {11},
keywords = {SAT solvers, Software product line, evolutionary algorithms},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@inproceedings{10.1145/2491627.2491654,
author = {Schulze, Michael and Mauersberger, Jan and Beuche, Danilo},
title = {Functional safety and variability: can it be brought together?},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491654},
doi = {10.1145/2491627.2491654},
abstract = {Today's product development creates multiple products over time, often by using reuse strategies like "Clone and Own", leading to very inefficient reuse of artifacts in the long term since synergy effects between the products e.g. from testing cannot be utilized. Applying a product line approach with explicitly modeling the commonalities and variabilities of system artifacts and deriving products from that common base is a way to tackle the problem. High variant complexity can often be found in the development of embedded systems, which in turn often control safety critical functions. For these systems functional safety is a major concern not only since the ISO 26262 got relevant for the automotive industry. The arising question is: Can variability in functional safety related assets be treated in the same way as for other artifacts like requirements, models, and source code? In this paper we demonstrate on the example of two commercial tools and an automotive use case that from the technical/tool point of view safety related artifacts can be treated like other artifacts regarding variability. This means linking with variability information and visualizing as well as deriving of variants is feasible. This is a big step forward, because now not only ordinary artifacts but also functional safety related assets can be reused in the same way as other product line artifacts. However, we have identified and will discuss challenges with respect to variable safety analyses, regulations, and reuse of certifications, which need further research and elaboration, in this paper.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {236–243},
numpages = {8},
keywords = {functional-safety, tool support, variant management},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3477314.3507255,
author = {Ca\~{n}izares, Pablo C. and P\'{e}rez-Soler, Sara and Guerra, Esther and de Lara, Juan},
title = {Automating the measurement of heterogeneous chatbot designs},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3507255},
doi = {10.1145/3477314.3507255},
abstract = {Chatbots are being increasingly used to provide a natural language interface to all kinds of software services. However, while there are many platforms and tools for chatbot development, they typically lack support to statically measure properties of the designed chatbots, as indicators of their size, complexity, quality or usability, and facilitating comparison.To attack this problem, in this paper we propose a suite of 20 metrics for chatbot designs. The metrics are defined on a neutral chatbot design language, becoming independent of the implementation platform. We have developed a tool, called Asymob, which supports the translation of chatbots defined in several platforms into this neutral format to perform the measurements. As a proof-of-concept, we evaluate the metrics over a collection of Dialogflow and Rasa chatbots from several sources and open-source repositories. Our metrics helped detecting quality issues statically, and served as a basis for comparing chatbots from different origins and built using different technologies.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {1491–1498},
numpages = {8},
keywords = {chatbot design, metrics, quality assurance},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.1145/1964138.1964141,
author = {Tang, Antony and Couwenberg, Wim and Scheppink, Erik and de Burgh, Niels Aan and Deelstra, Sybren and van Vliet, Hans},
title = {SPL migration tensions: an industry experience},
year = {2010},
isbn = {9781450305426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1964138.1964141},
doi = {10.1145/1964138.1964141},
abstract = {In a software development environment where legacy software systems have been successfully deployed, there are tensions that deter the organization from moving towards software product line engineering (SPLE). An example is the effort required to develop a product line architecture versus time-to-market pressure or the lack of evidence to justify the benefits of SPLE. In this report we discuss the tensions that exist in Oc\'{e} Technologies. A reactive software reuse approach has not yielded the desired long-term benefits of reusability. A proactive approach requires knowledge exchange and coordination between software management and technical staff. We describe how such knowledge sharing can ease the tensions and facilitate a SPLE migration process.},
booktitle = {Proceedings of the 2010 Workshop on Knowledge-Oriented Product Line Engineering},
articleno = {3},
numpages = {6},
keywords = {agile development process, architecture management, industry case study, software product line engineering},
location = {Reno, Nevada},
series = {KOPLE '10}
}

@inproceedings{10.1145/1964138.1964140,
author = {Leitner, Andrea and Kreiner, Christian},
title = {Managing ERP configuration variants: an experience report},
year = {2010},
isbn = {9781450305426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1964138.1964140},
doi = {10.1145/1964138.1964140},
abstract = {The concepts of Software Product Line Engineering (SPLE) have been adapted and applied to enterprise IT systems, in particular the ERP systems of a production company. Based on a 2-layer feature model for the domain of the company's business processes, individual, albeit similar division's ERP system configurations can be derived by feature selection forming a variant description model. It is indicated that regular release upgrades can also benefit from the SPLE approach.The customization capabilities of the ERP platform are captured in another model; building up this model is automated according to information extracted online. As well, customizing an ERP system -- based on the models mentioned - is performed online with the help of a connector developed in this project.Quantitative analysis and lessons learned during the project conclude this experience report.},
booktitle = {Proceedings of the 2010 Workshop on Knowledge-Oriented Product Line Engineering},
articleno = {2},
numpages = {6},
keywords = {IT management, enterprise resource planning, experience report, software product line engineering},
location = {Reno, Nevada},
series = {KOPLE '10}
}

@inproceedings{10.1145/3474624.3476016,
author = {Bezerra, Carla and Lima, Rafael and Silva, Publio},
title = {DyMMer 2.0: A Tool for Dynamic Modeling and Evaluation of Feature Model},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3476016},
doi = {10.1145/3474624.3476016},
abstract = {Managing dynamic variability has motivated several researchers to combine Dynamic Software Product Lines (DSPLs) practices with runtime variability mechanisms. By combining these approaches, a DSPL acquires important features, ranging from the ability to reconfigure by changing the context, adding or removing features, crash recovery, and re-adaptation based on changes in the model’s features. Feature model (FM) is an important artifact of a DPSL and there is a lack of tools that support the modeling of this artifact. We have extended the DyMMer tool for modeling FM of DSPLs from an adaptation mechanism based on MAPE-K to solve this problem. We migrated the DyMMer tool to a web version and incorporated new features: (i) modeling of FMs from SPLs and DSPLs, (ii) development of an adaptation mechanism for FM of DSPLs, (iii) repository of FMs, (iv) inclusion of thresholds for measures, and (v) user authentication. We believe that this tool is useful for research in the area of DSPLs, and also for dynamic domain modeling and evaluation. Video: https://youtu.be/WVHW6bI8ois},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {121–126},
numpages = {6},
keywords = {Dynamic Software Product Line, Feature Model, Modeling},
location = {Joinville, Brazil},
series = {SBES '21}
}

@inproceedings{10.1145/3030207.3030226,
author = {Stefan, Petr and Horky, Vojtech and Bulej, Lubomir and Tuma, Petr},
title = {Unit Testing Performance in Java Projects: Are We There Yet?},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3030226},
doi = {10.1145/3030207.3030226},
abstract = {Although methods and tools for unit testing of performance exist for over a decade, anecdotal evidence suggests unit testing of performance is not nearly as common as unit testing of functionality. We examine this situation in a study of GitHub projects written in Java, looking for occurrences of performance evaluation code in common performance testing frameworks. We quantify the use of such frameworks, identifying the most relevant performance testing approaches, and describe how we adjust the design of our SPL performance testing framework to follow these conclusions.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {401–412},
numpages = {12},
keywords = {jmh, open source, performance unit testing, spl, survey},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@inproceedings{10.1145/2755644.2755647,
author = {Duplyakin, Dmitry and Haney, Matthew and Tufo, Henry},
title = {Architecting a Persistent and Reliable Configuration Management System},
year = {2015},
isbn = {9781450335706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2755644.2755647},
doi = {10.1145/2755644.2755647},
abstract = {Streamlined configuration management plays a significant role in modern, complex distributed systems. Via mechanisms that promote consistency, repeatability, and transparency, configuration management systems (CMSes) address complexity and aim to increase the efficiency of administrative procedures, including deployment and failure recovery scenarios. Considering the importance of minimizing disruptions in these systems, we design an architecture that increases persistency and reliability of infrastructure management. We present our architecture in the context of hybrid, cluster-cloud environments and describe our highly available implementation that builds upon the open source CMS called Chef and infrastructure-as-a-service cloud resources from Amazon Web Services. We demonstrate how we enabled a smooth transition from the pre-existing single-server configuration to the proposed highly available management system. We summarize our experience with managing a 20-node Linux cluster using this implementation. Our analysis of utilization and cost of necessary cloud resources indicates that the designed system is a low-cost alternative to acquiring additional physical hardware for hardening cluster management. We also highlight the prototype's security and manageability features that are suitable for larger, production-ready deployments.},
booktitle = {Proceedings of the 6th Workshop on Scientific Cloud Computing},
pages = {11–16},
numpages = {6},
keywords = {automated infrastructure management, cloud computing, computing cluster, configuration management system, high availability, hybrid systems},
location = {Portland, Oregon, USA},
series = {ScienceCloud '15}
}

@article{10.1145/3088440,
author = {Acher, Mathieu and Lopez-Herrejon, Roberto E. and Rabiser, Rick},
title = {Teaching Software Product Lines: A Snapshot of Current Practices and Challenges},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
url = {https://doi.org/10.1145/3088440},
doi = {10.1145/3088440},
abstract = {Software Product Line (SPL) engineering has emerged to provide the means to efficiently model, produce, and maintain multiple similar software variants, exploiting their common properties, and managing their variabilities (differences). With over two decades of existence, the community of SPL researchers and practitioners is thriving, as can be attested by the extensive research output and the numerous successful industrial projects. Education has a key role to support the next generation of practitioners to build highly complex, variability-intensive systems. Yet, it is unclear how the concepts of variability and SPLs are taught, what are the possible missing gaps and difficulties faced, what are the benefits, and what is the material available. Also, it remains unclear whether scholars teach what is actually needed by industry. In this article, we report on three initiatives we have conducted with scholars, educators, industry practitioners, and students to further understand the connection between SPLs and education, that is, an online survey on teaching SPLs we performed with 35 scholars, another survey on learning SPLs we conducted with 25 students, as well as two workshops held at the International Software Product Line Conference in 2014 and 2015 with both researchers and industry practitioners participating. We build upon the two surveys and the workshops to derive recommendations for educators to continue improving the state of practice of teaching SPLs, aimed at both individual educators as well as the wider community.},
journal = {ACM Trans. Comput. Educ.},
month = oct,
articleno = {2},
numpages = {31},
keywords = {Software product lines, software engineering teaching, software product line teaching, variability modeling}
}

@article{10.1145/3543146.3543163,
author = {Llado, Catalina M.},
title = {PIPE 2.7 overview A Petri net tool for performance modeling and evaluation},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {4},
issn = {0163-5999},
url = {https://doi.org/10.1145/3543146.3543163},
doi = {10.1145/3543146.3543163},
abstract = {The Petri net modeling formalism allows for the convenient graphical visualization of system models, as well as the modeling and performance analysis of complex stochastic systems. PIPE is an open source, platform independent tool for creating and analysing Petri nets including GSPNs (Generalized Stochastic Petri Nets). It is implemented entirely in Java and provides an easy-to-use graphical user interface that allows creating, saving and loading of Petri as well as its qualitative and quantitative analysis. This paper describes PIPE 2.7, its main features, including its GUI, modeling power, and analysis functionality.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = jun,
pages = {76–80},
numpages = {5},
keywords = {modeling tools, performance analysis, petri nets, stochastic modeling}
}

@inproceedings{10.1145/3422392.3422498,
author = {Freire, Willian Marques and Massago, Mamoru and Zavadski, Arthur Cattaneo and Malachini, Aline Maria and Amaral, Miotto and Colanzi, Thelma Elita},
title = {OPLA-Tool v2.0: a Tool for Product Line Architecture Design Optimization},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422498},
doi = {10.1145/3422392.3422498},
abstract = {The Multi-objective Optimization Approach for Product Line Architecture Design (MOA4PLA) is the seminal approach that successfully optimizes Product Line Architecture (PLA) design using search algorithms. The tool named OPLA-Tool was developed in order to automate the use of MOA4PLA. Over time, the customization of the tool to suit the needs of new research and application scenarios led to several problems. The main problems identified in the original version of OPLA-Tool are environment configuration, maintainability and usability problems, and PLA design modeling and visualization. Such problems motivated the development of a new version of this tool: OPLA-Tool v2.0, presented in this work. In this version, those problems were solved by the source code refactoring, migration to a web-based graphical user interface (GUI) and inclusion of a new support tool for PLA modeling and visualization. Furthermore, OPLA-Tool v2.0 has new functionalities, such as new objective functions, new search operators, intelligent interaction with users during the optimization process, multi-user authentication and simultaneous execution of several experiments to PLA optimization. Such a new version of OPLA-Tool is an important achievement to PLA design optimization as it provides an easier and more complete way to automate this task.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {818–823},
numpages = {6},
keywords = {Software product line, multi-objective evolutionary algorithms, product line architecture},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1145/2499777.2499779,
author = {Antkiewicz, Micha\l{} and B\k{a}k, Kacper and Murashkin, Alexandr and Olaechea, Rafael and Liang, Jia Hui (Jimmy) and Czarnecki, Krzysztof},
title = {Clafer tools for product line engineering},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499779},
doi = {10.1145/2499777.2499779},
abstract = {Clafer is a lightweight yet expressive language for structural modeling: feature modeling and configuration, class and object modeling, and metamodeling. Clafer Tools is an integrated set of tools based on Clafer. In this paper, we describe some product-line variability modeling scenarios of Clafer Tools from the viewpoints of product-line owner, product-line engineer, and product engineer.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {130–135},
numpages = {6},
keywords = {Clafer, ClaferIG, ClaferMOO, ClaferMOO visualizer, ClaferWiki, clafer configurator},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/2491627.2491628,
author = {Passos, Leonardo and Guo, Jianmei and Teixeira, Leopoldo and Czarnecki, Krzysztof and W\k{a}sowski, Andrzej and Borba, Paulo},
title = {Coevolution of variability models and related artifacts: a case study from the Linux kernel},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491628},
doi = {10.1145/2491627.2491628},
abstract = {Variability-aware systems are subject to the coevolution of variability models and related artifacts. Surprisingly, little knowledge exists to understand such coevolution in practice. This shortage is directly reflected in existing approaches and tools for variability management, as they fail to provide effective support for such a coevolution. To understand how variability models and related artifacts coevolve in a large and complex real-world variability-aware system, we inspect over 500 Linux kernel commits spanning almost four years of development. We collect a catalog of evolution patterns, capturing the coevolution of the Linux kernel variability model, Makefiles, and C source code. Further, we extract general findings to guide further research and tool development.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {91–100},
numpages = {10},
keywords = {Linux, catalog, evolution, patterns, variability},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/3382494.3410677,
author = {Shu, Yangyang and Sui, Yulei and Zhang, Hongyu and Xu, Guandong},
title = {Perf-AL: Performance Prediction for Configurable Software through Adversarial Learning},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410677},
doi = {10.1145/3382494.3410677},
abstract = {Context: Many software systems are highly configurable. Different configuration options could lead to varying performances of the system. It is difficult to measure system performance in the presence of an exponential number of possible combinations of these options.Goal: Predicting software performance by using a small configuration sample.Method: This paper proposes Perf-AL to address this problem via adversarial learning. Specifically, we use a generative network combined with several different regularization techniques (L1 regularization, L2 regularization and a dropout technique) to output predicted values as close to the ground truth labels as possible. With the use of adversarial learning, our network identifies and distinguishes the predicted values of the generator network from the ground truth value distribution. The generator and the discriminator compete with each other by refining the prediction model iteratively until its predicted values converge towards the ground truth distribution.Results: We argue that (i) the proposed method can achieve the same level of prediction accuracy, but with a smaller number of training samples. (ii) Our proposed model using seven real-world datasets show that our approach outperforms the state-of-the-art methods. This help to further promote software configurable performance.Conclusion: Experimental results on seven public real-world datasets demonstrate that PERF-AL outperforms state-of-the-art software performance prediction methods.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {16},
numpages = {11},
keywords = {Software performance prediction, adversarial learning, configurable systems, regularization},
location = {Bari, Italy},
series = {ESEM '20}
}

@inproceedings{10.1145/1982185.1982522,
author = {Mohabbati, Bardia and Hatala, Marek and Ga\v{s}evi\'{c}, Dragan and Asadi, Mohsen and Bo\v{s}kovi\'{c}, Marko},
title = {Development and configuration of service-oriented systems families},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982522},
doi = {10.1145/1982185.1982522},
abstract = {Software Product Lines (SPLs) are families of software systems which share a common sets of feature and are developed through common set of core assets in order to promotes software reusability, mass customization, reducing cost, time-to-market and improving the quality of the product. SPLs are sets (i.e., families) of software applications developed as a whole for a specific business domain. Particular applications are derived from software families by selecting the desired features through configuration process. Traditionally, SPLs are implemented with systematically developed components, shared by members of the SPLs and reused every time a new application is derived. In this paper, we propose an approach to the development and configuration of Service-Oriented SPLs in which services are used as reusable assets and building blocks of implementation. Our proposed approach also suggests prioritization of family features according to stakeholder's non-functional requirements (NFRs) and preferences. Priorities of NFRs are used to filter the most important features of the family, which is performed by Stratified Analytic Hierarchical Process (S-AHP). The priorities also are used further for the selection of appropriate services implementation for business processes realizing features. We apply Mixed Integer Linear Programming to find the optimal service selection within the constraints boundaries specified by stakeholders.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {1606–1613},
numpages = {8},
keywords = {feature-oriented development, optimization, service selection, service-oriented architecture, software product line},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@inproceedings{10.1145/2364412.2364442,
author = {Cavalcante, Everton and Almeida, Andr\'{e} and Batista, Thais and Cacho, N\'{e}lio and Lopes, Frederico and Delicato, Flavia C. and Sena, Thiago and Pires, Paulo F.},
title = {Exploiting software product lines to develop cloud computing applications},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364442},
doi = {10.1145/2364412.2364442},
abstract = {With the advance of the Cloud Computing paradigm, new challenges in terms of models, tools, and techniques to support developers to design, build and deploy complex software systems that make full use of the cloud technology arise. In the heterogeneous scenario of this new paradigm, the development of applications using cloud services becomes hard, and the software product lines (SPL) approach is potentially promising for this context since specificities of the cloud platforms, such as services heterogeneity, pricing model, and other aspects can be catered as variabilities to core features. In this perspective, this paper (i) proposes a seamless adaptation of the SPL-based development to include important features of cloud-based applications, and (ii) reports the experience of developing HW-CSPL, a SPL for the Health Watcher (HW) System, which allows citizens to register complaints and consult information regarding the public health system of a city. Several functionalities of this system were implemented using different Cloud Computing platforms, and run time specificities of this application deployed on the cloud were analyzed, as well as other information such as change impact and pricing.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {179–187},
numpages = {9},
keywords = {cloud computing, cloud platforms, health watcher system, services, software product lines},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791105,
author = {Teixeira, Leopoldo and Alves, Vander and Borba, Paulo and Gheyi, Rohit},
title = {A product line of theories for reasoning about safe evolution of product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791105},
doi = {10.1145/2791060.2791105},
abstract = {A product line refinement theory formalizes safe evolution in terms of a refinement notion, which does not rely on particular languages for the elements that constitute a product line. Based on this theory, we can derive refinement templates to support safe evolution scenarios. To do so, we need to provide formalizations for particular languages, to specify and prove the templates. Without a systematic approach, this leads to many similar templates and thus repetitive verification tasks. We investigate and explore similarities between these concrete languages, which ultimately results in a product line of theories, where different languages correspond to features, and products correspond to theory instantiations. This also leads to specifying refinement templates at a higher abstraction level, which, in the long run, reduces the specification and proof effort, and also provides the benefits of reusing such templates for additional languages plugged into the theory. We use the Prototype Verification System to encode and prove soundness of the theories and their instantiations. Moreover, we also use the refinement theory to reason about safe evolution of the proposed product line of theories.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {161–170},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3460319.3464823,
author = {Mordahl, Austin and Wei, Shiyi},
title = {The impact of tool configuration spaces on the evaluation of configurable taint analysis for Android},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464823},
doi = {10.1145/3460319.3464823},
abstract = {The most popular static taint analysis tools for Android allow users to change the underlying analysis algorithms through configuration options. However, the large configuration spaces make it difficult for developers and users alike to understand the full capabilities of these tools, and studies to-date have only focused on individual configurations. In this work, we present the first study that evaluates the configurations in Android taint analysis tools, focusing on the two most popular tools, FlowDroid and DroidSafe. First, we perform a manual code investigation to better understand how configurations are implemented in both tools. We formalize the expected effects of configuration option settings in terms of precision and soundness partial orders which we use to systematically test the configuration space. Second, we create a new dataset of 756 manually classified flows across 18 open-source real-world apps and conduct large-scale experiments on this dataset and micro-benchmarks. We observe that configurations make significant tradeoffs on the performance, precision, and soundness of both tools. The studies to-date would reach different conclusions on the tools' capabilities were they to consider configurations or use real-world datasets. In addition, we study the individual options through a statistical analysis and make actionable recommendations for users to tune the tools to their own ends. Finally, we use the partial orders to test the tool configuration spaces and detect 21 instances where options behaved in unexpected and incorrect ways, demonstrating the need for rigorous testing of configuration spaces.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {466–477},
numpages = {12},
keywords = {Android taint analysis, configurable static analysis, empirical study},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1145/3510466.3510470,
author = {Greiner, Sandra and Nieke, Michael and Seidl, Christoph},
title = {Towards Trace-Based Synchronization of Variability Annotations in Evolving Model-Driven Product Lines},
year = {2022},
isbn = {9781450396042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510466.3510470},
doi = {10.1145/3510466.3510470},
abstract = {Annotative model-driven product lines allow to derive individual variants from a multi-variant model by exploiting annotations. Those declare the presence of each model element in a specific set of variants via a logical expression over features and may change during evolution. This provokes the risk of introducing conflicts causing logically cohesive elements of different models to appear in diverging sets of variants, which threatens the consistency of the product line. Existing work on propagating annotations across models employs the comparatively simple strategy of either overwriting or manually protecting any changed annotation in the target model but does not consider a backward propagation nor any form of synchronization. Therefore, we contribute a sophisticated method for synchronizing annotations which detects corresponding elements based on model transformation traces and resolves conflicting annotations by preserving syntactically different but semantically equal annotations according to the feature model. We demonstrate challenges and our solution method in a scenario of synchronizing two corresponding evolving multi-variant models.},
booktitle = {Proceedings of the 16th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {3},
numpages = {10},
keywords = {Model Transformation, Model-driven Software Product Line Engineering, Software Evolution},
location = {Florence, Italy},
series = {VaMoS '22}
}

@inproceedings{10.1145/3442391.3442407,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Validating Feature Models With Respect to Textual Product Line Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442407},
doi = {10.1145/3442391.3442407},
abstract = {Feature models (FM) are a valuable resource in the analysis of software product lines (SPL). They provide a visual abstraction of the variation points in a family of related software products. FMs can be manually created by domain experts or extracted (semi-) automatically from textual documents such as product descriptions or requirements specifications. Nevertheless, there is no way to measure the accuracy of a FM with respect to the information described in the source documents. This paper proposes a method to quantify and visualize whether the elements in a FM (features and relationships) conform to the information available in a set of specification documents. Both the correctness (choice of representative elements) and completeness (no missing elements) of the FM are considered. Designers can use this feedback to fix defects in the FM or to detect incomplete or inconsistent information in the source documents.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {15},
numpages = {10},
keywords = {Feature Model Validation, Machine Learning, Natural Language Processing, Requirements Engineering, Software Product Line},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.1145/3324884.3416620,
author = {Dorn, Johannes and Apel, Sven and Siegmund, Norbert},
title = {Mastering uncertainty in performance estimations of configurable software systems},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416620},
doi = {10.1145/3324884.3416620},
abstract = {Understanding the influence of configuration options on performance is key for finding optimal system configurations, system understanding, and performance debugging. In prior research, a number of performance-influence modeling approaches have been proposed, which model a configuration option's influence and a configuration's performance as a scalar value. However, these point estimates falsely imply a certainty regarding an option's influence that neglects several sources of uncertainty within the assessment process, such as (1) measurement bias, (2) model representation and learning process, and (3) incomplete data. This leads to the situation that different approaches and even different learning runs assign different scalar performance values to options and interactions among them. The true influence is uncertain, though. There is no way to quantify this uncertainty with state-of-the-art performance modeling approaches. We propose a novel approach, P4, based on probabilistic programming that explicitly models uncertainty for option influences and consequently provides a confidence interval for each prediction of a configuration's performance alongside a scalar. This way, we can explain, for the first time, why predictions may cause errors and which option's influences may be unreliable. An evaluation on 12 real-world subject systems shows that P4's accuracy is in line with the state of the art while providing reliable confidence intervals, in addition to scalar predictions.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {684–696},
numpages = {13},
keywords = {P4, configurable software systems, performance-influence modeling, probabilistic programming},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00107,
author = {Weber, Max and Apel, Sven and Siegmund, Norbert},
title = {White-box performance-influence models: a profiling and learning approach (replication package)},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00107},
doi = {10.1109/ICSE-Companion52605.2021.00107},
abstract = {These artifacts refer to the study and implementation of the paper 'White-Box Performance-Influence Models: A Profiling and Learning Approach'. In this document, we describe the idea and process of how to build white-box performance models for configurable software systems. Specifically, we describe the general steps and tools that we have used to implement our approach, the data we have obtained, and the evaluation setup. We further list the available artifacts, such as raw measurements, configurations, and scripts at our software heritage repository.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {232–233},
numpages = {2},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3377024.3377026,
author = {Kenner, Andy and Dassow, Stephan and Lausberger, Christian and Kr\"{u}ger, Jacob and Leich, Thomas},
title = {Using variability modeling to support security evaluations: virtualizing the right attack scenarios},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377026},
doi = {10.1145/3377024.3377026},
abstract = {A software system's security is constantly threatened by vulnerabilities that result from faults in the system's design (e.g., unintended feature interactions) and which can be exploited with attacks. While various databases summarize information on vulnerabilities and other security issues for many software systems, these databases face severe limitations. For example, the information's quality is unclear, often only semi-structured, and barely connected to other information. Consequently, it can be challenging for any security-related stakeholder to extract and understand what information is relevant, considering that most systems exist in different variants and versions. To tackle this problem, we propose to design vulnerability feature models that represent the vulnerabilities of a system and enable developers to virtualize corresponding attack scenarios. In this paper, we report a first case study on Mozilla Firefox for which we extracted vulnerabilities and used them to virtualize vulnerable instances in Docker. To this end, we focused on extracting information from available databases and on evaluating the usability of the results. Our findings indicate several problems with the extraction that complicate modeling, understanding, and testing of vulnerabilities. Nonetheless, the databases provide a valuable foundation for our technique, which we aim to extend with automatic synthesis and analyses of feature models, as well as virtualization for attack scenarios in future work.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {10},
numpages = {9},
keywords = {attack scenarios, docker-container, exploit, feature model, software architecture, variability model, vulnerability},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1145/2361999.2362028,
author = {Abbas, Nadeem and Andersson, Jesper and Weyns, Danny},
title = {Modeling variability in product lines using domain quality attribute scenarios},
year = {2012},
isbn = {9781450315685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361999.2362028},
doi = {10.1145/2361999.2362028},
abstract = {The concept of variability is fundamental in software product lines and a successful implementation of a product line largely depends on how well domain requirements and their variability are specified, managed, and realized. While developing an educational software product line, we identified a lack of support to specify variability in quality concerns. To address this problem we propose an approach to model variability in quality concerns, which is an extension of quality attribute scenarios. In particular, we propose domain quality attribute scenarios, which extend standard quality attribute scenarios with additional information to support specification of variability and deriving product specific scenarios. We demonstrate the approach with scenarios for robustness and upgradability requirements in the educational software product line.},
booktitle = {Proceedings of the WICSA/ECSA 2012 Companion Volume},
pages = {135–142},
numpages = {8},
keywords = {quality attributes, scenarios, software product lines, variability},
location = {Helsinki, Finland},
series = {WICSA/ECSA '12}
}

@inproceedings{10.1145/2499777.2500721,
author = {Seidl, Christoph and Schaefer, Ina and A\ss{}mann, Uwe},
title = {Variability-aware safety analysis using delta component fault diagrams},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500721},
doi = {10.1145/2499777.2500721},
abstract = {Component Fault Diagrams (CFD) allow the specification of fault propagation paths, which is employed for the design of safety-critical systems as well as their certification. Even though families of safety-critical systems exist with many similar, yet not equal, variants there is no dedicated variability mechanism for CFDs to reuse commonalities of all family members and to alter only variable parts. In this paper, we present a variability representation approach for CFDs based on delta modeling that allows to transform an initial CFD within a closed or open variant space. Furthermore, we provide delta-aware analysis techniques for CFDs in order to analyse multiple variants efficiently. We show the feasibility of our approach by means of an example scenario based on the personal home robot TurtleBot using a prototypical implementation of our concepts.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {2–9},
numpages = {8},
keywords = {component fault diagrams, delta modeling, minimum cut set, safety, software fault trees, variability},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3458817.3476197,
author = {Shu, Tong and Guo, Yanfei and Wozniak, Justin and Ding, Xiaoning and Foster, Ian and Kurc, Tahsin},
title = {Bootstrapping in-situ workflow auto-tuning via combining performance models of component applications},
year = {2021},
isbn = {9781450384421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458817.3476197},
doi = {10.1145/3458817.3476197},
abstract = {In an in-situ workflow, multiple components such as simulation and analysis applications are coupled with streaming data transfers. The multiplicity of possible configurations necessitates an auto-tuner for workflow optimization. Existing auto-tuning approaches are computationally expensive because many configurations must be sampled by running the whole workflow repeatedly in order to train the auto-tuner surrogate model or otherwise explore the configuration space. To reduce these costs, we instead combine the performance models of component applications by exploiting the analytical workflow structure, selectively generating test configurations to measure and guide the training of a machine learning workflow surrogate model. Because the training can focus on well-performing configurations, the resulting surrogate model can achieve high prediction accuracy for good configurations despite training with fewer total configurations. Experiments with real applications demonstrate that our approach can identify significantly better configurations than other approaches for a fixed computer time budget.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {28},
numpages = {15},
keywords = {auto-tuning, bootstrapping, component model combination, in-situ workflow},
location = {St. Louis, Missouri},
series = {SC '21}
}

@inproceedings{10.1145/3377024.3377037,
author = {Mortara, Johann and T\"{e}rnava, Xhevahire and Collet, Philippe},
title = {Mapping features to automatically identified object-oriented variability implementations: the case of ArgoUML-SPL},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377037},
doi = {10.1145/3377024.3377037},
abstract = {In Software Product Line (SPL) engineering, mapping domain features to existing code assets is essential for variability management. When variability is already implemented through Object-Oriented (OO) techniques, it is too costly and error-prone to refactor assets in terms of features or to use feature annotations. In this work, we delve into the possible usage of automatically identified variation points with variants in an OO code base to enable feature mapping from the domain level. We report on an experiment conducted over ArgoUML-SPL, using its code as input for automatic detection through the symfinder toolchain, and the previously devised domain features as a ground truth. We analyse the relevance of the identified variation points with variants w.r.t. domain features, adapting precision and recall measures. This shows that the approach is feasible, that an automatic mapping can be envisaged, and also that the symfinder visualization is adapted to this process with some slight additions.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {20},
numpages = {9},
keywords = {automatic identification of variation points, software product lines, understanding software variability, variability traceability},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1145/3550355.3552411,
author = {Hentze, Marc and Sundermann, Chico and Th\"{u}m, Thomas and Schaefer, Ina},
title = {Quantifying the variability mismatch between problem and solution space},
year = {2022},
isbn = {9781450394666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550355.3552411},
doi = {10.1145/3550355.3552411},
abstract = {A software product line allows to derive individual software products based on a configuration. As the number of configurations is an indicator for the general complexity of a software product line, automatic #SAT analyses have been proposed to provide this information. However, the number of configurations does not need to match the number of derivable products. Due to this mismatch, using the number of configurations to reason about the software complexity (i.e., the number of derivable products) of a software product line can lead to wrong assumptions during implementation and testing. How to compute the actual number of derivable products, however, is unknown. In this paper, we mitigate this problem and present a concept to derive a solution-space feature model which allows to reuse existing #SAT analyses for computing the number of derivable products of a software product line. We apply our concept to a total of 119 subsystems of three industrial software product lines. The results show that the derivation scales for real world software product lines and confirm the mismatch between the number of configurations and the number of products.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems},
pages = {322–333},
numpages = {12},
keywords = {product lines, solution-space analyses, variability mismatch},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3548606.3563537,
author = {Sato, Takami and Hayakawa, Yuki and Suzuki, Ryo and Shiiki, Yohsuke and Yoshioka, Kentaro and Chen, Qi Alfred},
title = {Poster: Towards Large-Scale Measurement Study on LiDAR Spoofing Attacks against Object Detection},
year = {2022},
isbn = {9781450394505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548606.3563537},
doi = {10.1145/3548606.3563537},
abstract = {LiDAR (Light Detection And Ranging) is an indispensable sensor for precise long- and wide-range 3D sensing of the surrounding environment. The recent rapid deployment of autonomous driving (AD) has highly benefited from the advancement of LiDARs. At the same time, the safety-critical application strongly motivates its security research. Recent studies demonstrate that they can manipulate the LiDAR point cloud and fool object detection by shooting malicious lasers against LiDAR scanning. However, prior efforts focus on limited types of LiDARs and object detection models, and their threat models are not clearly validated in the real world. To fill the critical research gap, we plan to conduct the first large-scale measurement study on LiDAR spoofing attacks against a wide variety of LiDARs with major object detectors. To perform this measurement, we first significantly improved the LiDAR spoofing capability (30x more spoofing points than the prior attack) with more careful optics and functional electronics, which allows us to be the first to clearly demonstrate and quantify key attack capabilities assumed in prior works. In this poster, we present our preliminary results on VLP-16 and our research plan.},
booktitle = {Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},
pages = {3459–3461},
numpages = {3},
keywords = {spoofing attack, lidar, autonomous driving, 3d object detection},
location = {Los Angeles, CA, USA},
series = {CCS '22}
}

@inproceedings{10.1145/3510003.3510084,
author = {Han, Ruidong and Yang, Chao and Ma, Siqi and Ma, JiangFeng and Sun, Cong and Li, Juanru and Bertino, Elisa},
title = {Control parameters considered harmful: detecting range specification bugs in drone configuration modules via learning-guided search},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510084},
doi = {10.1145/3510003.3510084},
abstract = {In order to support a variety of missions and deal with different flight environments, drone control programs typically provide configurable control parameters. However, such a flexibility introduces vulnerabilities. One such vulnerability, referred to as range specification bugs, has been recently identified. The vulnerability originates from the fact that even though each individual parameter receives a value in the recommended value range, certain combinations of parameter values may affect the drone physical stability. In this paper, we develop a novel learning-guided search system to find such combinations, that we refer to as incorrect configurations. Our system applies metaheuristic search algorithms mutating configurations to detect the configuration parameters that have values driving the drone to unstable physical states. To guide the mutations, our system leverages a machine learning based predictor as the fitness evaluator. Finally, by utilizing multi-objective optimization, our system returns the feasible ranges based on the mutation search results. Because in our system the mutations are guided by a predictor, evaluating the parameter configurations does not require realistic/simulation executions. Therefore, our system supports a comprehensive and yet efficient detection of incorrect configurations. We have carried out an experimental evaluation of our system. The evaluation results show that the system successfully reports potentially incorrect configurations, of which over 85% lead to actual unstable physical states.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {462–473},
numpages = {12},
keywords = {configuration test, deep learning approximation, drone security, range specification bug},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/2892664.2892700,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia and Zschaler, Steffen},
title = {Towards contractual interfaces for reusable functional quality attribute operationalisations},
year = {2016},
isbn = {9781450340335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2892664.2892700},
doi = {10.1145/2892664.2892700},
abstract = {The quality of a software system can be measured by the extent to which it possesses a desired combination of quality attributes (QAs). While some QAs are achieved implicitly through the interaction of various functional components of the system, others (e.g., security) can be encapsulated in dedicated software components. These QAs are known as functional quality attributes (FQAs). As applications may require different FQAs, and each FQA can be composed of many concerns (e.g., access control and authentication), integrating FQAs is very complex and requires dedicated expertise. Software architects are required to manually define FQA components, identify appropriate points in their architecture where to weave them, and verify that the composition of these FQA components with the other components is correct. This is a complex and error prone process. In our previous work we defined reusable FQAs by encapsulating them as aspectual architecture models that can be woven into a base architecture. So far, the joinpoints for weaving had to be identified manually. This made it difficult for software architects to verify that they have woven all the necessary FQAs into all the right places. In this paper, we address this problem by introducing a notion of contract for FQAs so that the correct application of an FQA (or one of its concerns) can be checked or, alternatively, appropriate binding points can be identified and proposed to the software architect automatically.},
booktitle = {Companion Proceedings of the 15th International Conference on Modularity},
pages = {201–205},
numpages = {5},
keywords = {Weaving Patterns, Quality Attributes, Model-Driven Development, Aspect-Orientation},
location = {M\'{a}laga, Spain},
series = {MODULARITY Companion 2016}
}

@inproceedings{10.1145/2857546.2857608,
author = {Rahmat, Azizah and Kassim, Suzana and Selamat, Mohd Hasan and Hassan, Sa'adah},
title = {Actor in Multi Product Line},
year = {2016},
isbn = {9781450341424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2857546.2857608},
doi = {10.1145/2857546.2857608},
abstract = {Software product line (SPL) involved variability modeling in domain engineering that will be matched to the respected application engineering. Several researches existed within the scope of mapping from reference architecture (RA) in domain engineering to system architecture in application engineering within the same domain. However, the mapping of cross domain RA or Multi Product Line (MPL) required more systematic mapping due to the several participating product line architecture (PLA) that will further instantiated to specific system architecture. The objective of this paper was to propose an actor-oriented approach in the mapping process of reference architecture, product line architecture and system architecture of MPL. Since the reference architecture consisted of several components, the scope of this research was within the functional decomposition or source code level. The experiment was involving the runtime behavior of the java code. The code with actor-oriented approach had shown the least amount of time taken to complete the main method compared to the non-actor-oriented approach. In conclusion, actor-oriented approach performs better performance in the mapping of reference architecture to product line architecture and system architecture. For future work, the consistency of the mapping will be evaluated.},
booktitle = {Proceedings of the 10th International Conference on Ubiquitous Information Management and Communication},
articleno = {61},
numpages = {8},
keywords = {reference architecture, multi product line, cross-domain reference architecture, actor, Software product line},
location = {Danang, Viet Nam},
series = {IMCOM '16}
}

@inproceedings{10.1145/2425415.2425420,
author = {Eyal-Salman, Hamzeh and Seriai, Abdelhak-Djamel and Dony, Christophe and Al-msie'deen, Ra'fat},
title = {Recovering traceability links between feature models and source code of product variants},
year = {2012},
isbn = {9781450318099},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2425415.2425420},
doi = {10.1145/2425415.2425420},
abstract = {Usually software product variants, developed by clone-and-own approach, form often a starting point for building Software Product Line (SPL). To migrate software products that deemed similar into a product line, it is essential to trace variability among software artifacts because the distinguishing factor between traditional software engineering and software product line engineering is the variability. Variability tracing is used to support conversion from traditional software development into software product line development and automate products derivation process such that core assets can be automatically configured for a product according to the features selection from the feature model. Tracing and maintaining interrelationships between artifacts within a software system also are needed to facilitate program comprehension, make the process of maintaining the system less dependent on individual experts. This paper presents a method based on information retrieval approach namely, latent semantic indexing, to establish traceability links between object-oriented source code of product variants and their feature model as representative of variability model.},
booktitle = {Proceedings of the VARiability for You Workshop: Variability Modeling Made Useful for Everyone},
pages = {21–25},
numpages = {5},
keywords = {variability, traceability links, source code, software product line, latent semantic indexing, feature models},
location = {Innsbruck, Austria},
series = {VARY '12}
}

@inproceedings{10.1145/2970276.2970322,
author = {Meinicke, Jens and Wong, Chu-Pan and K\"{a}stner, Christian and Th\"{u}m, Thomas and Saake, Gunter},
title = {On essential configuration complexity: measuring interactions in highly-configurable systems},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970322},
doi = {10.1145/2970276.2970322},
abstract = {Quality assurance for highly-configurable systems is challenging due to the exponentially growing configuration space. Interactions among multiple options can lead to surprising behaviors, bugs, and security vulnerabilities. Analyzing all configurations systematically might be possible though if most options do not interact or interactions follow specific patterns that can be exploited by analysis tools. To better understand interactions in practice, we analyze program traces to characterize and identify where interactions occur on control flow and data. To this end, we developed a dynamic analysis for Java based on variability-aware execution and monitor executions of multiple small to medium-sized programs. We find that the essential configuration complexity of these programs is indeed much lower than the combinatorial explosion of the configuration space indicates. However, we also discover that the interaction characteristics that allow scalable and complete analyses are more nuanced than what is exploited by existing state-of-the-art quality assurance strategies.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {483–494},
numpages = {12},
keywords = {Variability-Aware Execution, Feature Interaction, Configurable Software},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.1145/3447545.3451177,
author = {Canales, Felipe and Hecht, Geoffrey and Bergel, Alexandre},
title = {Optimization of Java Virtual Machine Flags using Feature Model and Genetic Algorithm},
year = {2021},
isbn = {9781450383318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447545.3451177},
doi = {10.1145/3447545.3451177},
abstract = {Optimizing the Java Virtual Machine (JVM) options in order to get the best performance out of a program for production is a challenging and time-consuming task. HotSpot, the Oracle's open-source Java VM implementation offers more than 500 options, called flags, that can be used to tune the JVM's compiler, garbage collector (GC), heap size and much more. In addition to being numerous, these flags are sometimes poorly documented and create a need of benchmarking to ensure that the flags and their associated values deliver the best performance and stability for a particular program to execute.Auto-tuning approaches have already been proposed in order to mitigate this burden. However, in spite of increasingly sophisticated search techniques allowing for powerful optimizations, these approaches take little account of the underlying complexities of JVM flags. Indeed, dependencies and incompatibilities between flags are non-trivial to express, which if not taken into account may lead to invalid or spurious flag configurations that should not be considered by the auto-tuner.In this paper, we propose a novel model, inspired by the feature model used in Software Product Line, which takes the complexity of JVM's flags into account. We then demonstrate the usefulness of this model, using it as an input of a Genetic Algorithm (GA) to optimize the execution times of DaCapo Benchmarks.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {183–186},
numpages = {4},
keywords = {optimization, java virtual machine, genetic algorithm, feature model, auto-tuning},
location = {Virtual Event, France},
series = {ICPE '21}
}

@inproceedings{10.1145/3194078.3194082,
author = {Pukhkaiev, Dmytro and G\"{o}tz, Sebastian},
title = {BRISE: energy-efficient benchmark reduction},
year = {2018},
isbn = {9781450357326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194078.3194082},
doi = {10.1145/3194078.3194082},
abstract = {A considerable portion of research activities in computer science heavily relies on the process of benchmarking, e.g., to evaluate a hypothesis in an empirical study. The goal is to reveal how a set of independent variables (factors) influences one or more dependent variables. With a vast number of factors or a high amount of factors' values (levels), this process becomes time- and energy-consuming. Current approaches to lower the benchmarking effort suffer from two deficiencies: (1) they focus on reducing the number of factors and, hence, are inapplicable to experiments with only two factors, but a vast number of levels and (2) being adopted from, e.g., combinatorial optimization they are designed for a different search space structure and, thus, can be very wasteful. This paper provides an approach for benchmark reduction, based on adaptive instance selection and multiple linear regression. We evaluate our approach using four empirical studies, which investigate the effect made by dynamic voltage and frequency scaling in combination with dynamic concurrency throttling on the energy consumption of a computing system (parallel compression, sorting, and encryption algorithms as well as database query processing). Our findings show the effectiveness of the approach. We can save 78% of benchmarking effort, while the result's quality decreases only by 3 pp, due to using only a near-optimal configuration.},
booktitle = {Proceedings of the 6th International Workshop on Green and Sustainable Software},
pages = {23–30},
numpages = {8},
keywords = {non-functional properties, fractional factorial design, benchmarking, adaptive instance selection, active learning},
location = {Gothenburg, Sweden},
series = {GREENS '18}
}

@inproceedings{10.1145/3368089.3409684,
author = {Kr\"{u}ger, Jacob and Berger, Thorsten},
title = {An empirical analysis of the costs of clone- and platform-oriented software reuse},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409684},
doi = {10.1145/3368089.3409684},
abstract = {Software reuse lowers development costs and improves the quality of software systems. Two strategies are common: clone &amp; own (copying and adapting a system) and platform-oriented reuse (building a configurable platform). The former is readily available, flexible, and initially cheap, but does not scale with the frequency of reuse, imposing high maintenance costs. The latter scales, but imposes high upfront investments for building the platform, and reduces flexibility. As such, each strategy has distinctive advantages and disadvantages, imposing different development activities and software architectures. Deciding for one strategy is a core decision with long-term impact on an organization’s software development. Unfortunately, the strategies’ costs are not well-understood - not surprisingly, given the lack of systematically elicited empirical data, which is difficult to collect. We present an empirical study of the development activities, costs, cost factors, and benefits associated with either reuse strategy. For this purpose, we combine quantitative and qualitative data that we triangulated from 26 interviews at a large organization and a systematic literature review covering 57 publications. Our study both confirms and refutes common hypotheses on software reuse. For instance, we confirm that developing for platform-oriented reuse is more expensive, but simultaneously reduces reuse costs; and that platform-orientation results in higher code quality compared to clone &amp; own. Surprisingly, refuting common hypotheses, we find that change propagation can be more expensive in a platform, that platforms can facilitate the advancement into innovative markets, and that there is no strict distinction of clone &amp; own and platform-oriented reuse in practice.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {432–444},
numpages = {13},
keywords = {software reuse, software product line, platform engineering, empirical study, economics, clone &amp; own},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3639478.3640046,
author = {Toledo, Rafael F. and Atlee, Joanne M. and Xiong, Rui Ming and Liu, Mingyu},
title = {(Neo4j)^ Browser: Visualizing Variable-Aware Analysis Results},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3640046},
doi = {10.1145/3639478.3640046},
abstract = {A software product line (SPL) implements a family of related software products. As such, analyzing a software produce line produces variable results that apply to some SPL variants and not to others. Typically, such results are annotated with presence conditions, which are logical expressions that represent the product variants to which the results apply. When analyzing large SPLs, these expressions that annotate results can become overwhelmingly large and difficult to reason about. In this paper, we present Neo4j Browser for visualizing and exploring the results of an SPL analysis. Neo4j Browser provides an interactive and customizable interface that allows the user to highlight results according to product variants of interest. Previous evaluations show that the Neo4j Browser improves the correctness and efficiency of the user's work and reduces the user's cognitive load in working with variable results. The tool can be downloaded at https://vault.cs.uwaterloo.ca/s/Rqy2f56PeC6s4XD, and a demo video presenting its features is at https://youtu.be/CoweflQQFWU.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {69–73},
numpages = {5},
keywords = {variability-aware visualizer, software product lines, graphical software models, Neo4j database},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.5555/2662572.2662583,
author = {Karimpour, Reza and Ruhe, Guenther},
title = {Bi-criteria genetic search for adding new features into an existing product line},
year = {2013},
isbn = {9781467362849},
publisher = {IEEE Press},
abstract = {Software product line evolution involves decisions like finding which products are better candidates for realizing new feature requests. In this paper, we propose a solution for finding trade-off evolution alternatives for products while balancing between overall value and product integrity. The purpose of this study is to support product managers with feature selection for an existing product line. For this purpose, first, the feature model of the product line is encoded into a single binary encoding. Then we employ a bi-criteria genetic search algorithm, NSGA-II, to find the possible alternatives with different value and product integrity. From the proposed set of trade-off alternatives, the product line manager can select the solutions that best fit with the concerns of their preference. The implementation has been initially evaluated by two product line configurations.},
booktitle = {Proceedings of the 1st International Workshop on Combining Modelling and Search-Based Software Engineering},
pages = {34–38},
numpages = {5},
keywords = {software product line, feature model, evolution},
location = {San Francisco, California},
series = {CMSBSE '13}
}

@inproceedings{10.1145/3626246.3653378,
author = {Pavlenko, Anna and Cahoon, Joyce and Zhu, Yiwen and Kroth, Brian and Nelson, Michael and Carter, Andrew and Liao, David and Wright, Travis and Camacho-Rodr\'{\i}guez, Jes\'{u}s and Saur, Karla},
title = {Vertically Autoscaling Monolithic Applications with CaaSPER: Scalable Container-as-a-Service Performance Enhanced Resizing Algorithm for the Cloud},
year = {2024},
isbn = {9798400704222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626246.3653378},
doi = {10.1145/3626246.3653378},
abstract = {Kubernetes has emerged as a prominent open-source platform for managing cloud applications, including stateful databases. These monolithic applications rely on vertical scaling, adjusting CPU cores based on load fluctuations. However, our analysis of Kubernetes-based Database-as-a-Service (DBaaS) offerings at Microsoft revealed that many customers consistently over-provision resources for peak workloads, neglecting cost-saving opportunities through resource scale-down. We found that there is a gap in the ability of existing vertical autoscaling tools to minimize resource slack and respond promptly to throttling, leading to increased costs and impacting crucial metrics such as throughput and availability.To address this challenge, we propose CaaSPER, a vertical autoscaling algorithm that blends reactive and proactive strategies. By dynamically adjusting CPU resources, CaaSPER minimizes resource slack, maintains optimal CPU utilization, and reduces throttling. Importantly, customers have the flexibility to prioritize either cost savings or high performance based on their preferences. Extensive testing demonstrates that CaaSPER effectively reduces throttling and keeps CPU utilization within target levels. CaaSPER is designed to be application-agnostic and platform-agnostic, with potential for extension to other applications requiring vertical autoscaling.},
booktitle = {Companion of the 2024 International Conference on Management of Data},
pages = {241–254},
numpages = {14},
keywords = {containers, kubernetes, resource optimization, vertical auto-scaling},
location = {Santiago AA, Chile},
series = {SIGMOD '24}
}

@inproceedings{10.1145/3106237.3106283,
author = {Gazzillo, Paul},
title = {Kmax: finding all configurations of Kbuild makefiles statically},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106283},
doi = {10.1145/3106237.3106283},
abstract = {Feature-oriented software design is a useful paradigm for building and reasoning about highly-configurable software. By making variability explicit, feature-oriented tools and languages make program analysis tasks easier, such as bug-finding, maintenance, and more. But critical software, such as Linux, coreboot, and BusyBox rely instead on brittle tools, such as Makefiles, to encode variability, impeding variability-aware tool development. Summarizing Makefile behavior for all configurations is difficult, because Makefiles have unusual semantics, and exhaustive enumeration of all configurations is intractable in practice. Existing approaches use ad-hoc heuristics, missing much of the encoded variability in Makefiles. We present Kmax, a new static analysis algorithm and tool for Kbuild Makefiles. It is a family-based variability analysis algorithm, where paths are Boolean expressions of configuration options, called reaching configurations, and its abstract state enumerates string values for all configurations. Kmax localizes configuration explosion to the statement level, making precise analysis tractable. The implementation analyzes Makefiles from the Kbuild build system used by several low-level systems projects. Evaluation of Kmax on the Linux and BusyBox build systems shows it to be accurate, precise, and fast. It is the first tool to collect all source files and their configurations from Linux. Compared to previous approaches, Kmax is far more accurate and precise, performs with little overhead, and scales better.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {279–290},
numpages = {12},
keywords = {Variability, Static Analysis, Makefiles, Kmax, Kbuild, Configuration},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.5555/3106050.3106055,
author = {Enriquez, Juan and Casas, Sandra},
title = {Variability in usability tests for Android applications},
year = {2017},
isbn = {9781538628034},
publisher = {IEEE Press},
abstract = {This article presents a software product line (SPL) as a framework to create usability tests for Android applications. This tool enables flexible integration and configuration of field tests, allowing automatic collection of data about user interactions with an application and the contexts in which it is used (metrics and indicators). The framework supports variability in both the mobile devices and the requirements of evaluators and facilitates the generation of a large set of tests. This article presents the most important aspects of the modeling, design, and implementation of the SPL for usability testing for Android applications; a case study that demonstrates the applicability of the approach; and an evaluation of the quality of the SPL in terms of modularity and reusability.},
booktitle = {Proceedings of the 2nd International Workshop on Variability and Complexity in Software Design},
pages = {13–19},
numpages = {7},
keywords = {usability testing, software product lines, mobile applications, feature model, context of use, aspect-oriented programming},
location = {Buenos Aires, Argentina},
series = {VACE '17}
}

@inproceedings{10.1145/2814251.2814263,
author = {Ochoa, Lina and Gonz\'{a}lez-Rojas, Oscar and Th\"{u}m, Thomas},
title = {Using decision rules for solving conflicts in extended feature models},
year = {2015},
isbn = {9781450336864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814251.2814263},
doi = {10.1145/2814251.2814263},
abstract = {Software Product Line Engineering has introduced feature modeling as a domain analysis technique used to represent the variability of software products and decision-making scenarios. We present a model-based transformation approach to solve conflicts among configurations performed by different stakeholders on feature models. We propose the usage of a domain-specific language named CoCo to specify attributes as non-functional properties of features, and to describe business-related decision rules in terms of costs, time, and human resources. These specifications along with the stakeholders' configurations and the feature model are transformed into a constraint programming problem, on which decision rules are executed to find a non-conflicting set of solution configurations that are aligned to business objectives. We evaluate CoCo's compositionality and model complexity simplification while using a set of motivating decision scenarios.},
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Software Language Engineering},
pages = {149–160},
numpages = {12},
keywords = {model transformation chain, extended feature model, domain-specific language, constraint satisfaction problem, conflicting configurations, Domain engineering},
location = {Pittsburgh, PA, USA},
series = {SLE 2015}
}

@article{10.1145/2580950,
author = {Th\"{u}m, Thomas and Apel, Sven and K\"{a}stner, Christian and Schaefer, Ina and Saake, Gunter},
title = {A Classification and Survey of Analysis Strategies for Software Product Lines},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2580950},
doi = {10.1145/2580950},
abstract = {Software-product-line engineering has gained considerable momentum in recent years, both in industry and in academia. A software product line is a family of software products that share a common set of features. Software product lines challenge traditional analysis techniques, such as type checking, model checking, and theorem proving, in their quest of ensuring correctness and reliability of software. Simply creating and analyzing all products of a product line is usually not feasible, due to the potentially exponential number of valid feature combinations. Recently, researchers began to develop analysis techniques that take the distinguishing properties of software product lines into account, for example, by checking feature-related code in isolation or by exploiting variability information during analysis. The emerging field of product-line analyses is both broad and diverse, so it is difficult for researchers and practitioners to understand their similarities and differences. We propose a classification of product-line analyses to enable systematic research and application. Based on our insights with classifying and comparing a corpus of 123 research articles, we develop a research agenda to guide future research on product-line analyses.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {6},
numpages = {45},
keywords = {type checking, theorem proving, static analysis, software product line, software analysis, program family, model checking, Product-line analysis}
}

@inproceedings{10.1145/3643832.3661860,
author = {Duan, Di and Sun, Zehua and Ni, Tao and Li, Shuaicheng and Jia, Xiaohua and Xu, Weitao and Li, Tianxing},
title = {F2Key: Dynamically Converting Your Face into a Private Key Based on COTS Headphones for Reliable Voice Interaction},
year = {2024},
isbn = {9798400705816},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643832.3661860},
doi = {10.1145/3643832.3661860},
abstract = {In this paper, we proposed F2Key, the first earable physical security system based on commercial off-the-shelf headphones. F2Key enables impactful applications, such as enhancing voiceprint-based authentication systems, reliable voice assistants, audio deepfake defense, and the legal validity of artifacts. The key idea of F2Key is to establish a stable acoustic sensing field across the user's face and embed the user's facial structures and articulatory habits into a user-specific generative model that serves as a private key. The private key can decrypt the Channel Impulse Response (CIR) profiles provided by the acoustic sensing field into an inferred spectrogram that can match the real one calculated from the corresponding speech, provided that the user's CIR-spectrogram mapping relationship is consistent with the one embedded in the generative model. Extensive experiments demonstrate that F2Key resists 99.9%, 96.4%, and 95.3% of speech replay attacks, mimicry attacks, and hybrid attacks, respectively. We discussed and evaluated F2Key from different perspectives, such as the health consideration and identical twins study, to show the practicality and reliability.},
booktitle = {Proceedings of the 22nd Annual International Conference on Mobile Systems, Applications and Services},
pages = {127–140},
numpages = {14},
keywords = {acoustic sensing, deepfake detection, earable sensing, physical security system},
location = {Minato-ku, Tokyo, Japan},
series = {MOBISYS '24}
}

@inproceedings{10.1145/3278122.3278123,
author = {Nieke, Michael and Mauro, Jacopo and Seidl, Christoph and Th\"{u}m, Thomas and Yu, Ingrid Chieh and Franzke, Felix},
title = {Anomaly analyses for feature-model evolution},
year = {2018},
isbn = {9781450360456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278122.3278123},
doi = {10.1145/3278122.3278123},
abstract = {Software Product Lines (SPLs) are a common technique to capture families of software products in terms of commonalities and variabilities. On a conceptual level, functionality of an SPL is modeled in terms of features in Feature Models (FMs). As other software systems, SPLs and their FMs are subject to evolution that may lead to the introduction of anomalies (e.g., non-selectable features). To fix such anomalies, developers need to understand the cause for them. However, for large evolution histories and large SPLs, explanations may become very long and, as a consequence, hard to understand. In this paper, we present a method for anomaly detection and explanation that, by encoding the entire evolution history, identifies the evolution step of anomaly introduction and explains which of the performed evolution operations lead to it. In our evaluation, we show that our method significantly reduces the complexity of generated explanations.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {188–201},
numpages = {14},
keywords = {Software Product Line, Feature Model, Explanation, Evolution Operation, Evolution, Anomalies},
location = {Boston, MA, USA},
series = {GPCE 2018}
}

@inproceedings{10.1145/3183440.3190328,
author = {Kr\"{u}ger, Jacob},
title = {When to extract features: towards a recommender system},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3190328},
doi = {10.1145/3183440.3190328},
abstract = {In practice, many organizations rely on cloning to implement customer-specific variants of a system. While this approach can have several disadvantages, organizations fear to extract reusable features later on, due to the corresponding efforts and risks. A particularly challenging and poorly supported task is to decide which features to extract. To tackle this problem, we aim to develop a recommender system that proposes suitable features based on automated analyses of the cloned legacy systems. In this paper, we sketch this recommender and its empirically derived metrics, which comprise cohesion, impact, and costs of features as well as the users' previous decisions. Overall, we will facilitate the adoption of systematic reuse based on an integrated platform.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {518–520},
numpages = {3},
keywords = {extractive approach, software maintenance, software product line},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3302333.3302341,
author = {Greiner, Sandra and Westfechtel, Bernhard},
title = {On Determining Variability Annotations In Partially Annotated Models},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302341},
doi = {10.1145/3302333.3302341},
abstract = {In model-driven software product line engineering (SPLE) the superset of products is developed over models. A feature model typically states the discriminating and common factors of the software. In annotative approaches model elements are associated with variability annotations which are boolean expressions over the features defining in which products the elements are visible. When the product line is defined over different models, the developer wants to annotate the model of one type and transform it to different representations, e.g., a (UML) class diagram into a relational database schema for establishing an object-relational mapping. Assigning the annotations manually to the target model is an error-prone and laborious task. In a black-box approach we automatically assign the correct annotations to the target model without analyzing the transformation specification. It is an easy task in the case of 1:1 mappings where the annotation of the source element is copied to the corresponding element in the target model. Typically this kind of information is available, e.g., in traces written during the transformation execution. In reality, more complex mappings are frequent but the correspondences harder to determine. Assuming a target model is already annotated with the annotation of 1:1 correspondences, a certain number of elements remains without annotations. This paper contributes strategies to determine missing annotations in partially annotated models. We compare a global include strategy with more sophisticated ones which take the model structure into account. Since we apply missing annotations locally on one model, we solve a general SPLE problem where completely annotated models reduce the manual user effort and are desirable for filtering.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {17},
numpages = {10},
keywords = {software evolution, feature propagation, annotative approach, Model-driven Software Product Line Engineering, (multi-variant) model transformations},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/2351676.2351678,
author = {Harman, Mark and Langdon, William B. and Jia, Yue and White, David R. and Arcuri, Andrea and Clark, John A.},
title = {The GISMOE challenge: constructing the pareto program surface using genetic programming to find better programs (keynote paper)},
year = {2012},
isbn = {9781450312042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351676.2351678},
doi = {10.1145/2351676.2351678},
abstract = {Optimising programs for non-functional properties such as speed, size, throughput, power consumption and bandwidth can be demanding; pity the poor programmer who is asked to cater for them all at once! We set out an alternate vision for a new kind of software development environment inspired by recent results from Search Based Software Engineering (SBSE). Given an input program that satisfies the functional requirements, the proposed programming environment will automatically generate a set of candidate program implementations, all of which share functionality, but each of which differ in their non-functional trade offs. The software designer navigates this diverse Pareto surface of candidate implementations, gaining insight into the trade offs and selecting solutions for different platforms and environments, thereby stretching beyond the reach of current compiler technologies. Rather than having to focus on the details required to manage complex, inter-related and conflicting, non-functional trade offs, the designer is thus freed to explore, to understand, to control and to decide rather than to construct.},
booktitle = {Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1–14},
numpages = {14},
keywords = {Search Based Optimization, SBSE, Pareto Surface, Non-functional Properties, Genetic Programming, Compilation},
location = {Essen, Germany},
series = {ASE '12}
}

@inproceedings{10.1109/ASE.2013.6693103,
author = {Pohl, Richard and Stricker, Vanessa and Pohl, Klaus},
title = {Measuring the structural complexity of feature models},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693103},
doi = {10.1109/ASE.2013.6693103},
abstract = {The automated analysis of feature models (FM) is based on SAT, BDD, and CSP - known NP-complete problems. Therefore, the analysis could have an exponential worst-case execution time. However, for many practical relevant analysis cases, state-of-the-art (SOTA) analysis tools quite successfully master the problem of exponential worst-case execution time based on heuristics. So far, however, very little is known about the structure of FMs that cause the cases in which the execution time (hardness) for analyzing a given FM increases unpredictably for SOTA analysis tools. In this paper, we propose to use width measures from graph theory to characterize the structural complexity of FMs as a basis for an estimation of the hardness of analysis operations on FMs with SOTA analysis tools. We present an experiment that we use to analyze the reasonability of graph width measures as metric for the structural complexity of FMs and the hardness of FM analysis. Such a complexity metric can be used as a basis for a unified method to systematically improve SOTA analysis tools.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {454–464},
numpages = {11},
keywords = {software product line, performance measurement, feature model, automated analysis},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@article{10.1109/TNET.2019.2925658,
author = {Liaskos, Christos and Tsioliaridou, Ageliki and Nie, Shuai and Pitsillides, Andreas and Ioannidis, Sotiris and Akyildiz, Ian F.},
title = {On the Network-Layer Modeling and Configuration of Programmable Wireless Environments},
year = {2019},
issue_date = {August 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2019.2925658},
doi = {10.1109/TNET.2019.2925658},
abstract = {Programmable wireless environments enable the software-defined propagation of waves within them, yielding exceptional performance. Several building-block technologies have been implemented and evaluated at the physical layer in the past. The present work contributes a network-layer solution to configure such environments for multiple users and objectives, and for any underlying physical-layer technology. Supported objectives include any combination of Quality of Service and power transfer optimization, eavesdropping, and Doppler effect mitigation, in multi-cast or uni-cast settings. In addition, a graph-based model of programmable environments is proposed, which incorporates core physical observations and efficiently separates physical and networking concerns. The evaluation takes place in a specially developed simulation tool, and in a variety of environments, validating the model and reaching insights into the user capacity of programmable environments.},
journal = {IEEE/ACM Trans. Netw.},
month = aug,
pages = {1696–1713},
numpages = {18}
}

@inproceedings{10.1145/2851553.2851569,
author = {Hork\'{y}, Vojt\v{e}ch and Kotr\v{c}, Jaroslav and Libi\v{c}, Peter and T\r{u}ma, Petr},
title = {Analysis of Overhead in Dynamic Java Performance Monitoring},
year = {2016},
isbn = {9781450340809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851553.2851569},
doi = {10.1145/2851553.2851569},
abstract = {In production environments, runtime performance monitoring is often limited to logging of high level events. More detailed measurements, such as method level tracing, tend to be avoided because their overhead can disrupt execution. This limits the information available to developers when solving performance issues at code level. One approach that reduces the measurement disruptions is dynamic performance monitoring, where the measurement instrumentation is inserted and removed as needed. Such selective monitoring naturally reduces the aggregate overhead, but also introduces transient overhead artefacts related to insertion and removal of instrumentation. We experimentally analyze this overhead in Java, focusing in particular on the measurement accuracy, the character of the transient overhead, and the longevity of the overhead artefacts.Among other results, we show that dynamic monitoring requires time from seconds to minutes to deliver stable measurements, that the instrumentation can both slow down and speed up the execution, and that the overhead artefacts can persist beyond the monitoring period.},
booktitle = {Proceedings of the 7th ACM/SPEC on International Conference on Performance Engineering},
pages = {275–286},
numpages = {12},
keywords = {performance measurement overhead, java, dynamic instrumentation},
location = {Delft, The Netherlands},
series = {ICPE '16}
}

@inproceedings{10.1145/3302333.3302344,
author = {Ferreira, Fischer and Diniz, Jo\~{a}o P. and Silva, Cleiton and Figueiredo, Eduardo},
title = {Testing Tools for Configurable Software Systems: A Review-based Empirical Study},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302344},
doi = {10.1145/3302333.3302344},
abstract = {Configurable software systems are software systems that can be adapted or configured according to a set of features with the goal of increasing reuse and productivity. However, testing configurable systems is very challenging due to the number of configurations to run with each test, leading to a combinatorial explosion in the number of configurations and tests. Currently, several testing techniques and tools have been proposed to deal with this challenge, but their potential practical application remains mostly unexplored. The lack of studies to explore the tools that apply those techniques motivated us to investigate the literature to find testing tools for configurable software systems and to understand how they work. In this paper, we conducted a systematic mapping and identified 34 testing tools for configurable software systems. We first summarized and discussed their main characteristics. We then designed and performed a comparative empirical study of the main sound testing tools found: VarexJ and SPLat. They are considered sound testing techniques because they explore all reachable configurations from a given test. Overall, we observed that VarexJ and SPLat presented distinct results for efficiency while testing the target systems and that, although VarexJ found more errors than SPLat for the majority of the target systems, such result deserves a more in-depth investigation because we expected a higher intersection of errors encountered by them.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {6},
numpages = {10},
keywords = {Testing Configurable Software Systems, Systematic Mapping Study, Software Product Line},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/3106237.3106273,
author = {Oh, Jeho and Batory, Don and Myers, Margaret and Siegmund, Norbert},
title = {Finding near-optimal configurations in product lines by random sampling},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106273},
doi = {10.1145/3106237.3106273},
abstract = {Software Product Lines (SPLs) are highly configurable systems. This raises the challenge to find optimal performing configurations for an anticipated workload. As SPL configuration spaces are huge, it is infeasible to benchmark all configurations to find an optimal one. Prior work focused on building performance models to predict and optimize SPL configurations. Instead, we randomly sample and recursively search a configuration space directly to find near-optimal configurations without constructing a prediction model. Our algorithms are simpler and have higher accuracy and efficiency.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {61–71},
numpages = {11},
keywords = {software product lines, searching configuration spaces, finding optimal configurations},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/3131151.3131162,
author = {Guedes, Gabriela and Silva, Carla and Soares, Monique},
title = {Comparing Configuration Approaches for Dynamic Software Product Lines},
year = {2017},
isbn = {9781450353267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131151.3131162},
doi = {10.1145/3131151.3131162},
abstract = {Dynamic Software Product Lines (DSPLs) are Software Product Lines (SPLs) in which the configuration may occur at runtime. DSPL approaches provide means for modeling variability as well as configuring the product according to its runtime context and/or non-functional requirements (NFRs) satisfaction. In this paper, we present a Requirements Engineering (RE) approach for DSPL, ConG4DaS (Contextual Goal models For Dynamic Software product lines), which provides: (i) models for capturing variability with goals, NFRs, contexts and the relationship between them; and (ii) a configuration process that takes contexts, NFRs and their priority and interactions into account. We have used simulation based assessment to compare ConG4DaS with another approach, REFAS (Requirements Engineering For self-Adaptive Software systems), with respect to the satisfaction level of the highest priority softgoal. For the comparison, we modeled two DSPL examples and simulated different scenarios where reconfiguration is necessary. Next, we compared the configurations selected by the approaches with respect to overall NFRs' satisfaction. The results showed that ConG4DaS, which uses utility function in the configuration process, selects configurations that better satisfy NFRs compared to REFAS, which uses constraint programming.},
booktitle = {Proceedings of the XXXI Brazilian Symposium on Software Engineering},
pages = {134–143},
numpages = {10},
keywords = {Self-Adaptive Systems, Goal Models, Dynamic Variability, Dynamic Software Product Lines},
location = {Fortaleza, CE, Brazil},
series = {SBES '17}
}

@inproceedings{10.1145/2911451.2911510,
author = {Cormack, Gordon V. and Grossman, Maura R.},
title = {Engineering Quality and Reliability in Technology-Assisted Review},
year = {2016},
isbn = {9781450340694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2911451.2911510},
doi = {10.1145/2911451.2911510},
abstract = {The objective of technology-assisted review ("TAR") is to find as much relevant information as possible with reasonable effort. Quality is a measure of the extent to which a TAR method achieves this objective, while reliability is a measure of how consistently it achieves an acceptable result. We are concerned with how to define, measure, and achieve high quality and high reliability in TAR. When quality is defined using the traditional goal-post method of specifying a minimum acceptable recall threshold, the quality and reliability of a TAR method are both, by definition, equal to the probability of achieving the threshold. Assuming this definition of quality and reliability, we show how to augment any TAR method to achieve guaranteed reliability, for a quantifiable level of additional review effort. We demonstrate this result by augmenting the TAR method supplied as the baseline model implementation for the TREC 2015 Total Recall Track, measuring reliability and effort for 555 topics from eight test collections. While our empirical results corroborate our claim of guaranteed reliability, we observe that the augmentation strategy may entail disproportionate effort, especially when the number of relevant documents is low. To address this limitation, we propose stopping criteria for the model implementation that may be applied with no additional review effort, while achieving empirical reliability that compares favorably to the provably reliable method. We further argue that optimizing reliability according to the traditional goal-post method is inconsistent with certain subjective aspects of quality, and that optimizing a Taguchi quality loss function may be more apt.},
booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {75–84},
numpages = {10},
keywords = {continuous active learning, e-discovery, electronic discovery, predictive coding, quality, relevance feedback, reliability, systematic review, technology-assisted review, test collections},
location = {Pisa, Italy},
series = {SIGIR '16}
}

@inproceedings{10.1145/302405.302409,
author = {DeBaud, Jean-Marc and Schmid, Klaus},
title = {A systematic approach to derive the scope of software product lines},
year = {1999},
isbn = {1581130740},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/302405.302409},
doi = {10.1145/302405.302409},
booktitle = {Proceedings of the 21st International Conference on Software Engineering},
pages = {34–43},
numpages = {10},
keywords = {software product line, reuse economic models, product line scoping, domain engineering},
location = {Los Angeles, California, USA},
series = {ICSE '99}
}

@inproceedings{10.1145/3560905.3568499,
author = {Sun, Yimiao and Wang, Weiguo and Mottola, Luca and Wang, Ruijin and He, Yuan},
title = {AIM: Acoustic Inertial Measurement for Indoor Drone Localization and Tracking},
year = {2023},
isbn = {9781450398862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560905.3568499},
doi = {10.1145/3560905.3568499},
abstract = {We present Acoustic Inertial Measurement (AIM), a one-of-a-kind technique for indoor drone localization and tracking. Indoor drone localization and tracking are arguably a crucial, yet unsolved challenge: in GPS-denied environments, existing approaches enjoy limited applicability, especially in Non-Line of Sight (NLoS), require extensive environment instrumentation, or demand considerable hardware/software changes on drones. In contrast, AIM exploits the acoustic characteristics of the drones to estimate their location and derive their motion, even in NLoS settings. We tame location estimation errors using a dedicated Kalman filter and the Interquartile Range rule (IQR). We implement AIM using an off-the-shelf microphone array and evaluate its performance with a commercial drone under varied settings. Results indicate that the mean localization error of AIM is 46% lower than commercial UWB-based systems in complex indoor scenarios, where state-of-the-art infrared systems would not even work because of NLoS settings. We further demonstrate that AIM can be extended to support indoor spaces with arbitrary ranges and layouts without loss of accuracy by deploying distributed microphone arrays.},
booktitle = {Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems},
pages = {476–488},
numpages = {13},
keywords = {acoustic signal, drone, indoor tracking},
location = {Boston, Massachusetts},
series = {SenSys '22}
}

@inproceedings{10.1145/2642937.2642960,
author = {Hamidi, Saeideh and Andritsos, Periklis and Liaskos, Sotirios},
title = {Constructing adaptive configuration dialogs using crowd data},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2642960},
doi = {10.1145/2642937.2642960},
abstract = {As modern software systems grow in size and complexity so do their configuration possibilities. Users are easy to be confused and overwhelmed by the amount of choices they need to make in order to fit their systems to their exact needs. We propose a method to construct adaptive configuration elicitation dialogs through utilizing crowd wisdom. A set of configuration preferences in the form of association rules is first mined from a crowd configuration data set. Possible configuration elicitation dialogs are then modeled through a Markov Decision Process (MDP). Association rules are used to inform the model about configuration decisions that can be automatically inferred from knowledge already elicited earlier in the dialog. This way, an MDP solver can search for elicitation strategies which maximize the expected amount of automated decisions, reducing thereby elicitation effort and increasing user confidence of the result. The method is applied to the privacy configuration of Facebook.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {485–490},
numpages = {6},
keywords = {software customization, markov decision processes, facebook, crowdsourcing, association rules mining},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@inproceedings{10.1145/3691620.3695321,
author = {Gropengie\ss{}er, Uwe and Liphardt, Julian and Matth\'{e}, Michael and M\"{u}hlh\"{a}user, Max},
title = {Feature Model Slicing for Real-time Selection of Mission-critical Edge Application},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695321},
doi = {10.1145/3691620.3695321},
abstract = {At first glance, running mission-critical applications at the edge appears to be an opportunity to benefit from scalability and reusability. The low latency to the edge makes it particularly interesting for mission-critical applications. The hardware heterogeneity of the edge, coupled with the strict requirement for the execution time of a mission-critical application, creates the need for flexible application control and, at the same time, increases the complexity of modeling such systems. With its Feature Models (FMs), software product line engineering offers a modeling option for various alternative compositions of an application. However, the calculation of valid configurations takes too long for the dynamic adaptation of an application flow of a mission-critical application. This paper presents an approach for slicing FMs to support mission-critical applications. Our approach supports the strict requirements on the execution time of mission-critical applications.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {2446–2447},
numpages = {2},
keywords = {software product lines, feature model, approximate computing, edge computing},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/2695664.2695909,
author = {Burity, Tha\'{\i}s and Elias, Gledson},
title = {A quantitative, evidence-based approach for recommending software modules},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695909},
doi = {10.1145/2695664.2695909},
abstract = {In distributed software product line projects, dependencies between components influence on communication and coordination needs among their respective development teams. As an alternative to reduce such needs, it seems interesting to cluster tightly-coupled components into loosely-coupled modules as long as each module is developed by a single team. In such a context, considering that numerous clustering possibilities exist, this paper presents a quantitative, evidence-based approach for recommending software modules by clustering software components. The proposed approach has a threefold foundation: quantitative measures that characterize dependencies between components; a search-based clustering algorithm to recommend modules; and a quantitative measure that characterize dependencies between modules, which can guide the allocation of development teams to modules.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1449–1456},
numpages = {8},
keywords = {software product line, simulated annealing, search based software engineering, global software development, design structure matrix, architectural metrics},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/3452296.3472913,
author = {Namyar, Pooria and Supittayapornpong, Sucha and Zhang, Mingyang and Yu, Minlan and Govindan, Ramesh},
title = {A throughput-centric view of the performance of datacenter topologies},
year = {2021},
isbn = {9781450383837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452296.3472913},
doi = {10.1145/3452296.3472913},
abstract = {While prior work has explored many proposed datacenter designs, only two designs, Clos-based and expander-based, are generally considered practical because they can scale using commodity switching chips. Prior work has used two different metrics, bisection bandwidth and throughput, for evaluating these topologies at scale. Little is known, theoretically or practically, how these metrics relate to each other. Exploiting characteristics of these topologies, we prove an upper bound on their throughput, then show that this upper bound better estimates worst-case throughput than all previously proposed throughput estimators and scales better than most of them. Using this upper bound, we show that for expander-based topologies, unlike Clos, beyond a certain size of the network, no topology can have full throughput, even if it has full bisection bandwidth; in fact, even relatively small expander-based topologies fail to achieve full throughput. We conclude by showing that using throughput to evaluate datacenter performance instead of bisection bandwidth can alter conclusions in prior work about datacenter cost, manageability, and reliability.},
booktitle = {Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
pages = {349–369},
numpages = {21},
keywords = {throughput, network management, data centers, Clos topologies},
location = {Virtual Event, USA},
series = {SIGCOMM '21}
}

@article{10.1145/3689747,
author = {Bittner, Paul Maximilian and Schulthei\ss{}, Alexander and Moosherr, Benjamin and Young, Jeffrey M. and Teixeira, Leopoldo and Walkingshaw, Eric and Ataei, Parisa and Th\"{u}m, Thomas},
title = {On the Expressive Power of Languages for Static Variability},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689747},
doi = {10.1145/3689747},
abstract = {Variability permeates software development to satisfy ever-changing requirements and mass-customization needs. A prime example is the Linux kernel, which employs the C preprocessor to specify a set of related but distinct kernel variants. To study, analyze, and verify variational software, several formal languages have been proposed. For example, the choice calculus has been successfully applied for type checking and symbolic execution of configurable software, while other formalisms have been used for variational model checking, change impact analysis, among other use cases. Yet, these languages have not been formally compared, hence, little is known about their relationships. Crucially, it is unclear to what extent one language subsumes another, how research results from one language can be applied to other languages, and which language is suitable for which purpose or domain. In this paper, we propose a formal framework to compare the expressive power of languages for static (i.e., compile-time) variability. By establishing a common semantic domain to capture a widely used intuition of explicit variability, we can formulate the basic, yet to date neglected, properties of soundness, completeness, and expressiveness for variability languages. We then prove the (un)soundness and (in)completeness of a range of existing languages, and relate their ability to express the same variational systems. We implement our framework as an extensible open source Agda library in which proofs act as correct compilers between languages or differencing algorithms. We find different levels of expressiveness as well as complete and incomplete languages w.r.t. our unified semantic domain, with the choice calculus being among the most expressive languages.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {307},
numpages = {33},
keywords = {configuration, language semantics, software product lines, variation}
}

@inproceedings{10.1145/2556624.2556631,
author = {Dintzner, Nicolas and Van Deursen, Arie and Pinzger, Martin},
title = {Extracting feature model changes from the Linux kernel using FMDiff},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556631},
doi = {10.1145/2556624.2556631},
abstract = {The Linux kernel feature model has been studied as an example of large scale evolving feature model and yet details of its evolution are not known. We present here a classification of feature changes occurring on the Linux kernel feature model, as well as a tool, FMDiff, designed to automatically extract those changes. With this tool, we obtained the history of more than twenty architecture specific feature models, over ten releases and compared the recovered information with Kconfig file changes. We establish that FMDiff provides a comprehensive view of feature changes and show that the collected data contains promising information regarding the Linux feature model evolution.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {22},
numpages = {8},
keywords = {software product line, feature model, evolution},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@inproceedings{10.1145/2668930.2688051,
author = {Hork\'{y}, Vojt\v{e}ch and Libi\v{c}, Peter and Marek, Luk\'{a}\v{s} and Steinhauser, Antonin and T\r{u}ma, Petr},
title = {Utilizing Performance Unit Tests To Increase Performance Awareness},
year = {2015},
isbn = {9781450332484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668930.2688051},
doi = {10.1145/2668930.2688051},
abstract = {Many decisions taken during software development impact the resulting application performance. The key decisions whose potential impact is large are usually carefully weighed. In contrast, the same care is not used for many decisions whose individual impact is likely to be small -- simply because the costs would outweigh the benefits. Developer opinion is the common deciding factor for these cases, and our goal is to provide the developer with information that would help form such opinion, thus preventing performance loss due to the accumulated effect of many poor decisions.Our method turns performance unit tests into recipes for generating performance documentation. When the developer selects an interface and workload of interest, relevant performance documentation is generated interactively. This increases performance awareness -- with performance information available alongside standard interface documentation, developers should find it easier to take informed decisions even in situations where expensive performance evaluation is not practical. We demonstrate the method on multiple examples, which show how equipping code with performance unit tests works.},
booktitle = {Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
pages = {289–300},
numpages = {12},
keywords = {performance testing, performance documentation, performance awareness, javadoc, java},
location = {Austin, Texas, USA},
series = {ICPE '15}
}

@inproceedings{10.1145/1960502.1960509,
author = {Santos, Lidiane and Silva, Lyrene and Batista, Thais},
title = {On the integration of the feature model and PL-AOVGraph},
year = {2011},
isbn = {9781450306454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1960502.1960509},
doi = {10.1145/1960502.1960509},
abstract = {In this paper we propose PL-AOVGraph, an extension to the aspect-oriented requirements modeling language, AOV-Graph, to support the definition of software product line requirements. With PL-AOVGraph it is possible to specify requirements and variabilities. In general SPL variabilities are represented using the Feature Model, however, this model does not represent the requirements of the system. PL-AOVGraph and the Feature Model are complementary approaches as they represent different perspectives of a system. With the goal of inserting PLAOVGraph in the SPL development process, this work proposes a bi-directional mapping between PL-AOVGraph and the Feature Model},
booktitle = {Proceedings of the 2011 International Workshop on Early Aspects},
pages = {31–36},
numpages = {6},
keywords = {variabilities, software product line, requirements, feature model, PL-AOVgraph},
location = {Porto de Galinhas, Brazil},
series = {EA '11}
}

@article{10.1145/2581376,
author = {Behjati, Razieh and Nejati, Shiva and Briand, Lionel C.},
title = {Architecture-Level Configuration of Large-Scale Embedded Software Systems},
year = {2014},
issue_date = {May 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2581376},
doi = {10.1145/2581376},
abstract = {Configuration in the domain of Integrated Control Systems (ICS) is largely manual, laborious, and error prone. In this article, we propose a model-based configuration approach that provides automation support for reducing configuration effort and the likelihood of configuration errors in the ICS domain. We ground our approach on component-based specifications of ICS families. We then develop a configuration algorithm using constraint satisfaction techniques over finite domains to generate products that are consistent with respect to their ICS family specifications. We reason about the termination and consistency of our configuration algorithm analytically. We evaluate the effectiveness of our configuration approach by applying it to a real subsea oil production system. Specifically, we have rebuilt a number of existing verified product configurations of our industry partner. Our experience shows that our approach can automatically infer up to 50% of the configuration decisions, and reduces the complexity of making configuration decisions.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {25},
numpages = {43},
keywords = {product configuration, formal specification, constraint satisfaction techniques, consistent configuration, UML/OCL, Model-based product-line engineering}
}

@inproceedings{10.1145/3001867.3001871,
author = {Behringer, Benjamin and Fey, Moritz},
title = {Implementing delta-oriented SPLs using PEoPL: an example scenario and case study},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001871},
doi = {10.1145/3001867.3001871},
abstract = {Software product line implementation techniques are complementary. Thus, moving fluidly between them would be beneficial. Our tool PEoPL, a novel instantiation of the MPS language workbench, supports projecting a common variational AST into user-editable projections, each of which represents a different product line implementation technique. PEoPL supports FOP-like, annotative and product projections and allows developers to move fluidly between them. In this paper, we lay the foundation for future delta-oriented projectional editors. We use an example scenario to discuss a mapping of DeltaJ language concepts to the variational AST and operations employed in PEoPL. In a case study, we show that PEoPL is expressive enough to represent DeltaJ product lines, and at the same time supports all delta manipulations.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {28–38},
numpages = {11},
keywords = {projectional editing, language workbenches, delta-oriented programming, Software product lines},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@article{10.1145/2579281.2579294,
author = {Castelluccia, Daniela and Boffoli, Nicola},
title = {Service-oriented product lines: a systematic mapping study},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/2579281.2579294},
doi = {10.1145/2579281.2579294},
abstract = {Software product line engineering and service-oriented architectures both enable organizations to capitalize on reuse of existing software assets and capabilities and improve competitive advantage in terms of development savings, product flexibility, time-to-market. Both approaches accommodate variation of assets, including services, by changing the software being reused or composing services according a new orchestration. Therefore, variability management in Service-oriented Product Lines (SoPL) is one of the main challenges today. In order to highlight the emerging evidence-based results from the research community, we apply the well-defined method of systematic mapping in order to populate a classification scheme for the SoPL field of interest. The analysis of results throws light on the current open issues. Moreover, different facets of the scheme can be combined to answer more specific research questions. The report reveals the need for more empirical research able to provide new metrics measuring efficiency and efficacy of the proposed models, new methods and tools supporting variability management in SoPL, especially during maintenance and verification and validation. The mapping study about SoPL opens further investigations by means of a complete systematic review to select and validate the most efficient solutions to variability management in SoPL.},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {1–6},
numpages = {6},
keywords = {variability management, software product line, service-oriented computing, service-oriented architecture, product line development, mapping study, empirical study}
}

@article{10.1145/3635308,
author = {Huang, Hui and Xiao, Di and Liang, Jia},
title = {Secure Low-complexity Compressive Sensing with Preconditioning Prior Regularization Reconstruction},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1551-6857},
url = {https://doi.org/10.1145/3635308},
doi = {10.1145/3635308},
abstract = {Compressive sensing (CS), a breakthrough technology in image processing, provides a privacy-preserving layer and image reconstruction while performing sensing and recovery processes, respectively. Unfortunately, it still faces high-complexity, low-security, and low-quality reconstruction challenges during image processing. Therefore, this article presents a secure low-complexity CS scheme with preconditioning prior regularization reconstruction. More specifically, the original image is compressed by a low-complexity LFSR-based sparse circulant matrix to obtain measurements. It is worth noting that measurements achieve preliminary distribution equalization through the Tanh sequence to acquire processed measurements. Furthermore, the privacy-preserving edge processing for processed measurements can achieve high security. Finally, preconditioning prior regularization CS reconstruction is designed to improve reconstruction performance. Simulation results and analyses demonstrate that the proposed scheme can achieve low-complexity sampling, high security, and superior reconstruction performance.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jan,
articleno = {116},
numpages = {22},
keywords = {preconditioning prior regularization reconstruction, joint quantization and diffusion, sparse circulant matrix, Compressive sensing}
}

@inproceedings{10.1145/2695664.2695875,
author = {Almeida, Andr\'{e} and Bencomo, Nelly and Batista, Thais and Cavalcante, Everton and Dantas, Francisco},
title = {Dynamic decision-making based on NFR for managing software variability and configuration selection},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695875},
doi = {10.1145/2695664.2695875},
abstract = {Due to dynamic variability, identifying the specific conditions under which non-functional requirements (NFRs) are satisfied may be only possible at runtime. Therefore, it is necessary to consider the dynamic treatment of relevant information during the requirements specifications. The associated data can be gathered by monitoring the execution of the application and its underlying environment to support reasoning about how the current application configuration is fulfilling the established requirements. This paper presents a dynamic decision-making infrastructure to support both NFRs representation and monitoring, and to reason about the degree of satisfaction of NFRs during runtime. The infrastructure is composed of: (i) an extended feature model aligned with a domain-specific language for representing NFRs to be monitored at runtime; (ii) a monitoring infrastructure to continuously assess NFRs at runtime; and (iii) a flexible decision-making process to select the best available configuration based on the satisfaction degree of the NRFs. The evaluation of the approach has shown that it is able to choose application configurations that well fit user NFRs based on runtime information. The evaluation also revealed that the proposed infrastructure provided consistent indicators regarding the best application configurations that fit user NFRs. Finally, a benefit of our approach is that it allows us to quantify the level of satisfaction with respect to NFRs specification.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1376–1382},
numpages = {7},
keywords = {variability, non-functional requirements, monitoring, SPLs},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/2695664.2695933,
author = {Sena, Dem\'{o}stenes and Coelho, Roberta and Kulesza, Uir\'{a}},
title = {An investigation on the evolutionary nature of exception handling violations in software product lines},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695933},
doi = {10.1145/2695664.2695933},
abstract = {The Exception Handling (EH) is a widely used mechanism for building robust systems and it is embedded in most of the mainstream programming languages. In the context of Software Product Lines (SPL), we can find exception handling code associated to common and variable features of an SPL. However, studies have shown that the exception handling code can also become a source of bugs which may affect the system the other way around, making it even less robust. This paper describes an empirical study whose goal was to investigate the evolutionary nature of exception handling violations. Can the exception handling behavior be preserved along SPL evolution scenarios? In order to carry out this study we extended a tool, called PLEA, which statically discovers the exception handling flows on SPLs (those implemented in Java and using annotation techniques), and it was executed over five to seven versions of two distinct SPLs. Our goal was to identify how the different kinds of evolution scenarios affected the exception handling policy initially defined. The results showed that the evolution of the SPLs exception handling code did not occur in a planned way in most of the scenarios. Consequently, it led to violations on the exception handling behavior. This paper points out the need of a set of EH behavior-preserving evolution scenarios for the SPLs exception handling code.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1616–1623},
numpages = {8},
keywords = {software product line, software evolution, exception handling},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/2188286.2188345,
author = {Bulej, Lubom\'{\i}r and Bure\v{s}, Tom\'{a}\v{s} and Keznikl, Jaroslav and Koubkov\'{a}, Alena and Podzimek, Andrej and T\r{u}ma, Petr},
title = {Capturing performance assumptions using stochastic performance logic},
year = {2012},
isbn = {9781450312028},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2188286.2188345},
doi = {10.1145/2188286.2188345},
abstract = {Compared to functional unit testing, automated performance testing is difficult, partially because correctness criteria are more difficult to express for performance than for functionality. Where existing approaches rely on absolute bounds on the execution time, we aim to express assertions on code performance in relative, hardware-independent terms. To this end, we introduce Stochastic Performance Logic (SPL), which allows making statements about relative method performance. Since SPL interpretation is based on statistical tests applied to performance measurements, it allows (for a special class of formulas) calculating the minimum probability at which a particular SPL formula holds. We prove basic properties of the logic and present an algorithm for SAT-solver-guided evaluation of SPL formulas, which allows optimizing the number of performance measurements that need to be made. Finally, we propose integration of SPL formulas with Java code using higher-level performance annotations, for performance testing and documentation purposes.},
booktitle = {Proceedings of the 3rd ACM/SPEC International Conference on Performance Engineering},
pages = {311–322},
numpages = {12},
keywords = {regression benchmarking, performance testing},
location = {Boston, Massachusetts, USA},
series = {ICPE '12}
}

@inproceedings{10.1145/1297846.1297918,
author = {Krueger, Charles W.},
title = {BigLever software gears and the 3-tiered SPL methodology},
year = {2007},
isbn = {9781595938657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1297846.1297918},
doi = {10.1145/1297846.1297918},
abstract = {BigLever Software Gears is a software product line development tool that allows you to engineer your product line portfolio as though it is a single system. Gears is designed to support and enable all three tiers in the new generation 3-Tiered Software Product Line (SPL) Methodology, across the full SPL engineering lifecycle. Gears and the 3-Tiered SPL Methodology have played an instrumental role in some of the industry's most notable real-world success stories including Salion, 2004 Software Product line Hall of Fame Inductee, and Engenio/LSI Logic, 2006 Software Product Line Hall of Fame inductee.},
booktitle = {Companion to the 22nd ACM SIGPLAN Conference on Object-Oriented Programming Systems and Applications Companion},
pages = {844–845},
numpages = {2},
keywords = {software product lines},
location = {Montreal, Quebec, Canada},
series = {OOPSLA '07}
}

@inproceedings{10.1145/1988676.1988683,
author = {Burge, Janet E. and Gannod, Gerald C. and Connor, Holly L.},
title = {Using rationale to drive product line architecture configuration},
year = {2011},
isbn = {9781450305969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1988676.1988683},
doi = {10.1145/1988676.1988683},
abstract = {The process of designing and building a software system requires making many decisions. These decisions, the alternatives considered, and the reasons behind the choices comprise the rationale for the completed system. The driving force behind many, if not most, of these decisions is the need to meet the stakeholder requirements for the system being developed. Software product line approaches allow developers to design and develop families of products that share a common platform of behaviors and infrastructure. These approaches are based on assembling a configuration of a set of common features (commonalities) along with a set of product specific features (variabilities) to form a new product with a low amount of effort. In this context, these variabilities represent a wide variety of potential "design" alternatives. The goal of our research is to bring the end-user into the process of configuring a software product through the use of system level rationale that maps product line features to system requirements. Specifically, in our approach we specify rationale at the level of a feature diagram. Accordingly, we are taking advantage of the natural correlation between alternative features in a feature diagram and the alternative structure used in design rationale. This allows the end-user to indicate which requirements apply to their product and to have that selection generate a set of product features that satisfy those requirements.},
booktitle = {Proceedings of the 6th International Workshop on SHAring and Reusing Architectural Knowledge},
pages = {29–36},
numpages = {8},
keywords = {product line architecture, featuremodel, design rationale},
location = {Waikiki, Honolulu, HI, USA},
series = {SHARK '11}
}

@inproceedings{10.1145/1837154.1837157,
author = {Siegmund, Norbert and Feigenspan, Janet and Soffner, Michael and Fruth, Jana and K\"{o}ppen, Veit},
title = {Challenges of secure and reliable data management in heterogeneous environments},
year = {2010},
isbn = {9781605589923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1837154.1837157},
doi = {10.1145/1837154.1837157},
abstract = {Ubiquitous computing is getting more important since requirements for complex systems grow fast. In these systems, embedded devices have to fulfill different tasks. They have to monitor the environment, store data, communicate with other devices, and react to user input. In addition to this complexity, quality issues such as security and reliability have to be considered, as well, due to their increasing use in life critical application scenarios. Finally, different devices with different application goals are used, which results in interoperability problems. In this paper, we highlight challenges for interoperability, data management, and security, which arise with complex systems. Furthermore, we present approaches to overcome different problems and how an integrated solution can be realized using software product line techniques.},
booktitle = {Proceedings of the First International Workshop on Digital Engineering},
pages = {17–24},
numpages = {8},
keywords = {software product lines, security, digital engineering, data management},
location = {Magdeburg, Germany},
series = {IWDE '10}
}

@inproceedings{10.1145/3586183.3606775,
author = {Liu, Tiantian and Lin, Feng and Wang, Chao and Xu, Chenhan and Zhang, Xiaoyu and Li, Zhengxiong and Xu, Wenyao and Huang, Ming-Chun and Ren, Kui},
title = {WavoID: Robust and Secure Multi-modal User Identification via mmWave-voice Mechanism},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606775},
doi = {10.1145/3586183.3606775},
abstract = {With the increasing deployment of voice-controlled devices in homes and enterprises, there is an urgent demand for voice identification to prevent unauthorized access to sensitive information and property loss. However, due to the broadcast nature of sound wave, a voice-only system is vulnerable to adverse conditions and malicious attacks. We observe that the cooperation of millimeter waves (mmWave) and voice signals can significantly improve the effectiveness and security of user identification. Based on the properties, we propose a multi-modal user identification system (named WavoID) by fusing the uniqueness of mmWave-sensed vocal vibration and mic-recorded voice of users. To estimate fine-grained waveforms, WavoID splits signals and adaptively combines useful decomposed signals according to correlative contents in both mmWave and voice. An elaborated anti-spoofing module in WavoID comprising biometric bimodal information defend against attacks. WavoID produces and fuses the response maps of mmWave and voice to improve the representation power of fused features, benefiting accurate identification, even facing adverse circumstances. We evaluate WavoID using commercial sensors on extensive experiments. WavoID has significant performance on user identification with over 98% accuracy on 100 user datasets.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {64},
numpages = {15},
keywords = {User authentication, mmWave sensing, multi-modal fusion, voice identification},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1145/2973839.2973852,
author = {Santos, Ismayle S. and Rocha, Lincoln S. and Neto, Pedro A. Santos and Andrade, Rossana M. C.},
title = {Model Verification of Dynamic Software Product Lines},
year = {2016},
isbn = {9781450342018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2973839.2973852},
doi = {10.1145/2973839.2973852},
abstract = {Dynamic Software Product Lines (DSPLs) extend the concept of Software Product Lines enabling adaptation at runtime according to context changes. Such dynamic behavior is typically designed using adaptation rules, context-triggered actions responsible for features activation and deactivation at runtime. The erroneous specification and the interleaving of adaptation rules (i.e., the parallel execution of adaptation rules) can lead DSPL to reach an undesired (improperly or defective) product configuration at runtime. Thus, in order to improve the reliability of DSPL behavior, design faults must be rigorously identified and eliminated in the early stages of DSPL development. In this paper, we address this issue introducing Dynamic Feature Transition Systems (DFTSs) that allow the modeling and formal verification of the DSPLs adaptive behavior. These transition systems are derived from the adaptation rules and a Context Kripke Structure, which is a context evolution model. Furthermore, we formally define five properties that can be used to identify existing design faults in DSPL design. Aiming to assess the feasibility of our approach, a feasibility study was conducted using two DSPLs, Mobile Visit Guides and Car. In both cases, design faults were automatically detected indicating that our formalism can help in the detection of design faults in the DSPLs adaptive behavior.},
booktitle = {Proceedings of the XXX Brazilian Symposium on Software Engineering},
pages = {113–122},
numpages = {10},
keywords = {Software Verification, Software Reliability, Model Checking, Dynamic Software Product Line},
location = {Maring\'{a}, Brazil},
series = {SBES '16}
}

@inproceedings{10.1109/ASE.2013.6693104,
author = {Sayyad, Abdel Salam and Ingram, Joseph and Menzies, Tim and Ammar, Hany},
title = {Scalable product line configuration: a straw to break the camel's back},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693104},
doi = {10.1109/ASE.2013.6693104},
abstract = {Software product lines are hard to configure. Techniques that work for medium sized product lines fail for much larger product lines such as the Linux kernel with 6000+ features. This paper presents simple heuristics that help the Indicator-Based Evolutionary Algorithm (IBEA) in finding sound and optimum configurations of very large variability models in the presence of competing objectives. We employ a combination of static and evolutionary learning of model structure, in addition to utilizing a pre-computed solution used as a "seed" in the midst of a randomly-generated initial population. The seed solution works like a single straw that is enough to break the camel's back -given that it is a feature-rich seed. We show promising results where we can find 30 sound solutions for configuring upward of 6000 features within 30 minutes.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {465–474},
numpages = {10},
keywords = {variability models, multiobjective optimization, evolutionary algorithms, automated configuration, SMT solvers},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@inproceedings{10.1145/2000259.2000274,
author = {Brosch, Franz and Buhnova, Barbora and Koziolek, Heiko and Reussner, Ralf},
title = {Reliability prediction for fault-tolerant software architectures},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000274},
doi = {10.1145/2000259.2000274},
abstract = {Software fault tolerance mechanisms aim at improving the reliability of software systems. Their effectiveness (i.e., reliability impact) is highly application-specific and depends on the overall system architecture and usage profile. When examining multiple architecture configurations, such as in software product lines, it is a complex and error-prone task to include fault tolerance mechanisms effectively. Existing approaches for reliability analysis of software architectures either do not support modelling fault tolerance mechanisms or are not designed for an efficient evaluation of multiple architecture variants. We present a novel approach to analyse the effect of software fault tolerance mechanisms in varying architecture configurations. We have validated the approach in multiple case studies, including a large-scale industrial system, demonstrating its ability to support architecture design, and its robustness against imprecise input data.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {75–84},
numpages = {10},
keywords = {software product lines, reliability prediction, fault tolerance, component-based software architectures},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@inproceedings{10.1145/2851553.2892038,
author = {Reichelt, David Georg and K\"{u}hne, Stefan},
title = {Empirical Analysis of Performance Problems on Code Level},
year = {2016},
isbn = {9781450340809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851553.2892038},
doi = {10.1145/2851553.2892038},
abstract = {Performance problems are well known on architecture level. On code level their occurrences have not been systematically researched so far. Since a lot of everyday work of software developers is done on code level, methods and tools with focus on frequent performance problems are relevant. In the presented thesis, a method for systematically evaluating the occurrence and the frequency of performance problems on code level is presented and applied to repositories. The results of this empirical research will be a classification of performance problems and a quantification of their frequency. This will raise the awareness on certain problem classes for developers and will provide a basis for the development of new performance tools for preventing performance problems.},
booktitle = {Proceedings of the 7th ACM/SPEC on International Conference on Performance Engineering},
pages = {117–120},
numpages = {4},
keywords = {performance engineering, performance analysis of software system versions, mining software repositories, change-based test selection},
location = {Delft, The Netherlands},
series = {ICPE '16}
}

@inproceedings{10.1109/ASE.2013.6693115,
author = {Garc\'{\i}a-Gal\'{a}n, Jes\'{u}s and Trinidad, Pablo and Ruiz-Cort\'{e}s, Antonio},
title = {Multi-user variability configuration: a game theoretic approach},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693115},
doi = {10.1109/ASE.2013.6693115},
abstract = {Multi-user configuration is a neglected problem in variability-intensive systems area. The appearance of conflicts among user configurations is a main concern. Current approaches focus on avoiding such conflicts, applying the mutual exclusion principle. However, this perspective has a negative impact on users satisfaction, who cannot make any decision fairly. In this work, we propose an interpretation of multi-user configuration as a game theoretic problem. Game theory is a well-known discipline which analyzes conflicts and cooperation among intelligent rational decision-makers. We present a taxonomy of multi-user configuration approaches, and how they can be interpreted as different problems of game theory. We focus on cooperative game theory to propose and automate a tradeoff-based bargaining approach, as a way to solve the conflicts and maximize user satisfaction at the same time.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {574–579},
numpages = {6},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@article{10.1145/3293534,
author = {Liando, Jansen C. and Gamage, Amalinda and Tengourtius, Agustinus W. and Li, Mo},
title = {Known and Unknown Facts of LoRa: Experiences from a Large-scale Measurement Study},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1550-4859},
url = {https://doi.org/10.1145/3293534},
doi = {10.1145/3293534},
abstract = {Long Range (LoRa) is a Low-power Wide-area Network technology designed for the Internet of Things. In recent years, it has gained significant momentum among industrial and research communities. Patented by Semtech, LoRa makes use of chirp spread spectrum modulation to deliver data with promises of long battery life, far-reaching communication distances, and a high node density at the cost of data rate. In this article, we conduct a series of experiments to verify the claims made by Semtech on LoRa technology. Our results show that LoRa is capable of communicating over 10km under line-of-sight environments. However, under non-line-of-sight environments, LoRa’s performance is severely affected by obstructions such as buildings and vegetations. Moreover, the promise of prolonged battery life requires extreme tuning of parameters. Last, a LoRa gateway supports up to 6,000 nodes with PRR requirement of &gt;70%. This study also explores the relationship between LoRa transmission parameters and proposes an algorithm to determine optimal settings in terms of coverage and power consumption under non-line-of-sight environments. It further investigates the impact of LoRa Wide-area Networks on energy consumption and network capacity along with implementation of a LoRa medium access mechanism and possible gains brought forth by implementing such a mechanism.},
journal = {ACM Trans. Sen. Netw.},
month = feb,
articleno = {16},
numpages = {35},
keywords = {wide-area networks, sensor networks, network performance analysis, low-power wide-area network, internet of things, Network measurement, LoRa}
}

@inproceedings{10.1145/3622748.3622750,
author = {Arasaki, Caio and Wolschick, Lucas and Freire, Willian and Amaral, Aline},
title = {Feature selection in an interactive search-based PLA design approach},
year = {2023},
isbn = {9798400709524},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622748.3622750},
doi = {10.1145/3622748.3622750},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line (SPL). PLA design can be formulated as an interactive optimization problem with many conflicting factors. Incorporating Decision Makers’ (DM) preferences during the search process may help the algorithms find more adequate solutions for their profiles. Interactive approaches allow the DM to evaluate solutions, guiding the optimization according to their preferences. However, this brings up human fatigue problems caused by the excessive amount of interactions and solutions to evaluate. A common strategy to prevent this problem is limiting the number of interactions and solutions evaluated by the DM. Machine Learning (ML) models were also used to learn how to evaluate solutions according to the DM profile and replace them after some interactions. Feature selection performs an essential task as non-relevant and/or redundant features used to train the ML model can reduce the accuracy and comprehensibility of the hypotheses induced by ML algorithms. This work aims to select features of an ML model used to prevent human fatigue in an interactive search-based PLA design approach. We applied four selectors and through results we were able to reduce 30% of features, obtaining an accuracy of 99%.},
booktitle = {Proceedings of the 17th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Machine Learning, Interactive search-based Software Engineering, Feature Selection},
location = {Campo Grande, Brazil},
series = {SBCARS '23}
}

@inproceedings{10.1145/3387514.3405900,
author = {Steffen, Samuel and Gehr, Timon and Tsankov, Petar and Vanbever, Laurent and Vechev, Martin},
title = {Probabilistic Verification of Network Configurations},
year = {2020},
isbn = {9781450379557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387514.3405900},
doi = {10.1145/3387514.3405900},
abstract = {Not all important network properties need to be enforced all the time. Often, what matters instead is the fraction of time / probability these properties hold. Computing the probability of a property in a network relying on complex inter-dependent routing protocols is challenging and requires determining all failure scenarios for which the property is violated. Doing so at scale and accurately goes beyond the capabilities of current network analyzers.In this paper, we introduce NetDice, the first scalable and accurate probabilistic network configuration analyzer supporting BGP, OSPF, ECMP, and static routes. Our key contribution is an inference algorithm to efficiently explore the space of failure scenarios. More specifically, given a network configuration and a property φ, our algorithm automatically identifies a set of links whose failure is provably guaranteed not to change whether φ holds. By pruning these failure scenarios, NetDice manages to accurately approximate P(φ). NetDice supports practical properties and expressive failure models including correlated link failures.We implement NetDice and evaluate it on realistic configurations. NetDice is practical: it can precisely verify probabilistic properties in few minutes, even in large networks.},
booktitle = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {750–764},
numpages = {15},
keywords = {Probabilistic inference, Network analysis, Failures, Cold edges},
location = {Virtual Event, USA},
series = {SIGCOMM '20}
}

@inproceedings{10.1145/3350768.3350774,
author = {Souza, Iuri Santos and Machado, Ivan and Seaman, Carolyn and Gomes, Gecynalda and Chavez, Christina and de Almeida, Eduardo Santana and Masiero, Paulo},
title = {Investigating Variability-aware Smells in SPLs: An Exploratory Study},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3350774},
doi = {10.1145/3350768.3350774},
abstract = {Variability-aware smell is a concept referring to artifact shortcomings in the context of highly-configurable systems that can degrade aspects such as program comprehension, maintainability, and evolvability. To the best of our knowledge, there is very little evidence that variability-aware smells exist in Software Product Lines (SPLs). This work presents an exploratory study that investigated (I) evidence that variability-aware smells exist in SPLs and (II) new types of variability-aware smell not yet documented in the literature based on a quantitative study with open source SPL projects. We collected quantitative data to generate reliable research evidence, by performing feature model and source code inspections on eleven open-source SPL projects. Our findings revealed that (1) instances of variability-aware smells exist in open-source SPL projects and (2) feature information presented significant associations with variability-aware smells. Furthermore, (3) the study presented six new types of variability-aware smells.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {367–376},
numpages = {10},
keywords = {Variability-Aware Smells, Software Product Lines, Exploratory Study, Empirical Study},
location = {Salvador, Brazil},
series = {SBES '19}
}

@article{10.1145/1183236.1183260,
author = {Sugumaran, Vijayan and Park, Sooyong and Kang, Kyo C.},
title = {Introduction},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183260},
doi = {10.1145/1183236.1183260},
journal = {Commun. ACM},
month = dec,
pages = {28–32},
numpages = {5}
}

@inproceedings{10.1145/2556624.2556637,
author = {Machado, Ivan do Carmo and Santos, Alcemir Rodrigues and Cavalcanti, Yguarat\~{a} Cerqueira and Trzan, Eduardo Gomes and de Souza, Marcio Magalh\~{a}es and de Almeida, Eduardo Santana},
title = {Low-level variability support for web-based software product lines},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556637},
doi = {10.1145/2556624.2556637},
abstract = {The Web systems domain has faced an increasing number of devices, browsers, and platforms to cope with, driving software systems to be more flexible to accomodate them. Software product line (SPL) engineering can be used as a strategy to implement systems capable of handling such a diversity. To this end, automated tool support is almost indispensable. However, current tool support gives more emphasis to modeling variability in the problem domain, over the support of variability at the solution domain. There is a need for mapping the variability between both abstraction levels, so as to determine what implementation impact a certain variability has. In this paper, we propose the FeatureJS, a FeatureIDE extension aiming at Javascript and HTML support for SPL engineering. The tool combines feature-oriented programming and preprocessors, as a strategy to map variability at source code with the variability modeled at a higher level of abstraction. We carried out a preliminary evaluation with an industrial project, aiming to characterize the capability of the tool to handle SPL engineering in the Web systems domain.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {15},
numpages = {8},
keywords = {web systems domain, software product line engineering, feature oriented software development, feature composition, FeatureIDE, Eclipse plugin},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@inproceedings{10.1145/2556624.2556628,
author = {Lengauer, Philipp and Bitto, Verena and Angerer, Florian and Gr\"{u}nbacher, Paul and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {Where has all my memory gone? determining memory characteristics of product variants using virtual-machine-level monitoring},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556628},
doi = {10.1145/2556624.2556628},
abstract = {Non-functional properties such as memory footprint have recently gained importance in software product line research. However, determining the memory characteristics of individual features and product variants is extremely challenging. We present an approach that supports the monitoring of memory characteristics of individual features at the level of Java virtual machines. Our approach provides extensions to Java virtual machines to track memory allocations and deal-locations of individual features based on a feature-to-code mapping. The approach enables continuous monitoring at the level of features to detect anomalies such as memory leaks, excessive memory consumption, or abnormal garbage collection times in product variants. We provide an evaluation of our approach based on different product variants of the DesktopSearcher product line. Our experiment with different program inputs demonstrates the feasibility of our technique.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {13},
numpages = {8},
keywords = {monitoring, memory footprint, feature-oriented software development, Java},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@article{10.1145/3637228,
author = {Ca\~{n}izares, Pablo C. and L\'{o}pez-Morales, Jose Mar\'{\i}a and P\'{e}rez-Soler, Sara and Guerra, Esther and de Lara, Juan},
title = {Measuring and Clustering Heterogeneous Chatbot Designs},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3637228},
doi = {10.1145/3637228},
abstract = {Conversational agents, or chatbots, have become popular to access all kind of software services. They provide an intuitive natural language interface for interaction, available from a wide range of channels including social networks, web pages, intelligent speakers or cars. In response to this demand, many chatbot development platforms and tools have emerged. However, they typically lack support to statically measure properties of the chatbots being built, as indicators of their size, complexity, quality or usability. Similarly, there are hardly any mechanisms to compare and cluster chatbots developed with heterogeneous technologies.To overcome this limitation, we propose a suite of 21 metrics for chatbot designs, as well as two clustering methods that help in grouping chatbots along their conversation topics and design features. Both the metrics and the clustering methods are defined on a neutral chatbot design language, becoming independent of the implementation platform. We provide automatic translations of chatbots defined on some major platforms into this neutral notation to perform the measurement and clustering. The approach is supported by our tool Asymob, which we have used to evaluate the metrics and the clustering methods over a set of 259 Dialogflow and Rasa chatbots from open-source repositories. The results open the door to incorporating the metrics within chatbot development processes for the early detection of quality issues, and to exploit clustering to organise large collections of chatbots into significant groups to ease chatbot comprehension, search and comparison.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {90},
numpages = {43},
keywords = {Chatbot design, metrics, clustering, quality assurance, model-driven engineering}
}

@inproceedings{10.1145/3442391.3442411,
author = {Fischer, Stefan and Ramler, Rudolf and Klammer, Claus and Rabiser, Rick},
title = {Testing of Highly Configurable Cyber-Physical Systems – A Multiple Case Study},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442411},
doi = {10.1145/3442391.3442411},
abstract = {Cyber-physical systems, i.e., systems that seamlessly integrate computation and physical components, are typically highly-configurable systems. Testing such systems is particularly challenging because they comprise a large number of heterogeneous components that can be configured and combined in different ways. Despite a plethora of work investigating software testing in general and software product line testing in particular, variability in tests and how industry does actually manage testing highly configurable cyber-physical systems is not well understood. In this paper, we report the results of a multiple case study we conducted with three companies developing and maintaining highly-configurable cyber-physical systems focusing on their testing practices, with a particular focus on how they manage variability in tests. We conclude that experienced-based selection of configurations for testing is currently predominant. Variability modeling techniques are not utilized and the dependencies between configuration options are only partially modeled at best. However, the companies are aware of the situation and have the need and desire to cover more configuration combinations by automated tests. This in turn raises many questions, which might also be of interest to the scientific community and motivate future research.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {19},
numpages = {10},
keywords = {variability testing, interview, industry case, configuration testing},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.1145/3634713.3634725,
author = {G\"{u}thing, Lukas and Bittner, Paul Maximilian and Schaefer, Ina and Th\"{u}m, Thomas},
title = {Explaining Edits to Variability Annotations in Evolving Software Product Lines},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634725},
doi = {10.1145/3634713.3634725},
abstract = {Software is subject to changes and revisions during its development life cycle. For configurable software systems, changes may be made to functionality of source code as well as variability information such as code-to-feature mappings. To explain how code-to-feature mappings change in edits made to configurable software, we relate the mappings before and after an edit in terms of the sets of variants they denote. We prove our explanations to be complete and unambiguous, meaning that every pair of code-to-feature mappings is explained in terms of exactly one relation. Based on a graph formalism, we provide an algorithm for fast detection of relations during commits to version control. In an initial study, we detect relations between feature annotations in 42 real-world software product-line repositories to better understand typical changes in the evolution of configurable software. We demonstrate that our formalism can be automated and that analyzing a commit requires only 135 ms on average.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {93–102},
numpages = {10},
keywords = {software evolution, software product lines, software variability},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.1145/2884781.2884823,
author = {Schr\"{o}ter, Reimar and Krieter, Sebastian and Th\"{u}m, Thomas and Benduhn, Fabian and Saake, Gunter},
title = {Feature-model interfaces: the highway to compositional analyses of highly-configurable systems},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884823},
doi = {10.1145/2884781.2884823},
abstract = {Today's software systems are often customizable by means of load-time or compile-time configuration options. These options are typically not independent and their dependencies can be specified by means of feature models. As many industrial systems contain thousands of options, the maintenance and utilization of feature models is a challenge for all stakeholders. In the last two decades, numerous approaches have been presented to support stakeholders in analyzing feature models. Such analyses are commonly reduced to satisfiability problems, which suffer from the growing number of options. While first attempts have been made to decompose feature models into smaller parts, they still require to compose all parts for analysis. We propose the concept of a feature-model interface that only consists of a subset of features, typically selected by experts, and hides all other features and dependencies. Based on a formalization of feature-model interfaces, we prove compositionality properties. We evaluate feature-model interfaces using a three-month history of an industrial feature model from the automotive domain with 18,616 features. Our results indicate performance benefits especially under evolution as often only parts of the feature model need to be analyzed again.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {667–678},
numpages = {12},
keywords = {variability modeling, software product line, modularity, feature model, configurable software, compositionality},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/2110147.2110166,
author = {M\ae{}rsk-M\o{}ller, Hans Martin and J\o{}rgensen, Bo N\o{}rregaard},
title = {Cardinality-dependent variability in orthogonal variability models},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110166},
doi = {10.1145/2110147.2110166},
abstract = {During our work on developing and running a software product line for eco-sustainable greenhouse-production software tools, which currently have three products members we have identified a need for extending the notation of the Orthogonal Variability Model (OVM) to support what we refer to as cardinality range dependencies. The cardinality-range-dependency type enables expressing that the binding of a certain number of variants to a variation point can influence variability in other places in the model. In other words, we acknowledge that variability can be influenced, not necessarily by the specific variants being bound, but by their sheer numbers.This paper contributes with an extension to the meta-model underlying the OVM notation, suggesting a notation for the new type of dependency and shows its applicability. The specific case, which initially required this extension, will work as running example throughout the paper and underline the need for the extension. Finally, the paper evaluates and discusses the general applicability of the proposed notation extension and future perspectives.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {165–172},
numpages = {8},
keywords = {variability modeling language, software product line engineering, orthogonal variability model (OVM), documentation},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@inproceedings{10.1145/3634713.3634717,
author = {May, Richard and Denecke, Kerstin},
title = {Conversational Agents in Healthcare: A Variability Perspective},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634717},
doi = {10.1145/3634713.3634717},
abstract = {Conversational agents in healthcare are gaining popularity, for example, in the context of eliciting medical histories. Furthermore, due to the growing diversity of use cases and stakeholders, they are becoming increasingly configurable and are often based on variability mechanisms. In this paper, we present a high-level perspective on typical variability aspects and describe common challenges based on our research and practical experience in developing and evaluating conversational agents in the healthcare domain. We introduce variability aspects that are classified into technology-related (e.g., intelligence framework, input/output mode) and user-related aspects (e.g., careflow integration, health literacy). Moreover, these aspects are described in a case study on the Digital Medical Interview Assistant (DMIA) for radiology. We highlight main challenges that arise in the context of evolution, verification, input processing, privacy and security compliance, as well as ethical considerations. Our findings are intended to help developers, researchers, and healthcare professionals understand the importance and impact of configurability and to spur further discussions on variability aspects of conversational agents.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {123–128},
numpages = {6},
keywords = {chatbot, configuration, conversational agent, healthcare, variability},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.1145/3510466.3511272,
author = {Kuiter, Elias and Kn\"{u}ppel, Alexander and Bordis, Tabea and Runge, Tobias and Schaefer, Ina},
title = {Verification Strategies for Feature-Oriented Software Product Lines},
year = {2022},
isbn = {9781450396042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510466.3511272},
doi = {10.1145/3510466.3511272},
abstract = {Highly-customizable software systems in form of software product lines are becoming increasingly relevant for safety-critical systems, in which the correctness of software is a major concern. To ensure the correct behavior of a software product line, each product can be verified in isolation—however, this strategy quickly becomes infeasible for a large number of products. In this paper, we propose proof plans, a novel strategy for verifying feature-oriented software product lines based on partial proofs. Our technique splits the verification task into small proofs that can be reused across method variants, which gives rise to a wider spectrum of verification strategies for software product lines. We describe applications of our technique and evaluate one of them on a case study by comparing it with established verification strategies.},
booktitle = {Proceedings of the 16th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {12},
numpages = {9},
keywords = {Software Product Lines, Proof Reuse, Deductive Verification},
location = {Florence, Italy},
series = {VaMoS '22}
}

@inproceedings{10.1145/2786805.2786845,
author = {Siegmund, Norbert and Grebhahn, Alexander and Apel, Sven and K\"{a}stner, Christian},
title = {Performance-influence models for highly configurable systems},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786845},
doi = {10.1145/2786805.2786845},
abstract = {Almost every complex software system today is configurable. While configurability has many benefits, it challenges performance prediction, optimization, and debugging. Often, the influences of individual configuration options on performance are unknown. Worse, configuration options may interact, giving rise to a configuration space of possibly exponential size. Addressing this challenge, we propose an approach that derives a performance-influence model for a given configurable system, describing all relevant influences of configuration options and their interactions. Our approach combines machine-learning and sampling heuristics in a novel way. It improves over standard techniques in that it (1) represents influences of options and their interactions explicitly (which eases debugging), (2) smoothly integrates binary and numeric configuration options for the first time, (3) incorporates domain knowledge, if available (which eases learning and increases accuracy), (4) considers complex constraints among options, and (5) systematically reduces the solution space to a tractable size. A series of experiments demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {284–294},
numpages = {11},
keywords = {sampling, machine learning, Performance-influence models},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/2647508.2647512,
author = {Koscielny, Jonathan and Holthusen, S\"{o}nke and Schaefer, Ina and Schulze, Sandro and Bettini, Lorenzo and Damiani, Ferruccio},
title = {DeltaJ 1.5: delta-oriented programming for Java 1.5},
year = {2014},
isbn = {9781450329262},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647508.2647512},
doi = {10.1145/2647508.2647512},
abstract = {Delta-oriented programming (DOP) is a modular, yet flexible approach to implement software product lines. In DOP, a product line is implemented by a set of deltas, which are containers of modifications to a program. A delta-oriented product line is specified by its code base, i.e., the set of delta modules, and a product line declaration specifying the set of possible product variants. In this paper, we present DOP for Java 1.5 extending previous proof-of-concept realizations of DOP for simple core Java-like languages. The novel prototypical implementation DeltaJ 1.5 provides full integrated access to the object-oriented features of Java. The extensions include delta operations to fully integrate the Java package system, to declare and modify interfaces, to explicitly change the inheritance hierarchy, to access nested types and enum types, to alter field declarations, and to unambiguously remove overloaded methods. Furthermore, we improve the specification of the product line declaration by providing a separate language. We have evaluated DeltaJ 1.5 using a case study.},
booktitle = {Proceedings of the 2014 International Conference on Principles and Practices of Programming on the Java Platform: Virtual Machines, Languages, and Tools},
pages = {63–74},
numpages = {12},
keywords = {software product line, program generation, delta-oriented programming},
location = {Cracow, Poland},
series = {PPPJ '14}
}

@inproceedings{10.1145/3427228.3427281,
author = {Zhang, Linghan and Tan, Sheng and Wang, Zi and Ren, Yili and Wang, Zhi and Yang, Jie},
title = {VibLive: A Continuous Liveness Detection for Secure Voice User Interface in IoT Environment},
year = {2020},
isbn = {9781450388580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427228.3427281},
doi = {10.1145/3427228.3427281},
abstract = {The voice user interface (VUI) has been progressively used to authenticate users to numerous devices and applications. Such massive adoption of VUIs in IoT environments like individual homes and businesses arises extensive privacy and security concerns. Latest VUIs adopting traditional voice authentication methods are vulnerable to spoofing attacks, where a malicious party spoofs the VUIs with pre-recorded or synthesized voice commands of the genuine user. In this paper, we design VibLive, a continuous liveness detection system for secure VUIs in IoT environments. The underlying principle of VibLive is to catch the dissimilarities between bone-conducted vibrations and air-conducted voices when human speaks for liveness detection. VibLive is a text-independent system that verifies live users and detects spoofing attacks without requiring users to enroll specific passphrases. Moreover, VibLive is practical and transparent as it requires neither additional operations nor extra hardwares, other than a loudspeaker and a microphone that are commonly equipped on VUIs. Our evaluation with 25 participants under different IoT intended experiment settings shows that VibLive is highly effective with over 97% detection accuracy. Results also show that VibLive is robust to various use scenarios.},
booktitle = {Proceedings of the 36th Annual Computer Security Applications Conference},
pages = {884–896},
numpages = {13},
keywords = {voice user interface, liveness detection, bone-conducted vibrations},
location = {Austin, USA},
series = {ACSAC '20}
}

@inproceedings{10.1145/2462326.2462332,
author = {Quinton, Cl\'{e}ment and Haderer, Nicolas and Rouvoy, Romain and Duchien, Laurence},
title = {Towards multi-cloud configurations using feature models and ontologies},
year = {2013},
isbn = {9781450320504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2462326.2462332},
doi = {10.1145/2462326.2462332},
abstract = {Configuration and customization choices arise due to the heterogeneous and scalable aspect of the cloud computing paradigm. To avoid being restricted to a given cloud and ensure application requirements, using several clouds to deploy a multi-cloud configuration is recommended but introduces several challenges due to the amount of providers and their intrinsic variability. In this paper, we present a model-driven approach based on Feature Models (FMs) originating from Software Product Lines (SPL) to handle cloud variability and then manage and create cloud configurations. We combine it with ontologies, used to model the various semantics of cloud systems. The approach takes into consideration application technical requirements as well as non-functional ones to provide a set of valid cloud or multi-cloud configurations and is implemented in a framework named SALOON.},
booktitle = {Proceedings of the 2013 International Workshop on Multi-Cloud Applications and Federated Clouds},
pages = {21–26},
numpages = {6},
keywords = {variability, ontology, multi-cloud, feature model, cloud computing},
location = {Prague, Czech Republic},
series = {MultiCloud '13}
}

@inproceedings{10.1145/3674805.3686694,
author = {Heldal, Rogardt},
title = {Is generalisation hindering the adoption of your findings?},
year = {2024},
isbn = {9798400710476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674805.3686694},
doi = {10.1145/3674805.3686694},
abstract = {Background: This paper documents an unsuccessful attempt to develop a comprehensive artefact to mitigate software product variability in complex organisational contexts. The narrative falls into two parts: the endeavour to create the artefact; and the subsequent retrospective analysis, designed to understand the reasons for its lack of adoption in practical settings. The primary emphasis here is on the retrospective analysis, which offers the most valuable insights and lessons learned. Methodology: The study encompassed five large international industrial organisations, all with products featuring embedded software. The process consisted of two key steps: knowledge acquisition from focus group interviews with representatives of the organisations; and leveraging that knowledge to pursue the development of a general artefact for mitigating software product variability in the organisations. This approach aligns with the design science paradigm. Results: Our results yield two key findings. First, the challenges of generalisation, particularly the risk of creating overly abstract solutions. Second, the complexities of industry adopting research findings. The two issues are closely intertwined, as the increased involvement of industry is essential for the development of concrete solutions. However, our solution was too abstract to be useful for the five organisations in question, and there was no interest in concretising it. Our study also critiques excessive reliance on empirical research and the assumption it will inevitably yield positive outcomes. Conclusions: If the aim of software engineering research is not only to formulate abstract solutions and identify pathways for progress, but also to co-create the future with practitioners, a new mindset is needed. In the past 15 years, we have collaborated with over 50 industrial organisations, including the healthcare sector and local government, which has invariably resulted in the successful creation of new knowledge. However, collaborations have often remained a partnership between industry and academia, with academia bearing the brunt of the workload, and, in many cases, risking solutions that are overly abstract and not properly validated.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {348–358},
numpages = {11},
keywords = {abstract solution, adoption, industry commitment, research methods, software variability},
location = {Barcelona, Spain},
series = {ESEM '24}
}

@article{10.1007/s00165-017-0432-4,
author = {Chrszon, Philipp and Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha and Baier, Christel},
title = {ProFeat: feature-oriented engineering for family-based probabilistic model checking},
year = {2018},
issue_date = {Jan 2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {1},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-017-0432-4},
doi = {10.1007/s00165-017-0432-4},
abstract = {The concept of features provides an elegant way to specify families of systems. Given a base system, features encapsulate additional functionalities that can be activated or deactivated to enhance or restrict the base system’s behaviors. Features can also facilitate the analysis of families of systems by exploiting commonalities of the family members and performing an all-in-one analysis, where all systems of the family are analyzed at once on a single family model instead of one-by-one. Most prominent, the concept of features has been successfully applied to describe and analyze (software) product lines. We present the tool ProFeat that supports the feature-oriented engineering process for stochastic systems by probabilistic model checking. To describe families of stochastic systems, ProFeat extends models for the prominent probabilistic model checker Prism by feature-oriented concepts, including support for probabilistic product lines with dynamic feature switches, multi-features and feature attributes. ProFeat provides a compact symbolic representation of the analysis results for each family member obtained by Prism to support, e.g., model repair or refinement during feature-oriented development. By means of several case studies we show how ProFeat eases family-based quantitative analysis and compare one-by-one and all-in-one analysis approaches.},
journal = {Form. Asp. Comput.},
month = jan,
pages = {45–75},
numpages = {31},
keywords = {Software product line analysis, Probabilistic model checking, Feature-oriented systems}
}

@inproceedings{10.1145/2745802.2745806,
author = {Santos, Alcemir Rodrigues and de Oliveira, Raphael Pereira and de Almeida, Eduardo Santana},
title = {Strategies for consistency checking on software product lines: a mapping study},
year = {2015},
isbn = {9781450333504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2745802.2745806},
doi = {10.1145/2745802.2745806},
abstract = {Context. Software Product Lines (SPL) has become one of the most prominents way to promote the systematic reuse of software artifacts. Like any other piece of software, with the SPL aging, it becomes necessary to manage their evolution. However, in this process, engineers might introduce divergences among the SPL artifacts. Thus, a number of initiatives address the management of such inconsistencies. Objective. In this paper, we mapped the existing approaches to inconsistency management within SPL. Method. We used the systematic mapping study methodology. Results. We classified and performed a characterization of the approaches found, which we mangaged to arrange in three main categories. Most papers selected proposed new methods as solution research. Besides, there is still a need for validation and evaluation studies. Conclusion. We identified a lack of support for a number of activities of consistency assurance. For instance, no paper addressed the tracking of findings, decisions, and actions, as well as, few papers describing either the handling or a management policy for identified inconsistencies.},
booktitle = {Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {5},
numpages = {14},
keywords = {software product line engineering, mapping study, literature review, consistency checking},
location = {Nanjing, China},
series = {EASE '15}
}

@article{10.1145/2000799.2000803,
author = {Dehlinger, Josh and Lutz, Robyn R.},
title = {Gaia-PL: A Product Line Engineering Approach for Efficiently Designing Multiagent Systems},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/2000799.2000803},
doi = {10.1145/2000799.2000803},
abstract = {Agent-oriented software engineering (AOSE) has provided powerful and natural, high-level abstractions in which software developers can understand, model and develop complex, distributed systems. Yet, the realization of AOSE partially depends on whether agent-based software systems can achieve reductions in development time and cost similar to other reuse-conscious development methods. Specifically, AOSE does not adequately address requirements specifications as reusable assets. Software product line engineering is a reuse technology that supports the systematic development of a set of similar software systems through understanding, controlling, and managing their common, core characteristics and their differing variation points. In this article, we present an extension to the Gaia AOSE methodology, named Gaia-PL (Gaia-Product Line), for agent-based distributed software systems that enables requirements specifications to be easily reused. We show how our methodology uses a product line perspective to promote reuse in agent-based software systems early in the development life cycle so that software assets can be reused throughout system development and evolution. We also present results from an application to show how Gaia-PL provided reuse that reduced the design and development effort for a large, multiagent system.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {17},
numpages = {27},
keywords = {software product line engineering, Agent-oriented software engineering}
}

@inproceedings{10.1145/1385486.1385488,
author = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Schirmeier, Horst and Sincero, Julio and Apel, Sven and Leich, Thomas and Spinczyk, Olaf and Saake, Gunter},
title = {FAME-DBMS: tailor-made data management solutions for embedded systems},
year = {2008},
isbn = {9781595939647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1385486.1385488},
doi = {10.1145/1385486.1385488},
abstract = {Data management functionality is not only needed in large-scale server systems, but also in embedded systems. Resource restrictions and heterogeneity of hardware, however, complicate the development of data management solutions for those systems. In current practice, this typically leads to the redevelopment of data management because existing solutions cannot be reused and adapted appropriately. In this paper, we present our ongoing work on FAME-DBMS, a research project that explores techniques to implement highly customizable data management solutions, and illustrate how such systems can be created with a software product line approach. With this approach a concrete instance of a DBMS is derived by composing features of the DBMS product line that are needed for a specific application scenario. This product derivation process is getting complex if a large number of features is available. Furthermore, in embedded systems also non-functional properties, e.g., memory consumption, have to be considered when creating a DBMS instance. To simplify the derivation process we present approaches for its automation.},
booktitle = {Proceedings of the 2008 EDBT Workshop on Software Engineering for Tailor-Made Data Management},
pages = {1–6},
numpages = {6},
location = {Nantes, France},
series = {SETMDM '08}
}

@inproceedings{10.1145/2451617.2451619,
author = {Kowal, Matthias and Schulze, Sandro and Schaefer, Ina},
title = {Towards efficient SPL testing by variant reduction},
year = {2013},
isbn = {9781450318679},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2451617.2451619},
doi = {10.1145/2451617.2451619},
abstract = {Testing software systems plays a pivotal role for quality, reliability, and safety of such systems. Several approaches exist that provide efficient algorithms to test one software system. However, in the context of variable software systems, called software product lines (SPLs), testing has to deal with potentially thousands of variants. Unfortunately, current approaches do not scale to this problem and thus testing SPLs efficiently is a challenging task. In this paper, we propose an approach to reduce the test set by explicitly modeling information about shared resources and communication in feature models. As a result, we can figure out features that interact with each other and thus are more likely to cause problems. We show with a small case study that our approach reduces both, the features under test as well as the time for computing all feature combinations to be tested.},
booktitle = {Proceedings of the 4th International Workshop on Variability &amp; Composition},
pages = {1–6},
numpages = {6},
keywords = {testing, software product lines, feature models},
location = {Fukuoka, Japan},
series = {VariComp '13}
}

@inproceedings{10.1109/ICSE.2019.00113,
author = {Ha, Huong and Zhang, Hongyu},
title = {DeepPerf: performance prediction for configurable software with deep sparse neural network},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00113},
doi = {10.1109/ICSE.2019.00113},
abstract = {Many software systems provide users with a set of configuration options and different configurations may lead to different runtime performance of the system. As the combination of configurations could be exponential, it is difficult to exhaustively deploy and measure system performance under all possible configurations. Recently, several learning methods have been proposed to build a performance prediction model based on performance data collected from a small sample of configurations, and then use the model to predict system performance under a new configuration. In this paper, we propose a novel approach to model highly configurable software system using a deep feedforward neural network (FNN) combined with a sparsity regularization technique, e.g. the L1 regularization. Besides, we also design a practical search strategy for automatically tuning the network hyperparameters efficiently. Our method, called DeepPerf, can predict performance values of highly configurable software systems with binary and/or numeric configuration options at much higher prediction accuracy with less training data than the state-of-the art approaches. Experimental results on eleven public real-world datasets confirm the effectiveness of our approach.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1095–1106},
numpages = {12},
keywords = {sparsity regularization, software performance prediction, highly configurable systems, deep sparse feedforward neural network},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/568760.568805,
author = {Johansson, Enrico and H\"{o}st, Martin},
title = {Tracking degradation in software product lines through measurement of design rule violations},
year = {2002},
isbn = {1581135564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/568760.568805},
doi = {10.1145/568760.568805},
abstract = {In order to increase reuse, a number of product versions may be developed based on the same software platform. The platform must, however, be managed and updated according to new requirements if it should be reusable in a series of releases. This means that the platform is constantly changed during its lifecycle, and changes can result in degradation of the platform. In this paper, a measurement approach is proposed as a means of tracking the degradation of a software platform and consequently in the product line. The tracking approach is evaluated in a case study where it is applied to a series of different releases of a product. The result of the case study indicates that the presented approach is feasible.},
booktitle = {Proceedings of the 14th International Conference on Software Engineering and Knowledge Engineering},
pages = {249–254},
numpages = {6},
keywords = {software product line, software platform, project tracking, graph, design rules, degradation},
location = {Ischia, Italy},
series = {SEKE '02}
}

@inproceedings{10.5555/2818754.2818819,
author = {Henard, Christopher and Papadakis, Mike and Harman, Mark and Le Traon, Yves},
title = {Combining multi-objective search and constraint solving for configuring large software product lines},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Software Product Line (SPL) feature selection involves the optimization of multiple objectives in a large and highly constrained search space. We introduce SATIBEA, that augments multi-objective search-based optimization with constraint solving to address this problem, evaluating it on five large real-world SPLs, ranging from 1,244 to 6,888 features with respect to three different solution quality indicators and two diversity metrics. The results indicate that SATIBEA statistically significantly outperforms the current state-of-the-art (p &lt; 0.01) for all five SPLs on all three quality indicators and with maximal effect size (\^{A}12 = 1.0). We also present results that demonstrate the importance of combining constraint solving with search-based optimization and the significant improvement SATIBEA produces over pure constraint solving. Finally, we demonstrate the scalability of SATIBEA: within less than half an hour, it finds thousands of constraint-satisfying optimized software products, even for the largest SPL considered in the literature to date.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {517–528},
numpages = {12},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/3510003.3510200,
author = {Dubslaff, Clemens and Weis, Kallistos and Baier, Christel and Apel, Sven},
title = {Causality in configurable software systems},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510200},
doi = {10.1145/3510003.3510200},
abstract = {Detecting and understanding reasons for defects and inadvertent behavior in software is challenging due to their increasing complexity. In configurable software systems, the combinatorics that arises from the multitude of features a user might select from adds a further layer of complexity. We introduce the notion of feature causality, which is based on counterfactual reasoning and inspired by the seminal definition of actual causality by Halpern and Pearl. Feature causality operates at the level of system configurations and is capable of identifying features and their interactions that are the reason for emerging functional and non-functional properties. We present various methods to explicate these reasons, in particular well-established notions of responsibility and blame that we extend to the feature-oriented setting. Establishing a close connection of feature causality to prime implicants, we provide algorithms to effectively compute feature causes and causal explications. By means of an evaluation on a wide range of configurable software systems, including community benchmarks and real-world systems, we demonstrate the feasibility of our approach: We illustrate how our notion of causality facilitates to identify root causes, estimate the effects of features, and detect feature interactions.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {325–337},
numpages = {13},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/1404946.1404947,
author = {Yie, Andres and Casallas, Rubby and Deridder, Dirk and Van Der Straeten, Ragnhild},
title = {Multi-step concern refinement},
year = {2008},
isbn = {9781605581439},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1404946.1404947},
doi = {10.1145/1404946.1404947},
abstract = {A Model-Driven Software Product Line (MD-SPL) uses metamodels, models, and transformations to create a family of products using a Model Refinement Line (MRL). However, an MD-SPL must evolve and provide mechanisms to add new crosscutting concerns, such as security or logging, to the applications. Our problem is that we want to preserve and reuse the original MRL. In this paper, we present the challenges associated with this problem. We illustrate them by evaluating different model-driven approaches to add crosscutting concerns into an application using high-level aspects. Furthermore, we propose an approach to add crosscutting concerns as early aspects and to preserve the original MRL. This approach adds a new MRL that refines a high-level model of the concern. This high-level model is related with the high-level application model in the original MRL. The refinement of the application model and the concern model proceeds in parallel. The presented approach is a work in progress and requires us to tackle several challenges in order to implement and validate the proposal.},
booktitle = {Proceedings of the 2008 AOSD Workshop on Early Aspects},
articleno = {1},
numpages = {8},
keywords = {software product line, model-driven engineering, model refinement, early aspects, aspect-oriented software development},
location = {Brussels, Belgium},
series = {EA '08}
}

@inproceedings{10.1145/3425269.3425278,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Supporting user preferences in search-based product line architecture design using Machine Learning},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425278},
doi = {10.1145/3425269.3425278},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA design requires intensive human effort as it involves several conflicting factors. In order to support this task, an interactive search-based approach, automated by a tool named OPLA-Tool, was proposed in a previous work. Through this tool the software architect evaluates the generated solutions during the optimization process. Considering that evaluating PLA is a complex task and search-based algorithms demand a high number of generations, the evaluation of all solutions in all generations cause human fatigue. In this work, we incorporated in OPLA-Tool a Machine Learning (ML) model to represent the architect in some moments during the optimization process aiming to decrease the architect's effort. Through the execution of a quantiqualitative exploratory study it was possible to demonstrate the reduction of the fatigue problem and that the solutions produced at the end of the process, in most cases, met the architect's needs.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Product Line Architecture, Machine Learning, Human-computer interaction},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1145/3185768.3186406,
author = {Kub\'{a}t, Petr and Bulej, Lubom\'{\i}r and Bureundefined, Tom\'{a}undefined and Hork\'{y}, Vojtech and Tuma, Petr},
title = {Adaptive Dispatch: A Pattern for Performance-Aware Software Self-Adaptation},
year = {2018},
isbn = {9781450356299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185768.3186406},
doi = {10.1145/3185768.3186406},
abstract = {Modern software systems often employ dynamic adaptation to runtime conditions in some parts of their functionality -- well known examples range from autotuning of computing kernels through adaptive battery saving strategies of mobile applications to dynamic load balancing and failover functionality in computing clouds. Typically, the implementation of these features is problem-specific -- a particular autotuner, a particular load balancer, and so on -- and enjoys little support from the implementation environment beyond standard programming constructs. In this work, we propose Adaptive Dispatch as a generic coding pattern for implementing dynamic adaptation. We believe that such pattern can make the implementation of dynamic adaptation features better in multiple aspects -- an explicit adaptation construct makes the presence of adaptation easily visible to programmers, lends itself to manipulation with development tools, and facilitates coordination of adaptation behavior at runtime. We present an implementation of the Adaptive Dispatch pattern as an internal DSL in Scala.},
booktitle = {Companion of the 2018 ACM/SPEC International Conference on Performance Engineering},
pages = {195–198},
numpages = {4},
keywords = {performance awareness, performance adaptation, Scala},
location = {Berlin, Germany},
series = {ICPE '18}
}

@inproceedings{10.1145/2642803.2642807,
author = {Torjusen, Arild B. and Abie, Habtamu and Paintsil, Ebenezer and Trcek, Denis and Skomedal, \r{A}smund},
title = {Towards Run-Time Verification of Adaptive Security for IoT in eHealth},
year = {2014},
isbn = {9781450327787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642803.2642807},
doi = {10.1145/2642803.2642807},
abstract = {This paper integrates run-time verification enablers in the feedback adaptation loop of the ASSET adaptive security framework for Internet of Things (IoT) in the eHealth settings and instantiates the resulting framework with Colored Petri Nets. The run-time enablers make machine-readable formal models of a system state and context available at run-time. In addition, they make requirements that define the objectives of verification available at run-time as formal specifications and enable dynamic context monitoring and adaptation. Run-time adaptive behavior that deviates from the normal mode of operation of the system represents a major threat to the sustainability of critical eHealth services. Therefore, the integration of run-time enablers into the ASSET adaptive framework could lead to a sustainable security framework for IoT in eHealth.},
booktitle = {Proceedings of the 2014 European Conference on Software Architecture Workshops},
articleno = {4},
numpages = {8},
keywords = {eHealth, IoT, Formal Run-time Verification, Adaptive Security},
location = {Vienna, Austria},
series = {ECSAW '14}
}

@inproceedings{10.1145/3067695.3067726,
author = {Zincir-Heywood, Nur and Kayacik, Gunes},
title = {Evolutionary computation in network management and security: GECCO 2017 tutorial},
year = {2017},
isbn = {9781450349390},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3067695.3067726},
doi = {10.1145/3067695.3067726},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1094–1112},
numpages = {19},
location = {Berlin, Germany},
series = {GECCO '17}
}

@inproceedings{10.1145/3636534.3690663,
author = {Mollahosseini, Poorya and Afzal, Sayed Saad and Adib, Fadel and Ghasempour, Yasaman},
title = {SURF: Eavesdropping on Underwater Communications from the Air},
year = {2024},
isbn = {9798400704895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636534.3690663},
doi = {10.1145/3636534.3690663},
abstract = {This paper investigates how an airborne node can eavesdrop on the underwater acoustic communication between submerged nodes. Conventionally, such eavesdropping has been assumed impossible as acoustic signals do not cross the water-air boundary. Here, we demonstrate that underwater acoustic communications signals can be picked up and (under certain conditions) decoded using an airborne mmWave radar due to the minute vibrations induced by the communication signals on the water surface. We implemented and evaluated a proof-of-concept prototype of our method and tested it in controlled (pool) and uncontrolled environments (lake). Our results demonstrate that an airborne device can identify the modulation and bitrate of acoustic transmissions from an uncooperative underwater transmitter (victim), and even decode the transmitted symbols. Unlike conventional over-the-air communications, our results indicate that the secrecy of underwater links varies depending on the modulation type and provide insights into the underlying reasons behind these differences. We also highlight the theoretical limitations of such a threat model, and how these results may have a significant impact on the stealthiness of underwater communications, with particular concern to submarine warfare, underwater operations (e.g., oil &amp; gas, search &amp; rescue, mining), and conservation of endangered species. Finally, our investigation uncovers countermeasures that can be used to improve or restore the stealthiness of underwater acoustic communications against such threats.},
booktitle = {Proceedings of the 30th Annual International Conference on Mobile Computing and Networking},
pages = {815–829},
numpages = {15},
keywords = {wireless, cross-medium communications, security, subsea internet of things},
location = {Washington D.C., DC, USA},
series = {ACM MobiCom '24}
}

@inproceedings{10.1145/2039239.2039242,
author = {Tartler, Reinhard and Lohmann, Daniel and Dietrich, Christian and Egger, Christoph and Sincero, Julio},
title = {Configuration coverage in the analysis of large-scale system software},
year = {2011},
isbn = {9781450309790},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2039239.2039242},
doi = {10.1145/2039239.2039242},
abstract = {System software, especially operating systems, tends to be highly configurable. Like every complex piece of software, a considerable amount of bugs in the implementation has to be expected. In order to improve the general code quality, tools for static analysis provide means to check for source code defects without having to run actual test cases on real hardware. Still, for proper type checking a specific configuration is required so that all header include paths are available and all types are properly resolved.In order to find as many bugs as possible, usually a "full configuration" is used for the check. However, mainly because of alternative blocks in form of #else-blocks, a single configuration is insufficient to achieve full coverage. In this paper, we present a metric for configuration coverage (CC) and explain the challenges for (properly) calculating it. Furthermore, we present an efficient approach for determining a sufficiently small set of configurations that achieve (nearly) full coverage and evaluate it on a recent Linux kernel version.},
booktitle = {Proceedings of the 6th Workshop on Programming Languages and Operating Systems},
articleno = {2},
numpages = {5},
location = {Cascais, Portugal},
series = {PLOS '11}
}

@inproceedings{10.1145/3325291.3325388,
author = {Yamamoto, Masahiro and Nakashima, Shota and Haruyama, Kazuo and Mu, Shenglin},
title = {Improvement of Road Surface Discrimination Performance of Movement Support System Using Ultrasonic Sensors},
year = {2019},
isbn = {9781450371735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325291.3325388},
doi = {10.1145/3325291.3325388},
abstract = {The number of accidents involving elderly individuals has been increasing with the increase of the aging population, and it is a serious problem. Most of the accident are caused by decline in ability of judgment and physical. Therefor, movement supporting tool such as cane for elderly people is used to prevent falls and improve the safety. Individuals moving without the help of a caregiver face a high risk of accidents (such as falls), despite using movement supporting tool. There is a number of studies on support system for elderly people to improve the safety and quality of daily life. They are studies to detect the dangerous road such as a step and obstacle from distance information obtained by an ultrasonic sensor. On the other hand, the actual fall accident has occurred in the road surface, such as grass or gravel not only a step or obstacle. Therefore, there is a need for a system to detect and determine the danger road. We previously showed the effectiveness of the road surface discrimination method using information, the distance to the road surface, the incident angle and the reflection intensity of the ultrasonic sensor. However, it had not been studied the effectiveness of the method at the time of the move. This paper describes the results of the effectiveness of the method at the time of the move.},
booktitle = {Proceedings of the 7th ACIS International Conference on Applied Computing and Information Technology},
articleno = {33},
numpages = {6},
keywords = {Ultrasonic sensor, Road surface discrimination, Reflection intensities, Movement support system, Falling down accident, Elderly people},
location = {Honolulu, HI, USA},
series = {ACIT '19}
}

@inproceedings{10.1145/2993412.3007554,
author = {Ayala, Inmaculada and Gallina, Barbara},
title = {Towards tool-based security-informed safety oriented process line engineering},
year = {2016},
isbn = {9781450347815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993412.3007554},
doi = {10.1145/2993412.3007554},
abstract = {For the purpose of certification, manufactures of nowadays highly connected safety-critical systems are expected to engineer their systems according to well-defined engineering processes in compliance with safety and security standards. Certification is an extremely expensive and time-consuming process. Since safety and security standards exhibit a certain degree of commonality, certification-related artifacts (e.g., process models) should to some extent be reusable. To enable systematic reuse and customization of process information, in this paper we further develop security-informed safety-oriented process line engineering (i.e., engineering of sets of processes including security and safety concerns). More specifically, first we consider three tool-supported approaches for process-related commonality and variability management and we apply them to limited but meaningful portions of safety and security standards within airworthiness. Then, we discuss our findings. Finally, we draw our conclusions and sketch future work.},
booktitle = {Proccedings of the 10th European Conference on Software Architecture Workshops},
articleno = {38},
numpages = {7},
keywords = {tool-supported process customization, security-informed safety-oriented process lines, security-informed safety},
location = {Copenhagen, Denmark},
series = {ECSAW '16}
}

@article{10.1145/2094091.2094095,
author = {Tartler, Reinhard and Lohmann, Daniel and Dietrich, Christian and Egger, Christoph and Sincero, Julio},
title = {Configuration coverage in the analysis of large-scale system software},
year = {2012},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {3},
issn = {0163-5980},
url = {https://doi.org/10.1145/2094091.2094095},
doi = {10.1145/2094091.2094095},
abstract = {System software, especially operating systems, tends to be highly configurable. Like every complex piece of software, a considerable amount of bugs in the implementation has to be expected. In order to improve the general code quality, tools for static analysis provide means to check for source code defects without having to run actual test cases on real hardware. Still, for proper type checking a specific configuration is required so that all header include paths are available and all types are properly resolved.In order to find as many bugs as possible, usually a "full configuration" is used for the check. However, mainly because of alternative blocks in form of #else-blocks, a single configuration is insufficient to achieve full coverage. In this paper, we present a metric for configuration coverage (CC) and explain the challenges for (properly) calculating it. Furthermore, we present an efficient approach for determining a sufficiently small set of configurations that achieve (nearly) full coverage and evaluate it on a recent Linux kernel version.},
journal = {SIGOPS Oper. Syst. Rev.},
month = jan,
pages = {10–14},
numpages = {5}
}

@inproceedings{10.1145/3344948.3344964,
author = {Kerdoudi, Mohamed Lamine and Ziadi, Tewfik and Tibermacine, Chouki and Sadou, Salah},
title = {A bottom-up approach for reconstructing software architecture product lines},
year = {2019},
isbn = {9781450371421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3344948.3344964},
doi = {10.1145/3344948.3344964},
abstract = {A large component and service-based software system exists in different forms, as different variants targeting different business needs and users. This kind of systems is provided as a set of "independent" products and not as a "single whole". The presence of a single model describing the architecture of the whole system may be of great interest for developers of future variants. Indeed, this enables them to see the invariant part of the whole, on top of which new functionality can be built, in addition to the different options they can use. We investigate in this work the use of software product line reverse engineering approaches, and in particular the framework named BUT4Reuse, for reconstructing an architecture model of a Software Architecture Product Line (SAPL), from a set of variants. We propose a generic process for reconstructing an architecture model of such a product line. We have instantiated this process for the OSGi Java framework and experimented it for building the architecture model of Eclipse IDE SPL.},
booktitle = {Proceedings of the 13th European Conference on Software Architecture - Volume 2},
pages = {46–49},
numpages = {4},
location = {Paris, France},
series = {ECSA '19}
}

@inproceedings{10.1145/1985484.1985494,
author = {Gurgel, Alessandro and Dantas, Francisco and Garcia, Alessandro},
title = {On-demand integration of product lines: a study of reuse and stability},
year = {2011},
isbn = {9781450305846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985484.1985494},
doi = {10.1145/1985484.1985494},
abstract = {The integration of multiple SPLs is increasingly becoming a trend to enable on-demand derivation of new products and accelerate their time-to-market. Integration of SPLs often implies the reuse of a previously-implemented feature across other SPLs. The reuse of a SPL feature is only viable if the underlying programming mechanisms enable its smooth composition within the code of other SPLs. If the required modifications are significant, the design of the target SPLs are likely to be destabilized. This paper presents an exploratory study on the integration of three product lines from the board game domain. We investigate how aspect-oriented and feature-oriented programming impact on the reuse and stability of those product lines.},
booktitle = {Proceedings of the 2nd International Workshop on Product Line Approaches in Software Engineering},
pages = {35–39},
numpages = {5},
keywords = {stability, software product line, reuse, product line integration},
location = {Waikiki, Honolulu, HI, USA},
series = {PLEASE '11}
}

@inproceedings{10.1145/3361525.3361544,
author = {Ni, Xiang and Schneider, Scott and Pavuluri, Raju and Kaus, Jonathan and Wu, Kun-Lung},
title = {Automating Multi-level Performance Elastic Components for IBM Streams},
year = {2019},
isbn = {9781450370097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361525.3361544},
doi = {10.1145/3361525.3361544},
abstract = {Streaming applications exhibit abundant opportunities for pipeline parallelism, data parallelism and task parallelism. Prior work in IBM Streams introduced an elastic threading model that sought the best performance by automatically tuning the number of threads. In this paper, we introduce the ability to automatically discover where that threading model is profitable. However this introduces a new challenge: we have separate performance elastic mechanisms that are designed with different objectives, leading to potential negative interactions and unintended performance degradation. We present our experiences in overcoming these challenges by showing how to coordinate separate but interfering elasticity mechanisms to maxmize performance gains with stable and fast parallelism exploitation. We first describe an elastic performance mechanism that automatically adapts different threading models to different regions of an application. We then show a coherent ecosystem for coordinating this threading model elasticty with thread count elasticity. This system is an online, stable multi-level elastic coordination scheme that adapts different regions of a streaming application to different threading models and number of threads. We implemented this multi-level coordination scheme in IBM Streams and demonstrated that it (a) scales to over a hundred threads; (b) can improve performance by an order of magnitude on two different processor architectures when an application can benefit from multiple threading models; and (c) achieves performance comparable to hand-optimized applications but with much fewer threads.},
booktitle = {Proceedings of the 20th International Middleware Conference},
pages = {163–175},
numpages = {13},
keywords = {runtime, elastic scheduling, Stream processing},
location = {Davis, CA, USA},
series = {Middleware '19}
}

@inproceedings{10.1145/3603287.3651199,
author = {Shatnawi, Hazim and Saquer, Jamil},
title = {Encoding Feature Models in Neo4j Graph Database},
year = {2024},
isbn = {9798400702372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603287.3651199},
doi = {10.1145/3603287.3651199},
abstract = {This study introduces an innovative approach to encoding and analyzing feature models within the Network Exploration and Optimization for Java (Neo4j) graph database, significantly enhancing the management of complex Software Product Lines (SPLs). We present a comparative analysis of traditional loading techniques against Neo4j's batch importer and the Awesome Procedures on Cypher (APOC) library, demonstrating the superior efficiency and effectiveness of our proposed methods, especially in handling large datasets. Our methodology extends beyond mere encoding; it capitalizes on Neo4j's Graph Data Science (GDS) library, employing Depth-First Search (DFS) and other advanced traversal techniques to navigate and manipulate these complex structures. The findings reveal not only a significant enhancement in the processing and analysis of feature models but also underscore the potential for more sophisticated SPL management strategies. By integrating innovative loading techniques, encoding strategies, and GDS traversal methods, this study lays a robust foundation for future advancements in the field.},
booktitle = {Proceedings of the 2024 ACM Southeast Conference},
pages = {157–166},
numpages = {10},
keywords = {Cypher, data science, feature model, graph data science library, graph traversals, load data in Neo4j, performance measurement},
location = {Marietta, GA, USA},
series = {ACMSE '24}
}

@inproceedings{10.1145/3196398.3196407,
author = {Laaber, Christoph and Leitner, Philipp},
title = {An evaluation of open-source software microbenchmark suites for continuous performance assessment},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196407},
doi = {10.1145/3196398.3196407},
abstract = {Continuous integration (CI) emphasizes quick feedback to developers. This is at odds with current practice of performance testing, which predominantely focuses on long-running tests against entire systems in production-like environments. Alternatively, software microbenchmarking attempts to establish a performance baseline for small code fragments in short time. This paper investigates the quality of microbenchmark suites with a focus on suitability to deliver quick performance feedback and CI integration. We study ten open-source libraries written in Java and Go with benchmark suite sizes ranging from 16 to 983 tests, and runtimes between 11 minutes and 8.75 hours. We show that our study subjects include benchmarks with result variability of 50% or higher, indicating that not all benchmarks are useful for reliable discovery of slowdowns. We further artificially inject actual slowdowns into public API methods of the study subjects and test whether test suites are able to discover them. We introduce a performance-test quality metric called the API benchmarking score (ABS). ABS represents a benchmark suite's ability to find slowdowns among a set of defined core API methods. Resulting benchmarking scores (i.e., fraction of discovered slowdowns) vary between 10% and 100% for the study subjects. This paper's methodology and results can be used to (1) assess the quality of existing microbenchmark suites, (2) select a set of tests to be run as part of CI, and (3) suggest or generate benchmarks for currently untested parts of an API.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {119–130},
numpages = {12},
keywords = {Go, Java, continuous integration, empirical study, microbenchmarking, software performance testing},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@article{10.1145/3034827,
author = {Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz},
title = {CASE Tool Support for Variability Management in Software Product Lines},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3034827},
doi = {10.1145/3034827},
abstract = {Software product lines (SPL) aim at reducing time-to-market and increasing software quality through extensive, planned reuse of artifacts. An essential activity in SPL is variability management, i.e., defining and managing commonality and variability among member products. Due to the large scale and complexity of today's software-intensive systems, variability management has become increasingly complex to conduct. Accordingly, tool support for variability management has been gathering increasing momentum over the last few years and can be considered a key success factor for developing and maintaining SPLs. While several studies have already been conducted on variability management, none of these analyzed the available tool support in detail. In this work, we report on a survey in which we analyzed 37 existing variability management tools identified using a systematic literature review to understand the tools’ characteristics, maturity, and the challenges in the field. We conclude that while most studies on variability management tools provide a good motivation and description of the research context and challenges, they often lack empirical data to support their claims and findings. It was also found that quality attributes important for the practical use of tools such as usability, integration, scalability, and performance were out of scope for most studies.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {14},
numpages = {45},
keywords = {software variability, computer-aided software engineering, Software engineering}
}

@inproceedings{10.1145/3576915.3623178,
author = {Smolka, Sven and Giesen, Jens-Rene and Winkler, Pascal and Draissi, Oussama and Davi, Lucas and Karame, Ghassan and Pohl, Klaus},
title = {Fuzz on the Beach: Fuzzing Solana Smart Contracts},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3623178},
doi = {10.1145/3576915.3623178},
abstract = {Solana has quickly emerged as a popular platform for building decentralized applications (DApps), such as marketplaces for non-fungible tokens (NFTs). A key reason for its success are Solana's low transaction fees and high performance, which is achieved in part due to its stateless programming model. Although the literature features extensive tooling support for smart contract security, current solutions are largely tailored for the Ethereum Virtual Machine. Unfortunately, the very stateless nature of Solana's execution environment introduces novel attack patterns specific to Solana requiring a rethinking for building vulnerability analysis methods.In this paper, we address this gap and propose FuzzDelSol, the first binary-only coverage-guided fuzzing architecture for Solana smart contracts. FuzzDelSol faithfully models runtime specifics such as smart contract interactions. Moreover, since source code is not available for the large majority of Solana contracts, FuzzDelSol operates on the contract's binary code. Hence, due to the lack of semantic information, we carefully extracted low-level program and state information to develop a diverse set of bug oracles covering all major bug classes in Solana. Our extensive evaluation on 6049 smart contracts shows that FuzzDelSol's bug oracles finds impactful vulnerabilities with a high precision and recall. To the best of our knowledge, this is the largest evaluation of the security landscape on the Solana mainnet.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1197–1211},
numpages = {15},
keywords = {blockchain security, fuzzing, solana},
location = {Copenhagen, Denmark},
series = {CCS '23}
}

@inproceedings{10.1145/3183440.3183499,
author = {Mukelabai, Mukelabai and Behringer, Benjamin and Fey, Moritz and Palz, Jochen and Kr\"{u}ger, Jacob and Berger, Thorsten},
title = {Multi-view editing of software product lines with PEoPL},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3183499},
doi = {10.1145/3183440.3183499},
abstract = {A software product line is a portfolio of software variants in an application domain. It relies on a platform integrating common and variable features of the variants using variability mechanisms---typically classified into annotative and compositional mechanisms. Annotative mechanisms (e.g., using the C preprocessor) are easy to apply, but annotations clutter source code and feature code is often scattered across the platform, which hinders program comprehension and increases maintenance effort. Compositional mechanisms (e.g., using feature modules) support program comprehension and maintainability by modularizing feature code, but are difficult to adopt. Most importantly, engineers need to choose one mechanism and then stick to it for the whole life cycle of the platform. The PEoPL (Projectional Editing of Product Lines) approach combines the advantages of both kinds of mechanisms. In this paper, we demonstrate the PEoPL IDE, which supports the approach by providing various kinds of editable views, each of which represents the same software product line using annotative or compositional variability mechanisms, or subsets of concrete variants. Software engineers can seamlessly switch these views, or use multiple views side-by-side, based on the current engineering task. A demo video of PEoPL is available at Youtube: https://youtu.be/wByUxSPLoSY},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {81–84},
numpages = {4},
keywords = {annotative, modular, product lines, projectional editing},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/2405136.2405145,
author = {Almeida, Andr\'{e} and Cavalcante, Everton and Batista, Thais and Lopes, Frederico and Delicato, Flavia C. and Pires, Paulo F. and Alves, Gustavo and Cacho, N\'{e}lio},
title = {Towards an SPL-based monitoring middleware strategy for cloud computing applications},
year = {2012},
isbn = {9781450316088},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2405136.2405145},
doi = {10.1145/2405136.2405145},
abstract = {Cloud-based applications are composed of services offered by distinct third-party cloud providers. The selection of the proper cloud services that fit the application needs is based on cloud-related information, i.e. properties of the services such as price, availability, response time, among others. Typically, applications rely on a middleware that abstracts away the burden of direct dealing with underlying mechanisms for service selection and communication with the cloud providers. In this context, in a previous work we already discussed the benefits of using the software product lines (SPL) paradigm for representing alternative cloud services and their properties, which is suitable for the process of choosing the proper services to compose the application. As most cloud-related information are dynamic and may change any time during the application execution, the continuous monitoring of such information is essential to ensure that the deployed application is composed of cloud services that adhere to the application requirements. In this paper we present an SPL-based monitoring middleware strategy to continuously monitoring the dynamic properties of cloud services used by an application.},
booktitle = {Proceedings of the 10th International Workshop on Middleware for Grids, Clouds and e-Science},
articleno = {9},
numpages = {6},
keywords = {software product lines, selection, monitoring strategy, monitoring, cloud computing},
location = {Montreal, Quebec, Canada},
series = {MGC '12}
}

@inproceedings{10.1145/303008.303063,
author = {Bayer, Joachim and Flege, Oliver and Knauber, Peter and Laqua, Roland and Muthig, Dirk and Schmid, Klaus and Widen, Tanya and DeBaud, Jean-Marc},
title = {PuLSE: a methodology to develop software product lines},
year = {1999},
isbn = {1581131011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/303008.303063},
doi = {10.1145/303008.303063},
booktitle = {Proceedings of the 1999 Symposium on Software Reusability},
pages = {122–131},
numpages = {10},
keywords = {software product line, domain-specific software architecture, domain engineering},
location = {Los Angeles, California, USA},
series = {SSR '99}
}

@inproceedings{10.1145/3624007.3624060,
author = {Correa Restrepo, Camilo and Robin, Jacques and Mazo, Raul},
title = {Generating Constraint Programs for Variability Model Reasoning: A DSL and Solver-Agnostic Approach},
year = {2023},
isbn = {9798400704062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624007.3624060},
doi = {10.1145/3624007.3624060},
abstract = {Verifying and configuring large Software Product Lines (SPL) requires automation tools. Current state-of-the-art approaches involve translating variability models into a formalism accepted as input by a constraint solver. There are currently no standards for variability modeling languages (VML). There is also a variety of constraint solver input languages. This has resulted in a multiplication of ad-hoc architectures and tools specialized for a single pair of VML and solver, fragmenting the SPL community. To overcome this limitation, we propose a novel architecture based on model-driven code generation, where the syntax and semantics of VMLs can be declaratively specified as data, and a standard, human-readable, formal pivot language is used between the VML and the solver input language. This architecture is the first to be fully generic by being agnostic to both VML and the solver paradigm. To validate the genericity of the approach, we have implemented a prototype tool together with declarative specifications for the syntax and semantics of two different VMLs and two different solver families. One VML is for classic, static SPL, and the other for run-time reconfigurable dynamic SPL with soft constraints to be optimized during configuration. The two solver families are Constraint Satisfaction Programs (CSP) and Constraint Logic Programs (CLP).},
booktitle = {Proceedings of the 22nd ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {138–152},
numpages = {15},
keywords = {Software Product Lines, Generic Architecture, Configuration Automation, Automated Reasoning},
location = {Cascais, Portugal},
series = {GPCE 2023}
}

@inproceedings{10.1145/3239235.3239240,
author = {Aljarallah, Sulaiman and Lock, Russell},
title = {An exploratory study of software sustainability dimensions and characteristics: end user perspectives in the kingdom of Saudi Arabia (KSA)},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3239240},
doi = {10.1145/3239235.3239240},
abstract = {Background: Sustainability has become an important topic globally and the focus on ICT sustainability is increasing. However, issues exist, including vagueness and complexity of the concept itself, in addition to immaturity of the Software Engineering (SE) field. Aims: The study surveys respondents on software sustainability dimensions and characteristics from their perspectives, and seeks to derive rankings for their priority. Method: An exploratory study was conducted to quantitatively investigate Saudi Arabian (KSA) software user's perceptions with regard to the concept itself, the dimensions and characteristics of the software sustainability. Survey data was gathered from 906 respondents. Results: The results highlight key dimensions for sustainability and their priorities to users. The results also indicate that the characteristics perceived to be the most significant, were security, usability, reliability, maintainability, extensibility and portability, whereas respondents were relatively less concerned with computer ethics (e.g. privacy and trust), functionality, efficiency and reusability. A key finding was that females considered the environmental dimension to be more important than males. Conclusions: The dimensions and characteristics identified here can be used as a means of providing valuable feedback for the planning and implementation of future development of sustainable software.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {14},
numpages = {10},
keywords = {sustainability dimensions, software sustainability, empirical study},
location = {Oulu, Finland},
series = {ESEM '18}
}

@inproceedings{10.1145/1138474.1138485,
author = {Yoshimura, Kentaro and Ganesan, Dharmalingam and Muthig, Dirk},
title = {Assessing merge potential of existing engine control systems into a product line},
year = {2006},
isbn = {1595934022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1138474.1138485},
doi = {10.1145/1138474.1138485},
abstract = {Engine Control Systems (ECS) for automobiles have many variants for many manufactures and several markets. To improve their development efficiency, exploiting ECS commonalities and predicting their variability are mandatory. The concept of software product line engineering meets this ECS business background. However, we should carefully investigate the expected technical, economical, and organizational effects of introducing the strategy into existing products. Thereafter, a strategy will be derived systematically and realize the desired benefits.This paper reports an experience with the up-front investigation performed for Hitachi's ECS. We focus on the approach to plan the migration of the existing family of individual systems into a future product line. The approach assesses potential ways of merging software from existing variants and eventually defines a procedure for performing the migration. To get a high quality strategy, we integrate the approach of software measurement, the expertise of software architects, and reverse engineering techniques.},
booktitle = {Proceedings of the 2006 International Workshop on Software Engineering for Automotive Systems},
pages = {61–67},
numpages = {7},
keywords = {software product line, reverse engineering, engine control systems, clone detection and classification},
location = {Shanghai, China},
series = {SEAS '06}
}

@inproceedings{10.1145/3576915.3623209,
author = {Yu, Zhiyuan and Zhai, Shixuan and Zhang, Ning},
title = {AntiFake: Using Adversarial Audio to Prevent Unauthorized Speech Synthesis},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3623209},
doi = {10.1145/3576915.3623209},
abstract = {The rapid development of deep neural networks and generative AI has catalyzed growth in realistic speech synthesis. While this technology has great potential to improve lives, it also leads to the emergence of ''DeepFake'' where synthesized speech can be misused to deceive humans and machines for nefarious purposes. In response to this evolving threat, there has been a significant amount of interest in mitigating this threat by DeepFake detection.Complementary to the existing work, we propose to take the preventative approach and introduce AntiFake, a defense mechanism that relies on adversarial examples to prevent unauthorized speech synthesis. To ensure the transferability to attackers' unknown synthesis models, an ensemble learning approach is adopted to improve the generalizability of the optimization process. To validate the efficacy of the proposed system, we evaluated AntiFake against five state-of-the-art synthesizers using real-world DeepFake speech samples. The experiments indicated that AntiFake achieved over 95% protection rate even to unknown black-box models. We have also conducted usability tests involving 24 human participants to ensure the solution is accessible to diverse populations.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {460–474},
numpages = {15},
keywords = {adversarial machine learning, deepfake defense, generative ai, speech synthesis},
location = {Copenhagen, Denmark},
series = {CCS '23}
}

@inproceedings{10.1145/3634737.3637675,
author = {Ott, Simon and Orthen, Benjamin and Weidinger, Alexander and Horsch, Julian and Nayani, Vijayanand and Ekberg, Jan-Erik},
title = {MultiTEE: Distributing Trusted Execution Environments},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634737.3637675},
doi = {10.1145/3634737.3637675},
abstract = {The adoption of wearable technologies, such as smartwatches or wristbands, is rising. End-users expect to use all of their devices in an interconnected and seamless manner to conduct digital transactions, e.g., to pay or identify via their smartwatches, and not only via their smartphones. As sensitive transactions are usually protected by hardware-enforced isolation mechanisms, such as Trusted Execution Environments (TEEs), this brings new challenges of interconnecting TEEs to collaboratively conduct such transactions. We therefore propose MultiTEE, a distributed TEE architecture for heterogeneous device clusters, enabling secure data exchange and cooperation between TEEs. MultiTEE relies on lightweight, secure channels between TEEs, combined with remote attestation for the integrity verification of software stacks, as well as a memory-safe implementation. This enables an interface between Trusted Applications (TAs) of the distributed TEE similar to the interfaces of classic, single device TEEs. To demonstrate the feasibility of our solution, we built a Proof of Concept (PoC), partially implementing the upcoming European Digital Identity (EUDI) wallet to show the usage of heterogeneous device clusters for electronic identification. We evaluate our solution regarding performance and security.},
booktitle = {Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
pages = {1617–1629},
numpages = {13},
keywords = {trusted execution environments, remote attestation, secure channels, channel binding},
location = {Singapore, Singapore},
series = {ASIA CCS '24}
}

@inproceedings{10.1145/2556624.2556645,
author = {Adelsberger, Stephan and Sobernig, Stefan and Neumann, Gustaf},
title = {Towards assessing the complexity of object migration in dynamic, feature-oriented software product lines},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556645},
doi = {10.1145/2556624.2556645},
abstract = {Dynamic Software Product Lines (DSPLs) implement features of a product family, from which products can be derived and reconfigured at runtime. This way, systems can alternate their configurations without service interruption. The activation and deactivation of features at runtime pose challenges for the implementation of a DSPL, in particular for handling object states such as runtime changes to object-scoped variables, their value assignments, and the variable properties. To quantify the complexity of this object migration, we propose a systematic code-level measurement approach which harvests feature implementations and the corresponding variability models for code introductions responsible for critical changes to object states.We have applied our measurement process tentatively to data sets representing 9 SPLs implemented using Fuji. This way, we arrived at first insights on object-migration complexity in SPLs. For example, we observed that the number of feature-specific object states is distributed very unequally in Fuji SPLs, with a few objects having an overly complex map of potential object states and the majority of objects potentially seeing transitions between 1 and 5 object states. We also evaluated different tactics of applying SAT solvers to analyze variability models in this context.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {17},
numpages = {8},
keywords = {object migration, feature-oriented programming, feature binding, dynamic software product line, constructor anomaly},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@inproceedings{10.1145/383962.384036,
author = {Baek, Sung Hoon and Kim, Bong Wan and Joung, Eui Joung and Park, Chong Won},
title = {Reliability and performance of hierarchical RAID with multiple controllers},
year = {2001},
isbn = {1581133839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383962.384036},
doi = {10.1145/383962.384036},
abstract = {Redundant arrays of inexpensive disks (RAID) offer fault tolerance against disk failures. However a storage system having more disks suffers from less reliability and performance. A RAID architecture tolerating multiple disk failures shows severe performance degradation in comparison to the RAID Level 5 due to the complexity of implementation. We present a new RAID architecture that tolerates at least three disk failures and offers similar throughout to the RAID Level 5. We call it the hierarchical RAID, which is hierarchically composed of RAID Levels. Furthermore, we formally introduce the mean-time-to-data-loss (MTTDL) of traditional RAID and the hierarchical RAID using Markov process for detailed comparison.},
booktitle = {Proceedings of the Twentieth Annual ACM Symposium on Principles of Distributed Computing},
pages = {246–254},
numpages = {9},
keywords = {three-failure-tolerant array, high reliability, hierarchical RAID, Markov process},
location = {Newport, Rhode Island, USA},
series = {PODC '01}
}

@inproceedings{10.1145/2000259.2000286,
author = {Cavalcanti, Ricardo de Oliveira and de Almeida, Eduardo Santana and Meira, Silvio R.L.},
title = {Extending the RiPLE-DE process with quality attribute variability realization},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000286},
doi = {10.1145/2000259.2000286},
abstract = {Software product lines engineering is a viable way to achieve the productivity gains desired by companies. Product line architecture must benefit from commonalities among products in the family and enable the variability among them. The aspect of variability in quality attributes has been neglected or ignored by most of the researchers as attention has been mainly put in functional variability. This paper describes an architecture and design process for software product lines that can properly deal with quality attribute variability. The proposed approach enhances the RiPLE-DE process for software product line engineering with activities and guidelines for quality attribute variability. An initial experimental study is presented to characterize and evaluate the proposed process enhancements.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {159–164},
numpages = {6},
keywords = {software reuse, software product lines (spl), software architecture, quality attribute variability},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@inproceedings{10.1145/2903150.2911708,
author = {Alberca, Carlos and Pastrana, Sergio and Suarez-Tangil, Guillermo and Palmieri, Paolo},
title = {Security analysis and exploitation of arduino devices in the internet of things},
year = {2016},
isbn = {9781450341288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2903150.2911708},
doi = {10.1145/2903150.2911708},
abstract = {The pervasive presence of interconnected objects enables new communication paradigms where devices can easily reach each other while interacting within their environment. The so-called Internet of Things (IoT) represents the integration of several computing and communications systems aiming at facilitating the interaction between these devices. Arduino is one of the most popular platforms used to prototype new IoT devices due to its open, flexible and easy-to-use architecture. Ardunio Yun is a dual board microcontroller that supports a Linux distribution and it is currently one of the most versatile and powerful Arduino systems. This feature positions Arduino Yun as a popular platform for developers, but it also introduces unique infection vectors from the security viewpoint. In this work, we present a security analysis of Arduino Yun. We show that Arduino Yun is vulnerable to a number of attacks and we implement a proof of concept capable of exploiting some of them.},
booktitle = {Proceedings of the ACM International Conference on Computing Frontiers},
pages = {437–442},
numpages = {6},
location = {Como, Italy},
series = {CF '16}
}

@inproceedings{10.1145/3302333.3302340,
author = {Krieter, Sebastian and Thiem, Tobias and Leich, Thomas},
title = {Using Dynamic Software Product Lines to Implement Adaptive SGX-enabled Systems},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302340},
doi = {10.1145/3302333.3302340},
abstract = {In the light of computational outsourcing and external data storage, data protection and trusted execution become increasingly important. Novel hardware such as Intel's Software Guard extensions (SGX) attempts to provide a solution to protect data and computations from unauthorized access and manipulation, even against attackers with physical access to a machine. However, the current generation of SGX limits the protected memory space that can be efficiently used to 128 MiB, which must be shared between data and binary code. Thus, we propose to use a software product line approach to tailor an application's binary code in such a way that it can be updated during runtime, with the goal to only store relevant features in the protected memory at a given time. We provide a prototypical implementation that enables basic support for loading and unloading features during runtime and evaluate our prototype in terms of execution times against non-adaptive execution.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {9},
numpages = {9},
keywords = {Software Product Lines, Runtime Adaptation, Intel Software Guard Extensions},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/2602576.2602585,
author = {Etxeberria, Leire and Trubiani, Catia and Cortellessa, Vittorio and Sagardui, Goiuria},
title = {Performance-based selection of software and hardware features under parameter uncertainty},
year = {2014},
isbn = {9781450325769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602576.2602585},
doi = {10.1145/2602576.2602585},
abstract = {Configurable software systems allow stakeholders to derive variants by selecting software and/or hardware features. Performance analysis of feature-based systems has been of large interest in the last few years, however a major research challenge is still to conduct such analysis before achieving full knowledge of the system, namely under a certain degree of uncertainty. In this paper we present an approach to analyze the correlation between selection of features embedding uncertain parameters and system performance. In particular, we provide best and worst case performance bounds on the basis of selected features and, in cases of wide gaps among these bounds, we carry on a sensitivity analysis process aimed at taming the uncertainty of parameters. The application of our approach to a case study in the e-health domain demonstrates how to support stakeholders in the identification of system variants that meet performance requirements.},
booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {23–32},
numpages = {10},
keywords = {uncertainty, software architectures, performance analysis, feature selection},
location = {Marcq-en-Bareul, France},
series = {QoSA '14}
}

@article{10.1145/3636512,
author = {Zhu, Weiyao and Wu, Ou and Su, Fengguang and Deng, Yingjun},
title = {Exploring the Learning Difficulty of Data: Theory and Measure},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3636512},
doi = {10.1145/3636512},
abstract = {‘‘Easy/hard sample” is a popular parlance in machine learning. Learning difficulty of samples refers to how easy/hard a sample is during a learning procedure. An increasing need of measuring learning difficulty demonstrates its importance in machine learning (e.g., difficulty-based weighting learning strategies). Previous literature has proposed a number of learning difficulty measures. However, no comprehensive investigation for learning difficulty is available to date, resulting in that nearly all existing measures are heuristically defined without a rigorous theoretical foundation. This study attempts to conduct a pilot theoretical study for learning difficulty of samples. First, influential factors for learning difficulty are summarized. Under various situations conducted by summarized influential factors, correlations between learning difficulty and two vital criteria of machine learning, namely, generalization error and model complexity, are revealed. Second, a theoretical definition of learning difficulty is proposed on the basis of these two criteria. A practical measure of learning difficulty is proposed under the direction of the theoretical definition by importing the bias-variance trade-off theory. Subsequently, the rationality of theoretical definition and the practical measure are demonstrated, respectively, by analysis of several classical weighting methods and abundant experiments realized under all situations conducted by summarized influential factors. The mentioned weighting methods can be reasonably explained under the proposed theoretical definition and concerned propositions. The comparison in these experiments indicates that the proposed measure significantly outperforms the other measures throughout the experiments.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {84},
numpages = {37},
keywords = {Learning difficulty, generalization error, bias-variance trade-off, model complexity}
}

@inproceedings{10.1145/2168697.2168699,
author = {Quinton, Cl\'{e}ment and Rouvoy, Romain and Duchien, Laurence},
title = {Leveraging feature models to configure virtual appliances},
year = {2012},
isbn = {9781450311618},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2168697.2168699},
doi = {10.1145/2168697.2168699},
abstract = {Cloud computing is a major trend in distributed computing environments. Software virtualization technologies allow cloud Infrastructure-as-a-Service (IaaS) providers to instantiate and run a large number of virtual appliances. However, one of the major challenges is to reduce the disk space footprint of such virtual appliances to improve their storage and transfer across cloud servers. In this paper, we propose to use a Software Product Line (SPL) approach and describe the virtual appliance as a set of common and variable elements modeled by means of Feature Model (FM). We describe a solution to reverse engineer a FM from a virtual appliance and we show how we take advantage of the SPL configuration mechanisms to significantly reduce the size of a virtual appliance.},
booktitle = {Proceedings of the 2nd International Workshop on Cloud Computing Platforms},
articleno = {2},
numpages = {6},
location = {Bern, Switzerland},
series = {CloudCP '12}
}

@article{10.1145/3659101,
author = {Tian, Huan and Tang, Jiewen and Li, Jun and Sha, Zhibing and Yang, Fan and Cai, Zhigang and Liao, Jianwei},
title = {Modeling Retention Errors of 3D NAND Flash for Optimizing Data Placement},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {1084-4309},
url = {https://doi.org/10.1145/3659101},
doi = {10.1145/3659101},
abstract = {Considering 3D NAND flash has a new property of process variation (PV), which causes different raw bit error rates (RBER) among different layers of the flash block. This article builds a mathematical model for estimating the retention errors of flash cells, by considering the factor of layer-to-layer PV in 3D NAND flash memory, as well as the factors of program/erase (P/E) cycle and retention time of data. Then, it proposes classifying the layers of flash block in 3D NAND flash memory into profitable and unprofitable categories, according to the error correction overhead. After understanding the retention error variation of different layers in 3D NAND flash, we design a mechanism of data placement, which maps the write data onto a suitable layer of flash block, according to the data hotness and the error correction overhead of layers, to boost read performance of 3D NAND flash. The experimental results demonstrate that our proposed retention error estimation model can yield a R2 value of 0.966 on average, verifying the accuracy of the model. Based on the estimated retention error rates of layers, the proposed data placement mechanism can noticeably reduce the read latency by 29.8% on average, compared with state-of-the-art methods against retention errors for 3D NAND flash memory.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = jun,
articleno = {62},
numpages = {24},
keywords = {Solid-state drivers, 3D flash memories, ECC, reliability, modeling, layer RBER variation}
}

@inproceedings{10.1145/1551722.1551730,
author = {Laguna, Miguel A. and Finat, Javier and Gonz\'{a}lez, Jos\'{e} A.},
title = {Mobile health monitoring and smart sensors: a product line approach},
year = {2009},
isbn = {9781605583983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1551722.1551730},
doi = {10.1145/1551722.1551730},
abstract = {The evolution of the population pyramid in developed countries, with an increasing proportion of aged people introduces new challenges to the public and private assistance services. A form of improving these services while keeping controlled the associated costs is to use remote continuous assistance. Wireless sensors allow obtaining real-time information of health parameters in a non-intrusive way. The determination of alert values for these parameters and the computing possibilities of the current mobile devices can facilitate a faster intervention which will minimize risks linked to delays in medical assistance. However, the diversity of risk situations is a factor that increases costs as many similar but not exactly identical products will be necessary now and in the future. We aim to solve this problem using an approach of software product lines, as multiple options can be easily incorporated to each final product implementation. This article presents the product line generic architecture and some examples of application, using wireless sensors connected to a central station by means of a smart phone, which is able to detect alarm situations.},
booktitle = {Proceedings of the 2009 Euro American Conference on Telematics and Information Systems: New Opportunities to Increase Digital Citizenship},
articleno = {8},
numpages = {8},
keywords = {software product line, sensor, remote health monitoring},
location = {Prague, Czech Republic},
series = {EATIS '09}
}

@inproceedings{10.1145/3377024.3377031,
author = {El-Sharkawy, Sascha and Krafczyk, Adam and Schmid, Klaus},
title = {Fast static analyses of software product lines: an example with more than 42,000 metrics},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377031},
doi = {10.1145/3377024.3377031},
abstract = {Context: Software metrics, as one form of static analyses, is a commonly used approach in software engineering in order to understand the state of a software system, in particular to identify potential areas prone to defects. Family-based techniques extract variability information from code artifacts in Software Product Lines (SPLs) to perform static analysis for all available variants. Many different types of metrics with numerous variants have been defined in literature. When counting all metrics including such variants, easily thousands of metrics can be defined. Computing all of them for large product lines can be an extremely expensive process in terms of performance and resource consumption.Objective: We address these performance and resource challenges while supporting customizable metric suites, which allow running both, single system and variability-aware code metrics.Method: In this paper, we introduce a partial parsing approach used for the efficient measurement of more than 42,000 code metric variations. The approach covers variability information and restricts parsing to the relevant parts of the Abstract Syntax Tree (AST).Conclusions: This partial parsing approach is designed to cover all relevant information to compute a broad variety of variability-aware code metrics on code artifacts containing annotation-based variability, e.g., realized with C-preprocessor statements. It allows for the flexible combination of single system and variability-aware metrics, which is not supported by existing tools. This is achieved by a novel representation of partially parsed product line code artifacts, which is tailored to the computation of the metrics. Our approach consumes considerably less resources, especially when computing many metric variants in parallel.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {8},
numpages = {9},
keywords = {variability models, software product lines, metrics, implementation, feature models, abstract syntax trees, SPL, AST},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.5555/3291291.3291298,
author = {Islam, Nayreet and Azim, Akramul},
title = {Assuring the runtime behavior of self-adaptive cyber-physical systems using feature modeling},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {A self-adaptive cyber-physical system (SACPS) can adjust its behavior and configurations at runtime in response to varying requirements obtained from the system and the environment. With the increasing use of the SACPS in different application domains, such variations are becoming more common. Users today expect the SACPS to guarantee its functional and timing behavior even in adverse environmental situations. However, uncertainties in the SACPS environment impose challenges on assuring the runtime behavior during system design.Software product line engineering (SPLE) is considered as a useful technique for handling varying requirements. In this paper, we present an approach for assuring the runtime behavior of the SACPS by applying an SPLE technique such as feature modeling. By representing the feature-based model at design time, we characterize the possible adaptation requirements to reusable configurations. The proposed approach aims to model two dynamic variability dimensions: 1) environment variability that describes the conditions under which the SACPS must adapt, and 2) structural variability, that defines the resulting architectural configurations. To validate our approach, the experimental analysis is performed using two case studies: 1) a traffic monitoring SACPS and 2) an automotive SACPS. We demonstrate that the proposed feature-based modeling approach can be used to achieve adaptivity which allows the SACPS to assure functional (defining execution of the correct set of adaptive tasks) and non-functional (defining execution of SACPS in the expected mode) correctness at runtime. The experimental results show that the feature-based SACPS demonstrates significant improvement in terms of self-configuration time, self-adaptation time and scalability with less probability of failure in different environmental situations.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {48–59},
numpages = {12},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@article{10.1109/TASLP.2024.3449037,
author = {Pawlak, Alan and Lee, Hyunkook and M\"{a}kivirta, Aki and Lund, Thomas},
title = {Spatial Analysis and Synthesis Methods: Subjective and Objective Evaluations Using Various Microphone Arrays in the Auralization of a Critical Listening Room},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3449037},
doi = {10.1109/TASLP.2024.3449037},
abstract = {Parametric sound field reproduction methods, such as the Spatial Decomposition Method (SDM) and Higher-Order Spatial Impulse Response Rendering (HO-SIRR), are widely used for the analysis and auralization of sound fields. This paper studies the performance of various sound field reproduction methods in the context of the auralization of a critical listening room, focusing on fixed head orientations. The influence on the perceived spatial and timbral fidelity of the following factors is considered: the rendering framework, direction of arrival (DOA) estimation method, microphone array structure, and use of a dedicated center reference microphone with SDM. Listening tests compare the synthesized sound fields to a reference binaural rendering condition, all for static head positions. Several acoustic parameters are measured to gain insights into objective differences between methods. All systems were distinguishable from the reference in perceptual tests. A high-quality pressure microphone improves the SDM framework's timbral fidelity, and spatial fidelity in certain scenarios. Additionally, SDM and HO-SIRR show similarities in spatial fidelity. Performance variation between SDM configurations is influenced by the DOA estimation method and microphone array construction. The binaural SDM (BSDM) presentations display temporal artifacts impacting sound quality.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {3986–4001},
numpages = {16}
}

@inproceedings{10.1145/2110147.2110158,
author = {Lopez-Herrejon, Roberto E. and Egyed, Alexander},
title = {Towards fixing inconsistencies in models with variability},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110158},
doi = {10.1145/2110147.2110158},
abstract = {Recent years have witnessed a convergence between research in SPL and Model-Driven Engineering (MDE) that leverages the complementary capabilities that both paradigms can offer. A crucial factor for the success of MDE is the availability of effective support for detecting and fixing inconsistencies among model elements. The importance of such support is attested by the extensive literature devoted to the topic. However, when coupled with variability, the research focus has been devoted to inconsistency detection, while leaving the important issue of fixing the inconsistency largely unaddressed. In this research-in-progress paper, we explore one of the issues that variability raises for inconsistency fixing. Namely, in which features to locate the fixes. We compute what is the minimal number of fixes and use it as a baseline to compare fixes obtained with a heuristic based on feature model analysis and random approaches. Our work highlights the pros and cons of both approaches and suggests how they could be addressed.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {93–100},
numpages = {8},
keywords = {variability, software product line, safe composition, model, feature oriented software development, consistency checking, consistency},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@article{10.1145/1183236.1183240,
author = {Montazemi, Ali Reza},
title = {How they manage IT: SMEs in Canada and the U.S.},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183240},
doi = {10.1145/1183236.1183240},
abstract = {Small and medium-sized companies in the U.S. make better use of IT than their Canadian counterparts.},
journal = {Commun. ACM},
month = dec,
pages = {109–112},
numpages = {4}
}

@inproceedings{10.1145/2950290.2950318,
author = {Macedo, Nuno and Brunel, Julien and Chemouil, David and Cunha, Alcino and Kuperberg, Denis},
title = {Lightweight specification and analysis of dynamic systems with rich configurations},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2950318},
doi = {10.1145/2950290.2950318},
abstract = {Model-checking is increasingly popular in the early phases of the software development process. To establish the correctness of a software design one must usually verify both structural and behavioral (or temporal) properties. Unfortunately, most specification languages, and accompanying model-checkers, excel only in analyzing either one or the other kind. This limits their ability to verify dynamic systems with rich configurations: systems whose state space is characterized by rich structural properties, but whose evolution is also expected to satisfy certain temporal properties. To address this problem, we first propose Electrum, an extension of the Alloy specification language with temporal logic operators, where both rich configurations and expressive temporal properties can easily be defined. Two alternative model-checking techniques are then proposed, one bounded and the other unbounded, to verify systems expressed in this language, namely to verify that every desirable temporal property holds for every possible configuration.},
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {373–383},
numpages = {11},
keywords = {formal specification language, Model-checking},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1145/2783258.2783270,
author = {Yan, Feng and Ruwase, Olatunji and He, Yuxiong and Chilimbi, Trishul},
title = {Performance Modeling and Scalability Optimization of Distributed Deep Learning Systems},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2783270},
doi = {10.1145/2783258.2783270},
abstract = {Big deep neural network (DNN) models trained on large amounts of data have recently achieved the best accuracy on hard tasks, such as image and speech recognition. Training these DNNs using a cluster of commodity machines is a promising approach since training is time consuming and compute-intensive. To enable training of extremely large DNNs, models are partitioned across machines. To expedite training on very large data sets, multiple model replicas are trained in parallel on different subsets of the training examples with a global parameter server maintaining shared weights across these replicas. The correct choice for model and data partitioning and overall system provisioning is highly dependent on the DNN and distributed system hardware characteristics. These decisions currently require significant domain expertise and time consuming empirical state space exploration.This paper develops performance models that quantify the impact of these partitioning and provisioning decisions on overall distributed system performance and scalability. Also, we use these performance models to build a scalability optimizer that efficiently determines the optimal system configuration that minimizes DNN training time. We evaluate our performance models and scalability optimizer using a state-of-the-art distributed DNN training framework on two benchmark applications. The results show our performance models estimate DNN training time with high estimation accuracy and our scalability optimizer correctly chooses the best configurations, minimizing the training time of distributed DNNs.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1355–1364},
numpages = {10},
keywords = {scalability, performance modeling, optimization, distributed system, deep learning},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}

@inproceedings{10.1109/ASE.2015.45,
author = {Sarkar, Atri and Guo, Jianmei and Siegmund, Norbert and Apel, Sven and Czarnecki, Krzysztof},
title = {Cost-efficient sampling for performance prediction of configurable systems},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.45},
doi = {10.1109/ASE.2015.45},
abstract = {A key challenge of the development and maintenance of configurable systems is to predict the performance of individual system variants based on the features selected. It is usually infeasible to measure the performance of all possible variants, due to feature combinatorics. Previous approaches predict performance based on small samples of measured variants, but it is still open how to dynamically determine an ideal sample that balances prediction accuracy and measurement effort. In this paper, we adapt two widely-used sampling strategies for performance prediction to the domain of configurable systems and evaluate them in terms of sampling cost, which considers prediction accuracy and measurement effort simultaneously. To generate an initial sample, we introduce a new heuristic based on feature frequencies and compare it to a traditional method based on t-way feature coverage. We conduct experiments on six real-world systems and provide guidelines for stakeholders to predict performance by sampling.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {342–352},
numpages = {11},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.5555/3106050.3106059,
author = {Al-Hajjaji, Mustafa and Lity, Sascha and Lachmann, Remo and Th\"{u}m, Thomas and Schaefer, Ina and Saake, Gunter},
title = {Delta-oriented product prioritization for similarity-based product-line testing},
year = {2017},
isbn = {9781538628034},
publisher = {IEEE Press},
abstract = {Testing every product of a software product line (SPL) is often not feasible due to the exponential number of products in the number of features. Thus, the order in which products are tested matters, because it can increase the early rate of fault detection. Several approaches have been proposed to prioritize products based on configuration similarity. However, current approaches are oblivious to solution-space differences among products, because they consider only problem-space information. With delta modeling, we incorporate solution-space information in product prioritization to improve the effectiveness of SPL testing. Deltas capture the differences between products facilitating the reasoning about product similarity. As a result, we select the most dissimilar product to the previously tested ones, in terms of deltas, to be tested next. We evaluate the effectiveness of our approach using an SPL from the automotive domain showing an improvement in the effectiveness of SPL testing.},
booktitle = {Proceedings of the 2nd International Workshop on Variability and Complexity in Software Design},
pages = {34–40},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {VACE '17}
}

@inproceedings{10.5555/3432601.3432616,
author = {Podolskiy, Vladimir and Patrou, Maria and Patros, Panos and Gerndt, Michael and Kent, Kenneth B.},
title = {The weakest link: revealing and modeling the architectural patterns of microservice applications},
year = {2020},
publisher = {IBM Corp.},
address = {USA},
abstract = {Cloud microservice applications comprise interconnected services packed into containers. Such applications generate complex communication patterns among their microservices. Studying such patterns can support assuring various quality attributes, such as autoscaling for satisfying performance, availability and scalability, or targeted penetration testing for satisfying security and correctness. We study the structure of containerized microservice applications via providing the methodology and the results of a structural graph-based analysis of 103 Docker Compose deployment files from open-sourced Github repositories. Our findings indicate the dominance of a power-law distribution of microservice interconnections. Further analysis highlights the suitability of the Barab\'{a}si-Albert model for generating large random graphs that model the architecture of real microservice applications. The exhibited structures and their usage for engineering microservice applications are discussed.},
booktitle = {Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering},
pages = {113–122},
numpages = {10},
keywords = {software vulnerability, microservice, cloud-native application, application topology},
location = {Toronto, Ontario, Canada},
series = {CASCON '20}
}

@inproceedings{10.1145/3023956.3023968,
author = {Mjeda, Anila and Wasala, Asanka and Botterweck, Goetz},
title = {Decision spaces in product lines, decision analysis, and design exploration: an interdisciplinary exploratory study},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023968},
doi = {10.1145/3023956.3023968},
abstract = {Context. From recent works on product properties resulting from configurations and the optimisation of these properties, one comes quickly to more complex challenges such as multi-objective optimisation, conflicting objectives, multiple stakeholders, and conflict resolution. The intuition is that Software Product Line Engineering (SPLE) can draw from other disciplines that deal with decision spaces and complex decision scenarios.Objectives. We aim to (1) explore links to such disciplines, (2) systematise and compare concepts, and (3) identify opportunities, where SPLE approaches can be enriched.Method. We undertake an exploratory study: Starting from common SPLE activities and artefacts, we identify aspects where we expect to find corresponding counterparts in other disciplines. We focus on Multiple Criteria Decision Analysis (MCDA), Multi-Objective Optimisation (MOO), and Design Space Exploration (DSE), and perform a comparison of the key concepts.Results. The resulting comparison relates SPLE activities and artefacts to concepts from MCDA, MOO, and DSE and identifies areas where SPLE approaches can be enriched. We also provide examples of existing work at the intersections of SPLE with the other fields. These findings are aimed to foster the conversation on research opportunities where SPLE can draw techniques from other disciplines dealing with complex decision scenarios.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {68–75},
numpages = {8},
keywords = {multi-objective optimisation, multi-criteria decision analysis, design-space exploration, decision modelling},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/3023956.3023961,
author = {Lity, Sascha and Al-Hajjaji, Mustafa and Th\"{u}m, Thomas and Schaefer, Ina},
title = {Optimizing product orders using graph algorithms for improving incremental product-line analysis},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023961},
doi = {10.1145/3023956.3023961},
abstract = {The individual analysis of each product of a software product line (SPL) leads to redundant analysis steps due to the inherent commonality. Therefore, incremental SPL analyses exploit commonalities and focus on the differences between products to reduce the analysis effort. However, existing techniques are influenced by the order in which products are analyzed. The more similar subsequently analyzed products are, the greater is the potential reduction of the overall analysis effort as similar products imply less differences to be analyzed. Hence, an order of products, where the total number of differences is minimized, facilitates incremental SPL analyses. In this paper, we apply graph algorithms to determine optimized product orders. We capture products as nodes in a graph, where solution-space information defines edge weights between product nodes. We adopt existing heuristics for finding an optimal solution of the traveling salesperson problem to determine a path in the product graph with minimal costs. A path represents an optimized product order w.r.t. minimized differences between all products. We realize a prototype of our approach and evaluate its applicability and performance showing a significant optimization compared to standard and random orders.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {60–67},
numpages = {8},
keywords = {product orders, graph algorithms, delta-oriented software product lines},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/3485832.3485896,
author = {Walker, Payton and Saxena, Nitesh},
title = {Evaluating the Effectiveness of Protection Jamming Devices in Mitigating Smart Speaker Eavesdropping Attacks Using Gaussian White Noise},
year = {2021},
isbn = {9781450385794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485832.3485896},
doi = {10.1145/3485832.3485896},
abstract = {Protection Jamming Devices (PJD) are specialized tools designed to sit on top of virtual assistant (VA) smart speakers and hinder them from “hearing” nearby user speech. PJDs aim to protect you from eavesdropping attacks by injecting a jamming signal directly into the microphones of the smart speaker. However, current signal processing routines can be used to reduce noise and enhance speech contained in noisy audio samples. Therefore, we identify a potential vulnerability for speech eavesdropping via smart speaker recordings, even when a PJD is being used. If an attacker can gain access to or facilitate smart speaker recordings they may be able to compromise a user’s speech with successful noise cancellation. Specifically, we are interested in the potential for Gaussian white noise (GWN) to be an effective jamming signal for a PJD. To our knowledge, the effectiveness of white noise and PJDs to protect against eavesdropping attacks has yet to receive a systematic evaluation that includes physical experiments with an actual PJD implementation. In this work we construct our own PJD, specialized for consistent experimentation, to simulate an attack scenario where recordings from a smart speaker, in the presence of normal speech and the PJDs jamming signal, are recovered. We perform substantial data collection under different settings to build a repository of 1500 recovered audio samples. We applied post-processing on our dataset and conducted an extensive signal/speech quality analysis including both time and frequency domain inspection, and evaluation of metrics including cross-correlation, SNR, and PESQ. Lastly, we performed feature extraction (MFCC) and built machine learning classifiers for tasks including speech (digit) recognition, speaker identification, and gender recognition. We also attempted song recognition using the Shazam app. For all speech recognition tasks that we attempted, we were able to achieve classification accuracies above that of random guessing (46% for digit recognition, 51% for speaker identification, 80% for gender identification), as well as demonstrate successful song recognition. These results highlight the real potential for attackers to compromise user speech, to some extent, using smart speaker recordings; even if the smart speaker is protected by a PJD.},
booktitle = {Proceedings of the 37th Annual Computer Security Applications Conference},
pages = {414–424},
numpages = {11},
keywords = {eavesdropping, jamming, speech masking, white noise},
location = {Virtual Event, USA},
series = {ACSAC '21}
}

@inproceedings{10.5555/2337223.2337243,
author = {Siegmund, Norbert and Kolesnikov, Sergiy S. and K\"{a}stner, Christian and Apel, Sven and Batory, Don and Rosenm\"{u}ller, Marko and Saake, Gunter},
title = {Predicting performance via automated feature-interaction detection},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Customizable programs and program families provide user-selectable features to allow users to tailor a program to an application scenario. Knowing in advance which feature selection yields the best performance is difficult because a direct measurement of all possible feature combinations is infeasible. Our work aims at predicting program performance based on selected features. However, when features interact, accurate predictions are challenging. An interaction occurs when a particular feature combination has an unexpected influence on performance. We present a method that automatically detects performance-relevant feature interactions to improve prediction accuracy. To this end, we propose three heuristics to reduce the number of measurements required to detect interactions. Our evaluation consists of six real-world case studies from varying domains (e.g., databases, encoding libraries, and web servers) using different configuration techniques (e.g., configuration files and preprocessor flags). Results show an average prediction accuracy of 95%.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {167–177},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1145/2897695.2897701,
author = {Abilio, Ramon and Vale, Gustavo and Figueiredo, Eduardo and Costa, Heitor},
title = {Metrics for feature-oriented programming},
year = {2016},
isbn = {9781450341776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897695.2897701},
doi = {10.1145/2897695.2897701},
abstract = {Feature-oriented programming (FOP) is a programming technique to implement software product lines based on composition mechanisms called refinements. A software product line is a set of software systems that share a common, managed set of features satisfying the specific needs of a particular market segment. The literature reports various software metrics for software product lines developed using object-oriented and aspect-oriented programming. However, after a literature review, we observed that we lack the definition of FOP-specific metrics. Based on this observation, this paper proposes a set of eight novel metrics for feature-oriented programming. These metrics were derived both from our experience in FOP and from existing software metrics. We demonstrate the applicability of the proposed metrics by applying them to a software product line.},
booktitle = {Proceedings of the 7th International Workshop on Emerging Trends in Software Metrics},
pages = {36–42},
numpages = {7},
keywords = {software quality, software product lines, software metrics, feature-oriented programming},
location = {Austin, Texas},
series = {WETSoM '16}
}

@inproceedings{10.1145/3302541.3311525,
author = {Machida, Fumio},
title = {Practices in Model Component Reuse for Efficient Dependability Analysis},
year = {2019},
isbn = {9781450362863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302541.3311525},
doi = {10.1145/3302541.3311525},
abstract = {Model-based dependability analysis provides an effective manner to evaluate and design the dependability of critical IT systems by abstracting the system architecture and operations. As the size and the complexity of systems increase, however, the process to compose the dependability model becomes complicated and time-consuming. Improving the efficiency of modeling process is practically an important challenge of dependability engineering. In this paper, we review the techniques for model component reuse that makes dependability model composition and analysis more efficient. In particular, component-based modeling approaches for reliability, availability, maintainability and safety analysis presented in the literature are summarized. In order to effectively apply model component reuse, we advocate the importance of asset-based dependability analysis approach that associates the reusable model components with underlying system development process. Finally, we discuss the necessary extensions of these techniques toward efficient dependability analysis for IoT systems which are significantly affecting real world.},
booktitle = {Companion of the 2019 ACM/SPEC International Conference on Performance Engineering},
pages = {65–70},
numpages = {6},
keywords = {safety, reuse, internet of things, dependability, availability},
location = {Mumbai, India},
series = {ICPE '19}
}

@proceedings{10.1145/3698576,
title = {KISV '24: Proceedings of the 2nd Workshop on Kernel Isolation, Safety and Verification},
year = {2024},
isbn = {9798400713019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Austin, TX, USA}
}

@inproceedings{10.1145/2884781.2884821,
author = {Devroey, Xavier and Perrouin, Gilles and Papadakis, Mike and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Featured model-based mutation analysis},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884821},
doi = {10.1145/2884781.2884821},
abstract = {Model-based mutation analysis is a powerful but expensive testing technique. We tackle its high computation cost by proposing an optimization technique that drastically speeds up the mutant execution process. Central to this approach is the Featured Mutant Model, a modelling framework for mutation analysis inspired by the software product line paradigm. It uses behavioural variability models, viz., Featured Transition Systems, which enable the optimized generation, configuration and execution of mutants. We provide results, based on models with thousands of transitions, suggesting that our technique is fast and scalable. We found that it outperforms previous approaches by several orders of magnitude and that it makes higher-order mutation practically applicable.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {655–666},
numpages = {12},
keywords = {variability, mutation analysis, featured transition systems},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1109/AST.2017.7,
author = {Al-Hajjaji, Mustafa and Kr\"{u}ger, Jacob and Schulze, Sandro and Leich, Thomas and Saake, Gunter},
title = {Efficient product-line testing using cluster-based product prioritization},
year = {2017},
isbn = {9781538615485},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/AST.2017.7},
doi = {10.1109/AST.2017.7},
abstract = {A software product-line comprises a set of products that share a common set of features. These features can be reused to customize a product to satisfy specific needs of certain customers or markets. As the number of possible products increases exponentially for new features, testing all products is infeasible. Existing testing approaches reduce their effort by restricting the number of products (sampling) and improve their effectiveness by considering the order of tests (prioritization). In this paper, we propose a cluster-based prioritization technique to sample similar products with respect to the feature selection. We evaluate our approach using feature models of different sizes and show that cluster-based prioritization can enhance the effectiveness of product-line testing.},
booktitle = {Proceedings of the 12th International Workshop on Automation of Software Testing},
pages = {16–22},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {AST '17}
}

@inproceedings{10.1145/2304736.2304757,
author = {Pascual, Gustavo Garc\'{\i}a and Alarc\'{o}n, M\'{o}nica Pinto and Fern\'{a}ndez, Lidia Fuentes},
title = {Component and aspect-based service product line for pervasive systems},
year = {2012},
isbn = {9781450313452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2304736.2304757},
doi = {10.1145/2304736.2304757},
abstract = {Pervasive systems have experienced an increase in demand due to the evolution and popularity of mobile devices and embedded systems. The development of applications for these systems imposes new challenges due to the necessity of adapting these applications both to the changes in the environment and to the resource-constrained devices (e.g. limited battery, memory and CPU) in which they run. These challenges are: (1) the same services are required by most applications for pervasive systems, and thus should be modeled as separate, ready-to-use (re)usable solutions; (2) services need to be customized to the requirements of applications, by generating different versions of the same service containing only the required functionality, and (3) the same service needs to be customized to the different devices in which a same application will run (e.g. with different operating systems, different memory and CPU capacities or different communication technologies). In order to consider all of the above challenges, in this paper we present a software product line approach that permits modelling the variability of these services using feature models, automatically generating different configurations of their software architecture depending on the particular requirements of each application. We use this approach to model typical services of pervasive systems, such as context-awareness and communication, and to evaluate the degree of variability, of reuse and of separation of concerns of these services.},
booktitle = {Proceedings of the 15th ACM SIGSOFT Symposium on Component Based Software Engineering},
pages = {115–124},
numpages = {10},
keywords = {spl, pervasive systems, context-awareness, cbse, aosd},
location = {Bertinoro, Italy},
series = {CBSE '12}
}

@inproceedings{10.1145/1147249.1147252,
author = {Kolb, Ronny and Muthig, Dirk},
title = {Making testing product lines more efficient by improving the testability of product line architectures},
year = {2006},
isbn = {1595934596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1147249.1147252},
doi = {10.1145/1147249.1147252},
abstract = {Product line engineering is a recent approach to software development that has shown to enable organizations to achieve significant reductions in development and maintenance cost as well as time-to-market of increasingly complex software systems. Yet, the testing process has not kept up with these reductions and the relative cost for testing product lines is actually becoming higher than in traditional single system development. Also, testing often cannot keep pace with accelerated development in product line engineering due to technical and organizational issues. This paper advocates that testing of product lines can be made more efficient and effective by considering testability already during architectural design. It explores the relationship between testability and product line architecture and discusses the importance of high testability for reducing product line testing effort and achieving required coverage criteria. The paper also outlines a systematic approach that will support product line organizations in improving and evaluating testability of product lines at the architectural level.},
booktitle = {Proceedings of the ISSTA 2006 Workshop on Role of Software Architecture for Testing and Analysis},
pages = {22–27},
numpages = {6},
keywords = {testing, testability, software product line, evaluation, design, architecture},
location = {Portland, Maine},
series = {ROSATEA '06}
}

@inproceedings{10.1145/3168365.3168374,
author = {Arcaini, Paolo and Gargantini, Angelo and Radavelli, Marco},
title = {An evolutionary process for product-driven updates of feature models},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168374},
doi = {10.1145/3168365.3168374},
abstract = {Feature models are a widely used modeling notation for variability and commonality management in software product line (SPL) engineering. In order to keep an SPL and its feature model aligned, feature models must be changed by including/excluding new features and products, either because faults in the model are found or to reflect the normal evolution of the SPL. The modification of the feature model able to satisfy these change requirements can be complex and error-prone. In this paper, we present a method that is able to automatically update a feature model in order to satisfy a given update request. Our method is based on an evolutionary algorithm and it iteratively applies structure-preserving mutations to the original model, until the model is completely updated. We evaluate the process on real-world feature models. Although our approach does not guarantee to completely update all possible feature models, empirical analysis shows that, on average, more than 80% of requested changes are applied.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {67–74},
numpages = {8},
keywords = {software product lines, search-based software engineering, mutation, feature models},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.5555/2075144.2075172,
author = {Reinhartz-Berger, Iris and Sturm, Arnon and Wand, Yair},
title = {External variability of software: classification and ontological foundations},
year = {2011},
isbn = {9783642246050},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software variability management deals with the adaptation of reusable artifacts, such as models, specifications, and code, for particular requirements. External variability, which refers to software functionality as visible to users, deserves a special attention as it is closely linked to requirements and hence to early development stages. Errors or inaccuracies in these stages are relatively inexpensive to detect and easy to correct, yet can lead to expensive outcomes if not corrected. Nevertheless, the analysis of external variability in the literature is done ad-hoc and requires improvement.In this paper we introduce a framework for classifying external variability types based on ontological principles. The framework defines the external view of software in terms of the behavior of the application domain. Behavior is formalized as state changes in response to external stimuli. Based on this view we classify the possible similarities and differences among applications and identify an integrated similarity measurement. We demonstrate the usage of this classification framework for feasibility studies in system development.},
booktitle = {Proceedings of the 30th International Conference on Conceptual Modeling},
pages = {275–289},
numpages = {15},
keywords = {variability management, software product line engineering, domain engineering, domain analysis},
location = {Brussels, Belgium},
series = {ER'11}
}

@article{10.1145/3428225,
author = {Shahin, Ramy and Chechik, Marsha},
title = {Automatic and efficient variability-aware lifting of functional programs},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428225},
doi = {10.1145/3428225},
abstract = {A software analysis is a computer program that takes some representation of a software product as input and produces some useful information about that product as output. A software product line encompasses many software product variants, and thus existing analyses can be applied to each of the product variations individually, but not to the entire product line as a whole. Enumerating all product variants and analyzing them one by one is usually intractable due to the combinatorial explosion of the number of product variants with respect to product line features. Several software analyses (e.g., type checkers, model checkers, data flow analyses) have been redesigned/re-implemented to support variability. This usually requires a lot of time and effort, and the variability-aware version of the analysis might have new errors/bugs that do not exist in the original one. Given an analysis program written in a functional language based on PCF, in this paper we present two approaches to transforming (lifting) it into a semantically equivalent variability-aware analysis. A light-weight approach (referred to as shallow lifting) wraps the analysis program into a variability-aware version, exploring all combinations of its input arguments. Deep lifting, on the other hand, is a program rewriting mechanism where the syntactic constructs of the input program are rewritten into their variability-aware counterparts. Compositionally this results in an efficient program semantically equivalent to the input program, modulo variability. We present the correctness criteria for functional program lifting, together with correctness proof sketches of shallow lifting. We evaluate our approach on a set of program analyses applied to the BusyBox C-language product line.},
journal = {Proc. ACM Program. Lang.},
month = nov,
articleno = {157},
numpages = {27},
keywords = {Variability-aware Programming, Software Product Lines, Program Rewriting, PCF, Lifting}
}

@inproceedings{10.1145/2897845.2897856,
author = {Meng, Guozhu and Xue, Yinxing and Mahinthan, Chandramohan and Narayanan, Annamalai and Liu, Yang and Zhang, Jie and Chen, Tieming},
title = {Mystique: Evolving Android Malware for Auditing Anti-Malware Tools},
year = {2016},
isbn = {9781450342339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897845.2897856},
doi = {10.1145/2897845.2897856},
abstract = {In the arms race of attackers and defenders, the defense is usually more challenging than the attack due to the unpredicted vulnerabilities and newly emerging attacks every day. Currently, most of existing malware detection solutions are individually proposed to address certain types of attacks or certain evasion techniques. Thus, it is desired to conduct a systematic investigation and evaluation of anti-malware solutions and tools based on different attacks and evasion techniques. In this paper, we first propose a meta model for Android malware to capture the common attack features and evasion features in the malware. Based on this model, we develop a framework, MYSTIQUE, to automatically generate malware covering four attack features and two evasion features, by adopting the software product line engineering approach. With the help of MYSTIQUE, we conduct experiments to 1) understand Android malware and the associated attack features as well as evasion techniques; 2) evaluate and compare the 57 off-the-shelf anti-malware tools, 9 academic solutions and 4 App market vetting processes in terms of accuracy in detecting attack features and capability in addressing evasion. Last but not least, we provide a benchmark of Android malware with proper labeling of contained attack and evasion features.},
booktitle = {Proceedings of the 11th ACM on Asia Conference on Computer and Communications Security},
pages = {365–376},
numpages = {12},
keywords = {malware generation, evolutionary algorithm, defense capability, android feature model},
location = {Xi'an, China},
series = {ASIA CCS '16}
}

@inproceedings{10.1109/ICSE-NIER.2019.00028,
author = {Trubiani, Catia and Apel, Sven},
title = {PLUS: performance learning for uncertainty of software},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2019.00028},
doi = {10.1109/ICSE-NIER.2019.00028},
abstract = {Uncertainty is particularly critical in software performance engineering when it relates to the values of important parameters such as workload, operational profile, and resource demand, because such parameters inevitably affect the overall system performance. Prior work focused on monitoring the performance characteristics of software systems while considering influence of configuration options. The problem of incorporating uncertainty as a first-class concept in the software development process to identify performance issues is still challenging. The PLUS (Performance Learning for Uncertainty of Software) approach aims at addressing these limitations by investigating the specification of a new class of performance models capturing how the different uncertainties underlying a software system affect its performance characteristics. The main goal of PLUS is to answer a fundamental question in the software performance engineering domain: How to model the variable configuration options (i.e., software and hardware resources) and their intrinsic uncertainties (e.g., resource demand, processor speed) to represent the performance characteristics of software systems? This way, software engineers are exposed to a quantitative evaluation of their systems that supports them in the task of identifying performance critical configurations along with their uncertainties.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {77–80},
numpages = {4},
keywords = {uncertainty, machine learning},
location = {Montreal, Quebec, Canada},
series = {ICSE-NIER '19}
}

@inproceedings{10.1145/3180155.3180257,
author = {Xue, Yinxing and Li, Yan-Fu},
title = {Multi-objective integer programming approaches for solving optimal feature selection problem: a new perspective on multi-objective optimization problems in SBSE},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180257},
doi = {10.1145/3180155.3180257},
abstract = {The optimal feature selection problem in software product line is typically addressed by the approaches based on Indicator-based Evolutionary Algorithm (IBEA). In this study we first expose the mathematical nature of this problem --- multi-objective binary integer linear programming. Then, we implement/propose three mathematical programming approaches to solve this problem at different scales. For small-scale problems (roughly less than 100 features), we implement two established approaches to find all exact solutions. For medium-to-large problems (roughly, more than 100 features), we propose one efficient approach that can generate a representation of the entire Pareto front in linear time complexity. The empirical results show that our proposed method can find significantly more non-dominated solutions in similar or less execution time, in comparison with IBEA and its recent enhancement (i.e., IBED that combines IBEA and Differential Evolution).},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1231–1242},
numpages = {12},
keywords = {multi-objective integer programming (MOIP), multi-objective optimization (MOO), optimal feature selection problem},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/2814228.2814229,
author = {Arzt, Steven and Nadi, Sarah and Ali, Karim and Bodden, Eric and Erdweg, Sebastian and Mezini, Mira},
title = {Towards secure integration of cryptographic software},
year = {2015},
isbn = {9781450336888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814228.2814229},
doi = {10.1145/2814228.2814229},
abstract = {While cryptography is now readily available to everyone and can, provably, protect private information from attackers, we still frequently hear about major data leakages, many of which are due to improper use of cryptographic mechanisms. The problem is that many application developers are not cryptographic experts. Even though high-quality cryptographic APIs are widely available, programmers often select the wrong algorithms or misuse APIs due to a lack of understanding. Such issues arise with both simple operations such as encryption as well as with complex secure communication protocols such as SSL. In this paper, we provide a long-term solution that helps application developers integrate cryptographic components correctly and securely by bridging the gap between cryptographers and application developers. Our solution consists of a software product line (with an underlying feature model) that automatically identifies the correct cryptographic algorithms to use, based on the developer's answers to high-level questions in non-expert terminology. Each feature (i.e., cryptographic algorithm) maps into corresponding Java code and a usage protocol describing API restrictions. By composing the user's selected features, we automatically synthesize a secure code blueprint and a usage protocol that corresponds to the selected usage scenario. Since the developer may change the application code over time, we use the usage protocols to statically analyze the program and ensure that the correct use of the API is not violated over time.},
booktitle = {2015 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (Onward!)},
pages = {1–13},
numpages = {13},
keywords = {typestate analysis, cryptography, Software product lines, API protocols},
location = {Pittsburgh, PA, USA},
series = {Onward! 2015}
}

@inproceedings{10.1145/3053600.3053619,
author = {Mangels, Tatiana and Murarasu, Alin and Oden, Forest and Fishkin, Alexey and Becker, Daniel},
title = {Efficient Analysis at Edge},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053619},
doi = {10.1145/3053600.3053619},
abstract = {Digitalization changes traditional business models by using digital technologies to improve existing offerings and to create new offerings. Current technological trends such as artificial intelligence, autonomous systems, and predictive maintenance are ideal candidate technologies to enable digitalization use cases. Often, these technologies rely on the availability of large amounts of data and the capability to process these data efficiently. In contrast to consumer markets, industrial products must fulfill higher non-functional requirements such as fast response times, 24/7 availability and stability, real-time processing, safety, or security requirements. As a consequence, processing capabilities -- ranging from multicore and manycores to even high end parallel clusters -- have to be exploited to achieve necessary performance and stability needs. In this paper, we introduce a Distributed Multicore Monitoring Framework (MoMo) which is a reference monitoring solution developed at Siemens Corporate Technology. It can be used to easily build efficient and stable diagnostic solutions which can help to understand the correctness, availability, reliability, and performance of large-scale distributed systems based on live data. Due to its small footprint MoMo can be used to analyze data directly at the data source which, for instance, can significantly reduce the network load. While MoMo's efficiency comes from the usage of multicore processors (CPUs) for running analysis in parallel, its usability is guaranteed by its capability to easily integrate with other monitoring frameworks and its usage of SPL - a domain-specific language which allows user to easily define diagnostic algorithms.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {85–90},
numpages = {6},
keywords = {parallel computing, monitoring, data analysis},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@inproceedings{10.5555/2820518.2820523,
author = {Hashimoto, Masatomo and Terai, Masaaki and Maeda, Toshiyuki and Minami, Kazuo},
title = {Extracting facts from performance tuning history of scientific applications for predicting effective optimization patterns},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {To improve performance of large-scale scientific applications, scientists or tuning experts make various empirical attempts to change compiler options, program parameters or even the syntactic structure of programs. Those attempts followed by performance evaluation are repeated until satisfactory results are obtained. The task of performance tuning requires a great deal of time and effort. On account of combinatorial explosion of possible attempts, scientists/tuning experts have a tendency to make decisions on what to be explored just based on their intuition or good sense of tuning. We advocate evidence-based performance tuning (EBT) that facilitates the use of database of facts extracted from tuning histories of applications to guide the exploration of the search space. However, in general, performance tuning is conducted as transient tasks without version control systems. Tuning histories may lack explicit facts about what kind of program transformation contributed to the better performance or even about the chronological order of the source code snapshots. For reconstructing the missing information, we employ a state-of-the-art fine-grained change pattern identification tool for inferring applied transformation patterns only from an unordered set of source code snapshots. The extracted facts are intended to be stored and queried for further data mining. This paper reports on experiments of tuning pattern identification followed by predictive model construction conducted for a few scientific applications tuned for the K supercomputer.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {13–23},
numpages = {11},
location = {Florence, Italy},
series = {MSR '15}
}

@article{10.1145/3292384.3292387,
author = {Brostr\"{o}m, Tom and Zhu, John and Robucci, Ryan and Younis, Mohamed},
title = {IoT boot integrity measuring and reporting},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {5},
url = {https://doi.org/10.1145/3292384.3292387},
doi = {10.1145/3292384.3292387},
abstract = {The current era can be characterized by the massive reliance on computing platforms in almost all domains, such as manufacturing, defense, healthcare, government. However, with the increased productivity, flexibility, and effectiveness that computers provide, comes the vulnerability to cyber-attacks where software, or even firmware, gets subtly modified by a hacker. The integration of a Trusted Platform Module (TPM) opts to tackle this issue by aiding in the detection of unauthorized modifications so that devices get remediation as needed. Nonetheless, the use of a TPM is impractical for resource-constrained devices due to power, space and cost limitations. With the recent proliferation of miniaturized devices along with the push towards the Internet-of Things (IoT) there is a need for a lightweight and practical alternative to the TPM. This paper proposes a cost-effective solution that incorporates modest amounts of integrated roots-of-trust logic and supports attestation of the integrity of the device's boot-up state. Our solution leverages crypto-acceleration modules found on many microprocessor and microcontroller based IoT devices nowadays, and introduces little additional overhead. The basic concepts have been validated through implementation on an SoC with an FPGA and a hard microcontroller. We report the validation results and highlight the involved tradeoffs.},
journal = {SIGBED Rev.},
month = nov,
pages = {14–21},
numpages = {8},
keywords = {trusted platform, security, measured boot, integrity, attestation, IoT}
}

@article{10.1145/3229096,
author = {Logre, Ivan and D\'{e}ry-Pinna, Anne-Marie},
title = {MDE in Support of Visualization Systems Design: a Multi-Staged Approach Tailored for Multiple Roles},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {EICS},
url = {https://doi.org/10.1145/3229096},
doi = {10.1145/3229096},
abstract = {Visualization systems such as dashboards are commonly used to analyze data and support users in their decision making, in communities as different as medical care, transport and software engineering. The increasing amount of data produced and continuous development of new visualizations exacerbate the difficulty of designing such dashboards, while the visualization need is broaden to specialist and non-specialist final users. In this context, we offer a multi-user approach, based on Model Driven Engineering (MDE). The idea is for the designer to express the visualization need by characterization, according to a given taxonomy. We provide a Domain Specific Language (DSL) to design the system and a Software Product Line (SPL) to capture the technological variability of visualization widgets. We performed a user study, using a software project management use case, to validate if dashboard users and designers are able to use a taxonomy to express their visualization need.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jun,
articleno = {14},
numpages = {17},
keywords = {visualization, meta-model, domain specific language}
}

@inproceedings{10.1145/3510466.3511274,
author = {Meixner, Kristof and Feichtinger, Kevin and Rabiser, Rick and Biffl, Stefan},
title = {Efficient Production Process Variability Exploration},
year = {2022},
isbn = {9781450396042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510466.3511274},
doi = {10.1145/3510466.3511274},
abstract = {Cyber-Physical Production Systems (CPPSs) manufacture highly-customizable products from a product family following a sequence of production steps. For a CPPS, basic planners design feasible production process sequences by arranging atomic production steps based on implicit domain knowledge. However, the manual design of production sequences is inefficient and hard to reproduce due to the large configuration space. In this paper, we introduce the Iterative Process Sequence Exploration (IPSE) approach that (i) elicits domain knowledge in an industrial variability artifact, using the Product-Process-Resource Domain-Specific Language (PPR–DSL); (ii) reduces configuration space size regarding structural product variability and behavioral process variability; and (iii) facilitates efficiently exploring the configuration space in a process decision model. For production process sequence design, IPSE is a first approach to combine structural and behavioral variability models. We investigated the feasibility of the IPSE in a study on a typical manufacturing work line in automotive production. We compare the IPSE to a traditional process sequence planning approach. Our study indicates IPSE to be more efficient than the traditional manual approach.},
booktitle = {Proceedings of the 16th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {14},
numpages = {9},
keywords = {Variability Modeling, Process Variability, Cyber-Physical Production System, Configuration Reduction.},
location = {Florence, Italy},
series = {VaMoS '22}
}

@inproceedings{10.1109/ASE.2015.16,
author = {Kowal, Matthias and Tschaikowski, Max and Tribastone, Mirco and Schaefer, Ina},
title = {Scaling size and parameter spaces in variability-aware software performance models},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.16},
doi = {10.1109/ASE.2015.16},
abstract = {In software performance engineering, what-if scenarios, architecture optimization, capacity planning, run-time adaptation, and uncertainty management of realistic models typically require the evaluation of many instances. Effective analysis is however hindered by two orthogonal sources of complexity. The first is the infamous problem of state space explosion---the analysis of a single model becomes intractable with its size. The second is due to massive parameter spaces to be explored, but such that computations cannot be reused across model instances. In this paper, we efficiently analyze many queuing models with the distinctive feature of more accurately capturing variability and uncertainty of execution rates by incorporating general (i.e., non-exponential) distributions. Applying product-line engineering methods, we consider a family of models generated by a core that evolves into concrete instances by applying simple delta operations affecting both the topology and the model's parameters. State explosion is tackled by turning to a scalable approximation based on ordinary differential equations. The entire model space is analyzed in a family-based fashion, i.e., at once using an efficient symbolic solution of a super-model that subsumes every concrete instance. Extensive numerical tests show that this is orders of magnitude faster than a naive instance-by-instance analysis.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {407–417},
numpages = {11},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1145/3493244.3493250,
author = {Wolfart, Daniele and Assun\c{c}\~{a}o, Wesley Klewerton Guez and Martinez, Jabier},
title = {Variability Debt: Characterization, Causes and Consequences},
year = {2021},
isbn = {9781450395533},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493244.3493250},
doi = {10.1145/3493244.3493250},
abstract = {Variability is an inherent property of software systems to create families of products dealing with needs of different customers and environments. However, some practices to manage variability may incur technical debt. For example, the use of opportunistic reuse strategies, e.g., clone-and-own, harms maintenance and evolution activities; or deciding to abandon variability management and deriving a single product with all the features might threaten system usability. These examples are common problems found in practice but, to the best of or knowledge, not properly investigated from the perspective of technical debt. To expand the knowledge on the research and practice of technical debt in the perspective of variability management, we report results of this phenomenon, which we defined as variability debt. Our work is based on 52 industrial case studies that report problems observed in the use of opportunistic reuse. The results show that variability debt is caused by business, operational and technical aspects; leads to complex maintenance, creates difficulties to customize and create new products, misuse of human resources, usability problems; and impacts artifacts along the whole life-cycle. Although some of these issues are investigated in the field of systematic variability management, e.g., software product lines, our contribution is to present them from a technical debt perspective to enrich and create synergies between the two fields. As additional contribution, we present a catalog of variability debts in the light of technical debts found in the literature.},
booktitle = {Proceedings of the XX Brazilian Symposium on Software Quality},
articleno = {17},
numpages = {10},
keywords = {Variability management, Variability Debt, Technical Debt, Software Product Lines},
location = {Virtual Event, Brazil},
series = {SBQS '21}
}

@inproceedings{10.1145/1960275.1960287,
author = {Figueiredo, Eduardo and Garcia, Alessandro and Maia, Marcelo and Ferreira, Gabriel and Nunes, Camila and Whittle, Jon},
title = {On the impact of crosscutting concern projection on code measurement},
year = {2011},
isbn = {9781450306058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1960275.1960287},
doi = {10.1145/1960275.1960287},
abstract = {Many concern metrics have been defined to quantify properties of crosscutting concerns, such as scattering, tangling, and dedication. To quantify these properties, concern metrics directly rely on the projection (assignment) of concerns into source code. Although concern identification tools have emerged over the last years, they are still rarely used in practice to support concern projection and, therefore, it is a task often performed manually. This means that the results of concern metrics are likely to be influenced by how accurately programmers assign concerns to code elements. Even though concern assignment is an important and long-standing problem in software engineering, its impact on accurate measures of crosscutting concerns has never been studied and quantified. This paper presents a series of 5 controlled experiments to quantify and analyse the impact of concern projection on crosscutting concern measures. A set of 80 participants from 4 different institutions projected 10 concern instances into the source code of two software systems. We analyse the accuracy of concern projections independently made by developers, and their impact on a set of 12 concern metrics. Our results suggest that: (i) programmers are conservative when projecting crosscutting concerns, (ii) all concern metrics suffer with such conservative behaviour, and (iii) fine-grained tangling measures are more sensitive to different concern projections than coarse-grained scattering metrics.},
booktitle = {Proceedings of the Tenth International Conference on Aspect-Oriented Software Development},
pages = {81–92},
numpages = {12},
keywords = {crosscutting concerns, concern projection, concern metrics},
location = {Porto de Galinhas, Brazil},
series = {AOSD '11}
}

@inproceedings{10.1145/2430502.2430507,
author = {Seidl, Christoph and A\ss{}mann, Uwe},
title = {Towards modeling and analyzing variability in evolving software ecosystems},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430507},
doi = {10.1145/2430502.2430507},
abstract = {A software ecosystem (SECO) encompasses a set of interdependent software systems where individual products are created by combining a common software platform with variable extensions. Examples are the SECOs surrounding Eclipse or Android. Due to independent release cycles of the multiple vendors for platform and extensions, SECOs are evolving frequently. This makes it hard to get a concise impression of the structure of a SECO and its variable artifacts during a given period of time. We contribute a metamodel to capture the variability in an arbitrary SECO and its evolution based on the notion of real time. We further present a procedure to create temporal perspectives on the SECO. Additionally, we provide means to analyze evolution of variability in between explicit releases of the platform, e.g., in accordance with the different release cycles of individual extensions. We demonstrate feasibility of our approach by modeling a part of the Eclipse SECO over a period of three years.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {3},
numpages = {8},
keywords = {variability, temporal perspective, technical ecosystem modeling notation (Tecmo), software product line (SPL), software ecosystem (SECO), evolution, analysis},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@inproceedings{10.1145/2866614.2866617,
author = {Bezerra, Carla I. M. and Monteiro, Jos\'{e} Maria and Andrade, Rossana M. C. and Rocha, Lincoln S.},
title = {Analyzing the Feature Models Maintainability over their Evolution Process: An Exploratory Study},
year = {2016},
isbn = {9781450340199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2866614.2866617},
doi = {10.1145/2866614.2866617},
abstract = {The feature model is one of the most important artifact of a Software Product Line (SPL). It is built in the early stages of SPL development and describes the main features and relationships. The feature model evolves according to the evolution of the SPL. Thus, it is important to build maintainable feature models. In this scenario, measures have been proven useful in the maintainability evaluation of the feature models. This paper presents an exploratory study on the impact of feature models maintainability over the SPL evolution process. In order to support this analysis, we built a dataset containing a compiled set of 21 maintainability structural measures extracted from 16 feature models and respective versions. Although not conclusive, our findings indicate that the feature models maintainability tends to decrease as it evolves. We also identified the most common changes performed in a feature model during its evolution process.},
booktitle = {Proceedings of the 10th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {17–24},
numpages = {8},
keywords = {Maintainability, Feature Model, Evolution},
location = {Salvador, Brazil},
series = {VaMoS '16}
}

@inproceedings{10.1145/2973839.2973842,
author = {Lima, Crescencio and Chavez, Christina},
title = {A Systematic Review on Metamodels to Support Product Line Architecture Design},
year = {2016},
isbn = {9781450342018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2973839.2973842},
doi = {10.1145/2973839.2973842},
abstract = {Product Line Architecture (PLA) design is a key activity for developing successful Software Product Line (SPL) projects. PLA design is a difficult task, mostly due to the complexity of the software systems that SPLs deal with, and their variabilities. Metamodels have been used to support the representation of assets that compose a PLA, SPL variability and the relationships among them. The goal of this study is to characterize the use of metamodeling on PLA design, aiming to identify the main characteristics of metamodels, the elements used for PLA and variability representation and trace the evolution of metamodels. We conducted a systematic literature review to identify the primary studies on the use of metamodels in PLA Design. Thirty-five studies that proposed metamodels to support PLA design were selected. The review main findings are: (i) it is difficult to identify the existence of research trends because the number of publication varies and metamodels lack standardization; (ii) several metamodels support feature representation; (iii) the majority of studies addressed variability representation with variation points in UML diagrams; and, (iv) five evolution lines that describe how metamodels evolved over the years were identified.},
booktitle = {Proceedings of the XXX Brazilian Symposium on Software Engineering},
pages = {13–22},
numpages = {10},
keywords = {Variability, Systematic Literature Review, Software Product Lines, Product Line Architecture, Metamodels},
location = {Maring\'{a}, Brazil},
series = {SBES '16}
}

@inproceedings{10.1145/3698576.3698764,
author = {Castes, Charly and Kalani, Neelu S. and Saltovskaia, Sofia and Terrier, No\'{e} and Wilkinson, Abel Vexina and Bugnion, Edouard},
title = {Kicking the Firmware Out of the TCB with the Miralis Virtual Firmware Monitor},
year = {2024},
isbn = {9798400713019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698576.3698764},
doi = {10.1145/3698576.3698764},
abstract = {The role of firmware has evolved over the past decades. Not only is firmware responsible for discovering, initializing, and monitoring the system's chipset, board, and devices, but it also acts as the root of trust and plays a leading role in confidential computing. Yet vulnerabilities in the non-security critical part of the firmware have repeatedly led to the compromise of the core TCB of the system.We propose an alternative architecture that excludes the non-security critical part of the firmware from the TCB by isolating it within a virtual machine with the introduction of a simple and verifiable virtual firmware monitor.We present the design of Miralis, the first virtual firmware monitor. Miralis can successfully boot Linux with a virtualized OpenSBI on RISC-V. We demonstrate through construction that the M-mode of RISC-V architecture meets the Popek &amp; Golberg criteria for classical virtualization. Our initial evaluation shows that Miralis removes vendor-provided, platform-specific firmware from the TCB with no measurable impact on boot and run-time performance.},
booktitle = {Proceedings of the 2nd Workshop on Kernel Isolation, Safety and Verification},
pages = {8–15},
numpages = {8},
location = {Austin, TX, USA},
series = {KISV '24}
}

@inproceedings{10.1145/3278122.3278130,
author = {Ruland, Sebastian and Luthmann, Lars and B\"{u}rdek, Johannes and Lity, Sascha and Th\"{u}m, Thomas and Lochau, Malte and Ribeiro, M\'{a}rcio},
title = {Measuring effectiveness of sample-based product-line testing},
year = {2018},
isbn = {9781450360456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278122.3278130},
doi = {10.1145/3278122.3278130},
abstract = {Recent research on quality assurance (QA) of configurable software systems (e.g., software product lines) proposes different analysis strategies to cope with the inherent complexity caused by the well-known combinatorial-explosion problem. Those strategies aim at improving efficiency of QA techniques like software testing as compared to brute-force configuration-by-configuration analysis. Sampling constitutes one of the most established strategies, defining criteria for selecting a drastically reduced, yet sufficiently diverse subset of software configurations considered during QA. However, finding generally accepted measures for assessing the impact of sample-based analysis on the effectiveness of QA techniques is still an open issue. We address this problem by lifting concepts from single-software mutation testing to configurable software. Our framework incorporates a rich collection of mutation operators for product lines implemented in C to measure mutation scores of samples, including a novel family-based technique for product-line mutation detection. Our experimental results gained from applying our tool implementation to a collection of subject systems confirms the widely-accepted assumption that pairwise sampling constitutes the most reasonable efficiency/effectiveness trade-off for sample-based product-line testing.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {119–133},
numpages = {15},
keywords = {Software Product Lines, Sample-Based Testing, Mutation Testing},
location = {Boston, MA, USA},
series = {GPCE 2018}
}

@inproceedings{10.5555/2666719.2666730,
author = {Sayyad, Abdel Salam and Ammar, Hany and Menzies, Tim},
title = {Software feature model recommendations using data mining},
year = {2012},
isbn = {9781467317597},
publisher = {IEEE Press},
abstract = {Feature Models are popular tools for describing software product lines. Analysis of feature models has traditionally focused on consistency checking (yielding a yes/no answer) and product selection assistance, interactive or offline. In this paper, we describe a novel approach to identify the most critical decisions in product selection/configuration by taking advantage of a large pool of randomly generated, generally inconsistent, product variants. Range Ranking, a data mining technique, is utilized to single out the most critical design choices, reducing the job of the human designer to making less consequential decisions. A large feature model is used as a case study; we show preliminary results of the new approach to illustrate its usefulness for practical product derivation.},
booktitle = {Proceedings of the Third International Workshop on Recommendation Systems for Software Engineering},
pages = {47–51},
numpages = {5},
keywords = {range ranking, feature models, design decisions},
location = {Zurich, Switzerland},
series = {RSSE '12}
}

@inproceedings{10.1145/3658644.3670358,
author = {Zhang, Tianfang and Ji, Qiufan and Ye, Zhengkun and Akanda, Md Mojibur Rahman Redoy and Mahdad, Ahmed Tanvir and Shi, Cong and Wang, Yan and Saxena, Nitesh and Chen, Yingying},
title = {SAFARI: Speech-Associated Facial Authentication for AR/VR Settings via Robust VIbration Signatures},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3670358},
doi = {10.1145/3658644.3670358},
abstract = {In AR/VR devices, the voice interface, serving as one of the primary AR/VR control mechanisms, enables users to interact naturally using speeches (voice commands) for accessing data, controlling applications, and engaging in remote communication/meetings. Voice authentication can be adopted to protect against unauthorized speech inputs. However, existing voice authentication mechanisms are usually susceptible to voice spoofing attacks and are unreliable under the variations of phonetic content. In this work, we propose SAFARI, a spoofing-resistant and text-independent speech authentication system that can be seamlessly integrated into AR/VR voice interfaces. The key idea is to elicit phonetic-invariant biometrics from the facial muscle vibrations upon the headset. During speech production, a user's facial muscles are deformed for articulating phoneme sounds. The facial deformations associated with the phonemes are referred to as visemes. They carry rich biometrics of the wearer's muscles, tissue, and bones, which can propagate through the head and vibrate the headset. SAFARI aims to derive reliable facial biometrics from the viseme-associated facial vibrations captured by the AR/VR motion sensors. Particularly, it identifies the vibration data segments that contain rich viseme patterns (prominent visemes) less susceptible to phonetic variations. Based on the prominent visemes, SAFARI learns on the correlations among facial vibrations of different frequencies to extract biometric representations invariant to the phonetic context. The key advantages of SAFARI are that it is suitable for commodity AR/VR headsets (no additional sensors) and is resistant to voice spoofing attacks as the conductive property of the facial vibrations prevents biometric disclosure via the air media or the audio channel. To mitigate the impacts of body motions in AR/VR scenarios, we also design a generative diffusion model trained to reconstruct the viseme patterns from the data distorted by motion artifacts. We conduct extensive experiments with two representative AR/VR headsets and 35 users under various usage and attack settings. We demonstrate that SAFARI can achieve over 96% true positive rate on verifying legitimate users while successfully rejecting different kinds of spoofing attacks with over 97% true negative rates.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {153–167},
numpages = {15},
keywords = {AR/VR headsets, authentication, speech vibrations},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@inproceedings{10.1145/974044.974089,
author = {Wu, Xiuping and Woodside, Murray},
title = {Performance modeling from software components},
year = {2004},
isbn = {1581136730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/974044.974089},
doi = {10.1145/974044.974089},
abstract = {When software products are assembled from pre-defined components, performance prediction should be based on the components also. This supports rapid model-building, using previously calibrated sub-models or "performance components", in sync with the construction of the product. The specification of a performance component must be tied closely to the software component specification, but it also includes performance related parameters (describing workload characteristics and demands), and it abstracts the behaviour of the component in various ways (for reasons related to practical factors in performance analysis). A useful set of abstractions and parameters are already defined for layered performance modeling. This work extends them to accommodate software components, using a new XML-based language called Component-Based Modeling Language (CBML). With CBML, compatible components can be inserted into slots provided in a hierarchical component specification based on the UML component model.},
booktitle = {Proceedings of the 4th International Workshop on Software and Performance},
pages = {290–301},
numpages = {12},
keywords = {submodel, software performance, software component, performance prediction, layered queue model, generative programming, LQN, CBML},
location = {Redwood Shores, California},
series = {WOSP '04}
}

@inproceedings{10.1145/2790282.2790292,
author = {Meng, Lingchuan and Johnson, Jeremy},
title = {High performance implementation of the inverse TFT},
year = {2015},
isbn = {9781450335997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2790282.2790292},
doi = {10.1145/2790282.2790292},
abstract = {The inverse truncated Fourier transform (ITFT) is a key component in the fast polynomial and large integer algorithms introduced by van der Hoeven. This paper reports a high performance implementation of the ITFT which poses additional challenges compared to that of the forward transform. A general-radix variant of the ITFT algorithm is developed to allow the implementation to automatically adapt to the memory hierarchy. Then a parallel ITFT algorithm is developed that trades off small arithmetic cost for full vectorization and improved multi-threaded parallelism. The algorithms are automatically generated and tuned to produce an arbitrary-size ITFT library. The new algorithms and the implementation smooths out the staircase performance associated with power-of-two modular FFT implementations, and provide significant performance improvement over zero-padding approaches even when high-performance FFT libraries are used.},
booktitle = {Proceedings of the 2015 International Workshop on Parallel Symbolic Computation},
pages = {87–94},
numpages = {8},
location = {Bath, United Kingdom},
series = {PASCO '15}
}

@inproceedings{10.1145/2935323.2935326,
author = {Henriksen, Troels and Larsen, Ken Friis and Oancea, Cosmin E.},
title = {Design and GPGPU performance of Futhark's redomap construct},
year = {2016},
isbn = {9781450343848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935323.2935326},
doi = {10.1145/2935323.2935326},
abstract = {This paper presents and evaluates a novel second-order operator, named 'redomap', that stems from 'map'-'reduce' compositions in the context of the purely-functional array language Futhark, which is aimed at efficient GPGPU execution. Main contributions are: First, we demonstrate an aggressive fusion technique that is centered on the 'redomap' operator. Second, we present a compilation technique for 'redomap' that efficiently sequentializes the excess parallelism and ensures coalesced access to global memory, even for non-commutative 'reduce' operators. Third, a detailed performance evaluation shows that Futhark's automatically generated code matches or exceeds performance of hand-tuned Thrust code. Our evaluation infrastructure is publicly available and we encourage replication and verification of our results.},
booktitle = {Proceedings of the 3rd ACM SIGPLAN International Workshop on Libraries, Languages, and Compilers for Array Programming},
pages = {17–24},
numpages = {8},
keywords = {map-reduce, functional language, autoparallelization, GPGPU},
location = {Santa Barbara, CA, USA},
series = {ARRAY 2016}
}

@inproceedings{10.1145/2889160.2889257,
author = {Fischer, Stefan},
title = {Reducing the test effort of variability-rich systems by using feature interaction knowledge and variability-aware source code analysis},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889257},
doi = {10.1145/2889160.2889257},
abstract = {To keep up with the growing demand for customized software solutions that are tailored to specific customer requirements, techniques like Software Product Line Engineering (SPLE) or the more ad-hoc clone-and-own (where engineers do not build each product from anew, but instead maximize the reuse of the available assets in building product families) have been devised. However testing such highly variable software systems has proven challenging. To improve this, the goal of this doctoral research plan is to propose an automated approach to reduce the effort for testing, while improving its effectiveness. The proposed approach aims to detect code parts that have to be considered when devising a test suite by employing source code analysis and test execution monitoring, which we argue have not been used to their full potential in this context.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {855–858},
numpages = {4},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/3460120.3485389,
author = {Ji, Xiaoyu and Zhang, Juchuan and Jiang, Shui and Li, Jishen and Xu, Wenyuan},
title = {CapSpeaker: Injecting Voices to Microphones via Capacitors},
year = {2021},
isbn = {9781450384544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460120.3485389},
doi = {10.1145/3460120.3485389},
abstract = {Voice assistants can be manipulated by various malicious voice commands, yet existing attacks require a nearby speaker to play the attack commands. In this paper, we show that even when no speakers are available, we can play malicious commands by utilizing the capacitors inside electronic devices, i.e., we convert capacitors into speakers and call it CapSpeaker. Essentially, capacitors can emit acoustic noises due to the inverse piezoelectric effect, i.e., varying the voltage across a capacitor can make it vibrate and thus emit acoustic noises. Forcing capacitors to play malicious voice commands is challenging because (1) the frequency responses of capacitors as speakers have poor performance in the range of audible voices, and (2) we have no direct control over the voltage across capacitors to manipulate their emitting sounds. To overcome the challenges, we use a PWM-based modulation scheme to embed the malicious audio onto a high-frequency carrier, e.g., above 20 kHz, and we create malware that can induce the right voltage across the capacitors such that CapSpeaker plays the chosen malicious commands. We conducted extensive experiments with 2 LED lamps (a modified one and a commercial one) and 5 victim devices (iPhone 4s, iPad mini 5, Huawei Nova 5i, etc.). Evaluation results demonstrate that CapSpeaker is feasible at a distance up to 10.5 cm, triggering a smartphone to receive voice commands, e.g., "open the door''.},
booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1915–1929},
numpages = {15},
keywords = {voice injection, malicious voice commands, iot security, capacitor sounds, asr security},
location = {Virtual Event, Republic of Korea},
series = {CCS '21}
}

@inproceedings{10.1145/1159733.1159762,
author = {Denger, Christian and Kolb, Ronny},
title = {Testing and inspecting reusable product line components: first empirical results},
year = {2006},
isbn = {1595932186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1159733.1159762},
doi = {10.1145/1159733.1159762},
abstract = {In recent years, product line development has increasingly received attention in industry as it enables software-developing organizations to reduce both cost and time of developing and maintaining increasingly complex systems as well as to address the demands for individually customized products. Successful product line development requires high quality of reusable artifacts in order to achieve the promised benefits. The unique issues of quality assurance in the context of systematic reuse, however, have not been quantitatively investigated so far. This paper describes a first empirical study comparing the two defect detection techniques, code inspections and functional testing, in the context of product line development. The primary goal of the study was to initially investigate the defect finding potential of the techniques on reusable software components with common and variant features. The major findings of the study are that the two techniques identified different types of defects on variants of a reusable component. Inspections are on average 66.39% more effective and need on average 36.84% less effort to detect a defect We found that both the testing and inspection techniques applied in the experiment were ineffective in identifying variant-specific defects. Overall, the results indicate that the standard quality assurance techniques seem to be insufficient to address special characteristics of reusable components.},
booktitle = {Proceedings of the 2006 ACM/IEEE International Symposium on Empirical Software Engineering},
pages = {184–193},
numpages = {10},
keywords = {software product line, reusable components, quality assurance, inspection, functional testing, controlled experiment},
location = {Rio de Janeiro, Brazil},
series = {ISESE '06}
}

@inproceedings{10.5555/2050655.2050659,
author = {Drago, Mauro Luigi and Ghezzi, Carlo and Mirandola, Raffaela},
title = {Towards quality driven exploration of model transformation spaces},
year = {2011},
isbn = {9783642244841},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Verifying that a software system has certain nonfunctional properties is a primary concern in many engineering fields. Although several model-driven approaches exist to predict quality attributes from system models, they still lack the proper level of automation envisioned by Model Driven Software Development. When a potential issue concerning non-functional properties is discovered, the identification of a solution is still entirely up to the engineer and to his/her experience. This paper presents QVT-Rational, our multi-modeling solution to automate the detection-solution loop. We leverage and extend existing model transformation techniques with constructs to elicit the space of the alternative solutions and to bind quality properties to them. Our framework is highly customizable, it supports the definition of nonfunctional requirements and provides an engine to automatically explore the solution space. We evaluate our approach by applying it to two well-known software engineering problems -- Object-Relational Mapping and components allocation -- and by showing how several solutions that satisfy given performance requirements can be automatically identified.},
booktitle = {Proceedings of the 14th International Conference on Model Driven Engineering Languages and Systems},
pages = {2–16},
numpages = {15},
keywords = {model transformations, feedback provisioning},
location = {Wellington, New Zealand},
series = {MODELS'11}
}

@article{10.1145/3534526,
author = {Jiang, Liuyue and Tran, Nguyen Khoi and Ali Babar, Muhammad},
title = {Mod2Dash: A Framework for Model-Driven Dashboards Generation},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {EICS},
url = {https://doi.org/10.1145/3534526},
doi = {10.1145/3534526},
abstract = {The construction of an interactive dashboard involves deciding on what information to present and how to display it and implementing those design decisions to create an operational dashboard. Traditionally, a dashboard's design is implied in the deployed dashboard rather captured explicitly as a digital artifact, preventing it from being backed up, version-controlled, and shared. Moreover, practitioners have to implement this implicit design manually by coding or configuring it on a dashboard platform. This paper proposes Mod2Dash, a software framework that enables practitioners to capture their dashboard designs as models and generate operational dashboards automatically from these models. The framework also provides a GUI-driven customization approach for practitioners to fine-tune the auto-generated dashboards and update their models. With these abilities, Mod2Dash enables practitioners to rapidly prototype and deploy dashboards for both operational and research purposes. We evaluated the framework's effectiveness in a case study on cyber security visualization for decision support. A proof-of-concept of Mod2Dash was employed to model and reconstruct 31 diverse real-world cyber security dashboards. A human-assisted comparison between the Mod2Dash-generated dashboards and the baseline dashboards shows a close matching, indicating the framework's effectiveness for real-world scenarios.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jun,
articleno = {172},
numpages = {28},
keywords = {visualization specification, visual data analytics, model-driven dashboard, decision making, data visualization, cyber situational awareness, big data analytics}
}

@inproceedings{10.1145/3141848.3141853,
author = {Schuster, Sven and Seidl, Christoph and Schaefer, Ina},
title = {Towards a development process for maturing Delta-oriented software product lines},
year = {2017},
isbn = {9781450355186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141848.3141853},
doi = {10.1145/3141848.3141853},
abstract = {A Software Product Line (SPL) exploits reuse-in-the-large to enable customization by explicitly modeling commonalities and variabilities of closely related software systems. Delta-Oriented Programming (DOP) is a flexible implementation approach to SPL engineering, which transforms an existing core product to another desired product by applying transformation operations. By capturing product alterations related to configurable functionality within delta modules, DOP closely resembles a natural process of software development, which proves beneficial in early stages of development. However, increasing complexity for a growing SPL in later development stages caused by the invasiveness of DOP drastically impairs maintenance and extensibility. Hence, a process utilizing the invasiveness of DOP in early development stages and restricting it in later stages would allow developers to mature growing delta-oriented SPLs. Moreover, ever-increasing complexity requires means to migrate into less invasive development approaches that are more suited for large-scale configurable applications. To this end, we propose a development process for delta-oriented SPLs including explicit variability points, metrics and refactorings as well as a semi-automatic reengineering of a delta-oriented SPL into a development approach based on blackbox-components. In this paper, we sketch this development process with its constituents and point out required research essential for successfully maturing a delta-oriented SPL.},
booktitle = {Proceedings of the 8th ACM SIGPLAN International Workshop on Feature-Oriented Software Development},
pages = {41–50},
numpages = {10},
keywords = {Software Product Lines, Delta-Oriented Programming},
location = {Vancouver, BC, Canada},
series = {FOSD 2017}
}

@inproceedings{10.1145/3471621.3471855,
author = {Zhang, Yangyong and Arora, Sunpreet and Shirvanian, Maliheh and Huang, Jianwei and Gu, Guofei},
title = {Practical Speech Re-use Prevention in Voice-driven Services},
year = {2021},
isbn = {9781450390583},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3471621.3471855},
doi = {10.1145/3471621.3471855},
abstract = {Voice-driven services (VDS) are being used in a variety of applications ranging from smart home control to payments using digital assistants. The input to such services is often captured via an open voice channel, e.g., using a microphone, in an unsupervised setting. One of the key operational security requirements in such setting is the freshness of the input speech. We present AEOLUS, a security overlay that proactively embeds a dynamic acoustic nonce at the time of user interaction, and detects the presence of the embedded nonce in the recorded speech to ensure freshness. We demonstrate that acoustic nonce can (i) be reliably embedded and retrieved, and (ii) be non-disruptive (and even imperceptible) to a VDS user. Optimal parameters (acoustic nonce’s operating frequency, amplitude, and bitrate) are determined for (i) and (ii) from a practical perspective. Experimental results show that AEOLUS yields 0.5% FRR at 0% FAR for speech re-use prevention upto a distance of 4 meters in three real-world environments with different background noise levels. We also conduct a user study with 120 participants, which shows that the acoustic nonce does not degrade overall user experience for 94.16% of speech samples, on average, in these environments. AEOLUS can therefore be used in practice to prevent speech re-use and ensure the freshness of speech input.},
booktitle = {Proceedings of the 24th International Symposium on Research in Attacks, Intrusions and Defenses},
pages = {282–295},
numpages = {14},
keywords = {voice-driven service, voice assistant security, replay attacks, nonce embedding},
location = {San Sebastian, Spain},
series = {RAID '21}
}

@inproceedings{10.1145/3297280.3297511,
author = {Allian, Ana Paula and Sena, Bruno and Nakagawa, Elisa Yumi},
title = {Evaluating variability at the software architecture level: an overview},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297511},
doi = {10.1145/3297280.3297511},
abstract = {Software architecture are designed for developing software systems needed for a diverse of business goals. Consequently, architecture has to deal with a significant amount of variability in functionality and quality attributes to create different products. Due to this variability, the evaluation in software architectures is much more complex, as different alternatives of systems might be developed leading to an expensive and time consuming task. Several methods and techniques have been proposed to evaluate product line architectures (PLAs) aiming to asses whether or not the architecture will lead to the desired quality attributes. However, there is little consensus on the existing evaluations methods is most suitable for evaluating variability in software architectures, instead of only considering PLAs. Understanding and explicitly evaluating variations in architectures is a cost-effective way of mitigating substantial risk to organizations and their software systems. Therefore, the main contribution of this research work is to present the state of the art about means for evaluating software architectures (including, PLAs, software architectures, reference and enterprise architectures) that contain variability information. We conducted a Systematic Mapping Study (SMS) to provide an overview and insight to practitioners about the most relevant techniques and methods developed for this evaluation. Results indicate that most evaluation techniques assess variability as a quality attribute in PLAs through scenario-based; however, little is known about their real effectiveness as most studies present gaps and lack of evaluation, which difficult the usage of such techniques in an industrial environment.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {2354–2361},
numpages = {8},
keywords = {evaluation, software architecture, software variability, systematic mapping study},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1109/ASE51524.2021.9678707,
author = {Mannion, Mike and Kaindl, Hermann},
title = {Enhancing requirements reuse through automated similarity matching},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678707},
doi = {10.1109/ASE51524.2021.9678707},
abstract = {Several socio-economic trends are driving customer demands towards individualization. Many suppliers are responding by offering supplier-led software product design customization choices ("mass customization"). Some are also offering customer-led software product design choices ("mass personalization"). This tutorial introduces these concepts and explores the implications for software product line development. One particular technical challenge is being able to respond to and manage at scale the increasing variety of common, supplier-led and customer-led features. We will discuss two different approaches to address this challenge. One is grounded in feature modelling, the other in case-based reasoning, where the latter relies on measuring similarities. We will then describe a specific product similarity evaluation process in which a product configured from a product line feature model is represented as a weighted binary string, the overall similarity between products is compared using a binary string metric, and the significance of individual feature combinations for product similarity can be explored by modifying the weights. We will illustrate our ideas with mobile phone worked examples, and discuss some of the benefits and limitations of this approach.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {4},
numpages = {1},
location = {Melbourne, Australia},
series = {ASE '21}
}

@article{10.1145/1101530.1101538,
author = {Brungart, Douglas S. and Simpson, Brian D.},
title = {Optimizing the spatial configuration of a seven-talker speech display},
year = {2005},
issue_date = {October 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {1544-3558},
url = {https://doi.org/10.1145/1101530.1101538},
doi = {10.1145/1101530.1101538},
abstract = {Although there is substantial evidence that performance in multitalker listening tasks can be improved by spatially separating the apparent locations of the competing talkers, very little effort has been made to determine the best locations and presentation levels for the talkers in a multichannel speech display. In this experiment, a call sign based color and number identification task was used to evaluate the effectiveness of three different spatial configurations and two different level normalization schemes in a seven-channel binaural speech display. When only two spatially adjacent channels of the seven-channel system were active, overall performance was substantially better with a geometrically spaced spatial configuration (with far-field talkers at −90°, −30°, −10°, 0°, +10°, +30°, and +90° azimuth) or a hybrid near-far configuration (with far-field talkers at −90°, −30°, 0°, +30°, and +90° azimuth and near-field talkers at ±90°) than with a more conventional linearly spaced configuration (with far-field talkers at −90°, −60°, −30°, 0°, +30°, +60°, and +90° azimuth). When all seven channels were active, performance was generally better with a “better-ear” normalization scheme that equalized the levels of the talkers in the more intense ear than with a default normalization scheme that equalized the levels of the talkers at the center of the head. The best overall performance in the seven-talker task occurred when the hybrid near-far spatial configuration was combined with the better-ear normalization scheme. This combination resulted in a 20% increase in the number of correct identifications relative to the baseline condition with linearly spaced talker locations and no level normalization. Although this is a relatively modest improvement, it should be noted that it could be achieved at little or no cost simply by reconfiguring the HRTFs used in a multitalker speech display.},
journal = {ACM Trans. Appl. Percept.},
month = oct,
pages = {430–436},
numpages = {7},
keywords = {informational masking, Cocktail party effect}
}

@article{10.5555/3294043.3294045,
author = {Lee, Hye-jin and Lee, Katie Ka-hyun and Choi, Junho},
title = {A structural model for unity of experience: connecting user experience, customer experience, and brand experience},
year = {2018},
issue_date = {November 2018},
publisher = {Usability Professionals' Association},
address = {Bloomingdale, IL},
volume = {14},
number = {1},
abstract = {Understanding customer experience from a holistic perspective requires examination of user experience in the context of marketing and branding. This study attempts to underpin the effects of UX on brand equity by developing and verifying a conceptual framework that connects user experience (UX), customer experience (CX), and brand experience (BX). A structural equation modeling test using data from smartphone users verified the effects of UX on brand equity mediated by CX. In the UX dimension, usability had a strong effect on brand equity, and affect and user value had an effect on customer experience. As a mediator, customer experience had an impact on brand equity with a high path weight. By implementing UX strategies that cohere with management strategies, companies can establish a high level of consumer perception of customer experience and brand value. The results and analyses of this research can help businesses establish a strategy for examining which element of UX is related to CX and BX.},
journal = {J. Usability Studies},
month = nov,
pages = {8–34},
numpages = {27},
keywords = {user experience, usability, customer experience, brand equity, affect}
}

@inproceedings{10.1145/3634737.3637643,
author = {R\"{o}ckl, Jonas and Bernsdorf, Nils and M\"{u}ller, Tilo},
title = {TeeFilter: High-Assurance Network Filtering Engine for High-End IoT and Edge Devices based on TEEs},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634737.3637643},
doi = {10.1145/3634737.3637643},
abstract = {Large botnets like Mirai, with 600,000 infected devices, prove that cyber criminals have recognized the potential of attacks against the fast-growing Internet of Things. Moreover, recent critical vulnerabilities like Ripple20 and Amnesia:33 show that taking over a remote system via the network is a real threat. Alarmingly, modern strains of malware rely on exploiting such vulnerabilities to spread, with an increasing tendency. Hence, effective techniques to mitigate the consequences of modern IoT malware are necessary.To that end, we propose TeeFilter, a novel network filtering engine that allows manufacturers and operators of IoT devices to restrict the network traffic of their devices. By selectively executing parts of the network stack in a Trusted Execution Environment, TeeFilter remains untampered even if the operating system is compromised. The operators can specify filtering rules in an LLVM-compatible programming language and compile them into eBPF code. Subsequently, TeeFilter can load and enforce the rules.We formally verify the majority of TeeFilter for correctness and memory safety to eradicate whole classes of vulnerabilities and prototype our system on real hardware to show that the network overhead is negligible. Therefore, we believe that our system is an impactful step to enhance the resiliency of future IoT infrastructure.},
booktitle = {Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
pages = {1568–1583},
numpages = {16},
keywords = {ARM TrustZone, traffic filtering, ethernet drivers, eBPF},
location = {Singapore, Singapore},
series = {ASIA CCS '24}
}

@inproceedings{10.1145/2554850.2554874,
author = {Ziadi, Tewfik and Henard, Christopher and Papadakis, Mike and Ziane, Mikal and Le Traon, Yves},
title = {Towards a language-independent approach for reverse-engineering of software product lines},
year = {2014},
isbn = {9781450324694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554850.2554874},
doi = {10.1145/2554850.2554874},
abstract = {Common industrial practices lead to the development of similar software products. These products are usually managed in an ad-hoc way which gradually results in a low product quality. To overcome this problem, it is essential to migrate these products into a Software Product Line (SPL). Towards this direction, this paper proposes a language-independent approach capable of reverse-engineering an SPL from the source code of product variants. A prototype tool and a case study show the feasibility and the practicality of the proposed approach.},
booktitle = {Proceedings of the 29th Annual ACM Symposium on Applied Computing},
pages = {1064–1071},
numpages = {8},
keywords = {software product lines, reverse-engineering},
location = {Gyeongju, Republic of Korea},
series = {SAC '14}
}

@inproceedings{10.1145/3229345.3229404,
author = {Ermel, Guilherme and Farias, Kleinner and Gon\c{c}ales, Lucian Jos\'{e} and Bischoff, Vinicius},
title = {Supporting the Composition of UML Component Diagrams},
year = {2018},
isbn = {9781450365598},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229345.3229404},
doi = {10.1145/3229345.3229404},
abstract = {Fast-changing business environments have become enterprise information systems more heterogeneous and complex. This extreme uncertainty leads to continuous development and integration of architecturally relevant components developed in parallel. In this context, the proper composition of such components is critical to reduce the development effort. However, the current composition tools are still considered imprecise and inflexible for this purpose. This article, therefore, proposes MoCoTo, a model composition tool to support the integration of UML component diagrams. It exploits equivalence relationships between the UML component elements to improve integration precision and accuracy. Developers and system analysts can benefit from using MoCoTo when evolving or maintaining architectural models of enterprise information systems. MoCoTo was implemented as an Eclipse platform plug-in. The tool was used to support the composition of architectural components in three realistic evolution scenarios of a Software Product Line. Our preliminary results indicated that MoCoTo was able to integrate architectural models represented with UML component diagrams. The metrics used to evaluate the effectiveness of the proposed tool (i.e., precision, recall and F-measure) presented values higher than 0.6 in all evaluation scenarios.},
booktitle = {Proceedings of the XIV Brazilian Symposium on Information Systems},
articleno = {56},
numpages = {9},
keywords = {UML, Software Modeling, Software Components, Model Composition, Empirical Studies},
location = {Caxias do Sul, Brazil},
series = {SBSI '18}
}

@inproceedings{10.1145/3387514.3405877,
author = {Huang, Qun and Sun, Haifeng and Lee, Patrick P. C. and Bai, Wei and Zhu, Feng and Bao, Yungang},
title = {OmniMon: Re-architecting Network Telemetry with Resource Efficiency and Full Accuracy},
year = {2020},
isbn = {9781450379557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387514.3405877},
doi = {10.1145/3387514.3405877},
abstract = {Network telemetry is essential for administrators to monitor massive data traffic in a network-wide manner. Existing telemetry solutions often face the dilemma between resource efficiency (i.e., low CPU, memory, and bandwidth overhead) and full accuracy (i.e., error-free and holistic measurement). We break this dilemma via a network-wide architectural design OmniMon, which simultaneously achieves resource efficiency and full accuracy in flow-level telemetry for large-scale data centers. OmniMon carefully coordinates the collaboration among different types of entities in the whole network to execute telemetry operations, such that the resource constraints of each entity are satisfied without compromising full accuracy. It further addresses consistency in network-wide epoch synchronization and accountability in error-free packet loss inference. We prototype OmniMon in DPDK and P4. Testbed experiments on commodity servers and Tofino switches demonstrate the effectiveness of OmniMon over state-of-the-art telemetry designs.},
booktitle = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {404–421},
numpages = {18},
keywords = {Network measurement, Distributed systems},
location = {Virtual Event, USA},
series = {SIGCOMM '20}
}

@inproceedings{10.1145/3486011.3486469,
author = {V\'{a}zquez-Ingelmo, Andrea and Alonso-S\'{a}nchez, Julia and Garc\'{\i}a-Holgado, Alicia and Garc\'{\i}a Pe\~{n}alvo, Francisco Jos\'{e} and Sampedro-G\'{o}mez, Jes\'{u}s and S\'{a}nchez-Puente, Antonio and Vicente-Palacios, V\'{\i}ctor and Dorado-D\'{\i}az, P. Ignacio and Sanchez, Pedro L.},
title = {Bringing machine learning closer to non-experts: proposal of a user-friendly machine learning tool in the healthcare domain},
year = {2021},
isbn = {9781450390668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486011.3486469},
doi = {10.1145/3486011.3486469},
abstract = {Applying Machine Learning to solve or support complex tasks is growing in popularity in a lot of different contexts. One of these contexts is the medical domain. Through Machine Learning, specific problems such as diagnosis, classification, disease detection, segmentation, assessment of organ functions, etc., can be eased by assisting physicians with useful models and their outcomes. However, understanding the application of Machine Learning and Artificial Intelligence algorithms requires expert knowledge and significant data science skills. This work presents a proposal for a user-friendly Machine Learning tool, focusing on providing a good user experience for physicians as well as an educative context for understanding the tasks involved in Machine Learning pipelines, their configuration, and their outputs.},
booktitle = {Ninth International Conference on Technological Ecosystems for Enhancing Multiculturality (TEEM'21)},
pages = {324–329},
numpages = {6},
keywords = {User-centered design, Machine Learning, Human-Computer Interaction, Health domain},
location = {Barcelona, Spain},
series = {TEEM'21}
}

@article{10.1145/3712288,
author = {Zhang, Qian and Guo, Kaiyi and Yang, Yifei and Wang, Dong},
title = {WearSE: Enabling Streaming Speech Enhancement on Eyewear Using Acoustic Sensing},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
url = {https://doi.org/10.1145/3712288},
doi = {10.1145/3712288},
abstract = {Smart eyewear has rapidly evolved in recent years, yet its mobile and in-the-wild characteristics often make voice interactions on such devices susceptible to external interferences. In this paper, we introduce WearSE, a system that utilizes acoustic signals emitted and received by speakers and microphones mounted on eyewear to perceive facial movements during speech, achieving multimodal speech enhancement. WearSE incorporates three key designs to meet the high demands for real-time operation and robustness on smart eyewear. First, considering the frequent use in mobile scenarios, we design a sensing-enhanced network to amplify the capability of acoustic sensing, eliminating dynamic multipath interferences. Second, we develop a lightweight speech enhancement network that enhances both the amplitude and phase of the speech spectrum. Through a casual network design, computational demands are significantly reduced, ensuring real-time operation on mobile devices. Third, addressing the scarcity of paired data, we design a memory-based back-translation mechanism to generate pseudo-acoustic sensing data using a large amount of publicly available speech data for network training. We construct a prototype system and extensively evaluate WearSE through experiments. In multi-speaker scenarios, our approach exhibits much better performance than pure audio speech enhancement methods. Comparisons with commercial smart eyewear also demonstrate that WearSE significantly surpasses existing noise reduction algorithms in these devices. The audio demo of WearSE is available on https://github.com/WearSE/wearse.github.io.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = mar,
articleno = {25},
numpages = {30},
keywords = {back-translation, cascaded network, sensing-enhanced network, smart eyewear, streaming speech enhancement}
}

@article{10.1007/s00165-013-0276-5,
author = {Sampath, Prahladavaradan},
title = {An elementary theory of product-line variations},
year = {2014},
issue_date = {Jul 2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {4},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-013-0276-5},
doi = {10.1007/s00165-013-0276-5},
abstract = {The primary aim of a software product-line is to maximise reuse of software components by managing the variability in component functionalities and product configurations. Feature oriented domain analysis (FODA) diagrams are a formalism for modelling the variability in a software product-line, and are used as a tool for managing a product-line and planning its evolution. This paper presents an elementary theory of variations in a product-line, leading up to a technique for extracting FODA diagrams from legacy product-lines. The theory is elementary in the sense that it is built using very simple mathematical structures, making minimal assumptions on the structure of product-lines. Examples drawn from the automotive domain are used to illustrate the theoretical developments.},
journal = {Form. Asp. Comput.},
month = jul,
pages = {695–727},
numpages = {33},
keywords = {Lattice theory, Formal concept analysis, FODA, SPLE}
}

@inproceedings{10.1145/2993236.2993253,
author = {Al-Hajjaji, Mustafa and Krieter, Sebastian and Th\"{u}m, Thomas and Lochau, Malte and Saake, Gunter},
title = {IncLing: efficient product-line testing using incremental pairwise sampling},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993253},
doi = {10.1145/2993236.2993253},
abstract = {A software product line comprises a family of software products that share a common set of features. It enables customers to compose software systems from a managed set of features. Testing every product of a product line individually is often infeasible due to the exponential number of possible products in the number of features. Several approaches have been proposed to restrict the number of products to be tested by sampling a subset of products achieving sufficient combinatorial interaction coverage. However, existing sampling algorithms do not scale well to large product lines, as they require a considerable amount of time to generate the samples. Moreover, samples are not available until a sampling algorithm completely terminates. As testing time is usually limited, we propose an incremental approach of product sampling for pairwise interaction testing (called IncLing), which enables developers to generate samples on demand in a step-wise manner. Furthermore, IncLing uses heuristics to efficiently achieve pairwise interaction coverage with a reasonable number of products. We evaluated IncLing by comparing it against existing sampling algorithms using feature models of different sizes. The results of our approach indicate efficiency improvements for product-line testing.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {144–155},
numpages = {12},
keywords = {sampling, model-based testing, combinatorial interaction testing, Software product lines},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@inproceedings{10.1145/378795.378860,
author = {Xiong, Jianxin and Johnson, Jeremy and Johnson, Robert and Padua, David},
title = {SPL: a language and compiler for DSP algorithms},
year = {2001},
isbn = {1581134142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/378795.378860},
doi = {10.1145/378795.378860},
abstract = {We discuss the design and implementation of a compiler that translates formulas representing signal processing transforms into efficient C or Fortran programs. The formulas are represented in a language that we call SPL, an acronym from Signal Processing Language. The compiler is a component of the SPIRAL system which makes use of formula transformations and intelligent search strategies to automatically generate optimized digital signal processing (DSP) libraries. After a discussion of the translation and optimization techniques implemented in the compiler, we use SPL formulations of the fast Fourier transform (FFT) to evaluate the compiler. Our results show that SPIRAL, which can be used to implement many classes of algorithms, produces programs that perform as well as “hard-wired” systems like FFTW.},
booktitle = {Proceedings of the ACM SIGPLAN 2001 Conference on Programming Language Design and Implementation},
pages = {298–308},
numpages = {11},
location = {Snowbird, Utah, USA},
series = {PLDI '01}
}

@inproceedings{10.1145/3267183.3267187,
author = {Lima, Crescencio and Assun\c{c}\~{a}o, Wesley K. G. and Martinez, Jabier and do Carmo Machado, Ivan and von Flach G. Chavez, Christina and Mendon\c{c}a, Willian D. F.},
title = {Towards an Automated Product Line Architecture Recovery: The Apo-Games Case Study},
year = {2018},
isbn = {9781450365543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267183.3267187},
doi = {10.1145/3267183.3267187},
abstract = {Software Product Line Engineering (SPLE) has been widely adopted for applying systematic reuse in families of systems. Given the high upfront investment required for SPLE adoption, organizations commonly start with more opportunistic reuse approaches (e.g., a single system that they clone and modify). However, maintenance problems appear when managing a large number of similar systems where each of them implements and evolves particular characteristics. One viable solution to solve this issue is to migrate to SPLs using an extractive approach. This initiative, in its early phases, includes the definition of a Product Line Architecture (PLA) supporting the variants derivation and also allowing the customization according to customers' needs. Our objective is to provide automatic support in PLA recovery to reduce the time and effort in this process. One of the main issues in the extractive approach is the explosion of the variability in the PLA representation. Our approach is based on identifying the minimum subset of cross-product architectural information for an effective PLA recovery. To evaluate our approach, we applied it in the case of the Apo-Games projects. The experimentation in this real family of systems showed that our automatic approach is able to identify variant outliers and help domain experts to take informed decisions to support PLA recovery.},
booktitle = {Proceedings of the VII Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {33–42},
numpages = {10},
keywords = {Variability, Software Product Lines, Product Line Architecture Recovery, Product Line Architecture},
location = {Sao Carlos, Brazil},
series = {SBCARS '18}
}

@inproceedings{10.1145/3180155.3180163,
author = {Guo, Jianmei and Shi, Kai},
title = {To preserve or not to preserve invalid solutions in search-based software engineering: a case study in software product lines},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180163},
doi = {10.1145/3180155.3180163},
abstract = {Multi-objective evolutionary algorithms (MOEAs) have been successfully applied for software product lines (SPLs) to search for optimal or near-optimal solutions that balance multiple objectives. However, MOEAs usually produce invalid solutions that violate the constraints predefined. As invalid solutions are unbuildable in practice, we debate the preservation of invalid solutions during the search. We conduct experiments on seven real-world SPLs, including five largest SPLs hitherto reported and two SPLs with realistic values and constraints of quality attributes. We identify three potential limitations of preserving invalid solutions. Furthermore, based on the state-of-the-art, we design five algorithm variants that adopt different evolutionary operators. By performance evaluation, we provide empirical guidance on how to preserve valid solutions. Our empirical study demonstrates that whether or not to preserve invalid solutions deserves more attention in the community, and in some cases, we have to preserve valid solutions all along the way.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1027–1038},
numpages = {12},
keywords = {constraint solving, multi-objective evolutionary algorithms, search-based software engineering, software product lines, validity},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@article{10.1109/TASLP.2023.3304491,
author = {Zhong, Jiaxin and Zhuang, Tao and Li, Mengtong and Kirby, Ray and Karimi, Mahmoud and Lu, Jing and Zhang, Dong},
title = {Sidelobe Suppression for a Steerable Parametric Source Using the Sparse Random Array Technique},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3304491},
doi = {10.1109/TASLP.2023.3304491},
abstract = {A steerable parametric source is designed to steer an audio beam without mechanically rotating the source. To achieve this without the generation of grating lobes requires an ultrasonic array with interelement spacing that is less than half the wavelength of the carrier ultrasound because of the spatial Nyquist criterion. However, ultrasonic wavelengths are typically smaller than the size of an ultrasonic transducer and this generates grating lobes in the radiation pattern, which is known as the spatial aliasing effect. This work proposes a method to suppress sidelobes including these grating lobes by optimizing the position and weight coefficients of the array elements by using a sparse random array technique. This is achieved by using the peak sidelobe level as the objective function and the simulated annealing algorithm for the optimization. Both simulation and experimental results demonstrate the sidelobe level can be effectively suppressed when the average interelement spacing is two wavelengths. It is also found that Westervelt directivity has a significant effect on the spatial aliasing, because it serves as a spatial filter on the product directivity of the ultrasound. Accordingly, the sidelobe suppression performance deteriorates at low audio frequencies and high ultrasound frequencies where Westervelt directivity tends to be broader. However, this deterioration in performance can be addressed by increasing the number of the array elements.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {3152–3161},
numpages = {10}
}

@inproceedings{10.1145/2025113.2025203,
author = {Cafeo, Bruno B.P. and Noppen, Joost and Ferrari, Fabiano C. and Chitchyan, Ruzanna and Rashid, Awais},
title = {Inferring test results for dynamic software product lines},
year = {2011},
isbn = {9781450304436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2025113.2025203},
doi = {10.1145/2025113.2025203},
abstract = {Due to the very large number of configurations that can typically be derived from a Dynamic Software Product Line (DSPL), efficient and effective testing of such systems have become a major challenge for software developers. In particular, when a configuration needs to be deployed quickly due to rapid contextual changes (e.g., in an unfolding crisis), time constraints hinder the proper testing of such a configuration. In this paper, we propose to reduce the testing required of such DSPLs to a relevant subset of configurations. Whenever a need to adapt to an untested configuration is encountered, our approach determines the most similar tested configuration and reuses its test results to either obtain a coverage measure or infer a confidence degree for the new, untested configuration. We focus on providing these techniques for inference of structural testing results for DSPLs, which is supported by an early prototype implementation.},
booktitle = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
pages = {500–503},
numpages = {4},
keywords = {software testing, dynamic software product lines},
location = {Szeged, Hungary},
series = {ESEC/FSE '11}
}

@inproceedings{10.1145/3377024.3377044,
author = {Kr\"{u}ger, Jacob and Berger, Thorsten},
title = {Activities and costs of re-engineering cloned variants into an integrated platform},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377044},
doi = {10.1145/3377024.3377044},
abstract = {Many software systems need to exist in multiple variants. Organizations typically develop variants using clone&amp;own---copying and adapting systems towards new requirements. However, while clone &amp; own is a simple and readily available strategy, it does not scale with the number of variants, and then requires a costly reengineering of the cloned variants into a configurable software platform (a.k.a., software product line). Ideally, organizations could rely on decision models or at least on substantial empirical data to assess the costs and benefits of such a re-engineering. Unfortunately, despite decades of research on product lines and platforms, such data is scarce, not least because obtaining it from industrial reengineering efforts is challenging. We address this gap with a study on re-engineering two cases of cloned variants of open-source Android and Java games. Student developers re-engineered the clones into software product lines, logging their activities and costs. They performed the types of activities typically associated with re-engineering, but the activities were intertwined and done iteratively. The costs were relatively similar among both cases, but the used variability mechanism had a substantial impact. Interestingly, beyond a common diffing tool, no dedicated re-engineering tool was particularly useful. We hope that our results support researchers working on re-engineering techniques and decision models, as well as practitioners trying to assess the costs and activities involved in re-engineering a software platform.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {21},
numpages = {10},
keywords = {software product lines, re-engineering, empirical study, clone &amp; own},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1145/3560905.3568522,
author = {Han, Feiyu and Yang, Panlong and Du, Haohua and Li, Xiang-Yang},
title = {Accuth: Anti-Spoofing Voice Authentication via Accelerometer},
year = {2023},
isbn = {9781450398862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560905.3568522},
doi = {10.1145/3560905.3568522},
abstract = {Most existing voice-based user authentication systems mainly rely on microphones to capture the unique vocal characteristics of an individual, which makes these systems vulnerable to various acoustic attacks and suffer high-security risks. In this work, we present Accuth, a novel authentication system that takes advantage of a low-cost accelerometer to verify the user's identity and resist spoofing acoustic attacks. Accuth captures unique sound vibrations during the human pronunciation process and extracts multi-level features to verify the user's identity. Specifically, we analyze and model the differences between the physical sound field of human beings and loudspeakers, and extract a novel sound-field-level liveness feature to defend against spoofing attacks. Accuth is an effective complement to existing authentication approaches as it only leverages a ubiquitous, low-cost, and small-size accelerometer. In real-world experiments, Accuth achieves over 90% identification accuracy among 15 human participants and an average equal error rate (EER) of 3.02% for spoofing attack detection.},
booktitle = {Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems},
pages = {637–650},
numpages = {14},
keywords = {accelerometer, biometrics, sound vibration, voice authentication},
location = {Boston, Massachusetts},
series = {SenSys '22}
}

@inproceedings{10.1145/3001867.3001868,
author = {Lachmann, Remo and Lity, Sascha and Al-Hajjaji, Mustafa and F\"{u}rchtegott, Franz and Schaefer, Ina},
title = {Fine-grained test case prioritization for integration testing of delta-oriented software product lines},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001868},
doi = {10.1145/3001867.3001868},
abstract = {Software product line (SPL) testing is a challenging task, due to the huge number of variants sharing common functionalities to be taken into account for efficient testing. By adopting the concept of regression testing, incremental SPL testing strategies cope with this challenge by exploiting the reuse potential of test artifacts between subsequent variants under test. In previous work, we proposed delta-oriented test case prioritization for incremental SPL integration testing, where differences between architecture test model variants allow for reasoning about the order of reusable test cases to be executed. However, the prioritization left two issues open, namely (1) changes to component behavior are ignored, which may also influence component interactions and, (2) the weighting and ordering of similar test cases result in an unintended clustering of test cases. In this paper, we extend the test case prioritization technique by (1) incorporating changes to component behavior allowing for a more fine-grained analysis and (2) defining a dissimilarity measure to avoid clustered test case orders. We prototyped our test case prioritization technique and evaluated its applicability and effectiveness by means of a case study from the automotive domain showing positive results.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {1–10},
numpages = {10},
keywords = {Test Case Prioritization, Model-Based Integration Testing, Delta-Oriented Software Product Lines},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@inproceedings{10.1109/ASE.2019.00123,
author = {Reichelt, David Georg and K\"{u}hne, Stefan and Hasselbring, Wilhelm},
title = {PeASS: a tool for identifying performance changes at code level},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00123},
doi = {10.1109/ASE.2019.00123},
abstract = {We present PeASS (Performance Analysis of Software System versions), a tool for detecting performance changes at source code level that occur between different code versions. By using PeASS, it is possible to identify performance regressions that happened in the past to fix them.PeASS measures the performance of unit tests in different source code versions. To achieve statistic rigor, measurements are repeated and analyzed using an agnostic t-test. To execute a minimal amount of tests, PeASS uses a regression test selection.We evaluate PeASS on a selection of Apache Commons projects and show that 81% of all unit test covered performance changes can be found by PeASS. A video presentation is available at https://www.youtube.com/watch?v=RORFEGSCh6Y and PeASS can be downloaded from https://github.com/DaGeRe/peass.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1146–1149},
numpages = {4},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/3629479.3629513,
author = {Wolfart, Daniele and Assun\c{c}\~{A}o, Wesley K. Guez and Martinez, Jabier},
title = {Variability Debt: A Multi-method Study},
year = {2023},
isbn = {9798400707865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629479.3629513},
doi = {10.1145/3629479.3629513},
abstract = {Technical debt is a metaphor to guide the identification, measurement, and general management of decisions that were mostly appropriate in the short term but created obstacles mainly for the evolution and maintenance of systems. Variability management, which is the ability to create variants of systems to satisfy different needs, is a potential source of technical debt. Variability debt, a term coined in this work, is caused by sub-optimal solutions in the implementation of variability management in software systems. We performed a systematic literature review to characterize variability debt, and conducted a field study in which we report quantitative and qualitative analysis based on documents (e.g., requirements, specifications, source code, and test cases) and a survey with stakeholders. The context is a large company with three different systems, where opportunistic reuse was applied to create variants for each system. We describe and characterize the variability debt phenomenon in this field study, and we assess the validity of the metaphor to create awareness in diverse company stakeholders and to guide technical debt management research related to variability aspects. The analysis of the field study’s artifacts show evidences of factors that complicate the evolution of the variants, such as code duplication and non-synchronized artifacts. Time pressure is identified as the main cause for not considering other options than opportunistic reuse. Technical practitioners mostly agree on the creation of usability problems and complex maintenance of multiple independent variants. However, this is not fully perceived by managerial practitioners.},
booktitle = {Proceedings of the XXII Brazilian Symposium on Software Quality},
pages = {358–367},
numpages = {10},
keywords = {Software reuse, Technical Debt, Variability Debt, Variability management},
location = {Bras\'{\i}lia, Brazil},
series = {SBQS '23}
}

@inproceedings{10.1145/1062455.1062552,
author = {Schmid, Klaus and John, Isabel and Kolb, Ronny and Meier, Gerald},
title = {Introducing the puLSE approach to an embedded system population at testo AG},
year = {2005},
isbn = {1581139632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062455.1062552},
doi = {10.1145/1062455.1062552},
abstract = {Over the last few years, product line engineering has become a major theme in software engineering research, and is increasingly becoming a central topic of software engineering practice in the embedded domain.Migrating towards a product line approach is not an easy feat. It is even less so, if it is done under tight technology constraints in an embedded environment. It becomes even more difficult if the transition directly aims at integrating two product families into a single product population. In this paper, we discuss our experiences with a project where we successfully dealt with these difficulties and achieved a successful product line transition. In our paper we strongly emphasize the role of technology transfer, as many facets of product line know-how had to be transferred to guarantee a complete transition to product line engineering. From the experiences of this project many lessons learned can be deduced, which can be transferred to different environments.},
booktitle = {Proceedings of the 27th International Conference on Software Engineering},
pages = {544–552},
numpages = {9},
keywords = {technology transfer, systematic software reuse, software product line, product line introduction},
location = {St. Louis, MO, USA},
series = {ICSE '05}
}

@proceedings{10.1145/3607822,
title = {SUI '23: Proceedings of the 2023 ACM Symposium on Spatial User Interaction},
year = {2023},
isbn = {9798400702815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1145/2814204.2814214,
author = {Font, Jaime and Arcega, Lorena and Haugen, \O{}ystein and Cetina, Carlos},
title = {Addressing metamodel revisions in model-based software product lines},
year = {2015},
isbn = {9781450336871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814204.2814214},
doi = {10.1145/2814204.2814214},
abstract = {Metamodels evolve over time, which can break the conformance between the models and the metamodel. Model migration strategies aim to co-evolve models and metamodels together, but their application is not fully automatizable and is thus cumbersome and error prone. We introduce the Variable MetaModel (VMM) strategy to address the evolution of the reusable model assets of a model-based Software Product Line. The VMM strategy applies variability modeling ideas to express the evolution of the metamodel in terms of commonalities and variabilities. When the metamodel evolves, the models continue to conform to the VMM, avoiding the need for migration. We have applied both the traditional migration strategy and the VMM strategy to a retrospective case study that includes 13 years of evolution of our industrial partner, an induction hobs manufacturer. The comparison between the two strategies shows better results for the VMM strategy in terms of model indirection, automation, and trust leak.},
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {161–170},
numpages = {10},
keywords = {Variability Modeling, Model-based Software Product Lines, Model and Metamodel Co-evolution},
location = {Pittsburgh, PA, USA},
series = {GPCE 2015}
}

@inproceedings{10.5555/2667025.2667027,
author = {Siegmund, Norbert and Mory, Maik and Feigenspan, Janet and Saake, Gunter and Nykolaychuk, Mykhaylo and Schumann, Marco},
title = {Interoperability of non-functional requirements in complex systems},
year = {2012},
isbn = {9781467318532},
publisher = {IEEE Press},
abstract = {Heterogeneity of embedded systems leads to the development of variable software, such as software product lines. From such a family of programs, stakeholders select the specific variant that satisfies their functional requirements. However, different functionality exposes different non-functional properties of these variants. Especially in the embedded-system domain, non-functional requirements are vital, because resources are scarce. Hence, when selecting an appropriate variant, we have to fulfill also non-functional requirements. Since more systems are interconnected, the challenge is to find a variant that additionally satisfies global nonfunctional (or quality) requirements. In this paper, we advert the problem of achieving interoperability of non-functional requirements among multiple interacting systems using a real-world scenario. Furthermore, we show an approach to find optimal variants for multiple systems that reduces computation effort by means of a stepwise configuration process.},
booktitle = {Proceedings of the Second International Workshop on Software Engineering for Embedded Systems},
pages = {2–8},
numpages = {7},
location = {Zurich, Switzerland},
series = {SEES '12}
}

@inproceedings{10.1145/3106237.3106252,
author = {Kn\"{u}ppel, Alexander and Th\"{u}m, Thomas and Mennicke, Stephan and Meinicke, Jens and Schaefer, Ina},
title = {Is there a mismatch between real-world feature models and product-line research?},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106252},
doi = {10.1145/3106237.3106252},
abstract = {Feature modeling has emerged as the de-facto standard to compactly capture the variability of a software product line. Multiple feature modeling languages have been proposed that evolved over the last decades to manage industrial-size product lines. However, less expressive languages, solely permitting require and exclude constraints, are permanently and carelessly used in product-line research. We address the problem whether those less expressive languages are sufficient for industrial product lines. We developed an algorithm to eliminate complex cross-tree constraints in a feature model, enabling the combination of tools and algorithms working with different feature model dialects in a plug-and-play manner. However, the scope of our algorithm is limited. Our evaluation on large feature models, including the Linux kernel, gives evidence that require and exclude constraints are not sufficient to express real-world feature models. Hence, we promote that research on feature models needs to consider arbitrary propositional formulas as cross-tree constraints prospectively.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {291–302},
numpages = {12},
keywords = {require constraints, model transformation, feature modeling, expressiveness, exclude constraints, cross-tree constraints, Software product lines},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@proceedings{10.1145/3586183,
title = {UIST '23: Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Francisco, CA, USA}
}

@inproceedings{10.1145/2568225.2568267,
author = {Salay, Rick and Famelis, Michalis and Rubin, Julia and Di Sandro, Alessio and Chechik, Marsha},
title = {Lifting model transformations to product lines},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568267},
doi = {10.1145/2568225.2568267},
abstract = {Software product lines and model transformations are two techniques used in industry for managing the development of highly complex software. Product line approaches simplify the handling of software variants while model transformations automate software manipulations such as refactoring, optimization, code generation, etc. While these techniques are well understood independently, combining them to get the benefit of both poses a challenge because most model transformations apply to individual models while model-level product lines represent sets of models. In this paper, we address this challenge by providing an approach for automatically ``lifting'' model transformations so that they can be applied to product lines. We illustrate our approach using a case study and evaluate it through a set of experiments.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {117–128},
numpages = {12},
keywords = {Software Product Lines, Model Transformations, Model Driven Engineering},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/1985793.1986047,
author = {Rubin, Julia and Botterweck, Goetz and Pleuss, Andreas and Weiss, David M.},
title = {Second international workshop on product line approaches in software engineering (PLEASE 2011)},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1986047},
doi = {10.1145/1985793.1986047},
abstract = {PLEASE workshop series focuses on exploring the present and the future of Software Product Line Engineering techniques. The main goal of PLEASE 2011 is to bring together industrial practitioner and software product line researchers in order to couple real-life industrial problems with concrete solutions developed by the community.We plan for an interactive workshop, where participants can apply their expertise to current industrial problems, while those who face challenges in the area of product line engineering can benefit from the suggested solutions. We also intend to establish ongoing, long-lasting relationships between industrial and research participants to the mutual benefits of both.The second edition of PLEASE is held in conjunction with the 33rd International Conference in Software Engineering (May 21-28, 2011, Honolulu, Hawaii).},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {1204–1205},
numpages = {2},
keywords = {variability management, software product lines, product line engineering},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@inproceedings{10.1145/3358960.3379144,
author = {Musaafir, Ahmed and Uta, Alexandru and Dreuning, Henk and Varbanescu, Ana-Lucia},
title = {A Sampling-Based Tool for Scaling Graph Datasets},
year = {2020},
isbn = {9781450369916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358960.3379144},
doi = {10.1145/3358960.3379144},
abstract = {Graph processing has become a topic of interest in many domains. However, we still observe a lack of representative datasets for in-depth performance and scalability analysis. Neither data collections, nor graph generators provide enough diversity and control for thorough analysis. To address this problem, we proposea heuristic method for scaling existing graphs. Our approach, based onsampling andinterconnection, can provide a scaled "version" of a given graph. Moreover, we provide analytical models to predict the topological properties of the scaled graphs (such as the diameter, degree distribution, density, or the clustering coefficient), and further enable the user to tweak these properties. Property control is achieved through a portfolio of graph interconnection methods (e.g., star, ring, chain, fully connected) applied for combining the graph samples. We further implement our method as an open-source tool which can be used to quickly provide families of datasets for in-depth benchmarking of graph processing algorithms. Our empirical evaluation demonstrates our tool provides scaled graphs of a wide range of sizes, whose properties match well with model predictions and/or user requirements. Finally, we also illustrate, through a case-study, how scaled graphs can be used for in-depth performance analysis of graph processing algorithms.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {289–300},
numpages = {12},
keywords = {heuristic methods, graph scaling tool, graph sampling, graph datasets scaling},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@inproceedings{10.1145/2695664.2695743,
author = {Garcia, Cleiton and Paludo, Marco and Malucelli, Andreia and Reinehr, Sheila},
title = {A software process line for service-oriented applications},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695743},
doi = {10.1145/2695664.2695743},
abstract = {The management of processes and systems is a complex and time-consuming activity for organizations and also an ongoing Information Technology (IT) challenge. Among the different approaches for bringing flexibility to the business processes and systems are Service-Oriented Architecture (SOA) and Business Process Management (BPM). The SOA approach has become popular providing services and interfaces, enabling integration of heterogeneous and distributed platforms and BPM leverages the cycles of improvements, control and evaluation of business processes. BPM and SOA should work together aiming at improving business processes and evolving systems architecture. One main problem to apply BPM and SOA is the lack of established processes and this work proposes a software process line in order to simplify variability control and enable the instantiation of new development process applying BPM and SOA. It also aims at developing an environment to support the proposed software process line in order to automate the process, integrating industrial tools with one specifically developed to perform the transformation of UMA models into BPM notation. The main contribution of this work is the definition of the software process line for engineering service-oriented products. It is highly relevant to software industry since software process lines lacks experiments, practices and tools.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1680–1687},
numpages = {8},
keywords = {web-based services, software process lines, SPL, SOA, BPM},
location = {Salamanca, Spain},
series = {SAC '15}
}

@article{10.1145/2211616.2211617,
author = {K\"{a}stner, Christian and Apel, Sven and Th\"{u}m, Thomas and Saake, Gunter},
title = {Type checking annotation-based product lines},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2211616.2211617},
doi = {10.1145/2211616.2211617},
abstract = {Software product line engineering is an efficient means of generating a family of program variants for a domain from a single code base. However, because of the potentially high number of possible program variants, it is difficult to test them all and ensure properties like type safety for the entire product line. We present a product-line-aware type system that can type check an entire software product line without generating each variant in isolation. Specifically, we extend the Featherweight Java calculus with feature annotations for product-line development and prove formally that all program variants generated from a well typed product line are well typed. Furthermore, we present a solution to the problem of typing mutually exclusive features. We discuss how results from our formalization helped implement our own product-line tool CIDE for full Java and report of our experience with detecting type errors in four existing software product line implementations.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {14},
numpages = {39},
keywords = {type system, software product lines, conditional compilation, Featherweight Java, CIDE, CFJ, #ifdef}
}

@inproceedings{10.1145/3001867.3001874,
author = {Queiroz, Rodrigo and Berger, Thorsten and Czarnecki, Krzysztof},
title = {Towards predicting feature defects in software product lines},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001874},
doi = {10.1145/3001867.3001874},
abstract = {Defect-prediction techniques can enhance the quality assurance activities for software systems. For instance, they can be used to predict bugs in source files or functions. In the context of a software product line, such techniques could ideally be used for predicting defects in features or combinations of features, which would allow developers to focus quality assurance on the error-prone ones. In this preliminary case study, we investigate how defect prediction models can be used to identify defective features using machine-learning techniques. We adapt process metrics and evaluate and compare three classifiers using an open-source product line. Our results show that the technique can be effective. Our best scenario achieves an accuracy of 73 % for accurately predicting features as defective or clean using a Naive Bayes classifier. Based on the results we discuss directions for future work.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {58–62},
numpages = {5},
keywords = {software product lines, features, defect prediction},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@inproceedings{10.1145/2577080.2577095,
author = {Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha and Baier, Christel},
title = {Probabilistic model checking for energy analysis in software product lines},
year = {2014},
isbn = {9781450327725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2577080.2577095},
doi = {10.1145/2577080.2577095},
abstract = {In a software product line (SPL), a collection of software products is defined by their commonalities in terms of features rather than explicitly specifying all products one-by-one. Several verification techniques were adapted to establish temporal properties of SPLs. Symbolic and family-based model checking have been proven to be successful for tackling the combinatorial blow-up arising when reasoning about several feature combinations. However, most formal verification approaches for SPLs presented in the literature focus on the static SPLs, where the features of a product are fixed and cannot be changed during runtime. This is in contrast to dynamic SPLs, allowing to adapt feature combinations of a product dynamically after deployment.The main contribution of the paper is a compositional modeling framework for dynamic SPLs, which supports probabilistic and nondeterministic choices and allows for quantitative analysis. We specify the feature changes during runtime within an automata-based coordination component, enabling to reason over strategies how to trigger dynamic feature changes for optimizing various quantitative objectives, e.g., energy or monetary costs and reliability. For our framework there is a natural and conceptually simple translation into the input language of the prominent probabilistic model checker PRISM. This facilitates the application of PRISM's powerful symbolic engine to the operational behavior of dynamic SPLs and their family-based analysis against various quantitative queries. We demonstrate feasibility of our approach by a case study issuing an energy-aware bonding network device.},
booktitle = {Proceedings of the 13th International Conference on Modularity},
pages = {169–180},
numpages = {12},
keywords = {software product lines, probabilistic model checking, energy analysis, dynamic features},
location = {Lugano, Switzerland},
series = {MODULARITY '14}
}

@inproceedings{10.1145/2739482.2764681,
author = {Martinez, Jabier and Rossi, Gabriele and Ziadi, Tewfik and Bissyand\'{e}, Tegawend\'{e} Fran\c{c}ois D. Assise and Klein, Jacques and Le Traon, Yves},
title = {Estimating and Predicting Average Likability on Computer-Generated Artwork Variants},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2764681},
doi = {10.1145/2739482.2764681},
abstract = {Computer assisted human creativity encodes human design decisions in algorithms allowing machines to produce artwork variants. Based on this automated production, one can leverage collective understanding of beauty to rank computer-generated artworks according to their average likability. We present the use of Software Product Line techniques for computer-generated art systems as a case study on leveraging the feedback of human perception within the boundaries of a variability model. Since it is not feasible to get feedback for all variants because of a combinatorial explosion of possible configurations, we propose an approach that is developed in two phases: 1) the creation of a data set using an interactive genetic algorithm and 2) the application of a data mining technique on this dataset to create a ranking enriched with confidence metrics.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1431–1432},
numpages = {2},
keywords = {software product lines, media arts, gentic algorithms},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@article{10.1145/3647643,
author = {Seixas Pereira, Let\'{\i}cia and Guerreiro, Jo\~{a}o and Rodrigues, Andr\'{e} and Guerreiro, Tiago and Duarte, Carlos},
title = {From Automation to User Empowerment: Investigating the Role of a Semi-automatic Tool in Social Media Accessibility},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1936-7228},
url = {https://doi.org/10.1145/3647643},
doi = {10.1145/3647643},
abstract = {This article focuses on evaluating SONAAR (Social Networks Accessible Authoring), a tool that combines automation and end-user empowerment to enhance the accessibility of social media content. SONAAR aims to increase user engagement in creating accessible content and expanding the availability of accessible media online. Additionally, SONAAR provides supplementary information to support the authoring of accessible media content. To assess SONAAR's effectiveness, we conducted three distinct studies. First, we analyzed user patterns and behaviors through log analysis. Next, we evaluated the clarity, helpfulness, and efficiency of the additional documentation and its potential to improve engagement in accessible practices. Finally, we explored user perceptions and challenges when interacting with SONAAR. The obtained findings indicate positive user feedback and provide valuable insights for improvement. These results underscore the importance of raising awareness and offering support for accessible practices, as well as the necessity for enhanced platform backing. Our study contributes to advancing accessible content authoring, promoting inclusivity and accessibility in online social media. We suggest future research directions to facilitate broader adoption of accessible practices and address user engagement challenges, ultimately enhancing the accessibility of social media content.},
journal = {ACM Trans. Access. Comput.},
month = sep,
articleno = {13},
numpages = {25},
keywords = {Accessibility, social media, visual content, user-generated content, artificial intelligence}
}

@proceedings{10.1145/3663529,
title = {FSE 2024: Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to FSE 2024, the ACM International Conference on the Foundations of Software Engineering (FSE) 2024. The conference now has a shorter name! FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {Porto de Galinhas, Brazil}
}

@inproceedings{10.5555/2337223.2337468,
author = {Colanzi, Thelma Elita},
title = {Search based design of software product lines architectures},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {The Product-Line Architecture (PLA) is the main artifact of a Software Product Line (SPL). However, obtaining a modular, extensible and reusable PLA is a people-intensive and non-trivial task, related to different and possible conflicting factors. Hence, the PLA design is a hard problem and to find the best architecture can be formulated as an optimization problem with many factors. Similar Software Engineering problems have been efficiently solved by search-based algorithms in the field known as Search-based Software Engineering. The existing approaches used to optimize software architecture are not suitable since they do not encompass specific characteristics of SPL. To easy the SPL development and to automate the PLA design this work introduces a multi-objective optimization approach to the PLA design. The approach is now being implemented by using evolutionary algorithms. Empirical studies will be performed to validate the neighborhood operators, SPL measures and search algorithms chosen. Finally, we intend to compare the results of the proposed approach with PLAs designed by human architects.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {1507–1510},
numpages = {4},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1109/ICSE43902.2021.00147,
author = {Mahmood, Wardah and Str\"{u}ber, Daniel and Berger, Thorsten and L\"{a}mmel, Ralf and Mukelabai, Mukelabai},
title = {Seamless Variability Management With the Virtual Platform},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00147},
doi = {10.1109/ICSE43902.2021.00147},
abstract = {Customization is a general trend in software engineering, demanding systems that support variable stakeholder requirements. Two opposing strategies are commonly used to create variants: software clone&amp;own and software configuration with an integrated platform. Organizations often start with the former, which is cheap, agile, and supports quick innovation, but does not scale. The latter scales by establishing an integrated platform that shares software assets between variants, but requires high up-front investments or risky migration processes. So, could we have a method that allows an easy transition or even combine the benefits of both strategies? We propose a method and tool that supports a truly incremental development of variant-rich systems, exploiting a spectrum between both opposing strategies. We design, formalize, and prototype the variability-management framework virtual platform. It bridges clone&amp;own and platform-oriented development. Relying on programming-language-independent conceptual structures representing software assets, it offers operators for engineering and evolving a system, comprising: traditional, asset-oriented operators and novel, feature-oriented operators for incrementally adopting concepts of an integrated platform. The operators record meta-data that is exploited by other operators to support the transition. Among others, they eliminate expensive feature-location effort or the need to trace clones. Our evaluation simulates the evolution of a real-world, clone-based system, measuring its costs and benefits.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1658–1670},
numpages = {13},
keywords = {variability management, software product lines, re-engineering, framework, clone management},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/2491411.2491459,
author = {Kim, Chang Hwan Peter and Marinov, Darko and Khurshid, Sarfraz and Batory, Don and Souto, Sabrina and Barros, Paulo and D'Amorim, Marcelo},
title = {SPLat: lightweight dynamic analysis for reducing combinatorics in testing configurable systems},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491459},
doi = {10.1145/2491411.2491459},
abstract = {Many programs can be configured through dynamic and/or static selection of configuration variables. A software product line (SPL), for example, specifies a family of programs where each program is defined by a unique combination of features. Systematically testing SPL programs is expensive as it can require running each test against a combinatorial number of configurations. Fortunately, a test is often independent of many configuration variables and need not be run against every combination. Configurations that are not required for a test can be pruned from execution. This paper presents SPLat, a new way to dynamically prune irrelevant configurations: the configurations to run for a test can be determined during test execution by monitoring accesses to configuration variables. SPLat achieves an optimal reduction in the number of configurations and is lightweight compared to prior work that used static analysis and heavyweight dynamic execution. Experimental results on 10 SPLs written in Java show that SPLat substantially reduces the total test execution time in many cases. Moreover, we demonstrate the scalability of SPLat by applying it to a large industrial code base written in Ruby on Rails.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {257–267},
numpages = {11},
keywords = {Software Product Lines, Efficiency, Configurable Systems, Automated testing},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inproceedings{10.1145/1562860.1562864,
author = {Siegmund, Norbert and Pukall, Mario and Soffner, Michael and K\"{o}ppen, Veit and Saake, Gunter},
title = {Using software product lines for runtime interoperability},
year = {2009},
isbn = {9781605585482},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1562860.1562864},
doi = {10.1145/1562860.1562864},
abstract = {Today, often small, heterogeneous systems have to cooperate in order to fulfill a certain task. Interoperability between these systems is needed for their collaboration. However, achieving this interoperability raises several problems. For example, embedded systems might induce a higher probability for a system failure due to constrained power supply. Nevertheless, interoperability must be guaranteed even in scenarios where embedded systems are used. To overcome this problem, we use services to abstract the functionality from the system which realizes it. We outline how services can be generated using software product line techniques to bridge the heterogeneity of cooperating systems. Additionally, we address runtime changes of already deployed services to overcome system failures. In this paper, we show the runtime adaption process of these changes which includes the following two points. First, we outline why feature-oriented programming is appropriate in such scenarios. Second, we describe the runtime adaption process of services with feature-oriented programming.},
booktitle = {Proceedings of the Workshop on AOP and Meta-Data for Software Evolution},
articleno = {4},
numpages = {7},
keywords = {software product lines, runtime adaption, interoperability},
location = {Genova, Italy},
series = {RAM-SE '09}
}

@article{10.1145/3176644,
author = {Xiang, Yi and Zhou, Yuren and Zheng, Zibin and Li, Miqing},
title = {Configuring Software Product Lines by Combining Many-Objective Optimization and SAT Solvers},
year = {2018},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3176644},
doi = {10.1145/3176644},
abstract = {A feature model (FM) is a compact representation of the information of all possible products from software product lines. The optimal feature selection involves the simultaneous optimization of multiple (usually more than three) objectives in a large and highly constrained search space. By combining our previous work on many-objective evolutionary algorithm (i.e., VaEA) with two different satisfiability (SAT) solvers, this article proposes a new approach named SATVaEA for handling the optimal feature selection problem. In SATVaEA, an FM is simplified with the number of both features and constraints being reduced greatly. We enhance the search of VaEA by using two SAT solvers: one is a stochastic local search--based SAT solver that can quickly repair infeasible configurations, whereas the other is a conflict-driven clause-learning SAT solver that is introduced to generate diversified products. We evaluate SATVaEA on 21 FMs with up to 62,482 features, including two models with realistic values for feature attributes. The experimental results are promising, with SATVaEA returning 100% valid products on almost all FMs. For models with more than 10,000 features, the search in SATVaEA takes only a few minutes. Concerning both effectiveness and efficiency, SATVaEA significantly outperforms other state-of-the-art algorithms.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {14},
numpages = {46},
keywords = {vector angle--based evolutionary algorithm (VaEA), satisfiability (SAT) solvers, many-objective optimization, Optimal feature selection}
}

@article{10.1145/3462699,
author = {Cheng, Long and Ahmed, Salman and Liljestrand, Hans and Nyman, Thomas and Cai, Haipeng and Jaeger, Trent and Asokan, N. and Yao, Danfeng (Daphne)},
title = {Exploitation Techniques for Data-oriented Attacks with Existing and Potential Defense Approaches},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
issn = {2471-2566},
url = {https://doi.org/10.1145/3462699},
doi = {10.1145/3462699},
abstract = {Data-oriented attacks manipulate non-control data to alter a program’s benign behavior without violating its control-flow integrity. It has been shown that such attacks can cause significant damage even in the presence of control-flow defense mechanisms. However, these threats have not been adequately addressed. In this survey article, we first map data-oriented exploits, including Data-Oriented Programming (DOP) and Block-Oriented Programming (BOP) attacks, to their assumptions/requirements and attack capabilities. Then, we compare known defenses against these attacks, in terms of approach, detection capabilities, overhead, and compatibility. It is generally believed that control flows may not be useful for data-oriented security. However, data-oriented attacks (especially DOP attacks) may generate side effects on control-flow behaviors in multiple dimensions (i.e., incompatible branch behaviors and frequency anomalies). We also characterize control-flow anomalies caused by data-oriented attacks. In the end, we discuss challenges for building deployable data-oriented defenses and open research questions.},
journal = {ACM Trans. Priv. Secur.},
month = sep,
articleno = {26},
numpages = {36},
keywords = {frequency anomaly, branch correlation, Data-oriented attacks, DOP, BOP}
}

@inproceedings{10.5555/1129601.1129621,
author = {Staszewski, R. B. and Muhammad, K. and Leipold, D.},
title = {Digital RF processor (DRP/spl trade/) for cellular phones},
year = {2005},
isbn = {078039254X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {RF circuits for multi-GHz frequencies have recently migrated to low-cost digital deep-submicron CMOS processes. Unfortunately, this process environment, which is optimized only for digital logic and SRAM memory, is extremely unfriendly for conventional analog and HF designs. We present fundamental techniques recently developed that transform the RF and analog circuit design complexity to digital domain for a wireless RF transceiver, so that it enjoys the benefits of digital approach, such as process node scaling and design automation. All-digital phase locked loop, all-digital control of phase and amplitude of a polar transmitter, and direct HF sampling techniques allow great flexibility in reconfigurable radio design. Digital signal processing concepts are used to help relieve analog design complexity, allowing one to reduce cost and power consumption in a reconfigurable design environment. Software layers are defined to enable these architectures to develop an efficient software defined radio. VHDL hardware description language is universally used throughout this SoC. The ideas presented have been used in Texas Instruments to develop two generations of commercial digital RF processors: a single-chip Bluetooth radio and a single-chip GSM radio.},
booktitle = {Proceedings of the 2005 IEEE/ACM International Conference on Computer-Aided Design},
pages = {122–129},
numpages = {8},
location = {San Jose, CA},
series = {ICCAD '05}
}

@article{10.1109/TASLP.2024.3477291,
author = {Delgado, Pablo M. and Herre, J\"{u}rgen},
title = {Towards Improved Objective Perceptual Audio Quality Assessment - Part 1: A Novel Data-Driven Cognitive Model},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3477291},
doi = {10.1109/TASLP.2024.3477291},
abstract = {Efficient audioquality assessment is vital for streamlining audio codec development. Objective assessment tools have been developed over time to algorithmically predict quality ratings from subjective assessments, the gold standard for quality judgment. Many of these tools use perceptual auditory models to extract audio features that are mapped to a basic audio quality score prediction using machine learning algorithms and subjective scores as training data. However, existing tools struggle with generalization in quality prediction, especially when faced with unknown signal and distortion types. This is particularly evident in the presence of signals coded using non-waveform-preserving parametric techniques. Addressing these challenges, this two-part work proposes extensions to the Perceptual Evaluation of Audio Quality (PEAQ - ITU-R BS.1387-1) recommendation. Part 1 focuses on increasing generalization, while Part 2 targets accurate spatial audio quality measurement in audio coding. To enhance prediction generalization, this paper (Part 1) introduces a novel machine learning approach that uses subjective data to model cognitive aspects of audio quality perception. The proposed method models the perceived severity of audible distortions by adaptively weighting different distortion metrics. The weights are determined using an interaction cost function that captures relationships between distortion salience and cognitive effects. Compared to other machine learning methods and established tools, the proposed architecture achieves higher prediction accuracy on large databases of previously unseen subjective quality scores. The perceptually-motivated model offers a more manageable alternative to general-purpose machine learning algorithms, allowing potential extensions and improvements to multi-dimensional quality measurement without complete retraining.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {4661–4675},
numpages = {15}
}

@article{10.1145/2897760,
author = {Hierons, Robert M. and Li, Miqing and Liu, Xiaohui and Segura, Sergio and Zheng, Wei},
title = {SIP: Optimal Product Selection from Feature Models Using Many-Objective Evolutionary Optimization},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/2897760},
doi = {10.1145/2897760},
abstract = {A feature model specifies the sets of features that define valid products in a software product line. Recent work has considered the problem of choosing optimal products from a feature model based on a set of user preferences, with this being represented as a many-objective optimization problem. This problem has been found to be difficult for a purely search-based approach, leading to classical many-objective optimization algorithms being enhanced either by adding in a valid product as a seed or by introducing additional mutation and replacement operators that use an SAT solver. In this article, we instead enhance the search in two ways: by providing a novel representation and by optimizing first on the number of constraints that hold and only then on the other objectives. In the evaluation, we also used feature models with realistic attributes, in contrast to previous work that used randomly generated attribute values. The results of experiments were promising, with the proposed (SIP) method returning valid products with six published feature models and a randomly generated feature model with 10,000 features. For the model with 10,000 features, the search took only a few minutes.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {17},
numpages = {39},
keywords = {Product selection}
}

@inproceedings{10.1145/2371401.2371404,
author = {Th\"{u}m, Thomas and Schaefer, Ina and Apel, Sven and Hentschel, Martin},
title = {Family-based deductive verification of software product lines},
year = {2012},
isbn = {9781450311298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2371401.2371404},
doi = {10.1145/2371401.2371404},
abstract = {A software product line is a set of similar software products that share a common code base. While software product lines can be implemented efficiently using feature-oriented programming, verifying each product individually does not scale, especially if human effort is required (e.g., as in interactive theorem proving). We present a family-based approach of deductive verification to prove the correctness of a software product line efficiently. We illustrate and evaluate our approach for software product lines written in a feature-oriented dialect of Java and specified using the Java Modeling Language. We show that the theorem prover KeY can be used off-the-shelf for this task, without any modifications. Compared to the individual verification of each product, our approach reduces the verification time needed for our case study by more than 85%.},
booktitle = {Proceedings of the 11th International Conference on Generative Programming and Component Engineering},
pages = {11–20},
numpages = {10},
keywords = {theorem proving, software product lines, program families, product-line analysis, deductive verification},
location = {Dresden, Germany},
series = {GPCE '12}
}

@inproceedings{10.5555/2050655.2050698,
author = {Cichos, Harald and Oster, Sebastian and Lochau, Malte and Sch\"{u}rr, Andy},
title = {Model-based coverage-driven test suite generation for software product lines},
year = {2011},
isbn = {9783642244841},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software Product Line (SPL) engineering is a popular approach for the systematic reuse of software artifacts across a large number of similar products. Unfortunately, testing each product of an SPL separately is often unfeasible. Consequently, SPL engineering is in conflict with standards like ISO 26262, which require each installed software configuration of safety-critical SPLs to be tested using a model-based approach with well-defined coverage criteria.In this paper we address this dilemma and present a new SPL test suite generation algorithm that uses model-based testing techniques to derive a small test suite from one variable 150% test model of the SPL such that a given coverage criterion is satisfied for the test model of every product. Furthermore, our algorithm simplifies the subsequent selection of a small, representative set of products (w.r.t. the given coverage criterion) on which the generated test suite can be executed.},
booktitle = {Proceedings of the 14th International Conference on Model Driven Engineering Languages and Systems},
pages = {425–439},
numpages = {15},
location = {Wellington, New Zealand},
series = {MODELS'11}
}

@proceedings{10.1145/3611643,
title = {ESEC/FSE 2023: Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to ESEC/FSE 2023, the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ESEC/FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. ESEC/FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {San Francisco, CA, USA}
}

@inproceedings{10.1145/3560905.3568508,
author = {Xu, Chenhan and Chen, Tianyu and Li, Huining and Gherardi, Alexander and Weng, Michelle and Li, Zhengxiong and Xu, Wenyao},
title = {Hearing Heartbeat from Voice: Towards Next Generation Voice-User Interfaces with Cardiac Sensing Functions},
year = {2023},
isbn = {9781450398862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560905.3568508},
doi = {10.1145/3560905.3568508},
abstract = {Voice user interfaces (VUIs) have been adopted in many IoT and mobile devices in daily life. VUIs provide a good user experience with lower-cost hardware (i.e., microphone) and higher throughput (compared with keyboard and touchscreen). Currently, identity authentication and receiving commands are the two most common interactions through VUIs, leaving physiological information in the voice unexploited. Recognizing this untapped potential, we propose VocalHR to extend VUIs beyond voice commands to heart activity sensing without additional hardware. VocalHR is built upon the voice-heart modulation effect, which is rooted in the cardiac activities' impacts on the behavior of the vocal organ during voice production. VocalHR captures voice features of cardiac activity in multiple voice organs and proposes a deep learning pipeline to transform features into cardiac activities. As this is the first study exploring voice-based heart activity sensing, we conducted extensive experiments on 43 demographically diverse subjects to verify the intrinsic link between voice and heart activities. On average, VocalHR can achieve less than 11.1% normalized sensing error on the heart event timing. Our further evaluation shows VocalHR is robust to different microphone specifications and varying speech rates.},
booktitle = {Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems},
pages = {149–163},
numpages = {15},
keywords = {contactless sensing, healthcare, voice biometrics, voice-user interface},
location = {Boston, Massachusetts},
series = {SenSys '22}
}

@inproceedings{10.1145/3071178.3071261,
author = {Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat},
title = {Mining cross product line rules with multi-objective search and machine learning},
year = {2017},
isbn = {9781450349208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3071178.3071261},
doi = {10.1145/3071178.3071261},
abstract = {Nowadays, an increasing number of systems are being developed by integrating products (belonging to different product lines) that communicate with each other through information networks. Cost-effectively supporting Product Line Engineering (PLE) and in particular enabling automation of configuration in PLE is a challenge. Capturing rules is the key for enabling automation of configuration. Product configuration has a direct impact on runtime interactions of communicating products. Such products might be within or across product lines and there usually don't exist explicitly specified rules constraining configurable parameter values of such products. Manually specifying such rules is tedious, time-consuming, and requires expert's knowledge of the domain and the product lines. To address this challenge, we propose an approach named as SBRM that combines multi-objective search with machine learning to mine rules. To evaluate the proposed approach, we performed a real case study of two communicating Video Conferencing Systems belonging to two different product lines. Results show that SBRM performed significantly better than Random Search in terms of fitness values, Hyper-Volume, and machine learning quality measurements. When comparing with rules mined with real data, SBRM performed significantly better in terms of Failed Precision (18%), Failed Recall (72%), and Failed F-measure (59%).},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1319–1326},
numpages = {8},
keywords = {rule mining, product line, multi-objective search, machine learning, configuration},
location = {Berlin, Germany},
series = {GECCO '17}
}

@inproceedings{10.1145/3448300.3467825,
author = {Walker, Payton and Saxena, Nitesh},
title = {SoK: assessing the threat potential of vibration-based attacks against live speech using mobile sensors},
year = {2021},
isbn = {9781450383493},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448300.3467825},
doi = {10.1145/3448300.3467825},
abstract = {Existing academic research on vibration-based speech attacks has introduced interesting and intellectually appealing threat vectors with proof-of-concept demonstrations in controlled environments. The attacks presented in these studies exploit different types of sensors such as MEMS motion sensors, laser-based sensors, and some other sensors (camera, position error signal, piezo-disc) to measure the vibrations induced on an object by nearby sensitive speech. Such sensors are commonly found on mobile devices like smartphones and tablets that can be exposed to sensitive speech, revealing the significance of this potential threat. These studies have amassed significant attention in news and media and introduced concern to people about the safety of their day-to-day speech and around their personal, wireless and IoT devices. However, we hypothesize that the controlled experiments in the prior research maintain critical parameter values that are favorable to attack success (deviating from the limiting settings in a real-world scenario) and produce results that suggest a greater real-life threat level than actually exists.The contributions made in this paper are as follows; First, we provide a detailed review of 10 existing academic research works related to vibration-based eavesdropping attacks. Second, we identify key experimental parameters that can impact the success of eavesdropping in the vibration domain. Third, we build a framework to evaluate the existing literature based on the Percent Parameters in Favored Settings (PPFS) Score metric that we define. Lastly, we use our defined framework to evaluate the feasibility of the existing vibration-based speech attacks to compromise live human speech to the extent of full speech recognition. The results of our evaluation suggest that none of the existing vibration-based eavesdropping attacks have a high likelihood of successfully compromising live human speech in a real-world scenario.},
booktitle = {Proceedings of the 14th ACM Conference on Security and Privacy in Wireless and Mobile Networks},
pages = {273–287},
numpages = {15},
keywords = {vibration, speech eavesdropping, side-channel, SoK},
location = {Abu Dhabi, United Arab Emirates},
series = {WiSec '21}
}

@inproceedings{10.1145/3576915.3623159,
author = {Van Beirendonck, Michiel and D'Anvers, Jan-Pieter and Turan, Furkan and Verbauwhede, Ingrid},
title = {FPT: A Fixed-Point Accelerator for Torus Fully Homomorphic Encryption},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3623159},
doi = {10.1145/3576915.3623159},
abstract = {Fully Homomorphic Encryption (FHE) is a technique that allows computation on encrypted data. It has the potential to drastically change privacy considerations in the cloud, but high computational and memory overheads are preventing its broad adoption. TFHE is a promising Torus-based FHE scheme that heavily relies on bootstrapping, the noise-removal tool invoked after each encrypted logical/arithmetical operation.  We present FPT, a Fixed-Point FPGA accelerator for TFHE bootstrapping. FPT is the first hardware accelerator to heavily exploit the inherent noise present in FHE calculations. Instead of double or single-precision floating-point arithmetic, it implements TFHE bootstrapping entirely with approximate fixed-point arithmetic. Using an in-depth analysis of noise propagation in bootstrapping FFT computations, FPT is able to use noise-trimmed fixed-point representations that are up to 50% smaller than prior implementations that prefer floating-point or integer FFTs.  FPT is built as a streaming processor inspired by traditional streaming DSPs: it instantiates directly cascaded high-throughput computational stages, with minimal control logic and routing networks. We explore different throughput-balanced compositions of streaming kernels with a user-configurable streaming width in order to construct a full bootstrapping pipeline. Our proposed approach allows 100% utilization of arithmetic units and requires only small bootstrapping key cache, enabling an entirely compute-bound bootstrapping throughput of 1 BS / 35us. This is in stark contrast to the established classical CPU approach to FHE bootstrapping acceleration, which is typically constrained by memory and bandwidth. FPT is fully implemented and evaluated as a bootstrapping FPGA kernel for an Alveo U280 datacenter accelerator card. FPT achieves two to three orders of magnitude higher bootstrapping throughput than existing CPU-based implementations, and 2.5x higher throughput compared to recent ASIC emulation experiments.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {741–755},
numpages = {15},
keywords = {fpga, fully homomorphic encryption, hardware accelerator, tfhe},
location = {Copenhagen, Denmark},
series = {CCS '23}
}

@inproceedings{10.1145/3136040.3136054,
author = {Linsbauer, Lukas and Berger, Thorsten and Gr\"{u}nbacher, Paul},
title = {A classification of variation control systems},
year = {2017},
isbn = {9781450355247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3136040.3136054},
doi = {10.1145/3136040.3136054},
abstract = {Version control systems are an integral part of today's software and systems development processes. They facilitate the management of revisions (sequential versions) and variants (concurrent versions) of a system under development and enable collaboration between developers. Revisions are commonly maintained either per file or for the whole system. Variants are supported via branching or forking mechanisms that conceptually clone the whole system under development. It is known that such cloning practices come with disadvantages. In fact, while short-lived branches for isolated development of new functionality (a.k.a. feature branches) are well supported, dealing with long-term and fine-grained system variants currently requires employing additional mechanisms, such as preprocessors, build systems or custom configuration tools. Interestingly, the literature describes a number of variation control systems, which provide a richer set of capabilities for handling fine-grained system variants compared to the version control systems widely used today. In this paper we present a classification and comparison of selected variation control systems to get an understanding of their capabilities and the advantages they can offer. We discuss problems of variation control systems, which may explain their comparably low popularity. We also propose research activities we regard as important to change this situation.},
booktitle = {Proceedings of the 16th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {49–62},
numpages = {14},
keywords = {software repositories, software product lines, configuration management, Variability management},
location = {Vancouver, BC, Canada},
series = {GPCE 2017}
}

@inproceedings{10.1109/ASE.2011.6100068,
author = {Pohl, Richard and Lauenroth, Kim and Pohl, Klaus},
title = {A performance comparison of contemporary algorithmic approaches for automated analysis operations on feature models},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100068},
doi = {10.1109/ASE.2011.6100068},
abstract = {The formalization of variability models (e.g. feature models) is a prerequisite for the automated analysis of these models. The efficient execution of the analysis operations depends on the selection of well-suited solver implementations. Regarding feature models, on the one hand, the formalization with Boolean expressions enables the use of SAT or BDD solvers. On the other hand, feature models can be transformed into a Constraint-Satisfaction Problem (CSP) in order to use CSP solvers for validation. This paper presents a performance comparison regarding nine contemporary high-performance solvers, three for each base problem structure (BDD, CSP, and SAT). Four operations on 90 feature models are run on each solver. The results will in turn clear the way for new improvements regarding the automatic verification of software product lines, since the efficient execution of analysis operations is essential to such automatic verification approaches.},
booktitle = {Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {313–322},
numpages = {10},
series = {ASE '11}
}

@inproceedings{10.1145/1882486.1882513,
author = {Miller, David J. and Watts, Philip M. and Moore, Andrew W.},
title = {Motivating future interconnects: a differential measurement analysis of PCI latency},
year = {2009},
isbn = {9781605586304},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1882486.1882513},
doi = {10.1145/1882486.1882513},
abstract = {Local interconnect architectures are at a cusp in which advances in throughput have come at the expense of power and latency. Moreover, physical limits imposed on dissipation and packaging mean that further advances will require a new approach to interconnect design. Although latency in networks has been the focus of the High-Performance Computing architect and of concern across the computer community, we illustrate how an evolution in the common PCI interconnect architecture has worsened latency by a factor of between 3 and 25 over earlier incarnations.},
booktitle = {Proceedings of the 5th ACM/IEEE Symposium on Architectures for Networking and Communications Systems},
pages = {94–103},
numpages = {10},
keywords = {microbenchmark, measurement, latency, interconnects, experimental evaluation, PCI express, FPGA},
location = {Princeton, New Jersey},
series = {ANCS '09}
}

@inproceedings{10.1145/3639477.3639750,
author = {Wang, Zikuan and Liu, Bohan and Zhan, Zeye and Zhang, He and Li, Gongyuan},
title = {An Ethnographic Study on the CI of A Large Scale Project},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639750},
doi = {10.1145/3639477.3639750},
abstract = {Continuous Integration (CI) is the foundation for achieving rapid iteration and short-cycle delivery. To achieve CI, a series of best practices and solutions have been proposed, which are referred to as patterns. However, there is a natural contradiction between the speed and continuity pursued by CI and the ever-expanding project scale and complexity. Various factors such as project size, outdated system architecture, complex organizational structure, or limited server resources can all lead to deviations from patterns in CI practices, resulting in anti-patterns. We conducted an ethnographic research to investigate the current state, anti-patterns, and challenges in resolving anti-patterns of the CI process within a large communication project at a globally leading IT company. We conducted a deep observation and participation in the project for seven months and conducted multiple rounds of interviews with related developers in the company. The project adopts a CI pipeline that has a three-level hierarchical structure. We evaluated the company's software development practices based on the pattern list. We identified three anti-patterns that contradicted the patterns listed, and we also discovered three new anti-patterns that were not on the list. Further, we analyzed the challenges of solving these anti-patterns. Additionally, we found seven better practices and analyzed why they are better.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {287–297},
numpages = {11},
keywords = {continuous integration, ethnographic study, anti-patterns, best practice},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/501983.501998,
author = {Crazzolara, Federico and Winskel, Glynn},
title = {Events in security protocols},
year = {2001},
isbn = {1581133855},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/501983.501998},
doi = {10.1145/501983.501998},
abstract = {The events of a security protocol and their causal dependency can play an important role in the analysis of security properties. This insight underlies both strand spaces and the inductive method. But neither of these approaches builds up the events of a protocol in a compositional way, so that there is an informal spring from the protocol to its model. By broadening the models to certain kinds of Petri nets, a restricted form of contextual nets, a compositional event-based semantics is given to an economical, but expressive, language for describing security protocols; so the events and dependency of a wide range of protocols are determined once and for all. The net semantics is formally related to a transition semantics, strand spaces and inductive rules, as well as trace languages and event structures, so unifying a range of approaches, as well as providing conditions under which particular, more limited, models are adequate for the analysis of protocols. The net semantics allows the derivation of general properties and proof principles which are demonstrated in establishing an authentication property, following a diagrammatic style of proof.},
booktitle = {Proceedings of the 8th ACM Conference on Computer and Communications Security},
pages = {96–105},
numpages = {10},
location = {Philadelphia, PA, USA},
series = {CCS '01}
}

@inproceedings{10.1145/2897053.2897058,
author = {Sharifloo, Amir Molzam and Metzger, Andreas and Quinton, Cl\'{e}ment and Baresi, Luciano and Pohl, Klaus},
title = {Learning and evolution in dynamic software product lines},
year = {2016},
isbn = {9781450341875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897053.2897058},
doi = {10.1145/2897053.2897058},
abstract = {A Dynamic Software Product Line (DSPL) aims at managing run-time adaptations of a software system. It is built on the assumption that context changes that require these adaptations at run-time can be anticipated at design-time. Therefore, the set of adaptation rules and the space of configurations in a DSPL are predefined and fixed at design-time. Yet, for large-scale and highly distributed systems, anticipating all relevant context changes during design-time is often not possible due to the uncertainty of how the context may change. Such design-time uncertainty therefore may mean that a DSPL lacks adaptation rules or configurations to properly reconfigure itself at run-time. We propose an adaptive system model to cope with design-time uncertainty in DSPLs. This model combines learning of adaptation rules with evolution of the DSPL configuration space. It takes particular account of the mutual dependencies between evolution and learning, such as using feedback from unsuccessful learning to trigger evolution. We describe concrete steps for learning and evolution to show how such feedback can be exploited. We illustrate the use of such a model with a running example from the cloud computing domain.},
booktitle = {Proceedings of the 11th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {158–164},
numpages = {7},
keywords = {machine learning, evolution, dynamic software product lines, adaptation},
location = {Austin, Texas},
series = {SEAMS '16}
}

@inproceedings{10.1145/3555776.3578611,
author = {Limaylla-Lunarejo, Maria-Isabel and Condori-Fernandez, Nelly and Luaces, Miguel R.},
title = {Towards a FAIR Dataset for non-functional requirements},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3578611},
doi = {10.1145/3555776.3578611},
abstract = {In the last years, the application of supervised Machine Learning (ML) algorithms in Requirements Engineering (RE) has allowed increasing the performance (e.g. accuracy, precision) and scalability of automatic requirements classification. However, the lack of publicly labeled datasets is still one concern when conducting ML experiments. Few publicly labeled datasets for non-functional requirements classification are available, and even less in the Spanish language. Moreover, most of the available datasets present some limitations, such as imbalanced classes (e.g. PROMISE NFR). This study aims to generate a FAIR dataset of non-functional requirements in the Spanish language for facilitating reuse in ML classification experiments. 109 non-functional requirements were collected from final degree projects from the University of A Coru\~{n}a. We conducted a pilot quasi-experiment for non-functional requirements labeling in the categories and subcategories of the ISO/IEC 25010 quality model. The labeling process was accomplished by 7 annotators. The inter-annotator agreement using a Fleiss' Kappa test obtained a substantial agreement in the category level (0.78) and a moderate agreement (0.48) when the classification is per subcategory.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1414–1421},
numpages = {8},
keywords = {FAIR principles, spanish dataset, non-functional requirements, data labeling},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@article{10.1145/3614438,
author = {Zhang, Shijia and Lu, Taiting and Zhou, Hao and Liu, Yilin and Liu, Runze and Gowda, Mahanth},
title = {I Am an Earphone and I Can Hear My User’s Face: Facial Landmark Tracking Using Smart Earphones},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
url = {https://doi.org/10.1145/3614438},
doi = {10.1145/3614438},
abstract = {This article presents EARFace, a system that shows the feasibility of tracking facial landmarks for 3D facial reconstruction using in-ear acoustic sensors embedded within smart earphones. This enables a number of applications in the areas of facial expression tracking, user interfaces, AR/VR applications, affective computing, and accessibility, among others. Although conventional vision-based solutions break down under poor lighting and occlusions, and also suffer from privacy concerns, earphone platforms are robust to ambient conditions while being privacy-preserving. In contrast to prior work on earable platforms that perform outer-ear sensing for facial motion tracking, EARFace shows the feasibility of completely in-ear sensing with a natural earphone form factor, thus enhancing the comfort levels of wearing. The core intuition exploited by EARFace is that the shape of the ear canal changes due to the movement of facial muscles during facial motion. EARFace tracks the changes in shape of the ear canal by measuring ultrasonic channel frequency response of the inner ear, ultimately resulting in tracking of the facial motion. A transformer-based machine learning model is designed to exploit spectral and temporal relationships in the ultrasonic channel frequency response data to predict the facial landmarks of the user with an accuracy of 1.83 mm. Using these predicted landmarks, a 3D graphical model of the face that replicates the precise facial motion of the user is then reconstructed. Domain adaptation is further performed by adapting the weights of layers using a group-wise and differential learning rate. This decreases the training overhead in EARFace. The transformer-based machine learning model runs on smart phone devices with a processing latency of 13 ms and an overall low power consumption profile. Finally, usability studies indicate higher levels of comforts of wearing EARFace’s earphone platform in comparison with alternative form factors.},
journal = {ACM Trans. Internet Things},
month = dec,
articleno = {1},
numpages = {29},
keywords = {IoT, facial reconstruction, earable computing, mobile computing, Wearable sensing}
}

@article{10.1145/3678546,
author = {Zhou, Juntao and Li, Yijie and Wang, Yida and Ding, Dian and Lu, Yu and Chen, Yi-Chao and Xue, Guangtao},
title = {Visar: Projecting Virtual Sound Spots for Acoustic Augmented Reality Using Air Nonlinearity},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
url = {https://doi.org/10.1145/3678546},
doi = {10.1145/3678546},
abstract = {Augmented reality that integrates virtual content in real-world surroundings has attracted lots of concentration as the growing trend of the metaverse. Acoustic augmented reality (AAR) applications have proliferated due to readily available earphones and speakers. AAR can provide omnidirectional engagement through the all-around sense of spatial information. Most existing AAR approaches offer immersive experiences by playing binaural spatial audios according to head-related transfer functions (HRTF). These involve complex modeling and require the user to wear a headphone. Air nonlinearity that can reproduce audible sounds from ultrasound offers opportunities to achieve device-free and omnidirectional sound source projection in AAR. This paper proposes Visar, a device-free virtual sound spots projection system leveraging air nonlinearity. Visar achieves simultaneous tracking and sound spot generation while suppressing unintended audio leakages caused by grating lobes and nonlinear effects in mixing lobes through optimization. Considering multi-user scenarios, Visar also proposed a multi-spot scheduling scheme to mitigate the mutual interference between the spots. Extensive experiments show the tracking error is 7.83cm and the orientation estimation error is 10.06°, respectively, envisioning the considerable potential of Visar in AAR applications.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {147},
numpages = {30},
keywords = {Acoustic augmented reality, Air nonlinearity, Sound source projection}
}

@inproceedings{10.1145/3386901.3388939,
author = {Rathore, Aditya Singh and Zhu, Weijin and Daiyan, Afee and Xu, Chenhan and Wang, Kun and Lin, Feng and Ren, Kui and Xu, Wenyao},
title = {SonicPrint: a generally adoptable and secure fingerprint biometrics in smart devices},
year = {2020},
isbn = {9781450379540},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386901.3388939},
doi = {10.1145/3386901.3388939},
abstract = {The advent of smart devices has caused unprecedented security and privacy concerns to its users. Although the fingerprint technology is a go-to biometric solution in high-impact applications (e.g., smart-phone security, monetary transactions and international-border verification), the existing fingerprint scanners are vulnerable to spoofing attacks via fake-finger and cannot be employed across smart devices (e.g., wearables) due to hardware constraints. We propose SonicPrint that extends fingerprint identification beyond smartphones to any smart device without the need for traditional fingerprint scanners. SonicPrint builds on the fingerprint-induced sonic effect (FiSe) caused by a user swiping his fingertip on smart devices and the resulting property, i.e., different users' fingerprint would result in distinct FiSe. As the first exploratory study, extensive experiments verify the above property with 31 participants over four different swipe actions on five different types of smart devices with even partial fingerprints. SonicPrint achieves up to a 98% identification accuracy on smartphone and an equal-error-rate (EER) less than 3% for smartwatch and headphones. We also examine and demonstrate the resilience of SonicPrint against fingerprint phantoms and replay attacks. A key advantage of SonicPrint is that it leverages the already existing microphones in smart devices, requiring no hardware modifications. Compared with other biometrics including physiological patterns and passive sensing, SonicPrint is a low-cost, privacy-oriented and secure approach to identify users across smart devices of unique form-factors.},
booktitle = {Proceedings of the 18th International Conference on Mobile Systems, Applications, and Services},
pages = {121–134},
numpages = {14},
location = {Toronto, Ontario, Canada},
series = {MobiSys '20}
}

@article{10.1145/1082983.1085124,
title = {Frontmatter (TOC, Letters, Election results, Software Reliability Resources!, Computing Curricula 2004 and the Software Engineering Volume SE2004, Software Reuse Research, ICSE 2005 Forward)},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1082983.1085124},
doi = {10.1145/1082983.1085124},
journal = {SIGSOFT Softw. Eng. Notes},
month = jul,
pages = {0},
numpages = {63}
}

@inproceedings{10.1145/3427228.3427277,
author = {Mitev, Richard and Pazii, Anna and Miettinen, Markus and Enck, William and Sadeghi, Ahmad-Reza},
title = {LeakyPick: IoT Audio Spy Detector},
year = {2020},
isbn = {9781450388580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427228.3427277},
doi = {10.1145/3427228.3427277},
abstract = {Manufacturers of smart home Internet of Things (IoT) devices are increasingly adding voice assistant and audio monitoring features to a wide range of devices including smart speakers, televisions, thermostats, security systems, and doorbells. Consequently, many of these devices are equipped with microphones, raising significant privacy concerns: users may not always be aware of when audio recordings are sent to the cloud, or who may gain access to the recordings. In this paper, we present the LeakyPick architecture that enables the detection of the smart home devices that stream recorded audio to the Internet in response to observing a sound. Our proof-of-concept is a LeakyPick device that is placed in a user’s smart home and periodically “probes” other devices in its environment and monitors the subsequent network traffic for statistical patterns that indicate audio transmission. Our prototype is built on a Raspberry Pi for less than USD $40 and has a measurement accuracy of 94% in detecting audio transmissions for a collection of 8 devices with voice assistant capabilities. Furthermore, we used LeakyPick to identify 89 words that an Amazon Echo Dot misinterprets as its wake-word, resulting in unexpected audio transmission. LeakyPick provides a cost effective approach to help regular consumers monitor their homes for sound-triggered devices that unexpectedly transmit audio to the cloud.},
booktitle = {Proceedings of the 36th Annual Computer Security Applications Conference},
pages = {694–705},
numpages = {12},
location = {Austin, USA},
series = {ACSAC '20}
}

@inproceedings{10.1145/3652620.3688341,
author = {Kegel, Karl and Domanowski, Andreas and Feichtinger, Kevin and Pascual, Romain and A\ss{}mann, Uwe},
title = {A Delta-Oracle for Fast Model Merge Conflict Estimation using Sketch-Based Critical Pair Analysis},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688341},
doi = {10.1145/3652620.3688341},
abstract = {Conflicting changes are a major challenge in branch-based development and modeling. State-of-the-art research proposes continuous analysis via attempted three-way merges to find potential merge conflicts early on. These approaches are computation-heavy due to the necessity of comparing all variant combinations, ideally for each change. This work proposes a conflict approximation algorithm (oracle) for quick feedback. The oracle approximates conflicts using critical pair analysis on tracked delta sequences, providing a quick feedback loop. The oracle is paired with a classical slow-but-precise full model comparison algorithm, which is run occasionally to validate the oracle's results. This work contributes the Sketch-based Critical Pair Analysis (SCPA) approach for fast merge conflict estimation. SCPA's runtime depends only on the number of changes and not the model size. We evaluate SCPA against EMFCompare in different simulated model evolution scenarios. We found that for the investigated model sizes, SCPA is faster by a magnitude while the number of found conflicts strongly correlates with EMFCompare.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {1046–1055},
numpages = {10},
keywords = {merge conflict estimation, critical pair analysis, oracle algorithm},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@article{10.1145/3714458,
author = {Li, Menglu and Ahmadiadli, Yasaman and Zhang, Xiao-Ping},
title = {A Survey on Speech Deepfake Detection},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3714458},
doi = {10.1145/3714458},
abstract = {The availability of smart devices leads to an exponential increase in multimedia content. However, advancements in deep learning have also enabled the creation of highly sophisticated Deepfake content, including speech Deepfakes, which pose a serious threat by generating realistic voices and spreading misinformation. To combat this, numerous challenges have been organized to advance speech Deepfake detection techniques. In this survey, we systematically analyze more than 200 papers published up to March 2024. We provide a comprehensive review of each component in the detection pipeline, including model architectures, optimization techniques, generalizability, evaluation metrics, performance comparisons, available datasets, and open source availability. For each aspect, we assess recent progress and discuss ongoing challenges. In addition, we explore emerging topics such as partial Deepfake detection, cross-dataset evaluation, and defenses against adversarial attacks, while suggesting promising research directions. This survey not only identifies the current state of the art to establish strong baselines for future experiments but also offers clear guidance for researchers aiming to enhance speech Deepfake detection systems.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {165},
numpages = {38},
keywords = {Deepfake, speech synthesis, speech Deepfake detection, spoofing countermeasures, ASV}
}

@inproceedings{10.1145/2491956.2491976,
author = {Bodden, Eric and Tol\^{e}do, T\'{a}rsis and Ribeiro, M\'{a}rcio and Brabrand, Claus and Borba, Paulo and Mezini, Mira},
title = {SPLLIFT: statically analyzing software product lines in minutes instead of years},
year = {2013},
isbn = {9781450320146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491956.2491976},
doi = {10.1145/2491956.2491976},
abstract = {A software product line (SPL) encodes a potentially large variety of software products as variants of some common code base. Up until now, re-using traditional static analyses for SPLs was virtually intractable, as it required programmers to generate and analyze all products individually. In this work, however, we show how an important class of existing inter-procedural static analyses can be transparently lifted to SPLs. Without requiring programmers to change a single line of code, our approach SPLLIFT automatically converts any analysis formulated for traditional programs within the popular IFDS framework for inter-procedural, finite, distributive, subset problems to an SPL-aware analysis formulated in the IDE framework, a well-known extension to IFDS. Using a full implementation based on Heros, Soot, CIDE and JavaBDD, we show that with SPLLIFT one can reuse IFDS-based analyses without changing a single line of code. Through experiments using three static analyses applied to four Java-based product lines, we were able to show that our approach produces correct results and outperforms the traditional approach by several orders of magnitude.},
booktitle = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {355–364},
numpages = {10},
keywords = {software product lines, inter-procedural static analysis, flow sensitive, context sensitive},
location = {Seattle, Washington, USA},
series = {PLDI '13}
}

@inproceedings{10.1145/2695664.2695907,
author = {Mefteh, Mariem and Bouassida, Nadia and Ben-Abdallah, Han\^{e}ne},
title = {Implementation and evaluation of an approach for extracting feature models from documented UML use case diagrams},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695907},
doi = {10.1145/2695664.2695907},
abstract = {Software product lines (SPL) aim at facing the increasing costs of software products by reusing core assets of existing products in a given domain. They are often described using feature models which, as we proposed in a previous work, can be built from possibly incomplete, documented UML use case diagrams assets using the Formal Concept Analysis method, semantic model and trigger model. In order to evaluate this approach, we present in this paper the UC2FM-tool which automates all its steps. In addition, we report on a comparison of the values of quality metrics of feature models produced by our approach with those of existing feature models built by experts for five different domains.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1602–1609},
numpages = {8},
keywords = {software product lines, measurement, feature model, evaluation},
location = {Salamanca, Spain},
series = {SAC '15}
}

@proceedings{10.1145/3643991,
title = {MSR '24: Proceedings of the 21st International Conference on Mining Software Repositories},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {MSR is a thriving research community that organizes a yearly conference with a solid reputation amongst software engineering researchers.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/1180995.1181063,
author = {Lin, Tao and Imamiya, Atsumi},
title = {Evaluating usability based on multimodal information: an empirical study},
year = {2006},
isbn = {159593541X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180995.1181063},
doi = {10.1145/1180995.1181063},
abstract = {New technologies are making it possible to provide an enriched view of interaction for researchers using multimodal information. This preliminary study explores the use of multiple information streams in usability evaluation. In the study, easy, medium and difficult versions of a game task were used to vary the levels of mental effort. Multimodal data streams during the three versions were analyzed, including eye tracking, pupil size, hand movement, heart rate variability (HRV) and subjectively reported data. Four findings indicate the potential value of usability evaluations based on multimodal information: First, subjective and physiological measures showed significant sensitivity to task difficulty. Second, different mental workload levels appeared to correlate with eye movement patterns, especially with a combined eye-hand movement measure. Third, HRV showed correlations with saccade speed. Finally, we present a new method using the ratio of eye fixations over mouse clicks to evaluate performance in more detail. These results warrant further investigations and take an initial step toward establishing usability evaluation methods based on multimodal information.},
booktitle = {Proceedings of the 8th International Conference on Multimodal Interfaces},
pages = {364–371},
numpages = {8},
keywords = {usability, physiological measures, multimodal, eye tracking},
location = {Banff, Alberta, Canada},
series = {ICMI '06}
}

@inproceedings{10.1145/1449913.1449918,
author = {Mendonca, Marcilio and Wasowski, Andrzej and Czarnecki, Krzysztof and Cowan, Donald},
title = {Efficient compilation techniques for large scale feature models},
year = {2008},
isbn = {9781605582672},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1449913.1449918},
doi = {10.1145/1449913.1449918},
abstract = {Feature modeling is used in generative programming and software product-line engineering to capture the common and variable properties of programs within an application domain. The translation of feature models to propositional logics enabled the use of reasoning systems, such as BDD engines, for the analysis and transformation of such models and interactive configurations. Unfortunately, the size of a BDD structure is highly sensitive to the variable ordering used in its construction and an inappropriately chosen ordering may prevent the translation of a feature model into a BDD representation of a tractable size. Finding an optimal order is NP-hard and has for long been addressed by using heuristics.We review existing general heuristics and heuristics from the hardware circuits domain and experimentally show that they are not effective in reducing the size of BDDs produced from feature models. Based on that analysis we introduce two new heuristics for compiling feature models to BDDs. We demonstrate the effectiveness of these heuristics using publicly available and automatically generated models. Our results are directly applicable in construction of feature modeling tools.},
booktitle = {Proceedings of the 7th International Conference on Generative Programming and Component Engineering},
pages = {13–22},
numpages = {10},
keywords = {software-product lines, model-driven development, formal verification, feature modeling, configuration},
location = {Nashville, TN, USA},
series = {GPCE '08}
}

@article{10.1109/TASLP.2018.2860786,
author = {Salehi, Haniyeh and Suelzle, David and Folkeard, Paula and Parsa, Vijay},
title = {Learning-Based Reference-Free Speech Quality Measures for Hearing Aid Applications},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2860786},
doi = {10.1109/TASLP.2018.2860786},
abstract = {Objective measures of speech quality are highly desirable in benchmarking and monitoring the performance of hearing aids HAs. Existing HA speech quality indices such as the hearing aid speech quality index HASQI are intrusive in that they require a properly time-aligned and frequency-shaped reference signal to predict the quality of HA output. Two new reference-free HA speech quality indices are proposed in this paper, based on a model that amalgamates perceptual linear prediction PLP, hearing loss HL modeling, and machine learning concepts. For the first index, HL-modified PLP coefficients and their statistics were used as the feature set, which was subsequently mapped to the predicted quality scores using support vector regression SVR. For the second index, HL-impacted gammatone auditory filterbank energies and their second-order statistics constituted the feature set, which was again mapped using SVR. Two databases involving HA recordings were collected and utilized for the evaluation of the robustness and generalizability of the two indices. Experimental results showed that the index based on the gammatone filterbank energies not only correlated well with HA quality ratings by hearing impaired listeners, but also exhibited robust performance across different test conditions and was comparable to the full-reference HASQI performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2277–2288},
numpages = {12}
}

@inproceedings{10.1109/ICSE.2017.58,
author = {Behringer, Benjamin and Palz, Jochen and Berger, Thorsten},
title = {PEoPL: projectional editing of product lines},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.58},
doi = {10.1109/ICSE.2017.58},
abstract = {The features of a software product line---a portfolio of system variants---can be realized using various implementation techniques (a.k.a., variability mechanisms). Each technique represents the software artifacts of features differently, typically classified into annotative (e.g., C preprocessor) and modular representations (e.g., feature modules), each with distinct advantages and disadvantages. Annotative representations are easy to realize, but annotations clutter source code and hinder program comprehension. Modular representations support comprehension, but are difficult to realize. Most importantly, to engineer feature artifacts, developers need to choose one representation and adhere to it for evolving and maintaining the same artifacts.We present PEoPL, an approach to combine the advantages of annotative and modular representations. When engineering a feature artifact, developers can choose the most-suited representation and even use different representations in parallel. PEoPL relies on separating a product line into an internal and external representation, the latter by providing editable projections used by the developers. We contribute a programming-language-independent internal representation of variability, five editable projections reflecting different variability representations, a supporting IDE, and a tailoring of PEoPL to Java. We evaluate PEoPL's expressiveness, scalability, and flexibility in eight Java-based product lines, finding that all can be realized, that projections are feasible, and that variant computation is fast (&lt;45ms on average for our largest subject Berkeley DB).},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {563–574},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@inproceedings{10.1145/3548606.3560643,
author = {Barua, Anomadarshi and Achamyeleh, Yonatan Gizachew and Faruque, Mohammad Abdullah Al},
title = {A Wolf in Sheep's Clothing: Spreading Deadly Pathogens Under the Disguise of Popular Music},
year = {2022},
isbn = {9781450394505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548606.3560643},
doi = {10.1145/3548606.3560643},
abstract = {A Negative Pressure Room (NPR) is an essential requirement by the Bio-Safety Levels (BSLs) in biolabs or infectious-control hospitals to prevent deadly pathogens from being leaked from the facility. An NPR maintains a negative pressure inside with respect to the outside reference space so that microbes are contained inside of an NPR. Nowadays, differential pressure sensors (DPSs) are utilized by the Building Management Systems (BMSs) to control and monitor the negative pressure in an NPR. This paper demonstrates a non-invasive and stealthy attack on NPRs by spoofing a DPS at its resonant frequency. Our contributions are: (1) We show that DPSs used in NPRs typically have resonant frequencies in the audible range. (2) We use this finding to design malicious music to create resonance in DPSs, resulting in an overshooting in the DPS's normal pressure readings. (3) We show how the resonance in DPSs can fool the BMSs so that the NPR turns its negative pressure to a positive one, causing a potential leak of deadly microbes from NPRs. We do experiments on 8 DPSs from 5 different manufacturers to evaluate their resonant frequencies considering the sampling tube length and find resonance in 6 DPSs. We can achieve a 2.5 Pa change in negative pressure from a ~7 cm distance when a sampling tube is not present and from a ~2.5 cm distance for a 1 m sampling tube length. We also introduce an interval-time variation approach for an adversarial control over the negative pressure and show that the forged pressure can be varied within 12 - 33 Pa. Our attack is also capable of attacking multiple NPRs simultaneously. Moreover, we demonstrate our attack at a real-world NPR located in an anonymous bioresearch facility, which is FDA approved and follows CDC guidelines. We also provide countermeasures to prevent the attack.},
booktitle = {Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},
pages = {277–291},
numpages = {15},
keywords = {resonance, pressure sensors, pathogens, negative pressure room},
location = {Los Angeles, CA, USA},
series = {CCS '22}
}

@inproceedings{10.1145/3561212.3561223,
author = {Kwak, Dongho and Krzyzaniak, Michael J. and Danielsen, Anne and Jensenius, Alexander Refsum},
title = {A mini acoustic chamber for small-scale sound experiments},
year = {2022},
isbn = {9781450397018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3561212.3561223},
doi = {10.1145/3561212.3561223},
abstract = {This paper describes the design and construction of a mini acoustic chamber using low-cost materials. The primary purpose is to provide an acoustically treated environment for small-scale sound measurements and experiments using ≤ &nbsp;10-inch speakers. Testing with different types of speakers showed frequency responses of &lt;&nbsp;10&nbsp;dB peak-to-peak (except the ”boxiness” range below 900&nbsp;Hz), and the acoustic insulation (soundproofing) of the chamber is highly efficient (approximately 20&nbsp;dB&nbsp;SPL in reduction). Therefore, it provides a significant advantage in conducting experiments requiring a small room with consistent frequency response and preventing unwanted noise and hearing damage. Additionally, using a cost-effective and compact acoustic chamber gives flexibility when characterizing a small-scale setup and sound stimuli used in experiments.},
booktitle = {Proceedings of the 17th International Audio Mostly Conference},
pages = {143–146},
numpages = {4},
keywords = {sound measurement, sound insulation, acoustic experiment, Mini acoustic chamber},
location = {St. P\"{o}lten, Austria},
series = {AM '22}
}

@proceedings{10.1145/3698062,
title = {WSSE '24: Proceedings of the 2024 The 6th World Symposium on Software Engineering (WSSE)},
year = {2024},
isbn = {9798400717086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3540250,
title = {ESEC/FSE 2022: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of all members of the organizing committee, we are delighted to welcome everyone to the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2022. The event continues the long, distinguished ESEC/FSE tradition of presenting the most innovative research, and facilitating interactions between scientists and engineers who are passionate about advancing the theory and practice of software engineering.},
location = {Singapore, Singapore}
}

@article{10.1145/3643553,
author = {Sun, Ke and Xia, Chunyu and Zhang, Xinyu and Chen, Hao and Zhang, Charlie Jianzhong},
title = {Multimodal Daily-Life Logging in Free-living Environment Using Non-Visual Egocentric Sensors on a Smartphone},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
url = {https://doi.org/10.1145/3643553},
doi = {10.1145/3643553},
abstract = {Egocentric non-intrusive sensing of human activities of daily living (ADL) in free-living environments represents a holy grail in ubiquitous computing. Existing approaches, such as egocentric vision and wearable motion sensors, either can be intrusive or have limitations in capturing non-ambulatory actions. To address these challenges, we propose EgoADL, the first egocentric ADL sensing system that uses an in-pocket smartphone as a multi-modal sensor hub to capture body motion, interactions with the physical environment and daily objects using non-visual sensors (audio, wireless sensing, and motion sensors). We collected a 120-hour multimodal dataset and annotated 20-hour data into 221 ADL, 70 object interactions, and 91 actions. EgoADL proposes multi-modal frame-wise slow-fast encoders to learn the feature representation of multi-sensory data that characterizes the complementary advantages of different modalities and adapt a transformer-based sequence-to-sequence model to decode the time-series sensor signals into a sequence of words that represent ADL. In addition, we introduce a self-supervised learning framework that extracts intrinsic supervisory signals from the multi-modal sensing data to overcome the lack of labeling data and achieve better generalization and extensibility. Our experiments in free-living environments demonstrate that EgoADL can achieve comparable performance with video-based approaches, bringing the vision of ambient intelligence closer to reality.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = mar,
articleno = {17},
numpages = {32},
keywords = {Daily-life logging, Egocentric non-visual sensors, Multi-modal data}
}

@inproceedings{10.1145/1456659.1456662,
author = {Chapman, Mark and van der Merwe, Alta},
title = {Contemplating systematic software reuse in a project-centric company},
year = {2008},
isbn = {9781605582863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1456659.1456662},
doi = {10.1145/1456659.1456662},
abstract = {Systematic software reuse is still the most promising strategy for increasing productivity and improving quality in the software industry. Although it is simple in concept, successful software reuse implementation is difficult in practice. A reason put forward for this is the dependence of software reuse on the context in which it is implemented. This paper describes an interpretive case study aimed at investigating the potential for the implementation of systematic software reuse in a project-centric company. The study confirmed the need for systematic software reuse and identified the reuse issues that could present challenges. The study also revealed a number of problems relating to the project-centric structure for which systematic reuse provides potential solutions.},
booktitle = {Proceedings of the 2008 Annual Research Conference of the South African Institute of Computer Scientists and Information Technologists on IT Research in Developing Countries: Riding the Wave of Technology},
pages = {16–26},
numpages = {11},
keywords = {systematic software reuse, software reuse, software product lines, software product line engineering, project-centric, interpretive case study, ethnography, action research},
location = {Wilderness, South Africa},
series = {SAICSIT '08}
}

@inproceedings{10.1145/3644815.3644965,
author = {Barreto Simedo Pacheco, Lorena and Rahman, Musfiqur and Rabbi, Fazle and Fathollahzadeh, Pouya and Abdellatif, Ahmad and Shihab, Emad and Chen, Tse-Hsun (Peter) and Yang, Jinqiu and Zou, Ying},
title = {DVC in Open Source ML-development: The Action and the Reaction},
year = {2024},
isbn = {9798400705915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644815.3644965},
doi = {10.1145/3644815.3644965},
abstract = {Machine Learning (ML) systems are gaining popularity, reshaping various domains ranging from customer services to software engineering. The effectiveness of ML systems is dependent on the quality of their training data. Therefore, practitioners invest substantial time experimenting with different data, parameters, and models to guarantee the quality of the end system. Prior work highlighted unique challenges of developing ML systems, particularly concerning versioning data and models. Recently, various tools such as DVC and MLFlow have emerged to aid developers in the storage and tracking of data. Despite their growing popularity, very little is known about their usage patterns and impact on open-source software (OSS) systems. To address this gap, we conducted an empirical study on 56 GitHub OSS projects that use DVC to understand the DVC usage pattern and the impact of using DVC on the software development process. We found that Versioning and tracking is the most adopted DVC feature, being utilized by all 56 projects and being the only adopted feature in 85.7% of them. Furthermore, we found that DVC has a significant impact on the software development process indicators such as the number of created pull requests (PRs), and the number of bug-fix commits. For instance, our findings showed that DVC causes a peak in the number of commits and PRs at the moment of the adoption, followed by a long-term decrease. We believe that our findings can assist practitioners in tailoring tools to better meet user requirements and help organizations realize potential outcomes of adopting such tools.},
booktitle = {Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI},
pages = {75–80},
numpages = {6},
keywords = {empirical software engineering, data version control, software evolution, SE4AI},
location = {Lisbon, Portugal},
series = {CAIN '24}
}

@inproceedings{10.1145/2910019.2910101,
author = {Cledou, Guillermina and Barbosa, Lu\'{\i}s Soares},
title = {An Ontology for Licensing Public Transport Services},
year = {2016},
isbn = {9781450336406},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910019.2910101},
doi = {10.1145/2910019.2910101},
abstract = {By 2050 it is expected that 66% of the world population will reside in cities, compared to 54% in 2014. One particular challenge associated to urban population growth refers to transportation systems, and as an approach to face it, governments are investing significant efforts enhancing public transport services. An important aspect of public transport is ensuring that licensing of such services fulfill existing government regulations. Due to the differences in government regulations, and to the difficulties in ensuring the fulfillment of their specific features, many local governments develop tailored Information and Communication Technology (ICT) solutions to automate the licensing of public transport services. In this paper we propose an ontology for licensing such services following the REFSENO methodology. In particular, the ontology captures common concepts involved in the application and processing stage of licensing public bus passenger services. The main contribution of the proposed ontology is to define a common vocabulary to share knowledge between domain experts and software engineers, and to support the definition of a software product line for families of public transport licensing services.},
booktitle = {Proceedings of the 9th International Conference on Theory and Practice of Electronic Governance},
pages = {230–239},
numpages = {10},
keywords = {Software Product Lines, Public Transport Licensing Services, Ontologies},
location = {Montevideo, Uruguay},
series = {ICEGOV '15-16}
}

@inproceedings{10.1145/3560905.3568518,
author = {Chen, Qianniu and Chen, Meng and Lu, Li and Yu, Jiadi and Chen, Yingying and Wang, Zhibo and Ba, Zhongjie and Lin, Feng and Ren, Kui},
title = {Push the Limit of Adversarial Example Attack on Speaker Recognition in Physical Domain},
year = {2023},
isbn = {9781450398862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560905.3568518},
doi = {10.1145/3560905.3568518},
abstract = {The integration of deep learning on Speaker Recognition (SR) advances its development and wide deployment, but also introduces the emerging threat of adversarial examples. However, only a few existing studies investigate its practical threat in physical domain, which either evaluate its feasibility only by directly replaying generated adversarial examples, or explore the partial channel interference for robustness improvement. In this paper, we propose a physical adversarial example attack, PhyTalker, which could generate and inject perturbations on voices in a live-streaming manner on attacking various SR models in different physical channels. Compared with the typical adversarial example for digital attacks, PhyTalker generates a subphoneme-level perturbation dictionary to decouple the perturbation optimization and injection. Moreover, we introduce the channel augmentation to compensate both device and environmental distortions, as well as model ensemble to improve the perturbation transferability. Finally, PhyTalker recognizes and localizes the latest recorded phoneme to determine the corresponding perturbations for real-time broadcasting. Extensive experiments are conducted with a large-scale corpus in real physical scenarios, and results show that PhyTalker achieves an overall Attack Success Rate (ASR) of 85.5% in attacking mainstream SR systems and Mel Cepstral Distortion (MCD) of 2.45dB in human audibility.},
booktitle = {Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems},
pages = {710–724},
numpages = {15},
keywords = {adversarial example attack, live-streaming, physical domain, speaker recognition},
location = {Boston, Massachusetts},
series = {SenSys '22}
}

@article{10.1145/3011286.3011302,
author = {Echeverr\'{\i}a, Jorge},
title = {Research on Augmenting the MDD Process with Variability Modeling},
year = {2017},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/3011286.3011302},
doi = {10.1145/3011286.3011302},
abstract = {Software Product Lines (SPLs) have proven to be successful at reducing the costs and time to market of product development through the planned reuse of software components into products within the same scope. SPL adoption has been typically regarded to follow a proactive approach, although recent surveys show that most of the SPLs are planned following reactive approaches. It seems necessary to refocus SPL engineering research, methodologies and tools for existing systems into SPL. We believe that systems following a Model Driven Development (MDD) approach can highly benefit from these re-engineering efforts, in order to enable them to manage variability. The aim of this research is to analyze how to improve the MDD process with variability modeling in real industrial environments. Nowadays, we have performed three empirical studies related to variability modeling in MDD approaches. These studies are the following: (1) an usability evaluation of a MDD approach with variability modeling, (2) comprehensibility of variability in model fragments for product configuration and (3) an evaluation about bug-fixing in a MDD-SPL tool},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {1–6},
numpages = {6},
keywords = {Variability Modeling, Usability Evaluation, Model Driven Development}
}

@proceedings{10.1145/3526114,
title = {UIST '22 Adjunct: Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
year = {2022},
isbn = {9781450393218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bend, OR, USA}
}

@inproceedings{10.1007/978-3-642-33666-9_17,
author = {Schroeter, Julia and Lochau, Malte and Winkelmann, Tim},
title = {Multi-perspectives on feature models},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_17},
doi = {10.1007/978-3-642-33666-9_17},
abstract = {Domain feature models concisely express commonality and variability among variants of a software product line. For supporting separation of concerns, e.g., due to legal restrictions, technical considerations and business requirements, multi-view approaches restrict the configuration choices on feature models for different stakeholders. However, recent approaches lack a formalization for precise, yet flexible specifications of views that ensure every derivable configuration perspective to obey feature model semantics. Here, we introduce a novel approach for preconfiguring feature models to create multi-perspectives. Such customized perspectives result from composition of various concern-relevant views. A structured view model is used to organize features in view groups, wherein a feature may be contained in multiple views. We provide formalizations for view composition and guaranteed consistency of perspectives w.r.t. feature model semantics. Thereupon, an efficient algorithm to verify consistency for entire multi-perspectives is provided. We present an implementation and evaluate our concepts by means of various experiments.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {252–268},
numpages = {17},
keywords = {software product lines, preconfiguration, feature models, customization, automated view composition},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@inproceedings{10.1145/3664475.3664734,
author = {Zicarelli, David},
title = {Advances in Real Time Audio Rendering - Part 2},
year = {2024},
isbn = {9798400706837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664475.3664734},
doi = {10.1145/3664475.3664734},
abstract = {We originally referred to "real-time" audio systems to draw a distinction with "non-real-time" systems where a series of audio samples is entirely determined and computed in advance, originally because computers were not fast enough to perform the needed mathematical calculations. If you can't listen to the sound as it is produced, you won't be able to change it live and know what you're changing. Thus, the desire to turn a computer into something more like a "musical instrument" was a primary motivation in the development of real-time audio systems.},
booktitle = {ACM SIGGRAPH 2024 Courses},
articleno = {7},
numpages = {16},
location = {Denver, CO, USA},
series = {SIGGRAPH Courses '24}
}

@inproceedings{10.1109/ICSE.2017.64,
author = {Souto, Sabrina and d'Amorim, Marcelo and Gheyi, Rohit},
title = {Balancing soundness and efficiency for practical testing of configurable systems},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.64},
doi = {10.1109/ICSE.2017.64},
abstract = {Testing configurable systems is important and challenging due to the enormous space of configurations where errors can hide. Existing approaches to test these systems are often costly or unreliable. This paper proposes S-SPLat, a technique that combines heuristic sampling with symbolic search to obtain both breadth and depth in the exploration of the configuration space. S-SPLat builds on SPLat, our previously developed technique, that explores all reachable configurations from tests. In contrast to its predecessor, S-SPLat sacrifices soundness in favor of efficiency. We evaluated our technique on eight software product lines of various sizes and on a large configurable system - GCC. Considering the results for GCC, S-SPLat was able to reproduce all five bugs that we previously found in a previous study with SPLat but much faster and it was able to find two new bugs in a recent release of GCC. Results suggest that it is preferable to use a combination of simple heuristics to drive the symbolic search as opposed to a single heuristic. S-SPLat and our experimental infrastructure are publicly available.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {632–642},
numpages = {11},
keywords = {testing, sampling, configuration},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@article{10.1145/2807593,
author = {Baudry, Benoit and Monperrus, Martin},
title = {The Multiple Facets of Software Diversity: Recent Developments in Year 2000 and Beyond},
year = {2015},
issue_date = {September 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2807593},
doi = {10.1145/2807593},
abstract = {Early experiments with software diversity in the mid 1970s investigated N-version programming and recovery blocks to increase the reliability of embedded systems. Four decades later, the literature about software diversity has expanded in multiple directions: goals (fault tolerance, security, software engineering), means (managed or automated diversity), and analytical studies (quantification of diversity and its impact). Our article contributes to the field of software diversity as the first work that adopts an inclusive vision of the area, with an emphasis on the most recent advances in the field. This survey includes classical work about design and data diversity for fault tolerance, as well as the cybersecurity literature that investigates randomization at different system levels. It broadens this standard scope of diversity to include the study and exploitation of natural diversity and the management of diverse software products. Our survey includes the most recent works, with an emphasis from 2000 to the present. The targeted audience is researchers and practitioners in one of the surveyed fields who miss the big picture of software diversity. Assembling the multiple facets of this fascinating topic sheds a new light on the field.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {16},
numpages = {26},
keywords = {program transformation, design principles, Software diversity}
}

@article{10.1145/3631120,
author = {Zhang, Guoming and Ji, Xiaoyu and Zhou, Xinyan and Qi, Donglian and Xu, Wenyuan},
title = {Ultrasound Communication Using the Nonlinearity Effect of Microphone Circuits in Smart Devices},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1550-4859},
url = {https://doi.org/10.1145/3631120},
doi = {10.1145/3631120},
abstract = {Acoustic communication has become a research focus without requiring extra hardware and facilitates numerous near-field applications such as mobile payment. To communicate, existing researchers use either an audible frequency band or an inaudible one. The former gains a high throughput but endures being audible, which can be annoying to users. The latter, although inaudible, falls short in throughput due to the available (near) ultrasonic bandwidth. In this article, we achieve both high speed and inaudibility for acoustic communication by utilizing the nonlinearity effect on microphones. We theoretically prove the maximum throughput of inaudible acoustic communication by modulating an audible signal onto an ultrasonic band. Then, we design and implement UltraComm, which utilizes a specially designed OFDM scheme. The scheme takes into account the characteristics of the nonlinear speaker-to-microphone channel, aiming to mitigate the effects of signal distortion. We evaluate UltraComm on different mobile devices and achieve throughput as high as 16.24 kbps.},
journal = {ACM Trans. Sen. Netw.},
month = feb,
articleno = {53},
numpages = {22},
keywords = {Ultrasound, microphone, inaudible acoustic communication, nonlinearity}
}

@inproceedings{10.1145/1454268.1454275,
author = {Bertoncello, Ivo Augusto and Dias, Marcelo Oliveira and Brito, Patrick H. S. and Rubira, Cec\'{\i}lia M. F.},
title = {Explicit exception handling variability in component-based product line architectures},
year = {2008},
isbn = {9781605582290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1454268.1454275},
doi = {10.1145/1454268.1454275},
abstract = {Separation of concerns is one of the overarching goals of exception handling in order to keep separate normal and exceptional behaviour of a software system. In the context of a software product line (SPL), this separation of concerns is also important for designing software variabilities related to different exception handling strategies, such as the choice of different handlers depending on the set of selected features. This paper presents a method for refactoring object-oriented product line architecture in order to separate explicitly their normal and exceptional behaviour into different software components. The new component-based software architecture includes variation points related to different choices of exception handlers that can be selected during product instantiations, thus facilitating the evolution of the exceptional behaviour. The feasibility of the proposed approach is assessed through a SPL of mobile applications.},
booktitle = {Proceedings of the 4th International Workshop on Exception Handling},
pages = {47–54},
numpages = {8},
keywords = {software architecture, exceptional behaviour, exception handling, component-based software development},
location = {Atlanta, Georgia},
series = {WEH '08}
}

@inproceedings{10.1145/1774088.1774566,
author = {Ballance, Robert A. and Cook, Jonathan},
title = {Monitoring MPI programs for performance characterization and management control},
year = {2010},
isbn = {9781605586397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1774088.1774566},
doi = {10.1145/1774088.1774566},
abstract = {Monitoring distributed programs on high performance supercomputers is a challenging task, yet it is essential for the proper administration of the machines and for users to understand what their program is doing on production runs. To this end, we created a flexible monitoring capability for a major class of scientific applications, programs using MPI, that efficiently gathers information from the distributed program and collects it at a central point. This data can then be used to both understand application-centric issues and system-centric issues; and for improvement, administration, and maintenance of both the complex applications producing important scientific results and the complex systems that execute them.},
booktitle = {Proceedings of the 2010 ACM Symposium on Applied Computing},
pages = {2305–2310},
numpages = {6},
keywords = {software monitoring, scientific applications},
location = {Sierre, Switzerland},
series = {SAC '10}
}

@article{10.1145/3708527,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Marchezan, Luciano and Arkoh, Lawrence and Egyed, Alexander and Ramler, Rudolf},
title = {Contemporary Software Modernization: Strategies, Driving Forces, and Research Opportunities},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708527},
doi = {10.1145/3708527},
abstract = {Software modernization is a common activity in software engineering, since technologies advance, requirements change, and business models evolve. Differently from conventional software evolution (e.g., adding new features, enhancing performance, or adapting to new requirements), software modernization involves re-engineering entire legacy systems (e.g., changing the technology stack, migrating to a new architecture style, or programming paradigms). Given the pervasive nature of software today, modernizing legacy systems is paramount to provide customers with competitive and innovative products and services, while keeping companies profitable. Despite the prevalent discussion of software modernization in gray literature, and the many papers in the literature, there is no work presenting a “big picture” of contemporary software modernization, describing challenges, and providing a well-defined research agenda. The goal of this work is to describe the state of the art in software modernization in the past 10 years. We collect the state of the art by performing a rapid review (searching five digital libraries), identifying potential 3,460 studies, leading to a final set of 127. We analyzed these studies to understand which strategies are employed, the driving forces that lead organizations to modernize their systems, and the challenges that need to be addressed. The results show that studies in the last 10 years have explored eight strategies for modernizing legacy systems, namely cloudification, architecture redesign, moving to a new programming language, targeting reuse optimization, software modernization for new hardware integration, practices to leverage automation, database modernization, and digital transformation. Modernization is triggered by 14 driving forces, with the most common ones being reducing operational costs, improving performance and scalability, and reducing complexity. In addition, based on the analysis of existing literature, we present a detailed discussion of research opportunities in this field. The main challenges are providing tooling support, followed by defining a modernization process and considering better evaluation metrics. The main contribution of our work is to equip practitioners and researchers with knowledge of the current state of contemporary software modernization so that they are aware of practices and challenges to be addressed when deciding to modernize legacy systems.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {Software Evolution, Software Migration, Re-designing, Re-engineering}
}

@inproceedings{10.1145/2463372.2463545,
author = {Wang, Shuai and Ali, Shaukat and Gotlieb, Arnaud},
title = {Minimizing test suites in software product lines using weight-based genetic algorithms},
year = {2013},
isbn = {9781450319638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463372.2463545},
doi = {10.1145/2463372.2463545},
abstract = {Test minimization techniques aim at identifying and eliminating redundant test cases from test suites in order to reduce the total number of test cases to execute, thereby improving the efficiency of testing. In the context of software product line, we can save effort and cost in the selection and minimization of test cases for testing a specific product by modeling the product line. However, minimizing the test suite for a product requires addressing two potential issues: 1) the minimized test suite may not cover all test requirements compared with the original suite; 2) the minimized test suite may have less fault revealing capability than the original suite. In this paper, we apply weight-based Genetic Algorithms (GAs) to minimize the test suite for testing a product, while preserving fault detection capability and testing coverage of the original test suite. The challenge behind is to define an appropriate fitness function, which is able to preserve the coverage of complex testing criteria (e.g., Combinatorial Interaction Testing criterion). Based on the defined fitness function, we have empirically evaluated three different weight-based GAs on an industrial case study provided by Cisco Systems, Inc. Norway. We also presented our results of applying the three weight-based GAs on five existing case studies from the literature. Based on these case studies, we conclude that among the three weight-based GAs, Random-Weighted GA (RWGA) achieved significantly better performance than the other ones.},
booktitle = {Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation},
pages = {1493–1500},
numpages = {8},
keywords = {weight-based gas, test minimization, feature pairwise coverage, fault detection capability},
location = {Amsterdam, The Netherlands},
series = {GECCO '13}
}

@article{10.1145/3492762,
author = {Sobhy, Dalia and Minku, Leandro and Bahsoon, Rami and Kazman, Rick},
title = {Continuous and Proactive Software Architecture Evaluation: An IoT Case},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3492762},
doi = {10.1145/3492762},
abstract = {Design-time evaluation is essential to build the initial software architecture to be deployed. However, experts’ assumptions made at design-time are unlikely to remain true indefinitely in systems that are characterized by scale, hyperconnectivity, dynamism, and uncertainty in operations (e.g. IoT). Therefore, experts’ design-time decisions can be challenged at run-time. A continuous architecture evaluation that systematically assesses and intertwines design-time and run-time decisions is thus necessary. This paper proposes the first proactive approach to continuous architecture evaluation of the system leveraging the support of simulation. The approach evaluates software architectures by not only tracking their performance over time, but also forecasting their likely future performance through machine learning of simulated instances of the architecture. This enables architects to make cost-effective informed decisions on potential changes to the architecture. We perform an IoT case study to show how machine learning on simulated instances of architecture can fundamentally guide the continuous evaluation process and influence the outcome of architecture decisions. A series of experiments is conducted to demonstrate the applicability and effectiveness of the approach. We also provide the architect with recommendations on how to best benefit from the approach through choice of learners and input parameters, grounded on experimentation and evidence.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {46},
numpages = {54},
keywords = {IoT, time series forecasting, software architecture evaluation, Continuous evaluation}
}

@article{10.1145/1988997.2003645,
author = {Thurimella, Anil Kumar},
title = {On the communication problem between domain engineering and application engineering: formalism using sets, conflicts of-interests and artifact redundancies},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1988997.2003645},
doi = {10.1145/1988997.2003645},
abstract = {Software Product Line Engineering enables customization and reuse during the development of software intensive systems. A typical product line process consists of a domain engineering process and several application engineering processes. We use two different heuristics to characterize artifact redundancies in a product line system: 1) artifacts should not be developed redundantly across domain engineering and application engineering and 2) no two application engineering teams should develop same artifacts redundantly. To provide a formal basis for the heuristics, we derive consistency equations by using mathematical notations of sets. We also use these consistency equations to describe artifact redundancies that occur due to conflicts-of-interests between domain engineering and application engineering. In particular, conflicts-of interests during product line scoping, product instantiation and product line evolution are covered. Furthermore, based on a literature review, we elicit several requirements to address the conflicts-of-interests and artifact redundancies.},
journal = {SIGSOFT Softw. Eng. Notes},
month = aug,
pages = {1–5},
numpages = {5},
keywords = {software product lines, sets, reuse, consistency}
}

@proceedings{10.1145/3571788,
title = {VaMoS '23: Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Odense, Denmark}
}

@article{10.1109/TASLP.2024.3356980,
author = {\"{O}zer, Yigitcan and M\"{u}ller, Meinard},
title = {Source Separation of Piano Concertos Using Musically Motivated Augmentation Techniques},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3356980},
doi = {10.1109/TASLP.2024.3356980},
abstract = {In this work, we address the novel and rarely considered source separation task of decomposing piano concerto recordings into separate piano and orchestral tracks. Being a genre written for a pianist typically accompanied by an ensemble or orchestra, piano concertos often involve an intricate interplay of the piano and the entire orchestra, leading to high spectro–temporal correlations between the constituent instruments. Moreover, in the case of piano concertos, the lack of multi-track data for training constitutes another challenge in view of data-driven source separation approaches. As a basis for our work, we adapt existing deep learning (DL) techniques, mainly used for the separation of popular music recordings. In particular, we investigate spectrogram- and waveform-based approaches as well as hybrid models operating in both spectrogram and waveform domains. As a main contribution, we introduce a musically motivated data augmentation approach for training based on artificially generated samples. Furthermore, we systematically investigate the effects of various augmentation techniques for DL-based models. For our experiments, we use a recently published, open-source dataset of multi-track piano concerto recordings. Our main findings demonstrate that the best source separation performance is achieved by a hybrid model when combining all augmentation techniques.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1214–1225},
numpages = {12}
}

@inproceedings{10.1145/3319008.3319356,
author = {Neto, Amadeu Anderlin and Kalinowski, Marcos and Garcia, Alessandro and Winkler, Dietmar and Biffl, Stefan},
title = {A Preliminary Comparison of Using Variability Modeling Approaches to Represent Experiment Families},
year = {2019},
isbn = {9781450371452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319008.3319356},
doi = {10.1145/3319008.3319356},
abstract = {Background: Replication is essential to build knowledge in empirical science. Experiment replications reported in the software engineering context present variabilities on their design elements, e.g., variables, materials. The understanding of these variabilities is required to plan experimental replications within a research program. However, the lack of an explicit representation of experiments' variabilities and commonalities is likely to hamper their understanding and replication planning. Aims: The goal of this paper is to explore the use of Variability Modeling Approaches (VMAs) to represent experiment families (i.e., an original study and its replications) and to investigate the feasibility of using VMAs to support experiment replication planning. Method: We selected two experiment families, analyzed their commonalities and variabilities, and represented them using a set of well-known VMAs: Feature Model, Decision Model, and Orthogonal Variability Model. Based on the resulting models, we conducted a preliminary comparison of using such alternative VMAs to support replication planning. Results: Subjects were able to plan consistent experiment replications with the VMAs as support. Additionally, through a qualitative analysis, we identified and discuss advantages and limitations of using the VMAs. Conclusions: It is feasible to represent experiment families and to plan replications using VMAs. Based on our emerging results, we conclude that the Feature Model VMA provides the most suitable representation. Furthermore, we identified benefits in a potential merge between the Feature Model and Decision Model VMAs to provide more details to support replication planning.},
booktitle = {Proceedings of the 23rd International Conference on Evaluation and Assessment in Software Engineering},
pages = {333–338},
numpages = {6},
keywords = {experiment replication, experiment lines, Experiment planning},
location = {Copenhagen, Denmark},
series = {EASE '19}
}

@article{10.1145/3571854,
author = {Zampetti, Fiorella and Tamburri, Damian and Panichella, Sebastiano and Panichella, Annibale and Canfora, Gerardo and Di Penta, Massimiliano},
title = {Continuous Integration and Delivery Practices for Cyber-Physical Systems: An Interview-Based Study},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3571854},
doi = {10.1145/3571854},
abstract = {Continuous Integration and Delivery (CI/CD) practices have shown several benefits for software development and operations, such as faster release cycles and early discovery of defects. For Cyber-Physical System (CPS) development, CI/CD can help achieving required goals, such as high dependability, yet it may be challenging to apply. This article empirically investigates challenges, barriers, and their mitigation occurring when applying CI/CD practices to develop CPSs in 10 organizations working in eight different domains. The study has been conducted through semi-structured interviews, by applying an open card sorting procedure together with a member-checking survey within the same organizations, and by validating the results through a further survey involving 55 professional developers. The study reveals several peculiarities in the application of CI/CD to CPSs. These include the need for (i) combining continuous and periodic builds while balancing the use of Hardware-in-the-Loop and simulators, (ii) coping with difficulties in software deployment (iii) accounting for simulators and Hardware-in-the-Loop differing in their behavior, and (vi) combining hardware/software expertise in the development team. Our findings open the road toward recommenders aimed at supporting the setting and evolution of CI/CD pipelines, as well as university curricula requiring interdisciplinarity, such as knowledge about hardware, software, and their interplay.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {73},
numpages = {44},
keywords = {empirical software engineering, Cyber-Physical Systems, Continuous Integration and Delivery}
}

@inproceedings{10.1145/3626705.3627785,
author = {Katins, Christopher and Feger, Sebastian Stefan and Kosch, Thomas},
title = {Pilots' Considerations Regarding Current Generation Mixed Reality Headset Use in General Aviation Cockpits},
year = {2023},
isbn = {9798400709210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626705.3627785},
doi = {10.1145/3626705.3627785},
abstract = {Pilots in non-commercial aviation have minimal access to digital support tools. Common 2D maps, displayed on tablets, are often the only digital information source that fails to adequately capture the 3D airspace and its surroundings, challenging the pilot’s workload and awareness. In this work, we developed and tested a Mixed Reality (MR) prototype with twelve General Aviation (GA) pilots using a full-sized flight simulator environment. The prototype’s demonstration showcased the capabilities of contemporary technology and its potential applications. Following the simulation, in-depth interviews were conducted with the participating pilots to discern their perspectives on integrating MR solutions into cockpit environments. The study revealed valuable insights into pilots’ concerns, design prerequisites for future systems, and potential use cases. This work not only highlights the feasibility of MR implementations but also provides a foundation for the development of enhanced digital tools for GA, aiming to alleviate pilot workload and augment situational awareness.},
booktitle = {Proceedings of the 22nd International Conference on Mobile and Ubiquitous Multimedia},
pages = {159–165},
numpages = {7},
keywords = {Augmented Reality, General Aviation, Highlighting, Mixed Reality, Workload},
location = {Vienna, Austria},
series = {MUM '23}
}

@inproceedings{10.1109/ASE.2011.6100075,
author = {Apel, Sven and Speidel, Hendrik and Wendler, Philipp and von Rhein, Alexander and Beyer, Dirk},
title = {Detection of feature interactions using feature-aware verification},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100075},
doi = {10.1109/ASE.2011.6100075},
abstract = {A software product line is a set of software products that are distinguished in terms of features (i.e., end-user-visible units of behavior). Feature interactions-- situations in which the combination of features leads to emergent and possibly critical behavior --are a major source of failures in software product lines. We explore how feature-aware verification can improve the automatic detection of feature interactions in software product lines. Feature-aware verification uses product-line-verification techniques and supports the specification of feature properties along with the features in separate and composable units. It integrates the technique of variability encoding to verify a product line without generating and checking a possibly exponential number of feature combinations. We developed the tool suite SPLVERIFIER for feature-aware verification, which is based on standard model-checking technology. We applied it to an e-mail system that incorporates domain knowledge of AT&amp;T. We found that feature interactions can be detected automatically based on specifications that have only local knowledge.},
booktitle = {Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {372–375},
numpages = {4},
series = {ASE '11}
}

@inproceedings{10.1145/3605390.3605422,
author = {Privitera, Alessandro Giuseppe and Fontana, Federico and Geronazzo, Michele},
title = {On the Effect of User Tracking on Perceived Source Positions in Mobile Audio Augmented Reality},
year = {2023},
isbn = {9798400708060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605390.3605422},
doi = {10.1145/3605390.3605422},
abstract = {Mobile Audio Augmented Reality (AAR) allows users to live sonic experience where virtual sound objects are integrated seamlessly with the real-world acoustic space. This paper focuses on user perception of the virtual source position and the impact of different out-of-the-box user tracking and head orientation methods made available by iPad’s Augmented Reality (AR) camera tracking system and AirPods Pro. We propose an experimental procedure to rank tracking approaches according to perceived accuracy in positioning eye-level and ground-level virtual audio sources compared to real source references. To correctly provide a plausible AAR scenario, the proposed consumer electronic setup simulates a virtual sound source employing the scattering delay network (SDN) algorithm for calibrated dynamic auralisation and customized head-related transfer functions (HRTFs) for personalized user acoustics. The natural listening experience of real sound sources leverages AirPods’s active signal processing algorithms for headphone hear-through capabilities. The main result of this study lies in observing an accommodation effect by users interacting with different tracking approaches and sound source positions. In summary, participants tend to prefer more stable tracking solutions despite accuracy and wide head movement range.},
booktitle = {Proceedings of the 15th Biannual Conference of the Italian SIGCHI Chapter},
articleno = {32},
numpages = {9},
keywords = {Virtual Acoustics, User tracking, Spatial audio, Sonic interactions in virtual environments, Earables., Augumented reality},
location = {Torino, Italy},
series = {CHItaly '23}
}

@inproceedings{10.1145/3613424.3623787,
author = {Liu, Zeshi and Chen, Shuo and Qu, Peiyao and Liu, Huanli and Niu, Minghui and Ying, Liliang and Ren, Jie and Tang, Guangming and You, Haihang},
title = {SUSHI: Ultra-High-Speed and Ultra-Low-Power Neuromorphic Chip Using Superconducting Single-Flux-Quantum Circuits},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3623787},
doi = {10.1145/3613424.3623787},
abstract = {The rapid single-flux-quantum (RSFQ) superconducting technology is highly promising due to its ultra-high-speed computation with ultra-low-power consumption, making it an ideal solution for the post-Moore era. In superconducting technology, information is encoded and processed based on pulses that resemble the neuronal pulses present in biological neural systems. This has led to a growing research focus on implementing neuromorphic processing using superconducting technology. However, current research on superconducting neuromorphic processing does not fully leverage the advantages of superconducting circuits due to incomplete neuromorphic design and approach. Although they have demonstrated the benefits of using superconducting technology for neuromorphic hardware, their designs are mostly incomplete, with only a few components validated, or based solely on simulation. This paper presents SUSHI (Superconducting neUromorphic proceSsing cHIp) to fully leverage the potential of superconducting neuromorphic processing. Based on three guiding principles and our architectural and methodological designs, we address existing challenges and enables the design of verifiable and fabricable superconducting neuromorphic chips. We fabricate and verify a chip of SUSHI using superconducting circuit technology. Successfully obtaining the correct inference results of a complete neural network on the chip, this is the first instance of neural networks being completely executed on a superconducting chip to the best of our knowledge. Our evaluation shows that using approximately 105 Josephson junctions, SUSHI achieves a peak neuromorphic processing performance of 1,355 giga-synaptic operations per second (GSOPS) and a power efficiency of 32,366 GSOPS per Watt (GSOPS/W). This power efficiency outperforms the state-of-the-art neuromorphic chips TrueNorth and Tianjic by 81 and 50 times, respectively.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {614–627},
numpages = {14},
keywords = {Superconducting, Spiking Neural Networks, Single-Flux-Quantum, Neuromorphic},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.5555/2666064.2666076,
author = {Hu, Jie and Yang, Ye and Wang, Qing and Ruhe, Guenther and Wang, Haitao},
title = {Value-based portfolio scoping: an industrial case study},
year = {2012},
isbn = {9781467317511},
publisher = {IEEE Press},
abstract = {Customization is considered as a promising way for better satisfying diversity of customer needs. In organizations short of resources, it is a frequent challenge to get balance between development and customization workload in order to ensure product success as well as customer satisfaction. In this paper, we proposed a value-based product portfolio scoping approach to determine optimal product scale for planning software product line adoption. The approach blends existing methods in domain analysis, requirements clustering, and valuation theory. An industrial case study is presented to demonstrate the application of the approach and its effectiveness.},
booktitle = {Proceedings of the Third International Workshop on Product LinE Approaches in Software Engineering},
pages = {45–48},
numpages = {4},
keywords = {scoping, requirements analysis, product portfolio, product line, customization, cost benefit},
location = {Zurich, Switzerland},
series = {PLEASE '12}
}

@article{10.1145/3709159,
author = {P\"{a}\ss{}ler, Juliane and ter Beek, Maurice H. and Damiani, Ferruccio and Dubslaff, Clemens and Johnsen, Einar Broch and Tapia Tarifa, Silvia Lizeth},
title = {Feature-Oriented Modelling and Analysis of a Self-Adaptive Robotic System},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0934-5043},
url = {https://doi.org/10.1145/3709159},
doi = {10.1145/3709159},
abstract = {Improved autonomy in robotic systems is needed for innovation in, e.g., the marine sector. Autonomous robots that are let loose in hazardous environments, such as underwater, need to handle uncertainties that stem from both their environment and internal state. While self-adaptation is crucial to cope with these uncertainties, bad decisions may cause the robot to get lost or even to cause severe environmental damage. Autonomous, self-adaptive robots that operate in uncontrolled environments full of uncertainties need to be reliable! Since these uncertainties are hard to replicate in test deployments, we need methods to formally analyse self-adaptive robots operating in uncontrolled environments. In this paper, we show how feature-oriented techniques can be used to formally model and analyse self-adaptive robotic systems in the presence of such uncertainties. Self-adaptive systems can be organised as two-layered systems with a managed subsystem handling the domain concerns and a managing subsystem implementing the adaptation logic. We consider a case study of an autonomous underwater vehicle (AUV) for pipeline inspection, in which the managed subsystem of the AUV is modelled as a family of systems, where each family member corresponds to a valid configuration of the AUV which can be seen as an operating mode of the AUV’s behaviour. The managing subsystem of the AUV is modelled as a control layer that is capable of dynamically switching between such valid configurations, depending on both environmental and internal uncertainties. These uncertainties are captured in a probabilistic and highly configurable model. Our modelling approach allows us to exploit powerful formal methods for feature-oriented systems, which we illustrate by analysing safety properties, energy consumption, and multi-objective properties, as well as performing parameter synthesis to analyse to what extent environmental conditions affect the AUV. The case study is realised in the probabilistic feature-oriented modelling language and verification tool ProFeat, and in particular exploits family-based probabilistic and parametric model checking.},
note = {Just Accepted},
journal = {Form. Asp. Comput.},
month = jan,
keywords = {Feature models, Probabilistic model checking, Parametric model checking, Self-adaptive systems, Cyber-physical systems, Robotics}
}

@inproceedings{10.1145/2739480.2754720,
author = {Assun\c{c}\~{a}o, Wesley K.G. and Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Vergilio, Silvia R. and Egyed, Alexander},
title = {Extracting Variability-Safe Feature Models from Source Code Dependencies in System Variants},
year = {2015},
isbn = {9781450334723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739480.2754720},
doi = {10.1145/2739480.2754720},
abstract = {To effectively cope with increasing customization demands, companies that have developed variants of software systems are faced with the challenge of consolidating all the variants into a Software Product Line, a proven development paradigm capable of handling such demands. A crucial step in this challenge is to reverse engineer feature models that capture all the required feature combinations of each system variant. Current research has explored this task using propositional logic, natural language, and search-based techniques. However, using knowledge from the implementation artifacts for the reverse engineering task has not been studied. We propose a multi-objective approach that not only uses standard precision and recall metrics for the combinations of features but that also considers variability-safety, i.e. the property that, based on structural dependencies among elements of implementation artifacts, asserts whether all feature combinations of a feature model are in fact well-formed software systems. We evaluate our approach with five case studies and highlight its benefits for the software engineer.},
booktitle = {Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1303–1310},
numpages = {8},
keywords = {reverse engineering, multi-objective evolutionary algorithms, feature models},
location = {Madrid, Spain},
series = {GECCO '15}
}

@inproceedings{10.1145/1944892.1944899,
author = {Galster, Matthias and Avgeriou, Paris},
title = {The notion of variability in software architecture: results from a preliminary exploratory study},
year = {2011},
isbn = {9781450305709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944892.1944899},
doi = {10.1145/1944892.1944899},
abstract = {Context: In the software product line domain, the concept of variability is well recognized. However, variability in the context of software architecture still seems to be poorly understood. Objective: In this paper, we aim at contributing to the development of a basic understanding of the notion of variability in the software architecture domain, beyond the idea of product lines. Method: We perform a preliminary exploratory study which consists of two parts: an expert survey among 11 subjects, and a mini focus group with 4 participants. For both parts, we collect and analyze mostly qualitative data. Results: Our observations indicate that there seems to be no common understanding of "variability" in the context of software architecture. On the other hand, some challenges related to variability in software architecture are similar to challenges identified in the product line domain. Conclusions: Variability in software architecture might require more theoretical foundations in order to establish "variability" as an architectural key concept and first-class quality attribute.},
booktitle = {Proceedings of the 5th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {59–67},
numpages = {9},
keywords = {variability, software architecture, questionnaire, product lines, mini focus group},
location = {Namur, Belgium},
series = {VaMoS '11}
}

@inproceedings{10.1145/3634713.3634714,
author = {Kodetzki, Maximilian and Bordis, Tabea and Runge, Tobias and Schaefer, Ina},
title = {Partial Proofs to Optimize Deductive Verification of Feature-Oriented Software Product Lines},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634714},
doi = {10.1145/3634713.3634714},
abstract = {Software product lines (SPLs) are a technique to efficiently develop families of software products. Code is implemented in functional features which are composed to individual software variants. SPLs are oftentimes used in safety-critical systems, which is why functional correctness is more important than ever. As an advanced approach, deductive verification offers the possibility to verify the behaviour of software against a formal specification. When deductive verification is applied for SPLs, it meets the challenges of an SPLs variability. Since most verification approaches do not scale for variant-rich product lines, we take up existing approaches of reuse of proof parts to develop our concept of partial proofs. We split proofs into a feature-specific and a product-specific part. The feature-specific part is only proven once for all products enabling advanced proof reuse. We implement our concept of partial proofs in the tool VarCorC and evaluate it on three case studies. We found that both the number of proof steps and the verification time can be reduced by using partial proofs. Further, we determine a trend of increasing improvements of verification costs for large-scale SPLs.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {17–26},
numpages = {10},
keywords = {deductive verification, formal methods, software product lines},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.1145/3395351.3399421,
author = {Acar, Abbas and Fereidooni, Hossein and Abera, Tigist and Sikder, Amit Kumar and Miettinen, Markus and Aksu, Hidayet and Conti, Mauro and Sadeghi, Ahmad-Reza and Uluagac, Selcuk},
title = {Peek-a-boo: i see your smart home activities, even encrypted!},
year = {2020},
isbn = {9781450380065},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395351.3399421},
doi = {10.1145/3395351.3399421},
abstract = {A myriad of IoT devices such as bulbs, switches, speakers in a smart home environment allow users to easily control the physical world around them and facilitate their living styles through the sensors already embedded in these devices. Sensor data contains a lot of sensitive information about the user and devices. However, an attacker inside or near a smart home environment can potentially exploit the innate wireless medium used by these devices to exfiltrate sensitive information from the encrypted payload (i.e., sensor data) about the users and their activities, invading user privacy. With this in mind, in this work, we introduce a novel multi-stage privacy attack against user privacy in a smart environment. It is realized utilizing state-of-the-art machine-learning approaches for detecting and identifying the types of IoT devices, their states, and ongoing user activities in a cascading style by only passively sniffing the network traffic from smart home devices and sensors. The attack effectively works on both encrypted and unencrypted communications. We evaluate the efficiency of the attack with real measurements from an extensive set of popular off-the-shelf smart home IoT devices utilizing a set of diverse network protocols like WiFi, ZigBee, and BLE. Our results show that an adversary passively sniffing the traffic can achieve very high accuracy (above 90%) in identifying the state and actions of targeted smart home devices and their users. To protect against this privacy leakage, we also propose a countermeasure based on generating spoofed traffic to hide the device states and demonstrate that it provides better protection than existing solutions.},
booktitle = {Proceedings of the 13th ACM Conference on Security and Privacy in Wireless and Mobile Networks},
pages = {207–218},
numpages = {12},
keywords = {wifi, smart-home, privacy, network traffic, ZigBee, BLE},
location = {Linz, Austria},
series = {WiSec '20}
}

@inproceedings{10.1145/3458817.3476160,
author = {Shang, Honghui and Li, Fang and Zhang, Yunquan and Liu, Ying and Zhang, Libo and Wu, Mingchuan and Wu, Yangjun and Wei, Di and Cui, Huimin and Liu, Xin and Wang, Fei and Ye, Yuxi and Gao, Yingxiang and Ni, Shuang and Chen, Xin and Chen, Dexun},
title = {Accelerating all-electron ab initio simulation of raman spectra for biological systems},
year = {2021},
isbn = {9781450384421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458817.3476160},
doi = {10.1145/3458817.3476160},
abstract = {Raman spectroscopy provides chemical and compositional information that can serve as a structural fingerprint for various materials. Therefore, simulations of Raman spectra, including both quantum perturbation analyses and ground-state calculations are of significant interest. However, highly accurate full quantum mechanical (QM) simulations of Raman spectra have previously been confined to small systems. For large systems such as biological materials, the computational cost of full QM simulations is extremely high, and their extension to such systems remains challenging. In the work described here, by employing robust new algorithms and advances in implementation for the many-core architectures, we are able to perform fast, accurate, and massively parallel full ab initio simulations of the Raman spectra of biological systems with excellent strong and weak scaling, thereby providing a starting point for applying QM approaches to structural studies of such systems.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {41},
numpages = {15},
keywords = {scalability, quantum mechanics, many-core processor, biological systems, all-electron},
location = {St. Louis, Missouri},
series = {SC '21}
}

@proceedings{10.1145/3634713,
title = {VaMoS '24: Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bern, Switzerland}
}

@inproceedings{10.1145/1509239.1509259,
author = {Niu, Nan and Easterbrook, Steve},
title = {Concept analysis for product line requirements},
year = {2009},
isbn = {9781605584423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1509239.1509259},
doi = {10.1145/1509239.1509259},
abstract = {Traditional methods characterize a software product line's requirements using either functional or quality criteria. This appears to be inadequate to assess modularity, detect interferences, and analyze trade-offs. We take advantage of both symmetric and asymmetric views of aspects, and perform formal concept analysis to examine the functional and quality requirements of an evolving product line. The resulting concept lattice provides a rich notion which allows remarkable insights into the modularity and interactions of requirements. We formulate a number of problems that aspect-oriented product line requirements engineering should address, and present our solutions according to the concept lattice. We describe a case study applying our approach to analyze a mobile game product line's requirements, and review lessons learned.},
booktitle = {Proceedings of the 8th ACM International Conference on Aspect-Oriented Software Development},
pages = {137–148},
numpages = {12},
keywords = {quality attribute scenarios, product line engineering, functional requirements profiles, formal concept analysis},
location = {Charlottesville, Virginia, USA},
series = {AOSD '09}
}

@inproceedings{10.1109/ICPC.2017.21,
author = {Tang, Yutian and Leung, Hareton},
title = {Constructing feature model by identifying variability-aware modules},
year = {2017},
isbn = {9781538605356},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2017.21},
doi = {10.1109/ICPC.2017.21},
abstract = {Modeling variability, known as building feature models, should be an essential step in the whole process of product line development, maintenance and testing. The work on feature model recovery serves as a foundation and further contributes to product line development and variability-aware analysis. Different from the architecture recovery process even though they somewhat share the same process, the variability is not considered in all architecture recovery techniques. In this paper, we proposed a feature model recovery technique VMS, which gives a variability-aware analysis on the program and further constructs modules for feature model mining. With our work, we bring the variability information into architecture and build the feature model directly from the source base. Our experimental results suggest that our approach performs competitively and outperforms six other representative approaches for architecture recovery.},
booktitle = {Proceedings of the 25th International Conference on Program Comprehension},
pages = {263–274},
numpages = {12},
keywords = {variability-aware modularity, product line, feature modules, feature model recovery, configuration},
location = {Buenos Aires, Argentina},
series = {ICPC '17}
}

@inproceedings{10.1145/3613904.3642311,
author = {Hassan, Waseem and Marzo, Asier and Hornb\ae{}k, Kasper},
title = {Using Low-frequency Sound to Create Non-contact Sensations On and In the Body},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642311},
doi = {10.1145/3613904.3642311},
abstract = {This paper proposes a method for generating non-contact sensations using low-frequency sound waves without requiring user instrumentation. This method leverages the fundamental acoustic response of a confined space to produce predictable pressure spatial distributions at low frequencies, called modes. These modes can be used to produce sensations either throughout the body, in localized areas of the body, or within the body. We first validate the location and strength of the modes simulated by acoustic modeling. Next, a perceptual study is conducted to show how different frequencies produce qualitatively different sensations across and within the participants’ bodies. The low-frequency sound offers a new way of delivering non-contact sensations throughout the body. The results indicate a high accuracy for predicting sensations at specific body locations.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {887},
numpages = {22},
keywords = {Vibrotactile feedback, low frequency sounds, midair, non-contact haptics, psychophysics, room modes.},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3636534.3649345,
author = {Zhang, Tianfang and Phan, Huy and Tang, Zijie and Shi, Cong and Wang, Yan and Yuan, Bo and Chen, Yingying},
title = {Inaudible Backdoor Attack via Stealthy Frequency Trigger Injection in Audio Spectrogram},
year = {2024},
isbn = {9798400704895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636534.3649345},
doi = {10.1145/3636534.3649345},
abstract = {Deep learning-enabled Voice User Interfaces (VUIs) have surpassed human-level performance in acoustic perception tasks. However, the significant cost associated with training these models compels users to rely on third-party data or outsource training services. Such emerging trends have drawn substantial attention to training-phase attacks, particularly backdoor attacks. Such attacks implant hidden trigger patterns (e.g., tones, environmental sounds) into the model during training, thereby manipulating the model's predictions in the inference phase. However, existing backdoor attacks can be easily undermined in practice as the inserted triggers are audible. Users may notice such attacks when listening to the training data and remaining alert for suspicious sounds. In this work, we present a novel audio backdoor attack that exploits completely inaudible triggers in the frequency domain of the audio spectrograms. Specifically, we optimize the trigger to be a frequency-domain pattern with the energy below the noise floor (e.g., background and hardware noises) at any given frequency, thereby rendering the trigger inaudible. To realize such attacks, we design a strategy that automatically generates inaudible triggers in the spectrum supported by commodity playback devices (e.g., smartphones and laptops). We further develop optimization techniques to enhance the trigger's robustness against speech content and onset variations. Experiments on hotword and speaker recognition indicate that our attack can achieve attack success rates of more than 98.2% and 81.0% under digital and physical attack scenarios. The results also demonstrate the trigger's inaudibility with a Signal-to-Noise Ratio (SNR) less than -3.54 dB against background noises. We further verify that our attack can successfully bypass state-of-the-art backdoor defense strategies based on learning and audio processing.},
booktitle = {Proceedings of the 30th Annual International Conference on Mobile Computing and Networking},
pages = {31–45},
numpages = {15},
keywords = {inaudible attack, audio backdoor attack, frequency injection, audio spectrogram},
location = {Washington D.C., DC, USA},
series = {ACM MobiCom '24}
}

@inproceedings{10.1145/2047862.2047868,
author = {Ribeiro, M\'{a}rcio and Queiroz, Felipe and Borba, Paulo and Tol\^{e}do, T\'{a}rsis and Brabrand, Claus and Soares, S\'{e}rgio},
title = {On the impact of feature dependencies when maintaining preprocessor-based software product lines},
year = {2011},
isbn = {9781450306898},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047862.2047868},
doi = {10.1145/2047862.2047868},
abstract = {During Software Product Line (SPL) maintenance tasks, Virtual Separation of Concerns (VSoC) allows the programmer to focus on one feature and hide the others. However, since features depend on each other through variables and control-flow, feature modularization is compromised since the maintenance of one feature may break another. In this context, emergent interfaces can capture dependencies between the feature we are maintaining and the others, making developers aware of dependencies. To better understand the impact of code level feature dependencies during SPL maintenance, we have investigated the following two questions: how often methods with preprocessor directives contain feature dependencies? How feature dependencies impact maintenance effort when using VSoC and emergent interfaces? Answering the former is important for assessing how often we may face feature dependency problems. Answering the latter is important to better understand to what extent emergent interfaces complement VSoC during maintenance tasks. To answer them, we analyze 43 SPLs of different domains, size, and languages. The data we collect from them complement previous work on preprocessor usage. They reveal that the feature dependencies we consider in this paper are reasonably common in practice; and that emergent interfaces can reduce maintenance effort during the SPL maintenance tasks we regard here.},
booktitle = {Proceedings of the 10th ACM International Conference on Generative Programming and Component Engineering},
pages = {23–32},
numpages = {10},
keywords = {software product lines, preprocessors, modularity},
location = {Portland, Oregon, USA},
series = {GPCE '11}
}

@inproceedings{10.1109/ICSE.2009.5070526,
author = {Th\"{u}m, Thomas and Batory, Don and K\"{a}stner, Christian},
title = {Reasoning about edits to feature models},
year = {2009},
isbn = {9781424434534},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSE.2009.5070526},
doi = {10.1109/ICSE.2009.5070526},
abstract = {Features express the variabilities and commonalities among programs in a software product line (SPL). A feature model defines the valid combinations of features, where each combination corresponds to a program in an SPL. SPLs and their feature models evolve over time. We classify the evolution of a feature model via modifications as refactorings, specializations, generalizations, or arbitrary edits. We present an algorithm to reason about feature model edits to help designers determine how the program membership of an SPL has changed. Our algorithm takes two feature models as input (before and after edit versions), where the set of features in both models are not necessarily the same, and it automatically computes the change classification. Our algorithm is able to give examples of added or deleted products and efficiently classifies edits to even large models that have thousands of features.},
booktitle = {Proceedings of the 31st International Conference on Software Engineering},
pages = {254–264},
numpages = {11},
series = {ICSE '09}
}

@inproceedings{10.1145/3677996.3678289,
author = {Suckrow, Pierre-Louis Wolfgang L\'{e}on and Weber, Christoph Johannes and Rothe, Sylvia},
title = {Diffusion-Based Sound Synthesis in Music Production},
year = {2024},
isbn = {9798400710995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677996.3678289},
doi = {10.1145/3677996.3678289},
abstract = {In this paper, we explore the usability of generative artificial intelligence in music production through the development of a digital instrument that incorporates diffusion-based sound synthesis in its sound generation. Current text-to-audio models offer a novel method of defining sounds, which we aim to render utilizable in a music-production environment. Selected pretrained latent diffusion models, enable the synthesis of playable sounds through textual descriptions, which we incorporated into a digital instrument that integrates with standard music production tools. The resultant user interface not only allows generating but also modifying the sounds by editing model and instrument-specific parameters. We evaluated the applicability of current diffusion models with their parameters as well as the fitness of possible prompts for music production scenarios. Adapting published diffusion model pipelines for integration into the instrument, we facilitate experimentation and exploration of this innovative sound synthesis method. Our findings show that despite facing some limitations in the models' responsiveness to specific music production contexts and the instrument's functionality, the tool allows the development of novel and intriguing soundscapes. The instrument and code is published under https://github.com/suckrowPierre/WaveGenSynth},
booktitle = {Proceedings of the 12th ACM SIGPLAN International Workshop on Functional Art, Music, Modelling, and Design},
pages = {55–64},
numpages = {10},
keywords = {sound generation, text-to-sound, user interface, user study},
location = {Milan, Italy},
series = {FARM 2024}
}

@inproceedings{10.1145/1842752.1842769,
author = {Weyns, Danny and Capilla, Rafael},
title = {Current and emerging topics in software architecture (ECSA 2010 Workshops Summary)},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842769},
doi = {10.1145/1842752.1842769},
abstract = {Since 2004 in St. Andrews (Scotland, U.K.), ECSA the European Conference on Software Architecture (formerly EWSA, the European Workshop on Software Architecture) has been considered as an important meeting point for researchers and practitioners on the topic of software architecture. ECSA has matured from a workshop format to a full software engineering conference in the subfield of software architecture.This year, ECSA has become more ambitious and expanded its scope and schedule up to four full days. The program includes a series of tutorials, a doctoral mentoring program, and four full-day workshops. New and existing software challenges have led to a variety of trends in software architecture research, which makes the conference and workshops more attractive and promotes the discussion on current and emerging topics.Based on the scientific and technical interest of the topics, the innovativeness of workshop topics, and the capacity of the conference workshop program, the workshop co-chairs selected four workshops from the nine submitted proposals. We summarize the aims and goals of each workshop and the contributions accepted for the four workshops:• 2nd International Workshop on Software Ecosystems (EcoSys). Piers Campbell, Faheem Ahmed, Jan Bosch, Sliger Jansen.• 1st International Workshop on Measurability of Security in Software Architectures (MeSSa). Reijo Savola, Teemu Kranst\'{e}n, Antti Evesti.• 8th Nordic Workshop on Model Driven Software Engineering (NW-MODE). Andrzej Wasowski, Dragos Truscan, Ludwik Kuzniarz.• 1st International Workshop on Variability in Software Product Line Architectures (VARI-ARCH). Alexander Helleboogh, Paris Avgeriou, Nelis Boucke, Patryck Heymans.The ECSA 2010 Workshop co-chairs would like to thanks all workshop organizers for their effort and enthusiasm to attract submission in different software architecture research topics and make the ECSA 2010 workshops a success.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {59–62},
numpages = {4},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@inproceedings{10.1145/2991079.2991126,
author = {Etigowni, Sriharsha and Tian, Dave (Jing) and Hernandez, Grant and Zonouz, Saman and Butler, Kevin},
title = {CPAC: securing critical infrastructure with cyber-physical access control},
year = {2016},
isbn = {9781450347716},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2991079.2991126},
doi = {10.1145/2991079.2991126},
abstract = {Critical infrastructure such as the power grid has become increasingly complex. The addition of computing elements to traditional physical components increases complexity and hampers insight into how elements in the system interact with each other. The result is an infrastructure where operational mistakes, some of which cannot be distinguished from attacks, are more difficult to prevent and have greater potential impact, such as leaking sensitive information to the operator or attacker. In this paper, we present CPAC, a cyber-physical access control solution to manage complexity and mitigate threats in cyber-physical environments, with a focus on the electrical smart grid. CPAC uses information flow analysis based on mathematical models of the physical grid to generate policies enforced through verifiable logic. At the device side, CPAC combines symbolic execution with lightweight dynamic execution monitoring to allow non-intrusive taint analysis on programmable logic controllers in realtime. These components work together to provide a realtime view of all system elements, and allow for more robust and finer-grained protections than any previous solution to securing the grid. We implement a prototype of CPAC using Bachmann PLCs and evaluate several real-world incidents that demonstrate its scalability and effectiveness. The policy checking for a nation-wide grid is less than 150 ms, faster than existing solutions. We additionally show that CPAC can analyze potential component failures for arbitrary component failures, far beyond the capabilities of currently deployed systems. CPAC thus provides a solution to secure the modern smart grid from operator mistakes or insider attacks, maintain operational privacy, and support N - x contingencies.},
booktitle = {Proceedings of the 32nd Annual Conference on Computer Security Applications},
pages = {139–152},
numpages = {14},
location = {Los Angeles, California, USA},
series = {ACSAC '16}
}

@inproceedings{10.1145/3243734.3243739,
author = {Ispoglou, Kyriakos K. and AlBassam, Bader and Jaeger, Trent and Payer, Mathias},
title = {Block Oriented Programming: Automating Data-Only Attacks},
year = {2018},
isbn = {9781450356930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243734.3243739},
doi = {10.1145/3243734.3243739},
abstract = {With the widespread deployment of Control-Flow Integrity (CFI), control-flow hijacking attacks, and consequently code reuse attacks, are significantly more difficult. CFI limits control flow to well-known locations, severely restricting arbitrary code execution. Assessing the remaining attack surface of an application under advanced control-flow hijack defenses such as CFI and shadow stacks remains an open problem. We introduce BOPC, a mechanism to automatically assess whether an attacker can execute arbitrary code on a binary hardened with CFI/shadow stack defenses. BOPC computes exploits for a target program from payload specifications written in a Turing-complete, high-level language called SPL that abstracts away architecture and program-specific details. SPL payloads are compiled into a program trace that executes the desired behavior on top of the target binary. The input for BOPC is an SPL payload, a starting point (e.g., from a fuzzer crash) and an arbitrary memory write primitive that allows application state corruption. To map SPL payloads to a program trace, BOPC introduces Block Oriented Programming (BOP), a new code reuse technique that utilizes entire basic blocks as gadgets along valid execution paths in the program, i.e., without violating CFI or shadow stack policies. We find that the problem of mapping payloads to program traces is NP-hard, so BOPC first reduces the search space by pruning infeasible paths and then uses heuristics to guide the search to probable paths. BOPC encodes the BOP payload as a set of memory writes. We execute 13 SPL payloads applied to 10 popular applications. BOPC successfully finds payloads and complex execution traces -- which would likely not have been found through manual analysis -- while following the target's Control-Flow Graph under an ideal CFI policy in 81% of the cases.},
booktitle = {Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1868–1882},
numpages = {15},
keywords = {program synthesis, exploitation, data only attacks, block oriented programming, binary analysis},
location = {Toronto, Canada},
series = {CCS '18}
}

@inproceedings{10.1145/2110147.2110160,
author = {Schroeter, Julia and Cech, Sebastian and G\"{o}tz, Sebastian and Wilke, Claas and A\ss{}mann, Uwe},
title = {Towards modeling a variable architecture for multi-tenant SaaS-applications},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110160},
doi = {10.1145/2110147.2110160},
abstract = {A widespread business model in cloud computing is to offer software as a service (SaaS) over the Internet. Such applications are often multi-tenant aware, which means that multiple tenants share hardware and software resources of the same application instance. However, SaaS stakeholders have different or even contradictious requirements and interests: For a user, the application's quality and non-functional properties have to be maximized (e.g., choosing the fastest available algorithm for a computation at runtime). In contrast, a resource or application provider is interested in minimizing the operating costs while maximizing his profit. Finally, tenants are interested in offering a customized functionality to their users. To identify an optimal compromise for all these objectives, multiple levels of variability have to be supported by reference architectures for multi-tenant SaaS applications. In this paper, we identify requirements for such a runtime architecture addressing the individual interests of all involved stakeholders. Furthermore, we show how our existing architecture for dynamically adaptive applications can be extended for the development and operation of multi-tenant applications.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {111–120},
numpages = {10},
keywords = {variability modeling, software-as-a-service, self-optimization, multi-tenancy, auto-tuning},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@article{10.1145/3701732,
author = {Chen, Zan and Wang, Tao and Li, Jun and Guo, Wenlong and Feng, Yuanjing and Qian, Xueming and Hou, Xingsong},
title = {Discard Significant Bits of Compressed Sensing: A Robust Image Coding for Resource-Limited Contexts},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3701732},
doi = {10.1145/3701732},
abstract = {Compressed sensing (CS) provides a robust and simple framework for compressing images in resource-constrained environments. However, CS-based image coding schemes often have poor rate-distortion (R-D) performance, particularly due to the quantization process. Our research indicates that leveraging the image prior enables the estimation of most significant bits (MSBs) from least significant bits (LSBs), which provides a quantization strategy to improve R-D performance without increasing coding complexity. That is discarding MSBs of measurements, and only transmitting LSBs to the decoder side. At the decoder side, we reconstruct images by solving an inverse-quantization set-constrained CS optimization problem. Our approach further employs a tailored designed deep denoiser as the proximal operator to enhance the reconstructed image quality. Extensive experimental results demonstrate that the proposed scheme achieves satisfactory performance, with promising R-D results (PSNR gains over 1.71 dB than JPEG at 0.50 bpp compression ratio), and robust bit error and loss resilience (reconstructed 29.98 dB even with 50% bit loss at 0.50 bpp compression ratio), meanwhile having lower encoding complexity (less than half encoding time of CCSDS-IDC).},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = dec,
articleno = {31},
numpages = {25},
keywords = {Compressive sensing, Quantization, Image prior, Robust image coding}
}

@inproceedings{10.5555/2486788.2486853,
author = {Sayyad, Abdel Salam and Menzies, Tim and Ammar, Hany},
title = {On the value of user preferences in search-based software engineering: a case study in software product lines},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Software design is a process of trading off competing objectives. If the user objective space is rich, then we should use optimizers that can fully exploit that richness. For example, this study configures software product lines (expressed as feature maps) using various search-based software engineering methods. As we increase the number of optimization objectives, we find that methods in widespread use (e.g. NSGA-II, SPEA2) perform much worse than IBEA (Indicator-Based Evolutionary Algorithm). IBEA works best since it makes most use of user preference knowledge. Hence it does better on the standard measures (hypervolume and spread) but it also generates far more products with 0% violations of domain constraints. Our conclusion is that we need to change our methods for search-based software engineering, particularly when studying complex decision spaces.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {492–501},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/3624007.3624058,
author = {Medeiros, Raul and D\'{\i}az, Oscar and Benavides, David},
title = {Unleashing the Power of Implicit Feedback in Software Product Lines: Benefits Ahead},
year = {2023},
isbn = {9798400704062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624007.3624058},
doi = {10.1145/3624007.3624058},
abstract = {Software Product Lines (SPLs) facilitate the development of a complete range of software products through systematic reuse. Reuse involves not only code but also the transfer of knowledge gained from one product to others within the SPL. This transfer includes bug fixing, which, when encountered in one product, affects the entire SPL portfolio. Similarly, feedback obtained from the usage of a single product can inform beyond that product to impact the entire SPL portfolio. Specifically, implicit feedback refers to the automated collection of data on software usage or execution, which allows for the inference of customer preferences and trends. While implicit feedback is commonly used in single-product development, its application in SPLs has not received the same level of attention. This paper promotes the investigation of implicit feedback in SPLs by identifying a set of SPL activities that can benefit the most from it. We validate this usefulness with practitioners using a questionnaire-based approach (n=8). The results provide positive insights into the advantages and practical implications of adopting implicit feedback at the SPL level.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {113–121},
numpages = {9},
keywords = {User behavior, Software Product Lines, Implicit feedback, Code generation},
location = {Cascais, Portugal},
series = {GPCE 2023}
}

@inproceedings{10.1145/3530019.3533679,
author = {Mannon, Mike and Kaindl, Hermann},
title = {Enhancing Product Comparison through Automated Similarity Matching},
year = {2022},
isbn = {9781450396134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3530019.3533679},
doi = {10.1145/3530019.3533679},
abstract = {The volume, variety and velocity of products in software-intensive systems product lines is increasing. One challenge is to understand the range of similarity between products. Reasons for product comparison include (i) to decide whether to build a new product or not (ii) to evaluate how products of the same type differ for strategic positioning or branding reasons (iii) to gauge if a product line needs to be reorganized (iv) to assess if a product falls within the national legislative and regulatory boundaries. We will discuss two different approaches to address this challenge. One is grounded in feature modelling, the other in case-based reasoning. We will also describe a specific product comparison process in which a product configured from a product line feature model is represented as a weighted binary string, the overall similarity between products is compared using a binary string metric, and the significance of individual feature combinations for product similarity can be explored by modifying the weights. We will illustrate our ideas with a mobile phone example, and discuss some of the benefits and limitations of this approach.},
booktitle = {Proceedings of the 26th International Conference on Evaluation and Assessment in Software Engineering},
pages = {463–464},
numpages = {2},
keywords = {product similarity, binary strings, Feature reuse},
location = {Gothenburg, Sweden},
series = {EASE '22}
}

@inproceedings{10.1145/3607199.3607203,
author = {Bolton, Connor and Long, Yan and Han, Jun and Hester, Josiah and Fu, Kevin},
title = {Characterizing and Mitigating Touchtone Eavesdropping in Smartphone Motion Sensors},
year = {2023},
isbn = {9798400707650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607199.3607203},
doi = {10.1145/3607199.3607203},
abstract = {Smartphone motion sensors provide cybersecurity attackers with a stealthy way to eavesdrop on nearby acoustic information. Eavesdropping on touchtones emitted by smartphone speakers when users input numbers into their phones exposes sensitive information such as credit card information, banking PINs, and social security card numbers to malicious applications with access to only motion sensor data. This work characterizes this new security threat of touchtone eavesdropping by providing an analysis based on physics and signal processing theory. We show that advanced adversaries who selectively integrate data from multiple motion sensors and multiple sensor axes can achieve over 99% accuracy on recognizing 12 unique touchtones. We further design, analyze, and evaluate several mitigations which could be implemented in a smartphone update. We found that some apparent mitigations such as low-pass filters can undesirably reduce the motion sensor data to benign applications by 83% but only reduce an advanced adversary’s accuracy by less than one percent. Other more informed designs such as anti-aliasing filters can fully preserve the motion sensor data to support benign application functionality while reducing attack accuracy by 50.1%.},
booktitle = {Proceedings of the 26th International Symposium on Research in Attacks, Intrusions and Defenses},
pages = {164–178},
numpages = {15},
keywords = {DTMF, eavesdropping, motion sensor, smartphone, touchtone},
location = {Hong Kong, China},
series = {RAID '23}
}

@inproceedings{10.1145/3384419.3430781,
author = {Sami, Sriram and Dai, Yimin and Tan, Sean Rui Xiang and Roy, Nirupam and Han, Jun},
title = {Spying with your robot vacuum cleaner: eavesdropping via lidar sensors},
year = {2020},
isbn = {9781450375900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384419.3430781},
doi = {10.1145/3384419.3430781},
abstract = {Eavesdropping on private conversations is one of the most common yet detrimental threats to privacy. A number of recent works have explored side-channels on smart devices for recording sounds without permission. This paper presents LidarPhone, a novel acoustic side-channel attack through the lidar sensors equipped in popular commodity robot vacuum cleaners. The core idea is to repurpose the lidar to a laser-based microphone that can sense sounds from subtle vibrations induced on nearby objects. LidarPhone carefully processes and extracts traces of sound signals from inherently noisy laser reflections to capture privacy sensitive information (such as speech emitted by a victim's computer speaker as the victim is engaged in a teleconferencing meeting; or known music clips from television shows emitted by a victim's TV set, potentially leaking the victim's political orientation or viewing preferences). We implement LidarPhone on a Xiaomi Roborock vacuum cleaning robot and evaluate the feasibility of the attack through comprehensive real-world experiments. We use the prototype to collect both spoken digits and music played by a computer speaker and a TV soundbar, of more than 30k utterances totaling over 19 hours of recorded audio. LidarPhone achieves approximately 91% and 90% average accuracies of digit and music classifications, respectively.},
booktitle = {Proceedings of the 18th Conference on Embedded Networked Sensor Systems},
pages = {354–367},
numpages = {14},
keywords = {lidar, eavesdropping, acoustic side-channel},
location = {Virtual Event, Japan},
series = {SenSys '20}
}

@inproceedings{10.1145/3610579.3611091,
author = {Spieck, Jan and Sixdenier, Pierre-Louis and Esper, Khalil and Wildermann, Stefan and Teich, J\"{u}rgen},
title = {Hybrid Genetic Reinforcement Learning for Generating Run-Time Requirement Enforcers},
year = {2023},
isbn = {9798400703188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610579.3611091},
doi = {10.1145/3610579.3611091},
abstract = {When designing embedded systems, engineers have to consider non-functional requirements, such as real-time or energy consumption constraints. To enforce or counteract any potential violation of such constraints, feedback-based control techniques can be applied, e.g., adapting the degree of parallelism or changing the DVFS settings of the resources. Of particular interest here are formal techniques for proving that the developed controllers either never lead to a violation of a given set of non-functional requirements or minimize the probability of such violations occurring. In the context of run-time requirement enforcement, it has been shown that either property can be described as one or a set of verification goals of a given or generated enforcement strategy. In this paper, we propose a design space exploration (DSE) methodology to determine a Pareto-optimal set of verifiable FSM-based feedback-based enforcers for a given set of verification goals. A major problem encountered here is that formally checking a set of verification goals can be quite time-intensive and, as a consequence, may lead to intolerably high exploration times. As a remedy, this paper proposes a hybrid DSE methodology based on a combination of multi-objective evolutionary algorithm search and reinforcement learning (RL).In particular, RL is used in each iteration of the evolutionary algorithm as a local search strategy to efficiently identify and fill gaps of diversity in the front of non-dominated solutions. It is shown that this leads to drastic reductions in exploration time. In three case studies, we compare the proposed approach with state-of-the-art methods and demonstrate considerably smaller optimization times alongside its capability to generate controllers exhibiting higher probabilities of satisfying a given set of requirement verification goals, as verified by model checkers.},
booktitle = {Proceedings of the 21st ACM-IEEE International Conference on Formal Methods and Models for System Design},
pages = {23–35},
numpages = {13},
keywords = {MPSoC, design space exploration, reinforcement learning, verification, runtime requirement enforcement, control},
location = {Hamburg, Germany},
series = {MEMOCODE '23}
}

@inproceedings{10.1145/2377836.2377842,
author = {Gamez, Nadia and Romero, Daniel and Fuentes, Lidia and Rouvoy, Romain and Duchien, Laurence},
title = {Constraint-based self-adaptation of wireless sensor networks},
year = {2012},
isbn = {9781450315661},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2377836.2377842},
doi = {10.1145/2377836.2377842},
abstract = {In recent years, the Wireless Sensor Networks (WSNs) have become a useful mechanism to monitor physical phenomena in environments. The sensors that make part of these long-lived networks have to be reconfigured according to context changes in order to preserve the operation of the network. Such reconfigurations require to consider the distributed nature of the sensor nodes as well as their resource scarceness. Therefore, self-adaptations for WSNs have special requirements comparing with traditional information systems. In particular, the reconfiguration of the WSN requires a trade-off between critical dimensions for this kind of networks and devices, such as resource consumption or reconfiguration cost. Thus, in this paper, we propose to exploit Constraint-Satisfaction Problem (CSP) techniques in order to find a suitable configuration for self-adapting WSNs, modelled using a Dynamic Software Product Line (DSPL), when the context changes. We exploit CSP modeling to find a compromise between contradictory dimensions. To illustrate our approach, we use an Intelligent Transportation System scenario. This case study enables us to show the advantages of obtaining suitable and optimized configurations for self-adapting WSNs.},
booktitle = {Proceedings of the 2nd International Workshop on Adaptive Services for the Future Internet and 6th International Workshop on Web APIs and Service Mashups},
pages = {20–27},
numpages = {8},
keywords = {wireless sensor networks, self-adaptation, dynamic software product lines, constraint-satisfaction problem},
location = {Bertinoro, Italy},
series = {WAS4FI-Mashups '12}
}

@inproceedings{10.1145/3597061.3597257,
author = {Jaiswal, Dibyanshu and Chatterjee, Debatri and Sarkar, Arindam and S, Meghana and Ramakrishnan, Ramesh Kumar and Pal, Arpan and Ghosh, Ratna},
title = {Assessment of Mental Workloads using Differential Dermal Potentials Recorded from in and around Ear},
year = {2023},
isbn = {9798400702112},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597061.3597257},
doi = {10.1145/3597061.3597257},
abstract = {Differential dermal potentials is a non-invasive bio-potential signal acquired from the skin surface is a relatively new mode for human activity and behavior monitoring. Literature suggests acquiring differential dermal potentials from either hand or leg, which might be uncomfortable for long term usage. In the present work, for the first time we have acquired the differential dermal potentials from in and around the ear and have used that for monitoring mental workload. Experimental data were acquired from ear lobes and ear canals using two different devices while the participants were engaged in a mental arithmetic task. Thereafter, the signals are analyzed and a set of the most discriminating features are identified. Using these features, a cognitive load prediction model is developed. Results show that, for rest vs. load classification, we achieved an accuracy of 89% for ear canal data and 94.2% for ear lobe data. We also observed a positive co-occurrence of signals collected from these two locations. Thus, our proposed approach can be used for monitoring mental workload condition in real-life applications and is suitable for long term usage.},
booktitle = {Proceedings of the 8th Workshop on Body-Centric Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {deep learning, machine learning, mental workload, cognitive load, endosomatic, bio-potential, differential dermal potential},
location = {Helsinki, Finland},
series = {BodySys '23}
}

@inproceedings{10.1145/3084226.3084253,
author = {Abrah\~{a}o, Silvia and Insfran, Emilio},
title = {Evaluating Software Architecture Evaluation Methods: An Internal Replication},
year = {2017},
isbn = {9781450348041},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3084226.3084253},
doi = {10.1145/3084226.3084253},
abstract = {Context: The size and complexity of software systems along with the demand for ensuring quality requirements have fostered the interest in software architecture evaluation methods. Although several empirical studies have been reported, the actual body of knowledge is still insufficient. To address this concern, we presented a family of four controlled experiments that compares a recently proposed method, the Quality-Driven Architecture Derivation and Improvement (QuaDAI) method against the well-known Architecture Tradeoff Analysis Method (ATAM).Objective: To provide further evidence on the efficiency, effectiveness, and perceived satisfaction of participants using these two software architecture evaluation methods. We report the results of a differentiated internal replication study.Method: The same materials used in the baseline experiments were employed in this replication but the participants were sixteen practitioners. In addition, we used a simpler design to reduce the treatments' application sequences.Results: The participants obtained architectures with better quality when applying QuaDAI, and they found this method to be more useful and likely to be used than ATAM, but no difference in terms of efficiency and perceived ease of use were found.Conclusions: The results are in line with the baseline experiments and support the hypothesis that QuaDAI achieve better results than ATAM when performing architectural evaluations; however, further work is need to improve the methods usability.},
booktitle = {Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering},
pages = {144–153},
numpages = {10},
keywords = {Software Architecture Evaluation, Experiment Replication},
location = {Karlskrona, Sweden},
series = {EASE '17}
}

@article{10.1109/TASLP.2015.2401513,
author = {Mamun, Nursadul and Jassim, Wissam A. and Zilany, Muhammad S. A.},
title = {Prediction of speech intelligibility using a neurogram orthogonal polynomial measure (NOPM)},
year = {2015},
issue_date = {April 2015},
publisher = {IEEE Press},
volume = {23},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2401513},
doi = {10.1109/TASLP.2015.2401513},
abstract = {Sensorineural hearing loss (SNHL) is an increasingly prevalent condition, resulting from damage to the inner ear and causing a reduction in speech intelligibility. This paper proposes a new speech intelligibility prediction metric, the neurogram orthogonal polynomial measure (NOPM). This metric applies orthogonal moments to the auditory neurogram to predict speech intelligibility for listeners with and without hearing loss. The model simulates the responses of auditory-nerve fibers to speech signals under quiet and noisy conditions. Neurograms were created using a physiologically based computational model of the auditory periphery. A well-known orthogonal polynomial measure, Krawtchouk moments, was applied to extract features from the auditory neurogram. The predicted intelligibility scores were compared to subjective results, and NOPM showed a good fit with the subjective scores for normal listeners and also for listeners with hearing loss. The proposed metric has a realistic and wider dynamic range than corresponding existing metrics, such as mean structural similarity index measure and neurogram similarity index measure, and the predicted scores are also well-separated as a function of hearing loss. The application of this metric could be extended for assessing hearing-aid and speech-enhancement algorithms.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {760–773},
numpages = {14},
keywords = {speech intelligibility, sensorineural hearing loss, orthogonal moment, neurogram, auditory-nerve model}
}

@inproceedings{10.1145/3055031.3055088,
author = {Han, Jun and Chung, Albert Jin and Tague, Patrick},
title = {Pitchln: eavesdropping via intelligible speech reconstruction using non-acoustic sensor fusion},
year = {2017},
isbn = {9781450348904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3055031.3055088},
doi = {10.1145/3055031.3055088},
abstract = {Despite the advent of numerous Internet-of-Things (IoT) applications, recent research demonstrates potential side-channel vulnerabilities exploiting sensors which are used for event and environment monitoring. In this paper, we propose a new side-channel attack, where a network of distributed non-acoustic sensors can be exploited by an attacker to launch an eavesdropping attack by reconstructing intelligible speech signals. Specifically, we present PitchIn to demonstrate the feasibility of speech reconstruction from non-acoustic sensor data collected offline across networked devices. Unlike speech reconstruction which requires a high sampling frequency (e.g., &gt; 5 KHz), typical applications using non-acoustic sensors do not rely on richly sampled data, presenting a challenge to the speech reconstruction attack. Hence, PitchIn leverages a distributed form of Time Interleaved Analog-Digital-Conversion (TIADC) to approximate a high sampling frequency, while maintaining low per-node sampling frequency. We demonstrate how distributed TI-ADC can be used to achieve intelligibility by processing an interleaved signal composed of different sensors across networked devices. We implement PitchIn and evaluate reconstructed speech signal intelligibility via user studies. PitchIn has word recognition accuracy as high as 79%. Though some additional work is required to improve accuracy, our results suggest that eavesdropping using a fusion of non-acoustic sensors is a real and practical threat.},
booktitle = {Proceedings of the 16th ACM/IEEE International Conference on Information Processing in Sensor Networks},
pages = {181–192},
numpages = {12},
keywords = {speech reconstruction, sensor fusion, security, privacy, non-acoustic sensors},
location = {Pittsburgh, Pennsylvania},
series = {IPSN '17}
}

@inproceedings{10.1145/3581783.3611988,
author = {Wang, Ao and Chen, Hui and Lin, Zijia and Ding, Zixuan and Liu, Pengzhang and Bao, Yongjun and Yan, Weipeng and Ding, Guiguang},
title = {Hierarchical Prompt Learning Using CLIP for Multi-label Classification with Single Positive Labels},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3611988},
doi = {10.1145/3581783.3611988},
abstract = {Collecting full annotations to construct multi-label datasets is difficult and labor-consuming. As an effective solution to relieve the annotation burden, single positive multi-label learning (SPML) draws increasing attention from both academia and industry. It only annotates each image with one positive label, leaving other labels unobserved. Therefore, existing methods strive to explore the cue of unobserved labels to compensate for the insufficiency of label supervision. Though achieving promising performance, they generally consider labels independently, leaving out the inherent hierarchical semantic relationship among labels which reveals that labels can be clustered into groups. In this paper, we propose a hierarchical prompt learning method with a novel Hierarchical Semantic Prompt Network (HSPNet) to harness such hierarchical semantic relationships using a large-scale pretrained vision and language model, i.e., CLIP, for SPML. We first introduce a Hierarchical Conditional Prompt (HCP) strategy to grasp the hierarchical label-group dependency. Then we equip a Hierarchical Graph Convolutional Network (HGCN) to capture the high-order inter-label and inter-group dependencies. Comprehensive experiments and analyses on several benchmark datasets show that our method significantly outperforms the state-of-the-art methods, well demonstrating its superiority and effectiveness. Our code will be available at https://github.com/jameslahm/HSPNet.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {5594–5604},
numpages = {11},
keywords = {hierarchical relationship, image recognition, multi-label classification, vision and language, weak supervision},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3510466.3510484,
author = {Ratzenb\"{o}ck, Michael and Gr\"{u}nbacher, Paul and Assun\c{c}ao, Wesley K. G. and Egyed, Alexander and Linsbauer, Lukas},
title = {Refactoring Product Lines by Replaying Version Histories},
year = {2022},
isbn = {9781450396042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510466.3510484},
doi = {10.1145/3510466.3510484},
abstract = {When evolving software product lines, new features are added over time and existing features are revised. Engineers also decide to merge different features or split features in other cases. Such refactoring tasks are difficult when using manually maintained feature-to-code mappings. Intensional version control systems such as ECCO overcome this issue with automatically computed feature-to-code mappings. Furthermore, they allow creating variants that have not been explicitly committed before. However, such systems are still rarely used compared to extensional version control systems like Git, which keep track of the evolution history by assigning revisions to states of a system. This paper presents an approach combining both extensional and intensional version control systems, which relies on the extensional version control system Git to store versions. Developers selectively tag existing versions to describe the evolution at the level of features. Our approach then automatically replays the evolution history to create a repository of the intensional variation control system ECCO. The approach contributes to research on refactoring features of existing product lines and migrating existing systems to product lines. We provide an initial evaluation of the approach regarding correctness and performance based on an existing system.},
booktitle = {Proceedings of the 16th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {8},
numpages = {10},
keywords = {version control systems, refactoring, feature-level evolution},
location = {Florence, Italy},
series = {VaMoS '22}
}

@inproceedings{10.1145/3540250.3549112,
author = {Ramkisoen, Poedjadevie Kadjel and Businge, John and van Bladel, Brent and Decan, Alexandre and Demeyer, Serge and De Roover, Coen and Khomh, Foutse},
title = {PaReco: patched clones and missed patches among the divergent variants of a software family},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549112},
doi = {10.1145/3540250.3549112},
abstract = {Re-using whole repositories as a starting point for new projects is often done by maintaining a variant fork parallel to the original. However, the common artifacts between both are not always kept up to date. As a result, patches are not optimally integrated across the two repositories, which may lead to sub-optimal maintenance between the variant and the original project. A bug existing in both repositories can be patched in one but not the other (we see this as a missed opportunity) or it can be manually patched in both probably by different developers (we see this as effort duplication). In this paper we present a tool (named PaReCo) which relies on clone detection to mine cases of missed opportunity and effort duplication from a pool of patches. We analyzed 364 (source to target) variant pairs with 8,323 patches resulting in a curated dataset containing 1,116 cases of effort duplication and 1,008 cases of missed opportunities. We achieve a precision of 91%, recall of 80%, accuracy of 88%, and F1-score of 85%. Furthermore, we investigated the time interval between patches and found out that, on average, missed patches in the target variants have been introduced in the source variants 52 weeks earlier. Consequently, PaReCo can be used to manage variability in “time” by automatically identifying interesting patches in later project releases to be backported to supported earlier releases.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {646–658},
numpages = {13},
keywords = {Variants, Software family, Social coding, Github, Forking, Effort duplication, Clone&amp;own, Clone detection, Bug-fixes},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/2479871.2479922,
author = {Bulej, Lubom\'{\i}r and Burea, Tom\'{a}\v{s} and Hork\'{y}, Vojt\v{c}ch and Keznikl, Jaroslav},
title = {Adaptive deployment in ad-hoc systems using emergent component ensembles: vision paper},
year = {2013},
isbn = {9781450316361},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2479871.2479922},
doi = {10.1145/2479871.2479922},
abstract = {Mobile cloud computing in the context of ad-hoc clouds brings new challenges when offloading computation from mobile devices. The management of application deployment needs to ensure that the offloading provides users with the expected benefits, but it suddenly needs to cope with a highly dynamic environment which lacks a central authority and in which computational nodes appear and disappear.We propose an approach to the management of ad-hoc systems in such dynamic environment using component ensembles that connect mobile devices with more powerful computation nodes. Our approach aims to address the challenges of scalability and robustness of such systems without the need for central authority, relying instead on simple patterns that lead to reasonable adaptation decisions based on limited and imprecise information.},
booktitle = {Proceedings of the 4th ACM/SPEC International Conference on Performance Engineering},
pages = {343–346},
numpages = {4},
keywords = {ensembles, adaptive deployment, ad-hoc cloud},
location = {Prague, Czech Republic},
series = {ICPE '13}
}

@article{10.1145/54132.54135,
author = {Calabaugh, Jerry},
title = {Software configuration—an NP-complete problem},
year = {1988},
issue_date = {Summer 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0095-0033},
url = {https://doi.org/10.1145/54132.54135},
doi = {10.1145/54132.54135},
abstract = {Configuration Control File (CCF) production is very complex, thousands of code packages, data blocks and parameter values must be linked under many constraints including:*Common data and code less than 8192 bytes*Maximum of 5 registers per task*All systems data must have common capabilitiesNP-complete problems are commonly known as knapsack or bin packing problems. They have no known algorithm which solves them in a time period bounded by a polynominal function of the number of inputs. Rules-of-thumb, or heuristics are the only practical approach to their solution. CCF segmentation to meet constraints discussed above is an example of Expert System technology applied to a classic NP-complete problem.Heuristics developed with traditional data processing techniques initially performed satisfactorily. However, as program development proceeded, Central Processor Unit (CPU) time for (CCF) production became a concern, both from a commitment of CPU resources and lost productivity. Traditional techniques failed to improve the heuristics and the project began to slip. Projected time to produce the CCF for a fully developed program was totally unacceptable, and jeopardized the project.Clearly another approach was required. Because existing hueristics were based on a concept of rules, research indicated an expert system using rules and a knowledge based approach had the highest probability of success.The paper emphasizes the development process of a knowledge based system from the perspective of the responsible project manager. The methodology is also applicable to common business problems.},
journal = {SIGMIS Database},
month = aug,
pages = {29–34},
numpages = {6}
}

@article{10.1145/3204459,
author = {Chen, Tao and Li, Ke and Bahsoon, Rami and Yao, Xin},
title = {FEMOSAA: Feature-Guided and Knee-Driven Multi-Objective Optimization for Self-Adaptive Software},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3204459},
doi = {10.1145/3204459},
abstract = {Self-Adaptive Software (SAS) can reconfigure itself to adapt to the changing environment at runtime, aiming to continually optimize conflicted nonfunctional objectives (e.g., response time, energy consumption, throughput, cost, etc.). In this article, we present Feature-guided and knEe-driven Multi-Objective optimization for Self-Adaptive softwAre (FEMOSAA), a novel framework that automatically synergizes the feature model and Multi-Objective Evolutionary Algorithm (MOEA) to optimize SAS at runtime. FEMOSAA operates in two phases: at design time, FEMOSAA automatically transposes the engineers’ design of SAS, expressed as a feature model, to fit the MOEA, creating new chromosome representation and reproduction operators. At runtime, FEMOSAA utilizes the feature model as domain knowledge to guide the search and further extend the MOEA, providing a larger chance for finding better solutions. In addition, we have designed a new method to search for the knee solutions, which can achieve a balanced tradeoff. We comprehensively evaluated FEMOSAA on two running SAS: One is a highly complex SAS with various adaptable real-world software under the realistic workload trace; another is a service-oriented SAS that can be dynamically composed from services. In particular, we compared the effectiveness and overhead of FEMOSAA against four of its variants and three other search-based frameworks for SAS under various scenarios, including three commonly applied MOEAs, two workload patterns, and diverse conflicting quality objectives. The results reveal the effectiveness of FEMOSAA and its superiority over the others with high statistical significance and nontrivial effect sizes.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {5},
numpages = {50},
keywords = {self-adaptive system, search-based software engineering, performance engineering, multi-objective optimization, multi-objective evolutionary algorithm, Feature model}
}

@inproceedings{10.1145/3634713.3634715,
author = {B\"{o}hm, Sabrina and Krieter, Sebastian and He\ss{}, Tobias and Th\"{u}m, Thomas and Lochau, Malte},
title = {Incremental Identification of T-Wise Feature Interactions},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634715},
doi = {10.1145/3634713.3634715},
abstract = {Developers of configurable software use the concept of selecting and deselecting features to create different variants of a software product. In this context, one of the most challenging aspects is to identify unwanted interactions between those features. Due to the combinatorial explosion of the number of potentially interacting features, it is currently an open question how to systematically identify a particular feature interaction that causes a specific fault in a set of software products. In this paper, we propose an incremental approach to identify such t-wise feature interactions based on testing additional configurations in a black-box setting. We present the algorithm Inciident, which generates and selects new configurations based on a divide-and-conquer strategy to efficiently identify the feature interaction with a preferably minimal number of configurations. We evaluate our approach by considering simulated and real interactions of different sizes for 48 real-world feature models. Our results show that on average, Inciident requires 80&nbsp;% less configurations to identify an interaction than using randomly selected configurations.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {27–36},
numpages = {10},
keywords = {Configurable Systems, Feature Interaction, Feature-Model Analysis, Software Product Lines},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.1145/1370750.1370753,
author = {Yoshimura, Kentaro and Narisawa, Fumio and Hashimoto, Koji and Kikuno, Tohru},
title = {FAVE: factor analysis based approach for detecting product line variability from change history},
year = {2008},
isbn = {9781605580241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370750.1370753},
doi = {10.1145/1370750.1370753},
abstract = {This paper describes a novel approach to detect variability in a software product line from its change history such that software changes are converted to vectors and a factor analysis is applied. To show the applicability of our approach, we conducted experimental applications using a software repository of automotive engine control software. As a result of the experiments, variability is detected from the change history of products.},
booktitle = {Proceedings of the 2008 International Working Conference on Mining Software Repositories},
pages = {11–18},
numpages = {8},
keywords = {variability, software product lines, reusable software, factor analysis},
location = {Leipzig, Germany},
series = {MSR '08}
}

@inproceedings{10.1145/3357765.3359525,
author = {Feichtinger, Kevin and Hinterreiter, Daniel and Linsbauer, Lukas and Pr\"{a}hofer, Herbert and Gr\"{u}nbacher, Paul},
title = {Supporting feature model evolution by suggesting constraints from code-level dependency analyses},
year = {2019},
isbn = {9781450369800},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357765.3359525},
doi = {10.1145/3357765.3359525},
abstract = {Feature models are a de facto standard for representing the commonalities and variability of product lines and configurable software systems. Requirements-level features are commonly implemented in multiple source code artifacts, which results in complex dependencies at the code level. As developers change and evolve features frequently, it is challenging to keep feature models consistent with their implementation. We thus present an approach combining feature-to-code mappings and code dependency analyses to inform engineers about possible inconsistencies. Our focus is on code-level changes requiring updates in feature dependencies and constraints. Our approach uses static code analysis and a variation control system to lift complex code-level dependencies to feature models. We present the suggested dependencies to the engineer in two ways: directly as links between features in a feature model and as a heatmap visualizing the dependency changes of all features in a model. We present results of an evaluation on the Pick-and-Place Unit system, which demonstrates the utility and performance of our approach and the quality of the suggestions.},
booktitle = {Proceedings of the 18th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {129–142},
numpages = {14},
keywords = {variation control system, static code analysis, product lines, dependency analysis},
location = {Athens, Greece},
series = {GPCE 2019}
}

@inproceedings{10.1145/3708557.3716329,
author = {Duarte, Carlos and Costa, Miguel and Seixas Pereira, Let\'{\i}cia and Guerreiro, Jo\~{a}o},
title = {Expanding Automated Accessibility Evaluations: Leveraging Large Language Models for Heading-Related Barriers},
year = {2025},
isbn = {9798400714092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708557.3716329},
doi = {10.1145/3708557.3716329},
abstract = {Ensuring digital resources are accessible to all users, including those with disabilities, is critical in today’s digital landscape. The growing volume of online content has intensified the need for automated accessibility evaluations to ensure compliance with accessibility guidelines. Yet, existing automated tools are limited in scope, being unable to identify many types of accessibility barriers. Recent advances in AI, particularly large language models (LLMs), offer opportunities to expand the range of automated accessibility checks. This work explores the ability of LLMs to detect accessibility barriers related to web page headings. We developed targeted prompts to help LLMs identify them and evaluated the effectiveness of three models – Llama 3.1, GPT-4o, and GPT-4o mini – in multiple versions of a reference webpage, each featuring different heading-related barriers. Findings reveal that model performance depends on barrier type: Llama 3.1 stands out at detecting structural issues like heading appropriateness and hierarchy, GPT-4o is better at identifying accessible names and semantic substitutions, while GPT-4o mini is the most versatile, handling complex structural modifications and labelling. This study highlights LLM’s potential in advancing web accessibility evaluation and bridging gaps in automated assessments.},
booktitle = {Companion Proceedings of the 30th International Conference on Intelligent User Interfaces},
pages = {39–42},
numpages = {4},
keywords = {Web Accessibility, LLM, Accessibility Evaluation, Headings, Llama, GPT},
location = {
},
series = {IUI '25 Companion}
}

@inproceedings{10.1145/3319535.3339815,
author = {Cao, Yulong and Xiao, Chaowei and Cyr, Benjamin and Zhou, Yimeng and Park, Won and Rampazzi, Sara and Chen, Qi Alfred and Fu, Kevin and Mao, Z. Morley},
title = {Adversarial Sensor Attack on LiDAR-based Perception in Autonomous Driving},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3339815},
doi = {10.1145/3319535.3339815},
abstract = {In Autonomous Vehicles (AVs), one fundamental pillar is perception,which leverages sensors like cameras and LiDARs (Light Detection and Ranging) to understand the driving environment. Due to its direct impact on road safety, multiple prior efforts have been made to study its the security of perception systems. In contrast to prior work that concentrates on camera-based perception, in this work we perform the first security study of LiDAR-based perception in AV settings, which is highly important but unexplored. We consider LiDAR spoofing attacks as the threat model and set the attack goal as spoofing obstacles close to the front of a victim AV. We find that blindly applying LiDAR spoofing is insufficient to achieve this goal due to the machine learning-based object detection process.Thus, we then explore the possibility of strategically controlling the spoofed attack to fool the machine learning model. We formulate this task as an optimization problem and design modeling methods for the input perturbation function and the objective function.We also identify the inherent limitations of directly solving the problem using optimization and design an algorithm that combines optimization and global sampling, which improves the attack success rates to around 75%. As a case study to understand the attack impact at the AV driving decision level, we construct and evaluate two attack scenarios that may damage road safety and mobility.We also discuss defense directions at the AV system, sensor, and machine learning model levels.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2267–2281},
numpages = {15},
keywords = {sensor attack, autonomous driving, adversarial machine learning},
location = {London, United Kingdom},
series = {CCS '19}
}

@inproceedings{10.1145/1403375.1403436,
author = {Rimolo-Donadio, Renato and Schuster, Christian and Gu, Xiaoxiong and Kwark, Young H. and Ritter, Mark B.},
title = {Analysis and optimization of the recessed probe launch for high frequency measurements of PCB interconnects},
year = {2008},
isbn = {9783981080131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1403375.1403436},
doi = {10.1145/1403375.1403436},
abstract = {Measurements of internal printed circuit board (PCB) structures such as striplines and vias face the problem of launching clean test signals into the device under test (DUT). Traditionally, coaxial connectors or surface probing with high frequency microprobes are used to provide interfaces to test equipment. Both approaches have to be carefully optimized in order to give adequate results for the multi-GHz range. This paper discusses a different access technique, the recessed probe launch (RPL), which was previously used by the authors for measurements up to 40 GHz. Full-wave 3D electromagnetic modeling is applied to analyze the parasitics of the proposed launch technique and to find strategies for its optimization. Comparison to measurement shows that the models are able to predict the major physics of the launch but several details still need to be explored, e.g. accurate modeling of the microprobes, material parameters, and network analyzer calibration.},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe},
pages = {252–255},
numpages = {4},
location = {Munich, Germany},
series = {DATE '08}
}

@inproceedings{10.1145/3451471.3451478,
author = {Gunawan, Fandi and K. Budiardjo, Eko},
title = {&nbsp;A Quest of Software Process Improvements in DevOps and Kanban: A Case Study in Small Software Company},
year = {2021},
isbn = {9781450388955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3451471.3451478},
doi = {10.1145/3451471.3451478},
abstract = {A good software process improves software products. In the case of a small software company, software development is a matter of survivability due to its limited resources to develop software. XYZ Company is a very small software company that adopted Kanban and DevOps and faced software delivery delays. It is necessary to recommend the software process improvements to solve this problem. Software process improvements are the outcomes of measurement and analysis of maturity levels using the ISO 29110 framework in a qualitative study. They are then analyzed using the Lean Six Sigma tools, namely gap analysis, root cause analysis, and Pareto analysis. Delphi method validated them and resulted in 18 improvement recommendations within four domains, namely (a) product, (b) people, (c) technology, and (d) process. The improvements span across two main processes within software development, namely (a) Project Management (PM) and (b) Software Implementation (SI). The XYZ Company or any agile-based software company could adopt the 18 improvement recommendations to enhance the software process and quality.},
booktitle = {Proceedings of the 2021 4th International Conference on Software Engineering and Information Management},
pages = {39–45},
numpages = {7},
keywords = {small software company, agile, Software Process Improvement, SPI, Kanban, ISO 29110, DevOps},
location = {Yokohama, Japan},
series = {ICSIM '21}
}

@inproceedings{10.1145/3652620.3688199,
author = {F\"{o}ldi\'{a}k, M\'{a}t\'{e}},
title = {Probabilistic Graph Queries for Design Space Exploration Under Uncertainty},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688199},
doi = {10.1145/3652620.3688199},
abstract = {Critical cyber-physical systems have an increasingly significant role in the world, and ensuring their safety is a high priority objective. State of the art approaches and engineering tools can support the development process from very early stages, with high-level system modeling, analysis capabilities, and exploration of alternatives. However, these approaches are limited when it comes evaluation of complex extra-functional characteristics over designs with uncertainties, typical to early system designs. In my thesis project, I intend to introduce probabilistic graph queries for high level, scalable probabilistic analysis, for analysing system models with design uncertainty and applicable in design space exploration. The approach will be evaluated on external case studies, focusing on key performance metrics related to applicability in the target context, such as runtime, precision and formal guarantees.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {142–148},
numpages = {7},
keywords = {cyber-physical systems, probabilistic analysis, graph queries, design space exploration, design uncertainty, lifting, safety},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3350768.3351993,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Towards the support of user preferences in search-based product line architecture design: an exploratory study},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3351993},
doi = {10.1145/3350768.3351993},
abstract = {Software Product Lines (SPLs) is a reuse approach in which a family of products is generalized in a common architecture that can be adapted to different clients. The Product Line Architecture (PLA) is one of the most important artifacts of a SPL. PLA design requires great human effort as it involves several factors that are usually in conflict. To ease this task, PLA design can be formulated as an optimization problem with many factors, i.e, as a multi-objective optimization problem. In this context, the MOA4PLA approach was proposed to optimize PLA design using search algorithms and metrics specific to the context. This approach supported by OPLA-Tool has already been used in several works demonstrating its applicability. However, MOA4PLA does not take into account aspects that are subjective, such as the preferences of a particular Decision Maker (DM). To do so, this paper presents a proposal to incorporate the user preferences in the optimization process performed by MOA4PLA, through an interactive process in which the DM subjectively evaluates the solutions in processing time. Thus, the solutions generated can be better suited to the DM's needs or preferences. In order to allow the user interaction, modifications were made in MOA4PLA and implemented in the OPLA-Tool. Aiming at an initial validation of the proposal, an exploratory study was carried out, composed of two experiments: a qualitative and a quantitative. These experiments were realized with the participation of a software architect. Empirical results pointed out that the proposed interactive process enables the generation of PLAs that are in accordance with the architect's preferences. Another significant contribution are the lessons learned on how to improve the interactive process.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {387–396},
numpages = {10},
keywords = {Product Line Architecture, Multi-Objective Optimization, Human-computer interaction},
location = {Salvador, Brazil},
series = {SBES '19}
}

@article{10.1109/TASLP.2024.3459430,
author = {Meng, Weixin and Li, Xiaoyu and Li, Andong and Luo, Xiaoxue and Yan, Shefeng and Li, Xiaodong and Zheng, Chengshi},
title = {Deep Kronecker Product Beamforming for Large-Scale Microphone Arrays},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3459430},
doi = {10.1109/TASLP.2024.3459430},
abstract = {Although deep learning based beamformers have achieved promising performance using small microphone arrays, they suffer from performance degradation in very challenging environments, such as extremely low Signal-to-Noise Ratio (SNR) environments, e.g., SNR &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$le$&lt;/tex-math&gt;&lt;/inline-formula&gt;−10 dB. A large-scale microphone array with dozens or hundreds of microphones can improve the performance of beamformers in these challenging scenarios because of its high spatial resolution. While a dramatic increase in the number of microphones leads to feature redundancy, causing difficulties in feature extraction and network training. As an attempt to improve the performance of deep beamformers for speech extraction in very challenging scenarios, this paper proposes a novel all neural Kronecker product beamforming denoted by ANKP-BF for large-scale microphone arrays by taking the following two aspects into account. Firstly, a larger microphone array can provide higher performance of spatial filtering when compared with a small microphone array, and deep neural networks are introduced for their powerful non-linear modeling capability in the speech extraction task. Secondly, the feature redundancy problem is solved by introducing the Kronecker product rule to decompose the original one high-dimension weight vector into the Kronecker product of two much lower-dimensional weight vectors. The proposed ANKP-BF is designed to operate in an end-to-end manner. Extensive experiments are conducted on simulated large-scale microphone-array signals using the DNS-Challenge corpus and WSJ0-SI84 corpus, and the real recordings in a semi-anechoic room and outdoor scenes are also used to evaluate and compare the performance of different methods. Quantitative results demonstrate that the proposed method outperforms existing advanced baselines in terms of multiple objective metrics, especially in very low SNR environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {4537–4553},
numpages = {17}
}

@inproceedings{10.1145/3384544.3384585,
author = {Ferguson, Danielle and Albright, Yan and Lomsak, Daniel and Hanks, Tyler and Orr, Kevin and Ligatti, Jay},
title = {PoCo: A Language for Specifying Obligation-Based Policy Compositions},
year = {2020},
isbn = {9781450376655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384544.3384585},
doi = {10.1145/3384544.3384585},
abstract = {Existing security-policy-specification languages allow users to specify obligations, but challenges remain in the composition of complex obligations, including effective approaches for resolving conflicts between policies and obligations and allowing policies to react to other obligations. This paper presents PoCo, a policy-specification language and enforcement system for the principled composition of atomic-obligation policies. PoCo enables policies to interact meaningfully with other policies' obligations, thus preventing unexpected and insecure behaviors that can arise from partially executed obligations or obligations that execute actions in violation of other policies.},
booktitle = {Proceedings of the 2020 9th International Conference on Software and Computer Applications},
pages = {331–338},
numpages = {8},
keywords = {policy specification, policy composition, obligations, Information security},
location = {Langkawi, Malaysia},
series = {ICSCA '20}
}

@inproceedings{10.1145/3613904.3642526,
author = {Seixas Pereira, Let\'{\i}cia and Matos, Maria and Duarte, Carlos},
title = {Exploring Mobile Device Accessibility: Challenges, Insights, and Recommendations for Evaluation Methodologies},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642526},
doi = {10.1145/3613904.3642526},
abstract = {With the ubiquitous use of mobile applications, it is paramount that they are accessible, so they can empower all users, including those with different needs. Determining if an app is accessible implies conducting an accessibility evaluation. While accessibility evaluations have been thoroughly studied in the web domain, there are still many open questions when evaluating mobile applications. This paper investigates mobile accessibility evaluation methodologies. We conducted four studies, including an examination of accessibility reports from European Member-states, interviews with accessibility experts, manual evaluations, and usability tests involving users. Our investigations have uncovered significant limitations in current evaluation methods, suggesting that the absence of authoritative guidelines and standards, similar to what exists for the web, but tailored specifically to mobile devices, hampers the effectiveness of accessibility evaluation and monitoring activities. Based on our findings, we present a set of recommendations aimed at improving the evaluation methodologies for assessing mobile applications’ accessibility.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {964},
numpages = {17},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3560905.3568515,
author = {Wang, Weiguo and Mottola, Luca and He, Yuan and Li, Jinming and Sun, Yimiao and Li, Shuai and Jing, Hua and Wang, Yulei},
title = {MicNest: Long-Range Instant Acoustic Localization of Drones in Precise Landing},
year = {2023},
isbn = {9781450398862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560905.3568515},
doi = {10.1145/3560905.3568515},
abstract = {We present MicNest: an acoustic localization system enabling precise landing of aerial drones. Drone landing is a crucial step in a drone's operation, especially as high-bandwidth wireless networks, such as 5G, enable beyond-line-of-sight operation in a shared airspace and applications such as instant asset delivery with drones gain traction. In MicNest, multiple microphones are deployed on a landing platform in carefully devised configurations. The drone carries a speaker transmitting purposefully-designed acoustic pulses. The drone may be localized as long as the pulses are correctly detected. Doing so is challenging: i) because of limited transmission power, propagation attenuation, background noise, and propeller interference, the Signal-to-Noise Ratio (SNR) of received pulses is intrinsically low; ii) the pulses experience non-linear Doppler distortion due to the physical drone dynamics while airborne; iii) as location information is to be used during landing, the processing latency must be reduced to effectively feed the flight control loop. To tackle these issues, we design a novel pulse detector, Matched Filter Tree (MFT), whose idea is to convert pulse detection to a tree search problem. We further present three practical methods to accelerate tree search jointly. Our real-world experiments show that MicNest is able to localize a drone 120 m away with 0.53% relative localization error at 20 Hz location update frequency.},
booktitle = {Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems},
pages = {504–517},
numpages = {14},
keywords = {acoustic localization, drone, microphone array},
location = {Boston, Massachusetts},
series = {SenSys '22}
}

@article{10.1109/TASLP.2024.3372891,
author = {de Souza, Luciana M. X. and Costa, M\'{a}rcio H. and Borges, Renata Coelho},
title = {Envelope-Based Multichannel Noise Reduction for Cochlear Implant Applications},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3372891},
doi = {10.1109/TASLP.2024.3372891},
abstract = {Cochlear implants (CI) are implantable electronic prostheses that aim to restore communication in people with severe to profound hearing loss. This is achieved by transforming the acoustic signals into electrical stimuli and applying them directly to the cochlea through a set of electrodes. Despite its effectiveness under optimal environmental conditions, its performance is severely degraded in the presence of noise. This work proposes an envelope-based minimum-variance distortionless response (MVDR) approach for multichannel noise reduction in CI applications. The original constraint in the conventional time-domain MVDR beamformer is modified to ensure that the second-order statistical moments of the processed and original speech-only envelopes are equivalent. Assuming strict computational limitations, as those found in commercial multi-electrode systems, a low computational-cost particular case of the proposed method results in a power-based MVDR that has a semi-analytical solution. The resulting algorithm provides a fast calculation method for its optimal coefficients without losing significant performance compared to its general form. Results obtained from computational simulations and objective performance criteria indicate higher intelligibility levels achieved by the proposed methods compared to the conventional time-domain MVDR beamformer. This evidence is corroborated by psychoacoustic experiments with ten normal-hearing volunteers listening to vocoded speech, and one CI user employing a research interface.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {1873–1884},
numpages = {12}
}

@inproceedings{10.1145/3384419.3430727,
author = {Sun, Ke and Chen, Chen and Zhang, Xinyu},
title = {"Alexa, stop spying on me!": speech privacy protection against voice assistants},
year = {2020},
isbn = {9781450375900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384419.3430727},
doi = {10.1145/3384419.3430727},
abstract = {Voice assistants (VAs) are becoming highly popular recently as a general means of interacting with the Internet of Things. However, the use of always-on microphones on VAs imposes a looming threat on users' privacy. In this paper, we propose MicShield, the first system that serves as a companion device to enforce privacy preservation on VAs. MicShield introduces a novel selective jamming mechanism, which obfuscates the user's private speech while passing legitimate voice commands to the VAs. It achieves this by using a phoneme level jamming control pipeline. Our implementation and experiments demonstrate that MicShield can effectively protect a user's private speech, without affecting the VA's responsiveness.},
booktitle = {Proceedings of the 18th Conference on Embedded Networked Sensor Systems},
pages = {298–311},
numpages = {14},
keywords = {voice assistant, selective jamming, privacy protection},
location = {Virtual Event, Japan},
series = {SenSys '20}
}

@article{10.1109/TASLP.2024.3473315,
author = {Westhausen, Nils L. and Kayser, Hendrik and Jansen, Theresa and Meyer, Bernd T.},
title = {Real-Time Multichannel Deep Speech Enhancement in Hearing Aids: Comparing Monaural and Binaural Processing in Complex Acoustic Scenarios},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3473315},
doi = {10.1109/TASLP.2024.3473315},
abstract = {Deep learning has the potential to enhance speech signals and increase their intelligibility for users of hearing aids. Deep models suited for real-world application should feature a low computational complexity and low processing delay of only a few milliseconds. In this paper, we explore deep speech enhancement that matches these requirements and contrast monaural and binaural processing algorithms in two complex acoustic scenes. Both algorithms are evaluated with objective metrics and in experiments with hearing-impaired listeners performing a speech-in-noise test. Results are compared to two traditional enhancement strategies, i.e., adaptive differential microphone processing and binaural beamforming. While in diffuse noise, all algorithms perform similarly, the binaural deep learning approach performs best in the presence of spatial interferers. Through a post-analysis, this can be attributed to improvements at low SNRs and to precise spatial filtering.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {4596–4606},
numpages = {11}
}

@inproceedings{10.1145/3494322.3494329,
author = {De Sanctis, Martina and Muccini, Henry and Vaidhyanathan, Karthik},
title = {A User-driven Adaptation Approach for Microservice-based IoT Applications},
year = {2022},
isbn = {9781450385664},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3494322.3494329},
doi = {10.1145/3494322.3494329},
abstract = {Modern IoT-based applications are developed by using microservices implementing various functionalities. However, they still tend to be rigid from a user’s perspective, i.e., the user typically adapts to how the software is designed. Conversely, we want the software and the IoT devices adapting to the user’s goal and its dynamic nature, thus making the user as one of the key design element. For these reasons, we present MiLA4U, a multi-level self-adaptive approach that works at the three different user, microservices, and devices levels. Specifically, it i)&nbsp;makes use of a goal model defining run-time user goals that must be achieved without compromising the overall QoS, by adaptations towards the other levels. It therefore ii)&nbsp;continuously monitors the QoS of the microservices and IoT, and iii)&nbsp;leverages multiple algorithms for the QoS-aware dynamic selection, execution, and adaptation of microservices and IoT devices. MiLA4U&nbsp;is experimented on a real case study. Evaluation results show that it is able to satisfy the user goals while guaranteeing higher QoS on the microservices and IoT devices compared to standard baselines.},
booktitle = {Proceedings of the 11th International Conference on the Internet of Things},
pages = {48–56},
numpages = {9},
keywords = {multi-level adaptation, microservices, User goals, IoT},
location = {St.Gallen, Switzerland},
series = {IoT '21}
}

@inproceedings{10.1145/3372297.3417234,
author = {Schwartz, Edward J. and Cohen, Cory F. and Gennari, Jeffrey S. and Schwartz, Stephanie M.},
title = {A Generic Technique for Automatically Finding Defense-Aware Code Reuse Attacks},
year = {2020},
isbn = {9781450370899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372297.3417234},
doi = {10.1145/3372297.3417234},
abstract = {Code reuse attacks have been the subject of a substantial amount of research during the past decade. This research largely resulted from early work on Return-Oriented Programming (ROP), which showed that the then newly proposed Non-Executable Memory (NX) defense could be bypassed. More recently, the research community has been simultaneously investigating new defenses that are believed to thwart code reuse attacks, such as Control Flow Integrity (CFI), and defense-aware techniques for attacking these defenses, such as Data-Oriented Programming (DOP). Unfortunately, the feasibility of defense-aware attacks are very dependent on the behaviors of the attacked program, which makes it difficult for defenders to understand how much protection a defense such as CFI may provide. To better understand this, researchers have introduced automated defense-aware code reuse attack systems. Unfortunately, the handful of existing systems implement a single fixed, defense-specific strategy that is complex and cannot be used to consider other defenses.In this paper, we propose a generic framework for automatically discovering defense-aware code reuse attacks in executables. Unlike existing work, which utilizes hard-coded strategies for specific defenses, our framework can produce attacks for multiple defenses by analyzing the runtime behavior of the defense. The high-level insight behind our framework is that code reuse attacks can be defined as a state reachability problem, and that defenses prevent some transitions between states. We implement our framework as a tool named Limbo, which employs an existing binary concolic executor to solve the reachability problem. We evaluate Limbo and show that it excels when there is little code available for reuse, making it complementary to existing techniques. We show that, in such scenarios, Limbo outperforms existing systems that automate ROP attacks, as well as systems that automate DOP attacks in the presence of fine-grained CFI, despite having no special knowledge about ROP or DOP attacks.},
booktitle = {Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1789–1801},
numpages = {13},
keywords = {software defenses, code reuse attacks},
location = {Virtual Event, USA},
series = {CCS '20}
}

@proceedings{10.1145/3680121,
title = {CoNEXT '24: Proceedings of the 20th International Conference on emerging Networking EXperiments and Technologies},
year = {2024},
isbn = {9798400711084},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 20th edition of the ACM Conference on Emerging Networking Experiment and Technologies (ACM CoNEXT 2024). CoNEXT is a premier and highly selective venue in computer networking. The first edition of the conference was organized in Toulouse in 2005, and this year we are in Los Angeles.CoNEXT employs a hybrid model with two submission deadlines (December and June). Two types of papers can be submitted to CoNEXT: (i) long papers presenting significant and novel research results on emerging computer networks and applications, and (ii) short papers for contributions whose novelty and impact show the same technical excellence, but whose description fits within 6 pages. CoNEXT employs a rigorous review process, including two rounds of reviews by TPC members, online discussions, and a TPC meeting. The accepted long papers are published in the journal Proceedings of the ACM on Networking (PACMNET) while the short papers appear in this conference proceedings.A total of 82 papers were submitted to the December 2023 deadline. 63 of these were long papers, and 19 were short papers. Five long papers were directly accepted and appeared in the June 2024 issue of PACMNET. Another five long papers submitted in December 2023 were revised by the authors and appeared in the September 2024 issue of PACMNET. Three short papers submitted in December 2023 were revised by the authors, and appear in these conference proceedings.},
location = {Los Angeles, CA, USA}
}

@inproceedings{10.1145/1868688.1868698,
author = {Apel, Sven and Scholz, Wolfgang and Lengauer, Christian and K\"{a}stner, Christian},
title = {Language-independent reference checking in software product lines},
year = {2010},
isbn = {9781450302081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868688.1868698},
doi = {10.1145/1868688.1868698},
abstract = {Feature-Oriented Software Development (FOSD) is a paradigm for the development of software product lines. A challenge in FOSD is to guarantee that all software systems of a software product line are correct. Recent work on type checking product lines can provide a guarantee of type correctness without generating all possible systems. We generalize previous results by abstracting from the specifics of particular programming languages. In a first attempt, we present a reference-checking algorithm that performs key tasks of product-line type checking independently of the target programming language. Experiments with two sample product lines written in Java and C are encouraging and give us confidence that this approach is promising.},
booktitle = {Proceedings of the 2nd International Workshop on Feature-Oriented Software Development},
pages = {65–71},
numpages = {7},
keywords = {type systems, software product lines, feature-oriented software development, FeatureTweezer, FeatureHouse},
location = {Eindhoven, The Netherlands},
series = {FOSD '10}
}

@article{10.1109/TASLP.2022.3183929,
author = {Choi, Jung-Woo and Zotter, Franz and Jo, Byeongho and Yoo, Jae-Hyoun},
title = {Multiarray Eigenbeam-ESPRIT for 3D Sound Source Localization With Multiple Spherical Microphone Arrays},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3183929},
doi = {10.1109/TASLP.2022.3183929},
abstract = {A 3D sound source localization (SSL) technique for multiple spherical arrays is proposed. Spherical arrays have been popularly used for direction of arrival (DoA) estimation, which serves as important prior knowledge for source separation, dereverberation, and binaural rendering for VR applications. In this work, a parametric 3D SSL technique is proposed that can detect multiple sources using multiple spherical array recordings. From the enriched information by using multiple arrays, we show that the parametric estimation of 3D positions is possible without scanning candidate positions in 3D space. To mitigate the problem of the ambiguous association between DoAs estimated from different arrays, a total covariance matrix including both auto- and cross-covariance matrices between arrays is utilized such that DoAs from arrays are automatically paired to uniquely localize 3D source positions. The joint parameter estimation is based on the generalized joint Schur decomposition of multiple matrices combined with a geometric projection robustly guiding the convergence of the proposed iteration algorithm. Simulations conducted in anechoic and reverberant room conditions reveal that the proposed technique can accurately determine multiple source positions in 3D space without ambiguities.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {2310–2325},
numpages = {16}
}

@inproceedings{10.1145/1282280.1282282,
author = {Yu, Ning and Vu, Khanh and Hua, Kien A.},
title = {An in-memory relevance feedback technique for high-performance image retrieval systems},
year = {2007},
isbn = {9781595937339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1282280.1282282},
doi = {10.1145/1282280.1282282},
abstract = {Content-based image retrieval with relevant feedback has been widely adopted as the query model of choice for improved effectiveness in image retrieval. The effectiveness of this solution, however, depends on the efficiency of the feedback mechanism. Current methods rely on searching the database, stored on disks, in each round of relevance feedback. This strategy incurs long delay making relevance feedback less friendly to the user, especially for very large databases. Thus, scalability is a limitation of existing solutions. In this paper, we propose an in-memory relevance feedback technique to substantially reduce the delay associated with feedback processing, and therefore improve system usability. Our new data-independent dimensionality-reduction technique is used to compress the metadata to build a small in-memory database to support relevance feedback operations with minimal disk accesses. We compare the performance of this approach with conventional relevance feedback techniques in terms of computation efficiency and retrieval accuracy. The results indicate that the new technique substantially reduces response time for user feedback while maintaining the quality of the retrieval.},
booktitle = {Proceedings of the 6th ACM International Conference on Image and Video Retrieval},
pages = {9–16},
numpages = {8},
keywords = {in memory relevance feedback, dimension reduction},
location = {Amsterdam, The Netherlands},
series = {CIVR '07}
}

@inproceedings{10.1145/3133956.3134052,
author = {Zhang, Guoming and Yan, Chen and Ji, Xiaoyu and Zhang, Tianchen and Zhang, Taimin and Xu, Wenyuan},
title = {DolphinAttack: Inaudible Voice Commands},
year = {2017},
isbn = {9781450349468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3133956.3134052},
doi = {10.1145/3133956.3134052},
abstract = {Speech recognition (SR) systems such as Siri or Google Now have become an increasingly popular human-computer interaction method, and have turned various systems into voice controllable systems (VCS). Prior work on attacking VCS shows that the hidden voice commands that are incomprehensible to people can control the systems. Hidden voice commands, though "hidden", are nonetheless audible. In this work, we design a totally inaudible attack, DolphinAttack, that modulates voice commands on ultrasonic carriers (e.g., f &gt; 20 kHz) to achieve inaudibility. By leveraging the nonlinearity of the microphone circuits, the modulated low-frequency audio commands can be successfully demodulated, recovered, and more importantly interpreted by the speech recognition systems. We validated DolphinAttack on popular speech recognition systems, including Siri, Google Now, Samsung S Voice, Huawei HiVoice, Cortana and Alexa. By injecting a sequence of inaudible voice commands, we show a few proof-of-concept attacks, which include activating Siri to initiate a FaceTime call on iPhone, activating Google Now to switch the phone to the airplane mode, and even manipulating the navigation system in an Audi automobile. We propose hardware and software defense solutions, and suggest to re-design voice controllable systems to be resilient to inaudible voice command attacks.},
booktitle = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
pages = {103–117},
numpages = {15},
keywords = {defense, mems microphones, security analysis, speech recognition, voice controllable systems},
location = {Dallas, Texas, USA},
series = {CCS '17}
}

@inproceedings{10.1145/3664475.3664731,
author = {McLeran, Aaron},
title = {Advances in Real Time Audio Rendering - Part 1},
year = {2024},
isbn = {9798400706837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664475.3664731},
doi = {10.1145/3664475.3664731},
abstract = {Hearing is the most time-sensitive of the human senses. The technology underlying real-time audio rendering must provide control over our physical, perceptual, cultural, and aesthetic worlds within the tightest of deadlines and with perfect temporal coherence. This course offers an introduction to state-of-the-art real-time audio rendering technology. We dive into the core concepts and challenges that define the problem space and touch on similarities shared by real-time graphic rendering and non-real-time audio rendering.},
booktitle = {ACM SIGGRAPH 2024 Courses},
articleno = {6},
numpages = {14},
location = {Denver, CO, USA},
series = {SIGGRAPH Courses '24}
}

@proceedings{10.1145/3560905,
title = {SenSys '22: Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems},
year = {2022},
isbn = {9781450398862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to ACM SenSys 2022, the 20th ACM Conference on Embedded Networked Sensor Systems, the premier computer systems conference focused on networked sensing systems and applications.},
location = {Boston, Massachusetts}
}

@inproceedings{10.1145/3394486.3403193,
author = {Mandros, Panagiotis and Kaltenpoth, David and Boley, Mario and Vreeken, Jilles},
title = {Discovering Functional Dependencies from Mixed-Type Data},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403193},
doi = {10.1145/3394486.3403193},
abstract = {Given complex data collections, practitioners can perform non-parametric functional dependency discovery (FDD) to uncover relationships between variables that were previously unknown. However, known FDD methods are applicable to nominal data, and in practice non-nominal variables are discretized, e.g., in a pre-processing step. This is problematic because, as soon as a mix of discrete and continuous variables is involved, the interaction of discretization with the various dependency measures from the literature is poorly understood. In particular, it is unclear whether a given discretization method even leads to a consistent dependency estimate. In this paper, we analyze these fundamental questions and derive formal criteria as to when a discretization process applied to a mixed set of random variables leads to consistent estimates of mutual information. With these insights, we derive an estimator framework applicable to any task that involves estimating mutual information from multivariate and mixed-type data. Last, we extend with this framework a previously proposed FDD approach for reliable dependencies. Experimental evaluation shows that the derived reliable estimator is both computationally and statistically efficient, and leads to effective FDD algorithms for mixed-type data.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1404–1414},
numpages = {11},
keywords = {mutual information, mixed data, functional dependency discovery},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1145/3384419.3430772,
author = {Wang, Ziqi and Chen, Zhe and Singh, Akash Deep and Garcia, Luis and Luo, Jun and Srivastava, Mani B.},
title = {UWHear: through-wall extraction and separation of audio vibrations using wireless signals},
year = {2020},
isbn = {9781450375900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384419.3430772},
doi = {10.1145/3384419.3430772},
abstract = {An ability to detect, classify, and locate complex acoustic events can be a powerful tool to help smart systems build context-awareness, e.g., to make rich inferences about human behaviors in physical spaces. Conventional methods to measure acoustic signals employ microphones as sensors. As signals from multiple acoustic sources are blended during propagation to a sensor, such methods impose a dual challenge of separating the signal for an acoustic event from background noise and from other acoustic events of interest. Recent research has proposed using radio-frequency (RF) signals, e.g., Wi-Fi and millimeter-wave (mmWave), to sense sound directly from source vibrations. Whereas these works allow separating an acoustic event from background noise, they cannot monitor multiple sound sources simultaneously. In this paper, we present UWHear, a system that simultaneously recovers and separates sounds from multiple sources. Unlike previous works using continuous-wave RF, UWHear employs Impulse Radio Ultra-Wideband (IR-UWB) technology, in order to construct an enhanced audio sensing system tackling the above challenges. Further, IR-UWB radios can penetrate light building materials, which enables UWHear to operate in some non-line-of-sight (NLOS) conditions. In addition to providing a theoretical guarantee for audio recovery using RF pulses, we also implement an audio sensing prototype exploiting a commercial-off-the-shelf IR-UWB radar. Our experiments show that UWHear can effectively separate the content of two speakers that are placed only 25cm apart. UWHear can also capture and separate multiple sounds and vibrations of household appliances while being immune to non-target noise coming from other directions.},
booktitle = {Proceedings of the 18th Conference on Embedded Networked Sensor Systems},
pages = {1–14},
numpages = {14},
keywords = {wireless vibrometry, audio sensing, RF sensing, IR-UWB radar},
location = {Virtual Event, Japan},
series = {SenSys '20}
}

@inproceedings{10.1145/3471621.3471866,
author = {Zhang, Shaohu and Das, Anupam},
title = {HandLock: Enabling 2-FA for Smart Home Voice Assistants using Inaudible Acoustic Signal},
year = {2021},
isbn = {9781450390583},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3471621.3471866},
doi = {10.1145/3471621.3471866},
abstract = {The use of voice-control technology has become mainstream and is growing worldwide. While voice assistants provide convenience through automation and control of home appliances, the open nature of the voice channel makes voice assistants difficult to secure. As a result voice assistants have been shown to be vulnerable to replay attacks, impersonation attacks and inaudible voice commands. Existing defenses do not provide a practical solution as they either rely on external hardware (e.g., motion sensors) or work under very constrained settings (e.g., holding the device close to a user’s mouth). We introduce the concept of using a gesture-based authentication system for smart home voice assistants called HandLock, which uses built-in microphones and speakers to generate and sense inaudible acoustic signals to detect the presence of a known (i.e., authorized) hand gesture. Our proposed approach can act as a second-factor authentication (2-FA) for performing specific sensitive operations like confirming online purchases through voice assistants. Through extensive experiments involving 45 participants, we show that HandLock can achieve on average 96.51% true-positive-rate (TPR) at the expense of 0.82% false-acceptance-rate (FAR). We perform a comprehensive analysis of HandLock under various settings to showcase its accuracy, stability, resilience to attacks, and usability. Our analysis shows that HandLock can not only successfully thwart impersonation attacks, but can do so while incurring very low overheads and is compatible with modern voice assistants.},
booktitle = {Proceedings of the 24th International Symposium on Research in Attacks, Intrusions and Defenses},
pages = {251–265},
numpages = {15},
keywords = {voice assistants, two-factor authentication, hand gesture, acoustic sensing},
location = {San Sebastian, Spain},
series = {RAID '21}
}

@inproceedings{10.1145/3641181.3641198,
author = {Rey, William P and Jain, Shreyansh M and Lambino, Juan Carlos S and Raymundo, Zachary Josh T},
title = {SenSabong: A sensor-based Application for the aid of training Cocks for CockFighting},
year = {2024},
isbn = {9798400709319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641181.3641198},
doi = {10.1145/3641181.3641198},
abstract = {This study presents the development and testing of a precise cockfighting training aid leveraging a modified impact sensor microphone. The research aimed to enhance cockfighting practices by providing trainers with a reliable tool to optimize rooster capabilities. The device, integrated with the SenSabong App, accurately recorded roosters' hits, distinguishing between fight-related and incidental impacts. Rigorous testing confirmed the device's reliability and effectiveness in improving rooster performance with recommended training procedures. The findings contribute to ethical and effective cockfighting training, representing a crucial step toward advancing the sport. Recommendations include integrating wireless connectivity for enhanced user experiences and incorporating gyroscope and accelerometer technologies for precise and real-time training feedback.},
booktitle = {Proceedings of the 2024 10th International Conference on Computing and Data Engineering},
pages = {133–140},
numpages = {8},
location = {Bangkok, Thailand},
series = {ICCDE '24}
}

@article{10.1145/3549526,
author = {Perez-Cerrolaza, Jon and Abella, Jaume and Kosmidis, Leonidas and Calderon, Alejandro J. and Cazorla, Francisco and Flores, Jose Luis},
title = {GPU Devices for Safety-Critical Systems: A Survey},
year = {2022},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3549526},
doi = {10.1145/3549526},
abstract = {&nbsp;Graphics Processing Unit (GPU) devices and their associated software programming languages and frameworks can deliver the computing performance required to facilitate the development of next-generation high-performance safety-critical systems such as autonomous driving systems. However, the integration of complex, parallel, and computationally demanding software functions with different safety-criticality levels on GPU devices with shared hardware resources contributes to several safety certification challenges. This survey categorizes and provides an overview of research contributions that address GPU devices’ random hardware failures, systematic failures, and independence of execution.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {147},
numpages = {37},
keywords = {spatial independence, time independence, Diagnostic coverage}
}

@inproceedings{10.1145/2188286.2188347,
author = {Dayarathna, Miyuru and Suzumura, Toyotaro},
title = {Hirundo: a mechanism for automated production of optimized data stream graphs},
year = {2012},
isbn = {9781450312028},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2188286.2188347},
doi = {10.1145/2188286.2188347},
abstract = {Stream programs have to be crafted carefully to maximize the performance gain that can be obtained from stream processing environments. Manual fine tuning of a stream program is a very difficult process which requires considerable amount of programmer time and expertise. In this paper we present Hirundo, which is a mechanism for automatically generating optimized stream programs that are tailored for the environment they run. Hirundo analyzes, identifies the structure of a stream program, and transforms it to many different sample programs with same semantics using the notions of Tri-Operator Transformation, Transformer Blocks, and Operator Blocks Fusion. Then it uses empirical optimization information to identify a small subset of generated sample programs that could deliver high performance. It runs the selected sample programs in the run-time environment for a short period of time to obtain their performance information. Hirundo utilizes these information to output a ranked list of optimized stream programs that are tailored for a particular run-time environment. Hirundo has been developed using Python as a prototype application for optimizing SPADE programs, which run on System S stream processing run-time. Using three example real world stream processing applications we demonstrate effectiveness of our approach, and discuss how well it generalizes for automatic stream program performance optimization.},
booktitle = {Proceedings of the 3rd ACM/SPEC International Conference on Performance Engineering},
pages = {335–346},
numpages = {12},
keywords = {stream processing, scalability, performance optimization, fault tolerance, data-intensive computing},
location = {Boston, Massachusetts, USA},
series = {ICPE '12}
}

@article{10.1145/3610873,
author = {Zhao, Running and Yu, Jiangtao and Zhao, Hang and Ngai, Edith C.H.},
title = {Radio2Text: Streaming Speech Recognition Using mmWave Radio Signals},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
url = {https://doi.org/10.1145/3610873},
doi = {10.1145/3610873},
abstract = {Millimeter wave (mmWave) based speech recognition provides more possibility for audio-related applications, such as conference speech transcription and eavesdropping. However, considering the practicality in real scenarios, latency and recognizable vocabulary size are two critical factors that cannot be overlooked. In this paper, we propose Radio2Text, the first mmWave-based system for streaming automatic speech recognition (ASR) with a vocabulary size exceeding 13,000 words. Radio2Text is based on a tailored streaming Transformer that is capable of effectively learning representations of speech-related features, paving the way for streaming ASR with a large vocabulary. To alleviate the deficiency of streaming networks unable to access entire future inputs, we propose the Guidance Initialization that facilitates the transfer of feature knowledge related to the global context from the non-streaming Transformer to the tailored streaming Transformer through weight inheritance. Further, we propose a cross-modal structure based on knowledge distillation (KD), named cross-modal KD, to mitigate the negative effect of low quality mmWave signals on recognition performance. In the cross-modal KD, the audio streaming Transformer provides feature and response guidance that inherit fruitful and accurate speech information to supervise the training of the tailored radio streaming Transformer. The experimental results show that our Radio2Text can achieve a character error rate of 5.7% and a word error rate of 9.4% for the recognition of a vocabulary consisting of over 13,000 words.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {142},
numpages = {28},
keywords = {Knowledge Distillation, Millimeter Wave, Radar Sensing, Streaming Speech Recognition, Wireless Sensing}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00122,
author = {Rosiak, Kamil},
title = {Extractive multi product-line engineering},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00122},
doi = {10.1109/ICSE-Companion52605.2021.00122},
abstract = {Cloning is a general approach to create new functionality within variants as well as new system variants. It is a fast, flexible, intuitive, and economical approach to evolve systems in the short run. However, in the long run, the maintenance effort increases. A common solution to this problem is the extraction of a product line from a set of cloned variants. This process requires a detailed analysis of variants to extract variability information. However, clones within a variant are usually not considered in the process, but are also a cause for unsustainable software. This thesis proposes an extractive multi product-line engineering approach to re-establish the sustainable development of software variants. We propose an approach to re-engineer intra-system and inter-system clones into reusable, configurable components stored in an integrated platform and synthesize a matching multilayer feature model.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {263–265},
numpages = {3},
keywords = {variability mining, refactoring, multi product-line, clone detection},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3360774.3360801,
author = {Lee, Yongwoo and Li, Jingjie and Kim, Younghyun},
title = {MicPrint: acoustic sensor fingerprinting for spoof-resistant mobile device authentication},
year = {2020},
isbn = {9781450372831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360774.3360801},
doi = {10.1145/3360774.3360801},
abstract = {Smartphones are the most commonly used computing platform for accessing sensitive and important information placed on the Internet. Authenticating the smartphone's identity in addition to the user's identity is a widely adopted security augmentation method since conventional user authentication methods, such as password entry, often fail to provide strong protection by itself.In this paper, we propose a sensor-based device fingerprinting technique for identifying and authenticating individual mobile devices. Our technique, called MicPrint, exploits the unique characteristics of embedded microphones in mobile devices due to manufacturing variations in order to uniquely identify each device. Unlike conventional sensor-based device fingerprinting that are prone to spoofing attack via malware, MicPrint is fundamentally spoof-resistant since it uses acoustic features that are prominent only when the user blocks the microphone hole. This simple user intervention acts as implicit permission to fingerprint the sensor and can effectively prevent unauthorized fingerprinting using malware. We implement MicPrint on Google Pixel 1 and Samsung Nexus to evaluate the accuracy of device identification. We also evaluate its security against simple raw data attacks and sophisticated impersonation attacks. The results show that after several incremental training cycles under various environmental noises, MicPrint can achieve high accuracy and reliability for both smartphone models.},
booktitle = {Proceedings of the 16th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {248–257},
numpages = {10},
keywords = {sensor, multi-factor authentication, microphone, device fingerprinting},
location = {Houston, Texas, USA},
series = {MobiQuitous '19}
}

@inproceedings{10.1145/3491418.3530759,
author = {Boubin, Jayson and Banerjee, Avishek and Yun, Jihoon and Qi, Haiyang and Fang, Yuting and Chang, Steve and Srinivasan, Kannan and Ramnath, Rajiv and Arora, Anish},
title = {PROWESS: An Open Testbed for Programmable Wireless Edge Systems},
year = {2022},
isbn = {9781450391610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491418.3530759},
doi = {10.1145/3491418.3530759},
abstract = {Edge computing is a growing paradigm where compute resources are provisioned between data sources and the cloud to decrease compute latency from data transfer, lower costs, comply with security policies, and more. Edge systems are as varied as their applications, serving internet services, IoT, and emerging technologies. Due to the tight constraints experienced by many edge systems, research computing testbeds have become valuable tools for edge research and application benchmarking. Current testbed infrastructure, however, fails to properly emulate many important edge contexts leading to inaccurate benchmarking. Institutions with broad interests in edge computing can build testbeds, but prior work suggests that edge testbeds are often application or sensor specific. A general edge testbed should include access to many of the sensors, software, and accelerators on which edge systems rely, while slicing those resources to fit user-defined resource footprints. PROWESS is an edge testbed that answers this challenge. PROWESS provides access across an institution to sensors, compute resources, and software for testing constrained edge applications. PROWESS runs edge workloads as sets of containers with access to sensors and specialized hardware on an expandable cluster of light-weight edge nodes which leverage institutional networks to decrease implementation cost and provide wide access to sensors. We implemented a multi-node PROWESS deployment connected to sensors across Ohio State University’s campus. Using three edge-native applications, we demonstrate that PROWESS is simple to configure, has a small resource footprint, scales gracefully, and minimally impacts institutional networks. We also show that PROWESS closely approximates native execution of edge workloads and facilitates experiments that other systems testbeds can not.},
booktitle = {Practice and Experience in Advanced Research Computing 2022: Revolutionary: Computing, Connections, You},
articleno = {11},
numpages = {9},
location = {Boston, MA, USA},
series = {PEARC '22}
}

@inproceedings{10.5555/2820126.2820133,
author = {Atlee, Joanne M. and Fahrenberg, Uli and Legay, Axel},
title = {Measuring behaviour interactions between product-line features},
year = {2015},
publisher = {IEEE Press},
abstract = {We suggest a method for measuring the degree to which features interact in feature-oriented software development. To this end, we extend the notion of simulation between transition systems to a similarity measure and lift it to compute a behaviour interaction score in featured transition systems. We then develop an algorithm which can compute the degree of feature interactions in a featured transition system in an efficient manner.},
booktitle = {Proceedings of the Third FME Workshop on Formal Methods in Software Engineering},
pages = {20–25},
numpages = {6},
location = {Florence, Italy},
series = {Formalise '15}
}

@article{10.1109/TASLP.2024.3451974,
author = {Hu, Jinbo and Cao, Yin and Wu, Ming and Kong, Qiuqiang and Yang, Feiran and Plumbley, Mark D. and Yang, Jun},
title = {Selective-Memory Meta-Learning With Environment Representations for Sound Event Localization and Detection},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3451974},
doi = {10.1109/TASLP.2024.3451974},
abstract = {Environment shifts and conflicts present significant challenges for learning-based sound event localization and detection (SELD) methods. SELD systems, when trained in particular acoustic settings, often show restricted generalization capabilities for diverse acoustic environments. Furthermore, obtaining annotated samples for spatial sound events is notably costly. Deploying a SELD system in a new environment requires extensive time for re-training and fine-tuning. To overcome these challenges, we propose environment-adaptive Meta-SELD, designed for efficient adaptation to new environments using minimal data. Our method specifically utilizes computationally synthesized spatial data and employs Model-Agnostic Meta-Learning (MAML) on a pre-trained, environment-independent model. The method then utilizes fast adaptation to unseen real-world environments using limited samples from the respective environments. Inspired by the Learning-to-Forget approach, we introduce the concept of selective memory as a strategy for resolving conflicts across environments. This approach involves selectively memorizing target-environment-relevant information and adapting to the new environments through the selective attenuation of model parameters. In addition, we introduce environment representations to characterize different acoustic settings, enhancing the adaptability of our attenuation approach to various environments. We evaluate our proposed method on the development set of the Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23) dataset and computationally synthesized scenes. Experimental results demonstrate the superior performance of the proposed method compared to conventional supervised learning methods, particularly in localization.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {4313–4327},
numpages = {15}
}

@inproceedings{10.1145/3377812.3382153,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {FeatureNET: diversity-driven generation of deep learning models},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3382153},
doi = {10.1145/3377812.3382153},
abstract = {We present FeatureNET, an open-source Neural Architecture Search (NAS) tool1 that generates diverse sets of Deep Learning (DL) models. FeatureNET relies on a meta-model of deep neural networks, consisting of generic configurable entities. Then, it uses tools developed in the context of software product lines to generate diverse (maximize the differences between the generated) DL models. The models are translated to Keras and can be integrated into typical machine learning pipelines. FeatureNET allows researchers to generate seamlessly a large variety of models. Thereby, it helps choosing appropriate DL models and performing experiments with diverse models (mitigating potential threats to validity). As a NAS method, FeatureNET successfully generates models performing equally well with handcrafted models.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {41–44},
numpages = {4},
keywords = {neural architecture search, configuration search, NAS, AutoML},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/24533.24544,
author = {Calabough, Jerry},
title = {Software configuration—an NP-complete problem},
year = {1987},
isbn = {0897912225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/24533.24544},
doi = {10.1145/24533.24544},
abstract = {Configuration Control File (CCF) production is very complex, thousands of code packages, data blocks and parameter values must be linked under many constraints including:
Common data and code less than 8192 bytesMaximum of 5 registers per taskAll systems data must have common capabilities
NP-complete problems are commonly known as knapsack or bin packing problems. They have no known algorithm which solves them in a time period bounded by a polynomial function of the number of inputs. Rules-of-thumb, or heuristics are the only practical approach to their solution. CCF segmentation to meet constraints discussed above is an example of Expert System technology applied to a classic NP-complete problem.Heuristics developed with traditional data processing techniques initially performed satisfactorily. However, as program development proceeded, Central Processor Unit (CPU) time for CCF production became a concern, both from a commitment of CPU resources and lost productivity. Traditional techniques failed to improve the heuristics and the project began to slip. Projected time to produce the CCF for a fully developed program was totally unacceptable, jeopardizing the project.Clearly another approach was required. Because existing heuristics were based on a concept of rules, research indicated an expert system using rules and a knowledge based approach had the highest probability of success.The paper emphasizes the development process of a knowledge based system from the perspective of the responsible project manager. The methodology is also applicable to common business problems.},
booktitle = {Proceedings of the Conference on The 1987 ACM SIGBDP-SIGCPR Conference},
pages = {182–194},
numpages = {13},
location = {Coral Gables, Florida, USA},
series = {SIGCPR '87}
}

@inproceedings{10.1145/2660190.2660191,
author = {Kolesnikov, Sergiy and Roth, Judith and Apel, Sven},
title = {On the relation between internal and external feature interactions in feature-oriented product lines: a case study},
year = {2014},
isbn = {9781450329804},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660190.2660191},
doi = {10.1145/2660190.2660191},
abstract = {The feature-interaction problem has been explored for many years. Still, we lack sufficient knowledge about the interplay of different kinds of interactions in software product lines. Exploring the relations between different kinds of feature interactions will allow us to learn more about the nature of interactions and their causes. This knowledge can then be applied for improving existing approaches for detecting, managing, and resolving feature interactions. We present a framework for studying relations between different kinds of interactions. Furthermore, we report and discuss the results of a preliminary study in which we examined correlations between internal feature interactions (quantified by a set of software measures) and external feature interactions (represented by product-line-specific type errors). We performed the evaluation on a set of 15 feature-oriented, Java-based product lines. We observed moderate correlations between the interactions under discussion. This gives us confidence that we can apply our approach to studying other types of external feature interactions (e.g., performance interactions).},
booktitle = {Proceedings of the 6th International Workshop on Feature-Oriented Software Development},
pages = {1–8},
numpages = {8},
keywords = {software measures, feature-oriented software development, feature interactions},
location = {V\"{a}ster\r{a}s, Sweden},
series = {FOSD '14}
}

@article{10.1145/3351239,
author = {Gao, Yang and Wang, Wei and Phoha, Vir V. and Sun, Wei and Jin, Zhanpeng},
title = {EarEcho: Using Ear Canal Echo for Wearable Authentication},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3351239},
doi = {10.1145/3351239},
abstract = {Smart wearable devices have recently become one of the major technological trends and been widely adopted by the general public. Wireless earphones, in particular, have seen a skyrocketing growth due to its great usability and convenience. With the goal of seeking a more unobtrusive wearable authentication method that the users can easily use and conveniently access, in this study we present EarEcho as a novel, affordable, user-friendly biometric authentication solution. EarEcho takes advantages of the unique physical and geometrical characteristics of human ear canal and assesses the content-free acoustic features of in-ear sound waves for user authentication in a wearable and mobile manner. We implemented the proposed EarEcho on a proof-of-concept prototype and tested it among 20 subjects under diverse application scenarios. We can achieve a recall of 94.19% and precision of 95.16% for one-time authentication, while a recall of 97.55% and precision of 97.57% for continuous authentication. EarEcho has demonstrated its stability over time and robustness to cope with the uncertainties on the varying background noises, body motions, and sound pressure levels.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {81},
numpages = {24},
keywords = {wearable devices, echo, ear canal, biometric, authentication, Acoustic}
}

@inproceedings{10.1145/381677.381707,
author = {Ritter, Mike and Friday, Robert J. and Garces, Rodrigo and San Filippo, Weill and Nguyen, Cuong-Thinh and Srivastava, Arty},
title = {Mobile connectivity protocols and throughput measurements in the Ricochet Microcellular data network (MCDN) system},
year = {2001},
isbn = {1581134223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/381677.381707},
doi = {10.1145/381677.381707},
abstract = {We describe the protocols implemented in the Ricochet MCDN system to provide continuous connectivity to mobile users traveling up to 70 mph. These protocols are general in nature for any frequency-hopping microcell-based system, particularly those that follow the FCC part 15.247 rules [9] and operate in unlicensed spectrum. We also present throughput measurements as a function of velocity and describe a model to predict those numbers based upon the protocols implemented. The MCDN system is a mesh-based system of microcells that are connected wirelessly to an interspersed mesh of wired access points (WAPs) that cover approximately 12 square miles on average [7]. The average microcell density is approximately 5-6 per square mile, with 3-8 overlapping cells at each point. Since the system is entirely packet-based, we have instantaneous hand-off between microcells as there are no complicated cellular-type negotiations for circuits required as all of the information needed to route the packet through the system is included in the header; however, when traveling through the mesh of microcells at a high rate of speed, the mobile unit must acquire new microcells fast enough to ensure continuous connectivity. The system must also know how to route packets to the mobile unit as it drops old cells and acquires new ones, as well as being able to contact a moving mobile unit. This paper discusses the acquisition, registration, and routing protocols that make this possible and reviews performance data of typical mobile users.},
booktitle = {Proceedings of the 7th Annual International Conference on Mobile Computing and Networking},
pages = {322–331},
numpages = {10},
keywords = {wireless routing, wireless protocols, wireless networks, Mobility, MCDN system architecture},
location = {Rome, Italy},
series = {MobiCom '01}
}

@proceedings{10.1145/3712464,
title = {SPCT '24: Proceedings of the 2024 4th International Conference on Signal Processing and Communication Technology},
year = {2024},
isbn = {9798400710636},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/2465808.2465809,
author = {Blanc, Mathieu and Gros, Damien and Briffaut, J\'{e}r\'{e}my and Toinard, Christian},
title = {Mandatory access control with a multi-level reference monitor: PIGA-cluster},
year = {2013},
isbn = {9781450319843},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2465808.2465809},
doi = {10.1145/2465808.2465809},
abstract = {The protection of High Performance Computing architectures is still an open research problem. Generally, current solutions only feature confinement using sandboxing but none address the problematic of information flow control. This is why a better integration of mandatory access control mechanisms is needed in the HPC environment. In this paper, we propose a global architecture to protect a whole cluster. This architecture uses the specific cluster technologies in order not to reduce the operating system performances. The protection of the cluster relies on three levels of protection and the use of two kinds of reference monitors. SELinux is installed on the computing nodes and deals with direct information flows. PIGA, only installed on a specific node, performs advanced flow control and detects advanced threats. We present the various components of our architecture called PIGA-Cluster, then the results of several benchmarks on a computing node that show a low impact on the operating system performances. We also apply various security properties in order to protect the computing nodes against simple and advanced attacks. This paper takes advantage of previous works dealing with workstations or virtualisation technologies and extends the concepts for the HPC environment.},
booktitle = {Proceedings of the First Workshop on Changing Landscapes in HPC Security},
pages = {1–8},
numpages = {8},
keywords = {low-latency network, information flow, access control},
location = {New York, New York, USA},
series = {CLHS '13}
}

@inproceedings{10.1145/2714576.2714586,
author = {Li, Lingjun and Xue, Guoliang and Zhao, Xinxin},
title = {The Power of Whispering: Near Field Assertions via Acoustic Communications},
year = {2015},
isbn = {9781450332453},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2714576.2714586},
doi = {10.1145/2714576.2714586},
abstract = {Asserting whether two devices are in close proximity is very important to many smartphone assisted security systems. For example, the smartphone based two-factor authentication usually requires the smartphone to stay in close proximity to the other device during authentication. However, relay attacks pose a serious threat to existing approaches for proximity assertions. In this paper, we present a novel near field assertion system that restricts the distance between the two devices to the scale of several centimeters. Our system explores acoustic communications and can prevent relay attacks. The generated assertion is a confidential binary sequence known only to the two devices. Our system is fully automated and light-weight, as demonstrated by extensive evaluations on a prototype.},
booktitle = {Proceedings of the 10th ACM Symposium on Information, Computer and Communications Security},
pages = {627–632},
numpages = {6},
keywords = {relay attacks, near field assertions, mobile security},
location = {Singapore, Republic of Singapore},
series = {ASIA CCS '15}
}

@inproceedings{10.1145/3540250.3549108,
author = {Bittner, Paul Maximilian and Tinnes, Christof and Schulthei\ss{}, Alexander and Viegener, S\"{o}ren and Kehrer, Timo and Th\"{u}m, Thomas},
title = {Classifying edits to variability in source code},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549108},
doi = {10.1145/3540250.3549108},
abstract = {For highly configurable software systems, such as the Linux kernel, maintaining and evolving variability information along changes to source code poses a major challenge. While source code itself may be edited, also feature-to-code mappings may be introduced, removed, or changed. In practice, such edits are often conducted ad-hoc and without proper documentation. To support the maintenance and evolution of variability, it is desirable to understand the impact of each edit on the variability. We propose the first complete and unambiguous classification of edits to variability in source code by means of a catalog of edit classes. This catalog is based on a scheme that can be used to build classifications that are complete and unambiguous by construction. To this end, we introduce a complete and sound model for edits to variability. In about 21.5ms per commit, we validate the correctness and suitability of our classification by classifying each edit in 1.7 million commits in the change histories of 44 open-source software systems automatically. We are able to classify all edits with syntactically correct feature-to-code mappings and find that all our edit classes occur in practice.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {196–208},
numpages = {13},
keywords = {software variability, software product lines, software evolution, mining version histories, feature traceability},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3297280.3297512,
author = {Markiegi, Urtzi and Arrieta, Aitor and Etxeberria, Leire and Sagardui, Goiuria},
title = {Test case selection using structural coverage in software product lines for time-budget constrained scenarios},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297512},
doi = {10.1145/3297280.3297512},
abstract = {Testing product lines is a challenging activity due to the large number of products to be tested. Many approaches focus on reducing the time for testing a product line by reducing the number of products to be tested, by employing, for instance, combinatorial approaches. However, even if the number of derived products by a combinatorial approach is limited, testing can still be time consuming. In this paper, we propose three different test case selection methods that consider a given time budget to test product lines in an efficient manner using structural coverage information. We analyze the three methods with three white-box coverage criteria (i.e., Decision Coverage, Condition Coverage and Modified Condition/Decision Coverage). We evaluate the different approaches with a case study from the automotive domain and mutation testing. The results suggest that considering coverage information at the domain engineering level helps on detecting more faults, particularly when time budgets are low.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {2362–2371},
numpages = {10},
keywords = {product line testing, structural coverage, test case selection},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/1858996.1859009,
author = {Vierhauser, Michael and Gr\"{u}nbacher, Paul and Egyed, Alexander and Rabiser, Rick and Heider, Wolfgang},
title = {Flexible and scalable consistency checking on product line variability models},
year = {2010},
isbn = {9781450301169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858996.1859009},
doi = {10.1145/1858996.1859009},
abstract = {The complexity of product line variability models makes it hard to maintain their consistency over time regardless of the modeling approach used. Engineers thus need support for detecting and resolving inconsistencies. We describe experiences of applying a tool-supported approach for incremental consistency checking on variability models. Our approach significantly improves the overall performance and scalability compared to batch-oriented techniques and allows providing immediate feedback to modelers. It is extensible as new consistency constraints can easily be added. Furthermore, the approach is flexible as it is not limited to variability models and it also checks the consistency of the models with the underlying code base of the product line. We report the results of a thorough evaluation based on real-world product line models and discuss lessons learned.},
booktitle = {Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering},
pages = {63–72},
numpages = {10},
keywords = {variability models, software product lines, performance, model consistency, memory consumption, lessons learned, incremental consistency checking},
location = {Antwerp, Belgium},
series = {ASE '10}
}

@proceedings{10.1145/3629479,
title = {SBQS '23: Proceedings of the XXII Brazilian Symposium on Software Quality},
year = {2023},
isbn = {9798400707865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bras\'{\i}lia, Brazil}
}

@article{10.1145/3729270,
author = {Fani, Davoud and Beheshti-Shirazi, Aliasghar and Ghanbari, Mohammad and Rezaei, Esmatollah},
title = {On Temporal Smoothness of Video Reconstruction Quality in the DCVS via Non-Uniform Sampling},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1551-6857},
url = {https://doi.org/10.1145/3729270},
doi = {10.1145/3729270},
abstract = {To the distributed video coding approach, which focuses on fully or partially shifting the computational complexity from video encoder to the decoder, the simplicity and highly compact sampling in emerging compressive sensing appear to be very efficient tools. So, the distributed compressive video sensing (DCVS) has attracted much attentions in the video coding community by applying constant high measurement rate (MR) for key frames sampling and constant low MR for non-key frames sampling. According to use of constant and different MRs for the key and non-key frames sampling, severe and undesirable fluctuations in quality of reconstructed video frames is a common unresolved shortcoming in the DCVS, which negatively affects the users’ visual experience. To suppress sharp and undesirable quality fluctuations, group of picture (GOP)-level non-uniform MR allocation models are proposed in this paper for the key and non-key frames at the encoder of the DCVS. This enhances visual quality without incurring noticeable computational cost to the encoder. A new multi-step reconstruction scheme is also proposed at the decoder exploiting spatial-temporal information in the reconstruction process with a tolerable computational complexity and remarkable reconstruction performance. It compensates for quality degradations, which may be caused by non-uniform MR allocation, to successive GOPs to reach high average quality and temporal smoothness of quality at the same time. Extensive experiments on different video sequences show that not only desirable high average reconstruction quality is maintained, but severe and undesirable quality fluctuations are also well suppressed. Hence, the users’ perceived quality is highly promoted, while the compression ratio does not exceed a certain target by restricting average MR to reach target MR in the long-term.},
note = {Just Accepted},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = apr,
keywords = {distributed compressive video sensing, non-uniform sampling, perceived quality, temporal smoothness of quality, visual quality}
}

@inproceedings{10.1111/cgf.15172,
author = {Schreiner, P. and Netterstr\o{}m, R. and Yin, H. and Darkner, S. and Erleben, K.},
title = {ADAPT: AI-Driven Artefact Purging Technique for IMU Based Motion Capture},
year = {2024},
publisher = {Eurographics Association},
address = {Goslar, DEU},
url = {https://doi.org/10.1111/cgf.15172},
doi = {10.1111/cgf.15172},
abstract = {While IMU based motion capture offers a cost-effective alternative to premium camera-based systems, it often falls short in matching the latter's realism. Common distortions, such as self-penetrating body parts, foot skating, and floating, limit the usability of these systems, particularly for high-end users. To address this, we employed reinforcement learning to train an AI agent that mimics erroneous sample motion. Since our agent operates within a simulated environment, it inherently avoids generating these distortions since it must adhere to the laws of physics. Impressively, the agent manages to mimic the sample motions while preserving their distinctive characteristics. We assessed our method's efficacy across various types of input data, showcasing an ideal blend of artefact-laden IMU-based data with high-grade optical motion capture data. Furthermore, we compared the configuration of observation and action spaces with other implementations, pinpointing the most suitable configuration for our purposes. All our models underwent rigorous evaluation using a spectrum of quantitative metrics complemented by a qualitative review. These evaluations were performed using a benchmark dataset of IMU-based motion data from actors not included in the training data.},
booktitle = {Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation},
pages = {1–13},
numpages = {13},
location = {Montreal, Quebec, Canada},
series = {SCA '24}
}

@article{10.1145/3534592,
author = {Wang, Chao and Lin, Feng and Ba, Zhongjie and Zhang, Fan and Xu, Wenyao and Ren, Kui},
title = {Wavesdropper: Through-wall Word Detection of Human Speech via Commercial mmWave Devices},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
url = {https://doi.org/10.1145/3534592},
doi = {10.1145/3534592},
abstract = {Most existing eavesdropping attacks leverage propagating sound waves for speech retrieval. However, soundproof materials are widely deployed in speech-sensitive scenes (e.g., a meeting room). In this paper, we reveal that human speech protected by an isolated room can be compromised by portable and commercial off-the-shelf mmWave devices. To achieve this goal, we develop Wavesdropper, a word detection system that utilizes a mmWave probe to sense the targeted speaker's throat vibration and recover speech contents in the obstructed condition. We proposed a CEEMD-based method to suppress dynamic clutters (e.g., human movements) in the room and a wavelet-based processing method to extract the delicate vocal vibration information from the hybrid signals. To recover speech contents from mmWave signals related to the vocal vibration, we designed a neural network to infer the speech contents. Moreover, we explored word detection on a conversation with multiple (two) probes and reveal that the adversary can detect words on multiple people simultaneously with only one mmWave device. We performed extensive experiments to evaluate the system performance with over 60,000 pronunciations. The experimental results indicate that Wavesdropper can achieve 91.3% accuracy for 57-word recognition on 23 volunteers.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jul,
articleno = {77},
numpages = {26},
keywords = {mmWave sensing, through walls, word detection}
}

@inproceedings{10.1145/2658761.2658767,
author = {Ruprecht, Andreas and Heinloth, Bernhard and Lohmann, Daniel},
title = {Automatic feature selection in large-scale system-software product lines},
year = {2014},
isbn = {9781450331616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658761.2658767},
doi = {10.1145/2658761.2658767},
abstract = {System software can typically be configured at compile time via a comfortable feature-based interface to tailor its functionality towards a specific use case. However, with the growing number of features, this tailoring process becomes increasingly difficult: As a prominent example, the Linux kernel in v3.14 provides nearly 14 000 configuration options to choose from. Even developers of embedded systems refrain from trying to build a minimized distinctive kernel configuration for their device – and thereby waste memory and money for unneeded functionality. In this paper, we present an approach for the automatic use-case specific tailoring of system software for special-purpose embedded systems. We evaluate the effectiveness of our approach on the example of Linux by generating tailored kernels for well-known applications of the Rasperry Pi and a Google Nexus 4 smartphone. Compared to the original configurations, our approach leads to memory savings of 15–70 percent and requires only very little manual intervention.},
booktitle = {Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences},
pages = {39–48},
numpages = {10},
keywords = {Software Tailoring, Software Product Lines, Linux, Feature Selection},
location = {V\"{a}ster\r{a}s, Sweden},
series = {GPCE 2014}
}

@inproceedings{10.1145/3485730.3485945,
author = {Liu, Tiantian and Gao, Ming and Lin, Feng and Wang, Chao and Ba, Zhongjie and Han, Jinsong and Xu, Wenyao and Ren, Kui},
title = {Wavoice: A Noise-resistant Multi-modal Speech Recognition System Fusing mmWave and Audio Signals},
year = {2021},
isbn = {9781450390972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485730.3485945},
doi = {10.1145/3485730.3485945},
abstract = {With the advance in automatic speech recognition, voice user interface has gained popularity recently. Since the COVID-19 pandemic, VUI is increasingly preferred in online communication due to its non-contact. Additionally, various ambient noise impedes the public applications of voice user interfaces due to the requirement of audio-only speech recognition methods for a high signal-to-noise ratio. In this paper, we present Wavoice, the first noise-resistant multi-modal speech recognition system that fuses two distinct voice sensing modalities, i.e., millimeter-wave (mmWave) signals and audio signals from a microphone, together. One key contribution is that we model the inherent correlation between mmWave and audio signals. Based on it, Wavoice facilitates the real-time noise-resistant voice activity detection and user targeting from multiple speakers. Furthermore, we elaborate on two novel modules into the neural attention mechanism for multi-modal signals fusion, and result in accurate speech recognition. Extensive experiments verify Wavoice's effectiveness under various conditions with the character recognition error rate below 1% in a range of 7 meters. Wavoice outperforms existing audio-only speech recognition methods with lower character error rate and word error rate. The evaluation in complex scenes validates the robustness of Wavoice.},
booktitle = {Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems},
pages = {97–110},
numpages = {14},
keywords = {voice user interface, multimodal fusion, mmWave sensing, Speech recognition},
location = {Coimbra, Portugal},
series = {SenSys '21}
}

@inproceedings{10.1145/3650212.3680339,
author = {Huang, Kaifeng and Xia, Yingfeng and Chen, Bihuan and He, Siyang and Zeng, Huazheng and Zhou, Zhuotong and Guo, Jin and Peng, Xin},
title = {Your “Notice” Is Missing: Detecting and Fixing Violations of Modification Terms in Open Source Licenses during Forking},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680339},
doi = {10.1145/3650212.3680339},
abstract = {Open source software brings benefit to the software community but also introduces legal risks caused by license violations, which result in serious consequences such as lawsuits and financial losses. To mitigate legal risks, some approaches have been proposed to identify licenses, detect license incompatibilities and inconsistencies, and recommend licenses. As far as we know, however, there is no prior work to understand modification terms in open source licenses or to detect and fix violations of modification terms.
 
 
 
 
 
 
 

 
 
 
 
 
 
 
To bridge this gap, we first empirically characterize modification terms in 48 open source licenses. These licenses all require certain forms of “notice” to describe the modifications made to the original work. Inspired by our study, we then design LiVo to automatically detect and fix violations of modification terms in open source licenses during forking. Our evaluation has shown the effectiveness and efficiency of LiVo. 18 pull requests for fixing modification term violations have received positive responses. 8 have been merged.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1022–1034},
numpages = {13},
keywords = {license violation, open source software, software license, software modification},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3674658.3674696,
author = {He, Yuan and Du, Qiang and Bai, Yunfeng and Tian, Bowen and Ke, Li},
title = {A Study of Bioelectrical Impedance-Based Activation Detection Method for Functional Brain Regions of Working Memory},
year = {2024},
isbn = {9798400717666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674658.3674696},
doi = {10.1145/3674658.3674696},
abstract = {Working memory is part of the cognitive system, and exploring the neural activation changes in its related brain functional areas can help the study of the cognitive processing process of working memory and its mechanisms. We used bioelectrical impedance technology to study the activation of working memory-related brain functional areas and the activation patterns of brain functional areas. The cerebral blood flow signals of frontal and parietal brain regions were collected by visual n-back experiment, and the systolic wave height values in the cerebral blood flow map were extracted as the characteristic parameters, and the difference between the systolic wave heights in the task state and the resting state was used to characterize the activation degree of each brain region, and to judge the differences in the activation degree of the two brain functional regions under different working memory loads. It was found that the activation levels of frontal and parietal brain regions were different under the three memory loads; moreover, the activation levels of bilateral frontal brain regions continued to increase with the increase of working memory load, whereas the activation levels of bilateral parietal brain regions tended to increase and then decrease with the increase of working memory load. The above results indicate that the bioelectrical impedance technique can accurately and intuitively reflect the blood flow response and activation degree of working memory brain functional areas, which further promotes the research of bioelectrical impedance technique on the activation of working memory brain functional areas.},
booktitle = {Proceedings of the 2024 16th International Conference on Bioinformatics and Biomedical Technology},
pages = {244–251},
numpages = {8},
keywords = {Bioelectrical impedance, Working memory, Functional brain area, Difference of activation degree},
location = {
},
series = {ICBBT '24}
}

@inproceedings{10.1109/ICSE43902.2021.00142,
author = {Nguyen, KimHao and Nguyen, ThanhVu},
title = {GenTree: Using Decision Trees to Learn Interactions for Configurable Software},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00142},
doi = {10.1109/ICSE43902.2021.00142},
abstract = {Modern software systems are increasingly designed to be highly configurable, which increases flexibility but can make programs harder to develop, test, and analyze, e.g., how configuration options are set to reach certain locations, what characterizes the configuration space of an interesting or buggy program behavior? We introduce GenTree, a new dynamic analysis that automatically learns a program's interactions---logical formulae that describe how configuration option settings map to code coverage. GenTree uses an iterative refinement approach that runs the program under a small sample of configurations to obtain coverage data; uses a custom classifying algorithm on these data to build decision trees representing interaction candidates; and then analyzes the trees to generate new configurations to further refine the trees and interactions in the next iteration. Our experiments on 17 configurable systems spanning 4 languages show that GenTree efficiently finds precise interactions using a tiny fraction of the configuration space.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1598–1609},
numpages = {12},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3571788.3571798,
author = {Hoff, Adrian and Seidl, Christoph and Lanza, Michele},
title = {Uniquifying Architecture Visualization through Variable 3D Model Generation},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571788.3571798},
doi = {10.1145/3571788.3571798},
abstract = {Software visualization facilitates the interactive exploration of large-scale code bases, e.g., to rediscover the architecture of a legacy system. Visualizations of software structure suffer from repetitive patterns that complicate distinguishing different subsystems and recognizing previously visited parts of an architecture. We leverage variability-modeling techniques to "uniquify" visualizations of subsystems via custom-tailored 3D models of recognizable landmarks: For each subsystem, we derive a descriptor and translate it to a (random but deterministic) configuration of a feature model of variable 3D geometry to support large numbers of different 3D models while capturing the design language of a particular type of landmark. We devised a hybrid variant derivation mechanism using a slots-and-hooks composition system for 3D geometry as well as adjusting visual characteristics, e.g., material. We demonstrate our method by creating various different trophies as landmarks for the visualization of a software system.},
booktitle = {Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {77–81},
numpages = {5},
keywords = {Variability Modeling, Software Visualization, 3D Model Generation},
location = {Odense, Denmark},
series = {VaMoS '23}
}

@inproceedings{10.1145/2961111.2962639,
author = {Knauss, Eric and Pelliccione, Patrizio and Heldal, Rogardt and \r{A}gren, Magnus and Hellman, Sofia and Maniette, Daniel},
title = {Continuous Integration Beyond the Team: A Tooling Perspective on Challenges in the Automotive Industry},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962639},
doi = {10.1145/2961111.2962639},
abstract = {The practice of Continuous Integration (CI) has a big impact on how software is developed today. Shortening integration and feedback cycles promises to increase software quality, feature throughput, and customer satisfaction. Thus, it is not a surprise that companies try to embrace CI in domains where it is rather difficult to implement.In this paper we present our findings from two rounds of interviews with a car manufacturer on the use of tools in system engineering and how these tools would support wider adoption of CL Our findings suggest a complex tool landscape with immense requirements that are not easily fulfilled by existing tools; this holds also for tools that well support CI in other domains. From this notion, we further explore what makes the automotive domain challenging when it comes to CI (namely complexity of system and value chain). We hope that our findings will help address such challenges.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {43},
numpages = {6},
keywords = {continuous integration, automotive systems engineering},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@inproceedings{10.1145/2517208.2517228,
author = {Ofenbeck, Georg and Rompf, Tiark and Stojanov, Alen and Odersky, Martin and P\"{u}schel, Markus},
title = {Spiral in scala: towards the systematic construction of generators for performance libraries},
year = {2013},
isbn = {9781450323734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517208.2517228},
doi = {10.1145/2517208.2517228},
abstract = {Program generators for high performance libraries are an appealing solution to the recurring problem of porting and optimizing code with every new processor generation, but only few such generators exist to date. This is due to not only the difficulty of the design, but also of the actual implementation, which often results in an ad-hoc collection of standalone programs and scripts that are hard to extend, maintain, or reuse. In this paper we ask whether and which programming language concepts and features are needed to enable a more systematic construction of such generators. The systematic approach we advocate extrapolates from existing generators: a) describing the problem and algorithmic knowledge using one, or several, domain-specific languages (DSLs), b) expressing optimizations and choices as rewrite rules on DSL programs, c) designing data structures that can be configured to control the type of code that is generated and the data representation used, and d) using autotuning to select the best-performing alternative. As a case study, we implement a small, but representative subset of Spiral in Scala using the Lightweight Modular Staging (LMS) framework. The first main contribution of this paper is the realization of c) using type classes to abstract over staging decisions, i.e. which pieces of a computation are performed immediately and for which pieces code is generated. Specifically, we abstract over different complex data representations jointly with different code representations including generating loops versus unrolled code with scalar replacement - a crucial and usually tedious performance transformation. The second main contribution is to provide full support for a) and d) within the LMS framework: we extend LMS to support translation between different DSLs and autotuning through search.},
booktitle = {Proceedings of the 12th International Conference on Generative Programming: Concepts &amp; Experiences},
pages = {125–134},
numpages = {10},
keywords = {synthesis, selective precomputation, scalar replacement, data representation, abstraction over staging},
location = {Indianapolis, Indiana, USA},
series = {GPCE '13}
}

@inproceedings{10.1145/1739230.1739240,
author = {Farias, Kleinner and Garcia, Alessandro and Whittle, Jon},
title = {Assessing the impact of aspects on model composition effort},
year = {2010},
isbn = {9781605589589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1739230.1739240},
doi = {10.1145/1739230.1739240},
abstract = {Model composition is a common operation used in many software development activities---for example, reconciling models developed in parallel by different development teams, or merging models of new features with existing model artifacts. Unfortunately, both commercial and academic model composition tools suffer from the composition conflict problem. That is, models to-be-composed may conflict with each other and these conflicts must be resolved. In practice, detecting and resolving conflicts is a highly-intensive manual activity. In this paper, we investigate whether aspect-orientation reduces conflict resolution effort as improved modularization may better localize conflicts. The main goal of the paper is to conduct an exploratory study to analyze the impact of aspects on conflict resolution. In particular, model compositions are used to express the evolution of architectural models along six releases of a software product line. Well-known composition algorithms, such as override, merge and union, are applied and compared on both AO and non-AO models in terms of their conflict rate and effort to solve the identified conflicts. Our findings identify specific scenarios where aspect-orientation properties, such as obliviousness and quantification, result in a lower (or higher) composition effort.},
booktitle = {Proceedings of the 9th International Conference on Aspect-Oriented Software Development},
pages = {73–84},
numpages = {12},
keywords = {software product lines, software metrics, software architecture, model composition, empirical studies},
location = {Rennes and Saint-Malo, France},
series = {AOSD '10}
}

@article{10.1145/3715002,
author = {Blasco, Daniel and Iglesias, Antonio and Echeverr\'{\i}a, Jorge and P\'{e}rez, Francisca and Cetina, Carlos},
title = {Introducing Phylogenetics in Search-based Software Engineering: Phylogenetics-aware SBSE},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715002},
doi = {10.1145/3715002},
abstract = {Phylogenetics studies the relationships, in terms of biological history and kinship, of a set of taxa (e.g., species). We argue that in Search-based Software Engineering (SBSE), the individuals of an evolutionary computation-driven population could be considered as taxa for which the leverage of Phylogenetic Inference might be beneficial. In this work, we present our Phylogenetics-aware SBSE approach. Our approach introduces a novel Phylogenetic Operation to promote results which are sufficiently aligned (in terms of lineage) with a certain reference given by the domain expert. Our approach is evaluated in two heterogeneous industrial case studies: Procedural Content Generation from Game Software Engineering, and Feature Location from Software Maintenance. The results are analyzed using quality-of-the-solution and acceptance-by-developers measurements. We performed a statistical analysis to determine whether the impact on the results is significant compared to baselines that do not leverage Phylogenetics. The results show that our approach significantly outperforms two baselines in both case studies. Furthermore, two focus groups confirmed the acceptance of our approach and stressed that solution acceptance may make the difference in industrial environments. Our work has the potential to motivate a new breed of research work on Phylogenetics awareness to produce better results in Software Engineering.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Phylogenetics, Search-based Software Engineering, Evolutionary Computation, Game Software Engineering, Procedural Content Generation, Software Maintenance, Feature Location, Model-Driven Engineering}
}

@inproceedings{10.1145/3302333.3302339,
author = {Meixner, Kristof and Winkler, Dietmar and Biffl, Stefan},
title = {Towards Combined Process &amp; Tool Variability Management in Software Testing},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302339},
doi = {10.1145/3302333.3302339},
abstract = {Context. Modern software engineering approaches that rely on continuous and automated testing, like Agile Software Engineering and the late DevOps movement, require integrated and fully functional testing tool chain environment, to efficiently identify defects in software artifacts. Such an environment includes the implementation of established testing processes that are utilized by the development teams. However, in practice, different testing tool chains and processes are implemented depending on particular project requirements such as programming language, selected testing tool, or system architecture. This variety of required technologies and processes frequently results in an environment of isolated test automation solutions. Thus, there is a need for a managed and controllable set of testing tool chain variants that consider structured methods to integrate variability. Goal. In this paper, we show ongoing work, as part of a flexible Test Automation Framework (TAF), with focus on requirements for the variability of testing tool chains, established testing processes, and candidate solution approaches. Method. We build on best practices from software and systems testing and variability management to implement variability in the TAF. Results. First results showed that several Test Automation (TA) solutions exist, which support variability in a limited manner and, therefore, increase the need for modeling variability in a flexible TAF. Conclusion. In the context of Software Test Automation, a combination of Variability Modeling (VM) methods for testing architectures, business processes, and a definition of common interface definitions is promising towards a TAF that enables a flexible tool and process integration.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {5},
numpages = {6},
keywords = {Variability Modeling, Testing Tool Chain, Test automation, Test Architecture, Software Testing, Process Variability},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/3387940.3392089,
author = {Ahlgren, John and Berezin, Maria Eugenia and Bojarczuk, Kinga and Dulskyte, Elena and Dvortsova, Inna and George, Johann and Gucevska, Natalija and Harman, Mark and L\"{a}mmel, Ralf and Meijer, Erik and Sapora, Silvia and Spahr-Summers, Justin},
title = {WES: Agent-based User Interaction Simulation on Real Infrastructure},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392089},
doi = {10.1145/3387940.3392089},
abstract = {We introduce the Web-Enabled Simulation (WES) research agenda, and describe FACEBOOK's WW system. We describe the application of WW to reliability, integrity and privacy at FACEBOOK1, where it is used to simulate social media interactions on an infrastructure consisting of hundreds of millions of lines of code. The WES agenda draws on research from many areas of study, including Search Based Software Engineering, Machine Learning, Programming Languages, Multi Agent Systems, Graph Theory, Game AI, and AI Assisted Game Play. We conclude with a set of open problems and research challenges to motivate wider investigation.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {276–284},
numpages = {9},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/1229375.1229381,
author = {Groher, Iris and Voelter, Markus},
title = {XWeave: models and aspects in concert},
year = {2007},
isbn = {9781595936585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1229375.1229381},
doi = {10.1145/1229375.1229381},
abstract = {Model-driven software development improves the way software is developed by capturing key features of the system in models which are developed and refined as the system is created. During the system's lifecycle models are combined and transformed between different levels of abstraction and viewpoints. Aspect-oriented techniques improve software development by providing modularization constructs for the encapsulation of crosscutting concerns. Existing research has already investigated many ways of combining the two paradigms. This paper contributes by presenting XWeave, a model weaver that supports weaving of both models and meta models. XWeave supports the composition of different architectural viewpoints and eases model evolution. Furthermore, the tool plays an important role in software product line engineering, as variable parts of architectural models can be woven according to some product configuration. The concepts are illustrated with an example of a home automation system.},
booktitle = {Proceedings of the 10th International Workshop on Aspect-Oriented Modeling},
pages = {35–40},
numpages = {6},
keywords = {model-driven software development, model weaving, aspect-oriented software development},
location = {Vancouver, Canada},
series = {AOM '07}
}

@proceedings{10.1145/3607199,
title = {RAID '23: Proceedings of the 26th International Symposium on Research in Attacks, Intrusions and Defenses},
year = {2023},
isbn = {9798400707650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hong Kong, China}
}

@inproceedings{10.1145/3674912.3674940,
author = {Stroud, Stephen and Jones, Karl and Edwards, Gerard and Robinson, Colin and Chandler-Crnigoj, Sebastian and Ellis, David},
title = {Enhancing Environmental Sound Recognition in Digital Simulations: A Novel Approach to Beamforming and Signal Identification},
year = {2024},
isbn = {9798400716843},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674912.3674940},
doi = {10.1145/3674912.3674940},
abstract = {This paper advances the field of environmental sound recognition, presenting a refined approach to beamforming and noise identification through digital simulations of realistic environments at Liverpool John Moores University. Amidst the growing demand for precise audio detection techniques in cluttered acoustic environments, our research introduces a method for identifying and highlighting specific sound signals. We use an advanced time-delay beamforming algorithm to achieve strategic audio zooming, addressing topical issues in urban surveillance and forensic sound examination for potential analysis in criminal cases.Our methodology is rooted in deploying a carefully configured array of virtual omnidirectional microphones, crucial for collecting real-world audio signals. Our technique's core lies in applying our advanced algorithm to the captured sound data, thoroughly assessing our system's capability to identify and isolate targeted sound sources.Our investigation further measures the robustness of our system to microphone failure, which continues to function even when microphones completely fail, highlighting its reliability, even when operating under compromised conditions. Through simulations that capture actual acoustic environments, our experiments reveal the algorithm's proficiency in coping with both sound reflections and reverberations, critical elements in authentically reproducing real-world scenarios.Finally, this study explores the extended applicability of our research findings, considering their potential impact across various sectors, including environmental surveillance, animal conservation, broadcasting, and sound engineering. Our work offers a forward-thinking strategy for environmental sound recognition within a digital simulation framework. It paves the way for practical applications that stand to gain from improved sound separation and analysis techniques. Our contributions engage with the broader dialogue on the evolution of surveillance technology, providing valuable perspectives that could influence the future of audio research.},
booktitle = {Proceedings of the International Conference on Computer Systems and Technologies 2024},
pages = {167–172},
numpages = {6},
location = {Ruse, Bulgaria},
series = {CompSysTech '24}
}

@inproceedings{10.1145/3442391.3442409,
author = {G\"{o}ttmann, Hendrik and Bacher, Isabelle and Gottwald, Nicolas and Lochau, Malte},
title = {Static Analysis Techniques for Efficient Consistency Checking of Real-Time-Aware DSPL Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442409},
doi = {10.1145/3442391.3442409},
abstract = {Dynamic Software Product Lines (DSPL) have recently gained momentum as integrated engineering methodology for (self-)adaptive software. DSPL enhance statically configurable software by enabling run-time reconfiguration to facilitate continuous adaptations to changing environmental contexts. In a previous work, we presented a model-based methodology for specifying and automatically analyzing real-time constraints of reconfiguration decisions in a feature-oriented and compositional way. Internally, we translate real-time-aware DSPL specifications into timed automata serving as input for off-the-shelf model checkers like Uppaal for automatically checking semantic consistency properties. However, due to the very high computational complexity of model checking timed automata, those consistency checks suffer from scalability problems thus obstructing practical applications of the proposed approach. In this paper, we tackle this issue by investigating various kinds of static-analysis techniques that (1) aim to avoid expensive model checker calls by statically detecting certain classes of inconsistencies beforehand and otherwise (2) perform model reduction by detecting and merging equivalence states prior to model checker calls. The results of our experimental evaluation show very promising performance improvements achievable by those techniques, especially by the model-reduction approach.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {17},
numpages = {9},
keywords = {Timed Automata, Reconfiguration Decisions, Dynamic Software Product Lines},
location = {Krems, Austria},
series = {VaMoS '21}
}

@proceedings{10.1145/3559712,
title = {SBCARS '22: Proceedings of the 16th Brazilian Symposium on Software Components, Architectures, and Reuse},
year = {2022},
isbn = {9781450397452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Uberlandia, Brazil}
}

@article{10.1145/3617946.3617958,
author = {Arcaini, Paolo and Miranskyy, Andriy},
title = {Report of the Fourth International Workshop on Quantum Software Engineering (Q-SE 2023)},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/3617946.3617958},
doi = {10.1145/3617946.3617958},
abstract = {The Fourth International Workshop on Quantum Software Engineering (Q-SE 2023), co-located with the 45th International Conference on Software Engineering (ICSE 2023), was held on May 14, 2023 in a hybrid manner in Melbourne, Australia, and online. This report presents the workshop structure, the keynote speech, and the themes of the presented papers.},
journal = {SIGSOFT Softw. Eng. Notes},
month = oct,
pages = {64–65},
numpages = {2}
}

@inproceedings{10.1145/2031759.2031771,
author = {Dobrica, Liliana and Ovaska, Eila},
title = {Analysis of a cross-domain reference architecture using change scenarios},
year = {2011},
isbn = {9781450306188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2031759.2031771},
doi = {10.1145/2031759.2031771},
abstract = {The content of this paper addresses the issue of how to perform analysis of a cross domain reference architecture. The cross domain reference architecture is designed based on the domains requirements and features modeling. The definition of a cross domain reference architecture is based on well known concepts from software architecture description, service orientation and product line. We apply a method based on change scenarios to analyze variability at the architectural level. In order to handle complexity in analysis we propose categories of change scenarios to be derived from each problem domain and we provide informal guidelines for each step of the analysis method.},
booktitle = {Proceedings of the 5th European Conference on Software Architecture: Companion Volume},
articleno = {10},
numpages = {9},
keywords = {variability, service, scenarios, quality, cross domain reference architecture, analysis methods},
location = {Essen, Germany},
series = {ECSA '11}
}

@inproceedings{10.1145/3495243.3560543,
author = {Wang, Chao and Lin, Feng and Liu, Tiantian and Zheng, Kaidi and Wang, Zhibo and Li, Zhengxiong and Huang, Ming-Chun and Xu, Wenyao and Ren, Kui},
title = {mmEve: eavesdropping on smartphone's earpiece via COTS mmWave device},
year = {2022},
isbn = {9781450391818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495243.3560543},
doi = {10.1145/3495243.3560543},
abstract = {Earpiece mode of smartphones is often used for confidential communication. In this paper, we proposed a remote(&gt;2m) and motion-resilient attack on smartphone earpiece. We developed an end-to-end eavesdropping system mmEve based on a commercial mmWave sensor to recover speech emitted from smartphone earpiece. The rationale of the attack is based on our observation that, soundwaves emitted from the smartphone's earpiece have a strong correlation with reflected mmWaves from the smartphone's rear. However, we find the recovered speech suffers from the sensor's self-noise and smartphone user's motion which limit attack distance to less than 2m, causing limited threats in real world. We modeled the motion interference under mmWave sensing and proposed a motion-resilient solution by optimizing the fitting function on I/Q plane. To achieve a practical attack with reasonable attack distance, we developed a GAN-based denoising scheme to eliminate the noise pattern of the sensor, which boosted the attack range to 6--8m. We evaluated mmEve with extensive experiments and find 23 different models of smartphones manufactured by Samsung, Huawei, etc. can be compromised by the proposed attack.},
booktitle = {Proceedings of the 28th Annual International Conference on Mobile Computing And Networking},
pages = {338–351},
numpages = {14},
keywords = {smartphone, mmWave sensing, eavesdropping, earpiece},
location = {Sydney, NSW, Australia},
series = {MobiCom '22}
}

@inproceedings{10.1109/ICSE-SEET52601.2021.00022,
author = {Azanza, Maider and Irastorza, Arantza and Medeiros, Raul and D\'{\i}az, Oscar},
title = {Onboarding in software product lines: concept maps as welcome guides},
year = {2021},
isbn = {9780738133201},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET52601.2021.00022},
doi = {10.1109/ICSE-SEET52601.2021.00022},
abstract = {With a volatile labour and technological market, onboarding is becoming increasingly important. The process of incorporating a new developer, a.k.a. the newcomer, into a software development team is reckoned to be lengthy, frustrating and expensive. Newcomers face personal, interpersonal, process and technical barriers during their incorporation, which, in turn, affects the overall productivity of the whole team. This problem exacerbates for Software Product Lines (SPLs), where their size and variability combine to make onboarding even more challenging, even more so for developers that are transferred from the Application Engineering team into the Domain Engineering team, who will be our target newcomers. This work presents concept maps on the role of sensemaking scaffolds to help to introduce these newcomers into the SPL domain. Concept maps, used as knowledge visualisation tools, have been proven to be helpful for meaningful learning. Our main insight is to capture concepts of the SPL domain and their interrelationships in a concept map, and then, present them incrementally, helping newcomers grasp the SPL and aiding them in exploring it in a guided manner while avoiding information overload. This work's contributions are four-fold. First, concept maps are proposed as a representation to introduce newcomers into the SPL domain. Second, concept maps are presented as the means for a guided exploration of the SPL core assets. Third, a feature-driven concept map construction process is introduced. Last, the usefulness of concept maps as guides for SPL onboarding is tested through a formative evaluation.Link to the online demo: https://rebrand.ly/wacline-cmap},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Joint Track on Software Engineering Education and Training},
pages = {122–133},
numpages = {12},
location = {Virtual Event, Spain},
series = {ICSE-JSEET '21}
}

@inproceedings{10.1145/1967486.1967572,
author = {Souer, Jurriaan and Joor, Dirk-Jan},
title = {An approach to identify commonalities in web application engineering for a web content management system},
year = {2010},
isbn = {9781450304214},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1967486.1967572},
doi = {10.1145/1967486.1967572},
abstract = {The process of Web applications engineering can be complex and time consuming. We argue that Web engineering based on a standardized platform with reusable components is a logical next step in the evolution of Web application development. One popular platform to create Web applications is called a Web Content Management Systems (WCMS) which allows organizations to develop Web applications in a time and resource efficient way. This paper presents a method to identify software commonalities in WCMS-based Web applications to improve the software product for future implementations based on feature modeling and e-business models. The resulting method provides insight in relevant e-business models and their corresponding functionalities. Moreover, this paper shows how these commonalities can be identified and how that could influence the software product line. The approach has been applied in a practical case study of a series of Web application engineering projects within the publishing vertical market. We have validated the approach with experts within the case study company and found that the approach is useful in aiding requirements engineers in the Web application engineering process and product managers in the software product management process.},
booktitle = {Proceedings of the 12th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {558–565},
numpages = {8},
keywords = {web engineering, web content management system, software product lines},
location = {Paris, France},
series = {iiWAS '10}
}

@inproceedings{10.1145/3510003.3510053,
author = {Xiang, Yi and Huang, Han and Zhou, Yuren and Li, Sizhe and Luo, Chuan and Lin, Qingwei and Li, Miqing and Yang, Xiaowei},
title = {Search-based diverse sampling from real-world software product lines},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510053},
doi = {10.1145/3510003.3510053},
abstract = {Real-world software product lines (SPLs) often encompass enormous valid configurations that are impossible to enumerate. To understand properties of the space formed by all valid configurations, a feasible way is to select a small and valid sample set. Even though a number of sampling strategies have been proposed, they either fail to produce diverse samples with respect to the number of selected features (an important property to characterize behaviors of configurations), or achieve diverse sampling but with limited scalability (the handleable configuration space size is limited to 1013). To resolve this dilemma, we propose a scalable diverse sampling strategy, which uses a distance metric in combination with the novelty search algorithm to produce diverse samples in an incremental way. The distance metric is carefully designed to measure similarities between configurations, and further diversity of a sample set. The novelty search incrementally improves diversity of samples through the search for novel configurations. We evaluate our sampling algorithm on 39 real-world SPLs. It is able to generate the required number of samples for all the SPLs, including those which cannot be counted by sharpSAT, a state-of-the-art model counting solver. Moreover, it performs better than or at least competitively to state-of-the-art samplers regarding diversity of the sample set. Experimental results suggest that only the proposed sampler (among all the tested ones) achieves scalable diverse sampling.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1945–1957},
numpages = {13},
keywords = {distance metric, diverse sampling, novelty search, software product lines},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/2556624.2556635,
author = {Devroey, Xavier and Perrouin, Gilles and Cordy, Maxime and Schobbens, Pierre-Yves and Legay, Axel and Heymans, Patrick},
title = {Towards statistical prioritization for software product lines testing},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556635},
doi = {10.1145/2556624.2556635},
abstract = {Software Product Lines (SPLs) are inherently difficult to test due to the combinatorial explosion of the number of products to consider. To reduce the number of products to test, sampling techniques such as combinatorial interaction testing have been proposed. They usually start from a feature model and apply a coverage criterion (e.g. pairwise feature interaction or dissimilarity) to generate tractable, fault-finding, lists of configurations to be tested. Prioritization can also be used to sort/generate such lists, optimizing coverage criteria or weights assigned to features. However, current sampling/prioritization techniques barely take product behaviour into account. We explore how ideas of statistical testing, based on a usage model (a Markov chain), can be used to extract configurations of interest according to the likelihood of their executions. These executions are gathered in featured transition systems, compact representation of SPL behaviour. We discuss possible scenarios and give a prioritization procedure validated on a web-based learning management software.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {10},
numpages = {7},
keywords = {statistical testing, prioritization, SPL testing},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@inproceedings{10.1145/2335484.2335506,
author = {Hirzel, Martin},
title = {Partition and compose: parallel complex event processing},
year = {2012},
isbn = {9781450313155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2335484.2335506},
doi = {10.1145/2335484.2335506},
abstract = {Complex event processing uses patterns to detect composite events in streams of simple events. Typically, the events are logically partitioned by some key. For instance, the key can be the stock symbol in stock quotes, the author in tweets, the vehicle in transportation, or the patient in health-care. Composite event patterns often become meaningful only after partitioning. For instance, a pattern over stock quotes is typically meaningful over quotes for the same stock symbol. This paper proposes a pattern syntax and translation scheme organized around the notion of partitions. Besides making patterns meaningful, partitioning also benefits performance, since different keys can be processed in parallel. We have implemented partitioned parallel complex event processing as an extension to IBM's System S high-performance streaming platform. Our experiments with several benchmarks from finance and social media demonstrate processing speeds of up to 830,000 events per second, and substantial speedups for expensive patterns parallelized on multi-core machines as well as multi-machine clusters. Partitioning the event stream before detecting composite events makes event processing both more intuitive and parallel.},
booktitle = {Proceedings of the 6th ACM International Conference on Distributed Event-Based Systems},
pages = {191–200},
numpages = {10},
keywords = {stream processing, regular expressions, pattern matching, parallelism, composite events, automata, SPL, CEP},
location = {Berlin, Germany},
series = {DEBS '12}
}

@inproceedings{10.1145/3495243.3560531,
author = {Shi, Cong and Zhang, Tianfang and Li, Zhuohang and Phan, Huy and Zhao, Tianming and Wang, Yan and Liu, Jian and Yuan, Bo and Chen, Yingying},
title = {Audio-domain position-independent backdoor attack via unnoticeable triggers},
year = {2022},
isbn = {9781450391818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495243.3560531},
doi = {10.1145/3495243.3560531},
abstract = {Deep learning models have become key enablers of voice user interfaces. With the growing trend of adopting outsourced training of these models, backdoor attacks, stealthy yet effective training-phase attacks, have gained increasing attention. They inject hidden trigger patterns through training set poisoning and overwrite the model's predictions in the inference phase. Research in backdoor attacks has been focusing on image classification tasks, while there have been few studies in the audio domain. In this work, we explore the severity of audio-domain backdoor attacks and demonstrate their feasibility under practical scenarios of voice user interfaces, where an adversary injects (plays) an unnoticeable audio trigger into live speech to launch the attack. To realize such attacks, we consider jointly optimizing the audio trigger and the target model in the training phase, deriving a position-independent, unnoticeable, and robust audio trigger. We design new data poisoning techniques and penalty-based algorithms that inject the trigger into randomly generated temporal positions in the audio input during training, rendering the trigger resilient to any temporal position variations. We further design an environmental sound mimicking technique to make the trigger resemble unnoticeable situational sounds and simulate played over-the-air distortions to improve the trigger's robustness during the joint optimization process. Extensive experiments on two important applications (i.e., speech command recognition and speaker recognition) demonstrate that our attack can achieve an average success rate of over 99% under both digital and physical attack settings.},
booktitle = {Proceedings of the 28th Annual International Conference on Mobile Computing And Networking},
pages = {583–595},
numpages = {13},
keywords = {position-independent attacks, over-the-air physical attacks, audio-domain backdoor attacks},
location = {Sydney, NSW, Australia},
series = {MobiCom '22}
}

@inproceedings{10.1145/55595.55613,
author = {Irgon, A. E. and Dragoni, A. H. and Huleatt, T. O.},
title = {FAST: A large scale expert system for application and system software performance tuning},
year = {1988},
isbn = {0897912543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/55595.55613},
doi = {10.1145/55595.55613},
booktitle = {Proceedings of the 1988 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems},
pages = {151–156},
numpages = {6},
location = {Santa Fe, New Mexico, USA},
series = {SIGMETRICS '88}
}

@inproceedings{10.5555/381473.381482,
author = {Bosch, Jan},
title = {Software product lines: organizational alternatives},
year = {2001},
isbn = {0769510507},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Software product lines enjoy increasingly wide adoption in the software industry. Most authors focus on the technical and process aspects and assume an organizational model consisting of a domain engineering unit and several application engineering units. In our cooperation with several software development organizations applying software product line principles, we have identified several other organizational models that are employed as well. In this article, we present a number of organizational alternatives, organized around four main models, i.e. development department, business units, domain engineering unit and hierarchical domain engineering units. For each model, its characteristics, applicability and advantages and disadvantages are discussed, as well as an example. Based on an analysis of these models, we present three factors that influence the choice of the organizational model, i.e. product-line assets, the responsibility levels and the type of organizational units.},
booktitle = {Proceedings of the 23rd International Conference on Software Engineering},
pages = {91–100},
numpages = {10},
location = {Toronto, Ontario, Canada},
series = {ICSE '01}
}

@inproceedings{10.1145/3275245.3275261,
author = {Campos, Denivan and Lima, Crescencio and do Carmo Machado, Ivan},
title = {MERCI: A Method to Evaluate Combinatorial Interaction Testing Tools for Software Product Lines},
year = {2018},
isbn = {9781450365659},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3275245.3275261},
doi = {10.1145/3275245.3275261},
abstract = {Testing a system is a routine activity, and it plays an important role in the software quality assurance process. However, testing highly-configurable systems, such as Software Product Lines (SPL), is a rather complex activity, due to the presence of variability in its engineering process, which increases the number of product configurations to test. The underlying idea to make testing feasible in SPL engineering is to select a small but representative subset of products to test, by employing techniques such as combinatorial interaction testing (CIT). This paper presents Method to Evaluate Combinatorial Interaction (MERCI), a novel method to evaluate the adequacy of existing CIT tools for SPL engineering, with respect to three measures: defect detection, test coverage, and test execution length. We carried out an empirical evaluation to compare four CIT tools: ACTS, CATS, PICTMaster and VPTag. The results show that the method may serve as an affordable strategy to evaluate how the CIT tools could behave in an SPL testing scenario.},
booktitle = {Proceedings of the XVII Brazilian Symposium on Software Quality},
pages = {151–159},
numpages = {9},
keywords = {Testing Tools, Software Testing Strategies, Software Product Lines, Combinatorial Interaction Testing},
location = {Curitiba, Brazil},
series = {SBQS '18}
}

@inproceedings{10.1145/3192366.3192420,
author = {Steindorfer, Michael J. and Vinju, Jurgen J.},
title = {To-many or to-one? all-in-one! efficient purely functional multi-maps with type-heterogeneous hash-tries},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192420},
doi = {10.1145/3192366.3192420},
abstract = {An immutable multi-map is a many-to-many map data structure with expected fast insert and lookup operations. This data structure is used for applications processing graphs or many-to-many relations as applied in compilers, runtimes of programming languages, or in static analysis of object-oriented systems. Collection data structures are assumed to carefully balance execution time of operations with memory consumption characteristics and need to scale gracefully from a few elements to multiple gigabytes at least. When processing larger in-memory data sets the overhead of the data structure encoding itself becomes a memory usage bottleneck, dominating the overall performance.  In this paper we propose AXIOM, a novel hash-trie data structure that allows for a highly efficient and type-safe multi-map encoding by distinguishing inlined values of singleton sets from nested sets of multi-mappings. AXIOM strictly generalizes over previous hash-trie data structures by supporting the processing of fine-grained type-heterogeneous content on the implementation level (while API and language support for type-heterogeneity are not scope of this paper). We detail the design and optimizations of AXIOM and further compare it against state-of-the-art immutable maps and multi-maps in Java, Scala and Clojure. We isolate key differences using microbenchmarks and validate the resulting conclusions on a case study in static analysis. AXIOM reduces the key-value storage overhead by 1.87x; with specializing and inlining across collection boundaries it improves by 5.1x.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {283–295},
numpages = {13},
keywords = {persistent data structures, performance, optimization, multi-map, many-to-many relation, hashtable, graph, functional programming, JVM, Data structures},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@inproceedings{10.1145/3627673.3679740,
author = {Zhou, Dongming and Pang, Zhengbin and Li, Wei},
title = {Learning Cross-modal Knowledge Reasoning and Heuristic-prompt for Visual-language Navigation},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3679740},
doi = {10.1145/3627673.3679740},
abstract = {Visual language navigation is an exciting and challenging multi-modal task. Most existing research focuses on the fusion of visual features and semantic space, which ignoring the importance of local highlight features and semantic knowledge alignment in images for agent navigation. Therefore, this paper proposes a novel visual language model combining Knowledge-augmented Reasoning and Soft-Prompt (KRSP) learning. First, we perform fine-grained processing of local regions in the image and to map context image features and text knowledge to the same common sub-space. We focus on regional knowledge to increase the model reasoning ability. Next, soft-prompt learning aligns keywords and sub-visual information in instruction features to solve the path mismatch problem in coarse-grained instructions. We use a large-scale pre-training model CoCoOp to collect highly matched soft action prompts into a unified instruction set. Finally, we propose a general cross-modal feature alignment loss function. The potential semantic correlation between sub-visual information and instruction space is closer through the penalty mechanism of the alignment function. This paper verifies the method effectiveness on the R2R and REVERIE datasets, and the experimental results show that KRSP achieves state-of-the-art performance. Among them, the KRSP of SPL evaluation metric increased by 4.5% in unseen scenarios.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {3453–3462},
numpages = {10},
keywords = {fine-grained alignment, pre-training model, prompt learning, visual-language navigation},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@article{10.1145/3580865,
author = {Boovaraghavan, Sudershan and Chen, Chen and Maravi, Anurag and Czapik, Mike and Zhang, Yang and Harrison, Chris and Agarwal, Yuvraj},
title = {Mites: Design and Deployment of a General-Purpose Sensing Infrastructure for Buildings},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
url = {https://doi.org/10.1145/3580865},
doi = {10.1145/3580865},
abstract = {There is increasing interest in deploying building-scale, general-purpose, and high-fidelity sensing to drive emerging smart building applications. However, the real-world deployment of such systems is challenging due to the lack of system and architectural support. Most existing sensing systems are purpose-built, consisting of hardware that senses a limited set of environmental facets, typically at low fidelity and for short-term deployment. Furthermore, prior systems with high-fidelity sensing and machine learning fail to scale effectively and have fewer primitives, if any, for privacy and security. For these reasons, IoT deployments in buildings are generally short-lived or done as a proof of concept. We present the design of Mites, a scalable end-to-end hardware-software system for supporting and managing distributed general-purpose sensors in buildings. Our design includes robust primitives for privacy and security, essential features for scalable data management, as well as machine learning to support diverse applications in buildings. We deployed our Mites system and 314 Mites devices in Tata Consultancy Services (TCS) Hall at Carnegie Mellon University (CMU), a fully occupied, five-story university building. We present a set of comprehensive evaluations of our system using a series of microbenchmarks and end-to-end evaluations to show how we achieved our stated design goals. We include five proof-of-concept applications to demonstrate the extensibility of the Mites system to support compelling IoT applications. Finally, we discuss the real-world challenges we faced and the lessons we learned over the five-year journey of our stack's iterative design, development, and deployment.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = mar,
articleno = {2},
numpages = {32},
keywords = {Distributed Sensor Network, Real-World Deployment, Sensing and Sensor Technologies}
}

@inproceedings{10.1145/2897053.2897062,
author = {McGee, Ethan T. and McGregor, John D.},
title = {Using dynamic adaptive systems in safety-critical domains},
year = {2016},
isbn = {9781450341875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897053.2897062},
doi = {10.1145/2897053.2897062},
abstract = {The development of safety-critical Cyber-Physical Systems (CPS) is expanding due to the Internet of Things' promise to make high-integrity applications and services part of everyday life. This expansion is seen in the dependencies some connected vehicles have on cloud services that provide guidance and accident avoidance / detection features. Such systems are safety-critical since failure could result in serious injury or death. Due to the severe consequences of failure, fault-tolerance, reliability and dependability should be primary driving qualities in the design and development of these systems. However, the cost of the analysis, evaluation and certification activities needed to ensure that the possibility of failure has been sufficiently mitigated is significantly higher than the cost of developing traditional software.Our group is exploring the addition of dynamic adaptive capabilities to safety-critical systems. We postulate that dynamic adaptivity could provide several enhancements to safety-critical systems. It would allow systems to reason about the environment within which they are sited and about their internal operation enabling decision making that is context-specific and appropriately prioritized. However, the addition of adaptivity with the associated overhead of reasoning is not without drawbacks particularly when hard real-time safety-critical systems are involved. In this brief position paper, we explore some of the questions and concerns that are raised when dynamic adaptive behavior is introduced into safety-critical systems as well as ways that the Architecture Analysis &amp; Design Language (AADL) can be used to model / analyze such systems.},
booktitle = {Proceedings of the 11th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {115–121},
numpages = {7},
keywords = {software product lines, safety critical systems, dynamic software product lines, dynamic adaptive systems},
location = {Austin, Texas},
series = {SEAMS '16}
}

@inproceedings{10.1109/ICSE43902.2021.00076,
author = {Hata, Hideaki and Kula, Raula Gaikovina and Ishio, Takashi and Treude, Christoph},
title = {Same File, Different Changes: The Potential of Meta-Maintenance on GitHub},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00076},
doi = {10.1109/ICSE43902.2021.00076},
abstract = {Online collaboration platforms such as GitHub have provided software developers with the ability to easily reuse and share code between repositories. With clone-and-own and forking becoming prevalent, maintaining these shared files is important, especially for keeping the most up-to-date version of reused code. Different to related work, we propose the concept of meta-maintenance---i.e., tracking how the same files evolve in different repositories with the aim to provide useful maintenance opportunities to those files. We conduct an exploratory study by analyzing repositories from seven different programming languages to explore the potential of meta-maintenance. Our results indicate that a majority of active repositories on GitHub contains at least one file which is also present in another repository, and that a significant minority of these files are maintained differently in the different repositories which contain them. We manually analyzed a representative sample of shared files and their variants to understand which changes might be useful for meta-maintenance. Our findings support the potential of meta-maintenance and open up avenues for future work to capitalize on this potential.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {773–784},
numpages = {12},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/2701319.2701329,
author = {Krieter, Sebastian and Schr\"{o}ter, Reimar and Fenske, Wolfram and Saake, Gunter},
title = {Use-Case-Specific Source-Code Documentation for Feature-Oriented Programming},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701329},
doi = {10.1145/2701319.2701329},
abstract = {Source-code documentation is essential to efficiently develop and maintain large software products. Documentation is equally important for software product lines (SPLs), which represent a set of different products with a common code base. Unfortunately, proper support for documenting the source code of an SPL is currently lacking, because source code variability is not considered by current documentation tools. We introduce a method to provide source-code documentation for feature-oriented programming and aim to support developers who implement, maintain, and use SPLs. We identify multiple use cases for developers working with SPLs and propose four different documentation types (meta, product, feature, and context) that fulfill the information requirements of these use cases. Furthermore, we design an algorithm that enables developers to create tailor-made documentation for each use case. Our method is based on the documentation tool Javadoc and allows developers to easily write documentation comments that contain little overhead or redundancy. To demonstrate the efficiency of our method, we present a prototypical implementation and evaluate our method with regard to documentation effort for the SPL developers by documenting two small SPLs.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {27–34},
numpages = {8},
keywords = {Source Code Documentation, Software Product Lines, Feature-Oriented Programming, API Documentation},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@proceedings{10.1145/3643667,
title = {Q-SE 2024: Proceedings of the 5th ACM/IEEE International Workshop on Quantum Software Engineering},
year = {2024},
isbn = {9798400705700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 5th International Workshop on Quantum Software Engineering (Q-SE 2024), co-located with ICSE 2024, provides a platform for researchers and practitioners to discuss challenges in developing quantum software in high-level quantum languages, novel solutions to build correct methods for testing quantum programs, executing quantum software, developing best practices, and creating a research roadmap of quantum software engineering.},
location = {Lisbon, Portugal}
}

@article{10.1145/3514233,
author = {Chen, Tao and Li, Miqing},
title = {The Weights Can Be Harmful: Pareto Search versus Weighted Search in Multi-objective Search-based Software Engineering},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3514233},
doi = {10.1145/3514233},
abstract = {In presence of multiple objectives to be optimized in Search-Based Software Engineering (SBSE), Pareto search has been commonly adopted. It searches for a good approximation of the problem’s Pareto-optimal solutions, from which the stakeholders choose the most preferred solution according to their preferences. However, when clear preferences of the stakeholders (e.g., a set of weights that reflect relative importance between objectives) are available prior to the search, weighted search is believed to be the first choice, since it simplifies the search via converting the original multi-objective problem into a single-objective one and enables the search to focus on what only the stakeholders are interested in.This article questions such a “weighted search first” belief. We show that the weights can, in fact, be harmful to the search process even in the presence of clear preferences. Specifically, we conduct a large-scale empirical study that consists of 38 systems/projects from three representative SBSE problems, together with two types of search budget and nine sets of weights, leading to 604 cases of comparisons. Our key finding is that weighted search reaches a certain level of solution quality by consuming relatively less resources at the early stage of the search; however, Pareto search is significantly better than its weighted counterpart the majority of the time (up to 77% of the cases), as long as we allow a sufficient, but not unrealistic search budget. This is a beneficial result, as it discovers a potentially new “rule-of-thumb” for the SBSE community: Even when clear preferences are available, it is recommended to always consider Pareto search by default for multi-objective SBSE problems, provided that solution quality is more important. Weighted search, in contrast, should only be preferred when the resource/search budget is limited, especially for expensive SBSE problems. This, together with other findings and actionable suggestions in the article, allows us to codify pragmatic and comprehensive guidance on choosing weighted and Pareto search for SBSE under the circumstance that clear preferences are available. All code and data can be accessed at .},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {5},
numpages = {40},
keywords = {self-adaptive systems, adaptive systems, configurable systems, user preference, quality indicator, quality evaluation, pareto optimization, multi-objective optimization, Search-based software engineering}
}

@inproceedings{10.1145/3365438.3410963,
author = {Alwidian, Sanaa and Amyot, Daniel},
title = {"Union is power": analyzing families of goal models using union models},
year = {2020},
isbn = {9781450370196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365438.3410963},
doi = {10.1145/3365438.3410963},
abstract = {A goal model family is a set of related goal models that conform to the same metamodel, with commonalities and variabilities between models. Such families stem from the evolution of initial models into several versions over time and/or the variation of models over the space dimension (e.g., products). In contexts where there are several versions/variations of a goal model, analyzing a set of related models with typical similarities, one model at a time, often involves redundant computations and may require repeated user assistance (e.g., for interactive analysis) and laborious activities. This paper proposes the use of union models as first-class artifacts to analyze families of goal models, in order to improve performance of language-specific analysis procedures. The paper empirically evaluates the performance gain resulting from adapting (or lifting) an existing analysis technique specific to the Goal-oriented Requirement Language (GRL) to a family of GRL models, all at once using a union model, compared to analyzing individual models. Our experiments show, based on the use of the IBM CPLEX optimizer, the usefulness and performance gains of using union models to perform a computationally expensive analysis, namely quantitative backward propagation, on families of GRL models.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
pages = {252–262},
numpages = {11},
keywords = {model family, goal modeling, backward propagation, analysis},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@inproceedings{10.1145/2971648.2971688,
author = {Wang, Wei and Yang, Lin and Zhang, Qian},
title = {Touch-and-guard: secure pairing through hand resonance},
year = {2016},
isbn = {9781450344616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2971648.2971688},
doi = {10.1145/2971648.2971688},
abstract = {Securely pairing wearables with another device is the key to many promising applications, such as mobile payment, sensitive data transfer and secure interactions with smart home devices. This paper presents Touch-And-Guard (TAG), a system that uses hand touch as an intuitive manner to establish a secure connection between a wristband wearable and the touched device. It generates secret bits from hand resonant properties, which are obtained using accelerometers and vibration motors. The extracted secret bits are used by both sides to authenticate each other and then communicate confidentially. The ubiquity of accelerometers and motors presents an immediate market for our system. We demonstrate the feasibility of our system using an experimental prototype and conduct experiments involving 12 participants with 1440 trials. The results indicate that we can generate secret bits at a rate of 7.84 bit/s, which is 58% faster than conventional text input PIN authentication. We also show that our system is resistant to acoustic eavesdroppers in proximity.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {670–681},
numpages = {12},
keywords = {wearable, secure piaring, resonance, modal analysis},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1145/3603555.3608539,
author = {Gollasch, David and Weber, Gerhard},
title = {Applying a Feature-Oriented Software Development Approach to Model Interaction Diversity},
year = {2023},
isbn = {9798400707711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603555.3608539},
doi = {10.1145/3603555.3608539},
abstract = {This research introduces a novel modelling approach based on methods from feature-oriented software development, aimed at enhancing accessibility and diversity in interactive systems. The method integrates user requirements, particularly accessibility and sensitivity to diversity, into software family development. Utilizing a user model subtree, it allows for customization based on users' needs, constraints, and preferences. A prototypical demonstration is shown through a voice user interface of an assistance robot. Despite an overall satisfying success rate of 96%, results suggest the quality of configuration slightly decreases with an increasing number of user constraints. This innovative approach offers significant potential, especially given the growing need for personalized human-computer interaction in our ageing society. However, it also prompts further research questions, such as its adaptability to non-software family systems and quality of configuration via smart AI models.},
booktitle = {Proceedings of Mensch Und Computer 2023},
pages = {427–431},
numpages = {5},
keywords = {Accessibility, Adaptive Systems, Feature-Oriented Software Development},
location = {Rapperswil, Switzerland},
series = {MuC '23}
}

@article{10.1145/3676557,
author = {Gilberto, Lucas G. and Bermejo, Fernando Ra\'{u}l and Tommasini, Fabi\'{a}n C. and Garc\'{\i}a Bauza, Cristian},
title = {Virtual Reality Audio Game for Entertainment and Sound Localization Training},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1544-3558},
url = {https://doi.org/10.1145/3676557},
doi = {10.1145/3676557},
abstract = {Within the gaming and electronics industry, there is a continuous evolution of alternative applications. Nevertheless, accessibility to video games remains a persistent hurdle for individuals with disabilities, especially those with visual impairments due to the inherent visual-oriented design of games. Audio games (AGs) are electronic games that rely primarily on auditory cues instead of visual interfaces. This study focuses on the creation of a virtual reality AG for cell phones that integrates natural head and torso movements involved in spatial hearing. Its assessment encompasses user experience, interface usability, and sound localization performance. The study engaged eighteen sighted participants in a pre-post test with a control group. The experimental group underwent 7 training sessions with the AG. Via interviews, facets of the gaming experience were explored, while horizontal plane sound source localization was also tested before and after the training. The results enabled the characterization of sensations related to the use of the game and the interaction with the interfaces. Sound localization tests demonstrated distinct enhancements in performance among trained participants, varying with assessed stimuli. These promising results show advances for future virtual AGs, presenting prospects for auditory training. These innovations hold potential for skill development, entertainment, and the integration of visually impaired individuals.},
journal = {ACM Trans. Appl. Percept.},
month = nov,
articleno = {4},
numpages = {24},
keywords = {Audio games, spatial hearing training, natural interfaces}
}

@article{10.1145/3661484,
author = {Kr\"{u}ger, Jacob and Li, Yi and Lossev, Kirill and Zhu, Chenguang and Chechik, Marsha and Berger, Thorsten and Rubin, Julia},
title = {A Meta-Study of Software-Change Intentions},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3661484},
doi = {10.1145/3661484},
abstract = {Every software system undergoes changes, for example, to add new features, fix bugs, or refactor code. The importance of understanding software changes has been widely recognized, resulting in various techniques and studies, for instance, on change-impact analysis or classifying developers’ activities. Since changes are triggered by developers’ intentions—something they plan or want to change in the system—many researchers have studied intentions behind changes. While there appears to be a consensus among software-engineering researchers and practitioners that knowing the intentions behind software changes is important, it is not clear how developers can actually benefit from this knowledge. In fact, there is no consolidated, recent overview of the state of the art on software-change intentions (SCIs) and their relevance for software engineering. We present a meta-study of 122 publications, which we used to derive a categorization of SCIs and to discuss motivations, evidence, and techniques relating to SCIs. Unfortunately, we found that individual pieces of research are often disconnected from each other, because a common understanding is missing. Similarly, some publications showcase the potential of knowing SCIs, but more substantial research to understand the practical benefits of knowing SCIs is needed. Our contributions can help researchers and practitioners improve their understanding of SCIs and how SCIs can aid software engineering tasks.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {300},
numpages = {41},
keywords = {Intentions, software evolution, change management, version control}
}

@inproceedings{10.1145/3335203.3335726,
author = {Cogranne, R\'{e}mi and Giboulot, Quentin and Bas, Patrick},
title = {The ALASKA Steganalysis Challenge: A First Step Towards Steganalysis},
year = {2019},
isbn = {9781450368216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335203.3335726},
doi = {10.1145/3335203.3335726},
abstract = {This paper presents ins and outs of the ALASKA challenge, a steganalysis challenge built to reflect the constraints of a forensic steganalyst. We motivate and explain the main differences w.r.t. the BOSS challenge (2010), specifically the use of a ranking metric prescribing high false positive rates, the analysis of a large diversity of different image sources and the use of a collection of steganographic schemes adapted to handle color JPEGs. The core of the challenge is also described, this includes the RAW image data-set, the implementations used to generate cover images and the specificities of the embedding schemes. The very first outcomes of the challenge are then presented, and the impacts of different parameters such as demosaicking, filtering, image size, JPEG quality factors and cover-source mismatch are analyzed. Eventually, conclusions are presented, highlighting positive and negative points together with future directions for the next challenges in practical steganalysis.},
booktitle = {Proceedings of the ACM Workshop on Information Hiding and Multimedia Security},
pages = {125–137},
numpages = {13},
keywords = {steganography, steganalysis, forensics, contest},
location = {Paris, France},
series = {IH&amp;MMSec'19}
}

@inproceedings{10.1145/1353482.1353496,
author = {Chakravarthy, Venkat and Regehr, John and Eide, Eric},
title = {Edicts: implementing features with flexible binding times},
year = {2008},
isbn = {9781605580449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1353482.1353496},
doi = {10.1145/1353482.1353496},
abstract = {In a software product line, the binding time of a feature is the time at which one decides to include or exclude a feature from a product. Typical binding site implementations are intended to support a single binding time only, e.g., compile time or run time. Sometimes, however, a product line must support features with variable binding times. For instance, a product line may need to include both embedded system configurations, in which features are selected and optimized early, and desktop configurations, in which client programs choose features on demand.We present a new technique for implementing the binding sites of features that require flexible binding times. Our technique combines design patterns and aspect-oriented programming: a pattern encapsulates the variation point, and targeted aspects---called edicts---set the binding times of the pattern participants. We describe our approach and demonstrate its usefulness by creating a middleware product line capable of serving the desktop and embedded domains. Our product line is based on JacORB, a middleware platform with many dynamically configurable features. By using edicts to select features at compile time, we create a version of JacORB more suited to resource-constrained environments. By configuring four JacORB subsystems via edicts, we achieve a 32.2% reduction in code size. Our examples show that our technique effectively modularizes binding-time concerns, supporting both compile-time optimization and run-time flexibility as needed.},
booktitle = {Proceedings of the 7th International Conference on Aspect-Oriented Software Development},
pages = {108–119},
numpages = {12},
location = {Brussels, Belgium},
series = {AOSD '08}
}

@inproceedings{10.5555/1105634.1105651,
author = {de Oliveira, Edson Alves and Gimenes, Itana M. S. and Huzita, Elisa Hatsue Moriya and Maldonado, Jos\'{e} Carlos},
title = {A variability management process for software product lines},
year = {2005},
publisher = {IBM Press},
abstract = {The software product line approach (PL) promotes the generation of specific products from a set of core assets for a given domain. This approach is applicable to domains in which products have well-defined commonalities and variation points. Variability management is concerned with the management of the differences between products throughout the PL lifecycle. This paper presents a UML-based process for variability management that allows identification, representation and delimitation of variabilities as well as identification of mechanisms for variability implementation. The process is illustrated with excerpts of a case study carried out within the context of an existing PL for the Workflow Management System (WfMS) domain. The case study was carried out based on the experimental software engineering concepts. The results have shown that the proposed process has made explicit a higher number of variabilities than does the existing PL process, and it offers better support for variability tracing.},
booktitle = {Proceedings of the 2005 Conference of the Centre for Advanced Studies on Collaborative Research},
pages = {225–241},
numpages = {17},
location = {Toranto, Ontario, Canada},
series = {CASCON '05}
}

@inproceedings{10.1145/3560905.3568517,
author = {Ji, Sijie and Xie, Yaxiong and Li, Mo},
title = {SiFall: Practical Online Fall Detection with RF Sensing},
year = {2023},
isbn = {9781450398862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560905.3568517},
doi = {10.1145/3560905.3568517},
abstract = {Falls are one of the leading causes of death in the elderly people aged 65 and above. In order to prevent death by sending prompt fall detection alarms, non-invasive radio-frequency (RF) based fall detection has attracted significant attention, due to its wide coverage and privacy preserving nature. Existing RF-based fall detection systems process fall as an activity classification problem and assume that human falls introduce reproducible patterns to the RF signals. We, however, argue that the fall is essentially an accident, hence, its impact is uncontrollable and unforeseeable. We propose to solve the fall detection problem in a fundamentally different manner. Instead of directly identifying the human falls which are difficult to quantify, we recognize the normal repeatable human activities and then identify the fall as abnormal activities out of the normal activity distribution. We implement our idea and build a prototype based on commercial Wi-Fi. We conduct extensive experiments with 16 human subjects. The experiment results show that our system can achieve high fall detection accuracy and adapt to different environments for real-time fall detection.},
booktitle = {Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems},
pages = {563–577},
numpages = {15},
keywords = {adaptive segmentation, device-free, fall detection, real-time system, self-supervised learning, wireless sensing},
location = {Boston, Massachusetts},
series = {SenSys '22}
}

@article{10.1109/TNET.2023.3327345,
author = {Sun, Haifeng and Huang, Qun and Lee, Patrick P. C. and Bai, Wei and Zhu, Feng and Bao, Yungang},
title = {Distributed Network Telemetry With Resource Efficiency and Full Accuracy},
year = {2024},
issue_date = {June 2024},
publisher = {IEEE Press},
volume = {32},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2023.3327345},
doi = {10.1109/TNET.2023.3327345},
abstract = {Network telemetry is essential for administrators to monitor massive data traffic in a network-wide manner. Existing telemetry solutions often face the dilemma between resource efficiency (i.e., low CPU, memory, and bandwidth overhead) and full accuracy (i.e., error-free and holistic measurement). We break this dilemma via a network-wide architectural design OmniMon, which simultaneously achieves resource efficiency and full accuracy in flow-level telemetry for large-scale data centers. OmniMon carefully coordinates the collaboration among different types of entities in the whole network to execute telemetry operations, such that the resource constraints of each entity are satisfied without compromising full accuracy. It further addresses consistency in network-wide epoch synchronization and accountability in error-free packet loss inference. We prototype OmniMon in DPDK and P4. Testbed experiments on commodity servers and Tofino switches demonstrate the effectiveness of OmniMon over state-of-the-art solutions.},
journal = {IEEE/ACM Trans. Netw.},
month = jan,
pages = {1857–1872},
numpages = {16}
}

@inproceedings{10.1145/2818000.2818009,
author = {Bogaerts, Jasper and Decat, Maarten and Lagaisse, Bert and Joosen, Wouter},
title = {Entity-Based Access Control: supporting more expressive access control policies},
year = {2015},
isbn = {9781450336826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818000.2818009},
doi = {10.1145/2818000.2818009},
abstract = {Access control is an important part of security that restricts the actions that users can perform on resources. Policy models specify how these restrictions are formulated in policies. Over the last decades, we have seen several such models, including role-based access control and more recently, attribute-based access control. However, these models do not take into account the relationships between users, resources and entities and their corresponding properties. This limits the expressiveness of these models. In this work, we present Entity-Based Access Control (EBAC). EBAC introduces entities as a primary concept and takes into account both attributes and relationships to evaluate policies. In addition, we present Auctoritas. Auctoritas is a authorization system that provides a practical policy language and evaluation engine for EBAC. We find that EBAC increases the expressiveness of policies and fits the application domain well. Moreover, our evaluation shows that entity-based policies described in Auctoritas can be enforced with a low policy evaluation latency.},
booktitle = {Proceedings of the 31st Annual Computer Security Applications Conference},
pages = {291–300},
numpages = {10},
keywords = {XACML, Relationship, Policy Language, Language, Entity, EBAC, Authorization, Attribute, Access Control Model, Access Control, ABAC},
location = {Los Angeles, CA, USA},
series = {ACSAC '15}
}

@article{10.1145/3627157,
author = {Wan, Zhijing and Wang, Zhixiang and Chung, Cheukting and Wang, Zheng},
title = {A Survey of Dataset Refinement for Problems in Computer Vision Datasets},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3627157},
doi = {10.1145/3627157},
abstract = {Large-scale datasets have played a crucial role in the advancement of computer vision. However, they often suffer from problems such as class imbalance, noisy labels, dataset bias, or high resource costs, which can inhibit model performance and reduce trustworthiness. With the advocacy of data-centric research, various data-centric solutions have been proposed to solve the dataset problems mentioned above. They improve the quality of datasets by re-organizing them, which we call dataset refinement. In this survey, we provide a comprehensive and structured overview of recent advances in dataset refinement for problematic computer vision datasets.1 Firstly, we summarize and analyze the various problems encountered in large-scale computer vision datasets. Then, we classify the dataset refinement algorithms into three categories based on the refinement process: data sampling, data subset selection, and active learning. In addition, we organize these dataset refinement methods according to the addressed data problems and provide a systematic comparative description. We point out that these three types of dataset refinement have distinct advantages and disadvantages for dataset problems, which informs the choice of the data-centric method appropriate to a particular research objective. Finally, we summarize the current literature and propose potential future research topics.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {172},
numpages = {34},
keywords = {Dataset refinement, data sampling, subset selection, active learning}
}

@article{10.1145/264645.264658,
author = {Kieras, David E. and Wood, Scott D. and Meyer, David E.},
title = {Predictive engineering models based on the EPIC architecture for a multimodal high-performance human-computer interaction task},
year = {1997},
issue_date = {Sept. 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1073-0516},
url = {https://doi.org/10.1145/264645.264658},
doi = {10.1145/264645.264658},
abstract = {Engineering models of human performance permit some aspects of usability of interface designs to be predicted from an analysis of the task, and thus they can replace to some extent expensive user-testing data. We successfully predicted human performance in telephone operator tasks with engineering models constructed in the EPIC (Executive Process-Interactive Control) architecture for human information processing, which is especially suited for modeling multimodal, complex tasks, and has demonstrated success in other task domains. Several models were constructed on an a priori basis to represent different hypotheses about how operators coordinate their activities to produce rapid task performance. The models predicted  the total time with useful accuracy and clarified  some important properties of the task. The best model was based directly on the GOMS analysis of the task and made simple assumptions about the operator's task strategy, suggesting that EPIC models are a feasible approach to predicting performance in multimodal high-performance tasks.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = sep,
pages = {230–275},
numpages = {46},
keywords = {usability engineering, cognitive models}
}

@inproceedings{10.1145/3540250.3549147,
author = {Pei, Kexin and She, Dongdong and Wang, Michael and Geng, Scott and Xuan, Zhou and David, Yaniv and Yang, Junfeng and Jana, Suman and Ray, Baishakhi},
title = {NeuDep: neural binary memory dependence analysis},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549147},
doi = {10.1145/3540250.3549147},
abstract = {Determining whether multiple instructions can access the same memory location is a critical task in binary analysis. It is challenging as statically computing precise alias information is undecidable in theory. The problem aggravates at the binary level due to the presence of compiler optimizations and the absence of symbols and types. Existing approaches either produce significant spurious dependencies due to conservative analysis or scale poorly to complex binaries.  

We present a new machine-learning-based approach to predict memory dependencies by exploiting the model's learned knowledge about how binary programs execute. Our approach features (i) a self-supervised procedure that pretrains a neural net to reason over binary code and its dynamic value flows through memory addresses, followed by (ii) supervised finetuning to infer the memory dependencies statically. To facilitate efficient learning, we develop dedicated neural architectures to encode the heterogeneous inputs (i.e., code, data values, and memory addresses from traces) with specific modules and fuse them with a composition learning strategy.  

We implement our approach in NeuDep and evaluate it on 41 popular software projects compiled by 2 compilers, 4 optimizations, and 4 obfuscation passes. We demonstrate that NeuDep is more precise (1.5x) and faster (3.5x) than the current state-of-the-art. Extensive probing studies on security-critical reverse engineering tasks suggest that NeuDep understands memory access patterns, learns function signatures, and is able to match indirect calls. All these tasks either assist or benefit from inferring memory dependencies. Notably, NeuDep also outperforms the current state-of-the-art on these tasks.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {747–759},
numpages = {13},
keywords = {Reverse Engineering, Memory Dependence Analysis, Machine Learning for Program Analysis, Large Language Models},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3551349.3556899,
author = {Fernandez-Amoros, David and Heradio, Ruben and Mayr-Dorn, Christoph and Egyed, Alexander},
title = {Scalable Sampling of Highly-Configurable Systems: Generating Random Instances of the Linux Kernel},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556899},
doi = {10.1145/3551349.3556899},
abstract = {Software systems are becoming increasingly configurable. A paradigmatic example is the Linux kernel, which can be adjusted for a tremendous variety of hardware devices, from mobile phones to supercomputers, thanks to the thousands of configurable features it supports. In principle, many relevant problems on configurable systems, such as completing a partial configuration to get the system instance that consumes the least energy or optimizes any other quality attribute, could be solved through exhaustive analysis of all configurations. However, configuration spaces are typically colossal and cannot be entirely computed in practice. Alternatively, configuration samples can be analyzed to approximate the answers. Generating those samples is not trivial since features usually have inter-dependencies that constrain the configuration space. Therefore, getting a single valid configuration by chance is extremely unlikely. As a result, advanced samplers are being proposed to generate random samples at a reasonable computational cost. However, to date, no sampler can deal with highly configurable complex systems, such as the Linux kernel. This paper proposes a new sampler that does scale for those systems, based on an original theoretical approach called extensible logic groups. The sampler is compared against five other approaches. Results show our tool to be the fastest and most scalable one.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {89},
numpages = {12},
keywords = {variability modeling, software product lines, random sampling, configurable systems, binary decision diagrams, SAT, Kconfig},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/337180.337455,
author = {Gannod, Gerald C. and Lutz, Robyn R.},
title = {An approach to architectural analysis of product lines},
year = {2000},
isbn = {1581132069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/337180.337455},
doi = {10.1145/337180.337455},
abstract = {This paper addresses the issue of how to perform architectural analysis on an existing product line architecture. The con tribution of the paper is to identify and demonstrate a repeatable product line architecture analysis process. The approach defines a “good” product line architecture in terms of those quality attributes required by the particular product line under development. It then analyzes the architecture against these criteria by both manual and tool-supported methods. The phased approach described in this paper provides a structured analysis of an existing product line architecture using (1) formal specification of the high-level architecture, (2) manual analysis of scenarios to exercise the architecture's support for required variabilities, and (3) model checking of critical behaviors at the architectural level that are required for all systems in the product line. Results of an application to a software product line of spaceborne telescopes are used to explain and evaluate the approach.},
booktitle = {Proceedings of the 22nd International Conference on Software Engineering},
pages = {548–557},
numpages = {10},
keywords = {software archtecture, software architecture analysis, product lines, interferometry software},
location = {Limerick, Ireland},
series = {ICSE '00}
}

@inproceedings{10.1145/1062455.1062551,
author = {Verlage, Martin and Kiesgen, Thomas},
title = {Five years of product line engineering in a small company},
year = {2005},
isbn = {1581139632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062455.1062551},
doi = {10.1145/1062455.1062551},
abstract = {In 1999, a new team at MARKET MAKER Software AG began to develop a software product line for managing and displaying stock market data and financial market news. The basic idea was to use web technology in all applications for delivering services to customers. It soon turned out that the company had to change both the processes and the organization. This report summarizes the changes made and the lessons learned over the past five years, when the product line idea was introduced into a small company which faced the pressure to quickly market the first product line instances.},
booktitle = {Proceedings of the 27th International Conference on Software Engineering},
pages = {534–543},
numpages = {10},
keywords = {project management, product line engineering, experience report, SME},
location = {St. Louis, MO, USA},
series = {ICSE '05}
}

@article{10.1145/1075395.1075397,
author = {Lin, Zhiqiang and Wang, Chao and Mao, Bing and Xie, Li},
title = {A policy flexible architecture for secure operating system},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {3},
issn = {0163-5980},
url = {https://doi.org/10.1145/1075395.1075397},
doi = {10.1145/1075395.1075397},
abstract = {As the fundamental software to guarantee information security, operating system is desired to support various security policies flexibly and to control the propagation and revocation of access rights efficiently. This paper presents a design and implementation of Policy Flexible Architecture (PFA) for secure operating system to achieve the coordination, extensibility, consistency and dynamic configuration of security policies. Through a thorough analysis of all related facilities, PFA classifies policy constructions into several levels according to their effects on security status; PFA emphasizes on centralized management of security policy as well as on centralized maintainability of security attributes; for the first time PFA gives revocation rules to specify how and when to revoke permissions. PFA has been applied to our Secure Enhanced Linux Operating System, and the experiment shows that the system holds at least three advantages. First, it helps users to choose intended security policies flexibly. Besides it supports users to add new security policies according to specific security requirements easily. Finally it sets permission revocations immediately and gets no significant performance penalty.},
journal = {SIGOPS Oper. Syst. Rev.},
month = jul,
pages = {24–33},
numpages = {10}
}

@inproceedings{10.1145/2578128.2578237,
author = {de Andrade, Hugo Sica and Almeida, Eduardo and Crnkovic, Ivica},
title = {Architectural bad smells in software product lines: an exploratory study},
year = {2014},
isbn = {9781450325233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2578128.2578237},
doi = {10.1145/2578128.2578237},
abstract = {The Software Product Lines (SPL) paradigm has arisen for taking advantage of existing common aspects between different products, while also considering product-specific features. The architecture of a SPL comprises a model that will result in product architectures, and may include solutions leading to bad (architectural) design. One way to assess such design decisions is through the identification of architectural bad smells, which are properties that prejudice the overall software quality, but are not necessarily faulty or errant. In this paper, we conduct an exploratory study that aims at characterizing bad smells in the context of product line architectures. We analyzed an open source SPL project and extracted its architecture to investigate the occurrence or absence of four smells initially studied in single systems. In addition, we propose a smell specific to the SPL context and discuss possible causes and implications of having those smells in the architecture of a product line. The results indicate that the granularity of the SPL features may influence on the occurrence of smells.},
booktitle = {Proceedings of the WICSA 2014 Companion Volume},
articleno = {12},
numpages = {6},
keywords = {software product lines, exploratory study, evaluation, architecture, architectural bad smells},
location = {Sydney, Australia},
series = {WICSA '14 Companion}
}

@article{10.1109/TASLP.2023.3260703,
author = {Erdem, Ege and Cvetkovi\'{c}, Zoran and Hac\i{}habibo\u{g}lu, H\"{u}seyin},
title = {3D Perceptual Soundfield Reconstruction via Virtual Microphone Synthesis},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3260703},
doi = {10.1109/TASLP.2023.3260703},
abstract = {Perceptual soundfield reconstruction (PSR) is a multichannel audio recording and reproduction framework based on time-intensity panning in the horizontal plane. A practical limitation of PSR is that the optimal directivity patterns required by the system cannot be trivially and precisely obtained in practice, and it is limited to the horizontal plane. This paper extends the horizontal PSR to three dimensions and proposes a virtual microphone synthesis approach to obtain the PSR directivity pattern via sound field extrapolation. The proposed 3D extension and virtual microphone synthesis are evaluated using numerical simulations and a subjective localisation test. Comparisons with second-order Ambisonics rendering indicate that subjects localise sources rendered using 3D PSR more accurately and also with a higher certainty, particularly at an off-centre listening position for the low-channel count reproduction system employed.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {1305–1317},
numpages = {13}
}

@inproceedings{10.1145/3417990.3421263,
author = {Pett, Tobias and Eichhorn, Domenik and Schaefer, Ina},
title = {Risk-based compatibility analysis in automotive systems engineering},
year = {2020},
isbn = {9781450381352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417990.3421263},
doi = {10.1145/3417990.3421263},
abstract = {Software is the new leading factor for innovation in the automotive industry. With the increase of software in road vehicles new business models, such as after-sale updates (i.e., Function-on-Demand) and Over-the-Air-Updates come into focus of manufacturers. When updating a road vehicle in the field, it is required to ensure functional safety. An update shall not influence existing functionality and break its safety. Hence, it must be compatible with the existing software. The compatibility of an update is ensured by testing. However, testing all variants of a highly configurable system, such as a modern car's software, is infeasible, due to the combinatorial explosion. To address this problem, in this paper, we propose a risk-based change-impact analysis to identify system variants relevant for retesting after an update. We combine existing concepts from product sampling, risk-based testing, and configuration prioritization and apply them to automotive architectures. For validating our concept, we use the Body Comfort System case study from the automotive industry. Our evaluation reveals that the concept backed by tool support may reduce testing effort by identifying and prioritizing incompatible variants wrt to a system update.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
articleno = {34},
numpages = {10},
keywords = {risk-based analysis, configurable systems, automotive engineering},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@article{10.1145/3688836,
author = {Luo, Chuan and Song, Jianping and Zhao, Qiyuan and Sun, Binqi and Chen, Junjie and Zhang, Hongyu and Lin, Jinkun and Hu, Chunming},
title = {Solving the t-Wise Coverage Maximum Problem via Effective and Efficient Local Search-Based Sampling},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3688836},
doi = {10.1145/3688836},
abstract = {To meet the increasing demand for customized software, highly configurable systems become essential in practice. Such systems offer many options to configure, and ensuring the reliability of these systems is critical. A widely used evaluation metric for testing these systems is  (t) -wise coverage, where  (t)  represents testing strength, and its value typically ranges from 2 to 6. It is crucial to design effective and efficient methods for generating test suites that achieve high  (t) -wise coverage. However, current state-of-the-art methods need to generate large test suites for achieving high  (t) -wise coverage. In this work, we propose a novel method called LS-Sampling-Plus that can efficiently generate test suites with high  (t) -wise coverage for  (2leq tleq 6)  while being smaller in size compared to existing state-of-the-art methods. LS-Sampling-Plus incorporates many core algorithmic techniques, including two novel scoring functions, a dynamic mechanism for updating sampling probabilities, and a validity-guaranteed systematic search method. Our experiments on various practical benchmarks show that LS-Sampling-Plus can achieve higher  (t) -wise coverage than current state-of-the-art methods, through building a test suite of the same size. Moreover, our evaluations indicate the effectiveness of all core algorithmic techniques of LS-Sampling-Plus. Furthermore, LS-Sampling-Plus exhibits better scalability and fault detection capability than existing state-of-the-art methods.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {13},
numpages = {64},
keywords = {Highly Configurable Software Systems, Software Testing, Local Search}
}

@inproceedings{10.5555/3539845.3539965,
author = {Lee, Jongmin and Lee, Junyeon and Suh, Taeweon and Koo, Gunjae},
title = {CacheRewinder: revoking speculative cache updates exploiting write-back buffer},
year = {2022},
isbn = {9783981926361},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {Transient execution attacks are critical security threats since those attacks exploit speculative execution which is an essential architectural solution that can improve the performance of out-of-order processors significantly. Such attacks change cache state by accessing secret data during speculative executions, then the attackers leak the secret information exploiting cache timing side-channels. Even though software patches against transient execution attacks have been proposed, the software solutions significantly slow down the performance of a system.In this paper, we propose CacheRewinder, an efficient hardware-based defense mechanism against transient execution attacks. CacheRewinder prevents leakage of secret information by revoking the cache updates done by speculative executions. To restore the cache state efficiently, CacheRewinder exploits the underutilized write-back buffer space as the temporary storage for victimized cache blocks evicted during speculative executions. Hence, when speculation fails CacheRewinder can quickly restore the cache state using the victim blocks held in the write-back buffer. Our evaluation exhibits CacheRewinder can effectively defend against transient execution attacks. The performance overhead by CacheRewinder is only 0.6%, which is negligible compared to the unprotected baseline processor. CacheRewinder also requires minimal storage cost since it exploits unused writeback buffer entries as storage for evicted cache blocks.},
booktitle = {Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe},
pages = {514–519},
numpages = {6},
keywords = {transient execution attacks, speculative execution, secure architecture, cache side-channels},
location = {Antwerp, Belgium},
series = {DATE '22}
}

@inproceedings{10.1145/3570361.3613281,
author = {Fan, Xiaoran and Pearl, David and Howard, Richard and Shangguan, Longfei and Thormundsson, Trausti},
title = {APG: Audioplethysmography for Cardiac Monitoring in Hearables},
year = {2023},
isbn = {9781450399906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570361.3613281},
doi = {10.1145/3570361.3613281},
abstract = {This paper presents Audioplethysmography (APG), a novel cardiac monitoring modality for active noise cancellation (ANC) headphones. APG sends a low intensity ultrasound probing signal using an ANC headphone's speakers and receives the echoes via the on-board feedback microphones. We observed that, as the volume of ear canals slightly changes with blood vessel deformations, the heartbeats will modulate these ultrasound echoes. We built mathematical models to analyze the underlying physics and propose a multi-tone APG signal processing pipeline to derive the heart rate and heart rate variability in both constrained and unconstrained settings. APG enables robust monitoring of cardiac activities using mass-market ANC headphones in the presence of music playback and body motion such as running.We conducted an eight-month field study with 153 participants to evaluate APG in various conditions. Our studies conform to the (Institutional Review Board) IRB policies from our company. The presented technology, experimental design, and results have been reviewed and further improved by feedback garnered from our internal Health Team, Product Team, User Experience (UX) Team and Legal team. Our results demonstrate that APG achieves consistently high HR (3.21% median error across 153 participants in all scenarios) and HRV (2.70% median error in interbeat interval, IBI) measurement accuracy. Our UX study further shows that APG is resilient to variation in: skin tone, sub-optimal seal conditions, and ear canal size.},
booktitle = {Proceedings of the 29th Annual International Conference on Mobile Computing and Networking},
articleno = {67},
numpages = {15},
keywords = {mobile health, wearable devices, heart rate monitoring, earable computing},
location = {Madrid, Spain},
series = {ACM MobiCom '23}
}

@inproceedings{10.1145/2739482.2768422,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Assun\c{c}\~{a}o, Wesley K.G. and Fischer, Stefan and Vergilio, Silvia R. and Egyed, Alexander},
title = {Genetic Improvement for Software Product Lines: An Overview and a Roadmap},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2768422},
doi = {10.1145/2739482.2768422},
abstract = {Software Product Lines (SPLs) are families of related software systems that provide different combinations of features. Extensive research and application attest to the significant economical and technological benefits of employing SPL practices. However, there are still several challenges that remain open. Salient among them is reverse engineering SPLs from existing variants of software systems and their subsequent evolution. In this paper, we aim at sketching connections between research on these open SPL challenges and ongoing work on Genetic Improvement. Our hope is that by drawing such connections we can spark the interest of both research communities on the exciting synergies at the intersection of these subject areas.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {823–830},
numpages = {8},
keywords = {variability, software product lines, genetic programming, genetic improvement, evolutionary algorithms},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@inproceedings{10.1145/2346536.2346547,
author = {Shang, Richard D. and Mohan, Kannan and Lang, Karl R. and Vragov, Roumen},
title = {A market mechanism for software component reuse: opportunities and barriers},
year = {2012},
isbn = {9781450311977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2346536.2346547},
doi = {10.1145/2346536.2346547},
abstract = {We propose a market based design for trading component-based software products bundled with reuse licenses that allows clients to customize software solutions onsite by reusing and reintegrating components across software systems using their own software development platforms. Using economic experiments in the laboratory with IT professionals we find that introducing reuse licenses has social welfare benefits in terms of both higher seller and higher buyer surplus as well as generating higher product variety in the market. We argue that as software development is increasingly using modularized and component-based approaches software vendor strategies based on flexible licenses permitting the reuse of software offer a viable and sustainable alternative to the traditional software business model build on monolithic user licenses.},
booktitle = {Proceedings of the 14th Annual International Conference on Electronic Commerce},
pages = {62–69},
numpages = {8},
location = {Singapore, Singapore},
series = {ICEC '12}
}

@inproceedings{10.1145/3241403.3241453,
author = {Koschke, Rainer},
title = {Industrial experience on code clean-up using architectural conformance checking},
year = {2018},
isbn = {9781450364836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241403.3241453},
doi = {10.1145/3241403.3241453},
abstract = {This paper reports experiences in using the reflexion method to reverse engineer the architecture of an industrial Java application and to specify the target architecture for an architectural refactoring in order to steer the refactoring of the code and to measure progress. The goal in this industrial case study was to clean up obsolete code after a larger migration and to provide an architectural documentation for new developers. The distinctiveness of this study is that it was conducted by the author who is both an academic researcher on architecture erosion and conformance as well as a professional developer who is also among the two original developers of the Java program that was refactored.We will report the amount of time required and the effects of the migration in terms of various quality metrics. We will also discuss the level of support the method offered for this refactoring.},
booktitle = {Proceedings of the 12th European Conference on Software Architecture: Companion Proceedings},
articleno = {48},
numpages = {7},
keywords = {software erosion, software architecture conformance checking, industrial experience report},
location = {Madrid, Spain},
series = {ECSA '18}
}

@inproceedings{10.1145/3607822.3614514,
author = {Kuratomo, Noko and Karic, Benjamin and Kray, Christian},
title = {Design and Effect of Guiding Sound for Pedestrians While Maintaining the Streetscape Perception},
year = {2023},
isbn = {9798400702815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607822.3614514},
doi = {10.1145/3607822.3614514},
abstract = {There are many reasons to direct the flow of people in urban environments: avoiding overcrowdedness during large-scale events, minimising contacts during a pandemic or redirecting pedestrians in case of temporary obstructions such as accidents. In practice, this is largely achieved through visual means such as signage, via physical barriers or by police officers. These approaches have in common that they visually change the environment, which might negatively affect people’s experience. In this paper, we explore stereophonic guiding sounds as an alternative crowd flow management technique, which does not visually affect an environment. We designed three different sounds to attract pedestrians that we played back on parametric loudspeakers to localise them precisely in space. We evaluated them in a study with 16 participants, who were asked to navigate an urban environment that we projected in an immersive video environment. Our results show that this approach is generally feasible, that sounds can motivate people to take a detour, and that the type of sounds affects the degree of motivation. The work reported here thus provides new insights into using stereophonic sound as a spatial user interface in controlling the flow of people in urban environments.},
booktitle = {Proceedings of the 2023 ACM Symposium on Spatial User Interaction},
articleno = {30},
numpages = {10},
keywords = {auditory display, guiding sound, stereophonic sound, streetscape},
location = {Sydney, NSW, Australia},
series = {SUI '23}
}

@inproceedings{10.1145/3491102.3501960,
author = {Shen, Vivian and Shultz, Craig and Harrison, Chris},
title = {Mouth Haptics in VR using a Headset Ultrasound Phased Array},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501960},
doi = {10.1145/3491102.3501960},
abstract = {Today’s consumer virtual reality (VR) systems offer limited haptic feedback via vibration motors in handheld controllers. Rendering haptics to other parts of the body is an open challenge, especially in a practical and consumer-friendly manner. The mouth is of particular interest, as it is a close second in tactile sensitivity to the fingertips, offering a unique opportunity to add fine-grained haptic effects. In this research, we developed a thin, compact, beamforming array of ultrasonic transducers, which can render haptic effects onto the mouth. Importantly, all components are integrated into the headset, meaning the user does not need to wear an additional accessory, or place any external infrastructure in their room. We explored several effects, including point impulses, swipes, and persistent vibrations. Our haptic sensations can be felt on the lips, teeth and tongue, which can be incorporated into new and interesting VR experiences.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {275},
numpages = {14},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@article{10.1109/TASLP.2015.2493366,
author = {Saeidi, Rahim and Alku, Paavo and B\"{a}ckstr\"{o}m, Tom},
title = {Feature extraction using power-law adjusted linear prediction with application to speaker recognition under severe vocal effort mismatch},
year = {2016},
issue_date = {January 2016},
publisher = {IEEE Press},
volume = {24},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2493366},
doi = {10.1109/TASLP.2015.2493366},
abstract = {Linear prediction is one of the most established techniques in signal estimation, and it is widely utilized in speech signal processing. It has been long understood that the nerve firing rate of human auditory system can be approximated by power law nonlinearity, and this has been the motivation behind using perceptual linear prediction in extracting acoustic features in a variety of speech processing applications. In this paper, we revisit the application of power law non-linearity in speech spectrum estimation by compressing/expanding power spectrum in autocorrelation-based linear prediction. The development of so-called LP-α is motivated by a desire to obtain spectral features that present less mismatch than conventionally used spectrum estimation methods when speech of normal loudness is compared to speech under vocal effort. The effectiveness of the proposed approach is demonstrated in a speaker recognition task conducted under severe vocal effort mismatch comparing shouted versus normal speech mode.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {42–53},
numpages = {12},
keywords = {vocal effort, speaker recognition, shouting, powerlaw, mismatch, linear prediction}
}

@inproceedings{10.1145/1555349.1555351,
author = {Lelarge, Marc},
title = {Efficient control of epidemics over random networks},
year = {2009},
isbn = {9781605585116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1555349.1555351},
doi = {10.1145/1555349.1555351},
abstract = {Motivated by the modeling of the spread of viruses or epidemics with coordination among agents, we introduce a new model generalizing both the basic contact model and the bootstrap percolation. We analyze this percolated threshold model when the underlying network is a random graph with fixed degree distribution. Our main results unify many results in the random graphs literature. In particular, we provide a necessary and sufficient condition under which a single node can trigger a large cascade. Then we quantify the possible impact of an attacker against a degree based vaccination and an acquaintance vaccination. We define a security metric allowing to compare the different vaccinations. The acquaintance vaccination requires no knowledge of the node degrees or any other global information and is shown to be much more efficient than the uniform vaccination in all cases.},
booktitle = {Proceedings of the Eleventh International Joint Conference on Measurement and Modeling of Computer Systems},
pages = {1–12},
numpages = {12},
keywords = {vaccination, random graphs, epidemics},
location = {Seattle, WA, USA},
series = {SIGMETRICS '09}
}

@inproceedings{10.1145/3422392.3422414,
author = {Marcondes, Arthur Roberto and Terra, Ricardo},
title = {An approach for updating forks against the original project},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422414},
doi = {10.1145/3422392.3422414},
abstract = {Several software projects start from an existing project. This practice, in the VCS ecosystem, is called fork. For instance, the Bootstrap project, initially developed on Twitter, today has more than 68,000 forks, which indicates that several projects started from the Bootstrap source code at a certain moment and are being customized. The problem occurs when customized projects want to obtain updates from the original project, i.e., new features, bug fixes, etc. The merge of the source code between the original and the customized projects usually generates conflicts that need human resolution. More important, the resolution of those conflicts might not be trivial and poses an arduous task for developers. This article, therefore, proposes an approach for updating forks against the original project where features are modularized, documented, traceable, and can be reused. We claim that the such task can no longer be carried out on an ad hoc basis. In a nutshell, instead of modify the method foo from the original project, the developer implements it locally and specifies, using one of the nine instructions of the proposed DSL, something like "replace the foo method with local implementation". We have developed a tool that automates our approach and conducted an evaluation on a large-scale real-world project that is regularly updated against your original project.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {213–222},
numpages = {10},
keywords = {Evolu\c{c}\~{a}o de software, conflitos de mesclagem, desenvolvimento colaborativo, fork, merge},
location = {Natal, Brazil},
series = {SBES '20}
}

@article{10.1145/3300148,
author = {Li, Miqing and Yao, Xin},
title = {Quality Evaluation of Solution Sets in Multiobjective Optimisation: A Survey},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3300148},
doi = {10.1145/3300148},
abstract = {Complexity and variety of modern multiobjective optimisation problems result in the emergence of numerous search techniques, from traditional mathematical programming to various randomised heuristics. A key issue raised consequently is how to evaluate and compare solution sets generated by these multiobjective search techniques. In this article, we provide a comprehensive review of solution set quality evaluation. Starting with an introduction of basic principles and concepts of set quality evaluation, this article summarises and categorises 100 state-of-the-art quality indicators, with the focus on what quality aspects these indicators reflect. This is accompanied in each category by detailed descriptions of several representative indicators and in-depth analyses of their strengths and weaknesses. Furthermore, issues regarding attributes that indicators possess and properties that indicators are desirable to have are discussed, in the hope of motivating researchers to look into these important issues when designing quality indicators and of encouraging practitioners to bear these issues in mind when selecting/using quality indicators. Finally, future trends and potential research directions in the area are suggested, together with some guidelines on these directions.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {26},
numpages = {38},
keywords = {performance assessment, multobjective optimisation, multi-criteria optimisation, metric, metaheuristic, measure, indicator, heuristic, exact method, evolutionary algorithms, Quality evaluation}
}

@inproceedings{10.1145/3302333.3302346,
author = {Gomes, Karine and Teixeira, Leopoldo and Alves, Thayonara and Ribeiro, M\'{a}rcio and Gheyi, Rohit},
title = {Characterizing safe and partially safe evolution scenarios in product lines: An Empirical Study},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302346},
doi = {10.1145/3302333.3302346},
abstract = {Evolving software product lines is often error-prone. Previous works have proposed classifying product line evolution into safe or partially safe, depending on the number of products that have their behavior preserved after evolution. Based on these notions, it is possible to derive transformation templates that abstract common evolution scenarios, such as adding an optional feature. However, existing works are focused on evaluating either safe or partially safe templates. Hence, in this work we aim to characterize product line evolution as a whole, measuring to what extent the evolution history is safe compared to partially safe, to better understand how product lines evolve. We measure how often existing templates happen using 2,300 commits from an open-source product line. According to our study, 91.7% of the commits represent partially safe evolution scenarios. Our results also show that 1,800 of these commits can automatically be classified as instances of existing templates. Among these, commits that do not modify other variability-aware models, are the most frequent, accounting for 72.3% out of the total of commits. For the remaining 500 commits, we identify that 24.4% are related to changes in the configuration knowledge, that is, the file responsible for the mapping between features and code.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {15},
numpages = {9},
keywords = {Software Product Lines, Safe Evolution, Product Line Evolution, Partially Safe Evolution, Empirical Study, Configurable Systems},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@article{10.1145/3478093,
author = {Zhang, Qian and Wang, Dong and Zhao, Run and Yu, Yinggang and Shen, Junjie},
title = {Sensing to Hear: Speech Enhancement for Mobile Devices Using Acoustic Signals},
year = {2021},
issue_date = {Sept 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3478093},
doi = {10.1145/3478093},
abstract = {Voice interactions and voice messages on mobile phones are rapidly growing in popularity. However, the user experience of these services is still worse than desired in noisy environments, especially in multi-talker scenarios, where the phone can only provide low-quality voice recordings. Speech enhancement using only audio as the input remains a grand challenge in these scenarios. In this paper, we handle this with the help of the emerging acoustic sensing technology. The key insight is that the inaudible acoustic signals emitted by speakers of phones can capture the subtle lip movements when people speak. Instead of enabling lip reading for the classification of limited voice commands, we further unlock the potential of acoustic sensing and leverage the captured lip information to improve the voice recording quality. We propose WaveVoice, a joint audio-sensory deep learning method for end-to-end speech enhancement on mobile phones. The model of WaveVoice is structured as an encoder-decoder network, in which audio and acoustic sensing data are processed through two individual CNN branches, respectively, and then fused into a joint network to generate enhanced speech. In addition, to improve the performance on new users, a self-supervised learning methodology is developed to adapt the model to extract speaker-specific features. We construct a dataset to train and evaluate WaveVoice. We also perform online tests under various noisy conditions to show the applicability of our system in real-world scenarios. Experimental results show that WaveVoice can effectively reconstruct the target clean speech from the noisy audio signals, and yield notably superior performance compared with the audio-only encoder-decoder model and the state-of-the-art speech enhancement methods. Given its promising performance, we believe that WaveVoice has made a substantial contribution to the advancement of mobile voice input.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {137},
numpages = {30},
keywords = {mobile speech enhancement, lip movement, deep learning, acoustic sensing}
}

@inproceedings{10.1145/2593929.2593930,
author = {Garc\'{\i}a-Gal\'{a}n, Jes\'{u}s and Pasquale, Liliana and Trinidad, Pablo and Ruiz-Cort\'{e}s, Antonio},
title = {User-centric adaptation of multi-tenant services: preference-based analysis for service reconfiguration},
year = {2014},
isbn = {9781450328647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593929.2593930},
doi = {10.1145/2593929.2593930},
abstract = {Multi-tenancy is a key pillar of cloud services. It allows different tenants to share computing resources transparently and, at the same time, guarantees substantial cost savings for the providers. However, from a user perspective, one of the major drawbacks of multi-tenancy is lack of configurability. Depending on the isolation degree, the same service instance and even the same service configuration may be shared among multiple tenants (i.e. shared multi-tenant service). Moreover tenants usually have different - and in most of the cases - conflicting configuration preferences. To overcome this limitation, this paper introduces a novel approach to support user-centric adaptation in shared multi-tenant services. The adaptation objective aims to maximise tenants’ satisfaction, even when tenants and their preferences change during the service life-time. This paper describes how to engineer the activities of the MAPE loop to support user-centric adaptation, and focuses on the analysis of tenants’ preferences. In particular, we use a game theoretic analysis to identify a service configuration that maximises tenants’ preferences satisfaction. We illustrate and motivate our approach by utilising a multi-tenant desktop scenario. Obtained experimental results demonstrate the feasibility of the proposed analysis.},
booktitle = {Proceedings of the 9th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {65–74},
numpages = {10},
keywords = {multi-tenancy, game theory, cloud, Adaptive systems},
location = {Hyderabad, India},
series = {SEAMS 2014}
}

@article{10.1109/TASLP.2021.3053388,
author = {Kelly, Finnian and Hansen, John H.L.},
title = {Analysis and Calibration of Lombard Effect and Whisper for Speaker Recognition},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3053388},
doi = {10.1109/TASLP.2021.3053388},
abstract = {Variations in vocal effort can create challenges for speaker recognition systems that are optimized for use with neutral speech. The Lombard effect and whisper are two commonly-occurring forms of vocal effort variation that result in non-neutral speech, the first due to noise exposure and the second due to intentional adjustment on the part of the speaker. In this article, a comparative evaluation of speaker recognition performance in non-neutral conditions is presented using multiple Lombard effect and whisper corpora. The detrimental impact of these vocal effort variations on discrimination and calibration performance on global, per-corpus, and per-speaker levels is explored using conventional error metrics, along with visual representations of the model and score spaces. A non-neutral speech detector is subsequently introduced and used to inform score calibration in several ways. Two calibration approaches are proposed and shown to reduce error to the same level as an optimal calibration approach that relies on ground-truth vocal effort information. This article contributes a generalizable methodology towards detecting vocal effort variation and using this knowledge to inform and advance speaker recognition system behavior.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {927–942},
numpages = {16}
}

@inproceedings{10.1145/3581791.3596856,
author = {Chan, Justin and Glenn, Antonio and Itani, Malek and Mancl, Lisa R. and Gallagher, Emily and Bly, Randall and Patel, Shwetak and Gollakota, Shyamnath},
title = {Wireless earbuds for low-cost hearing screening},
year = {2023},
isbn = {9798400701108},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581791.3596856},
doi = {10.1145/3581791.3596856},
abstract = {We present the first wireless earbud hardware that can perform hearing screening by detecting otoacoustic emissions. The conventional wisdom has been that detecting otoacoustic emissions, which are the faint sounds generated by the cochlea, requires sensitive and expensive acoustic hardware. Thus, medical devices for hearing screening cost thousands of dollars and are inaccessible in low and middle income countries. We show that by designing wireless ear-buds using low-cost acoustic hardware and combining them with wireless sensing algorithms, we can reliably identify otoacoustic emissions and perform hearing screening. Our algorithms combine frequency modulated chirps with wideband pulses emitted from a low-cost speaker to reliably separate otoacoustic emissions from in-ear reflections and echoes. We conducted a clinical study with 50 ears across two healthcare sites. Our study shows that the low-cost earbuds detect hearing loss with 100% sensitivity and 89.7% specificity, which is comparable to the performance of a $8000 medical device. By developing low-cost and open-source wearable technology, our work may help address global health inequities in hearing screening by democratizing these medical devices.Open-source hardware and code can be found here: https://github.com/uw-x/OAEbuds},
booktitle = {Proceedings of the 21st Annual International Conference on Mobile Systems, Applications and Services},
pages = {84–95},
numpages = {12},
keywords = {acoustic sensing, wearable technologies, otoacoustic emissions, mobile health, hearing screening, wireless earbuds},
location = {Helsinki, Finland},
series = {MobiSys '23}
}

@inproceedings{10.1145/2576768.2598305,
author = {Lopez-Herrejon, Roberto Erick and Javier Ferrer, Javier and Chicano, Francisco and Haslinger, Evelyn Nicole and Egyed, Alexander and Alba, Enrique},
title = {A parallel evolutionary algorithm for prioritized pairwise testing of software product lines},
year = {2014},
isbn = {9781450326629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2576768.2598305},
doi = {10.1145/2576768.2598305},
abstract = {Software Product Lines (SPLs) are families of related software systems, which provide different feature combinations. Different SPL testing approaches have been proposed. However, despite the extensive and successful use of evolutionary computation techniques for software testing, their application to SPL testing remains largely unexplored. In this paper we present the Parallel Prioritized product line Genetic Solver (PPGS), a parallel genetic algorithm for the generation of prioritized pairwise testing suites for SPLs. We perform an extensive and comprehensive analysis of PPGS with 235 feature models from a wide range of number of features and products, using 3 different priority assignment schemes and 5 product prioritization selection strategies. We also compare PPGS with the greedy algorithm prioritized-ICPL. Our study reveals that overall PPGS obtains smaller covering arrays with an acceptable performance difference with prioritized-ICPL.},
booktitle = {Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {1255–1262},
numpages = {8},
keywords = {software product lines, pairwise testing, feature models, combinatorial interaction testing},
location = {Vancouver, BC, Canada},
series = {GECCO '14}
}

@inproceedings{10.1145/3581783.3611859,
author = {Liu, Yingchi and Liu, Zhu and Ma, Long and Liu, Jinyuan and Fan, Xin and Luo, Zhongxuan and Liu, Risheng},
title = {Bilevel Generative Learning for Low-Light Vision},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3611859},
doi = {10.1145/3581783.3611859},
abstract = {Recently, there has been a growing interest in constructing deep learning schemes for Low-Light Vision (LLV). Existing techniques primarily focus on designing task-specific and data-dependent vision models on the standard RGB domain, which inherently contain latent data associations. In this study, we propose a generic low-light vision solution by introducing a generative block to convert data from the RAW to the RGB domain. This novel approach connects diverse vision problems by explicitly depicting data generation, which is the first in the field. To precisely characterize the latent correspondence between the generative procedure and the vision task, we establish a bilevel model with the parameters of the generative block defined as the upper level and the parameters of the vision task defined as the lower level. We further develop two types of learning strategies targeting different goals, namely low cost and high accuracy, to acquire a new bilevel generative learning paradigm. The generative blocks embrace a strong generalization ability in other low-light vision tasks through the bilevel optimization on enhancement tasks. Extensive experimental evaluations on three representative low-light vision tasks, namely enhancement, detection, and segmentation, fully demonstrate the superiority of our proposed approach. The code will be available at https://github.com/Yingchi1998/BGL.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {7758–7766},
numpages = {9},
keywords = {bilevel generative learning, low-light vision, raw generative block},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/2818000.2856129,
author = {G\"{o}tzfried, Johannes and M\"{u}ller, Tilo and de Clercq, Ruan and Maene, Pieter and Freiling, Felix and Verbauwhede, Ingrid},
title = {Soteria: Offline Software Protection within Low-cost Embedded Devices},
year = {2015},
isbn = {9781450336826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818000.2856129},
doi = {10.1145/2818000.2856129},
abstract = {Protecting the intellectual property of software that is distributed to third-party devices which are not under full control of the software author is difficult to achieve on commodity hardware today. Modern techniques of reverse engineering such as static and dynamic program analysis with system privileges are increasingly powerful, and despite possibilities of encryption, software eventually needs to be processed in clear by the CPU. To anyhow be able to protect software on these devices, a small part of the hardware must be considered trusted. In the past, general purpose trusted computing bases added to desktop computers resulted in costly and rather heavyweight solutions. In contrast, we present Soteria, a lightweight solution for low-cost embedded systems. At its heart, Soteria is a program-counter based memory access control extension for the TI MSP430 microprocessor. Based on our open implementation of Soteria as an openMSP430 extension, and our FPGA-based evaluation, we show that the proposed solution has a minimal performance, size and cost overhead while effectively protecting the confidentiality and integrity of an application's code against all kinds of software attacks including attacks from the system level.},
booktitle = {Proceedings of the 31st Annual Computer Security Applications Conference},
pages = {241–250},
numpages = {10},
keywords = {Trusted Computing, Software Protection, Embedded Systems},
location = {Los Angeles, CA, USA},
series = {ACSAC '15}
}

@inproceedings{10.1145/1944892.1944896,
author = {Mann, Stefan and Rock, Georg},
title = {Control variant-rich models by variability measures},
year = {2011},
isbn = {9781450305709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944892.1944896},
doi = {10.1145/1944892.1944896},
abstract = {The embedded systems market and especially the software part of it is growing drastically in automotive industry. Today we see that the value of software or functionality that is realized using software within cars is about 35% of the value of the car itself. We have typically more than 70 embedded control units (ECUs) in a car with functionality realized and controlled by software. The standardization of communication interfaces and operating system functionality as for example realized by AUTOSAR facilitates the distributed development of software. But the needs to produce software in time and in budget remain still a main task in automotive software industry. To cope with tight project plans, process models based on product line technology promise a good chance to be successful. Nevertheless, the need to control the product development remains still one of the most important questions in this area.The work presented here gives some new insights into the definition and application of measures with special emphasis on the variability aspects used within a product line development. Several known techniques as for example atomic sets or formal variability analysis are revisited and used within the context of variability metrics. The measures are categorized and can be used within a project to control and manage the defined variability.},
booktitle = {Proceedings of the 5th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {29–38},
numpages = {10},
keywords = {variability, product line, metrics, measures, feature model, architecture, architectural description},
location = {Namur, Belgium},
series = {VaMoS '11}
}

@article{10.1145/3644391,
author = {Yao, Rujing and Wu, Ou},
title = {A Taxonomy for Learning with Perturbation and Algorithms},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3644391},
doi = {10.1145/3644391},
abstract = {Weighting strategy prevails in machine learning. For example, a common approach in robust machine learning is to exert low weights on samples which are likely to be noisy or quite hard. This study summarizes another less-explored strategy, namely, perturbation. Various incarnations of perturbation have been utilized but it has not been explicitly revealed. Learning with perturbation is called perturbation learning and a systematic taxonomy is constructed for it in this study. In our taxonomy, learning with perturbation is divided on the basis of the perturbation targets, directions, inference manners, and granularity levels. Many existing learning algorithms including some classical ones can be understood with the constructed taxonomy. Alternatively, these algorithms share the same component, namely, perturbation in their procedures. Furthermore, a family of new learning algorithms can be obtained by varying existing learning algorithms with our taxonomy. Specifically, three concrete new learning algorithms are proposed for robust machine learning. Extensive experiments on image classification and text sentiment analysis verify the effectiveness of the three new algorithms. Learning with perturbation can also be used in other various learning scenarios, such as imbalanced learning, clustering, regression, and so on.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {129},
numpages = {38},
keywords = {Sample weighting, perturbation, robust machine learning, learning taxonomy}
}

@inproceedings{10.1145/3616195.3616220,
author = {Elliot, Alan and McGregor, Iain},
title = {FASS: Firefighter Audio Safety Systems},
year = {2023},
isbn = {9798400708183},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616195.3616220},
doi = {10.1145/3616195.3616220},
abstract = {A series of auditory cues were designed to assist firefighters with navigation and general safety in a fire emergency. Firefighters must maintain situational awareness at all times and this can be lost with disorientation, which is one of the main causes of injury and even death. Disorientation can be caused by restricted vision due to heavy smoke, a lack of familiarity with the surroundings as well as hearing and communication difficulties caused by the intensity of the fireground sounds. Five professional firefighters were interviewed to identify ways in which auditory affordances could be used to support their work. Existing sounds from both the emergency environment and those generated by firefighting equipment were assessed to determine their importance in maintaining situational awareness. Noise reduction technology was investigated, to assess its potential use in limiting the levels of noise exposure experienced. A series of auditory cues were designed to address the issues that were found using binaural spatialization and Augmented Reality methods. A prototype system was presented to firefighters to determine its effectiveness. The firefighters found that noise reduction would be effective in improving their situational awareness and ability to communicate effectively. Additionally, the firefighters found that spatially placed auditory cues had the potential to be effective in navigation and orientation in a fire emergency. The findings suggest that the use of noise reduction and auditory affordances have the potential to improve situational awareness for firefighters, increase safety and potentially save lives.},
booktitle = {Proceedings of the 18th International Audio Mostly Conference},
pages = {177–184},
numpages = {8},
location = {Edinburgh, United Kingdom},
series = {AM '23}
}

@proceedings{10.1145/3622748,
title = {SBCARS '23: Proceedings of the 17th Brazilian Symposium on Software Components, Architectures, and Reuse},
year = {2023},
isbn = {9798400709524},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Campo Grande, Brazil}
}

@proceedings{10.1145/3639478,
title = {ICSE-Companion '24: Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ICSE is the leading and, by far, the largest conference in Software Engineering, attracting researchers, practitioners, and students worldwide. ICSE2024 is co-located with 11 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/1233901.1233908,
author = {Navarro, Luis Daniel Benavides and Schwanninger, Christa and Sobotzik, Robert and S\"{u}dholt, Mario},
title = {ATOLL: aspect-oriented toll system},
year = {2007},
isbn = {9781595936578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1233901.1233908},
doi = {10.1145/1233901.1233908},
abstract = {Product line development places emphasis on quality attributes like understandability, maintainability, reusability and variability. Better modularization techniques like aspect-oriented programming are supposed to improve these attributes.In the context of an industrial case study in the domain of infrastructure software for toll systems from Siemens AG, Germany, we have investigated how OO designs can be enhanced using AO techniques. We have explored, in particular, how sequential crosscutting concerns can be modularized using AspectJ and how distributed ones can be modularized using AWED, a system that features aspects with explicit distribution. Concretely, we show how sequential and distributed aspects improve the implementation of the charge calculation functionality that is central to real-world tolling systems.},
booktitle = {Proceedings of the 6th Workshop on Aspects, Components, and Patterns for Infrastructure Software},
pages = {7–es},
keywords = {software product lines, aspect-oriented software development},
location = {Vancouver, British Columbia, Canada},
series = {ACP4IS '07}
}

@inproceedings{10.1145/3167020.3167063,
author = {Stanchev, Peter L. and Paneva-Marinova, Desislava and Iliev, Alexander},
title = {Enhanced User Experience and Behavioral Patterns for Digital Cultural Ecosystems},
year = {2017},
isbn = {9781450348959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167020.3167063},
doi = {10.1145/3167020.3167063},
abstract = {The world's digital content and media is growing rapidly at a never stopping rate. There are millions of digital media assets on display through mobile devices, home entertainment systems or computers. The vast pool of visual and audio information has to therefore be grouped in different ecosystems depending on their nature or intended audience to simplify the problem of searching, finding and personalizing datasets on demand. Though such is the case for the Digital Cultural Ecosystems, we still need to introduce number of smart methodologies to make the process of narrowing down vast number of digital assets in order to arrive at a desirable media and essentially personalize and automate the approach. In this paper, we propose a method that deals with the detection, extraction and personalization of media assets applied to the world of digital cultural ecosystems.},
booktitle = {Proceedings of the 9th International Conference on Management of Digital EcoSystems},
pages = {287–292},
numpages = {6},
keywords = {Sentiment Recognition, Non-Formal Learning, Image and Speech Processing, Human Behavior, Emotion Recognition, Digital Culture Ecosystem, Digital Cultural Assets},
location = {Bangkok, Thailand},
series = {MEDES '17}
}

@inproceedings{10.1145/3439961.3439975,
author = {Pald\^{e}s, Roberto Avila and Canedo, Edna Dias and Guimar\~{a}es, Fernando de Albuquerque and Calazans, Ang\'{e}lica Toffano Seidel},
title = {Functional Requirements Elicitation in IoT Systems: a follow-up study},
year = {2021},
isbn = {9781450389235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439961.3439975},
doi = {10.1145/3439961.3439975},
abstract = {As the Internet of Things (IoT) advances, specific views have been proposed for the entire software development cycle and also for Requirements Engineering (RE). The analysis of the use of RE techniques, tools, and models can contribute to obtain better results in this field. This paper presents a Systematic Mapping Study (SMS) to investigate techniques for Functional Requirements (FR) elicitation in IoT software systems, as well as gaps and limitations of current solutions. During the SMS, seventeen articles focused on FR in the IoT were found. The analysis was complemented with an input from the experience of practitioners who have dedicated to this topic, obtained through structured and semi-structured interviews. The results show that FR elicitation has started from the use of traditional techniques, but that these do not fully meet the specificities of the IoT. The majority of the models found are based on UML (Unified Modeling Language) and the most important techniques are based on scenarios. The tools that support these proposals are maturing or under development. In the conclusion, the study shows the advancements already achieved, as well as the challenges and opportunities that are still present.},
booktitle = {Proceedings of the XIX Brazilian Symposium on Software Quality},
articleno = {14},
numpages = {10},
keywords = {Systematic Mapping Study., Software System, Internet of Things, Functional Requirements Elicitation},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {SBQS '20}
}

@article{10.1145/2790303,
author = {Garc\'{\i}a-gal\'{a}n, Jes\'{u}s and Pasquale, Liliana and Trinidad, Pablo and Ruiz-Cort\'{e}s, Antonio},
title = {User-Centric Adaptation Analysis of Multi-Tenant Services},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/2790303},
doi = {10.1145/2790303},
abstract = {Multi-tenancy is a key pillar of cloud services. It allows different users to share computing and virtual resources transparently, meanwhile guaranteeing substantial cost savings. Due to the tradeoff between scalability and customization, one of the major drawbacks of multi-tenancy is limited configurability. Since users may often have conflicting configuration preferences, offering the best user experience is an open challenge for service providers. In addition, the users, their preferences, and the operational environment may change during the service operation, thus jeopardizing the satisfaction of user preferences. In this article, we present an approach to support user-centric adaptation of multi-tenant services. We describe how to engineer the activities of the Monitoring, Analysis, Planning, Execution (MAPE) loop to support user-centric adaptation, and we focus on adaptation analysis. Our analysis computes a service configuration that optimizes user satisfaction, complies with infrastructural constraints, and minimizes reconfiguration obtrusiveness when user- or service-related changes take place. To support our analysis, we model multi-tenant services and user preferences by using feature and preference models, respectively. We illustrate our approach by utilizing different cases of virtual desktops. Our results demonstrate the effectiveness of the analysis in improving user preferences satisfaction in negligible time.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = jan,
articleno = {24},
numpages = {26},
keywords = {multi-tenant services, human information processing, User systems}
}

@inproceedings{10.1145/2096123.2096134,
author = {Hazra, Jagabondhu and Das, Kaushik and Seetharam, Deva P. and Singhee, Amith},
title = {Stream computing based synchrophasor application for power grids},
year = {2011},
isbn = {9781450310611},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2096123.2096134},
doi = {10.1145/2096123.2096134},
abstract = {This paper proposes an application of stream computing analytics framework to high speed synchrophasor data for real time monitoring and control of electric grid. High volume streaming synchrophasor data from geographically distributed grid sensors (namely, Phasor Measurement Units) are collected, synchronized, aggregated when required and analyzed using a stream computing platform to estimate the grid stability in real time. This real time stability monitoring scheme will help the grid operators to take preventive or corrective measures ahead of time to mitigate any disturbance before they develop into wide-spread. A protptype of the scheme is demonstrated on a benchmark 3 machines 9 bus system and the IEEE 14 bus test system.},
booktitle = {Proceedings of the First International Workshop on High Performance Computing, Networking and Analytics for the Power Grid},
pages = {43–50},
numpages = {8},
keywords = {voltage stability, synchrophasor, stream computing, power grid},
location = {Seattle, Washington, USA},
series = {HiPCNA-PG '11}
}

@article{10.1109/TASLP.2014.2359628,
author = {Chen, Austin and Hasegawa-Johnson, Mark A.},
title = {Mixed stereo audio classification using a stereo-input mixed-to-panned level feature},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2359628},
doi = {10.1109/TASLP.2014.2359628},
abstract = {Many past studies have been conducted on speech/music discrimination due to the potential applications for broadcast and other media; however, it remains possible to expand the experimental scope to include samples of speech with varying amounts of background music. This paper focuses on the development and evaluation of two measures of the ratio between speech energy and music energy: a reference measure called speech-to-music ratio (SMR), which is known objectively only prior to mixing, and a feature called the stereo-input mix-to-peripheral level feature (SIMPL), which is computed from the stereo mixed signal as an imprecise estimate of SMR. SIMPL is an objective signal measure calculated by taking advantage of broadcast mixing techniques in which vocals are typically placed at stereo center, unlike most instruments. Conversely, SMR is a hidden variable defined by the relationship between the powers of portions of audio attributed to speech and music. It is shown that SIMPL is predictive of SMR and can be combined with state-of-the-art features in order to improve performance. For evaluation, this new metric is applied in speech/music (binary) classification, speech/music/mixed (trinary) classification, and a new speech-to-music ratio estimation problem. Promising results are achieved, including 93.06% accuracy for trinary classification and 3.86 dB RMSE for estimation of the SMR.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2025–2033},
numpages = {9},
keywords = {speech/music discrimination, speech processing, music processing, music information retrieval, mel-frequency cepstral coefficients, classification algorithms, audio segmentation, audio processing, audio classification, Gaussian mixture model}
}

@inproceedings{10.1145/2554850.2555034,
author = {Guimaraes, Everton and Garcia, Alessandro and Farias, Kleinner},
title = {On the impact of obliviousness and quantification on model composition effort},
year = {2014},
isbn = {9781450324694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554850.2555034},
doi = {10.1145/2554850.2555034},
abstract = {Researchers and practitioners advocate that design properties, such as obliviousness and quantification, can improve the modularity of software systems, thereby reducing the effort of composing design models. However, there is no empirical knowledge about how these design properties impact model composition effort. This paper, therefore, performs an empirical study to understand this impact. The main contributions are: (i) quantitative indicators to evaluate to what extent such design properties impact model composition effort; (ii) an objective evaluation of the impact of such modularity properties in 26 versions of two software projects by using statistical tests; and (iii) lessons learned on whether (and how) modularity anomalies related to misuse of quantification and obliviousness in the input models can significantly increase model composition effort.},
booktitle = {Proceedings of the 29th Annual ACM Symposium on Applied Computing},
pages = {1043–1048},
numpages = {6},
keywords = {modularity, model composition, measurement, aspect-oriented modeling},
location = {Gyeongju, Republic of Korea},
series = {SAC '14}
}

@article{10.1145/3631405,
author = {Demirel, Berken Utku and Dang, Ting and Al-Naimi, Khaldoon and Kawsar, Fahim and Montanari, Alessandro},
title = {Unobtrusive Air Leakage Estimation for Earables with In-ear Microphones},
year = {2024},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
url = {https://doi.org/10.1145/3631405},
doi = {10.1145/3631405},
abstract = {Earables (in-ear wearables) are gaining increasing attention for sensing applications and healthcare research thanks to their ergonomy and non-invasive nature. However, air leakages between the device and the user's ear, resulting from daily activities or wearing variabilities, can decrease the performance of applications, interfere with calibrations, and reduce the robustness of the overall system. Existing literature lacks established methods for estimating the degree of air leaks (i.e., seal integrity) to provide information for the earable applications. In this work, we proposed a novel unobtrusive method for estimating the air leakage level of earbuds based on an in-ear microphone. The proposed method aims to estimate the magnitude of distortions, reflections, and external noise in the ear canal while excluding the speaker output by learning the speaker-to-microphone transfer function which allows us to perform the task unobtrusively. Using the obtained residual signal in the ear canal, we extract three features and deploy a machine-learning model for estimating the air leakage level. We investigated our system under various conditions to validate its robustness and resilience against the motion and other artefacts. Our extensive experimental evaluation shows that the proposed method can track air leakage levels under different daily activities."The best computer is a quiet, invisible servant."~Mark Weiser},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jan,
articleno = {156},
numpages = {29},
keywords = {ear canal sealing, transfer function approximation, unobtrusive}
}

@article{10.1145/3532182,
author = {Arrieta, Aitor and Valle, Pablo and Agirre, Joseba A. and Sagardui, Goiuria},
title = {Some Seeds Are Strong: Seeding Strategies for Search-based Test Case Selection},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3532182},
doi = {10.1145/3532182},
abstract = {The time it takes software systems to be tested is usually long. Search-based test selection has been a widely investigated technique to optimize the testing process. In this article, we propose a set of seeding strategies for the test case selection problem that generates the initial population of Pareto-based multi-objective algorithms, with the goals of (1) helping to find an overall better set of solutions and (2) enhancing the convergence of the algorithms. The seeding strategies were integrated with four state-of-the-art multi-objective search algorithms and applied into two contexts where regression-testing is paramount: (1) Simulation-based testing of Cyber-physical Systems and (2) Continuous Integration. For the first context, we evaluated our approach by using six fitness function combinations and six independent case studies, whereas in the second context, we derived a total of six fitness function combinations and employed four case studies. Our evaluation suggests that some of the proposed seeding strategies are indeed helpful for solving the multi-objective test case selection problem. Specifically, the proposed seeding strategies provided a higher convergence of the algorithms towards optimal solutions in 96% of the studied scenarios and an overall cost-effectiveness with a standard search budget in 85% of the studied scenarios.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {17},
numpages = {47},
keywords = {regression testing, search-based software testing, Test case selection}
}

@article{10.1145/3597457,
author = {Liu, Tiantian and Wang, Chao and Li, Zhengxiong and Huang, Ming-Chun and Xu, Wenyao and Lin, Feng},
title = {Wavoice: An mmWave-Assisted Noise-Resistant Speech Recognition System},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1550-4859},
url = {https://doi.org/10.1145/3597457},
doi = {10.1145/3597457},
abstract = {As automatic speech recognition evolves, deployment of the voice user interface (VUI) has boomingly expanded. Especially since the COVID-19 pandemic, the VUI has gained more attention in online communication owing to its non-contact property. However, the VUI struggles to be applied in public scenes due to the degradation of received audio signals caused by various ambient noises. In this article, we propose Wavoice, the first noise-resistant multi-modal speech recognition system that fuses two distinct voices sensing modalities (i.e., millimeter-wave signals and audio signals from a microphone) together. One key contribution is to model the inherent correlation between millimeter-wave and audio signals. Based on it, Wavoice facilitates the real-time noise-resistant voice activity detection and user targeting from multiple speakers. Additionally, we elaborate on two novel modules for multi-modal fusion embedded into the neural network, leading to accurate speech recognition. Extensive experiments prove the effectiveness of Wavoice under adverse conditions—that is, the character recognition error rate below 1% in a range of 7 m. In terms of robustness and accuracy, Wavoice considerably outperforms existing audio-only speech recognition methods with lower character error and word error rates.},
journal = {ACM Trans. Sen. Netw.},
month = may,
articleno = {86},
numpages = {29},
keywords = {Multi-modal systems, mmWave sensing, speech recognition, biometrics}
}

@inproceedings{10.1145/2950290.2950311,
author = {Nguyen, ThanhVu and Koc, Ugur and Cheng, Javran and Foster, Jeffrey S. and Porter, Adam A.},
title = {iGen: dynamic interaction inference for configurable software},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2950311},
doi = {10.1145/2950290.2950311},
abstract = {To develop, analyze, and evolve today's highly configurable software systems, developers need deep knowledge of a system's configuration options, e.g., how options need to be set to reach certain locations, what configurations to use for testing, etc. Today, acquiring this detailed information requires manual effort that is difficult, expensive, and error prone. In this paper, we propose iGen, a novel, lightweight dynamic analysis technique that automatically discovers a program's interactions---expressive logical formulae that give developers rich and detailed information about how a system's configuration option settings map to particular code coverage. iGen employs an iterative algorithm that runs a system under a small set of configurations, capturing coverage data; processes the coverage data to infer potential interactions; and then generates new configurations to further refine interactions in the next iteration. We evaluated iGen on 29 programs spanning five languages; the breadth of this study would be unachievable using prior interaction inference tools. Our results show that iGen finds precise interactions based on a very small fraction of the number of possible configurations. Moreover, iGen's results confirm several earlier hypotheses about typical interaction distributions and structures.},
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {655–665},
numpages = {11},
keywords = {software testing, dynamic analysis, configurable systems, Program analysis},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@article{10.1109/TASLP.2015.2425955,
author = {Shah, Pratik and Lewis, Ian and Grant, Steven and Angrignon, Sylvain},
title = {Nonlinear acoustic echo cancellation using voltage and current feedback},
year = {2015},
issue_date = {Octember 2015},
publisher = {IEEE Press},
volume = {23},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2425955},
doi = {10.1109/TASLP.2015.2425955},
abstract = {Acoustic echo cancellation (AEC) is a well studied problem. The underlying assumption in most echo cancellation solutions is that the echo path following the reference signal is completely linear. However, in many handheld devices, the echo path following the reference signal is nonlinear. The reason for this nonlinearity in the echo path is the use of smaller inexpensive loudspeakers and the desire for generating high sound pressure levels. This brings about the need for a nonlinear echo canceler to maintain the required echo return loss enhancement (ERLE). Many software-based solutions have been proposed to solve this problem, but the computational complexity of these solutions is prohibitively high for practical implementation. This paper analyzes the sources of nonlinearities in smartphones and proposes a simple, elegant hardware modification to significantly reduce nonlinear echo. Thorough analysis and intensive testing results show that up to 6 dB of improvement in ERLE in a real device is possible using the proposed technique.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1589–1599},
numpages = {11},
keywords = {voltage feedback, nonlinear echo cancellation, current feedback}
}

@article{10.1109/TASLP.2024.3378099,
author = {Leer, Peter and Jensen, Jesper and Tan, Zheng-Hua and \O{}stergaard, Jan and Bramsl\o{}w, Lars},
title = {How to Train Your Ears: Auditory-Model Emulation for Large-Dynamic-Range Inputs and Mild-to-Severe Hearing Losses},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3378099},
doi = {10.1109/TASLP.2024.3378099},
abstract = {Advanced auditory models are useful in designing signal-processing algorithms for hearing-loss compensation or speech enhancement. Such auditory models provide rich and detailed descriptions of the auditory pathway, and might allow for individualization of signal-processing strategies, based on physiological measurements. However, these auditory models are often computationally demanding, requiring significant time to compute. To address this issue, previous studies have explored the use of deep neural networks to emulate auditory models and reduce inference time. While these deep neural networks offer impressive efficiency gains in terms of computational time, they may suffer from uneven emulation performance as a function of auditory-model frequency-channels and input sound pressure level, making them unsuitable for many tasks. In this study, we demonstrate that the conventional machine-learning optimization objective used in existing state-of-the-art methods is the primary source of this limitation. Specifically, the optimization objective fails to account for the frequency- and level-dependencies of the auditory model, caused by a large input dynamic range and different types of hearing losses emulated by the auditory model. To overcome this limitation, we propose a new optimization objective that explicitly embeds the frequency- and level-dependencies of the auditory model. Our results show that this new optimization objective significantly improves the emulation performance of deep neural networks across relevant input sound levels and auditory-model frequency channels, without increasing the computational load during inference. Addressing these limitations is essential for advancing the application of auditory models in signal-processing tasks, ensuring their efficacy in diverse scenarios.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {2006–2020},
numpages = {15}
}

@inproceedings{10.1145/1023833.1023847,
author = {Beltrame, Giovanni and Palermo, Gianluca and Sciuto, Donatella and Silvano, Cristina},
title = {Plug-in of power models in the StepNP exploration platform: analysis of power/performance trade-offs},
year = {2004},
isbn = {1581138903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1023833.1023847},
doi = {10.1145/1023833.1023847},
abstract = {In this paper, we propose a power/performance estimation layer designed for StepNP, a system-level architecture simulation and exploration platform for Network Processors and Multi-Processor Systems-on-Chip (MP-SoCs). The first goal of our work is to plug-in PIRATE, a parameterizable Network on-Chip in the StepNP platform, to support a fast exploration of on-chip interconnection networks. Up to now, StepNP does not provide any energy profiling, so our second goal is to dynamically plug-in power models of the different system components to provide power estimates quickly. The proposed power/performance exploration framework is based on a power characterization methodology and a system-level simulator to dynamically profile the given network application. This framework is intended to be used at different levels of the design, considering several levels of accuracy and taking full advantage of the StepNP performance profiling features. Experimental results are provided for the exploration of an ARM-based MP-SOC including a configurable NoC-IP executing an IPv4 forwarding application.},
booktitle = {Proceedings of the 2004 International Conference on Compilers, Architecture, and Synthesis for Embedded Systems},
pages = {85–92},
numpages = {8},
keywords = {platform based design, network on chip, multiprocessor, low-power design},
location = {Washington DC, USA},
series = {CASES '04}
}

@article{10.1145/2597999,
author = {Akiki, Pierre A. and Bandara, Arosha K. and Yu, Yijun},
title = {Adaptive Model-Driven User Interface Development Systems},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2597999},
doi = {10.1145/2597999},
abstract = {Adaptive user interfaces (UIs) were introduced to address some of the usability problems that plague many software applications. Model-driven engineering formed the basis for most of the systems targeting the development of such UIs. An overview of these systems is presented and a set of criteria is established to evaluate the strengths and shortcomings of the state of the art, which is categorized under architectures, techniques, and tools. A summary of the evaluation is presented in tables that visually illustrate the fulfillment of each criterion by each system. The evaluation identified several gaps in the existing art and highlighted the areas of promising improvement.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {9},
numpages = {33},
keywords = {model-driven engineering, Adaptive user interfaces}
}

@article{10.1145/3351342,
author = {Strobl, Eric V. and Spirtes, Peter L. and Visweswaran, Shyam},
title = {Estimating and Controlling the False Discovery Rate of the PC Algorithm Using Edge-specific P-Values},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3351342},
doi = {10.1145/3351342},
abstract = {Many causal discovery algorithms infer graphical structure from observational data. The PC algorithm in particular estimates a completed partially directed acyclic graph (CPDAG), or an acyclic graph containing directed edges identifiable with conditional independence testing. However, few groups have investigated strategies for estimating and controlling the false discovery rate (FDR) of the edges in the CPDAG. In this article, we introduce PC with p-values (PC-p), a fast algorithm that robustly computes edge-specific p-values and then estimates and controls the FDR across the edges. PC-p specifically uses the p-values returned by many conditional independence (CI) tests to upper bound the p-values of more complex edge-specific hypothesis tests. The algorithm then estimates and controls the FDR using the bounded p-values and the Benjamini-Yekutieli FDR procedure. Modifications to the original PC algorithm also help PC-p accurately compute the upper bounds despite non-zero Type II error rates. Experiments show that PC-p yields more accurate FDR estimation and control across the edges in a variety of CPDAGs compared to alternative methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {46},
numpages = {37},
keywords = {false discovery rate, directed acyclic graph, causal inference, PC algorithm, Bayesian network}
}

@inproceedings{10.5555/3351736.3351783,
author = {Lettner, Daniela and Eder, Klaus and Gr\"{u}nbacher, Paul and Pr\"{a}hofer, Herbert},
title = {Feature modeling of two large-scale industrial software systems: experiences and lessons learned},
year = {2015},
isbn = {9781467369084},
publisher = {IEEE Press},
abstract = {Feature models are frequently used to capture the knowledge about configurable software systems and product lines. However, feature modeling of large-scale systems is challenging as many models are needed for diverse purposes. For instance, feature models can be used to reflect the perspectives of product management, technical solution architecture, or product configuration. Furthermore, models are required at different levels of granularity. Although numerous approaches and tools are available, it remains hard to define the purpose, scope, and granularity of feature models. In this paper we thus present experiences of developing feature models for two large-scale industrial automation software systems. Specifically, we extended an existing feature modeling tool to support models for different purposes and at multiple levels. We report results on the characteristics and modularity of the feature models, including metrics about model dependencies. We further discuss lessons learned during the modeling process.},
booktitle = {Proceedings of the 18th International Conference on Model Driven Engineering Languages and Systems},
pages = {386–395},
numpages = {10},
keywords = {industrial software systems, feature modeling, experience report},
location = {Ottawa, Ontario, Canada},
series = {MODELS '15}
}

@inproceedings{10.5555/2663546.2663560,
author = {Pascual, Gustavo G. and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Run-time adaptation of mobile applications using genetic algorithms},
year = {2013},
isbn = {9781467344012},
publisher = {IEEE Press},
abstract = {Mobile applications run in environments where the context is continuously changing. Therefore, it is necessary to provide support for the run-time adaptation of these applications. This support is usually achieved by middleware platforms that offer a context-aware dynamic reconfiguration service. However, the main shortcoming of existing approaches is that both the list of possible configurations and the plans to adapt the application to a new configuration are usually specified at design-time. In this paper we present an approach that allows the automatic generation at run-time of application configurations and of reconfiguration plans. Moreover, the generated configurations are optimal regarding the provided functionality and, more importantly, without exceeding the available resources (e.g. battery). This is performed by: (1) having the information about the application variability available at runtime using feature models, and (2) using a genetic algorithm that allows generating an optimal configuration at runtime. We have specified a case study and evaluated our approach, and the results show that it is efficient enough as to be used on mobile devices without introducing an excessive overhead.},
booktitle = {Proceedings of the 8th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {73–82},
numpages = {10},
keywords = {middleware, genetic algorithms, feature models, dynamic reconfiguration, context, autonomic computing},
location = {San Francisco, California},
series = {SEAMS '13}
}

@inproceedings{10.1145/3377024.3377047,
author = {Mahmood, Wardah and Chagama, Moses and Berger, Thorsten and Hebig, Regina},
title = {Causes of merge conflicts: a case study of ElasticSearch},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377047},
doi = {10.1145/3377024.3377047},
abstract = {Software branching and merging allows collaborative development and creating software variants, commonly referred to as clone &amp; own. While simple and cheap, a trade-off is the need to merge code and to resolve merge conflicts, which frequently occur in practice. When resolving conflicts, a key challenge for developer is to understand the changes that led to the conflict. While merge conflicts and their characteristics are reasonably well understood, that is not the case for the actual changes that cause them.We present a case study of the changes---on the code and on the project-level (e.g., feature addition, refactoring, feature improvement)---that lead to conflicts. We analyzed the development history of ElasticSearch, a large open-source project that heavily relies on branching (forking) and merging. We inspected 40 merge conflicts in detail, sampled from 534 conflicts not resolvable by a semi-structured merge tool. On a code (structural) level, we classified the semantics of changes made. On a project-level, we categorized the decisions that motivated these changes. We contribute a categorization of code- and project-level changes and a detailed dataset of 40 conflict resolutions with a description of both levels of changes. Similar to prior studies, most of our conflicts are also small; while our categorization of code-level changes surprisingly differs from that of prior work. Refactoring, feature additions and feature enhancements are the most common causes of merge conflicts, most of which could potentially be avoided with better development tooling.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {9},
numpages = {9},
keywords = {software merging, conflict resolution, case study},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1145/2556624.2556644,
author = {Janota, Mikol\'{a}\v{s} and Botterweck, Goetz and Marques-Silva, Joao},
title = {On lazy and eager interactive reconfiguration},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556644},
doi = {10.1145/2556624.2556644},
abstract = {An interactive configuration tool needs to provide feedback to the user on possible further decisions while respecting constraints of the product being configured. In the presence of a large number of product features, it reduces the configuration effort if users can start from a default configuration and adapt only those features that are important to them. Hence, rather than completing an empty configuration (empty product), it is easier to move from one complete configuration to another (from one product to another). This paper shows how to provide tool support for this approach to interactive configuration. Two types of algorithms, based on recent advancements in SAT technology, are introduced: lazy and eager. While the eager provides more information to the user, the lazy scales to configuration models with tens of thousands of features. This is confirmed by an experimental evaluation carried out with the implemented prototype.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {8},
numpages = {8},
keywords = {minimal correction sets, interactive configuration, SAT},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@article{10.1145/3660641,
author = {Diffallah, Zhor and Ykhlef, Hadjer and Bouarfa, Hafida},
title = {Teacher-Student Framework for Polyphonic Semi-supervised Sound Event Detection: Survey and Empirical Analysis},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3660641},
doi = {10.1145/3660641},
abstract = {Polyphonic sound event detection refers to the task of automatically identifying sound events occurring simultaneously in an auditory scene. Due to the inherent complexity and variability of real-world auditory scenes, building robust detectors for polyphonic sound event detection poses a significant challenge. The task becomes furthermore challenging without sufficient annotated data to develop sound event detection systems under a supervised learning regime. In this article, we explore the recent developments in polyphonic sound event detection, with a particular emphasis on the application of Teacher-Student techniques within the semi-supervised learning paradigm. Unlike previous works, we have consolidated and organized the fragmented literature on Teacher-Student techniques for polyphonic sound event detection. By examining the latest research, categorizing Teacher-Student approaches, and conducting an empirical study to assess the performance of each approach, this survey offers valuable insights and practical guidance for researchers and practitioners in the field. Our findings highlight the potential benefits of utilizing multiple learners, ensuring consistent predictions, and making thoughtful choices regarding perturbation strategies.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {90},
numpages = {44},
keywords = {Polyphonic sound event detection, Teacher-Student framework, semi-supervised learning}
}

@inproceedings{10.1145/3613904.3642125,
author = {Liu, Zhihao and Li, Yu and Tu, Fangyuan and Zhang, Ruiyuan and Cheng, Zhanglin and Yokoya, Naoto},
title = {DeepTreeSketch: Neural Graph Prediction for Faithful 3D Tree Modeling from Sketches},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642125},
doi = {10.1145/3613904.3642125},
abstract = {We present DeepTreeSketch, a novel AI-assisted sketching system that enables users to create realistic 3D tree models from 2D freehand sketches. Our system leverages a tree graph prediction network, TGP-Net, to learn the underlying structural patterns of trees from a large collection of 3D tree models. The TGP-Net simulates the iterative growth of botanical trees and progressively constructs the 3D tree structures in a bottom-up manner. Furthermore, our system supports a flexible sketching mode for both precise and coarse control of the tree shapes by drawing branch strokes and foliage strokes, respectively. Combined with a procedural generation strategy, users can freely control the foliage propagation with diverse and fine details. We demonstrate the expressiveness, efficiency, and usability of our system through various experiments and user studies. Our system offers a practical tool for 3D tree creation, especially for natural scenes in games, movies, and landscape applications.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {471},
numpages = {19},
keywords = {3D modeling interface, ideation, neural networks, sketching system},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.1145/3487921,
author = {Hezavehi, Sara M. and Weyns, Danny and Avgeriou, Paris and Calinescu, Radu and Mirandola, Raffaela and Perez-Palacin, Diego},
title = {Uncertainty in Self-adaptive Systems: A Research Community Perspective},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/3487921},
doi = {10.1145/3487921},
abstract = {One of the primary drivers for self-adaptation is ensuring that systems achieve their goals regardless of the uncertainties they face during operation. Nevertheless, the concept of uncertainty in self-adaptive systems is still insufficiently understood. Several taxonomies of uncertainty have been proposed, and a substantial body of work exists on methods to tame uncertainty. Yet, these taxonomies and methods do not fully convey the research community’s perception on what constitutes uncertainty in self-adaptive systems and on the key characteristics of the approaches needed to tackle uncertainty. To understand this perception and learn from it, we conducted a survey comprising two complementary stages in which we collected the views of 54 and 51 participants, respectively. In the first stage, we focused on current research and development, exploring how the concept of uncertainty is understood in the community and how uncertainty is currently handled in the engineering of self-adaptive systems. In the second stage, we focused on directions for future research to identify potential approaches to dealing with unanticipated changes and other open challenges in handling uncertainty in self-adaptive systems. The key findings of the first stage are: (a) an overview of uncertainty sources considered in self-adaptive systems, (b) an overview of existing methods used to tackle uncertainty in concrete applications, (c) insights into the impact of uncertainty on non-functional requirements, (d) insights into different opinions in the perception of uncertainty within the community and the need for standardised uncertainty-handling processes to facilitate uncertainty management in self-adaptive systems. The key findings of the second stage are: (a) the insight that over 70% of the participants believe that self-adaptive systems can be engineered to cope with unanticipated change, (b) a set of potential approaches for dealing with unanticipated change, (c) a set of open challenges in mitigating uncertainty in self-adaptive systems, in particular in those with safety-critical requirements. From these findings, we outline an initial reference process to manage uncertainty in self-adaptive systems. We anticipate that the insights on uncertainty obtained from the community and our proposed reference process will inspire valuable future research on self-adaptive systems.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = dec,
articleno = {10},
numpages = {36},
keywords = {survey, uncertainty challenges, unanticipated change, uncertainty methods, uncertainty models, uncertainty, Self-adaptation}
}

@article{10.1145/3229048,
author = {Zheng, Yongjie and Cu, Cuong and Taylor, Richard N.},
title = {Maintaining Architecture-Implementation Conformance to Support Architecture Centrality: From Single System to Product Line Development},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3229048},
doi = {10.1145/3229048},
abstract = {Architecture-centric development addresses the increasing complexity and variability of software systems by focusing on architectural models, which are generally easier to understand and manipulate than source code. It requires a mechanism that can maintain architecture-implementation conformance during architectural development and evolution. The challenge is twofold. There is an abstraction gap between software architecture and implementation, and both may evolve. Existing approaches are deficient in support for both change mapping and product line architecture. This article presents a novel approach named 1.x-way mapping and its extension, 1.x-line mapping to support architecture-implementation mapping in single system development and in product line development, respectively. They specifically address mapping architecture changes to code, maintaining variability conformance between product line architecture and code, and tracing architectural implementation. We built software tools named xMapper and xLineMapper to realize the two approaches, and conducted case studies with two existing open-source systems to evaluate the approaches. The result shows that our approaches are applicable to the implementation of a real software system and are capable of maintaining architecture-implementation conformance during system evolution.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {8},
numpages = {52},
keywords = {variability conformance, architecture-centric feature traceability, architecture-centric development, architectural evolution, Architecture-implementation mapping}
}

@inproceedings{10.1145/1852786.1852800,
author = {Kasurinen, Jussi and Taipale, Ossi and Smolander, Kari},
title = {Test case selection and prioritization: risk-based or design-based?},
year = {2010},
isbn = {9781450300391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1852786.1852800},
doi = {10.1145/1852786.1852800},
abstract = {The objective of this qualitative study was to observe and empirically study how software organizations decide on which test cases to select for their software projects. As the software test processes are limited in resources such as time or money, a selection process usually exists for tested features. In this study we conducted a survey on 31 software-producing organizations, and interviewed 36 software professionals from 12 focus organizations to gain a better insight into testing practices. Our findings indicated that the basic approaches to test case selection are usually oriented towards two possible objectives. One is the risk-based selection, where the aim is to focus testing on those parts that are too expensive to fix after launch. The other is design-based selection, where the focus is on ensuring that the software is capable of completing the core operations it was designed to do. These results can then be used to develop testing organizations and to identify better practices for test case selection.},
booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {10},
numpages = {10},
keywords = {test case selection, software testing, grounded theory, empirical study},
location = {Bolzano-Bozen, Italy},
series = {ESEM '10}
}

@article{10.1145/3356773.3356796,
author = {Neto, Amadeu Anderlin},
title = {A Strategy to Support Replications of Controlled Experiments in Software Engineering},
year = {2020},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3356773.3356796},
doi = {10.1145/3356773.3356796},
abstract = {Replication is essential to build knowledge in empirical science. Experiment replications reported in the software engineering context present variabilities on their experiment elements, e.g., variables, materials. Further understanding these variabilities could help planning lack of strategy to support the representation of experiment variabilities and commonalities. In addition, there is also a gap related to effective reuse and traceability of experiment elements. These problems are likely to hamper the replication understanding and planning. In order to overcome these gaps, we intend to create a conceptual model and a tool to support replication planning. To develop these solutions, we will use concepts of experimentation and software product lines. Our idea is to build a core structure which allows the configuration of experiment elements based commonalities with previous replications and desired variabilities to fit the specific replication purposes. In this paper we describe related work, our research methodology, as well as the current research progress and expected future contributions.},
journal = {SIGSOFT Softw. Eng. Notes},
month = oct,
pages = {23},
numpages = {1},
keywords = {experiment replication, experiment planning, experiment lines}
}

@inproceedings{10.1145/3307334.3326073,
author = {Xu, Chenhan and Li, Zhengxiong and Zhang, Hanbin and Rathore, Aditya Singh and Li, Huining and Song, Chen and Wang, Kun and Xu, Wenyao},
title = {WaveEar: Exploring a mmWave-based Noise-resistant Speech Sensing for Voice-User Interface},
year = {2019},
isbn = {9781450366618},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307334.3326073},
doi = {10.1145/3307334.3326073},
abstract = {Voice-user interface (VUI) has become an integral component in modern personal devices (textite.g., smartphones, voice assistant) by fundamentally evolving the information sharing between the user and device. Acoustic sensing for VUI is designed to sense all acoustic objects; however, the existing VUI mechanism can only offer low-quality speech sensing. This is due to the audible and inaudible interference from complex ambient noise that limits the performance of VUI by causing denial-of-service (DoS) of user requests. Therefore, it is of paramount importance to enable noise-resistant speech sensing in VUI for executing critical tasks with superior efficiency and precision in robust environments. To this end, we investigate the feasibility of employing radio-frequency signals, such as millimeter wave (mmWave) for sensing the noise-resistant voice of an individual. We first perform an in-depth study behind the rationale of voice generation and resulting vocal vibrations. From the obtained insights, we presentWaveEar, an end-to-end noise-resistant speech sensing system.WaveEar comprises a low-cost mmWave probe to localize the position of the speaker among multiple people and direct the mmWave signals towards the near-throat region of the speaker for sensing his/her vocal vibrations. The received signal, containing the speech information, is fed to our novel deep neural network for recovering the voice through exhaustive extraction. Our experimental evaluation under real-world scenarios with 21 participants shows the effectiveness ofWaveEar to precisely infer the noise-resistant voice and enable a pervasive VUI in modern electronic devices.},
booktitle = {Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {14–26},
numpages = {13},
keywords = {voice-user interface, speech recognition, neural network, mmwave},
location = {Seoul, Republic of Korea},
series = {MobiSys '19}
}

@article{10.1145/3654668,
author = {Huang, Junjian and Ren, Hao and Liu, Shulin and Liu, Yong and Lv, Chuanlu and Lu, Jiawen and Xie, Changyong and Lu, Hong},
title = {Real-Time Attentive Dilated U-Net for Extremely Dark Image Enhancement},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {8},
issn = {1551-6857},
url = {https://doi.org/10.1145/3654668},
doi = {10.1145/3654668},
abstract = {Images taken under low-light conditions suffer from poor visibility, color distortion, and graininess, all of which degrade the image quality and hamper the performance of downstream vision tasks, such as object detection and instance segmentation in the field of autonomous driving, making low-light enhancement an indispensable basic component of high-level visual tasks. Low-light enhancement aims to mitigate these issues, and has garnered extensive attention and research over several decades. The primary challenge in low-light image enhancement arises from the low signal-to-noise ratio caused by insufficient lighting. This challenge becomes even more pronounced in near-zero lux conditions, where noise overwhelms the available image information. Both traditional image signal processing pipeline and conventional low-light image enhancement methods struggle in such scenarios. Recently, deep neural networks have been used to address this challenge. These networks take unmodified RAW images as input and produce the enhanced sRGB images, forming a deep learning based image signal processing pipeline. However, most of these networks are computationally expensive and thus far from practical use. In this article, we propose a lightweight model called attentive dilated U-Net (ADU-Net) to tackle this issue. Our model incorporates several innovative designs, including an asymmetric U-shape architecture, dilated residual modules for feature extraction, and attentive fusion modules for feature fusion. The dilated residual modules provide strong representative capability, whereas the attentive fusion modules effectively leverage low-level texture information and high-level semantic information within the network. Both modules employ a lightweight design but offer significant performance gains. Extensive experiments demonstrate that our method is highly effective, achieving an excellent balance between image quality and computational complexity—that is, taking less than 4ms for a high-definition 4K image on a single GTX 1080Ti GPU and yet maintaining competitive visual quality. Furthermore, our method exhibits pleasing scalability and generalizability, highlighting its potential for widespread applicability.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jun,
articleno = {231},
numpages = {19},
keywords = {Low-light image enhancement, image signal processing, convolutional neural networks, attention mechanism}
}

@inproceedings{10.1145/3236024.3236069,
author = {Amar, Hen and Bao, Lingfeng and Busany, Nimrod and Lo, David and Maoz, Shahar},
title = {Using finite-state models for log differencing},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236069},
doi = {10.1145/3236024.3236069},
abstract = {Much work has been published on extracting various kinds of models from logs that document the execution of running systems. In many cases, however, for example in the context of evolution, testing, or malware analysis, engineers are interested not only in a single log but in a set of several logs, each of which originated from a different set of runs of the system at hand. Then, the difference between the logs is the main target of interest.  In this work we investigate the use of finite-state models for log differencing. Rather than comparing the logs directly, we generate concise models to describe and highlight their differences. Specifically, we present two algorithms based on the classic k-Tails algorithm: 2KDiff, which computes and highlights simple traces containing sequences of k events that belong to one log but not the other, and nKDiff, which extends k-Tails from one to many logs, and distinguishes the sequences of length k that are common to all logs from the ones found in only some of them, all on top of a single, rich model. Both algorithms are sound and complete modulo the abstraction defined by the use of k-Tails.  We implemented both algorithms and evaluated their performance on mutated logs that we generated based on models from the literature. We conducted a user study including 60 participants demonstrating the effectiveness of the approach in log differencing tasks. We have further performed a case study to examine the use of our approach in malware analysis. Finally, we have made our work available in a prototype web-application, for experiments.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {49–59},
numpages = {11},
keywords = {model inference, log analysis},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

